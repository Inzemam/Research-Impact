DETAILSDistribution, posting, or copying of this PDF is strictly prohibited without written permission of the National Academies Press.  (Request Permission) Unless otherwise indicated, all materials in this PDF are copyrighted by the National Academy of Sciences.Copyright © National Academy of Sciences. All rights reserved.THE NATIONAL ACADEMIES PRESSVisit the National Academies Press at NAP.edu and login or register to get:Œ  
Œ  10% off the price of print titles
Œ  Special offers and discountsGET THIS BOOKFIND RELATED TITLESThis PDF is available at SHARECONTRIBUTORS
http://nap.edu/10656Who Goes There?: Authentication Through the Lens of Privacy232 pages | 6 x 9 | PAPERBACKISBN 978-0-309-08896-1 | DOI 10.17226/10656Stephen T. Kent and Lynette I. Millett, Editors; Committee on AuthenticationTechnologies and Their Privacy Implications; Computer Science andTelecommunications Board; Division on Engineering and Physical Sciences;National Research CouncilWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Who Goes There?Authentication Through the Lens of Privacy
Committee on Authentication Technologies andTheir Privacy ImplicationsComputer Science and Telecommunications BoardDivision on Engineering and Physical SciencesStephen T. Kent and Lynette I. Millett, EditorsTHE NATIONAL ACADEMIES PRESSWashington, D.C.www.nap.eduWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.THE NATIONAL ACADEMIES PRESS    500 Fifth Street, N.W.    Washington, DC 20001
NOTICE:  The project that is the subject of this report was approved by the
Governing Board of the National Research Council, whose members are drawnfrom the councils of the National Academy of Sciences, the National Academy ofEngineering, and the Institute of Medicine.  The members of the committee re-

sponsible for the report were chosen for their special competences and with re-gard for appropriate balance.This study was supported by Office of Naval Research Grant Number N00014-00-1-0855, 
National Science Foundation Grant Number ANI-0090219, General ServicesAdministration Purchase Order Number GS00C00AM00228, Social Security Ad-
ministration Purchase Order Number 0440-01-50677, and Federal Chief Informa-tion Officers Council Award Number GS00C00AM00228. The Vadasz FamilyFoundation gave supplemental funding. Any opinions, findings, conclusions, or
recommendations expressed in this publication are those of the author(s) and donot necessarily reflect the views of the organizations or agencies that providedsupport for the project.International Standard Book Number 0-309-08896-8 (Book)International Standard Book Number 0-309-52654-X (PDF)Cover designed by Jennifer M. Bishop.Additional copies of this report are available from the National Academies Press,500 Fifth Street, N.W., Lockbox 285, Washington, DC 20055; (800) 624-6242 or(202) 334-3313 (in the Washington metropolitan area); Internet, http://www.nap.edu.Copyright 2003 by the National Academy of Sciences.  All rights reserved.
Printed in the United States of AmericaWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.The National Academy of Sciences is a private, nonprofit, self-perpetuating soci-
ety of distinguished scholars engaged in scientific and engineering research, dedi-cated to the furtherance of science and technology and to their use for the general
welfare.  Upon the authority of the charter granted to it by the Congress in 1863,
the Academy has a mandate that requires it to advise the federal government onscientific and technical matters.  Dr. Bruce M. Alberts is president of the National
Academy of Sciences.The National Academy of Engineering was established in 1964, under the charter
of the National Academy of Sciences, as a parallel organization of outstandingengineers.  It is autonomous in its administration and in the selection of its mem-bers, sharing with the National Academy of Sciences the responsibility for advis-
ing the federal government.  The National Academy of Engineering also sponsors
engineering programs aimed at meeting national needs, encourages educationand research, and recognizes the superior achievements of engineers.  Dr. Wm. A.

Wulf is president of the National Academy of Engineering.The Institute of Medicine was established in 1970 by the National Academy of
Sciences to secure the services of eminent members of appropriate professions inthe examination of policy matters pertaining to the health of the public.  The
Institute acts under the responsibility given to the National Academy of Sciences
by its congressional charter to be an adviser to the federal government and, uponits own initiative, to identify issues of medical care, research, and education.Dr.Harvey V. Fineberg is president of the Institute of Medicine.
The National Research Council was organized by the National Academy of Sci-
ences in 1916 to associate the broad community of science and technology with
the Academys purposes of furthering knowledge and advising the federal gov-ernment.  Functioning in accordance with general policies determined by the
Academy, the Council has become the principal operating agency of both the
National Academy of Sciences and the National Academy of Engineering in pro-viding services to the government, the public, and the scientific and engineeringcommunities.  The Council is administered jointly by both Academies and the

Institute of Medicine.  Dr. Bruce M. Alberts and Dr. Wm. A. Wulf are chair and
vice chair, respectively, of the National Research Council.www.national-academies.orgWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.vCOMMITTEE ON AUTHENTICATION TECHNOLOGIES ANDTHEIR PRIVACY IMPLICATIONSSTEPHEN T. KENT, BBN Technologies, Chair
MICHAEL ANGELO, Compaq Computer Corporation
STEVEN BELLOVIN, AT&T Labs Research
BOB BLAKLEY, IBM Tivoli Software
DREW DEAN, SRI International
BARBARA FOX, Microsoft Corporation
STEPHEN H. HOLDEN, University of Maryland, Baltimore
DEIRDRE MULLIGAN, University of California, Berkeley
JUDITH S. OLSON, University of Michigan
JOE PATO, HP Labs Cambridge
RADIA PERLMAN, Sun Microsystems
PRISCILLA M. REGAN, George Mason University
JEFFREY SCHILLER, Massachusetts Institute of Technology
SOUMITRA SENGUPTA, Columbia University
JAMES L. WAYMAN, San Jose State University
DANIEL J. WEITZNER, Massachusetts Institute of TechnologyStaff
LYNETTE I. MILLETT, Study Director and Program OfficerJENNIFER M. BISHOP, Senior Project Assistant (beginning October2001)SUZANNE OSSA, Senior Project Assistant (through September 2001)Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.viiCOMPUTER SCIENCE AND TELECOMMUNICATIONS BOARDDAVID D. CLARK, Massachusetts Institute of Technology, ChairERIC BENHAMOU, 3Com Corporation
ELAINE COHEN, University of Utah
THOMAS E. DARCIE, University of Victoria
MARK E. DEAN, IBM Thomas J. Watson Research Center

JOSEPH FARRELL, University of California, Berkeley
JOAN FEIGENBAUM, Yale University
HECTOR GARCIA-MOLINA, Stanford University
RANDY H. KATZ, University of California, Berkeley

WENDY A. KELLOGG, IBM Thomas J. Watson Research Center
SARA KIESLER, Carnegie Mellon University
BUTLER W. LAMPSON, Microsoft Corporation, CSTB memberemeritusDAVID LIDDLE, U.S. Venture Partners
TERESA H. MENG, Stanford University

TOM M. MITCHELL, Carnegie Mellon University
DANIEL PIKE, GCI Cable and Entertainment

ERIC SCHMIDT, Google Inc.
FRED B. SCHNEIDER, Cornell University
BURTON SMITH, Cray Inc.
WILLIAM STEAD, Vanderbilt University

ANDREW J. VITERBI, Viterbi Group, LLC
JEANNETTE M. WING, Carnegie Mellon University
ALAN S. INOUYE, Interim Executive DirectorJON EISENBERG, Interim Assistant Director
KRISTEN BATCH, Research Associate
JENNIFER M. BISHOP, Senior Project Assistant
JANET BRISCOE, Administrative Officer
DAVID DRAKE, Senior Project Assistant
RENEE HAWKINS, Financial Associate
PHIL HILLIARD, Research Associate
MARGARET MARSH HUYNH, Senior Project Assistant
HERBERT S. LIN, Senior Scientist
LYNETTE I. MILLETT, Program Officer
DAVID PADGHAM, Research Associate
CYNTHIA A. PATTERSON, Program Officer
JANICE SABUDA, Senior Project AssistantWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.BRANDYE WILLIAMS, Staff AssistantSTEVEN WOO, Dissemination OfficerFor more information on CSTB, see its Web site at <http://www.cstb.org>, write to CSTB, National Research Council, 500 Fifth
Street, N.W., Washington, DC 20418; call at (202) 334-2605; or e-mail the
CSTB at cstb@nas.edu.viiiWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Preface
ixThe broadening use of the Internet implies that, more and more,people are communicating and sharing information with strang-
ers.  The result is growth in different kinds of demand to authenti-
cate system users, and the different motivations for requiring authentica-
tion imply different trade-offs in evaluating technical and nontechnical
options.  Motivations range from those related to system security (for

example, the ability to access critical systems or medical records) to those
related to business development (for example, the ability to use freeWeb-based resources or to have access to elements of electronic com-
merce).   The key questions surrounding these issues relate to what data

about a person are shared, how they are shared (including whether overtly
and cooperatively as well as by what technique), why they are shared
(fitting the purpose to the nature and amount of data), and how the data
are protected.Concerns that arise about adverse impacts on personal privacy fromparticular approaches to authentication may reflect judgments about the
rationale (e.g., how much information about a person is really needed to
authorize access to a particular system) as well as concern about the

soundness of the technical and procedural steps taken to protect the per-
sonal information gathered in the process of authentication.  Those con-

cerns are heightened by the growing ease of aggregation of information
collected from multiple sources (so-called data matching), the observed
tendency to collect information without an individuals knowledge, andWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.xPREFACEthe ease of publicizing or distributing personal information, like any otherinformation, via the Internet.THE COMMITTEE AND ITS CHARGEIn September 1999, the U.S. governments chief counselor for privacy,Peter Swire, met with the Computer Science and Telecommunications
Board (CSTB) in Washington, D.C., and described his need for studies of
biometrics and authentication.  Enthusiastic support by CSTB members,

given the importance of the topic and the ability to build on past CSTB
work, led to further discussion about initiating a project.  Richard Guida,
former chair of the Federal Public Key Infrastructure (FPKI) Steering
Committee and now with Johnson and Johnson, provided insight into
federal agency thinking about authentication and encouraged FPKI mem-
bers to be interested in and involved with the project.  The scope of the

project was broadened to encompass a range of authentication technolo-
gies and their privacy implications.  Funding for the project was obtained

from the National Science Foundation, the Office of Naval Research, the
General Services Administration, the Federal Chief Information Officers
Council, and the Social Security Administration.The task of the committee assembled by CSTBthe Committee onAuthentication Technologies and Their Privacy Implicationswas to ex-amine the interaction of authentication and privacy.  The committee

sought to identify the range of circumstances and the variety of environ-
ments in which greater or lesser degrees of identification are needed in
order to carry out governmental or commercial functions.  It also ad-

dressed ways in which law and policy can come to grips with the flaws
that are likely in the technology or its implementation. It considered how
the federal government can deploy improved authentication technologies
consistent with the desire to protect privacy.  It also examined the broad

implications of alternative approaches to selecting and implementing au-
thentication technologies by the federal government and others inter-
ested in their use.Consisting of 16 members from industry and academia (see Appen-dix A),  the committee was designed to have a range of technical expertise

relating to different kinds of authentication technologies and information-
system security technologies generally, to applications, and to the privacy
impacts of information technology and related policy.  The members

possess a range of computer science expertise (e.g., information system
security, cryptography, networking and distributed systems, human-
computer interaction) and associated nontechnical expertise (e.g., privacy
policy and law) as well as user perspectives (including organizations seek-
ing to employ authentication and end users with various concerns in suchWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PREFACExisectors as banking/finance and health).  One original committee member,
David Solo of Citigroup, was unable to continue his participation in the
project because of unforeseen time constraints.PROCESSEmpanelled during the winter of 2000, the committee met seven times
between March 2001 and August 2002 to plan its course of action, receive
testimony from relevant experts, deliberate on its findings, and draft its
final report.  It continued its work between meetings and into the fall and

end of 2002 by electronic communications.  During the course of its study,

the committee took briefings from information and authentication tech-
nology researchers and developers in industry and universities and from
leaders in government agencies involved in the development and deploy-
ment of authentication technologies.  It also heard from privacy and con-
sumer protection experts and representatives from various sectors of in-
dustry that use authentication technologies for business processes and
e-commerce.  The committee also went to VeriSign in California for a site
visit.  (See Appendix B
 for a complete list of briefers to the committee.)More than half of the committees meetings were held and most ofthis report was written after the events of September 11, 2001.  At its

October 2001 meeting, the committee decided, with CSTBs encourage-ment, to develop a short report addressing the concept of nationwide
identity systemsa topic that has received much media and policy atten-tion since the terrorist attacks.  Given that many of the committees dis-cussions and briefings were closely related to issues of identity and iden-
tification, the committee was well positioned to comment in a timely
fashion on the topic.  Supplemental funding for that activity was pro-

vided by the Vadasz Family Foundation.  That report was released in

April 2002 and is available from the National Academies Press.1ACKNOWLEDGMENTSAs with any project of this magnitude, thanks are due to the manyindividuals who contributed to the work of the committee. The commit-
tee thanks those who came to various meetings to provide briefings and
Warwick Ford for arranging the site visit at VeriSign in January.  Thanks

are also due to those who sponsored the study: the National Science Foun-1Computer Science and Telecommunications Board, National Research Council. IDsÑNot That Easy: Questions About Nationwide Identity Systems. Washington, D.C., National Acad-
emy Press, 2002.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.xiiPREFACEdation (George Strawn and Aubrey Bush), the Office of Naval Research(Andre van Tilborg), the General Services Administration (Mary Mitchell),

the Federal Chief Information Officers Council (Keith Thurston and Roger
Baker), and the Social Security Administration (Sara Hamer and Tony
Trenkle).  We are grateful to Peter Swire for commissioning the project, to

Richard Guida and Denise Silverberg for helping to muster support
through the FPKI Steering Committee, and to Kathi Webb of Rand for
providing early access to its biometrics study project.Finally, the committee thanks David D. Clark, chair of the CSTB, andMarjory S. Blumenthal, CSTBs director when this study was being car-ried out, for valuable insights.  The committee also thanks the following

members of the CSTB staff for their contributions.  Janet Briscoe provided

crucial administrative support, especially with the October 2001 work-
shop.  Suzanne Ossa was the initial senior project assistant for this project.

Jennifer Bishop took over as senior project assistant and provided signifi-
cant help with report preparation and editing; she also designed the cov-
ers of both this report and the earlier committee report and developed
many of the diagrams.  David Padgham provided background research

and descriptions of various pieces of legislation.  Wendy Edwards, an

intern with CSTB in the summer of 2002, also provided some background
research.  Steven J. Marcus made an editorial pass through an earlier draft

of the report, and Dorothy Sawicki and Liz Fikre made significant edito-
rial contributions in preparation for publishing.  Special thanks are due to

Lynette I. Millett, the study director for this project.  She worked very

closely with the chair and other committee members, transforming their
inputs into a coherent report that attempts to explain a complex topic in
an understandable fashion.Stephen T. Kent, ChairCommittee on Authentication
Technologies and Their Privacy
ImplicationsWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Acknowledgment of ReviewersxiiiThis report has been reviewed in draft form by individuals chosenfor their diverse perspectives and technical expertise, in accordance
with procedures approved by the National Research Councils Re-port Review Committee.  The purpose of this independent review is to

provide candid and critical comments that will assist the institution inmaking its published report as sound as possible and to ensure that thereport meets institutional standards for objectivity, evidence, and respon-
siveness to the study charge. The review comments and draft manuscript
remain confidential to protect the integrity of the deliberative process.
We wish to thank the following individuals for their review of this report:Ross Anderson, University of Cambridge,Scott Charney, Microsoft,
Carl Ellison, Intel Corporation,
Joel S. Engel, JSE Consulting,
Michael Froomkin, University of Miami School of Law,
John D. Halamka, Harvard Medical School,
Jerry Kang, University of California, Los Angeles,
Sally Katzen, Independent Consultant,
Deborah J. Mayhew, Deborah J. Mayhew and Associates,

Jeffrey Naughton, University of Wisconsin-Madison,
Marek Rejman-Greene, BTexaCT Technologies, and
Barbara Simons, IBM.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.xivACKNOWLEDGMENT OF REVIEWERSAlthough the reviewers listed above have provided many construc-tive comments and suggestions, they were not asked to endorse the con-
clusions or recommendations, nor did they see the final draft of the report
before its release.  The review of this report was overseen by Mildred S.

Dresselhaus and Randall Davis, both at the Massachusetts Institute of
Technology.  Appointed by the National Research Council, they were

responsible for making certain that an independent examination of this
report was carried out in accordance with institutional procedures and
that all review comments were carefully considered.  Responsibility for

the final content of this report rests entirely with the authoring committee
and the institution.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY1
1INTRODUCTION AND OVERVIEW16
Definitions and Terminology, 18
Authentication in Daily Life, 21
Current Tensions, 28
Four Overarching Privacy Concerns, 30
What This Report Does and Does Not Do, 312AUTHENTICATION IN THE ABSTRACT33
What Is Authentication and Why Is It Done?, 33Three Parties to Authentication, 36
Authenticating to Authorize, 37
Authenticating to Hold Accountable, 38What Do We Authenticate?, 41Identifiers, 42
Attributes, 43
Statements, 44How Do We Authenticate?, 45Authenticating Physical Identity, 47
Authenticating Psychological Identity, 47
Authenticating Possession of an Artifact, 49xvContentsWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.xviCONTENTSIdentification, 50The Relationship Between Authentication and Identification, 513PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS55
Privacy Impact of the Decision to Authenticate, 56
Access Control and Information Systems, 57
The Legal Foundations of Privacy, 62Constitutional Roots of Privacy, 63
The Common Law Roots of Privacy Law, 68
Statutory Privacy Protections, 69Information Privacy and Fair Information Practices, 71
Privacy of Communications, 75
Concluding Remarks, 784SECURITY AND USABILITY80
Threat Models, 81Threats, 81
Dealing with Threats, 84Authentication and PeopleUser-Centered Design, 86Lessons from User-Centered Design, 87
Lessons from Cognitive and Social Psychology, 90Factors Behind the Technology Choice, 95
Systems and Secondary Use, 97
Concluding Remarks, 1015AUTHENTICATION TECHNOLOGIES104
Technological Flavors of Authentication, 104
Basic Types of Authentication Mechanisms, 106Something You Know, 107
Something You Have, 110
Something You Are, 120Multifactor Authentication, 123
Centralized Versus Decentralized Authentication Systems, 125
Security Considerations for Individual AuthenticationTechnologies, 132Cost Considerations for Individual Authentication Technologies, 135
Concluding Remarks, 136Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.CONTENTSxvii6AUTHENTICATION, PRIVACY, AND THE ROLES138
OF GOVERNMENT
Regulator of Private Sector and Public Agency Behaviors
 and Processes, 140
Government-wide Law and Policy, 141
Agency- or Program-Specific Law and Policies, 145
Regulation of Private Sector Information ManagementActivity, 149Policy Activity in the Early 2000s, 151
Summary, 155Government as Issuer of Identity Documents, 155The Tangled Web of Government-Issued Identity Documents, 162
Threats to Foundational Documents, 165Government as Relying Party for Authentication Services, 169Access Certificates for Electronic Services, 170
The Internal Revenue ServiceElectronic Tax Filing, 172The Social Security Administration and PEBES, 175Nationwide Identity Systems, 176
Concluding Remarks, 1777A TOOLKIT FOR PRIVACY IN THE CONTEXT OF179
AUTHENTICATION
Privacy-Impact Toolkit, 181Attribute Choice, 182
Identifier Selection, 186
Identity Selection, 189
The Authentication Phase, 190Concluding Remarks, 192APPENDIXESABiographies of Committee Members and Staff197
BBriefers to the Study Committee207

CSome Key Concepts209
What Is CSTB?213
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.1Executive SummaryAs communications and computation technologies become increas-ingly pervasive in our lives, individuals are asked to authenticate
them-selvesÑto verify their identitiesÑin a variety of ways.   Ac-
tivities ranging from electronic commerce to physical access to buildings toe-government have driven the development of increasingly sophisticated
authentication systems.  Yet despite the wide variety of authentication tech-

nologies and the great range of activities for which some kind of authentica-
tion is required, virtually all involve the use of personal information, raising
privacy concerns.  The development, implementation, and broad deploy-

ment of authentication systems require that issues surrounding identity and
privacy be thought through carefully.  This report explores the interplay

between authentication and privacy. It provides a framework for thinking
through policy choices and decisions related to authentication systems.AuthenticationÕs implications for privacy do not necessarily equate to
violations of privacy, but understanding the distinctions requires being
aware of how privacy can be affected by the process of authentication.
Such awareness is usually absent, however, because authentication tends
to be thought about more narrowly, in connection with security.  In decid-

ing how to design, develop, and deploy authentication systems, it is nec-
essary to weigh privacy, security, cost, user convenience, and other inter-
ests.  A key point is that all of these factors are subject to choice: Whether
any given system violates privacy depends on how it is designed and
implemented.  Changes in technology and practice make this the time for

broader, more rigorous analyses of options in authentication.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.2WHO GOES THERE?The complexity of the interplay between authentication and privacybecomes clear when one tries to define authentication, which can take
multiple forms:¥Individual authentication is the process of establishing an under-
stood level of confidence that an identifier refers to a specific individual.¥Identity authentication is the process of establishing an understood
level of confidence that an identifier refers to an identity.  The authenti-

cated identity may or may not be linkable to an individual.¥Attribute authentication is the process of establishing an understood
level of confidence that an attribute applies to a specific individual.A common understanding and consistent use of these and other termsdefined in the report are a prerequisite for informed discussion.  The three

variants above illustrate that authentication is not a simple concept: As
the committeeÕs first report on nationwide identity systems1 argued, grap-
pling with these issues and their implications is just not that easy (Box ES.1).This summary of the report includes the findings and recommenda-tions of the authoring Committee on Authentication Technologies and
Their Privacy Implications.  Each of these findings and recommendations,

which are more fully developed and supported in the body of the report,
is followed by the number of the finding or recommendation in parenthe-
ses. This number corresponds to the chapter where the finding or recom-
mendation is found and its order of appearance in that chapter.SECURITY, AUTHENTICATION, AND PRIVACYAuthentication is not an end in itself.  In general, people are authenti-
cated so that their requests to do something can be authorized and/or so
that information useful in holding them accountable can be captured.
Authentication systems are deployed when control of access and/or pro-
tection of resources, both key functions of security, are necessary.The three generic means of authentication that tend to be used inpractice can be  described loosely as Òsomething you know,Ó Òsomethingyou have,Ó or 
Òsomething you are.Ó  The systems discussed in this re-
portÑbased on technologies such as passwords, public key infrastruc-tures (PKI), smart cards, and biometrics, among others (see Boxes ES.2,
ES.3, and ES.4)Ñgenerally implement one or a combination of these ap-proaches.1 Computer Science and Telecommunications Board, National Research Council. IDsÑNot That Easy: Questions About Nationwide Identity Systems. Washington, D.C., National Acad-emy Press, 2002.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY3BOX ES.1Nationwide Identity SystemsIn the first report of the Committee on Authentication Technologies and TheirPrivacy Implications, IDsÑNot That Easy: Questions About Nationwide IdentitySystems, it was noted that many large-scale identity systems are in effect nation-wide identity systems.  In particular, driverÕs licenses and even Social Security
cards qualify as such.  Such large-scale systems pose significant privacy and se-
curity challenges, which were elaborated on in that report.  A follow-on discussion
is located in Chapter 6 that includes the findings and recommendations below.Finding: State-issued driverÕs licenses are a de facto nationwideidentity system.  They are widely accepted for transactions that
require a form of government-issued photo ID. (6.5)Finding: Nationwide identity systems by definition create a wide-spread and widely used form of identification, which could easilyresult in inappropriate linkages among nominally independent da-
tabases.  While it may be possible to create a nationwide identity
system that would address some privacy and security concerns,the challenges of doing so are daunting.  (6.6)
Recommendation: If biometrics are used to uniquely identify li-cense holders and to prevent duplicate issuance, care must be tak-
en to prevent exploitation of the resulting centralized database andany samples gathered.  (6.3)
Recommendation: New proposals for improved driverÕs licensesystems should be subject to the analysis presented in this reportby the National Research CouncilÕs Committee on Authentication
Technologies and Their Privacy Implications and in the earlier(2002) report by the same committee: IDs
ÑNot That Easy: Ques-tions About Nationwide Identity Systems. (6.4)Finding: Core authentication technologies are generally moreneutral with respect to privacy than is usually believed.  How

these technologies are designed, developed, and deployed in
systems is what most critically determines their privacy impli-
cations. (5.6)But what kind of security is necessary, and is authentication required?When authentication is needed, which types might serve best?  For ex-

ample, when accountability is required, individual authentication may beWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.4WHO GOES THERE?BOX ES.2PasswordsPasswords pose serious security challenges.  They are a commonly used
form of authentication and are the quintessential example of Òsomething youknow.Ó  They require no specialized hardware or training and can be distributed,
maintained, and updated by telephone, fax, or e-mail.  But they do have serious
disadvantages, among them susceptibility to guessing and to theft.  In addition,

passwords generally do not change without human intervention, leaving them opento compromise.  Passwords are also easily shared, either intentionally or inadvert-
ently (when written down near a computer, for example), and a complex, expen-
sive infrastructure is necessary to enable resetting lost (forgotten) passwords.Because people have trouble remembering a large number of names and pass-words, there is a trend either toward name and password reuse across systems,
which undermines privacy (and security), or toward the creation of centralized sys-tems to keep track of these names and passwords, which has the same negativecentralization effect with respect to privacy and linkage.Finding:  Static passwords are the most commonly used form of
user authentication, but they are also the source of many system
security weaknesses, especially because they are often used inap-propriately. (5.1)Recommendation: Users should be educated with respect to theweaknesses of static passwords.  System designers must consid-
er trade-offs between usability and security when deploying au-
thentication systems that rely on static passwords to ensure thatthe protections provided are commensurate with the risk and harmfrom a potential compromise of such an authentication solution.
Great care should be taken in the design of systems that rely onstatic passwords. (5.1)necessary; otherwise, attribute authentication (or no authentication) maysuffice.Finding: Authorization does not always require individual au-thentication or identification, but most existing authorization
systems perform one of these functions anyway.  Similarly, a

requirement for authentication does not always imply that ac-
countability is needed, but many authentication systems gener-
ate and store information as though it were. (2.1)The use of authentication when it is not needed to achieve an appro-priate level of security could threaten privacy.  Overall, privacy protec-
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY5BOX ES.3Public Key SystemsPublic key systems (sometimes implemented as public key infrastructures, orPKIs) employ a sophisticated approach to authentication that relies heavily on
cryptography.  Public key cryptography is often touted as a virtual panacea for
e-commerce and e-government authentication and confidentiality challenges; how-ever, implementation and deployment details are key to this technologyÕs effective-ness, security, usability, and privacy protection.  A critical component of some
public key systems is a certificate authority (CA) that will certify that a particularkey belongs to a particular individual.  One way to implement this functionality is to

use a public CA (or trusted third party) to certify keys for multiple users and orga-nizations.  This practice, however, places much control in a centralized location,
raising privacy and security concerns.The complexity of public key systems has made their ease of use and deploy-ment a challenge.  Getting the underlying cryptography right is only half the battle.
Users must be educated with respect to how the systems should be used for max-
imum effectiveness.  Certificates must be distributed securely and revoked when
necessary.  These systems require considerable storage, bandwidth, and compu-tational ability.  Their privacy implications depend on how they are implemented

and used.  The scope of the PKI (as with any authentication system) will be one
determinant of how grave the attendant privacy risks are.  At one end of the spec-trum is a PKI designed to operate in a limited context (for example, in a single
organization or for a single function), and at the other end are PKIs that attempt toprovide service to a very large population for a broad set of purposes.Finding: Many of the problems that appear to be intrinsic to publickey infrastructures (as opposed to specific public key infrastruc-ture products) seem to derive from the scope of the public key
infrastructure. (5.5)Recommendation:  Public key infrastructures should be limited in
scope in order to simplify their deployment and to limit adverseprivacy effects.  Software such as browsers should provide better
support for private (versus public) certificate authorities and for
the use of private keys and certificates among multiple computersassociated with the same user to facilitate the use of private cer-tificate authorities. (5.3)Finding: Public certificate authorities and trusted third partiescould present significant privacy and security concerns. (5.3)Finding:  Public key infrastructures have a reputation for being dif-
ficult to use and hard to deploy.  Current products do little to dispel

this notion.  (5.4)
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.6WHO GOES THERE?tion, like security, is poor in most systems in large part because systemsbuilders are not motivated to improve it.There is an inherent tension between authentication and privacy, be-cause the act of authentication involves some disclosure and confirmation
of personal information.  Establishing an identifier or attribute for use

within an authentication system, creating transactional records, and re-
vealing information used in authentication to others with unrelated inter-
ests all have implications for privacy.  The many possible impacts of
BOX ES.4BiometricsIn addition to public key cryptography, biometrics is also often touted as aneffective authentication solution.  As with any authentication technology, however,

the truth of this claim depends, among other things, on the context in which thebiometric systems are used.  ÒBiometric authenticationÓ (often called biometrics) is
the automatic identification or authentication of  human individuals on the basis of
behavioral and physiological characteristics.  Biometrics has the obvious advan-
tage of authenticating the human, not just the presented token or password.  Com-
mon biometrics in use today verify fingerprints, retinas, irises, and faces, among
other things.  Downsides to biometrics include the fact that not all people can use
all systems, making a backup authentication method necessary (and consequentlyincreasing vulnerability); the fact that revocation is not possible for current systems
(the saying goes that most individuals Òhave only two thumbsÓ); and that remoteenrollment of a biometric measure (sending oneÕs fingerprint or iris scan over theInternet, for example) may defeat the purpose and is easily compromised.Finding: Biometric authentication technologies hold the promiseof improved user convenience.  Vendors of these technologies also

promise reduced system management costs, but this has yet to bedemonstrated in practice.  Moreover, these technologies can pose
serious privacy and security concerns if employed in systems that
make use of servers to compare biometric samples against storedtemplates (as is the case in many large-scale systems).  Their use
in very local contexts (for example, to control access to a laptop or
smart card) generally poses fewer security and privacy concerns.(5.2)Recommendation: Biometric technologies should not be used toauthenticate users via remote authentication servers because ofthe potential for large-scale privacy and security compromises in
the event of a successful attack (either internal or external) againstsuch servers.  The use of biometrics for local authentication
Ñforexample, to control access to a private key on a smart cardÑis amore appropriate type of use for biometrics. (5.2)Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY7authentication may not be considered by system designersÑwhosechoices strongly influence how privacy is affectedÑand they may not beappreciated by the public.  Most individuals do not understand the pri-

vacy and security aspects of the authentication systems they are required
to use in interactions with commercial and government organizations.  As

a result, individuals may behave in ways that compromise their own
privacy and/or undermine the security of the authentication systems.Finding: Authentication can affect decisional privacy, informa-tion privacy, communications privacy, and bodily integrity pri-
vacy interests.  The broader the scope of use of an authentica-

tion system, the greater its potential impact on privacy.   (3.1)
The tension between security and privacy does not mean that theymust be viewed as opposites.  The relationship between the two is com-

plex:  Security is needed in order to protect data (among other things),

and in many circumstances the data being protected are privacy-sensi-
tive.  At the same time, authentication may require the disclosure of per-

sonal information by a user.  If many have access to that personal infor-

mation, the value of the information for authentication is decreased, and
the decreased privacy of the informationÑthrough othersÕ access to per-
sonal information used in authenticationÑcan also compromise security.A critical factor in understanding the privacy implications of authen-tication technologies is the degree to which an authentication system is
decentralized.  A centralized password system, a public key system, or a

biometric system would be much more likely to pose security and privacy
hazards than would decentralized versions of any of these. The scope and
scale of an authentication system also bear on these issues.Finding: Scale is a major factor in the implications of authenti-cation for privacy and identity theft.  The bulk compromise of

private information (which is more likely to occur when such
information is accessible online) or the compromise of a widely
relied on document-issuing system, can lead to massive issu-
ance or use of fraudulent identity documents.  The result would

adversely affect individual privacy and private- and public-
sector processes. (6.4)Usability is a significant concern when determining how authentica-tion systems should be deployed and used in practice.  Such systems will

fail if they do not incorporate knowledge of human strengths and limita-
tions.  Users need to be aware when an authentication (and hence possi-

bly privacy-affecting) event is taking place.  In addition, user understand-
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.8WHO GOES THERE?ing of the security and privacy implications of certain technologies andcertain modes of use plays a major role in the effectiveness of the tech-
nologies.  For example, without a clear understanding of the security/

privacy threats to the system, users may behave in ways that undermine
the protections put in place by the designers.Finding:  People either do not use systems that are not designed
with human limitations in mind or they make errors in using
them; these actions can compromise privacy. (4.1)Recommendation: User-centered design methods should be in-tegral to the development of authentication schemes and pri-
vacy policies. (4.2)There are ways to lessen the impacts on privacy that authenticationsystems have.  Guidelines include the following:
Recommendation:  When designing an authentication system
or selecting an authentication system for use, one should¥Authenticate only for necessary, well-defined purposes;
¥Minimize the scope of the data collected;

¥Minimize the retention interval for data collected;

¥Articulate what entities will have access to the collected
data;¥Articulate what kinds of access to and use of the data will
be allowed;¥Minimize the intrusiveness of the process;

¥Overtly involve the individual to be authenticated in the
process;¥Minimize the intimacy of the data collected;

¥Ensure that the use of the system is audited and that the
audit record is protected against modification and de-
struction; and¥Provide means for individuals to check on and correct the
information held about them that is used for authentica-
tion. (3.2)More generally, systems should be designed, developed, and deployedwith more attention to reconciling authentication and privacy goals.Recommendation: The strength of the authentication systememployed in any system should be commensurate with the value
of the resources (information or material) being protected. (2.1)Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY9Recommendation: In designing or choosing an authenticationsystem, one should begin by articulating a threat model in or-
der to make an intelligent choice among competing technolo-
gies, policies, and management strategies.  The threat model

should encompass all of the threats applicable to the system.
Among the aspects that should be considered are the privacy
implications of the technologies. (4.1)Recommendation: Individual authentication should not be per-formed if authorization based on nonidentifying attributes will
suffice.  That is, where appropriate, authorization technologies

and systems that use only nonidentifying attributes should be
used in lieu of individual authentication technologies.  When

individual authentication is required, the system should be sub-
ject to the guidelines in Recommendation 3.2 (above).   (2.3)
Recommendation:  Systems that demand authentication for
purposes other than accountability, and that do not themselves
require accountability, should not collect accountability infor-
mation. (2.2)Recommendation: System designers, developers, and vendorsshould improve the usability and manageability of authentica-
tion mechanisms, as well as their intrinsic security and privacy
characteristics.  (4.5)
Recommendation:  Organizations that maintain online-acces-
sible databases containing information used to authenticate
large numbers of users should employ high-quality informa-
tion security measures to protect that information.  Wherever

possible, authentication servers should employ mechanisms
that do not require the storage of secrets. (6.2)MULTIPLE IDENTITIES, LINKAGE, AND SECONDARY USEWho do you find when you authenticate someone?  There is no single
identity, identifier, or role associated with each person that is globally
unique and meaningful to all of the organizations and individuals with
whom that person interacts.Finding: Most individuals maintain multiple identities as so-cial and economic actors in society. (1.1)Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.10WHO GOES THERE?People invoke these identities under different circumstances.  They
may identify themselves as named users of computer systems, employ-
ees, frequent fliers, citizens, students, members of professional societies,
licensed drivers, holders of credit cards, and so on.  These multiple iden-

tities allow people to maintain boundaries and protect privacy.  That

capacity diminishes with the number of identifiers used.Finding: The use of a single or small number of identifiersacross multiple systems facilitates record linkage.  Accordingly,

if a single identifier is relied on across multiple institutions, its
fraudulent or inappropriate use (and subsequent recovery ac-
tions) could have far greater ramifications than if used in only a
single system. (4.3)The networking of information systems makes it easier to link infor-mation across different, even unrelated, systems.  Consequently, many

different transactions can be linked to the same individual.  Systems that

facilitate linkages among an individualÕs different identities, identifiers,and attributes pose challenges to the goal of privacy protection.  Once

data have been collected (such as from an authentication event or subse-
quent transactions), dossiers may be created.Finding: The existence of dossiers magnifies the privacy risksof authentication systems that come along later and retro-
actively link to or use dossiers.  Even a so-called de-identified

dossier constitutes a privacy risk, in that identities often can be
reconstructed from de-identified data. (4.2)Secondary use of authentication systems (and the identifiers and/oridentities associated with them) is related to linkage.  Many systems are

used in ways that were not originally intended by the system designers.
The obvious example is the driverÕs license:  Its primary function is to
certify that the holder is authorized to operate a motor vehicle.  However,

individuals are now asked to present their driverÕs license as proof of age,proof of address, and proof of name in a variety of circumstances.  As

discussed in IDsÑNot That Easy and in this report, the primary use of an
authentication system may require security and privacy considerations
very different from those appropriate for subsequent secondary uses.  (For

example, a driverÕs license that certifies one is capable of driving a motorvehicle is a far cry from certification that one is not a threat to airline
travel.)  Given the difficulty of knowing all the ways in which a system

might be used, care must be taken to prevent secondary use of the system
as such use can easily lead to privacy and security risks.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY11Finding: Current authentication technology is not generallydesigned to prevent secondary uses  or mitigate their effects.  In

fact, it often facilitates secondary use without the knowledge or
consent of the individual being authenticated.  (4.4)
Finding: Secondary uses of authentication systems, that is, usesfor which the systems were not originally intended, often lead
to privacy and security problems. They can compromise the
underlying mission of the original system user by fostering
inappropriate usage models, creating security concerns for the
issuer, and generating additional costs. (4.5)At the extreme end of the identity spectrum is the concept of anonym-ity.  Anonymity continues to play an important role in preserving the

smooth functioning of societyÑand it helps to protect privacy.  The wide-
spread use of authentication implies less anonymity.Finding: Preserving the ability of citizens to interact anony-mously with other citizens, with business, and with the govern-
ment is important because it avoids the unnecessary accumula-
tion of identification data that could deter free speech and
inhibit legitimate access to public records. (6.7)Linkage and secondary uses of information and systems can belessened.Recommendation: A guiding principle in the design or selec-tion of authentication technologies should be to minimize the
linking of user information across systems unless the express
purpose of the system is to provide such linkage. (4.3)Recommendation:  Future authentication systems should be
designed to make secondary uses difficult, because such uses
often undermine privacy, pose a security risk, create unplanned-
for costs, and generate public opposition to the issuer. (4.4)THE UNIQUE ROLES OF GOVERNMENTGovernment institutions play multiple roles in the area where au-thentication and privacy intersect.  Their approaches to authentication

and privacy protection may differ from those of private sector entities for
structural and legal reasons.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.12WHO GOES THERE?Finding: Electronic authentication is qualitatively different forthe public sector and the private sector because of a govern-
mentÕs unique relationship with its citizens:a.Many of the transactions are mandatory.
b.Government agencies cannot choose to serve only se-
lected market segments.  Thus, the user population with

which they must deal is very heterogeneous and may be
difficult to serve electronically.c.Relationships between governments and citizens are
sometimes cradle to grave but characterized by intermit-
tent contacts, which creates challenges for technical au-
thentication solutions.d.Individuals may have higher expectations for government
agencies than for other organizations when it comes to
protecting the security and privacy of personal data. (6.2)As a provider of services, the government has been seeking ways tomore easily authenticate users who require such services.  In some cases,

interagency and intergovernmental solutions may conflict with the fun-
damental principles espoused in the Privacy Act of 1974.Finding: Many agencies at different levels of government havemultiple, and sometimes conflicting, roles in electronic authen-
tication.  They can be regulators of private sector behavior, issu-

ers of identity documents or identifiers, and also relying parties
for service delivery. (6.1)Finding: Interagency and intergovernmental authentication so-lutions that rely on a common identifier create a fundamental
tension with the privacy principles enshrined in the Privacy
Act of 1974, given the risks associated with data aggregation
and sharing.  (6.8)
Government plays a special role in issuing identity documents(driverÕs licenses, birth certificates, passports, Social Security cards) thatare foundational documents relied upon to establish identity in numer-
ous authentication systems.  However, the processes used to produce

these foundational documents are not necessarily sufficiently secure to
serve their stated function.  Further, although states issue driver
Õs licensesand the federal government issues passports, each may depend on the
other for reissuance or replacement; no single entity has a complete au-
thoritative database.  While on the one hand the lack of easy linkage can
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY13be seen as a privacy boon, on the other the relative ease with which somefoundational documents can be forged means that fraud is more likely
and security and privacy risks (including identity theft) are great.Finding:  Many of the foundational identification documents
used to establish individual user identity are very poor from a
security perspective, often as a result of having been generated
by a diverse set of issuers that may lack an ongoing interest in
ensuring the documentsÕ validity and reliability.  Birth certifi-
cates are especially poor as base identity documents, because
they cannot be readily tied to an individual. (6.3)Recommendation: Birth certificates should not be relied uponas the sole base identity document.  Supplemented with sup-

porting evidence, birth certificates can be used when proof of
citizenship is a requirement. (6.1)MOVING FORWARDWhen people express concerns about privacy, they speak about intru-sion into personal affairs, disclosure of sensitive personal information, and
improper attribution of actions to individuals.  The more personal the infor-

mation that is collected and circulated, the greater the reason for these
concernsÑand the proliferation of authentication activity implies more col-lection and circulation of personal information.  There are choices to be

made:  Is authentication necessary?  If so, how should it be accomplished?

What should happen to the information that is collected?  It is time to be

more thoughtful about authentication technologies and their implications
for privacy.  Some of this thinking must happen among technologists, but it

is also needed among business and policy decision makers.The tension between authentication and privacyÑand the need forgreater care in choosing how to approach authenticationÑwill grow inthe information economy.  In addition to the management control con-

cerns associated with security, the economic value of understanding the
behavior of customers and others is a strong motivator for capturing
personal information.  It is also a strong motivator for misusing such

information, even if it is only captured through authentication systems.The decision about where and when to deploy identity authenticationsystemsÑif only where confirmation of identity is already required todayor in a greater range of circumstancesÑwill shape society in both obviousand subtle ways.  The role of attribute authentication in protecting pri-

vacy is underexplored.  In addition, establishing practices and technical

measures that protect privacy costs money at the outset.  Many privacy
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.14WHO GOES THERE?breaches are easy to conceal or are unreported; therefore, failing to pro-tect privacy may cost less than the initial outlay required to establish
sound procedural and technical privacy protections.  If the individuals

whose information has been compromised and the agencies that are re-
sponsible for enforcing privacy laws were to become aware of privacy
breaches, the incentive for proactive implementation of technologies and
policies that protect privacy would be greater.Finding:  Privacy protection, like security, is very poor in manysystems, and there are inadequate incentives for system opera-
tors and vendors to improve the quality of both. (4.6)Finding: Effective privacy protection is unlikely to emerge vol-untarily unless significant incentives to respect privacy emerge
to counterbalance the existing incentives to compromise pri-
vacy.  The experience to date suggests that market forces alone

are unlikely to sufficiently motivate effective privacy protec-
tion. (4.7)Even if the choice is made to institute authentication systems onlywhere people today attempt to discern identity, the creation of reliable,
inexpensive systems will inevitably invite function creep and unplanned-
for secondary uses unless action is taken to avoid these problems.   Thus,
the privacy consequences of both the intended design and deployment
and the unintended uses of authentication systems must be taken into
consideration by vendors, users, policy makers, and the general public.Recommendation: Authentication systems should not infringeupon individual autonomy and the legal exercise of expressive
activities.  Systems that facilitate the maintenance and assertion

of separate identities in separate contexts aid in this endeavor,
consistent with existing practices in which individuals assert
distinct identities for the many different roles they assume.
Designers and implementers of such systems should respect
informational, communications, and other privacy interests as
they seek to support requirements for authentication actions.
(3.1)The federal government has passed numerous laws and regulationsthat place constraints on the behavior of private sector parties as well as
on government agencies.  Among them are the Family Educational Rights

and Privacy Act, the Financial Services Modernization Act, the Health
Insurance Portability and Accountability Act of 1996, and, in 1974, theWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY15Privacy Act, which regulates the collection, maintenance, use, and dis-semination of personal information by federal government agencies.
Given the plethora of privacy-related legislation and regulation, making
sense of government requirements can be daunting.TOOLKITWith a basic understanding of authentication, privacy interests andprotections, and related technologies, it is possible to consider how one
might design an authentication system that limits privacy intrusions while
still meeting its functional requirements.  This report provides a toolkit

for examining the privacy implications of various decisions that must be
made when an authentication system is being contemplated.  As men-

tioned previously, most of these decisions can be made irrespective of the
particular technology under consideration.The kind of authentication to be performed (attribute, identity, orindividual) is an initial choice that will bear on the privacy implications.
Viewed without regard to the resource that they are designed to protect,
attribute authentication systems present the fewest privacy problems and
individual authentication systems the most.  Despite the fact that it raises

more privacy concerns, in some instances individual authentication may
be appropriate for privacy, security, or other reasons.In the process of developing an authentication system, several ques-tions must be answered early.  Decisions will have to be made about

which attributes to use, which identifiers will be needed, which identity
will be associated with the identifier, and how the level of confidence
needed for authentication will be reached.  The answers to each of these

questions will have implications for privacy.  Chapter 7 elaborates on four

types of privacy (information, decisional, bodily integrity, and communi-
cations) and on how they are affected by the answers to each of the pre-
ceding questions.  The analysis proposed is technology-independent, for

the most part, and can be applied to almost any proposed authentication
system.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.161Introduction and Overview
The growth of technologies that ease surveillance, data collection,disclosure, aggregation, and distribution has diminished the ob-
scurity and anonymity that are typical of everyday interactions.From phone systems that block the calling number on outgoing calls and
simultaneously identify all incoming callers,1 to ÒloyaltyÓ programs that
collect data about individualsÕ purchasing habits,2 to the governmentÕs
use of tracking and identification technologies in an increasingly broad
range of environments, records of individualsÕ activities are now rou-
tinely made and stored for future use.  Technologies such as facial recog-
nition and video cameras are being deployed in an attempt to identify
and/or monitor individuals surreptitiously as they go about the most
mundane of activities.3  Ubiquitous computing promises to put computa-
1ÒPacific Bell Offers Privacy Manager,ÓRBOC Update 12(5) (new offering for per-call con-trol over incoming messages); Beth Whitehouse, ÒIn Pursuit of Privacy: Phone ServicesDesigned to Protect Can Also Be Extremely Frustrating,Ó Newsday, March 26, 2001, p. B03(problems arising from use of caller ID and call-blocking plans).2See, generally, Marion Agnew, ÒCRM Plus Lots of Data Equals More Sales for BordersÑRetail Convergence Aligns Web-based Marketing and Strategies with Those of PhysicalStores,Ó InformationWeek, May 7, 2001 (BordersÕ plan to merge online and off-line customerdata and loyalty programs); Kelly Shermach, ÒCoalition Loyalty Programs: Finding Strengthin Numbers,Ó Card Marketing 5(3):1 (benefits of shared data from joint marketing card prod-
ucts).3Lev Grossman, ÒWelcome to the Snooper Bowl: Big Brother Came to Super Sunday,Setting Off a New Debate About Privacy and Security in the Digital Age,Ó Time, FebruaryWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION AND OVERVIEW17tional power everywhere by embedding it seamlessly and unobtrusivelyinto homes, offices, and public spaces.  The fully networked environment

that ubiquitous computing is making possible raises complicated ques-
tions about privacy and identification.4  What does it mean when data
collection, processing, and surveillanceÑand perhaps authentication andidentificationÑbecome the norm?In applications ranging from electronic commerce to electronic taxfiling, to controlling entry to secured office buildings, to ensuring pay-ment, the need to verify identity and authorize access has driven the
development of increasingly advanced authentication systems.  These

systems vary widely in complexity and scope of use:  passwords in com-

bination with electronic cookies are used for many electronic commerce
applications, smart cards coupled with biometrics allow access to securedareas, and sophisticated public-key mechanisms are used to ensure the
integrity of many financial transactions.  While there are many authenti-

cation technologies, virtually all of them involve the use of personal infor-
mation and, in many cases, personally identifiable information, raising
numerous privacy concerns.This report examines authentication technologies through the lens ofprivacy.  It is aimed at a broad audience, from users (both end users and

organizations) of authentication systems, to people concerned with pri-
vacy broadly, to designers and implementers of authentication technolo-
gies and systems, to policy makers.12, 2001, p. 72 (the use of facial recognition technology by the Tampa Bay police departmentto search the 72,000 people in the crowd at Super Bowl XXXV); Ace Atkins, ÒSurveillanceTactic Faces Off with Privacy,Ó Tampa Tribune, February 7, 2001, p. 1 (police might buycontroversial new technology, tried out at the Super Bowl, that scans faces in public places;
surveillance cameras take pictures of people in crowds and a computer compares numericfacial patterns to a databank of criminals); Katherine Shaver, ÒArmey Protests CamerasSought on GW Parkway; Speed Deterrent Likened to Big Brother,Ó Washington Post, May 9,2001, p.  B01 (the National Park Service tested a radar camera from August 1999 to February
2000 in two areas of the George Washington Memorial Parkway in the Washington, D.C.,area, and House Majority Leader Richard Armey asked Department of the Interior Secre-
tary Gale A. Norton to ban the cameras, calling them Òa step toward a Big Brother surveil-lance stateÓ); Richard Morin and Claudia Deane, ÒDNA Databases Casting a Wider Net,Washington Post, May 8, 2001, p. A21 (the national DNA database and the fact that all 50states have passed some version of a DNA data-banking law); Ian Hopper, ÒNew Docu-ments Disclose Extent of FBIÕs Web Surveillance,Ó Sunday Gazette Mail, May 6, 2001, p.  P6D
(the FBIÕs use of Internet eavesdropping using its controversial Carnivore systemÑa set ofsoftware programs for monitoring Internet traffic [e-mails, Web pages, chat-room conversa-tions, and other signals]Ñ13 times between October 1999 and August 2000 and a similardevice, Etherpeek, another 11 times.)4See CSTBÕs report Embedded, Everywhere:  A Research Agenda for Networked Systems of
Embedded Computers (Washington, D.C., National Academy Press, 2001), particularly Chap-
ter 4, which discusses security and privacy in ubiquitous computing environments.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.18WHO GOES THERE?Notwithstanding considerable literature on privacy, the legal and so-cial meaning of the phrase Òthe right to privacyÓ is in flux.  Rather than
presenting an encyclopedic overview of the various technologies or an in-
depth treatise on privacy, this report explores the intersection of privacy
and authentication, which raises issues of identification, authorization,
and security.This introductory chapter presents definitions and terminology thatare used throughout the report.  It introduces four overarching privacy

concerns that illustrate how privacy and authentication can interact in
ways that negatively affect privacy.  It also provides a 
Òday-in-the-lifeÓscenario to motivate a discussion of authentication and privacy.  Finally,

there is a brief discussion of what this report does not do, along with an
outline of the rest of the report.DEFINITIONS AND TERMINOLOGYThroughout this report, numerous interrelated concepts associatedwith authentication, identity, and privacy are discussed.  Several of these
concepts are briefly defined below for clarity.  As noted in the committee
Õsfirst report, IDsÑNot That Easy, many of these concepts represent compli-cated, nuanced, and, in some instances, deeply philosophical topics.5Note that while the definitions below refer to individuals, they should
also be understood to apply, when appropriate, to nonhuman subjects
such as organizations, identified computers, and other entities.  Popular

belief to the contrary, authentication does not necessarily prove that a
particular individual is who he or she claims to be; instead, authentication
is about obtaining a level of confidence in a claim.  The concepts below are

teased apart both to describe how the terms are used in this report and to
highlight how ambiguous many of them remain.¥An identifier points to an individual.  An identifier could be a
name, a serial number, or some other pointer to the entity being identi-
fied.  Examples of personal identifiers include personal names, SocialSecurity numbers (SSNs), credit card numbers, and employee identifica-
tion numbers.  It is sometimes necessary to distinguish between identifi-

ers and the things that they identify.  In order to refer to an identifier in a
way that distinguishes it from the thing that it identifies, the identifier is
written in quotation marks (for example, ÒJoseph K.Ó is an identifier
Ñspecifically, a personal nameÑwhereas Joseph K. is a person).5Indeed, the committee has refined and evolved its core definitions since the publicationof its earlier report IDsÑNot That Easy: Questions About Nationwide Identity Systems (Wash-ington, D.C., National Academy Press, 2002).Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION AND OVERVIEW19¥An attribute is a property associated with an individual.  
Ex-amples of attributes include height, eye color, employer, and organiza-
tional role.¥Identification is the process of using claimed or observed at-
tributes of an individual to infer who the individual is.  
Identificationcan be done without the individualÕs having to (or being given the oppor-tunity to) claim any identifier (for example, an unconscious patient in an
emergency room might be identified without having to state his or her
name).¥Authentication is the process of establishing confidence in the
truth of some claim.  
The claim could be any declarative statementÑforexample, ÒThis individualÕs name is ÔJoseph K.,Õ Ó or 
ÒThis child is morethan 5 feet tall.Ó  Both identifiers and attributes can be authenticated, as
the examples just cited demonstrate.ÑIndividual authentication is the process of establishing an un-derstood level of confidence that an identifier refers to a specific
individual.  
Individual authentication happens in two phases:(1)an identification phase, during which an identifier to be

authenticated is selected in some way (often the identifier selected
is the one claimed by the individual), and (2) an authentication
phase, during which the required level of confidence is established
(often by challenging the individual to produce one or more authen-
ticators 
supporting the claim that the selected identifier refers tothe individual).  In the information security literature, individual

authentication is sometimes referred to as Òuser authentication.ÓIn the biometrics literature, individual authentication of an identi-
fier claimed by the individual is often called Òverification.ÓÑIdentity authentication is the process of establishing an under-
stood level of confidence that an identifier refers to an identity.  It

may or may not be possible to link the authenticated identity to an
individual.  
For example, verification of the password associatedwith a Hotmail account authenticates an identity (foo@example.com)
that may not be possible to link to any specific individual.  Identity

authentication happens in two phases: (1) an identification phase,
during which an identifier to be authenticated is selected in some
way (often the identifier is selected by a claimant), and (2) an
authentication phase, during which the required level of confi-
dence is established (often by challenging the claimant to produce
one or more authenticators supporting the claim that the selected
identifier refers to the identity).Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.20WHO GOES THERE?ÑAttribute authentication is the process of establishing an un-
derstood level of confidence that an attribute applies to a specific
individual.  
Attribute authentication happens in two phases: (1) anattribute selection phase, during which an attribute to be authenti-
cated is selected in some way, and (2) an authentication phase,
during which the required level of confidence is established, either
by direct observation of the individual for the purpose of verifying
the applicability of the attribute or by challenging the individual to
produce one or more authenticators supporting the claim that the
selected attribute refers to the individual.¥An authenticator is evidence that is presented to support the
authentication of a claim.  It increases confidence in the truth of the

claim.  
A receipt, for example, can act as an authenticator of a claim thatan item was purchased at a specific store.6  A driver
Õs license can act as anauthenticator that a particular name (a form of identifier) refers to the
individual who carries the license.  Knowledge of a secret or the ability to

display some distinctive physical characteristic such as a fingerprint can
also serve as the authenticators of an individualÕs name.¥Authorization is the process of deciding what an individual
ought to be allowed to do.  Authorization is distinct from authentication(which establishes what an individual ÒisÓ rather than what the indi-
vidual Òis allowed.Ó)  Authorization policies determine how authoriza-
tion decisions are made.  Authorization policies base decision making on
a variety of factors, including subject identifiers (such as names) and
subject attributes other than identifiers (such as employee status, credit
rating, and so on).¥The identity of X is the set of information about an individual X
that is associated with that individual in a particular identity system Y.
However, Y is not always named explicitly.  
An identity is not the sameas an identifierÑso ÒJoseph K.Ó is an identifier (specifically, a name), but
Joseph K.  is a person.  It is not always easy to determine which individual

an identifier refers to.  For example, 
ÒGeorge Bush, the president of theUnited States, who lives in Texas and who attended YaleÓ is an identifierthat refers to two individuals.  Identities also consist of more than just
namesÑso Richard Nixon was an individual, but his identity also in-cludes other facts, such as that he was president of the United States and
that he resigned that office.  Furthermore, identities contain statements

that are not strictly factsÑa man who was stranded on a desert island in6Confusion can arise when the same thing is used as both an authenticator and an identi-fier, as happens frequently with credit card numbers.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION AND OVERVIEW211971 and who believed in 1975 that Richard Nixon was still Presidentwould have his facts wrong but would not misidentify Nixon.  Finally,

people disagree about identities and about which individuals they refer
to; if one believes newspaperman Bob Woodward, there was an indi-
vidual who went by the code name ÒDeep ThroatÓ during the Watergate
investigation that led to NixonÕs resignation, but different people havedifferent opinions about who that individual is.¥Security refers to a collection of safeguards that ensure the confi-
dentiality of information, protect the integrity of information, ensure
the availability of information, account for use of the system, and pro-
tect the system(s) and/or network(s) used to process the information.
Security is intended to ensure that a system resists attacks and tolerates
failures.  (See Chapter 4 for a more in-depth discussion of security and

authentication.)¥Privacy is a multifaceted term with many contextually depen-
dent meanings.   One aspect of the right to privacy is the right of an

individual to decide for himself or herself when and on what terms his
or her attributes should be revealed.  
(See Chapter 3 for some historicalbackground on privacy and a brief exploration of current privacy law and
policy in the United States.)AUTHENTICATION IN DAILY LIFEIndividuals authenticate themselves to others and to information sys-tems in many different contexts.  The identifiers and attributes that they

authenticate vary, depending on the situation.  Individuals may identify

themselves as named users of computer systems, employees, frequent
flyers, citizens, students, members of professional societies, licensed driv-
ers, holders of credit cards, adults over the age of 18, and so on.  There

need not be any single identity associated with each person that is glo-
bally unique and meaningful to all of the organizations and individuals
with whom that person interacts.  Thus, people often assert different

identities under different circumstances.Finding 1.1:  Most individuals maintain multiple identities as
social and economic actors in society.To illustrate the myriad ways in which instances of identification andauthentication arise in everyday life and to highlight some of the impor-
tant issues associated with new systems, the committee hypothesized
scenarios in the life of Joseph K. as he goes on a business trip.  The italic

sentences describe JosephÕs actions; the indented paragraphs that followWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.22WHO GOES THERE?point out important associated issues.  (Specific technologies are discussed
in more detail later in the report.)Joseph first dials in to his corporate network from home and authenticates himself toa network access server.  He does so by claiming to be an employee of CompuDigi

Corporation, using a name and a smart card that is read by his computer.Successfully completing this authentication procedure authorizesJoseph to access the corporate network.  All employees have the same

basic access privileges for the network, so it might seem that there is
no need to authenticate each employee independently by name for
log-in purposes.  However, by assigning each employee a unique

log-in name, CompuDigi can track JosephÕs log-in sessions separatelyfrom those of other employees, enabling audit, and it can more easily
revoke JosephÕs access if he leaves the company or if his smart card islost or stolen.Joseph now accesses an airline Web site to book his flights, probably unaware thatauthentication of another sort is going on.The Web site employs Secure Sockets Layer (SSL), a security protocol,to provide confidentiality for data transmitted between JosephÕspersonal computer (PC) and the site.  This prevents eavesdroppers

on the path between the PC and the Web site from observing sensitive

data.  It also provides an implicit authentication of the Web site to

Joseph.  This authentication is based on the Internet name of the Web

site, as contained in the uniform resource locator (URL) that Joseph
implicitly selected from his list of commonly accessed Web sites.Joseph is generally unaware of this authentication process unless it fails andgenerates a warning message.  The only indication to him that the process has

succeeded is the appearance of a small padlock icon in the browser window (which
he may not notice).  Joseph now uses his airline frequent-flyer account number to

identify himself and a personal identification number (PIN) to authenticate this
identifier.The airline is not necessarily interested in JosephÕs identity as anemployee of CompuDigi but rather in his identity as a customer of
the airline.Based on his frequent-flyer status, Joseph is able to request a seat with betterlegroom in the front section of the aircraft.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION AND OVERVIEW23Joseph is authorized to upgrade his seat based on his frequent-flyerstatus (an attribute), which in turn is based on his travel history.
(JosephÕs frequent-flyer number may remain constant with the airlinefor many years, but his status and hence his authorization to upgrade
to a better seat will vary depending on how often he flies.)  Thus,

JosephÕs frequent-flyer number (an identifier) is used as a key for adatabase that the airline uses to determine his status and hence his
authorization.To pay for his flight, Joseph provides a credit card account number.  Knowledge of
the account number and expiration date serves to authenticate him as a
cardholder.Using a credit card number and expiration date as authenticators is arelatively weak form of authentication, since the account number
serves as the primary identifier as well.  This credit card data might

be stored on the Web server; or, it might be used only for the
transaction at hand and not be stored on the Web server.  If there

were a way for Joseph to be sure that the Web server was not storing
his credit card information, it might increase his trust in the system
(assuming that he had been notified of this policy).An electronic ticket is issued for JosephÕs flights.  Next, he wishes to connect to
the Web site of a hotel chain to book a room.This Web site supports a feature known as client certificates, a little-used facet of SSL that can be employed to automate the user-
authentication process.  When Joseph initially registered on the Web

site as a frequent guest of the hotel chain, the site interacted with his
browser in order to issue him a public key certificate (an electronic file
containing information related to JosephÕs interactions with this site;see Chapter 5 for more on public key cryptography, private keys, and
certificates).  This certificate contains an identifier that links to Joseph
Õsaccount but is otherwise not meaningful.  Thus, the certificate cannot

be used by Joseph to authenticate himself to any other Web sites.
During the initial certificate generation process, Joseph was prompted
to provide a password to be used by his browser to protect the private
key associated with the certificate.  This single password could protect
all of the private keys stored by JosephÕs browser for use with all of thecertificates issued by Web sites that Joseph visits.  Such use of the

password would simplify JosephÕs life if he had many certificates, butfew Web sites make use of client certificates, so in practice Joseph
would gain only a small benefit from this feature.  Note that in terms
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.24WHO GOES THERE?of security, the private key becomes a proxy for the password(s) andis thus no more secure than the combination of that password and
the physical means used to protect the encrypted private key.When Joseph visits the hotel Web site (having registered and received a certificateearlier), his browser is queried by the Web site to send JosephÕs certificate and touse the associated private key to verify JosephÕs frequent-guest account identifier.Joseph is prompted by the browser to enter the password to unlock his private
keys, and he is logged in to the Web site.Again, it is Joseph
Õs identity as a frequent client (rather than his name
or other attributes) that is important.  His status as a frequent guest

entitles him to a free room upgrade.  This is another example of

authorization based on data associated with JosephÕs identity in aspecific context.  In this context, Joseph elected to store credit card

information as part of his profile with the hotel chain, so it is used
automatically to guarantee his reservation in the event of a late
arrival.  If the Web site does not adequately protect the data that it

stores, JosephÕs credit card data may be inappropriately disclosed toothers.  The use of encryption to protect Joseph
Õs data in transit to thesite does not protect against this sort of security failure in any way.Joseph has also elected to store several frequent-flyer numbers in his hotel profileso that he can acquire ÒmileageÓ credit for his stay.
With this action, Joseph has voluntarily elected to provide data to thehotel chain, enabling the hotel to link his (otherwise) independent
hotel and airline identities.  This provides the hotel marketing

organization with an ability to market directly to Joseph on the basis
of his travel patterns and preferences, as well as to offer amenities in
a more customer-friendly fashion when Joseph stays at its hotels.  It

also provides an ability to sell JosephÕs name, address, and possiblyhis e-mail address to other companies, based on the attributes in his
frequent-traveler profile.Finally, Joseph logs in to a rental car Web site and arranges for a vehicle for histrip.  Here, Joseph authenticates himself using his name and his frequent-renter

account number; no explicit password or PIN is required.JosephÕs profile at this Web site allows the rental car company toselect automatically his favorite class of vehicle.  Joseph has also

provided a code that identifies him as an employee of CompuDigi,
making him eligible for the special rates negotiated by CompuDigiWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION AND OVERVIEW25for its employees.  This code is an attribute not specific to Joseph; it is
used as a basis for authorizing all employees to make use of the
corporate discount program.  Joseph
Õs profile includes credit carddata as well as his driverÕs license data, both of which are requiredfor rental car transactions.En route to the airport, Joseph makes use of an electronic toll tag lane, whichallows him to avoid longer lines for drivers paying tolls with cash.The toll tag device, mounted on the windshield of JosephÕs car,engages in an electronic (radio frequency (RF)) challenge/response
authentication protocol with a responder at each toll plaza,
authenticating the toll tag device to the toll collection system.  This

system authenticates the tagÕs number, which is linked to JosephÕsaccount identity in the toll system database.  In turn, this number is

linked to JosephÕs bank account, enabling automatic debit of hisaccount for each toll transaction.  The toll system may be concerned

only with receiving payment of the toll, so it is the identification of
the bank account that is of primary interest here.7Joseph arrives at the airport and makes use of a kiosk to acquire his boarding pass.
To authenticate himself, he swipes the same credit card that he used to purchase
the airline ticket through a magnetic-stripe reader.In this case, possession of the credit card is viewed as authenticationof identity.At the airport security checkpoint, Joseph must present his boarding pass and agovernment-issued photo identification (ID) for authentication.The name on the photo ID must match (either exactly or ÒcloselyÓ)the name on the boarding pass, and the photo on the ID must be a
good enough likeness to be acceptable to the security screening
personnel.Upon arrival at his destination airport, Joseph proceeds to the rental car area,where his car is waiting in a spot at which his name is displayed.  As he exits the

rental car lot, Joseph is required to present his driverÕs license.7While it may be possible to link the tag to a cash account that is not linked to the driver,in many cases such systems do make explicit the linkage between the account and the(presumed) driver.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.26WHO GOES THERE?This procedure is designed to authenticate Joseph as the individualwho holds the online reservation and to whose credit card the rental
will be charged.  In principle, the process should also verify that

Joseph holds a valid driverÕs license, a prerequisite for car rental.  In
contrast to the boarding-pass check at the airport, the rental
agreement has more information about Joseph, including the name
of the state that issued the driverÕs license and the license number.Such information is nominally part of this authentication process,
providing more evidence that the person presenting the license to the
electronic record is connected to a printed receipt.  Also, note that

while a passport would be an acceptable form of photo ID for use
with the boarding pass (and would be required for international
flights), it is not acceptable here, because there is a requirement for a
credential that demonstrates authorization to drive and that
establishes accountability of a particular individual for loss of or

damage to the automobile.  A driver
Õs license accomplishes bothgoals, because it directly asserts authorization to drive and because it
contains or can be used to obtain the driverÕs address.  The rental car
agency (depending on the state in which Joseph is renting) may have
reserved the right to screen JosephÕs driving record, which it mayaccess electronically using his driverÕs license number.When Joseph arrives at his hotel, he presents a credit card at the front desk.  The
hotel matches the name on the credit card against the room-reservation database
to identify Joseph.Since the primary concern of the hotel is that it is compensated forthe room rental, the presentation of a valid credit card (including
verification that the credit card account is in good standing, not
reported lost or stolen) is an acceptable form of authentication in this
context.8 The credit card is itself authenticated on the basis of the
information contained on the magnetic stripe on the back of the card
and on the basis of the appearance of the card (for example, the
appearance of a standard hologram as part of the card face).  If a

conflict occursÑtwo individuals with the same name claim the samereservation at the same hotel on the same dayÑadditionalidentification credentials will be required to resolve the conflict.8Note that hotels in countries other than the United States often are required to requestthe presentation of a passport and sometimes even retain the document until the guestchecks out.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION AND OVERVIEW27When Joseph arrives at the CompuDigi  meeting site, he uses his employee badge
to gain entrance to the building.  Joseph presents the card to a reader, which

requires him to enter a PIN, a procedure designed to prevent unauthorized use of
the card if it is lost or stolen.JosephÕs badge is a smart card, a credit-card-sized device thatcontains a processor, memory, and an input/output (I/O) interface.
On this card is stored a public key certificate and corresponding
private key.  The card engages in a cryptographic challenge/response

exchange with the buildingÕs physical security computer system toauthenticate  Joseph as a CompuDigi employee and to authorize him
to enter the building.This scenario illustrates that Joseph has many identities, not just one.These different identities represent him in his interactions with different
organizations, each of which identifies him in a distinct context.  In many

instances, there is no need for these distinct identities to be tightly linked
to one another, although there are exceptions.  Sometimes Joseph makes

an explicit choice to create the linkage (for example, for perceived ben-
efits); at other times the linkage is required by the context (for example,
the connection of his driverÕs license and his driving record). To the extentthat Joseph chooses, or is allowed, to maintain separate identities in his
interactions with organizations, he increases his privacy, because he dis-
closes to each organization only the information required for interactions
with that organization.By maintaining separate and nonlinked identities, Joseph has somecontrol over who gets which pieces of information about his activities,
preferences, and lifestyle.  Some of this control might be deliberate on

JosephÕs part, but some of it may have been the happenstance of a com-petitive market system in which linkages have not yet been fully unified
across corporate and government databases.  For Joseph to exercise pro-

active control over the dissemination and use of personal information
about himself, he must become aware of how and where that information
is being collected, linked, and used.  As activities within society become

increasingly automated, it becomes harder and harder for anyone to make
these informed decisions.Without informed, proactive control on JosephÕs part, the variousauthentication events described in this scenario pose risks in terms of
both security and privacy.  The rest of this report elaborates on various

authentication technologies and their relationship to privacy issues.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.28WHO GOES THERE?CURRENT TENSIONSThe development, implementation, and broad deployment of authen-tication systems require us to think carefully about the role of identity
and privacy in a free, open, and democratic society.  Privacy, including

control over the disclosure of oneÕs identity and the ability to remainanonymous, is an essential ingredient of a functioning democracy.  It is a
precondition for the exercise of constitutionally protected freedoms, such
as the freedom of association.9  It supports the robust exercise of the
freedom of expression by, for example, creating psychological space for
political dissent.10  It maintains social norms that protect human dignity
and autonomy by enabling expressions of respect and intimacy and the
establishment of boundaries between oneself and oneÕs community.119See National Association for the Advancement of Colored People v. Alabama Ex Rel. Patterson,
Attorney General, 357 U.S. 449; 78 S.  Ct. 1163 (1958); 2 L. Ed. 2d 1488 (1958) (the Court held
that the immunity from state scrutiny of membership lists was so related to the right of the
members to associate freely with others as to come within the protection of the U.S. Consti-tution); Joseph McIntyre, Executor of Estate of Margaret McIntyre, Deceased, Petitioner v. Ohio
Elections Commission, 514 U.S.  334; 115 S.  Ct.  1511 (1995) (statute prohibiting the distributionof anonymous campaign literature violated the First Amendment, as it was not narrowlytailored to serve an overriding state interest; the statute indiscriminately outlawed a cat-egory of speech with no relationship to the danger sought to be prevented); Buckley v.American Constitutional Law Foundation; Taley v. California
.  Also, see the work that the Elec-
tronic Privacy Information Center (EPIC) has done on anonymity, including an amicusbrief in the Watchtower Bible v.  Stratton
 case, arguing that 
Òan ordinance requiring door-to-door petitioners to obtain a permit and identify themselves upon demandÓ implicates privacy
as well as rights of anonymity, freedom of expression, and freedom of association.  More
information is available online at  <http://www.epic.org/free_speech/watchtower.html>.
10See Martin H. Redish, ÒThe Value of Free Speech,Ó 130 U. Pa. L. Rev. 591, pp. 601-604
(1982) (free expression supports citizensÕ participation in decision making); Alexander
Meiklejohn, Political Freedom: The Constitutional Powers of the People, New York, Oxford Uni-versity Press, 1965, pp. 3-89 (free expression provides citizens with access to informationnecessary to formulate opinions and make decisions); Rodney A.  Smolla, 
Smolla and Nimmeron Freedom of Speech: A Treatise on the First Amendment, Clark Boardman Callaghan, 1994,
¤13.01[3]  (by allowing disempowered groups to dissent, free expression provides stability);
and Julie E.  Cohen, ÒA Right to Read Anonymously: A Closer Look at ÔCopyright Manage-mentÕ in Cyberspace,
Ó 28 Conn. L. Rev. 981 (1996) (arguing that reading is intimately con-
nected with freedom of speech and thought and therefore the right to read anonymouslyshould be an understood guarantee of the First Amendment).11Robert C. Post, ÒThe Social Foundations of Privacy: Community and Self in the Com-mon Law Tort,Ó 77 Calif. L. Rev. 957 (1989).  Post argues that the common law tort of
invasion of privacy safeguards social normsÑÒrules of civilityÓÑis based on the belief thatpersonality and human dignity are injured when these rules of civility are broken.  He

concludes with an explanation of the role that the privacy tort plays in enabling individualsto receive and express respect, thereby enabling human dignity; in allowing individuals toreceive and express intimacy, thereby enabling human autonomy; and in establishing obli-Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION AND OVERVIEW29If individuals fear unchecked scrutiny, they will be less likely to par-ticipate vigorously in the political process and in society in general.12   If
individuals are denied privacyÑby the government, corporations, andother individualsÑthey are less able to explore ideas, formulate personalopinions, and express and act on these beliefs.  At the same time, 
Òpri-vacyÓ is sometimes used as a pretext for hiding illegal activities, and
society has, at times, a legitimate interest in requiring authentication or
identification, either for validating claims to rights and privileges or for
holding individuals responsible for their actions.Today, when individual authentication is demanded (such as beforeboarding an airplane), the individual whose identity is to be authenti-
cated is asked to participate in the process of proving who he or she is.13Authentication of identity generally (but not always; see Chapter 4) re-
quires an affirmative actÑthe individual must affirmatively introduceherself or knowingly produce a credential containing identity informa-
tion.  While a third party may at times provide information about an

individualÕs identity (such as an adult verifying the identity of a child),such information is more often a tool for confirming the identity pre-
sented by the individual being authenticated.  Because authentication

generally requires some affirmative act on the part of the individual, it is
rare that an individualÕs identity is surreptitiously noted and recorded inthe context of an authentication event.The decision about where to deploy authentication systemsÑbe itonly where today verification of identity is already required or in a greater
range of circumstancesÑwill shape society in both obvious and subtleways.  Even if the choice is made to implement authentication systems

only where people today attempt to discern identity, the creation of reli-
able, inexpensive systems will invite function creepÑthe use of authenti-cation systems for other than their originally intended purposesÑunlessaction is taken to prevent this from happening.14  Thus, the privacy con-
gations between community members, thereby defining the substance and boundaries ofcommunity life.  Id. at p. 238; Bloustein, 
ÒPrivacy As an Aspect of Human Dignity: AnAnswer to Dean Prosser,Ó 39 N.Y.U. L. Rev. 962, pp. 1000-1007 (1964) (arguing that the
privacy torts involve the same interest in preserving human dignity and individuality).12See, generally, the numerous privacy statutes that prevent the reuse of information andlimit governmental access because of social interest in promoting or protecting the underly-ing activities (for example, related to financial information and health care), many of which
are discussed in Chapters 3 and 6.13The criminal justice context is an exception in which the individualÕs identity may bedetermined without their active participation.14An example of secondary use is that of reliance on the driverÕs license for proof of agein establishments that sell alcohol.  In at least one establishment in Massachusetts, licenses
are swiped through a machine and all of the information contained in the magnetic stripeWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.30WHO GOES THERE?sequences of both the intended design and deployment and the unin-tended, secondary uses of authentication systems must be taken into con-
sideration by vendors, users, policy makers, and the general public.FOUR OVERARCHING PRIVACY CONCERNSWhile authentication systems can be used to preserve or enhanceprivacy, there are many ways, as described above, in which an authenti-
cation system, or even the act of authentication alone, can affect privacy;
that is, privacy is involved as a consequence or corollary of authentica-
tion.  Before discussing the details of authentication technologies and

their impact on privacy in later chapters, several categories of privacy risk
are described below.  While not applicable to all authentication systems,

these categories broadly characterize the risks to personal privacy that
authentication systems can create.¥Covert identification. Some authentication systems make it possible
to identify an individual without the individualÕs consent or even knowl-edge.  Such systems deny the individual, and society, the opportunity to

object to and to monitor the identification process.  These technologies are

particularly vulnerable to misuse because their use is hidden.¥Excessive use of authentication technology. Cost and public sensitivity
have historically checked the spread of authentication systems.  At the

same time that technological progress has reduced the cost of these sys-
tems (along with the costs of data collection and processing generally),
the public, owing to an increased sense of vulnerability and desire for
security or simple familiarity, has become accustomed to demands for
authentication.  Together, these trends increase the likelihood that au-

thentication systems will become more prevalent.  Led by a mentality of

Òmore is better,Ó the public and private sectors have been quick to in-
crease the collection of personal information where this process is sup-
ported by cheaper, easier technology.¥Excessive aggregation of personal information.
 The use of a single iden-
tifier (such as the Social Security number) or a small number of identifiers
creates the opportunity for more linking of previously separate reposito-
ries of personal information.  Today, different record keepers have differ-

ent ways of identifying individuals (and in some cases of tying their
identities to transaction histories).  The many cards that people carry in

their wallets reveal some of the multiple identities by which they areon the back is collected.  ÒSwipe at Your Privacy,Ó WHDH TV, June 4, 2002.  Available
online at  <http://www.whdh.com/features/articles/specialreport/H37/>.
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION AND OVERVIEW31known.  The adoption of a single (or small number of) authentication
systems across the public and private sector would greatly erode privacy
by facilitating the linkage of records maintained by many disparate record
keepers.15¥Chilling effects. Wherever identity authentication is required, there
is an opportunity for social control.  In some instances such control is a

laudable goal (such as in contexts that require high security and account-
ability).  But in other areas, there is a risk that new methods of social

exclusion and vehicles for prejudicial social control will be created.  For

example, in a world in which a single identifier (for example, a Social
Security number) is relied on by many public and private institutions, the
organization in charge of issuing this identifier (the government, in this
example) could interfere with a citizenÕs ability to engage in a wide rangeof legitimate private sector transactions by revoking the identifier; or, a
thief could interfere with the same abilities by stealing the identifier and
using it fraudulently.While there are risks to privacy with some authentication systems, itshould be noted that there are situations in which authentication provides
an important method of ensuring accountability and of protecting privacy.
For example, when specific individuals are granted access to personal or
proprietary information for limited purposes, authentication can play an
important role in monitoring and enforcing adherence to relevant regula-
tions and laws limiting individualsÕ access to these purposes.
WHAT THIS REPORT DOES AND DOES NOT DOThis report explores the concepts of authentication, identity, and pri-vacy.  It examines various authentication technologies and describes their

privacy implications.  The report does not recommend specific technolo-

gies for specific purposes, nor does it provide an explicit cost analysis
such as might be provided by a consultant.  Instead, the report discusses

the various technologies and elaborates on the trade-offs with respect to
privacy that each technology permits.  As the remainder of the report

makes clear, analyses of specific systems or proposed systems can pro-
ceed only with an understanding of the context in which a system will be
operating and an understanding of the goals that the system is trying to
meet.  This report provides a framework for these issues and the neces-

sary vocabulary within which to consider them.15See this committeeÕs first report, IDsÑNot That Easy:  Questions About Nationwide Iden-
tity Systems, Washington, D.C., National Academy Press, 2002, for a discussion of addi-
tional questions and issues raised by large-scale, widely used identity systems.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.32WHO GOES THERE?This report seeks to identify ways in which authentication technolo-gies are directly and indirectly affecting privacy.  It recognizes that both

government and commercial parties do, under many circumstances, have
a legitimate need to determine with whom they are dealing.  It explores

ways in which current authentication systems operate without adequate
heed to personal privacy.  The report recommends ways in which privacy

interests might be better served without compromising the legitimate
interests of commercial and government entities that employ authentica-
tion technologies.Chapters 2 and 3 elaborate on the concepts of authentication andprivacy to establish the framework for the discussion in the remainder of
the report.  Given the historical association of authentication with secu-

rity, Chapter 4 describes security concerns that motivate authentication
and then discusses how usability issues matter, both for security and
privacy.  Chapter 5 examines particular authentication technologies and

describes some of the technological issues that arise.   Chapter 6 outlines

some of the unique challenges facing governments and government agen-
cies with respect to authentication and privacy.  Finally, Chapter 7 pre-

sents a toolkit for thinking through the implications for privacy of the
choices made with respect to how authentication systems are developed
and deployed.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.332Authentication in the Abstract
Before examining specific authentication technologies and their im-plications for privacy, a discussion of the terms and concepts them-
selves is in order.  Colloquial uses of the term ÒauthenticationÓ are
occasionally misleading; for example, authentication is neither authoriza-
tion nor identification.  While this report does not attempt a comprehen-

sive examination of privacy in the context of an information-rich world,1this chapter and the next provide a foundation for thinking about authen-
tication and privacy and a context for the discussions of specific technolo-
gies in later chapters.WHAT IS AUTHENTICATION AND WHY IS IT DONE?Authentication is the process of establishing confidence in the truth ofsome claim.  While this report focuses primarily on authentication in the

context of information and computing systems, authentication occurs

outside this realm as well, as examples throughout the text illustrate.
Box2.1, for example, presents a brief discussion of authentication in the

context of absentee voting.  In the context of information security, the

unqualified term ÒauthenticationÓ is often used as shorthand to mean
Òverification of a claimed identity,Ó although for the purposes of this
report, a slightly more nuanced meaning is assumed.  (See Chapter 1 for
1Another CSTB committee is examining the broad topic of privacy in the information age;the status of this project is available online at <http://www.cstb.org/project_privacy/>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.34WHO GOES THERE?definitions of these and related terms.)  It is possible to authenticate both
human users and entities that are not humans (for example, cellular tele-
phone networks in the United States directly authenticate cell phone hand-
sets rather than handset users2), and it is possible to authenticate claimsthat do not relate to usersÕ personal names (for example, an individual
may claim to be tall enough to enjoy a height-restricted ride at a county
fair; this claim can be verified without knowing the individualÕs name).Authentication is not usually an end in itself.  Information systems
usually authenticate users in order to satisfy security or other require-
ments.3 Most commonly, security systems authenticate users in order to
authorize their requests to perform actions and in order to hold them
accountable for the actions that they perform. (See Figure 2.1 for a flow
chart describing how the policies of the system guide whether authentica-
tion, authorization, and/or accountability are needed.) In some instances authentication is unrelated to security; identification
and authentication are sometimes used to create or expand a relationship
between parties.  For example, cookies
4 are sometimes used to identify
2Often, databases can be used to map from a handset identifier to the name of the indi-vidual or organization that pays the bill for the handset.3In some cases, one such requirement is to protect classes of people (usually children)from material deemed inappropriate.  In 2002, CSTB released a report that looked at this
problem:  Youth, Pornography, and the Internet, Washington, D.C., National Academy Press,2002.4Cookies are mechanisms used by Web browsers and Web servers to track visits and/orprovide continuity of experience.BOX 2.1Absentee VotingIn many places, absentee voting couples a number of mechanisms in order toachieve authentication, authorization to vote exactly once, and the confidentiality
of the ballot itself.  The voterÕs identity is checked by way of a signature, both at
application time and on the outer envelope of the ballot itself.  A suitable entry in
some sort of recordkeeping system is used to record that this person has already
been issued a ballot.  But the ballot itself is sealed inside an inner, anonymous
envelope; this envelope is not opened until after it has been separated from theouter, authenticated (and nonanonymous) envelope.  The two events are separat-

ed temporally and spatially.  Despite the authentication of the voter (by means of
the signature), a measure of privacy protection is achieved through a combinationof the two envelopes and rigid procedures.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT35Yes
¥ Identify and perform individual authentication.¥ Retrieve any attributes necessary for authorization.
¥ Perform authorization.¥ Keep a record of individual and action.NoPolicy requiresaccountability?Policy requiresauthorization
based on
nonidentifyingattribute?Yes
No¥ Identify and perform individual authentication.¥ Retrieve any attributes necessary for authorization.¥ Do not keep a record of individual and action.
Policy requiresauthorization
based on
individual identity
or identifying 
attribute?Yes
No¥ Do not identify.¥ Perform attribute authentication.¥ Do not keep a record of attribute and action.   ¥Do not identify.¥Do not authenticate.¥Do not keep a record of any kind.Policy decisions aremade about
authorization andaccountability.FIGURE 2.1Authentication, authorization, and accountability.  The necessity of
authenticating the presenterÕs identity is based on the needs and policies of thesystem being accessed.  If only authorization is sought by the system, the identity

of the presenter may not be relevant.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.36WHO GOES THERE?individuals and track their browsing and purchasing behaviors.  Such moni-
toring is undertaken to bolster personalization and marketing efforts.The rest of this chapter describes elements of authentication systemsand protocols at a high level in order to provide a foundation from which
to examine specific authentication technologies.  A description of the par-

ties traditionally involved in authentication appears below, followed by a
more detailed discussion of authorization and accountability.Three Parties to AuthenticationAuthentication systems typically involve parties who play three rolesin the authentication process.  A ÒpresenterÓ presents credentials (an in-

depth discussion of the nature of credentials in general is provided in
Chapter 6) issued by a third-party ÒissuerÓ to a ÒverifierÓ who wishes to
determine the veracity of those credentials.  In some cases, one party

may play two roles.  For example, the verifier and issuer roles are often

combined.The issuer usually5 uses a separate system to perform an initial au-
thentication of prospective credential holders prior to issuing credentials,
to ensure that they are issued only to legitimate parties.  Consider the case

of a department of motor vehicles (DMV), which issues an important
credential, the driverÕs license.6  The DMV often relies on another creden-
tial, such as a birth certificate or a passport, to identify applicants for
licenses.  This reliance comes with its own risks, in that birth certificates,

for example, are not strongly linked to the individuals whom they iden-
tify and are so diverse in their format that it is difficult for DMV employ-
ees to authenticate them.  (The security and integrity problems associated

with documents such as birth certificates are discussed in Chapter 6.)When analyzing different authentication systems, it is important tolook at how a system implements all three roles.  One needs to look at the

security and privacy issues and the risks for each role and the processes
that the system performs and consider who has a vested interest in the
success (or failure!) of the authentication event.5Not all credentials require presenting supplementary forms of identification prior toissuance.  For example, one may acquire a free e-mail account from one of several services,

which then acts as a legitimate e-mail address, anonymously.6The original motivation for driverÕs licenses was to document authority to operate amotor vehicle.  However, today they are used more often as a primary identity credential in

many unrelated transactions, such as boarding an airliner or cashing a check.  (This is an
example of the secondary use of a credential overwhelming the primary use; see Chapter 4for more discussion of secondary uses.)Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT37Authorization can be attained by ascertaining attributes of the user,the system, or external conditions.  However, ensuring accountability

often requires that a particular individual be linked to a transaction.7Authenticating to AuthorizeAuthorization is the process of ensuring that the rules governing whomay do what to which resources are obeyed.  Authorization works by

asking an appropriate authority (referred to herein as a Òpolicy decision
pointÓ) for an authorization decision every time an individual8 submits a
request to the system to access resources.  If the policy decision point

decides to grant the request, the individual is allowed to access the re-
source; if the policy decision point decides to deny the request, the indi-
vidual is not allowed access.The International Organization for Standardization (ISO) Standard10181-3 defines a standard model and standard terminology for authori-
zation in an information technology and communications context.9  In the
ISO model, authorization decisions are based on authorization policy,
resource attributes (such as sensitivity of the data), context attributes (such
as time of day), request attributes, and subject attributes.  Subject at-

tributes might include the requesterÕs name and privilege attributes, such
as job role, group memberships, or security clearance.In order to make a determination, the policy decision point needs toknow something about the subject it is dealing with.  Since subjects might

try to impersonate one another, the verifier will want to make sure that
the subject attributes that it uses to make its decision have been authenti-
cated.  Policy decision points do not necessarily need to know anything

about a subjectÕs name or any other particular claimed identity in order to
authenticate the attributes that are relevant to the policies they enforce.
Two examples from Disney World illustrate this point:7Anonymous accountability mechanisms exist.  Consider, for example, a cash security
deposit for using a set of billiard balls.  If the user steals or damages the balls, he or she
forfeits the deposit (which is then used to finance the purchase of a new set).  The user who
returns the balls in good condition gets the deposit back.  The user is held financially
accountable for loss or damage to property (the billiard balls) with no record of identitybeing required.  Of course, this is anonymous only to the extent that anyone participating in

or witnessing the transaction may know the userÕs identity.8 In this case, the individual is the initiator of an operation that involves an authorization
decision.  The individual is the entity whose privileges are examined to determine if he or

she should be allowed to complete the requested operation.9This definition is adopted because it is the international standard for terminology in thisarea.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.38WHO GOES THERE?¥Disney World restricts access to some rides on the basis of physical
height of the rider.  The policy that enforces this restriction authenticates

the riderÕs height (an attribute) by requiring riders to walk past a sign that
has a picture of a hand positioned at the required height.  If the riderÕs

head is higher than the hand, the riderÕs height is authenticated, and the
rider is allowed onto the ride.  This system does not 
identify individual
riders at all, but it still enforces the desired policy.  It authenticates the

relevant attribute (in this case, height greater than the required minimum
value) of each rider.  It does not collect extraneous information about the

individual.¥Disney World also uses a system that is designed to prevent a
single-entry pass from being used by multiple users.  Disney World issues

each passholder a card at the time the pass is purchased.  The name of the

passholder is not recorded on the card, and, in fact, the card can be trans-
ferred freely from user to user until the first time it is used.  At the time of

first use, information about the passholderÕs finger geometry (not related
to the passholderÕs fingerprint) is linked to the card.  Any time after the

first use of the pass, the person presenting the pass must authenticate
ownership of the pass using a finger geometry verification check (by
holding his or her hand up to a measuring device).  If the check fails, the

person presenting the pass is denied access to the park.Finger geometry is not distinctive enough to identify the passholderuniquely; therefore, verifying finger geometry does not provide sufficient
certainty for accountability (see below).  However, finger geometry varies

sufficiently from person to person so that a randomly selected individual
who is not the passholder is not likely to match the finger geometry
linked to the card.  Therefore, this system works well enough to prevent

multiple users from using the same pass in most casesÑan acceptable
level of risk, given what the system is protecting.  This system uses a loose

form of biometric authentication to protect against fraud (here defined as
multiple users) without collecting information that identifies the legiti-
mate owner of the pass.  (See Chapter 5 for further details on various

biometrics technologies.)Authenticating to Hold AccountableAccountability is the ability to associate a consequence with a pastaction of an individual.  To hold individuals accountable it must be pos-

sible retrospectively to tie them to the actions or events for which ac-
countability is desired, or to be able independently to detect and respond
to inappropriate behavior.   Especially for purposes of after-the-fact ac-

countability, information from the authentication event must unambigu-
ously identify one and only one individual, who will be held responsibleWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT39for the event.10  An authentication event that identifies a group of peoplebut not a single individual within the group cannot easily support indi-
vidual accountability.11Accountability usually eventually requires personal identification of
individuals (unlike authorization, which can often be done without iden-
tifying individuals).  Accountability processes are used to generate the

evidence needed to punish the guilty when something is done in viola-
tion of policy or law.  Even so, the identifiers used need not always be in

the form of the name of the individual or even something that can be used
to find the individual of interest; they only need to be able to confirm that
a suspect is the individual actually responsible.  What is needed for this

purpose is something that can be linked to the individual with some level
of assurance.  A fingerprint or DNA sample, for example, can be used to

establish accountability.  Neither of these two types of evidence names

the individualÑnor does either provide a mechanism for finding him or
herÑbut both provide means to verify (after the individual is located
through other means) that the suspect probably is or is not the account-
able individual.Accountability in information systems usually works by allowinga process for observing and recording usersÕ actions in a log.  The log

can later be searched (as part of an investigation into wrongdoing, for
example).12While authentication (whether of an individualÕs name or of an at-tribute) is often necessary for accountability, authentication by itself is notalways sufficient to support accountability.  Authentication is a key to
attributing actions and allocating punishments to the correct individuals.10In some cases, it may be possible to impose a certain kind of ÒaccountabilityÓ withoutrequiring the authentication of an identity.  For example, some technologies are designed to

prevent unauthorized duplication of digital media (such as CDs).  In instances in which this
technology works, there is no accountability that requires identity authentication; insteadthe CD (or the machine) might just stop functioning.
11It is important to note that accountability is not the only mechanism for righting wrongs.Legal and other systems sometimes address wrongs without directly holding accountable theindividual responsible for the wrongÑfor example, by providing for redress.  Accountability

and redress are separate concepts.  One can have redress for a harm without holding the
perpetrator of the harm accountable.  For example, in disputes involving alleged violations of
copyright, the Digital Millennium Copyright Act of 1998 provides for redress by providing
copyright holders with a method of having material removed from Web sites.  This form of
redress prevents future harm but does not punish previous bad acts.12Note that this is conceptually distinct from the notion of auditing for the purposes ofsystem maintenance and/or troubleshooting.  Auditing may or may not require recording
individualized user activity.  While the mechanisms can be similar, the purposes of these
two kinds of log-based monitoring are distinct.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.40WHO GOES THERE?If identities in the log are not well authenticated, a user who falls undersuspicion of wrongdoing will dispute the validity of the log by claiming
that the actions attributed to him or her were really performed by some-
one elseÑsomeone else who fooled the authentication system and imper-
sonated the suspect.13Accountability is not always a requirement in information systems(or elsewhere), and even where it is, collecting and storing information
about individuals is not always necessary for accountability.  In order to

establish accountability, systems often collect and store information about

who did what, where, and how.  For example, financial markets support

payment mechanisms with and without accountability.  In a cash-for-

goods transaction, the buyer and seller often divulge no information to one
another.  In contrast, credit card transactions are information-rich: A record

is created that captures both the identity of each party and the details of the
transaction.  Credit records support accountability by uniquely mapping to

an individual (except perhaps in the case of family members who share a
single account and thus share each otherÕs accountability or in situations
involving powers of attorney) or to an organization.The inability to reconstruct a transaction from records retrospectively(that is, one may not be able to identify the other party or to prove that the
transaction occurred) is the norm in cash transactions.  This is at least in

part due to the fact that the exchange is complete when the parties sepa-
rate (goods have been exchanged for cash), so there is little need to make
provisions for future accountability for payment.  In a credit transaction,

by contrast, one party has goods, another has a promise of payment, and a
third has a record of the transaction directing the purchaser to make
payment.  The incomplete nature of the transfer and the ability of a buyer

or seller to deny or fabricate an exchange have resulted in the creation of
identity-based audit logs that support accountability.  These are needed

in order to complete the payment and therefore the transaction as a whole.These examples illustrate that anonymous, instantaneous liability-transfer mechanisms (such as cash) can reduce or even eliminate the need
for accountability in some transaction-based systems.   An example of an

anonymous liability-transfer mechanism that is not instantaneous is a
security deposit.  A security deposit is an up-front payment equal to or

exceeding the value of an item that is rented or borrowed (for example, a13Where authentication of individual identities is required, the individual identities inthe log will not necessarily be easily accessible to just anyone who looks into the log.  It is
possible to design an audit system that puts a nonidentifying unique identifier (essentially
a pseudonym, sometimes called an audit ID) into the log and allows specially authorizedusers to link these audit IDs to personal identities only after due-process rules have beenobserved.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT41cash deposit that is made for the billiard balls used at a bar or for checkingin at a hotel without a credit card).Cash is an extremely efficient mode of commerce in part because itdoes not require us to know or seek to know anything about the party on
the other side of the transaction.14  Other forms of payment are less effi-
cient, because creditworthiness, identity, or other attributes of one or more
parties to the transaction need to be established before the transaction is
completed, records of the transaction need to be kept, and settlement
procedures need to be executed.  This suggests that where accountability
(and therefore authentication) can be dispensed with, transactions can be
made simultaneously more efficient and more protective of privacy.15WHAT DO WE AUTHENTICATE?Earlier, authentication was defined as the process of establishing con-fidence in the truth of some claim, often a claimed identity.  The next

obvious question is, What kinds of claims are verified?  Individuals might

make a variety of claims that would help to support the goals of authori-
zation and accountability.When the goal that motivates authentication is to hold an individualaccountable, it is useful to verify some piece of information that is strongly
linked to or that uniquely identifies the individualÑfor example, an iden-
tifier (such as a personal name) or a very distinctive attribute (such as a
DNA profile).  It may also be useful in these cases to verify some piece of
information that will help contact or locate the individualÑfor example, a
residence address or e-mail address.When the goal that motivates authentication is to authorize individu-alsÕ actions, it is useful to verify some piece of information that will be
useful to the policy decision point in making its authorization decision.
This information may be a property of the individual (such as the fact that
an individual has paid for entrance to an event), or it may be a statement
about the individual by some authoritative party (such as when a credit
transaction is authorized by the issuing bank).14If the cash itself is authentic, then we trust in the issuing government to make good thepromise of payment, perhaps in precious metal, and we trust that there will always be an
exchange market for precious metal.  Most currencies now skip the precious-metal connec-
tion; governments back the currency instead with the direct promise of market stability.15Of course, providing records of transactions can be important for reasons other thanaccountability.  For example, information systems often maintain transaction logs for the
purpose of diagnosing and correcting system failures.  In addition, the ÒanonymityÓ of cash
creates challenges for law enforcement.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.42WHO GOES THERE?Presentation of a passport to an immigration agent authenticates thepassport holder for the purpose of authorizing entry into a country.  The

individual presents the passport, which claims his or her name and coun-
try of residence.  The immigration agent verifies the claim by examining

the picture in the passport to verify that the individual is the legitimate
holder of the passport and possibly by doing some kind of authenticity
check on the passport document itself.In summary, authentication is the process of verifying a claim.  A
claim asserts that an individual has some attribute.  An attribute may be

an identifier, a property, or a statement about the individual by a third
party.  Below is a discussion of these various kinds of attributes.
IdentifiersRecall from Chapter 1 that an identifier points to an individual.  An
identifier could be a name, a serial number, or some other pointer to the
entity being identified.  An identifier can be strong in the sense that it

allows unique mapping to a specific individual in a population, or it can
be weak, in that it could be correctly applied to more than one individual
in a population.  Whether an identifier is strong or weak will depend

upon the size of the population and the distinctiveness of the identifying
attribute.  However, multiple weak identifiers can, when combined,

uniquely identify a specific individual and therefore serve as the func-
tional equivalent of a single strong (unique) identifier.  Multiple weak

identifiers may lead to unique identification.16  Some identifiers, such as
pseudonyms, require a list of correspondences between pseudonyms and
individuals (often called a look-up table) for unique mapping back to an
individual, thus allowing only the holder of the list to identify the action
with the individual.  Some identifiers, such as common names, allow any

observer to map an action back to an individual (though not always with
100 percent confidence).  Authorization, in contrast, may require no iden-

tifier at all.  Systems that use multiple weak identifiers can be made just as

secure for the verifier as systems that use, for example, personal names,but the former may have the privacy advantage of not as easily identify-16Interestingly, while the use of multiple weak identifiers may enable a certain level ofsecurity through authentication, these identifiers can also be used to create unforeseen
linkages and therefore pose a risk to privacy (even while, individually, very weak identifi-ers might pose a minimal risk to privacy).  Work done by Latanya Sweeney suggests that
very little information is needed to uniquely identify a particular individual in even an
ostensibly anonymized database, suggesting that creating linkages between databasesÑeven without biometric data tying individuals to their dataÑmay not be difficult (see<http://lab.privacy.cs.cmu.edu/people/sweeney/confidentiality.html>).Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT43ing individuals to third parties who do not have access to the informationthat links the weak identifiers.A unique identifier refers unambiguously to one and only one indi-vidual in the population.  Legal personal names are commonly used iden-

tifiers for human individuals, despite the fact that they are not usually
unique except in very small populations.  In this case, additional identify-

ing information, perhaps limiting the population, is often implicitly re-
quired for identification (e.g.,ÒBob Smith of CentervilleÓ).  However, indi-

viduals can be identified by things other than names.  Many corporate

systems, for example, use employee ID numbers rather than personal
names as identifiers.  In large organizations, this is commonly done be-

cause ID numbers (unlike personal names) are unique.Names that are not personal names (such as pseudonyms and e-mailaddresses) can also be used as identifiers.  An entity issuing or relying on

such an identifier may wish to correlate it to a unique individual; indi-
viduals using these identifiers may wish to prevent such correlation.  Iden-

tifiers that are not personal names can be designed to protect individualsÕ
privacy by limiting or preventing correlation of the identifier to a specific
individual by limiting access to the look-up table (in a way that personal
names cannot).In general, the strength of the identification system is related to howdistinctive the combined identifiers are across the population in question.
ÒBob SmithÓ might be a strong identifier in Centerville, but quite weak
across the population of the entire country.  Identifying a person uniquely

across the entire population of the United States might require a name and
considerable additional data, such as a phone or Social Security number.AttributesAuthorization policies are often based on attributes that do not inher-ently identify an individual.  An attribute can be inherent (height, for

example) or assigned (vice president, for example); it can be permanent or
dynamic.  As an example, access to confidential company information in

a corporate intranet may be granted to any employee of the corporation.
The relevant attribute would be status as an employee.Some attributes can be granted to individuals with virtually no claimon the part of the individual being authorized.  The most common use of

this type of attribute is in granting guest access to a system.  Since by

definition anyone can be a guest, it is unnecessary to authenticate an
individualÕs identity in order to determine that he or she is entitled to the
guest attribute.  Obviously, this attribute is not very discriminating, so

one might ask why it is used at all.  The answer is that authorization

systems are normally set up to authorize every access to every resourceÑWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.44WHO GOES THERE?so even publicly available resources need to be governed by a policy thatsays, Ògrant access to the public,Ó where the public is defined as anyone
who has the guest attribute.Some attributes can be observed directly by the information system.For example, an individualÕs location can be determined if the system can
tell that the individual is using a terminal that is known to be on the
businessÕs premises.  Gender and race are in many instances observable

attributes.  Authentication of such attributes does not require prior au-

thentication of an identifier for the individual.It is sometimes possible to manifest attributes that ordinarily wouldnot be directly observable.  Bars do this when they stamp the hands of

patrons who are older than the minimum legal drinking age; this allows
people of legal age to enter and leave the bar without having to
reauthenticate their age by showing ID cards each time they enter.Some attributes cannot be observed directly by information systems.For example, an individualÕs employer or department number cannot be
determined by examining the individual or his or her surroundings.  Sys-

tems normally deal with such attributes by creating a process for register-ing individuals, 
assigning attributes to registered individuals, 
storing each
individualÕs assigned properties, and retrieving an individualÕs attributes
when needed for an authorization decision.In order to retrieve a specific individualÕs attributes from storage (orfrom a third party), the system must have an identifier for the individual,
which it can use to refer to the individual whose attributes it wants to
look up.  Furthermore, this identifier must be authenticated, so that the

system can have confidence that it is talking not to an impostor but in fact
to the individual whose attributes it is going to look up.Systems usually require individuals to authenticate themselves (us-ing a unique identifier) and subsequently to be assigned other attributes
that are then stored in an attribute database.  Then, at some later time, the

individual reauthenticates the same unique identifier to the system, and
some observer function in the system uses the unique identifier to look up
the assigned attributes in the database and to make an access decision.StatementsA statement records a belief or claim about an individual by an iden-tifiable party.17  Authorization decisions may in some cases be based on
attestations or assertions of authorities.  For example, a bankÕs decision to
17That is, it is possible to determine who the party is and hence whether the party isauthoritative for the statement.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT45extend credit to an individual may be based on a credit rating asserted bya credit agency.  The individualÕs credit rating is an attribute that is as-

signed by the credit agency to the individual.   In this case, the party

making the statement is an authority, and the party that uses the state-
ment to make an authorization decision is a relying party.Third-party assertions generally provide attribute information thatcannot be directly observed by the relying party.  In the example above,

an information system cannot observe an individualÕs credit ratingÑit
must instead query the credit agency to provide the rating.  In order to

retrieve accurately an individualÕs attributes from the authority, the rely-
ing party must have an appropriate identifier for the individual, which it
can correlate to the individualÕs identity and corresponding statements in
its database.HOW DO WE AUTHENTICATE?John Locke, in his ÒEssay Concerning Human Understanding,Ó18 dis-
tinguished two types of identityÑphysical identity and psychological
identity:[T]he identity of the same man consists .  .  .  in nothing but a participa-
tion of the same continued life, by constantly fleeting particles of matter,
in succession vitally united to the same organized body.Any substance vitally united to the present thinking being is a part ofthat very same self which now is; anything united to it by a conscious-ness of former actions, makes also a part of the same self, which is thesame both then and now.  Person, as I take it, is the name for this self.Wherever a man finds what he calls himself, there, I think, another maysay is the same person.It is by the consciousness it has of its present thoughts and actions that it[i.e., a person] is self to itself now, and so will be the same self, as far as
the same consciousness can extend to actions past or to come; and would

be by distance of time, or change of substance, no more two persons,than a man be two men by wearing other clothes to-day than he didyesterday.Locke also identifies the association of a set of past actions with apresent actor as being critical to the notion of personal identity, and he
associates identity with law and accountability:[A]s to this point of being the same self, it matters not whether thispresent self be made up of the same or other substancesÑI being as18John Locke. An Essay on Human Understanding, Part II, Chapter 27. 1690. Availableonline at <http://www.ilt.columbia.edu/publications/ locke_understanding.html>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.46WHO GOES THERE?much concerned, and as justly accountable for any action that was donea thousand years since, appropriated to me now by this self-conscious-ness, as I am for what I did the last moment.In this personal identity is founded all the right and justice of rewardand punishment.The distinction between physical and psychological identity is criticalto the understanding of authentication systems, because authentication
systems use features of both types of identity, and they differ in their
properties and application depending on which type of identity they au-
thenticate.  Password- and key-based authentication systems, for example,

can be used to make individuals express intent (because such systems
authenticate psychological identity), but they are prone to the theft of
authentication secrets.  Some biometrics, on the other hand, can be used

to identify individuals without those individualsÕ active participation and
awareness, so care needs to be taken when using biometrics in authentica-
tion systems designed to ensure accountability.Some authentication systems also authenticate claims not on the basisof physical or psychological identity but instead on the basis of the pos-
session of an artifact.Thus there are three generic approaches to authentication.  They are often
described as Òsomething you know,Ó Òsomething you have,Ó and Òsomething
you are.Ó19 The properties of each approach are discussed below.
Authentication systems often combine two or all three of these ap-proaches.  This is what is done at a typical automated teller machine

(ATM).  Both Òsomething you haveÓ (the bankcard) and Òsomething you

knowÓ the personal identification number or PIN are required to access
account information or to make changes to the account.  A combination of

approaches (sometimes referred to as multifactor authentication) gener-
ally provides more security than do single approaches alone, because the
strengths of one approach can be used to compensate for the weaknesses
of another.  At the same time, depending on implementation and other

systems choices, multifactor authentication may raise more privacy con-
siderations than single-factor authentication.19The National Institute of Standards and Technology articulated these principles asrelated to computer security in the 1970s (D.E. Raphael and J.R. Young, Automated PersonalIdentification, SRI International, 1974; National Bureau of Standards, Evaluation Techniquesfor Human Identification,  FIPSPUB-48, April 1977).
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT47Authenticating Physical IdentityAuthentication systems based on physical characteristics (somethingyou are) authenticate individuals by observing physical characteristics of
the body of the individual; these systems are often called biometric authen-
tication systems.  The physical characteristics used differ from system to

system; some use fingerprints, others use the geometry of the human hand,
others use the pattern of tissues in the iris of the eye, and so on.A person wishing to be authenticated by means of a biometric mecha-nism need not remember anything, nor does he or she need to remember
to carry an object.  Unlike passwords, properly chosen biometrics cannot

be readily shared in normal use.20Authenticating Psychological IdentityAuthentication systems based on covert knowledge (something youknow) authenticate users by requiring the individual to recite a secret
(sometimes personal information).  These systems rely on what Locke

would consider the identity of a ÒpersonÓÑthat is, they depend on the
psychological continuity of a personÕs memory.21   The benefit of this
20While a number of effective techniques for attacking biometric systems have been pub-lished, the majority of users, who are neither malicious nor technologically sophisticated,will not use these techniques to circumvent protections against sharing for casual reasonsÑe.g., to share biometric identifiers with family members, colleagues, and so on
(T. Matsumoto, H. Matsumoto, K. Yamada, and S. Hoshino,  ÒImpact of Artificial Gummy
Fingers on Fingerprint Systems,Ó Proceedings of SPIE 4677 (January 2002), available online at
<http://research.nii.ac.jp/kaken-johogaku/reports/H13_overview/A04-00-1.pdf>; L.
Thalheim, J. Krissler, and P. Ziegler, ÒBiometric Access Protection Devices and Their Pro-grams Put to the Test,Ó CÕt Magazine 11 (May 21, 2002):114, available online at <http://
www.heise.de/ct/english/02/11/114>; T. van der Putte and J. Keuning,  ÒBiometrical Fin-

gerprint Recognition: DonÕt Get Your Fingers Burned,Ó Proceeding of the IFIP TC8/WG8.8Fourth Working Conference on Smart Card Research and Advanced Applications, Kluwer Aca-
demic Press, 2000, pp.  289-303, available online at  <http://www.keuning.com/biometry/
Biometrical_Fingerprint_Recognition.pdf>; and D. Blackburn, M. Bone, P. Grother, and
J. Phillips, Facial Recognition Vendor Test 2000: Evaluation Report, U.S.  Department of De-
fense, January 2001, available online at <www.frvt.org>).21Another way of thinking about identity continuity is to consider the case where twodifferent names (or instances of the same name) correspond to the same principal (this isknown in the distributed systems literature as an Òindirect nameÓ or Òsymbolic linkÓ).  The

classic example comes from the registration of title to real estate.  It is very common that
someone who wishes to sell a house uses a name different from his or her name at the timethe house was purchased: the person might have changed their name in marrying, or after a
criminal conviction.  A classic identity problem is knowing that the ÒMrs. Janet RogersÓ
wishing to sell property at 1423 Constitution Avenue is the same person as the ÒMiss JanetFoster SmithÓ who purchased it 11 years ago.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.48WHO GOES THERE?approach is that it does not require a person to remember to carry aparticular object and is in general one of the simplest forms of authentica-
tion.  However, what is known can be forgotten, shared, guessed, or just

plain stolen by being overheard or from the noting of written reminders.
There are at least two types of covert knowledge: synthetic secrets and
private secrets.Synthetic secrets are items of information created specifically for the
purpose of authentication; they typically have no relation to characteris-
tics of the individual or to events in the (human) individualÕs life.  Pass-

words are a type of synthetic secret (when used properly) and the classic
example of the Òsomething you knowÓ approach to authentication.The principal problem with using a synthetic secret for authenticationis that because it is unrelated to the individualÕs life in any meaningful
way, it is often difficult to remember.  A  joke in the security community

illustrates the point: ÒThere are two rules for choosing a good password:
(1) pick something you canÕt remember and (2) donÕt write it down.Ó  This

problem arises because synthetic secrets that are easy to remember are
also usually easy for others to discover or guess.22Private secrets are items of information that are so intimately associ-
ated with an individual or with events in the (human) individualÕs life
that no other person (or few others) would be expected to know about
them.The use of private secrets for authentication causes several problems.People resist the use of private secrets for authentication on the grounds
that they are private and should not have to be revealed to third parties
(even to third parties who wish to authenticate us).  Private secrets are

rarely completely private.23  This leads to another problem:  Any item of
information that is used as a private secret to authenticate an individual
will typically be shared with all the people and organizations that want to
authenticate the individual (technical measures exist that could prevent
sharing this, but they are not widely used).  Each party who authenticates

the individual therefore comes to know the information that is supposed
to be a private secret, and thus the information becomes less private and
less secret as time goes by.22One approach that makes passwords more difficult to share or guess is to requirepeople to memorize images instead of sequences of words, letters, numbers, and/or charac-ters.  See ÒIn a User-friendly World, One PictureÕs Worth 1,000 Passwords: Image-Driven

Log-ons Are Easier to Use and More Secure, High-Tech Researchers Claim,Ó by Michael J.Kennedy in the Los Angeles Times, June 4, 2002, for a description of this technology andsome of the companies exploring its use.23For example, a DMV and its clerks can find out driverÕs license numbers.  An
individualÕs mother knows her own maiden name, and so do other members of the family.Many people know or can find out Social Security numbers.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT49Not only do these problems compromise the individualÕs privacy, butthey also gradually destroy the usefulness of the information as an au-
thenticator.  When the informationÕs value as an authenticator has been

substantially degraded, another piece of (private) information will have
to be chosen as a new basis for the private-secret authentication process.
If this sequence of events is repeated enough times, many of the
individualÕs private secrets will have been revealed to large numbers of
other parties.  The individualÕs privacy will have been put at considerable

risk, as will the ability of other parties to authenticate the individual.
Social Security numbers are an excellent case study in this phenomenon,
as a recent incident involving the Princeton University admissions office
illustrates.  Both Princeton University and Yale University use Social Se-

curity numbers as student identifiers.  YaleÕs admissions office used the

Social Security number as an authentication key to allow students to ac-
cess the status of their applications for admission via a Web browser.  A

member of the Princeton University admissions office staff discovered
this and apparently used Social Security numbers obtained from the
records of applicants to Princeton in order to access the Yale admissions
Web site and learn about YaleÕs admissions decisions.24Authenticating Possession of an ArtifactAnother traditional approach to authentication is the possession of aunique object.  A typical house key is an example of the Òsomething you

haveÓ approach.  (Box 2.2 describes authentication by means of a car key

fob.)  The object should be hard to duplicate, at least by simple observa-

tion by a third party.  In other words, it is possible to duplicate a house

key, but merely observing a key being used does not allow the observer to
duplicate the key.The object that is possessed may have a range of functionality.  It may
be as simple as a traditional house key, whose shape defines it.  It may be

a credit card with raised lettering and a magnetic stripe for storing infor-
mation.  It may also be a smart card, with an embedded processor that

may be used to store information or to act as a cryptographic processor.These different types of objects have different levels of resistance totampering and duplication.  A house key is readily duplicated, if desired,

by the person in possession of it.  A credit card also may be duplicated,

provided the person in possession of the card has the appropriate equip-
ment.    Smart cards, particularly those that perform cryptographic opera-
24See ÒIvy Imbroglio: Princeton Says It Spied on Yale,Ó Wall Street Journal, July 26, 2002,p. B1.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.50WHO GOES THERE?tions, are in theory harder to duplicate, because they do not ever discloseduring normal operation the secret information that would be required to
duplicate them.The Òsomething you haveÓ approach has the advantage that theholder does not need to remember a password.  The object that is used can

be designed to be hard to copy, so it cannot be readily used by more than
one person at a time, although it could be loaned (in which case the
original person loses access to it while it is on loan).  However, objects can

easily be lost or stolen, and sometimes people are not in a position to carry
an item when they want to use a system.IDENTIFICATIONThe processes of authentication and identification are related but dis-tinct.  While the former may require the verifying party to authenticate an

identifier that refers to the individual, the identification of an individual is
distinct from the authentication of the individualÕs claimed identity.  Some
authentication technologies (particularly biometric technologies) are used
in both processes.  Identification associates an identifier with an indi-

vidual without the requirement of a claim on the part of the subject.More specifically, identifying an individual refers to the process ofexamining and/or interrogating an individual (usually an individual who
has been encountered before) with the objective of determining whichBOX 2.2Car Key FobsRemote car door key fobs and garage door openers work by way of radiosignals.  Very early units sent a constant signal, which is clearly insecure.  Later

versions used an 8-bit key to distinguish among different transmitters.  Again, thiswas inadequate; thieves learned to record and retransmit the signals.  Many mod-
ern units use a so-called rolling code, which is generated from a pseudo-random
number generator.  Eavesdropping on a few code transmissions should not pro-
vide enough information to predict the next code.  To avoid problems from the loss
of synchronization, a range of codes is accepted by the receiver.  A mechanism is

also provided to resynchronize the transmitter and the receiver.  No identification
signal per se is transmitted by such devices.  Many new cars use a radio transpon-
der embedded in the keys themselves to unlock the ignition.  Some of these use

rolling codes; others use challenge/response technologies.  Key fobs contain a
modest amount of storage and computational ability.  Specialized equipment is
required to copy them, but they are easily stolen.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT51identifier refers to that individual.  In contrast, authenticating an identi-
fier refers to the process of verifying the linkage between an identifier
(usually claimed by the individual, but sometimes observed) and the
individual.THE RELATIONSHIP BETWEENAUTHENTICATION AND IDENTIFICATIONGiven that authentication and identification are distinct but relatedconcepts, it is important to understand the interplay between them.  While

authentication for accountability almost always eventually requires iden-
tifying an individual at some point (as discussed previously), authentica-
tion for authorization does not always require this.  In the first place,

authorization does not always require authentication.  When authoriza-

tion does require authentication, as discussed previously, it does not
always require individual authentication.  Even when authorization

requires individual authentication, it often does not require the authenti-
cated identifier to be a personal name.  The common use of credit cards is

an example.  The credit card number is the unique identifier.  If the card

has not been reported lost or stolen, it is assumed that the holder of the
card is authorized to use it.  For credit card purchases made by phone or

over the Internet during which the physical holding of the card cannot be
observed, a secondary piece of information from the card, such as an
expiration date or additional code number, is requested.25It is essential to develop authentication systems whose strength (andoften therefore whose intrusiveness into privacy) is in line with the secu-
rity needs of and threats to the resources being protected.  In some cases it

may be appropriate to require users to forgo some privacy when they are
authenticated for purposes of accessing very sensitive or very valuable
resources.  Note that the information being protected may itself be pri-

vacy-sensitive, and thus may merit strong authentication on that basis
alone.Recommendation 2.1. The strength of the authentication sys-tem employed in any system should be commensurate with the
value of the resources (information or material) being protected.
Authorization systems usually do identify individualsÕ personalnames, even when it is not necessary to do so to meet the required secu-25Of course, the second piece of information from the card is valid the first time it is everused and becomes less valuable with each additional use.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.52WHO GOES THERE?rity goals.  The main reason for this is convenience: Even for authoriza-
tion policies that do not use them directly, personal names are familiar,
sufficiently differentiable to discriminate among user populations of mod-
est size, and convenient to use as a way to look up other individual
attributes.  They are therefore often used as the ÒlabelÓ on the Òfile folderÓ

that contains the individualÕs other attributes.  Of course, 
some authoriza-
tion policies actually do require the use of personal names, and some
authorization systems collect personal names up-frontÑthat is, preven-
tivelyÑinstead of waiting until it is clear that personal names are neces-
sary before collecting them.Finding 2.1. Authorization does not always require individualauthentication or identification, but most existing authoriza-
tion systems perform one of these functions anyway.  Similarly,

a requirement for authentication does not always imply that
accountability is needed, but many authentication systems gen-
erate and store information as though it were.Recommendation 2.2.  Systems that demand authentication for
purposes other than accountability, and that do not themselves
require accountability, should not collect accountability infor-
mation.Recommendation 2.3. Individual authentication should not beperformed if authorization based on nonidentifying attributes
will suffice.  Where appropriate, authorization technologies and

systems that use only nonidentifying attributes should be used
in lieu of individual authentication technologies.  When indi-

vidual authentication is required, the system should be subject
to the guidelines in Recommendation 3.2 (see Chapter 3).
The CSTB report IDsÑNot That Easy raised a number of questionsthat should be addressed before the implementation of any large-scale
identity system.  These same questions apply generally to authentication

systems, given that authentication and identity are often closely con-
nected.  While smaller-scale authentication systems may imply decreased

urgency (that is, a system to restrict access to a hotel swimming pool, in
which the attribute necessary for authorization  is Òcurrent hotel guest,Ó

may require less rigorous attention to these questions than a system that
would track the enrollment status of all foreign students in the United
States on the basis of their visas or other IDs), the principles outlined in
IDsÑNot That Easy still hold, especially with regard to understanding the
goals of the system and minimizing unnecessary data collection and re-Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION IN THE ABSTRACT53tention.  The questions are reprinted here from 
IDsÑNot That Easy26 for
reference.¥What is the 
purpose of the system? Possible purposes of an identitysystem include expediting and/or tracking travel;  prospectively moni-

toring individualsÕ activities in order to detect suspicious acts; retrospec-
tively identifying perpetrators of crimes.¥What is the 
scope of the population to whom an ÒIDÓ would be issued
and, presumably, recorded in the system?  How would the identities of

these individuals be authenticated?¥What is the 
scope of the data that would be gathered about individu-
als participating in the system and correlated with their system identity?
ÒIdentification systems,Ó despite the name, often do much more than just
identify individuals; many identity systems use IDs as keys to a much
larger collection of data.  Are these data identity data only (and what is

meant by identity data)? Or are other data collected, stored, and/or ana-
lyzed as well?   With what confidence would the accuracy and quality of

this data be established and subsequently determined?¥Who would be the user(s) of the system (as opposed to those who
would participate in the system by having an ID)?  If the public sector or

government will be the primary user, what parts of the government will
be users, in what contexts, and with what constraints?  In what setting(s)

in the public sphere would such a system be used?  Would state and local

governments have access to the system?  Would the private sector be

allowed to use the system?  What entities in the private sector would be

allowed to use the system? Who could contribute, view, and/or edit data
in the system?¥What 
types of use would be allowed?  Who would be able to ask for
an ID, and under what circumstances?  Assuming that there are datasets

associated with an individualÕs identity, what types of queries would be
permitted (e.g., ÒIs this person allowed to travel?Ó ÒDoes this person have
a criminal record?Ó).  Beyond simple queries, would analysis and data

mining of the information collected be permitted?  If so, who would be

allowed to do such analysis and for what purpose(s)?¥Would participation in and/or identification by the system be 
vol-untary or mandatory?  In addition, would participants have to be aware of
or consent to having their IDs checked (as opposed to, for example, being
subjected to surreptitious facial recognition)?26Computer Science and Telecommunications Board, National Research Council. IDsÑNot That Easy: Questions About Nationwide Identity Systems.  Washington, D.C., National
Academy Press, 2002, pp. 9-11.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.54WHO GOES THERE?¥What 
legal structures protect the systemÕs integrity as well as the
data subjectÕs privacy and due process rights, and which structures deter-
mine the liability of the government and relying parties for system misuse
or failure?The next chapter explores the history and meaning of privacy, con-cluding with a recommendation for the development of authentication
systems modeled on these questions.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.553Privacy Challenges inAuthentication SystemsIn principle, authentication technologies can both advance and under-mine privacy interests.  In practice, however, a combination of forces,

including the followingÑ¥The influence of the prevalent security paradigm of fully mediated
access,¥The desire of businesses to collect personal information cheaply
and unobtrusively,¥The pressure on governments and businesses to streamline their
interactions and reduce costs, and¥The resiliency of digital informationÑ
is more likely to lead to authentication systems that¥Increase requests for identification,
¥Increase the collection of personal information,

¥Decrease the ability of individuals to understand and participate in
data collection decisions,¥Facilitate record linkage and profiling, and

¥Decrease the likelihood that individuals will receive notice of or
have the right to object to third-party access to personal information.While authentication systems can undermine privacy in these ways,they can also be used in privacy-enhancing or privacy-preserving ways,Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.56WHO GOES THERE?primarily by securing personal data and preventing unauthorized access tothe data.  The privacy-enhancing benefits of authentication systems are de-

rived from the security features of the overall systems in which they are
deployed and are not intrinsic to the authentication components themselves.As with any technology, careful consideration of the privacy risks,benefits, and trade-offs involved must be considered before authentica-
tion systems are designed and deployed.  To some extent, tension be-

tween authentication and privacy is inherent, because the act of authenti-
cation often requires some revelation and confirmation of personal
information.1PRIVACY IMPACT OF THE DECISION TO AUTHENTICATEFirst, let us look in broad terms at what an authentication systemrequires and examine how the collection, retention, reuse, and linkage of
personal information might affect privacy interests:¥Establishing an initial identifier or attribute for use within the sys-tem may require an individual to reveal personal facts or information
(such as name, address, fingerprints).  A requirement to reveal identify-

ing personal information may inhibit participation in certain activities
(such as medical tests).¥The act of authentication itself may cause the creation of records ofindividualsÕ actions (such as where they shop, what they read, and whenthey come and go) that are linkable to one of three entities: a specific
individual (individual authentication); a (possibly pseudonymous) iden-
tity that may or may not be linked to an individual (identity authentica-
tion); or, an attribute that applies to a specific individual (attribute au-
thentication).¥In addition, transactional information revealing details of an event(purchase, building entry) may be created as a result of or subsequent to
authentication and can then be linked back to the identity or individual
and be retained in the relevant record.¥The requirements of the authentication or initial identity-establish-ment process may impose objectionable requirements (for example, they
might conflict with religious beliefs2 or impose on bodily integrity).
1In fact, some private sector and public sector policies impose requirements on thosewho collect data related to the protection of those data.2In June 2002, CNN reported ÒMuslim Woman to Challenge Ban on Veil in DriverÕs LicensePhoto,Ó available online at <http://www.cnn.com/2002/LAW/06/27/license.veil.ap/
index.html>.  For religious reasons, a woman wanted to wear a veil for her driver
Õs licensephoto in spite of objections from the State of Florida that allowing it would jeopardizepublic safety.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS57¥Personal information or data may be exposed at multiple pointsand to multiple entities during the operation of an authentication system:
They may be revealed during the authentication process, created during
the authentication process, and/or retained as a result of the authentica-
tion process, all of which affect privacy.  Personal information may also

remain within a device possessed by the individual, reside in a system
run by a single entity, or enable many entities to observe and/or collect
personal information.¥Authentication may require the use of an identifier that, even if notpersonally identifiable per se, can be used to compile a dossier of facts
(records of use of the identifier) that otherwise would be difficult or im-
possible to correlate.  This collection of discrete facts may lead to a revela-

tion of the individualÕs identity.¥Depending on where the userÕs identity and other authentication-related data are stored, they may be accessible to a variety of individuals
within one or more institutions, and they may be more or less susceptible to
access by hostile third parties through technical exploits or legal processes.This general examination of authentication systems and the personalinformation practices that result from such systems harks back to the
several general privacy risks created or increased by authentication sys-
tems, as described in Chapter 1 of this report: covert identification, exces-
sive use of authentication technology, excessive aggregation of personal
information, and chilling effects.Given this categorization of privacy risks, an examination of relevantprivacy interests will provide a better understanding of the foundations
and contours of such interests, the values they protect, and the challenges
that authentication technologies pose to privacy interests.ACCESS CONTROL AND INFORMATION SYSTEMSAccess policies are a defining aspect of information systems.  In a
networked environment, the mediation of absolutely every user interac-
tion with the system and its resources is a first step in enforcing access-
control policies, identifying misuse, and investigating breaches.  The

Internet, perhaps the canonical example of a large, networked informa-
tion system and a vast network of networks, while in many respects
Òopen,Ó is a highly mediated environment. Standards and protocols es-
tablish the who, what, when, where, and how of information exchanges.33As Larry Lessig (in Code and Other Laws of Cyberspace, New York, Basic Books, 1999) andJoel Reidenberg (in ÒLex Informatica: The Formulation of Privacy Rules Through Technol-ogy,Ó Texas Law Review 76(1998):553-593) argue, these standards establish the code by which
online behavior is regulated.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.58WHO GOES THERE?Decisions about whether a given user may communicate with a resource,whether a given computer may communicate with another, whether a
given network may communicate with another, and what extent of infor-
mation exchange is allowed in each instance dominate the Internet. This
is in part because the Internet exists at the collective will of individuals,
private parties, and government entities to allow information to flow
across their systems.  Without these agreements to support the exchange

of bits, there would be no Internet.4These agreements also conceal the organizational boundaries and in-stitutional rules that users traverse when they access a site.  Users are

generally unaware of the intricacies established by their Internet service
provider (ISP) or of the communication requirements for moving around
on the Internet.  The reality is that what users experience as a library or a
public space is in fact a mixture of public and private networks.  Not only

are users generally ignorant of the jurisdictional boundaries they cross,
but they are also usually oblivious of the presence of other users. One
commentator said that being on the Internet is Òlike being in a movietheater without a view of the other seats. . .[where] masses of silent,
shuffling consumers . . . register their presence only by the fact of a turn-
stile-like ÔhitÕ upon each web page they visit. . .
Ó5  These characteristics of
the online world are in stark contrast with the physical world in three
important respects:1.In the physical world there are many clearly defined public spaces
and many privately owned spaces in which access control is nonexistent
or minimal;2.In physical space, relatively few actions are mediated; and

3.In the off-line world, if mediation occurs it is typically evidenced
by a physical sign.In the off-line world, individuals and institutions make decisionsabout whether or not to mediate interactions between individuals and
resources.  For example, a university may decide not to control who walks

across the campus but to control who enters certain buildings. Similarly,
libraries and bookstores generally do not exert control over who enters
the premises or what materials they access, but they do exert control over4For a detailed look at the technological underpinnings of the Internet, see ComputerScience and Telecommunications Board, National Research Council, The InternetÕs Coming ofAge, Washington, D.C., National Academy Press, 2001.5Jonathan Zittrain. ÒThe Rise and Fall of Sysopdom.Ó Harvard Journal of Law and Technology10(1997):495. Available online at <http://jolt.law.harvard.edu/low/articles/10hjolt495.html>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS59the terms on which individuals may remove things from the premises.  In
contrast, in a networked environmentÑthat is, an engineered systemÑthe answer to the question Should we mediate access? is almost always
yes; the inquiry begins with the questions How much do we mediate?
With what mechanism?With increasing frequency, authentication systems are being deployedto control access and movement in physical spaces as well as to control
access to networked systems themselves.   The increase in the scope of

authentication and identification supported by networked systems is ex-
tending the scope of recorded interactions.  The systems and the hard-

ware that interacts with them are changing the information that can be
collected during interactions and the extent to which it can be reused.   As

discussed below, these changes challenge the privacy of individuals in
four significant respects.1.Computer technology reduces the costs of record keeping.
  The reduction
in costs has escalated the data collection and retention associated with
authentication events.  Increased data collection and retention exacerbate

the privacy consequences of authentication events.  Flashing one
Õs driverÕslicense in a corner store is qualitatively different from providing a digital
copy of oneÕs driverÕs license to an online merchant.  In the latter case, the
driverÕs license information is provided to the merchant in a format thatencourages capture and allows for retention and reuse. One potential
outcome of this change is that identity authentication (or the authentica-
tion of a relatively unique attribute or set of attributes with the same
effect) is more likely to result in a personally identifiable stored record
than was the case in earlier environments.  A recent example illustrates

this point. Using a scanner that allows him to read and capture data from
the magnetic stripes on the back of Massachusetts driverÕs licenses, abarkeep in Boston has built a database of personal informationÑinclud-ing driverÕs license number, height, weight, date of birth, eye and haircolor, address, and, in some instances, Social Security numberÑon hispatrons.6  Without the state-issued driver
Õs license, collecting such dataon individuals would be expensive and cumbersome and would meet
with privacy objections.  The introduction of machine-readable cards and

the market availability of readers have increased the chances that per-
sonal information would be captured, reused, and potentially sold.  The

introduction of technologyÑwithout any change in policyÑhas led topractices that are more invasive of privacy.6Jennifer Lee. ÒFinding Pay Dirt in Scannable DriverÕs Licenses.Ó New York Times, March21, 2002.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.60WHO GOES THERE?2.Once data are collected, computerized record-keeping facilitates recordlinkage.7 Distributed relational databases allow diverse records with a
common attribute or attributes to be more readily combined.  This ability

to link and profile record subjects supports the secondary use of informa-
tion. To build on the driverÕs license example above, stores across thecountry are making similar use of scannable driverÕs license data.8  As
customer records across various sectors of the economy become tied to
driverÕs license data, it becomes markedly easier to share and merge fordifferent purposes the data collected by different establishments. And it is
not only the private sector that makes use of the scannable licenses to
control access.  Some government buildings are also using these scan-

nable licenses to record information about visitors.3.Rules codified for use in computerized systems are generally less flexible(for both good and bad uses) than policies implemented by humans.  Businessesand other entities often treat long-time customers and first-time custom-
ers differently.9  A long-time customer may not need to provide the same
level of authentication before engaging in an interaction or transaction.
Information systems, while they can be programmed to treat different
people differently, generally apply authentication rules designed for the
worst-case scenario (in this instance, the new customer). In other words,
unless otherwise directed, the system will demand the same information
from a repeat visitor as from a newcomer and will retain that information.
Therefore, the baseline data collected in information systems transactions
tends to be richer than that collected in manual systems.4.Information technology enables covert identification and possibly overtidentity authentication on a large scale.  The covert nature of some informa-
tion systems used for identification and identity authentication (such as
the driverÕs license scanners discussed above) denies individuals full in-formation about the transaction and impedes oversight and accountabil-7See the 1993 report of the Committee on National Statistics, Private Lives and PublicPolicies: Confidentiality and Accessibility of Government Statistics, Washington, D.C., NationalAcademy Press, 1993, as well as the same committeeÕs 2000 workshop report Improving
Access to and Confidentiality of Research Data, Washington, D.C., National Academy Press,2000, for more on issues surrounding data collection, linkage, and confidentiality. Available
online at <http://www7.nationalacademies.org/cnstat/>.8The DriverÕs Privacy Protection Act of 1994 prohibits states from disclosing this infor-mation, except in limited circumstances, without individual consent.  While the law does

not prohibit the creation of such databases by the private sector, it is clear that scannablelicenses undermine congressional policy to limit the use of driverÕs license data for non-driving-related purposes.9The downside of this practice is discrimination.  Without accurate data, rules about who
is a risky customer are more likely to be influenced by the biases of the business or indi-vidual.  Accurate data can check these tendencies.
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS61ity through the political process. While individuals are aware that thelicense is being scanned, they are not necessarily informed that informa-
tion from it may be retained, reused, exchanged, or used to link with
other systems.  Indeed, individuals are unlikely to know what informa-

tion can actually be retrieved from scanning the back of the license.  Even

if people were to learn over time the data collection possibilities inherent
in a driverÕs license, there will always be circumstances in which nondis-closure  of those possibilities can cause problems.
There are other systems that, while discussed prior to implementa-tion or debated by the public after the fact, nevertheless provide little
signal to the individual at the time that identification occurs.  For ex-

ample, many cities have installed cameras to detect drivers running red
lights.  In locations where such cameras have been proposed or imple-

mented, initial opposition has often generated community discussion
about what information is collected, what decisions can be made on the
basis of it, and what recourse is available to individuals.10  While this
public debate increases the general awareness of the individuals who
reside in an area (but not necessarily those who pass through), the collect-
ing of information in this way is more covert than the scanning of driverÕslicenses described above.  An individual gives over a driver
Õs license.Here, an individual drives through an intersectionÑhardly an activitythat signals an identification or authentication event.  While the cameras

are more easily understood by individuals as identification (surveillance)
tools than is the driverÕs license reader, it is less likely that the presence ofa camera will be noticed.The increasing use of the Internet and other networked systems tosupport access to information, deliver services, and communicate raises
questions about the access-control policies governing these interactions
and their impact on individual privacy.  Similarly, the use of information
systems and networking to control access to and movement in physical
spaces and to support attribute- and identity-based service and sales deci-
sions off-line raises questions about the authentication systems that sup-
port these interactions and their privacy implications. Ubiquitous com-
puting, sensor-equipped buildings, and smart highways are the direction
of the future. They raise important questions about what kind of authen-
tication occurs, how the data used and generated during authentication
events are handled, and how the answers to these questions support or10William Matthews. ÒBattle Lines Form over Red-Light Cameras.Ó Federal Computer Week(September 3, 2001).  Available online at  <http://www.fcw.com/geb/articles/2001/sep/
geb-comm2-09-01.asp>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.62WHO GOES THERE?undermine individual privacy, access to information, freedom of associa-tion, and other democratic values.A highly mediated environment of networked systems requires sys-tem owners to choose between attribute authentication and identity au-
thentication. This choice and the decisions about retention, reuse, and
disclosure that flow from it influence the degree of privacy that individu-
als using the system enjoy.  To the extent that individuals are aware of the

chosen policies and their implications, the privacy provided by the sys-
tem will in turn influence individualsÕ decisions about how and in what
circumstances to interact with it.THE LEGAL FOUNDATIONS OF PRIVACYPrivacy is a fundamental tenet of legal systems and political philoso-phies that value individual freedom, autonomy, and political participa-
tion.  Privacy has many and varied definitions and is evoked in many

contexts to achieve differing results.  It has important political, emotional,

social, and legal dimensions.  It protects against intrusions in physical

places, interference with personal decisions, misuse of personal informa-
tion, and various interests similar to property interests. The underlying
values that privacy protects include individuality and autonomy; inti-
macy; fairness; and limited, tolerant government.Early legal definitions of privacy center on the notion of being leftalone.  Phrases such as 
Òa manÕs home is his castleÓ11 and 
Òthe right to belet aloneÓ12 capture this notion of privacy, which encompasses the ability
of individuals to retreat to the safety of home, pull the shades, and lock
the doors, freeing themselves from prying neighbors and state surveil-
lance.  While a powerful and important element of privacy, this right to

seclusion became increasingly incapable of protecting individuals as soci-
ety became more interdependent and as interactions became more infor-
mation-rich. Social and technological changes in the 1960s and 1970s gen-
erated renewed interest on the part of philosophers and lawyers in
defining and conceptualizing privacy.13  From their analyses and writ-
ings emerged an appreciation for a more complex and multifaceted con-
cept of privacy and its legal foundations.11Ò. . . [T]he house of every one is to him as his castle and fortress.Ó SemayneÕs Case, 5 C.Rep. 91a, 77 Eng. Rep. 194 (K.B. 1603).12ÒThey conferred, as against the Government, the right to be let aloneÑthe most com-prehensive of rights, and the right most valued by civilized men.Ó Justice Brandeis dissent-
ing in Olmstead v. United States
, 277 U.S. 438, 478 (1928).13See, for example, Edward J. Bloustein, ÒPrivacy as an Aspect of Human Dignity,Ó NewYork University Law Review 39 (December 1964): 962-1007; Charles Fried, 
ÒPrivacy,Ó Yale LawWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS63Privacy law in the United States derives from many sources, includ-ing common law, the U.S. Constitution and state constitutions, and state
and federal statutes.  As the values that it protects suggest, privacy law

comprises several branches.  This report examines the potential privacy

impact of authentication technologies on four areas of privacy, each of
which has a constitutional basis in the United States:1.Bodily integrity, which protects the individual from intrusivesearches and seizures;2.Decisional privacy, which protects the individual from interferencewith decisions about self and family;3.Information privacy, which protects the individualÕs interest in con-trolling the flow of information about the self to others; and4.Communications privacy, a subset of information privacy that pro-tects the confidentiality of individualsÕ communications.
As discussed above, authentication technology can intrude on each ofthese privacy interests.  Authentication methods may require contact with

or close proximity to the body, potentially raising concerns under the
Òbodily integrityÓ branch of privacy law.  Authentication may introduce
new opportunities to collect and reuse personal information, intruding on
Òinformation privacy.Ó   Authentication systems may be deployed in a
manner that interferes with individualsÕ Òdecisional privacyÓ by creatingopportunities for others to monitor and interfere with important expres-
sive or other personal activities. Authentication methods may raise new
opportunities to intercept or monitor a specific individualÕs communica-tions, revealing the personÕs thoughts and the identities of the individualswith whom he or she communicates. This section provides some histori-
cal context for the privacy interests listed above.Constitutional Roots of PrivacyThe word ÒprivacyÓ is notably absent from the U.S. Constitution.
However, the values and interests that privacy protects are explicitly ex-
pressed in various amendments and have been held by the U.S. Supreme
Court to be implicit in other amendments.  For example, the Fourth

Amendment prohibition against unreasonable searches and seizures and
the Fifth Amendment prohibition of compelled self-incrimination explic-Journal (January 1968): 475-493; Judith Jarvis Thompson, 
ÒThe Right to Privacy,Ó Philosophyand Public Affairs 4 (summer 1975): 303; James Rachels, 
ÒWhy Privacy Is Important,Ó Philoso-phy and Public Affairs 4 (summer 1975): 323-333; William M. Beaney, 
ÒThe Right to Privacyand American Law,Ó Law and Contemporary Problems 31 (1966): 357.
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.64WHO GOES THERE?itly protect privacy interests in personal papers and effects and in per-sonal thoughts and beliefs, respectively,14 while the First Amendment
prohibition against the suppression of speech and assembly has been
found to implicitly include the right to speak and to assemble anony-
mously. The Supreme Court has interpreted the First, Third, Fourth, Fifth,
Ninth, and Fourteenth Amendments as providing protection for different
aspects of personal privacy.  Although it is important to note that consti-

tutional claims arise only in cases in which some state action interferes
with privacy, the values represented by these constitutional claims reso-
nate broadly throughout society.First Amendment Interest in Privacy and AnonymityThe First Amendment guarantees the freedoms of speech, associa-tion, and access to information.  Numerous Supreme Court cases docu-

ment the right of individuals to speak, associate, and receive information
without having their identities revealed.  The ability to speak anony-

mously is rooted not only in the Constitution but also in the actions forg-
ing a consensus for its ratification.  The Federalist Papers were penned

under several noms de plume. The Supreme Court has affirmed the right
of anonymity in political speech and the right to solicit door to door
without registering or identifying oneself.15  Similarly, the Court has rec-ognized the chilling effect that the disclosure of membership lists would
have on the freedom to associate, and therefore it has shielded such lists
from government scrutiny.16  The ability to receive information anony-
mously, the corollary of the right to speak anonymously, while less clearly14ÒWhen the Fourth and Fifth Amendments were adopted, Ôthe form that evil had there-tofore takenÕ had been necessarily simple. Force and violence were then the only means
known to man by which a Government could directly effect self-incrimination. It could
compel the individual to testifyÑa compulsion effected, if need be, by torture. It couldsecure possession of his papers and other articles incident to his private lifeÑa seizureeffected, if need be, by breaking and entry. Protection against such invasion of Ôthe sancti-ties of a manÕs home and the privacies of lifeÕ was provided in the Fourth and Fifth Amend-
ments by specific language.Ó Justice Brandeis dissenting in 
Olmstead v. United States, 277
U.S. 473, quoting Boyd v. United States
, 116 U.S. 616, 630.15See McIntyre v. Ohio Elections Commission
, 514 U.S. 334 (striking down a state statuterequiring political leafleteers to identify themselves on their leaflets). Recently the SupremeCourt upheld a similar challenge to a local ordinance requiring all individuals petitioning
door to door to register and identify themselves (Watchtower Bible and Tract Society, Inc. v.
Village of Stratton, 00-1737).  Also see 
Watchtower Bible and Tract Society of New York, Inc., et al.v. Village of Stratton, et al.
 (00-1737) 240 F.3d 553, reversed and remanded;  available online
at <http://supct.law.cornell.edu/supct/html/00-1737.ZS.html>.16NAACP v. Alabama
, 357 U.S. 449 (1958) (striking down a state statute that requiredorganizations to disclose their membership to the state).Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS65articulated by the Court, can be found in cases forbidding the governmentfrom requiring individuals to affirmatively register to receive certain

kinds of information17 and affirming the right of individuals to possess
for in-home consumption ÒobsceneÓ materials that could not legally be
sold.18  Recently the Colorado Supreme Court held that the First Amend-
ment to the U.S. Constitution and the state constitution Òprotect theindividualÕs right to purchase books anonymously, free from governmen-tal interference.Ó19Third Amendment Privacy ProtectionThe Court has found protection of a right to privacy against unrea-sonable surveillance and compulsory disclosure in the Third Amend-
mentÕs protection against quartering soldiers.  This protection has gener-
ally been viewed as secondary to the broader protection of the Fourth
Amendment.Fourth Amendment Roots of Privacy LawThe Fourth Amendment to the U.S. Constitution protects individualsagainst unreasonable searches of their persons and places and against
unreasonable seizures of their property.  Fourth Amendment jurispru-

dence articulates limits on government searches of individuals, residences
and other private places, and communications.  The principle on which

the Fourth Amendment is based derives from an even older tradition in
British common law. As early as the early 17th century, British courts
were placing limits on the power of the Crown to enter anyoneÕs home.Though the power of the monarch was still substantial, SemayneÕs Case20in 1603 says that Òthe house of every one is to him as his castle andfortress.Ó Over time, this basic limitation on entry into the sanctity of
oneÕs home has been stated with more precision.  The state may not enter
17See Lamont v. Postmaster General
, 381 U.S. 301 (1965) (striking down a postal regulationrequiring individuals to register a desire to receive communist propaganda).18See Stanley v. Georgia
, 394 U.S. 557 (1969) (striking down a state statute criminalizingin-home possession of obscene material); Denver Area Educational Telecommunications Con-sortium, Inc. v. FCC
, 518 U.S. 727 (striking down cable statute requiring individuals to re-quest in writing segregated, patently offensive cable programming as overly restrictive in
light of alternatives that protected the anonymity of viewers).19Tattered Cover, Inc. v. City of Thorton
, Colo. Sup Ct 2002 Colo. LEXIS 269, April 8, 2002;see also Julie E. Cohen, ÒA Right to Read Anonymously: A Closer Look at ÔCopyrightManagementÕ in Cyberspace,
Ó 28 Conn. L. Rev. 981 (1996) (arguing that the right to read
anonymously is protected by the First Amendment).20See SemayneÕs Case, 5 C. Rep. 91a, 77 Eng. Rep. 194 (K.B. 1603).Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.66WHO GOES THERE?without a reason and a warrant issued by a court; in addition, the statemust Òknock and announceÓ the search.  Announcing the search and
presenting the target of the search with a copy of the warrant for inspec-
tion is critical to assure that the state does not enter without a warrant and
that the reasons for which the warrant were issued can be challenged, at
least after the fact. These procedural safeguards have been found neces-
sary to guard against abuse of the invasive searching power granted to
the state.Searches conducted without simultaneous notice are considered secretsearches and generally prohibited under U.S. constitutional law. For obvi-
ous reasons, wiretapping and other types of electronic surveillance are, by
definition, secret. A telephone wiretap that first announces to the parties
being tapped that their voices are being recorded is not likely to yield any
useful evidence. Yet, courts have allowed that wiretapping, though gener-
ally violating the rule against secret searches, may be allowed in limited
circumstances. Historically, electronic surveillance was only allowed for a
limited class of serious crimes, and only after other investigative means had
failed.21  In recent years the list of crimes has grown.  In addition, the
statutory protections for electronic communications such as e-mail do not
directly parallel those established for voice communications in the wake of
Supreme Court rulings, not to mention that the effects of the USA PA-
TRIOT Act of 2001 (Public Law 107-56, Uniting and Strengthening America
by Providing Appropriate Tools Required to Intercept and Obstruct Terror-
ism Act of 2001) on opportunities for surveillance and accountability are
still to be determined. (See the sections below entitled ÒStatutory PrivacyProtectionÓ and 
ÒPrivacy of Communications.Ó)Fifth Amendment Protection of PrivacyThe protection against self-incrimination also serves as a basis for atype of privacy protection, including primarily decisional privacy and,
somewhat more weakly, bodily integrity.  Although the principle of the

Fifth AmendmentÑthat no person shall be compelled to be a witnessagainst himself or herselfÑmay be relevant in many contexts, its applica-tion is limited to criminal cases or other government proceedings.  The

Court has adopted a rather narrow view of the coverage of the Fifth
Amendment by making a distinction between testimonial evidence, in-
volving communication by the individual and thus falling under the Fifth
Amendment, and physical evidence, entailing the taking of something21Recent developments may be changing this baseline, however.  For a general discus-
sion of the law, see Computer Science and Telecommunications Board, National ResearchCouncil, CryptographyÕs Role in Securing the Information Society, Washington, D.C., NationalAcademy Press, 1996.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS67from an individual and thus falling outside the protection of the FifthAmendment.  This distinction was made most clearly in 
Schmerber v.California,22 in which the Court ruled that there was no Fifth Amendment
protection against blood tests, viewed as physical evidence, to determine
blood alcohol content following a car accident.  The Court distinguished

between situations in which a defendant was forced verbally to incrimi-
nate himself or herself and situations in which marks or material were
taken from him or her for identification purposes (fingerprints, photo-
graphs) or for purposes of preventing the dissipation of evidence (blood
test).  Although the latter situations would not be covered by the Fifth

Amendment, the Court indicated that the Sixth Amendment protection of
counsel, the Fourth Amendment protection against unreasonable searches
and seizures, and the due process clause23 would provide protection
against the stateÕs overreaching in such situations.Ninth Amendment Penumbras, Fourteenth Amendment Due ProcessClause, and Decisional and Informational PrivacyAs mentioned above, privacy has been invoked to protect theindividualÕs right to make decisions about important aspects of life with-out government interference.  A line of Supreme Court cases starting with

Griswold v. Connecticut
24 in 1965 began to establish such a right, althoughvarious justices viewed the source of the right differently.  Justice Dou-

glas believed the privacy right emanated from the First, Third, Fourth,
Fifth, and Ninth amendments, which created ÒpenumbrasÓ of privacy
protection.  Other justices preferred to lodge the right in the Ninth Amend-

ment.  In 
Roe v. Wade
,25 the Court held that the right to privacy was
founded in the Fourteenth AmendmentÕs liberty clause and restrictionson state action.  The right to privacy protected in this line of cases has

been primarily limited to reproductive and family interests, including the
individualÕs right to make choices with respect to childbearing, child rear-ing, and the use of contraceptives.26  In Whalen v. Roe,27 the Court articu-22Schmerber v. California
, 384 U.S. 757 (1966).23In Rochin v. California,
 342 U.S. 165 (1952), Justice Frankfurter, writing for the majority,
said that the forced regurgitation of stomach contents was conduct that Òshocks the con-scienceÓ and violates the due process clause of the Fourteenth Amendment.
24Griswold v. Connecticut, 
381 U.S. 479 (1965).25Roe v. Wade
, 410 U.S. 113 (1973).26In Paul v. Davis
 (424 U.S. 693 (1976)), the Supreme Court refused to expand the areas of
personal privacy considered ÒfundamentalÓ to include erroneous information in a flyer
listing active shoplifters.  The court limited these fundamental privacy areas to 
Òmattersrelating to marriage, procreation, contraception, family relationships, and child rearing andeducationÓ (713).
27Whalen v. Roe, 
429 U.S. 589 (1977).Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.68WHO GOES THERE?lated a constitutional basis for a right of information privacy, arguing thatthe constitutionally protected Òzone of privacyÓ protects both an interest
in avoiding disclosure of personal matters and an interest in independent
decision making.  Although recognizing an expanded privacy interest,

the Court unanimously found that the New York law in question, which
required the maintenance of computerized records of prescriptions for
certain drugs, did not pose a significant constitutional threat to either
privacy interest, in part because of the security of the computer system
and the restrictions on disclosure.  In subsequent cases, the Court has not

expanded constitutional protections for information privacy.The Common Law Roots of Privacy LawAs mentioned above, constitutional privacy protections limit stateaction; they do not protect against intrusion by private individuals or
entities.  Historically, tort law has provided protection for some aspects of

personal privacy. English and early American case law provides examples
of the use of tort law to protect against trespass into private spaces, un-
wanted knowledge of private events, and unwanted publicity of private
matters.  In 1890, concerned with tabloid journalists
Õ and photographers
Õintrusion on private matters, Samuel D. Warren and Louis D. Brandeis, in
ÒThe Right to Privacy,Ó28 set forth the Òright to an inviolate personality.ÓAmerican courts and legislatures adopted various expressions of the new
privacy tort throughout the early 20th century.  In 1960, William L. Prosser

structured and defined these various tort law privacy protections into
four separate privacy torts:1.Intrusion upon seclusion: objectionable intrusion into the private af-
fairs or seclusion of an individual,2.Public disclosure of private facts:  publication of private information
that a reasonable person would object to having made public,3.False light: publication of objectionable, false information about an
individual, and4.Misappropriation of name or likeness:  unauthorized use of an
individualÕs picture or name for commercial advantage.29The 1964 Restatement of Torts (a clarification and compilation of thelaw by the American Law Institute) adopted the Prosser framework.3028Samuel D. Warren and Louis D. Brandeis. ÒThe Right to Privacy.Ó Harvard Law Review4 (December 1890):195.29William L. Prosser, ÒPrivacy,Ó 48 Cal. L. Rev. 383 (1960).
30Restatement of Torts (2d) 1964.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS69Together, these torts provide a basis for privacy suits against those whopublish embarrassing false information or intimate information about an
individual, peep or spy on an individual, or commercially exploit an
individualÕs picture, name, or reputation. Today privacy torts providelimited protection for individuals. As torts, they are unlikely to directly
shape the design and use of authentication systems.  However, the prin-

ciples behind the intrusion-upon-seclusion, public-disclosure-of-private-
facts, false-light, and misappropriation-of-name-or-likeness torts are use-
ful reminders of some of the things that privacy is designed to protect
againstÑintrusion into personal affairs, disclosure of sensitive personalinformation, and improper assignment of actions to individuals.  Each of

these is relevant to the discussion of authentication systems.Statutory Privacy ProtectionsIn recent years, the Federal Trade Commission (FTC) Act of 191431has become a tool for enforcing privacy statementsÑwhatever they maybeÑmade by commercial actors to the public.  Section 5 of the FTC Act
gives the FTC jurisdiction over Òunfair and deceptive trade practices.ÓImportantly, while the statute clearly provides an enforcement opportu-
nity where statements about data collection practices are made, it alone
provides no independent basis for compelling such statements, or for
driving their contents.32  A series of workshops, industry-developed self-
regulatory guidelines, and enforcement actions by the FTC and offices of
the states attorneys general have provided some check on objectionable
or questionable private sector practices.Over the years, Congress has enacted a number of privacy statutes.Most have come in response to changes in technology, to market failures,
or to narrow interpretations of the Fourth Amendment. Market failures
have led, as one would suspect, to statutes that primarily regulate private
sector behavior. Narrow rulings on the protections afforded by the Fourth
Amendment have led to statutes regulating government access to infor-
mation.  Finally, statutes that address both market failures and narrow

constitutional interpretations have most often resulted from advances in
technology that cause civil libertarians and industry to push for new
privacy protections against the expansion of governmental and private
sector authority to collect and use private information.3115 U.S.C. ¤¤ 41-51.
32Jeff Sovern has articulated the position that the FTC actually has the authority to goafter various unsavory data practices under its current legislation and mandate.  See Jeff
Sovern, ÒProtecting Privacy with Deceptive Trade Practices Legislation,Ó Fordham Law Re-view 69(4):1305.
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.70WHO GOES THERE?The existing federal and state statutory privacy protections are oftendescribed as piecemeal or patchwork.33  Personal information contained
in Òsystems of recordsÓ held by the federal government are covered by
the Privacy Act  of 1974,
34 the Freedom of Information Act of 1967,35 andother federal statutes dealing with particular records or record keepers.36Statutes of many states on access to information contain privacy excep-
tions, and some states have ÒminiÓ privacy acts.  In general, rules govern-
ing access to and use of state and local records containing personal infor-
mation are less stringent. Personal information held by the private sector
is afforded the weakest statutory protections.  While 11 federal statutes

currently provide some form of privacy protection for records held by
specific private sector entities37 and a set of statutory-like regulatory pro-
tections applies to health information,38 much detailed personal informa-
tion in the hands of businesses is available for reuse and resale to private
third parties and available to the government with little in the way of
legal standards or procedural protections.  (Chapter 6 in this report goes

into more detail about some of these statutes and the roles that govern-
ment plays in the privacy and authentication sense.)Business records are subject to few privacy regulations. While recentstatutes have increased the privacy regulations in the private sector, the33See Colin J. Bennett, Regulating Privacy: Data Protection and Public Policy in Europe andthe United States, Ithaca, N.Y., Cornell University Press, 1992; David Flaherty
, ProtectingPrivacy in Surveillance Societies, Chapel Hill, University of North Carolina Press, 1989;Priscilla M. Regan, Legislating Privacy: Technology, Social Values, and Public Policy, ChapelHill, University of North Carolina Press, 1995; and Paul Schwartz and Joel Reidenberg, DataPrivacy Law, Charlottesville, Va., Michie, 1996.345 U.S.C. ¤ 552a.
355 U.S.C. ¤ 552.
36DriverÕs Privacy Protection Act of 1994, 18 U.S.C. ¤ 2721 (1994); Family Educational
Rights and Privacy Act of 1974, 20 U.S.C. ¤ 1232g.
37Right to Financial Privacy Act of 1978, 12 U.S.C. ¤ 3401; Electronic Communications
Privacy Act of 1986, 18 U.S.C. ¤ 2510 (1995); Communications Assistance and Law Enforce-
ment Act of 1994, PL 103-414, 108 Stat. 4279 (1994) (providing heightened protections for
transactional data); Cable Communications Act of 1984, PL 98-549, 98 Stat. 2779 (1984)(codified as amended in scattered sections of 47 U.S.C.); Video Privacy Protection Act of1988, 18 U.S.C. ¤ 2710 (1994); Consumer Credit Reporting Reform Act of 1996, 15 U.S.C.
1681 ¤ 2 (1997); Telemarketing and Consumer Fraud and Abuse Prevention Act of 1994, 15
U.S.C. ¤¤ 6101
â 6108; Privacy of Customer Information (Customer Proprietary Network
Information Rules of the Telecommunications Reform Act of 1996), 47 U.S.C. ¤ 222 (c), (d)
(1996); Fair Credit Reporting Act of 1970, 15 U.S.C. ¤ 1681 et seq.; Children
Õs Online PrivacyProtection Act (1998), 16 U.S.C. ¤¤ 6501 et seq; Financial Services Modernization Act (1999),15 U.S.C. ¤ 6801 et seq.
38On April 14, 2001, privacy regulations were issued by the Department of Health andHuman Services by authority granted under the Health Insurance Portability and Account-ability Act of 1996 (see Chapter 6 for more information on HIPAA).Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS71U.S. legal and regulatory approach continues to be driven by concernsabout a given sector or a narrow class of information (see Chapter 6).  In

addition to piecemeal rules governing private sector use of personal in-
formation, the general rule established in two 1970s cases leaves personal
information ÒvoluntarilyÓ provided to businesses without Fourth Amend-
ment protection.39 The rationale espoused in these two cases dramatically
shaped privacy case law and led to statutory protections for privacy.  The

principle that in general individuals have no constitutionally based pri-
vacy interest in information about them contained in the routine records
of a business has specific consequences for individual privacy in authen-
tication systems that routinely collect information about an individual
during the course of an authentication event that precedes a transaction.INFORMATION PRIVACY ANDFAIR INFORMATION PRACTICESStatutory protections for personal information all rest on the samecore set of Òfair information practices,Ó which were developed in response
to the move from paper to computerized records.  The first 
Òcode of fairinformation practices,Ó developed in 1973 by an advisory committee in
the then-Department of Health, Education, and Welfare (HEW), provided
a core statement of principles that may be enforced either by statute or
voluntarily.40  These principles set out basic rules designed to minimize
the collection of information, ensure due-process-like protections where
personal information is relied upon, protect against secret data collection,
provide security, and ensure accountability.  In general, the principles

emphasized individual knowledge, consent, and correction, as well as the
responsibility of organizations to publicize the existence of  a record sys-

tem, to assure the reliability of data, and to prevent misuse of data.  Al-

though the practices cited in the HEW code have been broadly accepted,
slightly different iterations of fair information practices have been offered
by different bodies.41,42  Because of the broad recognition accorded the
39In 1976, in United States v. Miller,
 the Supreme Court held that individuals had no
constitutionally protected privacy interest in checks held by a bank.  Shortly thereafter, in

1979, in Smith v. Maryland
, the Court ruled that because the numbers dialed by a telephonesubscriber were routinely collected business records of phone companies, subscribers hadno Fourth Amendment privacy interest in them and therefore no right to receive notice of
or to object to their disclosure to the government.40SecretaryÕs Advisory Committee on Automated Personal Data Systems. U.S. Depart-ment of Health, Education, and Welfare.  Records, Computers and the Rights of Citizens, Wash-ington, D.C., 1973.  Available online at  <http://aspe.os.dhhs.gov/datacncl/1973privacy/
tocprefacemembers.htm>.41When discussions of online privacy began in the early 1990s, the concept and prin-Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.72WHO GOES THERE?fair information practice principles, they are explained in detail in Table 3.1and used later in this report for analyzing the privacy impact of different
authentication systems.  In general, though, the individual principles have

not been implemented with uniform rigor.  Limitations on the collection of

information have not been widely adopted, consent has been largely re-
nounced in favor of choice, and access has been harder to achieve.The concept of notice is in some respects a simple idea: people are tobe informed about how personally identifiable information is collected,
used internally, and disclosed or exchanged.  An organizationÕs informa-tion practices should be, in theory, transparent.  In practice, there are

questions about how complete notices need to be without either compro-
mising the proprietary interests of the organization or confusing people.
Additionally, what really constitutes effective notice?43ciples of Òfair information practicesÓ provided the foundation for policy discussions.  Two
executive branch study commissionsÑthe Information Infrastructure Task Force (IITF) andthe National Information Infrastructure Advisory Council (NIIAC)Ñdeveloped privacyprinciples for the National Information Infrastructure (NII).  In both cases, these study
commissions echoed many of the traditional principles developed earlier, often modifying,
and in some cases weakening, some of the core principles, such as consent and redress.  But
both commissions also struggled with questions about fair information practice that arenew in the online environment.  The IITF and the NIIAC recognized emergent principles,

including the need to provide some opportunity for individuals to use technical controls,such as encryption, to protect the confidentiality and integrity of personally identifiableinformation.  Both acknowledged that individuals should be able to remain anonymous as

they conduct some online activities.  The importance of educating the public about the
privacy implications of online activities was highlighted in the codes developed by the IITFand the NIIAC.  Although these early online privacy study commissions advocated a fairly

detailed list of fair information practices, by 2000 the various iterations of fair informationpractices for online privacy discussed by the Federal Trade Commission and others largelyfocus on four: notice, choice, access, and security.  Efforts to articulate more clearly the

essence of information privacy were not limited to the United States.  Indeed, the most
comprehensive of these codes of fair information practices is the one crafted by the Organi-zation for Economic Cooperation and Development (OECD) in 1980.  The OECD code em-

phasized eight principles: collection limitation, data quality, purpose specification, use limi-tation, security safeguards, openness, individual participation, and accountability.42Different countries have adopted these principles to varying extents.  Canada, for ex-
ample, has developed a national privacy code, the Model Code for the Protection of Per-sonal Information.  This code was developed through a consensus process that included
representation from CanadaÕs Direct Marketing Association.  More information is availableonline at <http://www.csa.ca/standards/privacy/>.43Other problems with the effectiveness of notices are illustrated by experience with theFinancial Services Modernization Act of 1999 (commonly referred to as the Gramm-Leach-
Bliley Act), discussed in more detail in Chapter 6, which requires financial institutions togive notice to customers regarding the sharing of personal information with a third party.Financial institutions have complained about the expense incurred in sending notices.  Con-

sumers have complained that notices are incomprehensible and unhelpful.  See Mark
Hochhauser, Lost in the Fine Print: Readability of Financial Privacy Notices, July 2001.  Avail-
able online at <http://www.privacyrights.org/ar/GLB-Reading.htm>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS73TABLE 3.1Fair Information Principles and Practices
PrinciplePractice/Meaning
Collection limitationCollect the minimum amount of
information that is needed for therelationship or transaction at issueÑÑBy lawful and fair means.ÑWith the knowledge and consent ofthe individual.Data qualityInformation should be relevant, accurate,
timely, and complete.Purpose specificationUse of data should be specified at the
time that data are collected.Use limitation (restriction on secondaryData should only be used for the specific
uses)purpose for which they are collected and
for which the individual understands
they will be used, except under twoconditions:ÑWith the prior consent of theindividual, andÑWith the appropriate legal authority.SecurityThe integrity of the information and the
system should be maintained to ensureagainst loss, destruction, unauthorizedaccess, modification, unauthorized use,
or disclosure.Openness/noticeThere should be no secret data systems.
People should be able to ascertain theexistence of data systems and theirpurposes and uses.Individual participationAn individual has rights to
ÑKnow if he or she is a subject of asystem,ÑAccess information about him- orherself,ÑChallenge the quality of thatinformation, andÑCorrect and amend that information.AccountabilityThe organization collecting and using
information can be held responsible for
abiding by these principles through:ÑEnforcement and/orÑRedress.xxxWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.74WHO GOES THERE?Federal agencies comply with notice provisions of the Privacy Act of1974 by publishing requests for comments in the Federal Register when
they plan to create Òsystems of recordsÓÑthose information systems thatcontain personally identifiable information such as a name or Social Secu-
rity number. Few individuals read the Federal Register to see whether a
federal agency that maintains data on them in a system of record has
announced in a routine use notice changes in the way that the agency
intends to use those data.The concept of Òconsent,Ó or the less stringent 
Òchoice,Ó is a more
complex idea.  In theory, individuals are to be given some power or

control over how personally identifiable information about them is used.
In practice, the primary question is whether such control comes from
giving the individual the opportunity to opt in by giving prior permission
or the opportunity to opt out by allowing them to say no.  Privacy advo-

cates argue that Òopt inÓ is more consistent with the idea of consent, while
Òopt outÓ erroneously assumes that individuals tacitly give consent to
secondary uses.  Organizations argue that 
Òopt outÓ gives individuals
adequate opportunity to choose and does not overburden consumers or
industry.Recognizing the complexity and importance of access and security inthe online environment, the FTC convened an advisory committee to ex-
amine and advise on these subjects.44  With regard to access, the commit-
tee addressed four questions: (1) What is the meaning of access (merely
view or view and modify)? (2) Access to what?  (3) Who provides access?

and (4) How easy should access be?  The Advisory Committee on Online
Access and Security was unable to agree on a clear recommendation and
instead presented a range of access options.  In part, the committee recog-

nized that the dilemmas presented by the need to authenticate for access
purposes complicated access options and necessitated an evaluation of
the particular circumstances.The Advisory Committee on Online Access and Security recognizedthat security likewise is contextual, that costs and inconveniences affect
the level of security that administrators are willing to set and users are
willing to bear, and that the establishment of a security system should
begin with a risk assessment.  The committee outlined five options for

achieving security and recommended a solution including these three
principles: (1) every Web site should have a security program, (2) the44Federal Trade Commission (FTC). Final Report of the FTC Advisory Committee on OnlineAccess and Security. Washington, D.C., May 15, 2000. Available online at <http://www.ftc.gov/acoas/papers/finalreport.htm>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS75elements of the security program should be specified, and (3) the securityprogram should be appropriate to the circumstances.However, in the absence of the comprehensive adoption of statutesbased on fair information practices, much personal information remains
vulnerable to misuse, abuse (including the potential for identity theft),
and unfettered government access.  This situation poses serious privacy

threats to authentication information held by private sector entities.PRIVACY OF COMMUNICATIONSConcern about the privacy of individual communication has grownas society comes to depend more and more on electronic and networked
communications.  While the privacy of communications is only one as-

pect of the larger privacy policy framework, the evolution of the law of
communications privacy provides useful insights for policy makers and
system designers considering the privacy implications of authentication
systems.  The development of communications privacy law illustrates

that as technology reaches farther and farther into sensitive, protected
areas of human activity, a legal response can guarantee that this sensitive
information, which may not have previously been revealed, accessible, or
even in existence, will be protected in accord with basic constitutional
values.  At the same time, lawmakers and courts have recognized that

along with protecting the privacy of communications, laws also need to
provide for law enforcement access to confidential information where
necessary, consistent with basic Fourth Amendment protections.  Debates

over the appropriate balance between individual privacy interests and
law enforcement power revolve around the proposition that increasingly
powerful technologies demand increasingly strong privacy protections.
While the privacy issues raised by authentication technologies encom-
pass more than the communications inside or associated with those au-
thentication systems, new privacy protections are indeed needed for these
emerging technologies.Communications privacy law has generally governed law enforce-ment access to interpersonal communications (wiretapping), but it also
covers access by unauthorized private third parties. The expressive na-
ture of communications has resulted in legislative and judicial recogni-
tion of the sensitivity of personal communications through special proce-
dures controlling law enforcement access to communications.  In general,

advances in communications technology precipitate debates about the
appropriate level of Fourth Amendment protection. These debates reveal
an evolving notion of what information is sensitive and thus deserving of
protection from both governmental and commercial intrusion.As advanced communications technologies such as the InternetWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.76WHO GOES THERE?(e-mail, the World Wide Web, and so on), wireless phones, and otherdevices complement and in some cases replace telephone communica-
tions, the United States as a nation has generally recognized the need to
create privacy protections similar to those established for voice communi-
cations by the Supreme Court.45 From telegraph to telephone, wireline
phone to cell phone, e-mail to the World Wide Web, users of the major
new communication technologies have acquired privacy protections for
their communications. Thus far in the history of electronic communica-
tions, policy makers, commercial providers, and even those in the field of
law enforcement have come to agree that new technologies demand pri-
vacy protections,46 both out of faithfulness to basic constitutional values
and to assure the commercial viability and acceptance of the latest com-
munications technologies.  However, the scope of such protections has

consistently fallen short of the standards, based on the Fourth Amend-
ment, that govern real-time voice communications.  At the same time, the

range of information and communications flowing through these new
communications technologies has dramatically increased.  Thus today,

many kinds of information are potentially accessible under the secret
searches of wiretap law.  In addition, in light of recent events, there is an

expanding sense of what government may legitimately need to access to
meet national security and law enforcement requirements.Most recently, Congress has struggled with the question of the pro-tection of online transactional records such as logs tracking the Web pages
viewed by individual users and records of electronic mail messages sent
and received. Though law enforcement argued that these logs revealed
little information and should be easily available for any investigative pur-
pose at all, the legislature found that this information is sufficiently sensi-
tive to warrant extra protection.Electronic communications have required the expansion of privacyprotection commensurate with new technology capabilities (see Box 3.1).
The Electronic Communications Privacy Act (ECPA) of 1986 was sup-
ported by a coalition of businesses and privacy advocates who under-
stood that protections similar to those for first-class mail were a necessary
precursor to business and individual adoption of e-mail as a communica-45See Katz v. United States
, 389 U.S. 347 (1967); Available online at <http://laws.findlaw.com/us/389/347.html>.46In Kyllo v. United States
, Justice Scalia, writing for the majority, noted ÒWe think thatobtaining by sense-enhancing technology any information regarding the interior of thehome that could not otherwise have been obtained without physical Ôintrusion into a consti-tutionally protected areaÕ (Silverman, 365 U.S., at 512) constitutes a searchÑat least where(as here) the technology in question is not in general public useÓ; see <http://supct.law.cornell.edu/supct/html/99-8508.ZO.html>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS77tions tool for sensitive information.47  Similarly, the privacy amendments
to ECPA in 1994 creating a higher level of protection for transactional
information generated in Web-based interactions recognized that this in-
formation was more sensitive than the numbers dialed on a phone, and
consequently that public use of the Web would be aided by creating more
stringent protections against access.BOX 3.1Expansion of Fourth Amendment Protection andTechnological CapabilitiesIn their early stages, important new communications technologies such asthe telephone and electronic mail were not accorded the privacy protections thatare now taken for granted. In each case, the application of Fourth Amendmentprotections was unsettled, so the legislative branch had to step in to provide some
level of protection. When the telephone first came into use, law enforcement wasable to conduct unfettered surveillance of private conversations because the U.S.Supreme Court ruled telephone calls to be beyond the protection of the Fourth
Amendment. Though telephone callers never invited law enforcement officers tolisten in on their calls, the Court held that Fourth Amendment protections onlyapplied to intrusions on oneÕs property (either physical or real). As conversations
had no property interest attached to them, they merited no privacy protection. Lat-er, however, the Supreme Court reversed itself and declared that Òthe FourthAmendment protects people, not places.Ó1Early electronic mail systems also lacked clear legal protection. To some, thefact that an e-mail message passed through the hands of third parties (Internetservice providers or other operators of electronic mail systems) meant that the
sender and the recipient had forfeited their privacy rights by handing over themessage to others. At the urging of the nascent electronic mail industry and priva-cy advocates, however, Congress extended privacy protection to e-mail and set
clear rules governing law enforcement access.  These rules are now in some flux
owing to the passage of the USA PATRIOT Act of 2001 and uncertainty about howit will be applied and enforced.1See Katz v. United States
, 389 U.S. 347 (1967), which states ÒBecause the Fourth Amend-ment protects people rather than places, its reach cannot turn on the presence or absence of
a physical intrusion into any given enclosure. The ÔtrespassÕ doctrine of Olmstead 
v. United
States, 277 U.S. 438, and Goldman v. United States, 316 U.S. 129, is no longer controlling.
ÓAvailable online at <http://laws.findlaw.com/us/389/347.html>.47See Electronic Communications Privacy Act of 1986 (PL 99-508).  Available online at
<http://www.cpsr.org/cpsr/privacy/wiretap/ecpa86.html>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.78WHO GOES THERE?CONCLUDING REMARKSAuthentication technologies, like other technical advances, renew thedebate about how much privacy protection should be provided to per-
sonal information generated in the authentication process. As with other
advances, in order to speed adoption, policy makers, industry, law en-
forcement, and privacy advocates should identify the privacy-sensitive
features of these technologies and develop appropriate protections.Finding 3.1: Authentication can affect decisional privacy, infor-mation privacy, communications privacy, and bodily integrity
privacy interests.  The broader the scope of an authentication

system, the greater its potential impact on privacy.Recommendation 3.1: Authentication systems should not in-fringe upon individual autonomy and the legal exercise of ex-
pressive activities.  Systems that facilitate the maintenance and
assertion of separate identities in separate contexts aid in this
endeavor, consistent with existing practices in which individu-
als assert distinct identities for the many different roles they
assume.  Designers and implementers of such systems should

respect informational, communications, and other privacy in-
terests as they seek to support requirements for authentication
actions.In terms of developing an actual system, and considering fair infor-mation principles and practices as described in this chapter, as well as
how authentication works in the abstract (as discussed in Chapter 2), the
following guidelines are offered for the development of authentication
systems that would protect privacy interests as much as possible.Recommendation 3.2:   When designing an authentication sys-
tem or selecting an authentication system for use, one should:¥Authenticate only for necessary, well-defined purposes;
¥Minimize the scope of the data collected;

¥Minimize the retention interval for data collected;

¥Articulate what entities will have access to the collected data;

¥Articulate what kinds of access to and use of the data will be
allowed;¥Minimize the intrusiveness of the process;

¥Overtly involve the individual to be authenticated in the
process;Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.PRIVACY CHALLENGES IN AUTHENTICATION SYSTEMS79¥Minimize the intimacy of the data collected;¥Ensure that the use of the system is audited and that theaudit record is protected against modification and destruc-
tion; and¥Provide means for individuals to check on and correct theinformation held about them that is used for authentication.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.804Security and UsabilityPrevious chapters describe abstract notions of authentication andprivacy.  Fully understanding the implications of authentication

for privacy requires considering authentication systems as a whole.No working authentication technology exists in a vacuum.  How is the

technology deployed?  What policies are in place with respect to its use?

Which resources is the system meant to protect? What are the goals of the
system?  Understanding the technology as part of a larger system is key to

evaluating its privacy implications.  In this chapter, authentication is ex-

amined within a broader systems context.  Two important systems-level

characteristics of authentication systems are discussed: security and us-
ability.As noted previously, security is a primary reason for the deploymentof authentication systems. Security is also vital to the preservation of
privacy in that one must make use of security technology in order to
protect privacy-related information.  It is not simply the technical mecha-

nisms of security that matter but also the processes and policies govern-
ing who has access (and how) to sensitive data. It is therefore essential to
understand the security requirements and properties of both the authen-
tication system itself and the resources it is protecting.  To that end, a

discussion of threat models and how to think about security risks is pre-
sented.  The people using these systems are an important component of

them, and their needs and behaviors must be taken into account.  Accord-

ingly, the committee develops the notion of user-centered design, with
particular emphasis on the authentication context.  Finally, it remarks on
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY81and makes recommendations about secondary and unplanned-for uses ofauthentication systems.THREAT MODELSAs noted previously, a significant motivator for authentication tech-nologies is increased system security.  Ultimately, understanding the con-

text in which the system will be deployed and the threats likely to be
faced will enable determining whether authorization, accountability, and/
or identification (as well as authentication) will be required.  While au-

thentication technologies are generally used to increase system security,
security is not a binary property of a system.1  A system is secure, or
insecure, only relative to a perceived threat.2 To understand this concise
characterization, some definitions are required.ThreatsThe terms ÒattackÓ and ÒthreatÓ are often used interchangeably insecurity discussions, and in informal discussions this is an acceptable
practice.  However, in this report the committee adopts more precise

definitions for these terms and other, related security terms to facilitate an
understanding of the security issues related to authentication systems:¥A 
vulnerability is a security-relevant flaw in a system. Vulnerabili-
ties arise as a result of hardware or software bugs or procedural, person-
nel, or physical security problems.¥An 
attack is a means of exploiting a vulnerability. Attacks may be
1As part of overall system security, the security of the authentication component itself(separate from broader system security issues) is crucial, because without it, a primarypurpose of the authentication process is undermined.  For any authentication technology,
the possible vulnerabilities, present and future, must be evaluated.  Apart from flaws par-

ticular to a given method, there are several questions that can be asked of any scheme, suchas whether the authentication credentials can be shared (and if so, whether the originalowner still retains the ability to use them), whether the credentials can be forged, and
which sorts of errors can be due to human limitations (such as forgetting a secret or losing atoken).  Another question that bears on the security of the authentication system is whethera secret value is transmitted to or stored by the verifier.  In some sense, a proper under-

standing of the vulnerabilities is even more important than the vulnerabilities themselves.Do system security administrators understand the failure modes? Do users understand theweak points?2For an in-depth discussion of computer and information security, see Computer Scienceand Telecommunications Board, National Research Council, Trust in Cyberspace, Washington,D.C., National Academy Press, 1999. Available online at <http://cstb.org/pub_trust/>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.82WHO GOES THERE?technical, procedural, physical, and so on, corresponding to the type ofvulnerability being exploited.  Passive wiretapping, buffer overflows, and

social engineering (for example, deceiving individuals such that they re-
veal privileged information) are examples of attacks.¥An adversary is an entity (an individual or an organization) with
hostile intent.  Hackers, criminals, terrorists, and overly aggressive mar-

keters are examples of adversaries.¥A threat is a motivated, capable adversary. The adversary is moti-
vated to violate the security of a target (system) and has the ability to
mount attacks that will exploit vulnerabilities of the target.3¥A countermeasure is a security mechanism or procedure designed to
counter one or more types of attack. A countermeasure does not remove a
vulnerability but instead prevents some types of attack from effectively
exploiting one or more vulnerabilities.  Secure Sockets Layer (SSL), for

example, can be used to encrypt communication between a browser and a
server in order to counter passive wiretapping attacks that could disclose
a static password.4In practice, every system contains vulnerabilities of some sort, whenviewed in a broad context. The existence of a vulnerability does not in
itself make a system insecure. Rather, a system is insecure only in the
context of a perceived threat, because that threat is motivated and capable
of exploiting one or more vulnerabilities present in the system.  For ex-

ample, in order to exploit a vulnerability in an implementation of a user-
authentication system, an adversary might have to possess a very sophis-
ticated technical capability.  If likely adversaries do not possess that

capability, then the system may be considered adequately secure for an
intended application context.  Of course, the adversary could also bribe

one or more insiders. All vulnerabilities must be considered.To understand threats, one usually begins with a list of common ad-versaries and a discussion of their possible motivations, capabilities, and
degree of aversion to detection. The following examples illustrate this
notion:¥Hackers represent a class of adversaries who tend to be opportunis-
tic.  That is, a target often is selected because of its vulnerability rather

than for its strategic value. Hacker capabilities are primarily in the form of
attacks launched via network access, as opposed to the exploitation of3This definition is consistent with the use of the term in political and military contexts,such as references to the ÒSoviet threatÓ during the Cold War.
4Of course, this protection does not address other fundamental vulnerabilities of staticpasswords, such as ease of guessing.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY83physical or personnel vulnerabilities. Often these attacks are not stealthy,and many hackers do not seem to be especially averse to detection. Indi-
vidual hackers do not tend to possess significant resources, but groups of
them may collaborate to bring to bear significant computing resources
against a target (some distributed denial-of-service attacks are an example,
although such attacks may also be carried out by individuals). A small
number of hackers are highly skilled, and these individuals create attack
tools that are distributed to a much larger, less-skilled hacker community.
Because of the opportunistic nature of hackers and because the hacker
community is so large, all systems with network connectivity should con-
sider hackers as threats. Many hackers seem to place little value on their
time, and thus may be willing to expend considerable personal time on
what might seem a trivial target, perhaps motivated more by the desire
for bragging rights than by the value of the data accessed.¥Insiders are authorized users in some organizational context. Thus,
they have legitimate access to some set of computers and networks within
that context and usually have physical access to computers employed by
other users in that context.  The threat from insiders can arise in one of

two ways.  First, benignly intended insiders may behave inappropriately

out of either curiosity or error, causing damage.  Second, malicious insid-

ers may intend to cause damage.  In both cases, the set of things that could

be damaged or taken usually is constrained by the organizational context
in which an insider operates. Insiders are usually averse to detection,
although a disgruntled employee who is being fired may be less so. Mali-
cious insiders typically have limited resources, but their intimate knowl-
edge of systems and physical access to them give malicious insiders ad-
vantages relative to external attackers.¥Industrial spies, in contrast to hackers, select targets on the basis ofthe perceived value of some aspect of the target (for example, content),
and they are highly averse to detection. They tend to employ stealthy
online attacks to reduce the risk of detection, but they also may employ
attacks (for example, bribery) against personnel.  Because these adversar-

ies are paid to conduct attacks, their methods take personnel and materiel
costs into account.  Industrial spies may also take jobs in order to acquire
insider access (see above).¥Criminals often select targets for financial gain and thus often pre-
fer stealthy attacks that minimize the risk of detection. (An exception
might be denial-of-service attacks used to extort.) They may be willing to
exploit personnel or physical security vulnerabilities as well as to engage
in technical attacks. They may employ considerable financial resources.¥Activists launch attacks whose goal might be to generate publicity
(for example, by disrupting services) to serve a more subtle purpose (for
example, to acquire data used to embarrass the target). They are not espe-Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.84WHO GOES THERE?cially averse to detection, nor do they generally possess significant re-sources, but they may exploit ideological biases in personnel (insiders)
and may engage in physical attacks.¥Nation-state spies and terrorists typically select targets with great
care and employ very stealthy techniques to avoid detection and attribu-
tion. They may bring significant financial and technical resources to bear
against the target. They may employ a full range of attacks to exploit
physical, procedural, and personnel vulnerabilities. Even state-sponsored
espionage budgets are not infinite, so the cost of attacking a target is
balanced against the expected gains.In the context of user-authentication technologies, it is important tounderstand the threats against which the technologies are effective and
under what circumstances the technologies will fail.5  If security is com-
promised, privacy is likely to be compromised as well.  In addition, as the

rest of the report describes, even with sufficient security there still may be
threats to privacy.  Choices about which kinds of authentication systems

to deploy need to take these factors into account.Recommendation 4.1: In designing or choosing an authentica-tion system, the first step should be to articulate a threat model
in order to make an intelligent choice among competing tech-
nologies, policies, and management strategies.  The threat

model should encompass all of the threats applicable to the
system.  Among the aspects that should be considered are the

privacy implications of using the technologies.Dealing with ThreatsAssuming (perhaps unrealistically) that a serious and explicit evalua-tion of the security of a system relative to a perceived threat model has
been carried out, the next question is what to do in response:  How should

one protect the system?  The answer will depend on the potential losses

(including the potential damage to reputations) if attacks were to succeed,
as well as on the risk-management strategies adopted by those making
the decisions.  Several fundamental approaches to securing systems have
been adopted over time, and multiple approaches usually are employed
that complement one another.65For example, an authentication technology that uses a one-time password list may bevery effective against hackers using network-based attacks but ineffective against an in-sider who might have ready access to the list taped to a monitor.6The concise mantra adopted by the Department of Defense in the 1990s, ÒPrevent, de-Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY85If a vulnerability cannot be eliminated, security countermeasures areoften deployed to mitigate the risk that the vulnerability will be exploited.
Countermeasures usually thwart some specific, known class of attacks
and thus may not offer protection against new attacks that exploit the
vulnerability. Although deploying countermeasures is not as attractive as
eliminating underlying vulnerabilities, it is an essential part of the secu-
rity technology arsenal, since countermeasures can be added into an ex-
isting system to complement the security capabilities of that system.The minimization of vulnerabilities and deployment of countermea-sures generally will improve system security, but rarely will they ensure
system security in the face of a wide range of attacks. Thus, it is also
prudent to engage in monitoring in order to detect traffic patterns or
system behavior that may be indicative of an attack.  This process is

referred to as Òintrusion detectionÓ and is commonly employed in many
environments today.  Of course, it implies being able to differentiate the

behavior of intruders from that of normal authorized users.  Even if an

intrusion-detection system detects an attack only after the attack has been
successful, the detection can be useful for evaluating the extent or mode
of the attack.Responses to attacks usually take the form of applying new softwarereleases or updates, deploying new countermeasures, or adjusting intru-
sion detection systems to better monitor newly discovered vulnerabili-
ties. Rarely is it practical to engage in any form of retaliatory action against
an attacker; this is true for several reasons, including the difficulty of
locating the source of an attack and legal constraints on such activities.Once a sufficient threat analysis has been undertaken, the securityrequirements of the system should be more explicit.7  It is at this point
that decisions can be made about whether authentication is necessary andtect, respond,Ó reflects this multifaceted theme. Another commonly accepted approach to
security is captured by the phrase Òdefense in depth.Ó The notion here is that multiple,
independent security mechanisms provide greatly improved security, since all must be
circumvented in order to breach security.  Defense in depth is compatible with efforts to
Òprevent, detect, respond,Ó though it also can be pursued independently.   Preventative
measures attempt to prevent attacks from succeeding. For example, it is obviously desirable
to remove vulnerabilities from a system whenever that proves feasible. The security patches(security-relevant software updates) issued by vendors are an example of this process. Ap-plying security patches generally is viewed as a preventative measure since it prevents later
exploitation of a vulnerability. But in many cases a patch is remedialÑthat is, the patch isdistributed after an attacker has already exploited a vulnerability.7One of the reasons that e-commerce has blossomed is that banks assume the risk of badcredit card transactions (perhaps passing some portion of this risk on to merchants).  Risk
analysis and apportionment must therefore be part of a careful threat and security require-ment analysis.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.86WHO GOES THERE?what requirements the authentication subsystem would need to satisfy.The overall security requirements of the system will determine whether
an authentication component is required, and, as indicated previously,
the authentication system itself will have its own security needs.  Finally,
the perceptions and actualities of ease of use and usefulness (in this case,
the prevention of perceived harm) play important roles in how secure
systems of countermeasures are in practice.   This is discussed further in

the next section.AUTHENTICATION AND PEOPLEÑUSER-CENTERED DESIGNAuthentication and privacy schemes will fail if they do not incorpo-rate knowledge of human strengths and limits.  People cannot remember
all of their passwords, so they write them down (typically under the
keyboard or on their handheld computers) or make them easy to remem-
ber (using petsÕ names or their own birth dates) and consequently easy to
guess.  People do not change passwords, because it makes them too hard
to remember.  Furthermore, people do not want to delete a file containing

pages and pages of characters such as Ò XØØØØØØØ  ı:I€ØØØØ  H   
.ÓIts purpose is unclear, and there is a fear of disabling some application on
the computer.  Similarly, although much private information is stored in

the cookies on computersÑinformation about personal identificationnumbers, preferences, and time-stamped indicators of which Web sites
were visitedÑpeople do not remove these after every transaction, even incases where the cookies provide no real benefit to the user.8  Two possible
reasons for this are that since there are no visible traces of their existence
and use, people forget that the cookies are there, and if they do remember,
it is too hard to find and delete them. Similarly, people find it difficult to
read the Òfine printÓ of privacy notices that companies are now required
to send out, both because the print itself may be too small and because the
notices are usually written in obfuscated style.9Therefore, in order to work effectively, authentication and privacyschemes need to be designed with the same consideration of human
strengths and limits as for any other technology, maybe more.  To do this,

one can borrow heavily from the practice of user-centered design and8Rarely are there negative consequences from cookie contents, so people continue totrust the system. This potentially misplaced trust leads to other vulnerabilities and raises aphilosophical problem. Like civil liberties, society gains from an atmosphere of trust.  Is thedesire to preserve that atmosphere enough to accept a few vulnerabilities, or is it so impor-
tant to prevent the vulnerabilities that the atmosphere of trust will be sacrificed?9The problem of providing effective privacy notice is appreciated by consumer protec-tion experts.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY87from some additional aspects of psychology.  The following section out-
lines the practice of user-centered design and discusses how it applies to
the design of authentication and privacy schemes.  Some other facts about

human behavior that are relevant to these issues are noted.  A suggested

design and evaluation process is presented for considering various alter-
natives for authentication and privacy that are proposed in this report.Lessons from User-Centered DesignUser-centered design puts the userÕs needs and capabilities at theforefront.  Much of today
Õs technology is driven by the pursuit of a tech-nology solution, without regard to whether the technology provides the
full functionality that the user needs in the context of its use or whether it
is learnable, usable, and pleasant. After the failure of a number of prod-
ucts and systemsÑnotably the Apple Newton, the Mars orbiter, and theFlorida 2000 election ballotÑmore and more software engineers are put-ting Òuser-experience engineers,Ó or people with human factors experi-
ence, on their teams.10   User-experience engineers use various methods
to understand the userÕs needs and to design an interface that is easy tolearn, usable, and enjoyable.   There is evidence that the software is mea-

surably better and the products more successful in the marketplace when
these methods are used.User-experience engineers rely on a number of design principles thatare based on the known strengths and limits of human cognition.  Cogni-

tive psychologists have known for years, for example, that human short-
term memory is limited and fragile, that learning takes time, and that
people make moment-by-moment decisions about cost or effort and per-
ceived benefit.  However, people are also remarkably perceptive, in both

the literal and conceptual sense.  Humans have great powers of under-

standing visual input, particularly if it follows certain principles of group-

ing and use of color and dimension.  People make errors by misremem-
10Extracted from several articles by user-experience expert Donald Norman. Normanprovides the example of the Soviet UnionÕs Phobos 1 satellite in a 1990 article in Communi-cations of the ACM. The orbiter was lost on its way to Mars not long after launch. Later, itwas found that the cause of the error was an operator who sent a sequence of digitalcommands to the satellite but mistyped a single character. Unfortunately, this error trig-gered a test sequence stored in the read-only memory that was supposed to be executed
only when the spacecraft was on the ground. The wrong sequence set the satellite in rota-tion, and it was no longer possible to resume control over it; it was lost in space (D.A.Norman, ÒCommentary: Human Error and the Design of Computer Systems,Ó Communica-tions of the ACM 33(1990): 4-7.) Norman commented on the 2000 Florida ballot in an inter-
view with  Kenneth Chang of the 
New York Times (ÒFrom Ballots to Cockpits, Questions ofDesign,Ó New York Times, January 23, 2001).
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.88WHO GOES THERE?bering, by not attending to signals, and by simple mistaken acts such ashitting a key adjacent to the one intended.On the basis of knowledge accumulated from decades of research incognitive psychology, user-experience engineers have developed a core
set of design principles, a common set of which are shown in Table 4.1.
How these lessons apply to systems employing authentication and pri-
vacy schemes is explored next.TABLE 4.1Key Design Principles in User-Centered Design
PrinciplePractices
Build on what the user knows.Do not tax the memory of users by
having them learn too many new things.Use their words.  Build the system
interaction with a story or metaphor in
mind that the user will easilyunderstand.Simplify.Do not add features that aren
Õt useful.Package large feature sets in terms ofclusters of things for a particular task,
not jumbled together to cover allpossibilities.Allow users to do things in the order inDo not make users do things in the order
which they think of them.that the computer needs if it is different
from what users would do naturally.
(The computer can keep track of thingsbetter than the user can.)Display information in clustered,Place things that go together
meaningful visual displays.conceptually near each other in the
display and in an order that fits what
users are trying to achieve.Design for errors.People make errors.  Design so that
errors are not costly to the user or
difficult to correct.  Allow each action to
be undone once the user has seen theconsequence of the error.Pace the interaction so that the user is inThe user can be slow, but the system
control.shouldn
Õt be.  Slow reaction to a user
action discourages acceptance of atechnology.xxxWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY89Issues of Limited MemoryThe issue of forgetting passwords is clearly an issue of limitedmemory and of the first two principles shown in Table 4.1.  Learning

and recall are hard and take time.  Because of this limitation, people

will augment their memories by writing things down.  Recently, new

services have been offered on the Internet and in some operating sys-
tems to alleviate the problem that many people have with many differ-
ent passwordsÑfor example, one for ordering tickets online, one foraccess to digital libraries, and others for getting financial information
online.  This type of service will build for a person a single portal to

which he or she signs on with one password.  It stores all of that

personÕs passwords and automatically accesses the services for whichhe or she has signed up.  Many of the user
Õs resources are conse-quently subject to theft with the loss of a single passwordÑthe oneused to access such a resource.SimplicityThere are systems that have the desired functionality of allowingthe user fine-grained control but that tend to be far too complicated to
use.  Some e-mail systems, for example, allow the user to formulate

rules to sort incoming e-mail into folders for examination later in some
order of priority.  There are two major problems with these systems.

First, people find that the rules that they have to write are maddeningly
difficult because they must specify their preferences in very specific
terms having to do with searchable fields in e-mail.  For example, when

a correspondent has more than one e-mail account, the rule writer must
remember not just a particular personÕs name, but all of the characterstrings that might appear in that personÕs e-mail ÒFromÓ line.  Second, if
the rules designate things to be automatically deleted, the user may
have no trace of unintended consequences.  Similar effects follow when
users attempt to program their telephones to deny calls from
telemarketers.  They must specify their rules not by naming people from

whom they want to hear but by designating telephone numbers.  The

system was designed without regard for the difficulty that the user will
incur at the interface.Technologies that are difficult to learn will exclude people who arenot willing to or cannot expend the effort needed to learn them.  Of note,

a recent study of peopleÕs understanding of Pretty Good Privacy (PGP), acommunications encryption technology, showed that many users failed
outright to understand what they were supposed to do and made cata-
strophic errors, such as sending the private key instead of the publicWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.90WHO GOES THERE?key.11  However, criticisms of user-interface problems are usually more
accurately applied to an implementation than to a building-block technol-
ogy per se.Making Things VisibleAssociated with peopleÕs ability to understand complex visual dis-plays is their inattention to things not visible:  ÒOut of sight, out of mind.ÓThe example of cookies earlier in this report is an instance of invisibility.
Things invisible are often not attended to.  The files in 
ÒtempÓ folders
similarly fail with regard to visibility and readability.  But, since the pos-

sible consequences of the loss of privacy are not brought to the userÕsattention on a daily basis, it can be easy to forget about their possible
harm.  The fact that people ignore things that are not visible may explain

why they claim to be concerned with privacy and then do not protect
their privacy when given the opportunity.Indirect Effects of Bad User-Centered DesignPoorly designed systems can also have indirect effects on privacy,accuracy, and control.  Many of the databases in use at large financial

companies, motor vehicle bureaus, social service agencies, and so on are
legacy systems with badly designed interfaces and poor checks on the
accuracy of data entered, resulting in errors in database records.  Many of

these errors are caused by poorly designed interfaces, poor training for
the high-turnover workforce (high turnover caused by the boredom of
such a low-level job), and low motivation.  And, since it costs an organiza-

tion money to ensure accuracy (for example, verifying by doubly entering
the data and finding mismatches, or building algorithms that will check
on accuracy and duplication, and so on), errors are accepted.  It is the

victim of an error who seems to have to bear the cost of correction.Lessons from Cognitive and Social PsychologyOther aspects of human behavior, including the following, affect thesuccess of authentication and privacy schemes:¥How people make decisions,¥The basis on which they trust other people and institutions,11A. Whitten and J.D. Tygar. ÒUsability of Security:  A Case Study.
Ó Proceedings of the 9thUSENIX Security Symposium. August 1999.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY91¥Their assumption that the physical world does not apply to thevirtual world, and¥How a personÕs behavior changes when that person is not visibleto others.These aspects of human behavior are all relevant to the design ofauthentication and privacy schemes.Decision MakingThroughout daily life, people make small decisions about whether totake actions or not.  Often such decisions are based on simple calculations

of cost and benefit.  In making decisions, people frequently overvalue

things that have immediate value and undervalue actions that may have
a long-term payoff.  And, sometimes the cost is clear and the benefit

unknown.  This tendency causes people to not make the effort to do

something that would protect their long-term interests.For example, knowing that people do not like having their privateinformation accessible to others unknown to them, some entrepreneurs
have built software called Òcookie cuttersÓ that automatically preservesthe cookies a person chooses to preserve and flushes the rest when a
Web session is over.  Unfortunately, not only are such programs gener-

ally expensive, but they also require a particular level of skill for instal-
lation and require users to specify their preferences in much the same
way as e-mail filtering programs and telephone screening devices do.
The cost is high, both in time and in the effort to understand and specify
things, and the benefits are really unclear.  It is hard to articulate the

benefit of keeping this information private, both now and in an unfore-
seen future.The fact that people often see immediate benefits but not long-termconsequences also explains why they are willing to divulge private infor-
mation for short-term freebies.  For example, some people will allow their

Web activity to be monitored if they are given a free e-mail account.TrustMuch of our society is based on trust.  People take actions that make
them vulnerable, believing that others will do them no harm.  They

accept payment in checks or credit cards assuming that the payer has
the resources and will pay when asked.  They follow traffic rules in

general, expecting others to do so as well.  People buy things sight-

unseen on the Web, expecting delivery of the goods and/or services as
advertised.  People believe, often wrongly, that when they share their
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.92WHO GOES THERE?e-mail address or Social Security number, this information will not be
sold to third parties who will either solicit them to buy unwanted things
or defraud them in some way (steal from their accounts).  In normal,

everyday behavior, people do not authenticate one another.  They spend

little effort and make assumptions based on commonly available data.
Furthermore, people are usually not proved wrong after doing this, so
their experience with the system encourages them to behave this way.On what information do people base their judgments with respect totrust and lying?  The best information that someone is trustworthy comes

from direct experience:  A person was trusted in the past and all behavioral

evidence is that he or she caused the trusting person no harm or, better yet,
looked after his or her interests.  The second source of information when

making such judgments is a set of physical and behavioral attributes that
suggest someone is similar to the trusting person. The chain of reasoning
goes like this: If I am trustworthy and you are like me, then you must be
trustworthy, too.  The third source is endorsements, either from someone a

person knows, such as a friend who has had direct experience with the
person or institution in question, or from some famous person who would
lose reputation if he or she lied about anotherÕs trustworthiness. The fourthsource is assessment of reputation directlyÑfor example, the reputation ofa person in a powerful position could be lost if he or she were not trustwor-
thy or an organization that has been around for a long time might not
continue to exist if it was less trustworthy.Knowing how people make decisions, some people and institutionshide behind false information.  Some individuals might buy from a Web

site that has an endorsement symbol from the Better Business Bureau,
even though there is not necessarily an immediate way to authenticate the
valid use of the symbol.  It could simply have been copied from another

site.  Some people might choose to enter credit card information into a site

that seems to be designed well and not into one that seems to be slapped
together, making the assumption that a well-designed site costs money
and could not have been afforded by a fly-by-night vendor.  Because

people do not spend the time and effort to investigate authenticity and
the shortcut attributes that they use are well known, they are left open to
fraud at many levels.1212The Federal Trade Commission is charged with consumer protection broadly and hasbeen investigating conduct on the Web.  More information on the FTC, privacy, and fair
information practices is presented in Chapter 3.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY93Assumptions from the Real World That Do Not Transferto the VirtualIn our social interaction in the physical-spatial world, some behaviorsare correlatedÑfor example, when I see you, you can see me.  When I get
closer to you, you get closer to me; when I hear you, you can hear me.  The

digital world has uncorrelated these attributes.  With Web cameras or

one-way videoconferencing, I can see you, but you canÕt see me.  A whole
host of surveillance devices allows detection without obvious detectabil-
ity.  What used to be reciprocal is now one-way.  However, people behave

as if the virtual world had the same reciprocity as the real world.  One

day, for example, as a demonstrator was explaining online video-
conferencing on a public site, she was startled when someone she could
not see spoke to her.  That person, in turn, could see her, but, because of

the camera angle, could not see that she had an audience of high-level
executives.  Thinking that she was alone, he made several lewd remarks.

The demonstrator and the executives were surprised and offended by the
encounter.  They had wrongly assumed that the only others online were

those they could see.There are a number of cases today in which the person using a serviceor technology is told the conditions (for example, that others may monitor
an interaction or that the numbers a person types in will be used for other
purposes), but they forget.  The many years that humans spend learning

physical reality are hard to dismiss when one is merely told that things
might be otherwise.Accountability  A related issue is accountability.  There is mounting evi-
dence that when people are anonymous or hidden, they will behave in
ways that are very different from normal.  For example, when feedback

for a product or service is anonymous, people are more likely to say
negative things, knowing that they are not held accountable for their
behavior.  They can get away with more.  A similar effect comes into play

in e-mail flaming.  When the recipient is not visible to the writer of the e-

mail, the writer tends to say more emotionally charged things, things that
he or she would soften if the recipient were visible.  There is evidence

from experiments on trust that if people cannot be seen by others, they
will behave in a more self-serving, as opposed to a cooperative and trust-
ing way.Actions  There are at least two approaches to accommodating the knownlimits of human behavior when considering authentication and privacy
schemes: designing to fit those known limits and training people to be
cautious when caution is warranted.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.94WHO GOES THERE?Finding 4.1:  People either do not use systems that are not de-
signed with human limitations in mind or they make errors in
using them; these errors can compromise privacy.Recommendation 4.2: User-centered design methods should beintegral to the development of authentication schemes and pri-
vacy policies.Training, Public Education Campaigns, and RegulationsIt is unlikely that technologies and the policies associated with themwill be transparent to everyone.  People have a hard time changing expec-

tations and assumptions in a world that is invisible to them.  Conse-

quently, some protective measures may have to be put in place to make
up for these human shortcomings.  In many cases, it is not apparent to the

user what information is being given to the verifier.  Most users do not

know what information is on magnetic-stripe cards, smart cards, and bar-
coded cards.  Printed information on the cards is more apparent to the

user, but information learned by way of back-end processes (for example,
a check of a credit report) can be invasive of privacy, yet invisible to the
user.  As for protective measures at a micro level, Web sites could have

visible statements of the sort heard in many recorded messages on cus-
tomer service hotlinesÑnamely, that the conversation (site use) may bemonitored to ensure quality.  Similarly, small explanatory or warning

signs might appear when one moves the mouse pointer over a field that
asks for information from the user.  More broadly, a public education

campaign may be necessary to keep people from unknowingly making
themselves vulnerable, just as people are warned of new scams.  The goal

of such education should not be to cause people to mistrust others but to
train them to understand the nature of information systems and how they
can and should protect themselves.Ultimately, most individuals do not understand the privacy andsecurity aspects of the authentication systems that they are required to
use in interactions with commercial and government organizations. As
a result, they may behave in a way that compromises their privacy and/
or undermines the security of the authentication systems.  To remedy

these problems, system interfaces should be developed that reveal the
information collected and how it is used.  Research is needed to explore

how to do this effectively.  In addition, as part of the deployment of any

new system that includes the collection of privacy-sensitive data, indi-
viduals should be educated about the privacy and security aspects of
the authentication systems that they use, including the risks associated
with the systems.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY95FACTORS BEHIND THE TECHNOLOGY CHOICEA crucial factor that will encourage or discourage the use of anyauthentication technology is ease of deployment.  A scheme that relies on

something that users already have (or already ÒareÓ) is easier to deploythan one that requires shipping (and perhaps installing) new equipment.
Smart cards are relatively hard to deploy, however, since few people have
the smart card readers and associated software on their computers.  In

addition, most card issuers insist on owning their own cards, which is
why cards that could technically ÒshareÓ issuers (for example, a payment
card that is also an airline affinity-program card) have not been successful
in the market.  With respect to possession of the correct hardware, how-

ever, most computers sold in the past several years have Universal Serial
Bus (USB) ports; a USB-based token that functioned like a smart card
might require less effort to deploy, although the software barrier would
still exist.  This observation is not an endorsement of USB tokens but

rather points out how many factors, cost among them, may inhibit or
facilitate the deployment of hardware-based authentication technologies.The building blocks from which different authentication systems areconstructed may be used to protect or invade privacy.  Among the pri-

vacy issues that arise are what data are revealed to the issuer upon initial
interaction with the system and what data are created and stored at that
time, as well as when  authentication events occur.  A system in which

personal data are retained after an authentication transaction is more
intrusive of privacy than one in which no data are collected or retained.
One of the most crucial issues with respect to privacy and authentication
systems, as discussed in Chapter 2, is linkage:  Can the results of an au-
thentication process be linked to other sets of data?  If the same individual

uses cash for two activities, those activities are not linkable through the
cash that was used; however, two uses of a stored-value card are linkable
even if the card itself was purchased anonymously.Cost is another factor in deciding which authentication technologiesto deploy.  Authentication can be expensive.  There may be hardware,

software, and procedural-development costs for the presenter, the veri-
fier, and the issuer.  Integrating authentication procedures into existing

systems and procedures may be expensive as well.  Data collection and

verification, both for initial registration and for recovery from lost creden-
tials, can impose additional overhead.  After deployment, authentication

systems have ongoing costs.  New users must be added to the system and

old users deleted.  Hardware and software require maintenance and up-

grades.  Procedural updates and user and administrator training and

education are important (and costly) too; many systems fail not for tech-
nical reasons, but because someone did not follow proper procedures.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.96WHO GOES THERE?That, in turn, raises the issue of skills:  What does it cost to obtain the
services of enough people with appropriate abilities to deploy and oper-
ate an authentication system?  Finally, costs have to be apportioned.  Who

pays for all of this?  What is the balance of costs between the user, the

issuer, the verifier, and any back-end systems?Balanced against the costs of using an authentication system are theperceived and actual gains from deploying the system.  Since vulnerabili-

ties are hidden and threats are generally not observable except to those
who have done ample analysis of the particular system and its context,
people using the system may underestimate the benefit of risk mitigation.
This can result in inappropriate choices of authentication systems or in
inappropriate usage models that defeat the purpose of the system.  For

example, a recent study found that many users did not adhere to the rules
about not making their passwords guessable, or about keeping them se-
cret and changing them often.  People had a number of excuses for ignor-

ing such rules:  they were not the kind of people who keep secrets, they

shared passwords because doing so showed how they trusted other
people, they thought they were not targets because they had nothing of
value, and so on.13Other factors besides cost and security have a bearing on technologychoices.  Not all authentication systems work for all population sizes.

Some are too expensive on a per-user basis to work with large numbers of
users; others have too high an up-front cost to be suitable for small popu-
lations.  Furthermore, all systems have error rates encompassing false

positives and/or false negatives.  The likelihood and cost of each type of

error, along with a priori estimates of the number of potential imposters,
must be assessed to determine what trade-offs should be made.14  In
addition, the consequences of privacy invasions represent costs that may
be hard to monetize (absent actual damages) and may be incurred by
someone other than the system owner.Finally, an authentication system must be matched to the needs of thecontext in which it is employed, keeping in mind that authentication may
be excessive if simple authorization will suffice.  Notwithstanding that

stronger authentication is often suggested as an initial step to improve
security, the issues are often much subtler.  Deciding whether and how to

employ authentication systems (or any other security technology) requires13D. Weirich and  M.A. Sasse. 
ÒPersuasive Password Security.Ó  Proceedings of CHI 2001ACM Conference on Human Factors in Computing Systems. Seattle, Wash.,  April 2001.
14Creation of exception-handling procedures for dealing with incorrect decisions opensup additional vulnerabilities for the system, as imposters might claim to have been falselyrejected and request handling as an exception.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY97careful thought and decisions about what types of risk-management strat-egies are acceptable.  Avoiding expedient responses and knee-jerk choices

is critical, both for security and for privacy protection.  This complexity,

the multitude of contexts in which authentication systems could be de-
ployed, and the ultimate need for someone to make policy decisions about
security and privacy requirements are why simple cost-benefit analyses
are unlikely to be effective in guiding the needed choices.A final factor that must be considered when deciding how to pro-ceedÑnamely, the recognition that authentication systems must be ad-equate to protect the resources they are guarding against the perceived
threats, while at the same time remaining simple enough for administra-
tors and others to use.  This important point is often overlooked when

developing technologies.  Authentication systems will ultimately be used

by people, soÑas described in the previous sectionÑtheir ease of use andunderstandability will have a major impact on their effectiveness.SYSTEMS AND SECONDARY USEUnderstanding only the underlying technologies is insufficient forappreciating the ramifications of authentication systems.  It is important

to know how the systems will work in context to determine their security
and privacy implications.  As discussed previously, some of the gravest

privacy violations come about because of the inappropriate linking of
data within or across systems.  This can happen because the same identi-

fier is used in multiple systems, because efforts were made to correlate
data that has had the identification information removed, or for other
reasons.  Once data have been collected about an individual, dossiers

(anonymous or identified) can be created.  The creation of dossiers

coupled with an authentication system may pose a privacy risk.  Avoid-

ing this risk requires awareness of the broader context in which the au-
thentication systems and the data associated with authentication events
will be used.Finding 4.2: The existence of dossiers magnifies the privacyrisks of authentication systems that come along later and retro-
actively link to or use dossiers.  Even a so-called de-identified

dossier constitutes a privacy risk, in that identities often can be
reconstructed from de-identified data.Finding 4.3: The use of a single or small number of identifiersacross multiple systems facilitates record linkage.  Accordingly,

if a single identifier is relied on across multiple institutions, its
fraudulent or inappropriate use (and subsequent recovery ac-Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.98WHO GOES THERE?tions) could have far greater ramifications than if it is used inonly a single system.Recommendation 4.3: A guiding principle in the design or se-lection of authentication technologies should be to minimize
the linking of user information across systems unless the ex-
press purpose of the system is to provide such linkage.In addition to dossier creation through planned-for uses of authenti-cation systems, secondary use and unplanned-for uses increase the risk of
privacy violations.  Unintended uses of a given technology or information

system can always have inadvertent side effects, and there are numerous
examples in the literature of a system meeting its design specifications
but failing in the field because it was used in ways not anticipated by its
designers. In the realm of authentication technology and individual pri-
vacy, unplanned-for secondary uses can have grave consequences for
users.  Arguably, much identity theft is accomplished through secondary
uses of authentication or identification data.  See Box 4.1 for a more in-

depth discussion of identity theft.An authentication system could be designed and deployed in a se-cure, privacy-sensitive fashion.  However, if some aspect of the system

were to be used in ways not originally intended, both security and any
privacy protection available could be at risk. The driverÕs license is acanonical example of inappropriate secondary use.  Its primary function

is to identify those authorized to drive motor vehicles.  It is quite unlikely

that the designers of the original processes by which driverÕs licenses areissued anticipated they would be used as a security document needed to
board a commercial airliner or drink in bars.Most information systems, including authentication systems, do notexplicitly guard against secondary uses, although occasionally there are
contractual relationships that limit secondary use (such as credit card
agreements).  In some cases, the credential presented may be used for

additional verification purposes in contexts unrelated to the original pur-
pose (such as with a driverÕs license or when an e-commerce Web sitegives permission for the cookies that users have allowed it to place on
their hard drives to be used by other entities).  In other instances, the data

collected prior to, during, or subsequent to authentication may be used in
ways that have little to do with the authentication step itself.  A simple

example would be when information used to register at a Web site in
order to access content that is later used for marketing purposes by that
Web site.  A more insidious form of unplanned-for usage would involve a

technology designed for a certain security context, user population, and
so on that is later (intentionally or unintentionally) used in a new contextWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY99BOX 4.1Identity TheftIdentity theft occurs when someone usurps a portion of another personÕs per-sonal identifying information in order to pose as that person.  The information

usually includes some combination of name, address, Social Security number,motherÕs maiden name, password, credit card number, date of birth, driverÕs li-cense number, and employer.  With this information the ÒthiefÓ can open new ac-

counts, order products, rent apartments, take out a mortgage, and/or borrow mon-ey, all under the identity of the first party.  Identity theft goes beyond the theft of a
credit card to the misappropriation of a personÕs very identity.  It constitutes fraud.
From the victimÕs perspective, there is no easy way to develop an audit trail thatproves who actually made the transaction.  Was it the person who owns or is
authorized to use the account, or was it the person who appropriated the account?
Note that a significant part of the problem is that nonsecret data are used by manyentities for authentication purposes.  An additional problem is the difficulty in re-
voking authenticators and identifiers when they are misused.  In essence, the level

of confidence required, in the vocabulary introduced in Chapter 1 of this report, isnot high enough.  As Chapter 7 describes, the choice of identifier and authenticator
is crucial for privacy protection and better security.Reports of identity theft have increased over time.  In March 2002, the Gen-
eral Accounting Office (GAO), acknowledging that there are no comprehensivestatistics on identity theft, reviewed data from a number of sourcesÑincluding con-
sumer reporting agencies, the Federal Trade Commission (FTC), the Social Secu-rity Administration (SSA), and federal law enforcementÑall of which indicated thatthe prevalence of identity theft was growing.1  Victims of identity theft pay dearly;
in 1997, the Secret Service estimated that victims lost an aggregate $745 million.But victims also face nonfinancial hardships, including criminal records, difficultyfinding a job, and inability to get mortgages or credit.  Sallie Twentyman, a victim of

identity theft, testified before the Senate Committee on the Judiciary that the con-fusion and frustration that resulted felt like a Òfinancial cancer.Ó  In Ms. Twenty-
manÕs case, her renewal credit card was stolen before it reached her, and the thief
changed ÒherÓ address and opened more accounts in ÒherÓ name.2A survey conducted by Privacy Rights Clearinghouse and the California Pub-lic Interest Research Group found that the average victim of identity theft did not
find out that he or she was a victim until 14 months after the identity theft occurredand that it took the victim an average of 175 hours to resolve the problems thatoccurred as a result of the identity theft.3 In January 2002, the FTC announced
that identity theft was the top consumer fraud complaint in 2001, accounting for 42percent of the complaints in the Consumer Sentinel database.1General Accounting Office. Identity Theft: Prevalence and Cost Appear to Be Growing,GAO-02-363. Washington, D.C., Government Printing Office, pp. 3-5, March 2002.2Sallie Twentyman (witness), ÒIdentity Theft: Restoring Your Good Name,Ó testimony before
the Senate Committee on the Judiciary, March 20, 2002.3CALPIRG (Sacramento, Calif.) and Privacy Rights Clearinghouse (San Diego, Calif.), No-where to Turn: Victims Speak Out on IdentityTheft, May 2000.(continues)Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.100WHO GOES THERE?BOX 4.1Continued
Although identity theft existed before the Internet, there is concern that it willescalate even more in the digital world.  The Internet has given identity thieves
easier access to more sources of information.  With e-signatures and digital certif-

icates, it will be imperative to ensure that the digital representation is authentic, orassociated with the correct individual, and that the transaction can be audited.There is a dual need: to determine who the consumer is and to ensure that the
personal information of the consumer is protected.In 1998, Congress passed the Identity Theft and Assumption Deterrence Act,which legally recognized that the victims of identity theft were the individuals whose
identities were stolen and not the financial institution that lost money; made it ille-gal to steal another personÕs personal information (not necessarily documents)with the intent to commit a violation; and increased potential sentencing for viola-
tors.  The act also required the FTC to establish a national clearinghouse for iden-
tity theft complaint data and to educate consumers about how to protect them-selves.4 The FTC has held workshops on the subject.  Other government agencies
have also taken action regarding identity theft.  For example, the Treasury Depart-
ment sponsored an ID Theft Summit in March 2000.  Finally, most states have
passed laws that criminalize identity theft.In response to the human and financial costs of identity theft, a variety ofpolicy responses have been proposed.  Most recognize the necessity of a multi-
pronged effort involving the public and private sectors and employing legal and
technological tools.  The education of consumers is essential to ensure that they
take steps to minimize the possibility of identity theft and to alert them to signs ofpossible theft.  Public and private organizations can help prevent identity theft by

reducing the amount of data that is exposed, limiting the release of information thatis given at the point of service, and enhancing the security of data that are collect-ed.  Additionally, aggressive criminal investigations, prosecution, and punishment

are seen as critical.In the 107th Congress much attention was focused on identity theft.  A num-
ber of bills were introduced.  In the House, these include the Identity Theft Protec-

tion Act of 2001 (H.R. 220), the Social Security Number Protection Act of 2002(H.R. 4513), the ID Theft Loophole Closure Act (H.R. 2077), and the Protect Vic-tims of Identity Theft Act of 2001 (H.R. 3368).  In the Senate, bills include the

Restore Your Identity Act of 2001 (S. 1742), the Social Security Number MisusePrevention Act of 2001 (S. 848), and the Identity Theft Prevention Act of 2001 (S.1399).without a determination as to whether the security and privacy safe-guards still hold.Abstractly, the difficulties that arise from secondary use are primarilydue to incorrect assumptions.  Secondary uses are implicitly relying on

whatever assurances, security models, and privacy protections the origi-
nal designers and implementers were working with.  These may not align
4See the Web site <http://www.consumer.gov/idtheft>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY101well with the needs of the secondary user.  In addition, the original sys-
tem was probably designed with a particular threat model in mind.   How-

ever, that threat model may not be appropriate for secondary uses.  This

incongruity can make it difficult to respond to an attack on the primary
system, since with widespread secondary use, the universe of motiva-
tions behind the attack is much larger.  Another problem is that the data

collected for primary purposes may not be the data that are needed by
secondary uses, or, they may not be of appropriate quality or reliability.
In addition, secondary uses can facilitate information leakage from the
original system, which can cause both security and privacy problems.  As

noted in the committeeÕs first report,15 understanding and clearly articu-
lating the goals of the system is crucial and may help mitigate any prob-
lems that might arise from secondary uses.  Nonetheless, the risks of

secondary use are significant and must be considered.Finding 4.4: Current authentication technology is not generallydesigned to prevent secondary uses or mitigate their effects.  In

fact, it often facilitates secondary use without the knowledge or
consent of the individual being authenticated.Finding 4.5: Secondary uses of authentication systems, that is,uses for which the systems were not originally intended, often
lead to privacy and security problems. They can compromise
the underlying mission of the original system user by fostering
inappropriate usage models, creating security concerns for the
issuer, and generating additional costs.Recommendation 4.4:  Future authentication systems should be
designed to make secondary uses difficult, because such uses
often undermine privacy, pose a security risk, create unplanned-
for costs, and may generate public opposition to the issuer.CONCLUDING REMARKSAs is evident from this and preceding chapters, neither authentica-tion nor privacy is a simple issue.  Instead, the issues interact in complex

ways with the total system.  This is seen clearly in the case of knowledge-

based authentication, which relies on some prior history of contact and on
reasonably private and unguessable knowledge that the authentic client15Computer Science and Telecommunications Board, National Research Council. IDsÑNot That Easy: Questions About Nationwide Identity Systems. Washington, D.C., National Acad-emy Press, 2002.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.102WHO GOES THERE?will nevertheless have access to.  Seen from this perspective, password-
based authentication is popular because it demands so little up-frontÑinitial passwords can easily be chosen or assigned without any need for
history, special hardware, custom software on the clientÕs computer (ifany), and so on.  By the same token, the weaknesses of passwords
Ñguessability, the cost of recovering forgotten passwords, and so onÑstemfrom many of the same roots.Privacy has similar attributes.  Many organizations that rely on iden-tity-based authorization do so not because they wish to, but because it is
easy:  they can rely on an infrastructure that someone else has built.  This
accounts for the ubiquity of driverÕs licenses as a de facto ÒofficialÓ iden-
tification and age-authorization card:  The departments of motor vehicles

(DMVs) are paying the cost of issuing the cards; everyone else can ride
free.  Furthermore, it is precisely this overloading of function that gives

rise to privacy violations:  Many different transactions can be linked back

to the same individual.  Switching to better
Ñand privacy-protectingÑtechnologies is thus not an easy task.Finally, computer security in many organizations is not as strong as itcould be or needs to be.16  In the federal government, there are numerous
efforts to document federal agency computer security plans and practices
and some that find glaring weaknesses in these plans and practices.17Many reports over the years have described security issues, concerns, and
research challenges.18  While security and privacy are often discussed as
though they were in opposition to one another, in many ways adequate
security is a prerequisite for privacy.  If data are not well protected, they

may compromise the privacy of the individual to whom they pertain.
However, achieving information security sometimes requires the disclo-
sure of personal information (for example, by requiring authentication).
At the same time, insufficient privacy protection may mean that personal
information about others is easily discovered, calling into question the
reliability of authentication systems that depend on such information.
While this report urges that care be taken to avoid unnecessary authenti-
cation and identification (and therefore avoid unnecessary privacy risks),16See Computer Science and Telecommunications Board, National Research Council,Cybersecurity Today and Tomorrow:  Pay Now or Pay Later
, Washington, D.C., National Acad-emy Press, 2002.17See Representative Stephen HornÕs report card on federal agency computer securityefforts as one measure of the state of preparedness in this area; available online at <http://www.house.gov/reform/gmit/hearings/2000hearings/000911computersecurity/000911reportcard.htm>.  Another measure is the Office of Management and Budget
Õsscorecards, available at <http://www.whitehouse.gov/omb/budintegration/scorecards/agency_scorecards.html>.18See CSTBÕs reports on security at the Web site <http://cstb.org/topic_security/>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.SECURITY AND USABILITY103the interplay between achieving privacy protection and security in thedevelopment of information systems should be carefully considered.  Pri-

vacy and security, while often in tension, are complementary as well.
Security can protect private data, and maintaining privacy can aid in
avoiding security breaches.  Usability is a key component of this mix,

since hard-to-understand or hard-to-use systems will be prone to errors
and may drive an individual  to work around either the security mecha-

nisms or the mechanisms that would protect privacy.  Lessons learned in
trying to create secure, usable systems therefore apply when seeking to
develop systems that protect privacy.Finding 4.6: Privacy protection, like security, is very poor inmany systems, and there are inadequate incentives for system
operators and vendors to improve the quality of both.Finding 4.7: Effective privacy protection is unlikely to emergevoluntarily unless significant incentives to respect privacy
emerge to counterbalance the existing incentives to compro-
mise privacy.  The experience to date suggests that market forces

alone are unlikely to sufficiently motivate effective privacy pro-
tection.Recommendation 4.5: System designers, developers, and ven-dors should improve the usability and manageability of au-
thentication mechanisms, as well as their intrinsic security and
privacy characteristics.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.1045Authentication Technologies
This chapter describes the basic technologies used as building blocksfor authentication systems, especially those employed in computer
and network environments.  First, it describes technological choices
that determine a dimension of authentication separate from the different
kinds of authentication described in Chapter 2. Then, technological
instantiations of the three main authentication mechanisms (something
you know, have, are) are described.  Multifactor authentication is consid-

ered, and decentralized and centralized systems are compared.  Finally,

security and cost considerations for individual authentication technolo-
gies are discussed.  Throughout, this chapter also touches on the privacy

implications of specific technologies in the context of authentication sys-
tems, as appropriate.TECHNOLOGICAL FLAVORS OF AUTHENTICATIONÒIndividual authenticationÓ is defined in Chapter 1 as the process ofestablishing an understood level of confidence that an identifier refers to
a specific individual.  In an information systems context, it often is useful

to distinguish among several types or modes of authentication, both for
individuals (often referred to as ÒusersÓ) and for devices (such as comput-
ers).  This is a dimension distinct from the individual/attribute/identity

authentication types discussed in Chapter 2.  In the security literature,

these modes are often referred to as one-way as opposed to two-wayWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES105authentication, initial versus continuous authentication, and data originversus peer-entity authentication.Much individual authentication in an information system takes placein a client/server context, in which the individual user is the client (a
ÒpresenterÓ in the terminology introduced in Chapter 2) and some com-
puter is a form of server (the ÒverifierÓ).  A user is required to authenti-
cate his or her identity to a computer, usually as a prerequisite for gaining
access to resources (access control or authorization).  This is typically an

explicit one-way authentication process; that is, the user authenticates
himself or herself to the computer.  If the user is authenticating to a

computer directly (for example, when sitting at a desktop or laptop com-
puter), there is an implicit two-way authentication; the user sees the com-
puter with which he or she is interacting and presumably knows that it is
the one he or she wishes to use.1However, if the user is authenticating to a computer accessed via acommunication network, there is often no way to verify that the com-
puter at the other end of the communication path is the one that the user
is trying to contact.  The user typically relies on the communication infra-

structure operating properly and thus connecting him or her to the in-
tended computer.  This assumption may be violated by any of a number

of attacks against the communication path, starting with the computer
that the user is employing locally.  This lack of explicit, secure, two-way

authentication can subvert many types of individual authentication
mechanisms.  If a presenter provides an identifier and authenticator to the

wrong verifier, both security and privacy are adversely affected.  Thus,

two-way authentication is preferred so that a presenter can verify the
identity of the verifier to which a secret may be disclosed.Initial authentication takes place when an individual first establishesa connection of some sort to a system.  This may be a direct, very local

connection, such as logging in to a desktop or laptop computer, or it may
be a remote connection to a computer via a communication network.  In

either case, there is an assumption that future communication, for some
period of time, is taking place between the two parties who were initially
authenticated.  For a direct connection, as defined here, this assumption

usually relies on physical and procedural security measures; there is an
assumption that the user will log out when leaving the computer unat-
tended and in a place where others might access it.  This is a form of

implicit, continuous authentication.  This assumption may not always be
1Looks can be deceiving, and even visual inspection of a proximate device is not alwayssufficient to authenticate it.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.106WHO GOES THERE?valid, and sometimes users are required to reauthenticate themselves ex-plicitly to the computer periodically, to verify that they are still present.
This periodic reauthentication requirement is an explicit attempt at con-
tinuous authentication, although it is not really continuous.  Periodic

reauthentication is also burdensome for the user and thus not commonly
employed.When the connection between the user and a computer is through anetwork, there are many more opportunities for the connection to be
ÒhijackedÓÑthat is, for an attacker to inject traffic into the connection orto seize the connection from the legitimate user.  In remote-access con-

texts, it is appropriate to employ explicit measures to ensure continuous
authentication.  Typically, this continuity is effected using cryptographic

means, based on a secret (a cryptographic key) shared between a local
computer employed by the user and a remote computer being accessed
by the user, for the life of the connection.  In this latter context, the techni-

cal term for the security service being provided is Òdata origin authentica-tion.Ó  Continuous authentication is generally a result of a transition from
initial, individual authentication to data origin authentication.  It is the

source (origin) of the data sent between two systemsÑfor example, be-tween a userÕs desktop and a serverÑthat is being authenticated ratherthan the user per se.  A further technical distinction is sometimes applied.

If the authentication mechanism ensures the timeliness of the communi-
cation and thus provides protection against attacks that replay old mes-
sages, the service is referred to as Òpeer-entity authentication.ÓIndividual authentication increasingly takes place in the context ofinformation systems, and thus all of the flavors of authentication de-
scribed above are relevant to this discussion of individual authentication
technologies.BASIC TYPES OF AUTHENTICATION MECHANISMSBy the mid-1970s, three basic classes of authentication technologiesfor use with information systems had been identified.2  They are colloqui-
ally characterized as Òsomething you know, something you have, andsomething you areÓ and were discussed abstractly in Chapter 2.  This
section focuses on specific technological examples of each of these basic
classes.   In the first class are authentication technologies based on what

an individual can memorize (know).  Passwords and personal identifica-
2D.E. Raphael and J.R. Young. Automated Personal Identification. Stanford Research Insti-tute International, 1974; National Bureau of Standards. ÒEvaluation Techniques for HumanIdentification.Ó FIPSPUB-48, April 1977
.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES107tion numbers (PINs) are the canonical examples of such technology.  In
the Òsomething you haveÓ class are physical objects that are (assumed to
be) hard to forge or to alter, such as magnetic-stripe cards, smart cards,
SecurID cards, and so on.  The object is issued to an identified individual
and retained by the individual, so that possession of the object serves to
identify the individual.  In the last class are biometric authentication tech-

nologies, which measure physical and behavioral characteristics of an
individual.  Each of these classes of authentication technologies has ad-

vantages and limitations with regard to security, usability, and cost.Something You KnowSimple, password-based authentication is the most common form ofinitial, one-way authentication used in information systems.  A user re-

members a short string of characters (typically six to eight) and presents
the character string to a system for verification when requested.  The

string of characters is reused many times in authenticating to the same
system; hence the passwords are usually referred to as Òstatic.Ó This sort
of system is susceptible to many forms of attack.  Because users have

trouble choosing and remembering values with a significant number of
ÒrandomÓ bits, passwords generally are vulnerable to guessing attacks.
Unless passwords are protected (usually this means encrypted) for trans-
mission over communication paths, they are subject to interception and
subsequent use by a wiretapper.  The lack of two-way authentication

means that a user can be tricked into revealing a password if he or she
connects to an attacker instead of to the desired system.Password-based authentication is cheap to implement; it may notrequire any explicit software purchases.  It is easy for users and develop-

ers to understand, so training costs are low.  But, on a life-cycle basis,

passwords are expensive for an organization to administer, largely be-
cause of the costs of help-desk support for users who forget passwords.
Users find passwords difficult to manage as they deal with a growing
number of systems that require them.  This leads to password reuse (that
is, using the same password for multiple systems) and insecure storage of
passwords (for example, in unencrypted files on computers).  Both of

these practices undermine the security of password-based systems.  In the

former case, if one system is compromised and the passwords used in it
become known, other systems in which the user employs the same pass-
words could be compromised.  That is, compromise of the user
Õs desktopor laptop or personal digital assistant (PDA) compromises all of the pass-
words employed by that user to access many other systems.Many of the common recommendations for improving the security ofpasswords without changing the fundamental mechanisms involved tradeWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.108WHO GOES THERE?one form of insecurity for another.  For example, if users are encouraged
to choose passwords that are hard to guess, they will probably have to
record the passwords somewhere (because these passwords are not easily
remembered), making them vulnerable to attacks against the stored pass-
words.  Users are encouraged to change passwords periodically, which

also increases the likelihood of recording the passwords in vulnerable
locations.Passwords are easily shared.  A user can tell others his or her pass-
word; in many situations this is common and even encouraged as a sign
of trust.3  Passwords can also be shared inadvertently, as they are often
written down in semipublic places.4  This is not always a serious problem
if the threat model focuses primarily on outsiders, but insiders represent
threats in many contexts, and users often do not consider this type of
threat when sharing passwords or recording them.In principle, passwords can offer a great deal of anonymity.  In prac-
tice, however, most people cannot remember many different passwords,
and they tend to reuse the same passwords for different purposes.  More-

over, if allowed to select an identifier as well as a password, the user may
choose to use the same values for multiple systems.  This makes it poten-
tially easy to link multiple accounts to the same user across system bound-
aries, even though the base technology does not necessarily impose such
linkages.  Additionally, the recovery mechanisms for lost passwords gen-

erally require oneÕs motherÕs maiden name, an e-mail address, or someother form of personal information.  Thus, the infrastructure for pass-

word maintenance often requires sharing other forms of information that
is personal and so presumed to be less likely to be forgotten (see Box 5.1).
This, too, potentially undermines privacy.Finding 5.1:  Static passwords are the most commonly used form
of user authentication, but they are also the source of many
system security weaknesses, especially because they are often
used inappropriately.Recommendation 5.1: Users should be educated with respect tothe weaknesses of static passwords.  System designers must

consider trade-offs between usability and security when de-
ploying authentication systems that rely on static passwords to
ensure that the protections provided are commensurate with3D. Weirich and M.A. Sasse. ÒPersuasive Password Security.Ó Proceedings of CHI 2001Conference on Human Factors in Computing Systems, Seattle, Wash., April 2001.4J. Nielsen, ÒSecurity and Human Factors,Ó Useit.com
Õs Alertbox,  November 26, 2000.
Accessed on March 26, 2002, at <http://www.useit.com/alertbox/20001126.html>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES109the risk and harm from a potential compromise of such an au-thentication solution.  Great care should be taken in the design
of systems that rely on static passwords.More secure authentication technologies can be based on passwordtechnology at some levels.  For example, schemes such as encrypted key

exchange (EKE)5 and Kerberos (a network authentication protocol)
6 also
BOX 5.1Knowledge-Based AuthenticationA form of authentication with similarities to both passwords and challenge/response is knowledge-based authentication.  In this case, users present data

elements that the verifier can approve on the basis of previous transactions andregistration activity.  For example, the Internal Revenue Service (IRS) and a num-ber of state revenue agencies have implemented an electronic signature and au-
thentication technique that relies on a taxpayerÕs presenting data from the previousyearÕs transaction.  In addition to checking traditional identifying information pre-
sented on a tax return, such as the tax identification number and date of birth,
these tax authorities have the taxpayer sign a tax return by entering data from theprevious yearÕs tax return.  For instance, at the federal level, the IRS electronic
authentication program for tax year 2001 allowed the taxpayer to sign his or her
return with a self-selected PIN.  The IRS verifies the identity of the taxpayer usingthe self-selected PIN when the taxpayer provides the adjusted gross income (AGI)for tax year 2000.  In subsequent years, the taxpayer has the choice of using the

same or a different PIN, but must update the AGI data for each previous tax year.By using the data from the previous yearÕs return, the IRS is able to authenticatethe transaction on the basis of knowledge that presumably only the IRS, the tax-
payer, and whoever might have prepared the return for the taxpayer possesses.This type of authentication relies heavily on the data in question being kept secure.(Chapter 6 in this report provides more information on electronic authentication in
the IRS e-file program and the distinction between electronic signatures and au-thentication.)Similarly, the popular online payment site paypal.com deposits a smallamount into a userÕs bank account and asks the user for the amount deposited.  In
effect, so-called knowledge-based authentication is a form of password.  The cru-cial difference is that it is communicated to the user via some out-of-hand mecha-
nism to which an imposter is presumed not to have access.  Additionally, it is
assumed that the legitimate user will look up the authenticator rather than know it;this is no different from a conventional password that is written down.  The crucial
distinction is that knowledge-based technology generally relies on a prior history ofcontact between the client and the verifier.5S. Bellovin and M. Merritt. ÒEncrypted Key Exchange: Password-Based Protocols SecureAgainst Dictionary Attacks.Ó Proceedings of the IEEE Symposium on Security and Privacy. Oak-Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.110WHO GOES THERE?make use of static passwords.  These schemes employ sophisticated cryp-
tographic mechanisms and protocols to counter many of the attacks that
are effective against static passwords.  They typically provide one-way,

initial authentication, which may transition to two-way, data-origin and
peer-entity authentication for subsequent communication.  These are not,

per se, password-based authentication technologies.  The section 
ÒMulti-factor AuthenticationÓ discusses in more detail authentication protocols
of this sort.Something You HaveThe Òsomething you haveÓ class of authentication technologies is
based on the possession of some form of physical token that is presumed
to be hard to forge or alter.  Many forms of physical tokens are used for

authentication and for authorization outside the context of information
systems, and they exhibit varying degrees of resistance to forgery and
alteration.  For example, many driver
Õs licenses and credit cards make useof holograms as a deterrent to forgery, relying on visual verification by a
human being when they are presented.  Yet credit cards are now used

extensively for purchases by mail or telephone or over the Web.  In these

contexts, there is no visual verification of the credential, so the antitamper
security mechanisms are ineffective.  In information systems, the security

of hardware tokens, to first order, is usually based on the ability of these
devices to store, and maybe make direct use of, one or more secret values.
Each of these secrets can be much larger and more random than typical
passwords, so physical tokens address some of the vulnerabilities, such as
guessability, cited above for passwords.  Nonetheless, the simplest forms
of tokens share some of the same vulnerabilities as passwordsÑthat is,they both deal with static, secret values.A magnetic-stripe card is an example of a simple physical authentica-tion token.  Tokens of this sort offer the primary benefit of storing larger

secrets, but they offer almost no protection if the token is lost or stolen,
because readers are readily available and can extract all data (secrets)
from the magnetic stripe.  After a secret is read from the card (even in the

context of a legitimate authentication process), the secret is vulnerable in
the same ways that a password is (for example, it can be intercepted if
transmitted via an insecure communication channel or compromisedland, Calif., May 1992, pp. 72-84. Available online at <http://citeseer.nj.nec.com/bellovin92encrypted.html>.6More information on Kerberos is available online at <http://web.mit.edu/kerberos/www/>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES111while held in storage in a computer7).  When a magnetic-stripe card is
swiped, all the data can be read from the card and become accessible to
malicious software in the system.  This possible misuse argues against

storing the secrets used to authenticate multiple, distinct identities on one
card.  (The storage space on cards of this sort also is very limited.) Con-

versely, requiring a user to carry multiple physical cards to maintain
multiple secrets is inconvenient for the user and adds to overall costs.
Although magnetic-stripe cards and their readers are not very expensive,
computer systems (other than in retail sales contexts) generally do not
offer readers as standard equipment, so there are cost barriers to the use
of such cards for individual authentication in home or corporate environ-
ments.  This is a good example of trade-offs among competing goals of

security, user convenience, and cost.  Sometimes applications for mag-

netic cards focus on authorization rather than authentication, as in the
case of transit fare cards (see Box 5.2).BOX 5.2New York City Transit MetrocardThe New York Metropolitan Transportation Authority uses a magnetic-stripefare card called a Metrocard.  The Metrocard is accepted on all New York City

buses and subway trains.  A magnetic stripe on the card holds the stored value
information; however, the same information is recorded on a central computersystem.  Thus, although the card appears to be an anonymous-bearer instrument,

transactions charged to a particular card are logged.  While this feature is primarily
an antifraud device, it has in fact been used by police agencies to verify suspectsÕmovements.  Furthermore, since cards may be purchased by credit card, the po-

tential exists to track an individual even without possession of the individualÕs phys-ical card.The privacy implications of this system have been known for some time.  A
1997 study1 on multipurpose fare media not only discussed the privacy issue but
also found that it was indeed a major concern of consumers.  Fare cards have a
modest amount of storage and no computational ability.  Because they are copy-

able, there is a need for consultation with the central site.1Transit Cooperative Research Program Research Results Digest, June 1997, availableonline at < http://gulliver.trb.org/publications/tcrp/tcrp_rrd_16.pdf>.7In a well-designed system, a secret read from a card would not be retained in storage onthe system for very long, and the vulnerability here would be much less than when pass-words are stored in a file.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.112WHO GOES THERE?A not-so-obvious form of Òsomething you haveÓ authentication is a
Web cookie.  The need for cookies arises because the protocol used for

Web access, hypertext transfer protocol (HTTP), is ÒstatelessÓ; HTTP doesnot provide a reliable way for a server to know that a particular request is
coming from the same user as a previous request.8  Pure stateless opera-
tion would make it difficult to provide functions such as browsing
through an online catalog and collecting a Òshopping cartÓ full of items
that the user has decided to purchase.  Also, if the user is browsing

through information that needs authentication (such as information about
the userÕs bank account), it would be inconvenient if the user had to typea name and password each time a different page was viewed.The solution to this problem, designed by Netscape, is called aÒcookie.Ó A cookie is data given by a Web server to the client to maintainstate.  Each time a client makes a request to a server, any cookies provided

to the client by that server are sent to the server along with the request.
Thus, for example, the identifier and password provided by a user for
initial authentication may be transformed into a cookie to facilitate con-
tinuous authentication of the HTTP session.  Sometimes a cookie is a

bigger secret than an individual could remember.  It may be like a secret

stored in a token; in this case, the token is the userÕs computer, with all theattendant security problems that arise from storing secrets in a file on a
computer and the problems that arise if the secret is transmitted across a
communication network without encryption.  Sometimes a cookie is used

to track an individualÕs authorization in the context of an HTTP session.In such cases, the cookie itself may be a cryptographically protected value
in order to prevent a user from tampering with it and thus fooling the
Web server.The use of cookies is often criticized as a mechanism that violatesprivacy, but it depends on how they are used.  If they are used solely to

effect session continuity, overcoming the limitations of HTTP, and if the
server does not maintain information about the user, they can be a pri-
vacy-neutral or even privacy-enhancing technology.  But cookies are

sometimes used to track a user
Õs movements through multiple sites.  Many
sites that do not require authentication will set a cookie on the first visit.
This lets the site track return visits by presumably the same user, even
though the site operators do not know who that person is in a larger
context.  Often, this technique is employed by third-party advertising
8The statelessness of the HTTP protocol implies that each request sent for a page iscompletely independent of any requests that came before.  Thus preserving information
from one click to the next requires additional technology  (D. Kristol and L. Montulli,
ÒHTTP State Management Mechanism,Ó Request for Comments (RFC) 2965).
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES113sites; this use of cookies permits tracking users (and their interests) acrossmultiple Web sites.  This is a form of covert identification (see Chapter 1);

the userÕs identity as a Web site visitor and a dossier of his or her activityare compiled and retained, and an identifier in the form of the cookie is
assigned.  It is not necessary to use cookies to track user activity, however.

Even if cookies were banned, it would still be possible to track a userÕsWeb history through other mechanisms such as log files, browser caches,
and browser history files.9Smart cards are credit-card-size tokens that contain memory and, of-ten, a processor.  Smart cards that act only as memory devices are essen-

tially as vulnerable as magnetic-stripe cards in terms of extracting the
secrets stored on the cards, because readers are widely available, and
malicious software can extract stored values from the card.  The costs for

these cards is somewhat higher than those for magnetic-stripe cards, and
smart card readers are more expensive as well, but smart storage cards
offer more data storage than magnetic-stripe cards do, and they resist
wear better.10  Universal Serial Bus (USB) storage tokens are another
hardware storage token format.  They have a potential advantage in that

many PCs offer USB interfaces, thus eliminating reader cost and availabil-
ity as barriers to deployment.Tokens that act only as storage devices may be used to provide initial,one-way authentication analogous to static passwords.  However, be-

cause these devices can hold larger, Òmore randomÓ secret values (that is,
an arbitrary collection of bits as opposed to something meaningful or
mnemonic to a person), they can provide somewhat better security.  In-

creasingly, tokens of this sort are being used to bootstrap continuous
data-origin authentication schemes that are implemented using the pro-
cessing capabilities of a computer to which the token is (locally) con-
nected.  (Recall that the authentication taking place here is authenticating

a local computer to a remote computer, not a person to a remote com-
puter.) These schemes are often challenge/response protocols, as de-
scribed below.  Since these protocols are executed in the computer, not the
9So-called ÒWeb bugsÓ are another mechanism used to surreptitiously observe an
individualÕs actions online.  They are objects, usually one-pixel-square graphic images, em-
bedded within the HTML source on a Web site that cause part of the displayed Web page tobe retrieved from another Web site, thereby transmitting information about the requester toa third party.  Web bugs are used on a surprisingly large number of sites, primarily for

statistical purposes and to gauge the effectiveness of advertising.  The information trans-
mitted to the ÒbuggerÓ includes an IP address and the last site visited and may be linked tocookies to collect individual Web surfing profiles.  Web bugs are also embedded in e-mail

messages by spammers, who use them to validate live addresses.10The magnetic stripe can abrade, and the data records on it may be degraded by expo-sure to magnetic fields.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.114WHO GOES THERE?token, they also can make use of secrets stored in the computer rather thanon separate hardware storage tokens.  The term Òsoftware tokenÓ has beencoined to refer to the use of secrets stored on a computer and employed in
conjunction with an authentication protocol.  Software tokens are not as

secure as hardware storage tokens, since the secrets used by the software
are held in files in a computer on a long-term basis.  At best, these secrets

typically are protected by a password.  Thus, any attack against the com-

puter that compromises these files allows an attacker to retrieve the stored
secrets through password-guessing attacks.  In contrast, a well-designed

authentication technology that uses a hardware storage token would read
the secret(s) stored on the token, use them, then erase them from the com-
puter memory as quickly as possible.  These actions present a smaller win-
dow of opportunity for the compromise of the secret(s), making the use of
hardware storage tokens potentially more secure.  The main attraction of

software tokens is the low cost; the software may be free or inexpensive,
and there is no need to buy token readers.Some of the earliest hardware authentication tokens11 and some of
the most popular ones employed today, such as SecurID (see Box 5.3), do
not interface directly with an authentication system.  Instead, the user is

required to act as an interface, relaying information between an informa-
tion system and the token.  Tokens of this sort typically implement a type

of authentication known as algorithmic challenge/response, or just chal-
lenge/response.  Challenge/response schemes operate much like human-

enacted authentication scenarios.  Most movie goers would recognize the

words ÒHalt! Who goes there?Ó as the beginning of a challenge/responseexchange between a guard and an individual approaching a guarded
area.  The password in such a scenario would usually change daily, con-

sistent with human limitations for adapting to new passwords.  In an

online authentication technology, the challenge can change every time,
making the corresponding response unique in order to thwart eavesdrop-
ping attacks.Challenge/response schemes are a generic technique to prove knowl-edge of a secret, sometimes even without disclosing it to the party per-
forming the authentication check.12  Challenge/response schemes are
analogous to Intruder: Friend or foe (IFF) systems originally developed11J. Herman, S. Kent, and P. Sevcik. ÒPersonal Authentication System for Access Controlto the Defense Data Network.Ó Proceedings of the 15th Annual IEEE Electronics and AerospaceSystems Conference (EASCON), September 1982.
12Research into a class of algorithms known as Òzero knowledge algorithmsÓ is moving
work forward in this area. As a starting point for what this work involves, see S.Goldwasser, S. Micali, and C. Rackoff, ÒThe Knowledge Complexity of Interactive Proof-Systems,Ó in 
Proceedings of the Seventeenth Annual ACM Symposium on Theory of Computing,Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES115by the military for automated authentication of aircraft by ground per-sonnel operating antiaircraft batteries.  Although challenge/response sys-

tems for information systems were originally implemented using hard-
ware tokens, software tokens now are employed frequently for this
purpose and there are manual analogs.  Imagine a large sheet of paper

with many different numbered passwords.  The verifier sends the num-

ber; the presenter sends back the corresponding password (which illus-
trates why these systems are sometimes called one-time password
schemes). In practice, the verifier sends some string of characters to the
presenter (user); the presenter computes a response value based on that
string and on a secret known to the user.  This response value is checked

by the verifier and serves as the ÒpasswordÓ for only one transaction or
session.  As typically employed in a user-to-system authentication ex-

change, this is an example of a one-way initial authentication scheme, butBOX 5.3SecurIDRSA Security (formerly Security Dynamics) markets a challenge/responsecard.  This card, trademarked SecurID, contains a built-in clock and a liquid crystal

display.  Rather than requiring the user to obtain a challenge from a host computer,
the challenge is implicitly the current time.  To gain access, the user merely enters
a user ID and then enters the current number displayed on the card.  The number
displayed changes periodically, usually every 30 seconds to 1 minute dependingon the card configuration.The SecurID has become quite popular in some contexts.  It is relatively easy
(though not especially convenient) to use, it requires no special hardware, and it iseasily integrated with existing, password-style authentication software.  Some ver-
sions of the card require the user to enter a PIN into the card (where it is combined
with the card-resident key), making this into a two-factor authentication system.  In
this case, even if an adversary acquires a card, the ability to impersonate theaffected user depends on guessing the PIN.  If the usual practice of monitoring

failed log-in attempts is being followed,  an adversary guessing PINs for use with a
captured card will probably be detected prior to guessing the PIN.New York,  ACM Press;  O. Goldreich and H. Krawczyk,  1985,  
ÒOn the Composition ofZero-Knowledge Proof Systems,Ó Proceedings of 17th International Colloquium on Automata,Languages and Programming (ICALP), Coventry, U.K, July 16-20, 1990;  U. Fiege, A. Fiat, andA. Shamir, ÒZero Knowledge Proofs of Identity,Ó  Proceedings of the Nineteenth Annual ACMConference on Theory of Computing,  New York, ACM Press, 1987; and J. J. Quisquater and L.Guillou, ÒHow to Explain Zero-knowledge Protocols to Your Children,Ó Advances inCryptology (Crypto Õ89), Springer-Verlag, pp. 628-631, 1990.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.116WHO GOES THERE?it is much more secure, relative to a variety of threats, than are staticpasswords.There are many variations on this scheme: Often a shared secretknown to both the presenter and the verifier can be used to generate and
verify the response.  In schemes based on public key cryptosystems, a

presenter may employ a private key to generate a response that the veri-
fier checks using the corresponding public key associated with the pre-
senter.  These operations can be carried out using software (such as

S/Key13), or the user may employ a hardware token to perform the calcu-lation.  (See Box 5.4, on how such a technology might be used.)
Hardware tokens that contain processors (for example, cryptographicprocessor smart cards, PC cards, some proximity cards,14 or USB proces-
sor tokens) are qualitatively different from all of the previous token types.
They can be much more secure than hardware storage tokens or software
tokens, because they can maintain secret values within the card and never
export them (i.e., transmit secrets off the card).  A smart card typically

performs cryptographic operations in the card, using stored secret values
to execute parts of an authentication protocol, such as a challenge/re-
sponse protocol, on behalf of the cardholder.  With a token capable of

cryptographic operations, the secrets contained in the token are not ex-
posed as a result of inserting the token into a reader, and no secrets are
released to the computers to which the readers are attached or transmit-
ted across a communication path.  Typically, a user must enter a PIN to

enable a smart card token, and the entry of a wrong PIN value multiple
times in succession (logically) disables the token.  This provides some

protection in case the card is lost or stolen.  Nonetheless, capable adver-
13See L. Lamport, ÒPassword Authentication with Insecure Communication,Ó Communi-cations of the ACM 24(11)(November 1981):770-772, and Phil Karn
Õs reference implementa-tion of S/Key described in Neil Haller, ÒThe S/Key One-Time Password System,Ó RFC
1760, February 1995; available online at <http://www.faqs.org/rfcs/rfc1760.html>.14A proximity card contains information stored electronically within the card.  The infor-
mation is transmitted via radio over a short distance (typically less than 10 centimeters)after the card is queried.  Users like these cards because they require very few steps to
perform authentication and therefore are quite fast.  These cards are vulnerable to physical

attacks that extract data from them.  They also may be susceptible to interception of the
transmitted data (over a short distance) and to spoofing attacks, in which the attackertransmits the same sort of query as a legitimate verifier and records the response.  For

disabled users, proximity cards may be attractive alternatives to magnetic-stripe cards sincecard readers for the former have instructions that are typically visual, they are not alwayslocated in positions accessible to those in wheelchairs, and they are hard to insert for those
whose manual dexterity is poor.  For more, see John Gill and J.N. Slater, 
ÒNightmare onSmart Street,Ó  Tiresias:  International Information on Visual Disability
, 2002 (updated), avail-able online at <http://www.tiresias.org/reports/tidecon2.htm>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES117BOX 5.4Token-Based Authentication System: A ScenarioLaura visits BestCare hospital for a chronic illness that requires many labora-tory tests and radiological examinations. Dr. Jones, her physician, inquires wheth-
er Laura has an Internet-connected computer at her home that she can use toconnect to BestCareÕs Web-based patient information portal.  This portal can be
used to check results, communicate with various health care providers, request
and check clinic scheduling information, view physician-recommended literature,and join health-care-related chat groups. Since Laura has been having a difficulttime scheduling her visits by phone and is finding it hard to ask informed questions
of her physician during her visits, she eagerly accepts the offer. Dr. Jones thenrecords LauraÕs acceptance in her computerized record and tells her she will haveto click on some health-related pop-up advertisements to receive free access.BestCareÕs Web-based patient information center allows patients to checktheir records and communicate with doctors at their convenience, without havingto schedule an appointment.  Such a utility saves time for both doctor and patient

and helps BestCare honor its commitment to providing continuous care to patientswith chronic illnesses.  Statistics collected from patients
Õ visits to the site help Dr.
Jones with her research in chronic disease management, and BestCare receives
additional revenue from healthcare companies in exchange for posting their adver-tisements on the site.Because BestCare has decided to require additional security measures pro-tecting patient records from unauthorized access through its Web site, Laura isgiven a token card in addition to a user ID and a password.  At home, Laura asks
her son to help; he configures the Web browser, enables the token, and changes
the password from the one given to her at the hospital. From now on, Laura willenter her new password in her token card, which then displays a different pass-word that she enters with her user ID to access the portal. She is required to read
and accept the privacy notice, which she only skims in her rush to get to the portal.When the portal displays a message that her test results are pending, her sonÑwho is still looking over her shoulderÑis curious about the tests.  Not wanting to
upset her son with information about her unconfirmed diagnosis, Laura decidesthat she will keep the token in a secure place so that only she can access the portalin the future.Dr. Jones carries a similar token card to access clinical information about herpatients. The system only requires her to use the token when she is not accessingthe information from a workstation directly connected to the BestCare network.
This reduces the human-effort cost for the care providers but means that the Best-Care network must maintain good security with standard firewalls and intrusiondetection.Several parties have been involved in setting up and maintaining the patientinformation portal. BestCare has outsourced its information technology to Helpfu-lIT, Inc., and has set up its Internet portal business with its medical records vendor,
GoodMedRec Co., which hosts its Web servers and databases with BetterASP,Inc. BestCare has also contracted with several pharmaceutical, nursing home, and(continues)Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.118WHO GOES THERE?other health care companies to advertise their services on its site.  It arranged thecontracts through an Internet advertising management company named FineClick.In this scenario, several sets of information are collected. HelpfulIT managesuser ID/token/password information in an authentication database. GoodMedRecmanages user ID and user demographic information and clinical information in a
database. Every time Laura signs on, BetterASP maintains user ID/IP addressmappings in its audit systems for performance and billing purposes. On behalf ofBestCare, GoodMedRec provides FineClick with an abbreviated set of Òclinicalcodes of interestÓ in order to conduct customized marketing, and FineClick main-
tains clinical codes and IP address information for billing.Authentication/Authorization/IdentificationLaura is authorized to view only her own records, not those of other patients.Two-factor authentication, using something she knows (the password) and some-thing she has (the token card), prevents Laura from accessing information that shedoes not have permission to see.  The system records Laura
Õs user ID and dateand time of access every time she looks at her records online. Such identificationprotects BestCare from liability.  Laura might also need to identify herself if she
wants to ask a doctor a question that pertains to her own medical record.  She is

not required to identify herself when she searches for general health information,uses the chat groups, or explores advertisements.Because it is time-consuming for BestCare staff to continually update theWeb portalÕs record of which patient is assigned to which physician, all physiciansusing the system have been given access to all patient files.  Again, it is important
for legal reasons to know who has looked at a patientÕs records, so when Dr. Jonesaccesses LauraÕs record using the system, she must be identified and logged asthe person who has seen the record.Two-factor authentication also protects doctor and patient from unauthorizedaccess by outsiders.  Even if Laura
Õs token card is stolen, her medical record issafe as long as the thief does not know LauraÕs password.  Unfortunately, Laura
has written down her password because she is afraid she might forget it; in doing
so, she has made her information more vulnerable.Dr. Jones is similarly protected from token card theft. However, Dr. Jones hasshared her user ID and password with her assistant, who is not otherwise autho-
rized to view confidential patient records. Although Dr. Jones has no reason todoubt her assistantÕs trustworthiness, such sharing exposes the medical records ofall BestCareÕs patients to unauthorized access and tampering, whether maliciousor purely accidental.There is a downside to using tokens as extra security measures to protectagainst unauthorized access to medical records; authorized users might be denied
access at inconvenient times because of a lost or malfunctioning token.  In this
scenario, if Dr. Jones is away from the office and her token card is lost or broken,she cannot view LauraÕs questions as soon as they are asked and LauraÕs healthmight be compromised.  However, if it becomes common practice for Dr. Jones
and her colleagues to circumvent such security measures by calling the office andasking another physician or an administrative assistant to access the record,BOX 5.4Continued
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES119LauraÕs privacy may be compromised. Although BestCareÕs privacy policies mayexplicitly address this issue, Laura may not understand its implications.BreachesThe vice president of HelpfulIT, Inc., is told that patients are having problemsaccessing its systems from home, and he suspects these problems are associatedwith FineClickÕs advertisement-download process. An audit log analysis is initiat-ed, and a staff programmer correlates user IDs, IP addresses, clinical codes, andadvertisement clicks from different sources in a spreadsheet. A problem is found inspecific codes and advertisements, and it is fixed. The spreadsheet is posted to
the help desk as a solution for a possible problem.Now LauraÕs clinical data are being used, shared, and cited to resolve anoperational problem.  The staff member at HelpfulIT, Inc., who resolved the prob-

lem does not have medical records access for patients but is now able to combinethe user ID, IP address, and types of advertisements (each collected from differentinteractions) to determine which patient suffers from which illness.Another major problem relates to the safekeeping of the databases. In light ofthe increasing number of identified vulnerabilities in systems such as BetterASPÕsWeb servers, the increasing sophistication of attack methods, and the inertia of
system administrators when it comes to patching their systems, it is easier thanever to compromise many information systems. Sometimes, inadvertent mistakeslead to the publication of private information over the Internet without adequate
authentication and encryption.  Misuse and accidental errors by insiders contribute
to serious privacy problems. Although such potential breaches of security and compromises to privacy
are sobering to consider, LauraÕs main concern is that her family remain unawareof her condition until she knows more facts and can share them at the time of herchoosing. Back home, LauraÕs son uses the computer after she is finished explor-ing BestCareÕs portal. He clicks the ÒBackÓ button on the open browser window
and, curious, clicks several more times, eventually reaching a disease manage-ment site that Laura had been browsing. His level of concern rising, he checks the
browserÕs history file to identify the last sites visited. On the basis of this indirectand incomplete information, he concludes that his mother is seriously ill and, in astate of great emotion, confronts her. Laura is forced to explain her disease to her
son before she is ready to do so.Privacy IntrusivenessToken cards are reasonable authentication methods from a security perspective,and this scenario describes how they might be used in a particular context.  Potential

privacy violations, however, are a result of faults in overall system protection. Thescenario demonstrates that the choice of information delivery technology, the decisionto allow access by ostensibly uninterested parties for maintenance purposes, and
human factors such as password sharing for the sake of convenience, may open upunforeseen vulnerabilities. Many of these same privacy concerns could, however, arisewith authentication technologies not based on token cards.  How systems are imple-

mented and deployed, and what policies are put in place to govern their usage, all bearon what the privacy implications of authentication systems will be.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.120WHO GOES THERE?saries with access to sophisticated technology (of the sort that might com-monly be found in a college physics lab) can extract secret values stored
on smart cards.15  Processor tokens are noticeably more expensive than
magnetic-stripe cards or other storage tokens.  The cost of readers varies,
depending on which token technology is employed.Often a token of this sort is used only for initial one- or two-wayauthentication,16 based on the execution of the cryptographic operations
of a challenge/response protocol within the token.  This provides a secure

foundation for two-way, continuous authentication schemes based on
this initial exchange.  Responsibility for continuous authentication often

is borne by the computer to which the smart card is attached, because
smart card interfaces are too slow to deal with all the data transmitted or
received on a connection.  However, the continuous authentication

bootstrapped from a smart card offers the opportunity for better security
overall.Hardware tokens have the desirable security property of not beingable to be readily replicated by users, although the difficulty of unautho-
rized replication varies widely, as noted above.  In principle, if one user

chooses to share his or her token with another, the first user relinquishes
the ability to authenticate himself or herself as long as the hardware token
is loaned.  This guarantee is diminished with some forms of hardware

storage tokens and with software tokens, since one can copy the files that
personalize them.  The extent to which tokens preclude sharing repre-

sents a significant improvement over static passwords.  A necessary cor-

ollary to this observation is that the loss of any token results in a replace-
ment cost, and the replacement process is more difficult than for a
password.  A help desk cannot remotely replace a token in the way that it

can reset a password.  Hardware cryptographic tokens and software to-

kens entail costs for integration into applications because they execute
authentication protocols rather than act just as repositories for secrets.Something You AreThe final class of technologiesÑÒsomething you areÓÑ refers to the
use of biometrics to authenticate individuals.  Biometric authentication,

which has received much attention in the media of late, is the automatic15See, for example, R. Anderson and M. Kuhn, ÒTamper ResistanceÑA CautionaryNote,Ó The Second USENIX Workshop on Electronic Commerce Proceedings, Oakland, Calif.,
November 18-21, 1996.16Typically, two-way authentication relies on the use of public key cryptography andcertificates.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES121identification or identity verification of human individuals on the basis ofbehavioral and physiological characteristics.17Biometric authentication is fundamentally different from the othertwo classes because it does not rely on secrets.  Biometrics themselves are

not secrets; people commonly leave fingerprints on everything they touch.
Our voice, handwriting, and facial image can be captured without our
knowledge.  Rather, biometric authentication relies on registering and

later matching what are believed to be distinguishing physical or behav-
ioral characteristics of individuals.  There are many different examples of

biometric authentication: it can be based on fingerprints, iris scanning,
voice analysis, handwriting dynamics, keystroke dynamics, and so on.Biometric authentication operates by matching measured physicalcharacteristics of a person against a template or a generating model of
these characteristics that was created when the person was registered
with an authentication system. The match between a captured biometric
and the template is never exact, because of the ÒnoiseÓ associated with the
measurement processes, the way the characteristic is presented to the
sensor, and changes in the underlying biometric characteristic itself.  Thus,

these technologies require an administrator to set threshold values and a
decision policy that controls how close the match must be and how many
attempts to be authenticated the user will be allowed to make.The scoring aspect of biometrics is a major departure from otherclasses of individual authentication technologies, which provide a simple,
binary determination of whether an authentication attempt was success-
ful.  The scoring aspect of biometric authentication technologies means

that they exhibit Type I (false negative) and Type II (false positive) errors.
Type I errors run the risk of inconveniencing or even alienating individu-
als whose authentication attempts are erroneously rejected.  Type II er-

rors are security and privacy failures, as they represent authentication
decisions that might allow unauthorized access.  For any specific biomet-
ric technology implementation, there is a trade-off between these two
types of errors: Changing the scoring to reduce Type I errors increases
Type II errors, and vice versa.  Some implementations of biometric au-

thentication technologies exhibit relatively poor tradeoffs between these
two error types, forcing an administrator to choose between inconve-
niencing legitimate users while rejecting (appropriately) almost all im-
poster attempts, or minimizing inconvenience to legitimate users while
accepting a higher rate of successful imposters.17J.L. Wayman, ÒFundamentals of Biometric Authentication Technologies,Ó  InternationalJournal of Imaging and Graphics 1(1)(2001);  B. Miller, ÒEverything You Need to Know AboutBiometrics,Ó PIN Industry Sourcebook, Warfel and Miller, 1989.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.122WHO GOES THERE?Biometric values that are captured for authentication and transmittedto a remote location for verification must be protected in transit.  They are

vulnerable to interception and replay, just like (static) passwords, unless
suitably protected during transmission across communication networks.
It also is important to ensure that the transmitted value represents a legiti-
mate, digitized sample of a user biometric.  Otherwise, an attacker might

inject a string of bits that purports to be a biometric sample in an effort to
subvert the system.  Typically (though not always), the range of possible

values for a biometric sample is so large that guessing is not a viable
means of attack.  But since biometric values are not secrets per se, it is

conceivable that an attacker has gained access to a userÕs fingerprint, forexample, and has digitized it in an effort to masquerade as the user.
Moreover, if unencrypted (or weakly encrypted) biometric templates are
stored in centralized authentication servers, an attack against one of these
servers could result in the disclosure of the templates for all the users
registered with the compromised server.  With access to the templates

and knowledge of the scoring algorithm, an attacker could engage in off-
line analysis to synthesize bit strings that would pass as legitimate bio-
metric samples for specific users.Today biometric authentication systems are not widely deployed, andthere are many implementation variants for the same type of biometric
(for example, a plethora of fingerprint systems).  However, if biometric

authentication were to become widely deployed and if there were signifi-
cant consolidation and standardization in the industry (resulting in fewer
variants), the compromise of an authentication server could have a very
significant impact owing to the special characteristics of biometrics:
namely, that they are not secret and cannot be easily modified.As with hardware tokens, the deployment of biometric authentica-tion sensors entails hardware-acquisition costs, although the cost here is
typically for each access point of a system rather than for each user of the
system.  Sensors for biometric authentication have been expensive, and

this has been a barrier to adoption.  However, the cost of some biometric

sensors, especially fingerprint-scanning sensors, has declined, making
them affordable for use in individual computers and laptops.  Although

all biometric measures change over time, an individual cannot forget his
or her biometric values, unlike passwords and PINs, nor can they be lost,
like hardware tokens.  Thus, life-cycle costs can, in principle, be lower for

biometric authentication technologies, primarily because of reduced help-
desk costs.  However, in practice, most biometric authentication systems

require the use of a password or PIN to improve security, and this elimi-
nates the cost advantage that would have accrued from fewer help-desk
calls.  In fact, one can improve the performance of biometric authentica-

tion systems in some contexts by offering an alternative authenticationWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES123mechanism for some individuals who are not compatible with a specifictechnology.  (For example, some percentage of the general population has

fingerprints that are not well recognized by fingerprint scanners.)Finding 5.2: Biometric user-authentication technologies holdthe promise of improved user convenience.  Vendors of these

technologies also promise reduced system management costs,
but this has yet to be demonstrated in practice.  Moreover, these

technologies can pose serious privacy and security concerns if
employed in systems that make use of servers to compare bio-
metric samples against stored templates (as is the case in many
large-scale systems).  Their use in very local contexts (for ex-

ample, to control access to a laptop or smart card) generally
poses fewer security and privacy concerns.Recommendation 5.2: Biometric technologies should not beused to authenticate users via remote authentication servers
because of the potential for large-scale privacy and security
compromises in the event of a successful attack (either internal
or external) against such servers.  The use of biometrics for

local authenticationÑfor example, to control access to a private
key on a smart cardÑis a more appropriate type of use for
biometrics.Biometric authentication offers only one-way initial authentication.As noted above, biometric authentication does not provide direct protec-
tion for secrets, so it does not provide a basis for bootstrapping from
initial to continuous authentication, nor does it support two-way authen-
tication, unlike many of the Òsomething you haveÓ technologies described
above.  Thus, biometric authentication is not an appropriate replacement

for other authentication technologies, specifically for cryptographic tech-
nologies used to provide two-way initial authentication and continuous
authentication.  Box 5.5 provides some commonsense guidelines for the

uses of biometric authentication systems.MULTIFACTOR AUTHENTICATIONIt is often asserted that individual authentication can be improved byemploying multiple Òfactors.Ó Generally this translates into using authen-
tication technologies from two of the classes described above.  Examples

include a PIN plus a hardware token (something you know and some-
thing you have) or a PIN and a biometric (something you know and
something you are).  There is a reasonable basis for this strategy, but it is
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.124WHO GOES THERE?BOX 5.5Items to Keep in Mind When Using Biometrics1.  Never design or use a biometric system that allows either remote enroll-
ment or re-enrollment.  Such systems have no good way of connecting a user with

the enrolled biometric record other than additional authentication, so the advan-tage of using biometrics is lost.2.  Biometric measures can reveal your identity if they are linked at enrollment
or at subsequent usage to your name, Social Security number, or other identifyinginformation.3.  Remember that biometric measures cannot be reissued if stolen or sold.
Consequently, your biometric measures will be only as secure as the most inse-cure site that has them.  Do not enroll in a system that does not seek to preserve
anonymity unless you have complete trust in the system administration.4.  All biometric access-control systems must have exception-handling mech-
anisms for those individuals who either cannot enroll or cannot reliably use thesystem for whatever reason.  If you are uncomfortable with enrolling in a biometric

system for positive identification, insist on routinely using the exception-handlingmechanism instead.5.  The most secure and most privacy-sensitive biometric systems are those
in which each user controls his or her own template.  However, simply controlling
your own biometric template, say by holding it on a token, does not guaranteeeither privacy or security.6.  Because biometric measures are not perfectly repeatable, are not com-
pletely distinctive, and require specialized data collection hardware, biometric sys-tems are not useful for tracking people. Anyone who wants to physically track you
will use your credit card purchases, phone records, or cell phone emanations in-stead. Anyone wanting to track your Internet transactions will do so with cookies,Web logs, or other technologies.not foolproof.  The assumption underlying the perceived security benefits
of multifactor authentication is that the failure modes for different factors
are largely independent.  So, for example, a hardware token might be lost

or stolen, but the PIN required for use with the token would not be lost or
stolen at the same time.  This assumption is not always true, however.

For example, a PIN attached to a hardware token is compromised at the
same time that the token is lost or stolen.  If a fingerprint is used to

activate a hardware token, is it not likely that copies of the fingerprint will
appear on the token itself? As noted earlier, one cannot evaluate the rela-
tive security of mechanisms without reference to a threat model, and
some threat models undermine the perceived security of multifactor au-
thentication.  Nonetheless, multifactor authentication can improve the

security of authentication under many circumstances.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES125CENTRALIZED VERSUS DECENTRALIZEDAUTHENTICATION SYSTEMSA crucial issue for authentication technologies is whether they areinherently centralized or decentralized.  This distinction affects both their

deployability and their privacy implications.Some technologies require little or no infrastructure; any system canmake use of such technologies without relying on additional systems for
support.  This is one of the major drivers of the use of static passwords:

They are extremely easy to set up in a highly localized fashion.  An appli-

cation can create and maintain its own password database with about the
same effort as that needed for maintaining a database of authorized users.
(In fact, doing so properly is rather more complex, but there are numer-
ous poorly implemented password systems.) Some public key authentica-
tion technologies have similar decentralized properties.  Some challenge/

response protocols are designed for local use and require minimal infra-
structure.  The one-time password
18 and Secure Shell (SSH)19 protocols
are good examples.  The latter makes use of public key cryptography but
not a public key infrastructure (described later in this section).  Both

arguably provide a more secure authentication capability than passwords,
but they are still intended for use in local contexts, such as a single com-
puter or at most a single organization.Some types of authentication technologies require some degree ofcentralizationÑfor example, to help amortize the costs associated withdeployment to gain security benefits.  Kerberos and public key infrastruc-

ture (PKI) are good examples of such systems.  In Kerberos, the key distri-

bution center (KDC) is a centralized infrastructure component that stores
the passwords of all users, preventing them from having to be shared
with each system to which a user connects.  The KDC is aware of all sites
with which the user interacts, because the KDC is invoked the first time
that a user establishes a connection in any given log-in session.  The

content sent over the connections is not necessarily revealed; neverthe-
less, the central site operates as the verifier, acting as an intermediary
between the user (presenter) and the sites that rely on Kerberos for au-
thentication.  The scope of a Kerberos system deployment is typically

limited, which in practice mitigates some of the privacy concerns.  Al-

though Kerberos systems can be interconnected, most usage of Kerberos
is within an individual organization. When cross-realm authentication is
employed, only those transactions that involve multiple realms are known18See RFC 1760 at <http://www.faqs.org/rfcs/rfc1760.html>.19See <http://www.ietf.org/internet-drafts/draft-ietf-secsh-userauth-16.txt>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.126WHO GOES THERE?outside a userÕs home realm.20  This limits the adverse privacy aspects ofusing such a system.  However, if a single Kerberos realm was used to

authenticate individuals to systems across organizational boundaries, the
privacy implications would be much worse.  Thus, the same technology

can be used in different contexts with vastly different privacy implica-
tions.  For more information about Kerberos, see Figure 5.1.
The Passport and Liberty systems, though very different in detail, arecentralized systems designed expressly to authenticate large user popula-
tions to a wide range of disparate systems, with attendant privacy impli-
cations.  While their designs differ slightly, both offer users the same basic

feature: the convenience of single sign-on to a variety of Web services.
From a privacy perspective, the obvious drawback to centralized authen-
tication systems is that all Web clients cannot be expected to trust the
same authentication service with what could be personally identifying
information.  Passport and Liberty both address this fundamental ob-

stacle by allowing what they call a federated topology.   
ÒFederated,Ó in
this context, means that peer authentication services can interoperate with
different subsets of service providers.  For example, a car rental company

could rely on four different airlinesÕ authentication services.  Theoreti-
cally, a single user could navigate seamlessly between multiply affiliated
sites after authenticating only once.  Even in their federated form, how-

ever, there are two types of privacy risk inherent in these single sign-on
systems:  exposure of what we call identity data (the set of all information

associated with an individual within this identity system) by the authen-
tication service and the aggregation of an entityÕs (or his or her identifierÕs)downstream behavior.  While the committee did not undertake a detailed

analysis of these two systems (one of which is proprietary and one of
which has a specification developed and licensed by a private consor-
tium), as with any authentication system the privacy implications will
ultimately depend on choices made at the design, implementation, and
use stages.21  Detailed analysis of a particular product is beyond the scope
of this report.Public key infrastructure has often been touted as a universal authen-tication technology, one that might have national or even global scope.20A Kerberos ÒrealmÓ is a local administrative domain. Typically an organization will
have its own realm. Different realms can be configured to interoperate with each other, but
this requires explicit action from each organization.21Recently, the FTC stepped in (at the request of privacy advocates) to assure thatMicrosoftÕs security and privacy policy is correctly represented to consumers.  Many ob-
servers commented that this move should be considered a warning that governments inter-nationally will scrutinize how centralized authentication services collect, protect, and useconsumer data.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES127FIGURE 5.1  Kerberos:
1.User provides a principal (user name) and password to the client system.

2.Client queries the Initial Ticket Service of the Kerberos key distribution
center (KDC) for a ticket-granting ticket (TGT), which will allow the clientto request tickets for specific services later on.  The client
Õs request in-cludes a derivative of the userÕs password, which the Initial Ticket Serviceverifies.3.The KDC
Õs Initial Ticket Service provides the client with a dual-encryptedinitial TGT containing a log-in session key. The client system converts theuserÕs password into an encryption key and attempts to decrypt the TGT.4.The client uses the TGT and the log-in session key to request tickets to
specific services from the KDCÕs Ticket-Granting Service.5.The Ticket-Granting Service decrypts the TGT with its own key, and then
decrypts the service request using the TGTÕs session key. If decryption issuccessful on both counts, the Ticket-Granting Service accepts the userÕsauthentication and returns a service ticket and a service-session key (en-crypted with the log-in session key) for the targeted service.  This result

can be cached and reused by the client.6.The client uses the log-in session key provided in step 3 to decrypt the
service ticket, gaining access to the service-session key.  This key is then

used to request access to the target service.  This request is accompanied
by an encrypted time stamp as an authenticator.7.Access to the target service is granted.  Steps 4 through 7 can be repeated
when access to other services is needed; service messages can be encryptedwith the service-session key. A time limit is built into the log-in session insteps 3 and 5; the user will need to enter the password again when the
log-in session has timed out.KDCServerClient1Initial Ticket ServiceTicket-granting 
ServiceTarget ServiceTarget ServiceTarget Service236745Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.128WHO GOES THERE?Certainly a very large-scale PKI would have very serious privacy implica-tions, as it might provide a single, uniform identifier that an individual
would employ in transactions with many different organizations.  (See

Box 5.6 for a brief description of public key cryptography.) Since each
public key certificate carries a clearly visible identifier for the person
represented by the certificate, it is easy to link different uses of the same
certificate to that personÕs identity.The General Services AdministrationÕs Access Certificates for Elec-tronic Services (ACES) program, described more fully in Chapter 6 in
this report,22 has this flavor for citizen interactions with the U.S. gov-
ernment.  In Japan, plans call for the creation of a national-level PKI that

would be used not only for individual interactions with the government
but also for a wide range of private sector interactions.  VeriSign and

other so-called trusted third party (TTP) certificate authorities (CAs) in
both the United States and Europe promote the notion of using a single
public key certificate as the universal personal authenticator for a wide
range of transactions.For example, if citizens were issued a single Òinteract with the gov-ernmentÓ public key certificate, it might be relatively easy to determine if,
say, the individual who had a reservation to visit Yosemite National Park
was the same person who had sought treatment in a Department of Veter-
ans Affairs (VA) hospital for a sexually transmitted disease.  By contrast,

if the VA and the National Park Service each issued their own certificates,
or if they relied on some other decentralized authentication mechanism,
such linkage would be harder to establish.  Thus, it is not the use of PKI

per se (except as it is an authentication system with all of the privacy
implications intrinsic to authentication itselfÑsee Chapters 2, 3, and 7 inthis report) but rather the scope of the PKI that influences the privacy of
the authentication system.PKI technology does not intrinsically require large scale or use acrossmultiple domains in order to be useful or cost-effective to deploy.  This

report has already argued that individuals typically have multiple identi-
ties and that most identities are meaningful only in limited contexts, which
suggests that many PKIs could arise, each issuing certificates to individu-
als in a limited context, with an identifier that is meaningful only in that
context.23  PKIs of this sort can be privacy-preserving, in contrast to very
large-scale PKIs.  Proposals have been made to use PKIs in a highly de-
22See <http://www.gsa.gov/aces/>.23For another view of PKI, digital certificates, and privacy, see Stefan Brands, RethinkingPublic Key Infrastructures and Digital Certificates: Building in Privacy, Cambridge, Mass., MITPress, 2000.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES129BOX 5.6Public Key CryptographyPublic key cryptosystems were first described in the open literature by Whit-field Diffie and Martin Hellman at Stanford University in 1976.1  In public key sys-
tems, each user has two keys.  One is kept private while the other, as the name
implies, is usually made public.  These keys are mathematically related in such a
way that knowledge of the public key does not allow one to determine the corre-
sponding private key.  (The reverse may or may not be true, depending on the
public key algorithm in question.)  This property of public key cryptosystems means
that data encrypted with one userÕs public key can be decrypted using his or hercorresponding private key, without sharing the private key with others.  Converse-
ly, data that are transformed with a userÕs private key (digital signing) can be ver-ified with the corresponding public key, again without the need to divulge the key
used to generate the signature.  This latter relationship is most relevant to user-
authentication systems based on public key technology.  The use of public key
systems significantly transforms the problem of key distribution: Distribution of

public keys requires authentication and integrity of the public keys (we have toknow whose public keys we are using) but not confidentiality (because the publickeys need not be kept secret).There are two basic ways in which public key systems are used:2¥Encryption.  Public key systems are used to provide confidentiality by hav-ing the recipient of a confidential message first provide its public key to the sender.This transaction does not have to be held in secret because the key distribution,the public key, does not have to be kept confidential.  The sender then encrypts the

communication in the recipientÕs public key and sends it to the recipient.  Only therecipient can decrypt the message using his or her private key.¥Digital signature.  Public key systems such as the RSA system
3 and the
Digital Signature Standard4 can provide what is often referred as a 
Òdigital signa-ture.Ó  Digitally signed data are not encrypted by the process.  Instead, they are
protected against unauthorized modification, and the identity of the signer of the
data can be determined (data origin authentication) if, for example, a PKI has beenestablished (or if the verifier trusts, through some other means, that the public keyof the signer is as described).  A message or document is digitally signed by trans-

forming the data5 using the signer
Õs private key.1See W. Diffie and M. Hellman, ÒNew Directions in Cryptography,Ó IEEE Transactions onInformation Theory IT-22(6)(1976):644-654.2A third type of public key cryptosystem, public key agreement algorithms, is not discussedhere, since these systems usually are employed for confidentiality but not for authentication.3RSA was developed by Ron Rivest, Adi Shamir, and Leonard Adelman at MIT.  See 
ÒAMethod for Obtaining Digital Signatures and Public-Key Cryptosystems,Ó  Communications ofthe ACM 21,2 (February 1978): 120-126.
4See Federal Information Processing Standards Publication 186 on the Digital SignatureStandard, available online at <http://www.itl.nist.gov/fipspubs/fip186.htm>.5In practice, the data to be signed are first compressed in a one-way fashion, using a hashalgorithm, and the resulting hash value is digitally signed.  This variant on the basic scheme is

employed because public key signature operations are relatively slow and signing a large
amount of data would be very burdensome.(continues)Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.130WHO GOES THERE?Digital CertificatesMost uses of public key systems require that one know that a given public keybelongs to a particular person or organization.  One obvious way to obtain the

public key securely is to obtain it directly from the sender in a secure out-of-bandchannel (for example, by way of a personal interaction).  This approach, while
viable in some circumstances, in general does not scale very well.However, the very nature of a digital signature lends itself to a solution to thisproblem.  Specifically, if a recipient knows one public key, the issuer of that publickey can ÒvouchÓ for the association between a different public key and its owner by
issuing a digital document of that assertion.  With some additional structure, this
system becomes the basis for digital certificates, and therefore a PKI.  The entity
that signs (issues) a certificate usually is referred to as a certificate authority (CA).
Note that CAs collect data from many users as part of the certificate issuance(registration) process and assign a single identifier to each user. This practiceencourages a user to collapse multiple identities into a single identity for presumed
ease of use in interactions with a diverse set of organizations, heightening the riskof linkage.Because a certificate represents a binding between an identifier (presumablyassociated with the owner) and a key, it inherently contains some notion of identity.Just how strong this notion is, and the form of identity bound into a certificate,depends on the policies of the CA and on the intended use of the certificate.  In

some forms, a certificate can contain a name, an e-mail address, or an accountnumber.  In others, there may be no meaningful identification, just the public key
itself.  The basic notion behind the use of a certificate is to establish a certification
path between a known public key and the certificate being verified.BOX 5.6Continued
centralized fashion24,25 that supports this notion of multiple identities for
an individual and thus supports privacy.  However, multiple PKIs might
impose burdens on users, who would be required to manage the multi-
tude of certificates that would result.  In a sense, this is not too different

from the common, current situation in which an individual may hold
many physical credentials and has to manage their use.  If individuals are

going to accept and make use of a multitude of PKIs, software needs to
provide a user interface that minimizes the burden on users.24S. Kent. ÒHow Many Certification Authorities Are Enough?Ó  Proceedings of MILCOM(unclassified papers) 97(1)(November 1997):61-68.25S. Kent. ÒSecurity Issues in PKI and Certification Authority Design.Ó Advanced  Security
Technologies in Networking.  NATO Science Series. Burke, Va., IOS Press, pp.  33-52, 2001.
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES131Finding 5.3: Public certificate authorities and trusted third par-ties present significant potential privacy and security concerns.Finding 5.4:  Public key infrastructures have a reputation for
being difficult to use and hard to deploy.  Current products do

little to dispel this notion.Finding 5.5: Many of the problems that appear to be intrinsic topublic key infrastructures (as opposed to specific public key
infrastructure products) seem to derive from the scope of the
public key infrastructures.Recommendation 5.3:  Public key infrastructures should be lim-
ited in scope in order to simplify their deployment and to limit
adverse privacy effects.  Software such as browsers should pro-

vide better support for private (versus public) certificate au-
thorities and for the use of private keys and certificates among
multiple computers associated with the same user to facilitate
the use of private certificate authorities.This analysis suggests that authentication technologies that implysome degree of centralization can be operated over a range of scales with
vastly differing privacy implications.  Thus, neither Kerberos nor PKI

intrinsically undermines privacy (beyond the fact that they are authenti-
cation systems and as such can affect privacy), although each could be
used in a way that would do so.  In general, decentralized systems tend to

be more preserving of privacy:  No single party has access to more than its

own transaction records.  An individual may use the same password for

two different Web sites; for a third party to verify this, the party would
need at least the cooperation of both sites and (depending on the precise
password storage technology being used) perhaps special-purpose moni-
toring software on both sites.  But if users employ the same identifiers at

each site, the potential for privacy violations is significantly increased.
This same observation applies to any form of decentralized authentica-
tion system.An essential requirement for preserving privacy in authenticationsystems is allowing an individual to employ a different identifier when he
or she asserts a different identityÑfor example, in different organiza-tional contexts.  The use of different identifiers makes it harder to corre-

late the individualÕs activities across systems, which helps preserve pri-vacy.  This goal can be achieved with technologies ranging from

passwords to PKIs.  Also, if each system collects less personal information
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.132WHO GOES THERE?on its usersÑonly what is required to satisfy the requirements of thatsystemÑthis, too, is privacy-preserving.Finding 5.6: Core authentication technologies are generallymore neutral with respect to privacy than is usually believed.
How these technologies are designed, developed, and deployed
in systems is what most critically determines their privacy im-
plications.SECURITY CONSIDERATIONS FOR INDIVIDUALAUTHENTICATION TECHNOLOGIESAuthentication technologies are often characterized by the securitythat they offer, specifically in terms of resistance to various types of at-
tack.  Many authentication technologies rely on the use of secret values

such as passwords, PINs, cryptographic keys, and so on.  Secrets may be

vulnerable to guessing attacks if they are selected from a set of values that
is too small or predictable.  Passwords, when selected by individuals and

not subject to screening, often exhibit this vulnerability.26  Secrets also
may be compromised by computational attacks, even when the secrets
are chosen from large sets of values.  For example, a large, randomly

chosen cryptographic key would generally be immune to guessing at-
tacks.  But this key could be used in an authentication protocol in a man-

ner that permits an attacker to perform computations that reveal the value
of the key.If secrets are transmitted across a communication network, from pre-senter to verifier, as part of an authentication process, they are vulnerable
to interception unless otherwise protected (e.g., by encryption).  An en-

crypted communication path is often necessary, but it is not sufficient to
protect secrets against being transmitted.  An attacker might masquerade

as a system that a user wants to access and thus trick the user into reveal-
ing an authentication secret, even though the secret was encrypted en
route.27  A secret need not be transmitted across a network to be subject to
attacks of this sort.  Several years ago, thieves installed a fake ATM in a
26In this context, the user (presenter) also is acting as the issuer, and as an issuer is doinga poor job.27This is an example of why two-way authentication is important.  A form of this attack
sometimes takes place when a user employs Secure Sockets Layer to encrypt communica-
tion between a browser and a Web server.  The user may reveal credit card account infor-
mation (account number, expiration date, shipping address, and so on) to a sham merchant,who then can use this information to carry out unauthorized transactions.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES133shopping mall.28  Unsuspecting individuals inserted ATM cards and en-
tered PINs, which were collected by the thieves and used to make unau-
thorized withdrawals from the users
Õ accounts.  Thus, even physical prox-
imity and an ability to see a verifier does not ensure that it is the device it
appears to be and one to which authentication information should be
presented!Secret values that are too big for individuals to remember must bestored.  The way in which the secrets are stored may make them vulner-

able.  For example, passwords written on a note stuck on a monitor in the

workplace may be observed by other employees, custodial staff, or even
visitors.  Secret values stored in a file on a computer can be compromised

by a wide variety of attacks against the computer, ranging from physical
theft to network intrusions.  Even secret values stored in hardware dedi-

cated to authentication can be extracted illicitly, with varying degrees of
difficulty, depending on the technology used to store the secrets.Often there is a requirement to prevent individuals from sharing au-thentication data in support of individual accountability.  If authentica-

tion data are known to individuals or are easily extracted from storage,
then individuals may voluntarily make copies and thus circumvent this
system goal (see Chapter 4).  Even when secrets are stored in physical

tokens, the tokens may be loaned to others, in violation of procedural
aspects of a security policy.Sometimes authentication is based not on the possession of a secretvalue but on the possession of a physical item that is presumed to be
resistant to tampering and forgery.  An authentication system may be

attacked successfully if the assumptions about its tamper- or forgery-
resistance prove to be false.  In many cases, the security of the credential

is derived from the integrity of the data associated with the credential,
rather than on the physical characteristics of the credential.  For example,

a physical credential might contain digitally signed data attesting to a
name and employee ID number.  Verification of the credential, and thus

authentication of the individual possessing the credential, would be based
on successful validation of the digital signature associated with the data.
Careful use of public key cryptography can make the digital signature
highly secure, protecting against modification of the signed data or cre-
ation of new, fake signed data.  However, it may be quite feasible to copy

the data to additional physical credentials.  These duplicate credentials

represent a form of forgery.  Unless the signed data are linked directly to28In 1993 in Connecticut, a fraudulent ATM was installed in a shopping center.  See the
RISKS digest for more details; available online at <http://catless.ncl.ac.uk/Risks/14.60.html#subj3>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.134WHO GOES THERE?the holder of the credential (for example, by means of biometrics), thissort of forgery by duplication is a security concern.Biometric authentication also relies on the possession of a physicalitem that is presumed to be resistant to tampering and forgery, namely
some measurable part of an individualÕs body or behavior.  Examples
include fingerprints, voiceprints, hand geometry, iris patterns, and so on.
Biometric values are not secrets; we leave fingerprints on many items that
we touch, our voices and facial images may be recorded, and so on.29Thus, the security of biometric authentication systems relies extensively
on the integrity of the process used to capture the biometric values and on
the initial, accurate binding of those values to an identifier.  It is critical

that later instances of the biometric capture process ensure that it is a real
person whose biometric features are being capturedÑthis may mean re-quiring biometric sensors to be continuously monitored by humans.  Bio-

metric authentication systems may be fooled by fake body parts or photo-
graphs created to mimic the body parts of real individuals.30  They also
may be attacked by capturing the digitized representation of a biometric
feature for an individual and injecting it into the system, claiming that the
data are a real scan of some biometric feature.The preceding analysis of the security vulnerabilities of classes ofauthentication technologies, while accurate, does not determine whether
any of these technologies is suitable for use in any specific context.
Instead, a candidate technology must be evaluated relative to a per-
ceived threat in order to determine whether the technology is adequately

secure.  Nonetheless, it is important to understand these vulnerabilities
29ÒThe physical characteristics of a personÕs voice, its tone and manner, as opposed tothe content of a specific conversation, are constantly exposed to the public.  Like a man
Õsfacial characteristics, or handwriting, his voice is repeatedly produced for others to hear.No person can have a reasonable expectation that others will not know the sound of his
voice, any more than he can reasonably expect that his face will be a mystery to the world.ÓÑJustice Potter Stewart for the majority in U.S. v. 
Dionisio, 410 U.S. 1, 1973.30See T. Matsumoto, H. Matsumoto, K. Yamada, and S. Hoshino,   
ÒImpact of ArtificialGummy Fingers on Fingerprint Systems,Ó Proceedings of the International Society for OpticalEngineering (SPIE) 4677 (January  2002), available online at <http://research.nii.ac.jp/kaken-
johogaku/reports/H13_overview/A04-00-1.pdf>; L. Thalheim, J. Krissler, and P. Ziegler,
ÒBiometric Access Protection Devices and Their Programs Put to the Test,Ó cÕt Magazine 11
(May 21, 2002):114, available online at <http://www.heise.de/ct/english/02/11/114>; T.van der Putte and J. Keuning,  ÒBiometrical Fingerprint Recognition: DonÕt Get Your Fin-gers Burned,Ó Proceedings of the IFIP TC8/WG8.8 Fourth Working Conference on Smart CardResearch and Advanced Applications, Kluwer Academic Publishers, Dordrecht, The Nether-
lands, 2000, pp. 289-303,  available online at <http://www.keuning.com/biometry/

Biometrical_Fingerprint_Recognition.pdf>; and D. Blackburn, M. Bone, P. Grother, and
J. Phillips, Facial Recognition Vendor Test 2000: Evaluation Report, U.S. Department of De-fense, January 2001, available online at <www.frvt.org>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES135when evaluating the security characteristics of individual authentica-tion technologies.COST CONSIDERATIONS FOR INDIVIDUAL AUTHENTICATION TECHNOLOGIES
Costs are an important factor in the selection of authentication tech-nologies.  These costs take many forms.  Capital costs are associated with
the acquisition of any hardware or software needed for an authentication
technology.  The hardware and software costs may be a function of the

number of individuals being authenticated, or of the number of points at
which authentication takes place, or both.  For example, an authentication

system that makes use of hardware tokens has a per-user cost, since each
user must have his or her own token, and each device that will authenti-
cate the user (for example, each desktop or laptop computer) must be
equipped with a reader for the token.  A biometric authentication system
might typically require readers at each point where individuals are au-
thenticated, and there would be a per-device, not a per-person cost.  A

software-based authentication system may impose costs only for each
computer, not each individual, although licensing terms directed by a
vendor might translate into per-user costs as well.Many authentication systems also make use of some common infra-structure, which also has associated hardware and software acquisition
costs.  The infrastructure may be offline and infrequently used, or it may

be online and require constant availability.  In the online case, it may be

necessary to acquire replicated components of the infrastructure, to geo-
graphically disperse these components, and to arrange for uninterruptible
power supplies, in order to ensure high availability.  The key distribution

center component of a Kerberos system (see Box 5.7) and the ACE/Server
used by the SecurID system are examples of the latter sort of infrastruc-
ture.  A certificate authority in a PKI is an example of the online type of

infrastructure component.Operation of an authentication system involves labor costs of vari-ous types.  Help desks must be manned to respond to users
Õ questions
and problems.  If the system relies on secret values that users are re-

quired to remember, the help desk will have to interact with users to
reset forgotten secret values.  If the system makes use of hardware to-

kens, provisions will have to be made to replace lost or stolen tokens.
Users and system administrators must be trained to work with an au-
thentication technology and with that technologyÕs interaction withvarying operating systems and applications.  Application developers

must learn how to make use of an authentication technology and to
integrate it into their applications.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.136WHO GOES THERE?Box 5.7KerberosThe Kerberos authentication system was developed at the MassachusettsInstitute of Technology (MIT) in about 1985 as part of MITÕs Project Athena.1  AKerberos system consists of three parties: (1) a client, which attempts to authenti-cate itself to (2) a server and (3) a key distribution center (KDC), also known as a
Kerberos server, which acts as an intermediary for the authentication.Every Kerberos ÒprincipalÓ (either client or server) has a secret key known
both by the principal and the KDC.  Most Kerberos systems emulate a traditional

password-based authentication system inasmuch as the human end user knows asecret password. However, unlike a traditional password-based authentication
system, this password is never transmitted over a network and is therefore not
easily stolen by eavesdropping. When used, this password is hashed into a secret
key that is used to complete the Kerberos protocol.When a client wishes to make use of a server, it queries the KDC for a ÒticketÓfor the server. The ticket contains the identity of the client along with a time stampand validity period. This ticket is encrypted by the KDC so that only the server candecrypt it. Among the information encrypted into the ticket is a randomly generated
ÒsessionÓ key. The ticket and the session key are then provided to the client by way
of a Ticket-Granting Ticket service.The Ticket-Granting Ticket service is a Kerberos service in which the KDC isalso the server. In general, a ticket for the Ticket-Granting Ticket service is ob-tained at log-in time. Additional tickets are then obtained for other services fromthe KDC as needed. These additional tickets, encrypted in the session key of the
Ticket-Granting Ticket instead of the clientÕs password, are sent by the KDC to theclient. This permits the Kerberos system to obtain as many tickets as are neces-
sary without the clientÕs secret key (password) needing to be kept in memory onthe clientÕs workstation (computer).1For more information on Project Athena, see <http://Web.mit.edu/olh/Welcome/intro.html>.This brief discussion illustrates how complex it can be to evaluate thecost of an individual authentication system.  Initial capital outlays are

greater for some types of systems; ongoing costs of other types of systems
may eventually outweigh these capital outlays.  Different contexts merit

different levels of security and will tolerate different costs for authentica-
tion technology.  Thus, there can be no single right answer to the question

of how much authentication technology should cost.CONCLUDING REMARKSThe preceding chapters describe three different conceptual types ofauthentication (identity, attribute, and individual), and this chapter fo-Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION TECHNOLOGIES137A clientÕs secret key, as mentioned earlier, is typically known to the client asa password, which is hashed into the key when needed. Servers store their secretkey (needed to decrypt incoming tickets to obtain the stored session key) some-where on the server, typically in a file.Kerberos provides for clients to authenticate themselves to servers and forservers to be authenticated to clients. At the end of a successful authentication
transaction, the session key created by the KDC and stored in the ticket is known
to both the client and the server. This key can then be used to provide integrityprotection for information sent between client and server and for the encryption(confidentiality) of information transfer as well.Kerberos is an online system. The KDC is required to be involved in mosttransactions.2 A ticket is typically issued with a lifetime measured in hours.3 Shortticket lifetime simplifies the revocation problem. If an entityÕs key is compromised,all that needs to be done is to change that key in the KDC (and on the server if aserver key is compromised). The use of symmetric algorithms means that Ker-beros computations are much faster than the equivalent public key operations.Kerberos still relies on the limited human storage capacity for passwords.  It
also relies on the userÕs computer for computational ability, which means that itmust be trusted by the user.Recent work on the Kerberos protocol introduces the use of public key en-cryption into the initial Ticket Granting Ticket (log-in) transaction. This means thatonly clientsÕ public keys need to be stored by the KDC, reducing the amount of
secret information that needs to be protected by the KDC.2But this is not true for all transactions. Once a ticket is obtained by a client for a particularservice, that ticket can be reused for multiple transactions without further involvement of the
KDC.3In version 4 Kerberos, the maximum ticket lifetime was 21 hours. Version 5 lifts this restric-tion, but it is still a good idea to limit ticket lifetime.cuses on the technologies that go into building an authentication systemand some of the technology-related decisions that must be made.  Some of

these decisions will bear on the privacy implications of the overall sys-
tem.  In general, decentralized systems tend to be more preserving of

privacy, but the core authentication technologies that make up authenti-
cation systems tend to be privacy-neutral.  What matters most in terms of

privacy are design, implementation, and policy choices, as described else-
where in this report.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.1386Authentication, Privacy, and the
Roles of GovernmentGovernment institutions play multiple roles in the area where au-thentication technologies intersect with privacy concerns.  Not

only do all levels of government (state, federal, and local) useauthentication systems, but the technologies are employed within and
across government institutions at each level as well.  Furthermore, gov-

ernment plays multiple roles in the authentication process.  As a relying

party, government uses authentication technologies for electronic gov-
ernment applications and for physical and systems security applications.
Given the size of its workforce and its user base, government is a signifi-
cant user of these technologies.  The governmentÕs role in the authentica-
tion process (as regulator, issuer, and relying party) is important, since so
many forms of authentication or identification rely on some form of gov-
ernment-issued identity or identifier.It is not surprising, therefore, that government organizations haveconflicting and supporting roles in authentication.  As an example, the

Social Security Administration (SSA) fills all three roles simultaneously.
The Social Security number (SSN) was designed by SSA for its own use in
recording earnings.  For its intended purpose, in conjunction with other

SSA business processes and controls, the SSNÕs security level meets the
SSAÕs requirements.  In this case, the SSA is both the issuing and the

relying party, so the incentives for risk mitigation are properly aligned.When the parties that issue and rely on an identifier are different, theincentives for risk mitigation are not necessarily aligned.  For instance,
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT139secondary uses of the SSN have proliferated, beginning with their use bythe Internal Revenue Service (IRS) and state taxation agencies, and now
extending to many private sector organizations such as credit reporting

agencies.  There is an inherent conflict between the higher confidence levels

desired by the relying party and the extra cost imposed on the issuer to
meet this confidence level.  For example, it is probably neither reasonable

nor cost-effective for the SSA to change its SSN issuance and maintenance
processes in order to help the private sector manage business risk around
creditworthiness just because most credit bureaus use SSNs as unique iden-
tifiers for credit history.  An examination of the various roles that govern-

ment fills in authentication processes and privacy protection, anchored by
specific examples, helps to explain this complexity.The issuance of IDs illustrates how different levels of governmentinteract with the public through specific programs for sometimes unique
reasons.  The principle of federalism
Ñthe division (and in some casesoverlap) of government responsibilities among federal, state, and local
government,1 designed into the U.S. constitutional form of government
Ñhelps to explain why it is important not to view the government role in
any area as monolithic.By design, as protected by public law and policy, government activi-ties are assumed to be fair, impartial, and immune from commercial ma-
nipulation.2   This legal and policy context for the government manage-
ment of information and related technology makes government use of
these technologies a special case and certainly different from their use by
the private sector.  Individuals who are citizens of or permanent residents

in the United States also have a unique relationship with government
agencies.  Sometimes by choice, and in many instances by compulsion,

citizens and residents are both participants in governance and users of
government goods and services.  For instance, citizens may choose to

comment on proposed changes in information-reporting requirements1One insightful definition of federalism and its complexity comes from Woodrow Wil-son:  ÒTo make town, city, county, state, and federal government live with a like strengthand an equally assured healthfulness, keeping each unquestionably its own master and yetmaking all interdependent and cooperative, combining independence and mutual helpful-
ness.Ó  See Woodrow Wilson, 
ÒThe Study of Administration,Ó Political Science Quarterly2(June 1887):197-222, quoted in Dell S. Wright, ÒA Century of Intergovernmental Adminis-trative State: WilsonÕs Federalism, New Deal Intergovernmental Relations, and Contempo-rary Intergovernmental Management,Ó A Centennial History of the American AdministrativeState, Ralph Clark Chandler, ed.  New York, N.Y., The Free Press, 1987, p. 220.
2Charles Goodsell.  The Case for Bureaucracy, 3rd ed.  New York, N.Y., Seven BridgesPress, 1994.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.140WHO GOES THERE?such as the questions on census forms.  Alternatively, some relationshipswith government are straightforward and contractual, just as with a busi-
ness.  For example, when it is time to repay student loans, beneficiaries

have a legal obligation to return the money that they borrowed from the
government with interest.Unlike private sector organizations, though, public agencies cannotchoose their customers.  Public law and regulation instead of business

plans dictate whether an individual is a beneficiary or a regulated entity.
While private sector organizations may only want an individualÕs busi-ness under certain conditions (for example, one can only get a mortgage
or a credit card upon meeting certain eligibility criteria), most citizens
interact with government organizations from cradle to grave.  From the

recording of birth to the issuance of death certificatesÑand annually inbetween for some government programsÑcitizensÕ interaction with gov-
ernment is virtually inescapable.Finding 6.1: Many agencies at different levels of governmenthave multiple, and sometimes conflicting, roles in electronic
authentication.  They can be  regulators of private sector behav-

ior, issuers of identity documents or identifiers, and also  rely-

ing parties for service delivery.REGULATOR OF PRIVATE SECTOR AND PUBLIC AGENCYBEHAVIORS AND PROCESSESThe government acts as a regulator of multiple sectors, includinghealth and medical services, financial services, and education.  For this

analysis, these regulatory activities are put in three groups: (1) govern-
ment-wide law and policy that are focused internally on the activities of
federal agencies in a particular domain (for example, privacy, electronic
government, or computer security); (2) program- or agency-specific law
and policy that apply to specific types of transactions but may cut across
a number of government agency and private sector organization bound-
aries for transactions such as federally funded health care or higher edu-
cation; and (3) public law or policy intended to regulate the information
management activities of the private sector broadly or more specifically
in certain areas such as financial services.  This section summarizes some
of this public law and government policy and concludes by identifying
some pending legislation that is relevant to privacy and authentication.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT141Government-wide Law and PolicyPrivacy Act and Computer Matching ActA recent Government Accounting Office (GAO) report3 refers to the
Privacy Act of 1974 (5 U.S.C. Sec. 552a)4 as the 
Òprimary [U.S.] law regu-lating the federal governmentÕs collection and maintenance of personalinformation.Ó  Generally speaking, the Privacy Act aimed at balancing the
federal governmentÕs need to maintain information about individuals
with the rights of individuals to be protected against unwanted invasions
of their privacy.  The act attempts to regulate the collection, maintenance,

use, and dissemination of personal information by federal government
agencies.  As one source summarizes, the act provides privacy protection

in three ways:1.It sustains some traditional major privacy principles.  For example,
an agency shall maintain no record describing how any individual exer-
cises rights guaranteed by the First Amendment unless expressly autho-
rized by statute or by the individual about whom the record is main-
tained or unless pertinent to and within the scope of an authorized law
enforcement activity.2.It provides an individual who is a citizen of the United States, or an
alien lawfully admitted for permanent residence, with access and emen-
dation arrangements for records maintained on him or her by most, but
not all, federal agencies.  General exemptions in this regard are provided

for systems of records maintained by the Central Intelligence Agency and
federal criminal law enforcement agencies.3.The act embodies a number of principles of fair information prac-
tice.  For example, it sets certain conditions concerning the disclosure of

personally identifiable information; prescribes requirements for the ac-
counting of certain disclosures of such information; requires agencies to
Òcollect information to the greatest extent practicable directly from thesubject individual when the information may result in adverse determi-
nations about an individualÕs rights, benefits, and privileges under Fed-eral programsÓ; requires agencies to specify their authority and purposesfor collecting personally identifiable information from an individual; re-
quires agencies to Òmaintain all records which are used by the agency in3Government Accounting Office (GAO). Internet Privacy:  AgenciesÕ Efforts to Implement
OMBÕs Privacy Policy [GGD-00-191].  Washington, D.C.,  GAO, September 2000.  Available
online at <http://www.gao.gov/archive/2000/gg00191.pdf>.4The full text of the act itself is available online at <http://www.usdoj.gov/04foia/privstat.htm>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.142WHO GOES THERE?making any determination about any individual with such accuracy, rel-evance, timeliness, and completeness as is reasonably necessary to assure
fairness to the individual in the determinationÓ; and provides civil andcriminal enforcement arrangements.5However, passed in great haste during the final week of the 93rdCongress, the ÒActÕs imprecise language, limited legislative history, andsomewhat outdated regulatory guidelines have rendered it a difficult stat-
ute to decipher and apply.Ó6One major complicating factor in the implementation and regulationof Privacy Act provisions has been the Òlack of specific mechanisms foroversight.Ó7  Indeed, some have cited the absence of a central agency for
the oversight and coordination of the nationÕs privacy matters as a majorreason for the ineffectiveness of American privacy laws in general.8  In
comparison, several other nations have dedicated whole departments and
appointed high-level officials to oversee their privacy matters.The Computer Matching and Privacy Protection Act of 1988 (PL 100-503) amended the Privacy Act of 1974 by adding new provisions regulat-
ing the federal governmentÕs use of computer matchingÑthe computer-ized comparison of information about individuals, usually for the purpose

of determining the eligibility of those individuals for benefits.  The main

provisions of the act include the following:¥Give individuals an opportunity to receive notice of computermatching and to contest information before having a benefit denied or
terminated;¥Require that federal agencies engaged in matching activities estab-lish data protection boards to oversee matching activities;¥Require federal agencies to verify the findings of computer match-5Text for these three items is adapted from Harold C. Relyea,  The Privacy Act:  Emerging
Issues and Related Legislation, Congressional Research Service (CRS) Report RL30824,  Wash-ington, D.C.,  CRS, Library of Congress, September 2000.
6Department of Justice.  ÒOverview of the Privacy Act of 1974Ó (Introduction), 2000.
Available online at <http://www.usdoj.gov/04foia/1974intro.htm>.7Charles R. Booz.  ÒElectronic Records and the Right to Privacy:  At the Core.
Ó Informa-tion Management Journal 35 (3): 18.
8See David H. Flaherty, ÒThe Need for an American Privacy Protection Commission,ÓGovernment Information Quarterly 1(3)(1984):235-258.  In later work, he also observes that
design of the system and policy choices are crucial to privacy protection.  See, for example,
ÒPrivacy Impact Assessments: An Essential Tool For Data Protection,Ó presentation to a
plenary session ÒNew Technologies, Security and FreedomÓ at the 22nd Annual Meeting ofPrivacy and Data Protection Officials held in Venice, Italy, September 27-30, 2000. Availableonline at <http://www.anu.edu.au/people/Roger.Clarke/DV/PIAsFlaherty.html>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT143ing programs before suspending, denying, or otherwise ÒadverselyÓ af-
fecting an individualÕs benefits; and¥Require agencies to negotiate written agreements with other agen-cies participating in the matching programs.An amendment to the act that was passed in 1990 somewhat alteredthe original actÕs due process provisions.  Specifically, the amendment
changed some of the details regarding subject notification of adverse find-
ings and gave data protection boards the ability to waive independent
verification of information under certain circumstances.In December 2000, the Office of Management and Budget (OMB) is-sued a memorandum reminding federal agencies of the actÕs require-ments.9,10  According to the memorandum, as 
Ògovernment increasinglymoves to electronic collection and dissemination of data, under the Gov-
ernment Paperwork Elimination Act and other programs, opportunities
to share data across agencies will likely increase.Ó  Therefore, 
Òagenciesmust pay close attention to handling responsibly their own data and the
data they share with or receive from other agencies.ÓComputer Security Act and Recent AmendmentsThe Computer Security Act of 1987 (PL 100-235) addressed the im-portance of ensuring and improving the security and privacy of sensitive
information in federal computer systems.  The act required that the Na-

tional Institute of Standards and Technology (formerly the National Bu-
reau of Standards) develop standards and guidelines for computer sys-
tems to control loss and unauthorized modification or disclosure of
sensitive information and to prevent computer-related fraud and misuse.
The act also required that operators of federal computer systems, includ-
ing both federal agencies and their contractors, establish security plans.11Additionally, the law stipulated that agency plans for protecting sensitive
information and systems be cost-effective, and most important, it estab-
lished a standard for risk mitigation.  Specifically, the law says that fed-

eral agencies must Òestablish a plan for the security and privacy of eachFederal computer system identified by that agency pursuant to subsec-
tion (a) that is commensurate with the risk and magnitude or the harm9Office of Management and Budget (OMB). ÒGuidance on Inter-Agency Sharing of Per-sonal DataÑProtecting Personal Privacy,Ó M-01-05, December 20.  Available online at
<http://www.whitehouse.gov/omb/memoranda/m01-05.html>.10This and related activities at OMB were part of the context that led to this study.11For the full text of the act, see <http://www.epic.org/crypto/csa/csa.html>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.144WHO GOES THERE?resulting from the loss, misuse, or unauthorized access to or modificationof the information contained in such system.ÓGovernment Paperwork Elimination ActPart of the impetus for federal agencies to move quickly toward elec-tronic government (and therefore authentication, to an extent) is public
law.  Enacted in 1998, the Government Paperwork Elimination Act

(GPEA)12 both requires federal agencies to move from paper-based to
electronic transactions with the public and provides some of the enablers
necessary to make such a transition.   It also amplifies federal privacy

protections regarding sensitive data collected during the electronic au-
thentication process.Following on the tradition of the Paperwork Reduction Act (PRA)13of 1995, one of the goals of GPEA is to minimize the burden imposed on
the public by federal paperwork requirements.  More specifically, though,

the goal of both the PRA and GPEA is for federal agencies to minimize the
information-collection burden on the public, regardless of whether the
collection instrument is a paper form, an electronic transaction, or a phone

survey.14  GPEA recognizes the benefits to both federal agencies and the
public of moving from paper-based to electronic transactions, including
reduced error rates, lower processing costs, and improved customer satis-
faction.  As a result, GPEA required agencies by the end of Fiscal Year

2003 to provide for the electronic maintenance, submission, or transaction
of information as a substitute for paper where practicable.  Additionally,

the law stipulates that agencies use and accept electronic signatures in
this process.GPEA goes so far as to define the term Òelectronic signatureÓ and to
legitimate the legal force of such signatures in the scope of public interac-
tions with federal agencies.15  In doing so, federal law and policy help to
clear up what has historically been the subject of some debate among
federal agencies about what is legally sufficient to ÒsignÓ a transaction
with a member of the public.  Section 1709(1) of GPEA reads:
The term Òelectronic signatureÓ means a method of signing an electronic
message thatÑ(A) identifies and authenticates a particular person as the12Government Paperwork Elimination Act of 1998 (PL 105-277, Div. C, tit XVII).13Paperwork Reduction Act of 1995 (44 U.S.C. Chapter 35).14For background on the goals of GPEA, see Senate Report 105-355, and for backgroundon the PRA and GPEA, see OMB, ÒProcedures and Guidance; Implementation of theGPEA,Ó Federal Register, May 2, 2000.
15For more on electronic signatures, see the discussion of the Electronic Signatures inGlobal and National Commerce Act (E-SIGN) of 2000 later in this chapter.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT145source of the electronic message; and (B) indicates such personÕs ap-proval of the information contained in the electronic message.It is important to note as well what the definition does not do, whichis to specify the technologies or policies that an agency might use to
comply with this definition.  The OMB implementation guidance to fed-

eral agencies cites examples of appropriate technologiesÑshared secretssuch as PINs and passwords, digitized signatures or biometrics such as
fingerprints, and cryptographic digital signatures such as those used in
PKIs.16  The OMB guidance does, though, suggest an analytical frame-
work for an agency to use to help determine the risk inherent in the
transaction it hopes to automate and which authentication technology
might most appropriately mitigate that risk.GPEA also cleared up what might otherwise have been a contentiousdebate among federal agency general counsel offices throughout Wash-
ington, D.C., by addressing directly the enforceability of electronic signa-
tures.  For transactions involving electronic records submitted or main-

tained consistent with the policy enabled by GPEA and using electronic
signatures in accordance with the same policy, neither the electronic
record nor the signature is to be denied legal effect just because it is
electronic instead of paper.  Both Congress and the OMB state that the

intent is to prevent agencies or the public from reverting to paper instead
of electronic transactions and signatures because of concerns that any
subsequent prosecutionÑin a benefits fraud case, for instanceÑmight bethrown out of court.One other provision of the law pertinent to the topic of this studyrelates to the protection of information collected in the course of provid-
ing electronic signatures services.  Consistent with the fair information

practices (described in Chapter 3 of this report) and the Privacy Act,
GPEA requires that information gathered from the public to facilitate
electronic signatures services be disclosed only for that purpose.Agency- or Program-Specific Law and PoliciesFamily Educational Rights and Privacy ActThe Family Educational Rights and Privacy Act (FERPA) of 1974 (20U.S.C. ¤ 1232g; 34 CFR Part 99) is a federal law designed to protect the
privacy of a studentÕs education records.  The law applies to all schools
that receive funds under an applicable program of the U.S. Department of16See the Web site <http://www.whitehouse.gov/omb/fedreg/gpea2.html>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.146WHO GOES THERE?Education.  FERPA gives parents certain rights with respect to their
childrenÕs education records (for example, the right to inspect and reviewall of the studentÕs education records maintained by the school and theright to request that a school correct records believed to be inaccurate or
misleading).  These rights transfer to the student, or former student, who
has reached the age of 18 or is attending any school beyond the high
school level.Under the law, schools must also have written permission from theparent or eligible student before releasing any information from a
studentÕs record.  Schools may disclose, with notification, directory-type
information, such as a studentÕs name, address, telephone number, dateand place of birth, and so on.17  As schools move toward authentication
technologies such as PKI, issues arise as to how FERPA applies.18Health Insurance Portability and Accountability ActThe Health Insurance Portability and Accountability Act (HIPAA),19or PL 104-191, was passed by Congress and became law in 1996.  Its
purpose was, among other things, to improve the continuity of health
insurance coverage and the efficiency in health care delivery by mandat-
ing standards for electronic data interchanges and to protect the confiden-
tiality and security of health information.  Title I of HIPAA deals with

health insurance access, portability, and renewability (for example, when
a worker loses or changes his or her job), while Title II of the act contains
what are referred to as the actÕs administrative simplification provisions.These provisions fall roughly into three categories: transactions and code
set standards,20 privacy standard,
21 and security standard.
22 The privacy
standard, along with the security standard, provides rules for legal con-
trols over patientsÕ medical records.
17The full text of the act is available online at <http://www.epic.org/privacy/educa-tion/ferpa.html>.18EDUCAUSE, an organization that promotes information technology in higher educa-tion, has looked at this and related issues in its initiative PKI for Networked Higher Educa-tion. See the Web site <http://www.educause.edu/netatedu/groups/pki/> for more in-formation.19The full text of the act is available online at <http://aspe.os.dhhs.gov/admnsimp/pl104191.htm>.20Health Insurance Reform: Standards for Electronic Transactions. 45 CFR Parts 160 and162.  Federal Register: August 17, 2000 (Volume 65, Number 160), pp. 50312-50372.
21Standards for Privacy of Individually Identifiable Health Information. 45 CFR Parts160 through 164. Federal Register: December 28, 2000 (Volume 65, Number 250), pp. 82461-
82510.22Security and Electronic Signature Standards: Proposed Rule. 45 CFR Part 142. FederalRegister: August 12, 1998 (Volume 63, Number 155), pp. 43241-43280.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT147The process for creating these standards, or rules, has been fairlycomplicated.  Indeed, according to the Department of Health and Human

Services (HHS), the process is a Òdeliberate [one] designed to achieveconsensus within HHS and across other Federal departments.Ó23  How-
ever, in general, once a proposed rule has made its way through several
federal groups (such as the HHS Data CouncilÕs Committee on HealthData Standards, advisers to the secretary of HHS, and the OMB), the
proposal is published and comments from the public are solicited.  These

comments, which are also open to public view, are then Òanalyzed andconsidered in the development of the final rules.Ó24HIPAA is aimed at all organizations that store, process, or transmitelectronic health care information through which an individual might be
identified.  Accordingly, the act applies to virtually all health care organi-

zations, includingÑamong othersÑhealth plans (insurers), health careclearinghouses, health care providers (including health maintenance or-
ganizations, hospitals, clinics, physician group practices, and single-phy-
sician offices), billing agencies, and universities.HIPAA also provides for serious civil and criminal penalties for fail-ure to comply with the rules.  For example, the penalties include fines of

up to $25,000 for multiple violations of the same rule in one year, as well
as fines of up to $250,000 and up to 10 yearsÕ imprisonment for knowingly
misusing individually identifiable health information.The privacy rule formally defines Òprotected health information,Ówhich includes individual patient information such as name, Social Se-
curity number, address, and so on; and clinical information such as
disease, treatment, drugs, test results, and so on.  It permits disclosure

of this information for necessary treatment, payment, and operations
(TPO) functions. For all other uses, especially for fund-raising and mar-
keting functions, explicit authorization from the patient is required.
(There are exceptions, such as for military patients and for clinical re-
search, which is largely governed by the informed consent rule.)  If

information is provided to an organization not covered by HIPAA for
TPO functions (such as a bill collection agency), the rule requires ex-
plicit business associate (and possibly chain of trust) agreements that
make the recipients responsible for HIPAA-specified privacy and secu-
rity rules.The original privacy rule issued in December 2000 required collectingand tracking of a consent form signed by all patients that explained the
privacy practices of the institution providing care. Subsequently, a techni-23From the Web site <http://aspe.os.dhhs.gov/admnsimp/8steps.htm>.24Ibid.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.148WHO GOES THERE?cal correction proposed in January 200125 removed the consent form re-
quirement and replaced it with a privacy notice that does not require a
patientÕs signature.  Several basic rights for patients are provided in the
privacy rule: the right to access their records, the right to emend those
records, and the right to see what disclosures of their information have
been made. Institutions are required to employ a privacy officer, who
provides services related to the privacy rights of patients.Fundamental to the privacy rule are the Òminimum necessaryÓ and
Òneed to knowÓ principles. Based on these principles, the rule requires
institutions to develop and implement policies and procedures to define
formal role-based or user-based access authorization rules, and the secu-
rity rule requires assurance that these policies and procedures are being
followed.  Additionally, a common and uniform sanctions policy is re-

quired for addressing privacy and security policy violations.  Patient pri-
vacy has always been important in care institutions; the HIPAA privacy
rule formalizes the concept in a legal framework with significant penal-
ties for noncompliance.There are several complexities in meeting HIPAA regulations. If in-formation-access rules are incorrectly defined, the care process could be
adversely affected, an obviously unacceptable trade-off.  The roles of care

providers in an organization are fluid: nurses working in shifts or filling
in, on-call consultants rotating on a weekly basis, medical students on
monthly rotations, multiple physicians in consulting or specialty roles,
and so on.  Practically, it is very difficult to assign roles to a fine access

granularity and to implement such a system in mostly vendor-supported
and heterogeneous clinical application environments without raising the
risks to proper health care.Managing authorizations and tracking privacy notices are operationalchanges for institutions, but centrally tracking all disclosures for review
by the patient if requested is a difficult and costly problem in large insti-
tutions. In the context of an academic medical center, for example, HIPAA
remains vague in addressing the matter of information collected for and
by clinical trial and other kinds of research. Open questions about educa-
tional rounds and HIPAA were addressed in the latest rule making, and
there are other questions that may be clarified, but only later. The privacy
rule was required to be adopted by April 14, 2003, but it is likely that there
will be a gradual culture change in care environments toward better pri-
vacy protection.25Standards for Privacy of Individually Identifiable Health Information; Proposed Rule.45 CFR Parts 160 through 164. Federal Register: March 27, 2002 (Volume 67, Number 59),pp. 14775-14815.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT149Regulation of Private Sector Information Management ActivityElectronic Signatures in Global and National Commerce ActAimed at eliminating Òlegal barriers to the use of electronic technol-ogy to form and sign contracts, collect and store documents, and send and
receive notices and disclosures,Ó26 the Electronic Signatures in Global
and National Commerce (E-SIGN) Act (PL 106-229) became law in June
2000.  The act cleared the way for electronic signatures and contracts to

carry the same legal significance as that of traditional paper contracts and
handwritten signatures.27  Indeed, President Clinton signed the act into
law with a smart card.The E-SIGN Act includes consumer consent provisions that Òrequireinformation to be made available electronically only after the recipient
affirmatively consents to receive the information [in that manner].Ó28 In
fact, recipients must give this consent electronically as well, to ensure that
they possess the necessary technological capability (usually Internet ac-
cess and/or an e-mail account).  The act does not specify the use of any

particular technological solution; rather, it Òleav[es] those choices [up] tothe marketplace.Ó29  However, some critics view this aspect of the act as
being a very real disadvantage, fearing a Òmishmash of incompatiblesolutionsÓ30 and a 
Òstandards battle that could take years to resolve.Ó31In 2001, the Federal Trade Commission and the Department of Com-merce completed a congressionally mandated study32 on the impact of
the consumer consent provisions of the E-SIGN Act.  According to the
26OMB.  ÒGuidance on Implementing the Electronic Signatures in Global and Na-tional Commerce ActÓ (M-00-15), September 25, 2000.  Available online at <http://
www.whitehouse.gov/omb/memoranda/m00-15.html>.27Electronic signatures, in this context, are not the same as the digital signatures thatwere described in Chapter 5.  One of the critiques of this law is that electronic signatures do
not embody the same security methods and principles as digital signatures.28News Bytes News Networks. ÒE-SIGN Law Appears to Work Fine So FarÑGovÕtStudy.Ó June 27, 2001.
29John Schwartz. ÒE-Signatures Become Valid for Business.Ó New York Times. October 2,2000, p. C1.30Abby Ellin. ÒE-Sign on the Dotted Line.Ó Business 2.0, November, 2000.  Available
online at <http://www.business2.com/articles/mag/0,1640,8542,FF.html>.31Jesse Berst. ÒSign of Trouble:  The Problem with E-Signatures.
Ó ZDNet, July 17,2000.  Available online at <http://www.zdnet.com/anchordesk/stories/story/
0,10738,2604099,00.html>.32Federal Trade Commission (FTC) and Department of Commerce (DOC). Electronic Sig-natures in Global and National Commerce Act: The Consumer Consent Provision in Section101(c)(1)(C)(ii).  Washington, D.C., FTC, DOC, June 2001.  Available online at <http://
www.ftc.gov/os/2001/06/esign7.htm>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.150WHO GOES THERE?report, the actÕs consumer consent provisions seem to be Òworking satis-factorily.Ó  The report also suggests that 
Òimplementation issues [such assignature and authentication standards] should be worked out in the
marketplace and through state and federal regulationsÓ rather than by
congressional action to Òamend the statute.ÓFinancial Services Modernization ActThe Financial Services Modernization Act of 1999 (commonly referredto as the Gramm-Leach-Bliley Act) repealed long-standing legislation that
prohibited banks, securities firms, and insurance companies from ventur-
ing into business with each other.  Under Gramm-Leach-Bliley, these types

of companies may now join to form what the act calls financial holdings
companies (FHCs).  What this means with respect to personal privacy is

that, for instance, a consumerÕs bank can now develop a relationship andshare information with that same consumerÕs insurance company, bro-kerage firm, credit union, or other financial institution, creating new op-
portunities to cross-market services.33  However, Gramm-Leach-Bliley
also contains provisions for protecting consumersÕ personal information:
(1) consumers must be given notice of a companyÕs intent to share theirinformation with a third party and (2) they must be given the option to
decline such information sharing.Nevertheless, Gramm-Leach-Bliley is viewed by many privacy advo-cates as being rife with loopholesÑto the point of rendering any privacyprotections that it spells out moot.  For instance, Dan Gillmor, a technol-

ogy columnist, describes what he views as two major problems with the
act:¥Consumers must opt out of information sharingÑÒthat is, [con-sumers must] explicitly notify the institutions that they [do not] want
their data sharedÑrather than Ôopt in,Õ which is to allow data sharing
only after a consumer gives his or her permission.Ó¥ÒAffiliated companies (such as those under the same corporateumbrella or in joint marketing deals), so broadly defined as to be almost
meaningless, are exempt in every respect.Ó3433Cecelia Kempler and Robert Wood.  2000,  ÒLiving with the Gramm-Leach-Bliley Act.ÓWashington, D.C.,  LeBoeuf, Lamb, Greene & MacRae L.L.P.  Available online at <http://
www.insurelegal.com/livingwith031500.html#1>.34Dan Gillmor. ÒGramm-LeachÕs Privacy Problem.Ó  Computerworld 35 (30)(2001): 34.
Available online at <http://www.computerworld.com/cwi/story/0,1199,NAV47-74_STO62385,00.html>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT151Business, on the other hand, takes a different view of Gramm-Leach-Bliley.  Indeed, most financial holdings companies, while taking into

account the additional resources and time they must allocate to meeting
the actÕs privacy provisions, see the act as beneficial to their business.As Federal Reserve Vice Chairman Roger Ferguson put it in a recent
speech, Gramm-Leach-Bliley offers Òopportunities for banking organi-zations to expand their lines of business and their range of customer
services.Ó35Nevertheless, there has been recent activity within state legislaturesto strengthen or Òenhance the protections of Gramm-Leach-Bliley, includ-ing requiring actual consentÑor opt-inÑbefore information sharing,Ó36breeding significant concern among financial firms at the prospect of hav-ing to account for up to 50 different state privacy laws along with Gramm-
Leach-Bliley.37Another challenge has been the requirement for banks and financialinstitutions to notify customers, in readable and understandable fashion,
about their privacy policies.  Privacy advocates have noted that many of

the policy notifications were written in hard-to-understand legal language
and/or distributed in a way that did not draw attention to what was
being disclosed.Policy Activity in the Early 2000sThe last couple of years have seen a flurry of activity relating to bothauthentication and privacy issues.  Some of this legislation predated the

terrorist attacks of September 11, 2001, but a great deal of the new legisla-
tion is a direct result of the perceived inadequacies of government infor-
mation management practices leading up to the attacks.  This and the

following section include recently enacted legislation whose implementa-
tion continues to be planned.35Roger W. Ferguson, ÒUmbrella Supervision:  Emerging Approaches,
Ó speech before
the National Association of Urban Bankers, Urban Financial Services Coalition, San Fran-
cisco, Calif., May 26, 2001.  Available online at <http://www.federalreserve.gov/
boarddocs/speeches/2000/20000526.htm>.36M. Maureen Murphy. Privacy Protection for Customer Financial Information, Congres-sional Research Service (CRS) Report for Congress RS20185.  Washington, D.C.,  CRS, Li-
brary of Congress, August 2001.37Indeed, in June 2002, voters in North Dakota approved a referendum that would barthe sale of personal data collected by banks, credit unions, and other financial services firmsto third parties.  See P. Thibodeau, 
ÒN.D. Voters Side Overwhelmingly with Privacy,ÓComputerWorld,  June 12, 2002.
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.152WHO GOES THERE?USA PATRIOT ActPL 107-56, Uniting and Strengthening America by Providing Appro-priate Tools Required to Intercept and Obstruct Terrorism Act of 2001Ñthe USA PATRIOT Act
Ñgives federal officials broader authority to moni-tor communications and share information.38  Enacted in response to theterrorist attacks of September 11, 2001, the actÕs intent is to combat terror-ism, and it encompasses among other things criminal and foreign intelli-
gence investigations, money laundering, and alien terrorists.  The most

salient effects on individual privacy result from the increased surveil-
lance-related empowerment of government agencies.In general, there are four surveillance mechanisms provided by U.S.law: interception orders, search warrants, pen registers and trap and trace
orders, and subpoenas.  Interception orders have given authorities a clearly

delineated process for eavesdropping electronically on telephone and face-
to-face conversations and electronic communications in serious criminal
cases.  Search warrants allow the search of premises and the seizure of

tangible things, including records.  Pen registers and trap-and-trace orders
surreptitiously identify the source and destination of calls to and from a
particular telephone.  Subpoenas compel the production of evidence, in-

cluding physical items and testimony.  There are also differing standards of

proof for each of these mechanisms, based on whether domestic law en-
forcement or foreign intelligence agencies are conducting the surveillance.The USA PATRIOT Act has changed laws governing all four of themechanisms described above.  It permits pen registers and trap and trace
orders for electronic communications, as well as for phone conversations,
and authorizes nationwide execution of some surveillance-related court
orders.  Voice mail is treated like stored e-mail, which gives it less protec-

tion than telephone conversations.  The act allows interception of commu-

nications relevant to the investigation of a suspected computer trespasser
and permits sneak-and-peek search warrants and delayed (possibly for-
ever) notification of the subject of the search.  It also empowers the attor-

ney general to acquire education records relevant to the investigation of
terrorist offenses and to collect DNA samples from prisoners convicted of
certain terrorism-related offenses.The act reduces restrictions on foreign intelligence gathering (that is,on U.S. agencies gathering intelligence about other countries and their
citizens) within the United States and facilitates information sharing be-
tween foreign intelligence and domestic law enforcement agencies.  The
38See Charles Doyle, The USA PATRIOT Act: A Sketch, Congressional Research ServiceReport RL21203, and Charles Doyle, The USA PATRIOT Act: A Legal Analysis, Congres-sional Research Service Report RL31377.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT153act eased some of the restrictions on foreign intelligence gathering intro-duced during the 1970sÑit now permits roving surveillance, expands theallowable circumstances for a search or surveillance under the Foreign
Intelligence Surveillance Act of 1978 (FISA) (PL 95- 511, 92 Stat. 1783),39and sanctions court-ordered access to any tangible item held by lodging,
car rental, and storage businesses.  Roving surveillance means that war-

rants need not include specific information about the instrument or loca-
tion of a search.  Unlike domestic law enforcement, FISA searches do not

require probable cause of criminality; rather, they require only that the
target be suspected of being an agent of a foreign government.  Informa-

tion obtained from these searches may be shared with the FBI.  Likewise,

domestic law enforcement agencies may also share certain information
(e.g., criminal wiretap results) with foreign intelligence agencies.The USA PATRIOT Act also addresses the financial aspects of terror-ism, particularly money laundering.  It requires financial services profes-
sionals to file suspicious activity reports (SARs) in certain circumstances
and also increases Òspecial measuresÓ and 
Òdue diligence requirementsÓrelated to foreign money laundering.  As a result of the act, standards for
customer identification and record keeping have become more stringent,
and financial institutions are being encouraged to share information with
law enforcement agencies.E-Government ActThe E-Government Act of 2002 (PL 107-347) provides further impetusfor the Government Paperwork Elimination Act  of 1998 to enable elec-

tronic government at the federal level.  The act also authorizes increased

funding for e-government projects, creates an administrator for a new e-
government office within the OMB, extends provisions of the Govern-
ment Information Security Reform Act of 2000 (PL 106-398, Subtitle G,
¤1061-1065),40 provides uniform safeguards to protect the confidentiality39For more information on FISA, see <http://www.eff.org/Censorship/Terrorism_militias/fisa_faq.html> and the law itself at <http://www4.law.cornell.edu/uscode/50/ch36.html>.40The Government Information Security Reform Act (GISRA) of 2000 required agenciesto report annually to OMB on the security of their information systems and to make infor-mation system security part of their regular process of doing business (e.g., in budget re-
quests).  Under a sunset provision, GISRA was originally intended to expire in November
2002; however, the Federal Information Security Management Act (H.R. 3844), which waslater incorporated into the E-Government Act, made the GISRA provisions mentioned above
permanent.  See Judi Hasson, 
ÒEgov Agenda Takes Shape,Ó Federal Computer Week, Decem-ber 2, 2002.  Available online at <http://www.fcw.com/fcw/articles/2002/1202/news-
egov-12-02-02.asp>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.154WHO GOES THERE?of information collected from the public for statistical purposes, and re-quires the issuance of government-wide policies for standards for federal
Web site usability and for records management for federal Web sites.
Especially relevant for this report, the law includes provisions on elec-
tronic signature technologies and privacy impact analyses.The provision concerning electronic signature technologies is in-tended to promote the compatibility of agency solutions.  Executive agen-

cies must ensure that their use and acceptance of electronic signatures to
secure electronic transactions with the federal government are compat-
ible with the pertinent policies issued by the OMB.  The law further desig-

nates the General Services Administration (GSA) as the lead federal
agency to create a framework for the interoperability of electronic signa-
tures solutions, which is to include digital signatures.41The privacy provisions of the act recognize that more citizen-centerede-government requires an exchange of personally identifiable informa-
tion between users and federal agencies.  In response, the act requires that

agencies conduct privacy impact statements when developing or procur-
ing a new information system.  While the act leaves the details of the

content of privacy impact statements to the OMB to develop, it is reason-
able to assume that the OMB will use the best practice of the Chief Infor-
mation Officers (CIO) Council as a starting point for this policy.42As of this writing, there are still many implementation details to beworked out, but this new law clearly provides more tools for agencies
that seek to implement authentication technologies and to consider the
privacy implications of those decisions.Homeland Security Act of 2002The Homeland Security Act of 2002 (PL 107-296) establishes a cabi-net-level Department of Homeland Security, bringing together a myriad
of federal agencies currently spread among a number of federal-level
cabinet agencies and executive branch organizations.  As a consumer of

intelligence data from the FBI and CIA as well as a coordinator of the
dissemination of such data throughout the federal government and to
state and local governments, the new Department of Homeland Security
will have significant information management responsibilities that have
both authentication and privacy implications.  Much of this information

management would appear to fall under the purview of the undersecretary41 GSA
Õs Federal Bridge Certification Authority is a move in this direction; for moreinformation, see <http://www.cio.gov/fbca/>.42See the CIO CouncilÕs Web site for information on the IRSÕs implementation of theprivacy impact statement as a best practice.  Available online at <http://www.cio.gov/
documents/pia_for_it_irs_model.pdf>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT155for information analysis and infrastructure protection; the delegation ofsuch responsibilities requires more scrutiny.SummaryIn recent years, the desire for highly integrated electronic govern-ment is driving government organizations toward interagency and inter-
governmental authentication technology and policy solutions.43  The Ac-cess Certificates for Electronic Services (ACES) system, described later in
this chapter, is an example of such a proposed solution.  However, com-

plications arise when this type of solution is undertaken in the public
sector.  For example, public sector authentication solutions often involve

significant roles for private sector trusted third parties, complicating roles
and responsibilities and therefore accountability.  In addition, single-sign-

on approaches allow interaction with multiple government agencies, but
might lead to many of the privacy concerns cited in this report.  It is not

clear that the current privacy policy framework is sufficiently robust or
flexible to provide the privacy protections needed to accommodate these
approaches (see Chapter 3 in this report for more on the current state of
privacy law and policy in the United States).  While there may be a desire

for a simple, secure, privacy-preserving means by which citizens can in-
teract with multiple government agencies, it is difficult to satisfy all of
these criteria simultaneously.  Indeed, as is clear from the discussion in

this chapter, privacy law and policy in the United States tend to be with-
out overarching or apparent unifying principles.  The lack of cohesive-

ness in the legal and policy framework could lead to gaps, inconsisten-
cies, and overlaps, making compliance difficult and potentially more
expensive.GOVERNMENT AS ISSUER OF IDENTITY DOCUMENTSThe preceding sections addressed the first role of government, asregulator, and this section discusses its second role with respect to au-
thenticationÑas an issuer of identity documents, often in conjunctionwith the private sector.  Anyone who has traveled on a commercial airline

since September 11, 2001, has a sense of the unique role that government
fills in issuing identification documents.  The airlines, enforcing regula-

tions issued by the federal government, are quite clear in their instruc-43See Computer Science and Telecommunications Board, National Research Council, In-formation Technology Research, Innovation, and E-Government, Washington, D.C., NationalAcademy Press, 2002, for a broad look at e-government innovation and approaches that canhelp accelerate innovation in government.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.156WHO GOES THERE?tions; passengers must present a Ògovernment-issued photo ID.Ó44  Aphoto ID issued by a travelerÕs employer is not sufficient.  It must be
government-issued.  The government is able to compel individuals to

hold certain IDs and follow specified processes in order to help ensure the
integrity of the ID issuing process.  As a result, government-issued IDs

are presumed to be of higher quality than those issued by private organi-
zations (although fraudulent government IDs are demonstrably not hard
to come by).  (See Box 6.1 for a general discussion of credentials presented

by those wishing to be authenticated.)The integrity of any authentication system that relies on a govern-ment-issued identifier depends on the integrity of a small number of
foundation ID documents issued by government organizations.  What is

somewhat surprising, though, is how circular the process of either obtain-
ing, getting a duplicate, or even retiring a government-issued ID is and
how involved in the process the private sector is.  As discussed below,

hospital staff play an integral role in recording birth information that is
used for the issuance of birth certificates and then Social Security num-
bers.   It is also possible to get printed copies of birth certificates from

private sector companies working on behalf of local governments.45  At
the other end of the continuum of life, private funeral directors often issue
death records for sharing with a variety of government organizations.The first identity document issued for most native-born U.S. citizensis a birth certificate.  Birth and death records are maintained by municipal

governments.  Many births (all of those outside public hospitals) occur

without the presence of any government employee.  Many a new parent

has faced the admonition from hospital staff not to leave the hospital
without naming the new child, making it possible for the city or county
office of vital records to record the birth and issue a birth certificate.  In

the case of a small local government, a town clerk may serve this function.
It is interesting to note, however, that the hospital staff play a role in
documenting the birth for purposes of establishing the identity of the
child and his or her relationship to its parents.  A delivery room nurse and

the physician who delivered the child are likely to sign the paperwork
necessary to obtain the birth certificate.  The presence of a nonmedical

government employee during childbirth would, in fact, probably be con-
sidered to constitute a significant invasion of privacy.44See the Web site <http://www.tsa.gov/workingwithtsa/travel.shtm> for federal ruleson government-issued IDs for flying commercially.  John Gilmore is presently challenging
these regulations in federal court. See <http://cryptome.org/freetotravel.htm> for a chro-
nology of that suit.45See, for example, the Web site <http://www.vitalchek.com/provider_overview.asp>for information on obtaining birth certificates and other vital records.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT157BOX 6.1On the Nature of CredentialsUnderstanding the nature of credentials is an important component of under-standing authentication technologies, processes, and systems, since the ÒverifierÓ
in an authentication transaction verifies credentials presented by the Òpresenter.ÓCredentials may be bound in some way to the individual to whom they were is-sued, or they may be bearer credentials.  The former are necessary for identifica-

tion, while the latter may be acceptable for some forms of authorization.  A driverÕs
license or a passport is an example of the former, while an admission ticket for anentertainment event or a discount coupon is an example of the latter.  Cash is a

true bearer credential with very good anticounterfeit protection.  A credential in-
tended to be bound to a specific individual should effect the binding in some waythat can be checked by a verifier; otherwise it risks becoming a bearer credential.
Most driverÕs licenses, employee ID cards, and all passports include a photo toallow a human verifier to determine if the individual presenting the credential is theone to whom the credential was issued.  Machine-verifiable credentials may be

bound to bearers through use of personal identification numbers or biometrics.Most credit cards do not include a photo, even though they are credentialsintended for use by a specific individual.  Instead, most credit cards contain a

signature strip, and verifiers (merchants) are supposed to compare the purchas-erÕs signature with that on the card for what are referred to as Òcard presentÓ trans-actions.  This signature-verification approach to user authentication is generally

poorer than the photo ID approach, and it is not always used by merchants, espe-cially in the United States.  A growing number of credit card transactions are con-
ducted as mail order or telephone order (MOTO) transactions or Internet transac-
tions, and in these cases neither a photo nor a signature is available to the verifierto be checked.  To help address this deficiency, credit card verification generally
entails an online check.  This is necessary in part because the credit card, as an

authorization credential, is tied to data that cannot easily be maintained on thecredit card, for example, the outstanding purchase total relative to the cardholderÕscredit limit.This example points to another aspect of credential systems: off-line versusonline verification.  Thanks in part to ubiquitous networking, credit cards have ef-
fectively become online verification systems, which they were not when they were
first used.  Online verification may be effected simply by querying a database using
an identifier (for example, a credit card number), or it may involve a complex inter-action between the credential and its issuer (for example, as many smart cards
operate).  Online verification also is attractive in that it supports rapid revocation of
credentials.  Many credentials are issued with an explicit period of validity.  They
must be periodically renewed.  The issuer can revoke a credential by refusing to

renew it, but this is not a very responsive way to revoke a credential.  If failure to
renew is the only way to revoke a credential, the issuer must trade off the costs ofmore frequent renewal against accepting the costs imposed by delaying revoca-
tion until the next renewal period.  If the issuer can physically recall a credential, an
intermediate form of revocation is possible; but in a world where an increasingnumber of transactions are not conducted in person, physical revocation is often
not a viable option.  (If one tried to use an invalid credit card in a store, it might be(continues)Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.158WHO GOES THERE?It is also important to note that not all births occur in hospitals.  Be-
sides the proverbial delivery in a taxicab, one must consider home births
and so on.  The U.S. Constitution defines anyone born in the United States

as a citizen, whether or not he or she is born in a hospital (at the time the
Constitution was written, far fewer children were born in hospitals), so
that (hypothetical) controls on birth certificates based on hospital licens-
ing would miss some number of children.  In effect, the issuance of a birth

certificate approximates the issuance of an identity certificate at birth.In the past, human witnesses could serve to authenticate someoneÕsidentity in the way that the birth certificate is meant to do now.  Children

tended to be born at home, in the presence of witnesses from the local
community.  They grew up among family and neighbors, many of whom

remained available for later testimony as to the identity of the eventualretained or destroyed by the merchant, but card confiscation is not possible whenthe transaction is conducted via the phone, mail, or Internet.)For any physical credential, there is always a concern about how easily thecredential can be forged or altered.  If one can readily modify a credential or create
a bogus credential and have a verifier accept the credential as valid, then the
credential system has failed.  For credit cards, knowledge of a legitimate credit
card number (plus expiration date and billing address) is sufficient to effect mostMOTO transactions.  Credit card account numbers (and expiration dates and ad-

dresses) cannot be well-protected secrets, because they must be transmitted tomerchants to effect transactions.  This points out a fundamental deficiency of cred-
it cards as credentials in the MOTO environment: the information needed to pose
as the legitimate cardholder is not a well-protected secret.  Ancillary measures are
adopted by merchants to counter credit card fraudÑfor example, reliance on auto-matic number identification (ANI) for phone orders placed to toll free numbers, and
shipping only to the billing address associated with an account.An obvious security concern for physical credentials is the ability of a verifierto detect forgeries.  Many driver
Õs licenses and current U.S. passports includeantitamper and anticounterfeiting measuresÑfor example, holograms that are de-signed to make it easy for a human verifier to determine if the credential is legiti-mate.  Many credit cards also make use of holograms, to raise the bar against

generation of fake physical credit cards.  If legitimate credentials come in many
forms, the verifier is less likely to be able to spot fakes.  Birth certificates exhibit this
problem (among others), since there are more than 17,000 jurisdictions that may
issue these documents, and the formats vary widely.  Machine-verifiable creden-
tials ameliorate this problem, but they are typically more expensive to create, andthe cost of deploying verification technology also creates barriers to deployment.Against this backdrop, one can examine various forms of credentials to seehow they rate.  For example, a driver
Õs license is an identity credential for a namedBOX 6.1Continued
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT159individual.  It carries a photo of the individual to whom it was issued and is de-
signed for off-line verification by a human.  It can be physically revoked by a law
enforcement officer or officer of the court.  Because of wide variability in formats

and anticounterfeiting measures, forged licenses may be hard to detect, especiallywhen the license does not purport to be from the state in which it is being verified.A license typically contains dataÑfor example, home addressÑwhich are not wellmaintained on the credential and which are not generally essential to the primaryfunction for which the license was issued.A combination of a user ID and a password constitutes a bearer credential inpractice, even when the intention is otherwise.  Authentication takes place over a
network, and any binding to an individual is based on the assumption that the userdid not share the password and that the pair was not guessed by an attacker.Most credit cards are essentially bearer credentials, although that is not theintention.  Cards that bear a photo of the cardholder offer added protection in card-
present transactions but do not improve the security of MOTO transactions.  Coun-

terfeit cards, even ones that make use of holograms, have been produced bythieves, demonstrating the limits of current, anticounterfeiting measures.A smart card with a photo ID is a hybrid form of individual credential designedfor both human verification and machine-based, typically online, verification.  The
human verification aspect of such cards is vulnerable to tampering attacks, except tothe extent that anticounterfeiting measures are applied.  The machine verification

aspect of these cards can be of very high quality: That is, creating a fake public keycertificate that would be accepted by the verifier can be made infeasible from a
mathematical perspective.  However, it may be possible to covertly acquire the pri-

vate key and certificate of a legitimate individual from his or her card and insert theminto a smart card with another individualÕs photo, thus allowing the second individualto pose as the first for both human and machine verification purposes.  This illus-

trates the difficulty of developing very high assurance credential technology, althoughtechnology of this sort does pose significant barriers to counterfeiting.adult.  In other words, family and neighbors could testify to continuity of
identity despite biometric changes in the identified person.  The current

birth certificate is meant to serve as a substitute, but it has several flaws of
its own.4646There are many contexts in which human witnesses serve as verifiers of identity ofeither people or objects.  For example, chains of custody in court cases require human

witnesses to testify to the provenance of something admitted into evidence.  Human wit-
nesses can, of course, authenticate other individuals; this requires establishing the authorityand veracity of the witness.  This report does not investigate the pros and cons of human

witness testimony, as the committeeÕs charge was to examine authentication technologies,but certainly there are situations in which human witness testimony may be the best (oronly) method available for verifying identity.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.160WHO GOES THERE?There is, of course, a human element in the issuance procedure, sothere are errors both of commission (for example, not accounting prop-
erly for stillborn children or creating fraudulent documentation of a
ÒbirthÓ that never occurred) and omission (childbirths that go unre-
corded).47  As in all automated systems, there must be a manual process
for handling these errors, and the impact of the errors must be consid-
ered.  In addition, in order to ensure that the document refers to the

correct individual when it is issued, it is important to issue the initial
identity document as close to birth as possible.  This begs the question of

whether biometrics of some sort are needed, which raises some interest-
ing problems with respect to the many biometrics (including footprints,
which are standard in some locales) that might be used as part of an
identity document for a small child.  Photographs of newborn infant faces

are difficult to use for anything other than determining the race of the
child, if that.  Eye color changes over the first year of life, and newborns

have not yet acquired their permanent hair color.  While fingerprints are

set at birth, there are some important technical limitations: the ridges are
so close together that a standard 500-dots-per-inch scanner does not have
enough resolution to scan them accurately, and todayÕs algorithms do nothandle finger growth, which causes the ridges to spread farther apart.The Social Security Administration (SSA) has some nearly uniquetechnical requirements for its system of authenticating people.  Parents

are given a strong incentive to get an SSN for their child before filing the
first federal income tax return after the childÕs birth: They must do so toclaim the child as a dependent (this is a relatively recent change).  Hence,

the SSN is likely to be issued near the time of birth.  While some children
have large enough unearned incomes to require paying taxes, many chil-
dren will have no use for their SSN until they are teenagers getting their
first summer jobs.  The first technical requirement, therefore, is that al-

though the binding between a person and his or her SSN will sit dormant
for approximately 15 years, it must routinely be verifiable at that later
time.  Very few commercial systems have this property: If they have not
47There are other randomization processes at work in addition to errors in documenta-tion. It is known, for instance, that on rare occasions parents have taken the wrong child
home from a hospital nursery. As discussed already, family mobility is a randomizer forthe early childhood years.  A child whose parents move frequently while the child is matur-
ing can lose all witnesses to his or her identity continuity except for his or her parents, and
if those parents should die or otherwise not be acceptable as witnesses, that child wouldhave no witnesses to identity.  As individuals urbanize and become more mobile, if the
only method of establishing identity is through human witnesses to identity continuity, it
may not be possible to establish identity with close to 100 percent confidence.  Put another
way, it may be becoming easier for someone to infiltrate an urbanized nation under anassumed identity.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT161interacted with a customer for 15 years, very few, if any, businesses arerequired (or want, for that matter) to keep that customer record.  Until

recently, most people had little or no contact with the SSA during their
working years.  Now, people receive annual account statements (previ-

ously called Personal Earnings and Benefits Estimate) from the SSA.  If

the statement is correct, there is no need for follow-up, so individuals
may still go three to four decades between their first contributions and
their next interaction with the SSA, when they claim benefits after retire-
ment.  Hence the second technical requirement:  The authentication tech-
nology needs to work with absolutely minimal interaction over spans of
50 or more years.48  This requirement suggests that authentication sys-
tems that must persist for long periods of time will have to support re-
newal, as systems (presumably) change over time.49A third technical requirement for the SSA system of authenticatingpeople is that the system must work for the entire population, not just a
significant percentage.  As interaction with the SSA, the IRS, and other

government agencies is legally mandatory, not being able to authenticate
to them is not an option.  Either the authentication system must work for
everyone (for example, ruling out biometrics that have significant failure
rates for reasons beyond peopleÕs control), or there must be a non-oner-ous exception mechanism.  Declaring that certain sets of people are not

worth serving (or going to great lengths to serve) is an option in most
commercial contexts, but not for the government.  If the choice is to in-

clude an exception mechanism, it will need to be designed in from the
beginning, and the security implications of the exception mechanism will
need thorough analysis, since experience indicates that security problems
are often found in little-used parts of systems.Finding 6.2: Electronic authentication is qualitatively differentfor the public sector and the private sector because of a
governmentÕs unique relationship with its citizens:a.Many of the transactions are mandatory.
b.Government agencies cannot choose to serve only selected

market segments.  Thus, the user population with which they

must deal is very heterogeneous and possibly difficult to
serve electronically.48It should be noted that no cryptographic system other than the one-time pad has re-mained secure for 50 years in modern history.49Note that this may mean more than just changing key sizes, as is done for some au-thentication algorithms; other cryptographic parts of the system, such as hash functions,
may need replacing.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.162WHO GOES THERE?c.Relationships between governments and citizens are 
some-times cradle to grave but characterized by intermittent contacts,which creates challenges for technical authentication solutions.
d.Individuals may have higher expectations for govern-

ment agencies than for other organizations when it comes to
protecting the security and privacy of personal data.This finding echoes the analysis in CSTBÕs 2002 report InformationTechnology Research, Innovation, and E-Government,50 which described spe-
cial considerations in e-government.  Ubiquity (access for everyone, any-

where, anytime) and trustworthiness (citizens will not tolerate unautho-
rized or accidental disclosure of personal information) were described as
areas in which government leads in demand for technologies.The Tangled Web of Government-Issued Identity DocumentsIt is overly simplistic to view government as monolithic.  It may turnout that each agency picks its own authentication technology and deploys
it among the user base that the agency serves.  Some agencies may volun-

tarily agree to use another agencyÕs authenticators.  In fact, most models
of e-government presume a degree of integration among government
agencies and between levels of government.51,52  Making such electronic
interactions work seamlessly for users will require a degree of inter-

operability for both policy and technology.  To this end, as noted in the

discussion of the E-Government Act, the federal government is sponsor-
ing the deployment of a PKI that will authenticate transactions across the
federal government and (potentially) with state governments as well.  In

many cases, the government will both issue and rely on the authenticator.
As discussed below, once an individual has one of the widely recognized
government-issued IDs, he or she can obtain other government-issued
IDs.  In essence, government acts as a certificate authority (CA) of sorts in

the issuance of these documents. (Figure 6.1 illustrates the interdepen-
dencies of foundational identity documents.)50Computer Science and Telecommunications Board, National Research Council. Infor-mation Technology Research, Innovation, and E-Government. Washington, D.C., National Acad-emy Press, 2002.51For example, see K. Layne and J. Lee, ÒDeveloping Fully Functional E-Government: AFour Stage Model,Ó Government Information Quarterly 18: 122-136, and Janine S. Hiller and
France B”langer, ÒPrivacy Strategies for Electronic Government,Ó E-Government 2001, MarkA. Abramson and Grady E. Means, eds.  New York, Rowman and Littlefield, 2001.
52See also the white paper ÒFederal Electronic Government Infrastructure: The E-Au-thentication GatewayÑConnecting People to Services,Ó available online at <http://
www.cio.gov/eauthentication/presentations/authentication_gateway_whitepaper.pdf>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT163Issuance of an SSN requires proof of age and of citizenship or appro-priate noncitizen status and a current proof of identity.  For adults, the

proof of identity is generally another government-issued photo identifi-
cation, although it need not be: nonphotographic government records
such as marriage and divorce records and adoption records are also
accepted.  Additionally, some private sector identification cards are

accepted: for example, employer and school ID cards (usually with photo-
graphs), along with insurance policies and ID cards (usually without
photographs).  For a child, proof of identity is based on a parent
Õs vouch-ing for the childÕs name, possibly indirectly through the hospitalÕs regis-trar and/or county/state officials.A California state ID card (for identification purposes, equivalent to adriverÕs license), for example, is issued upon presentation of a birth cer-tificate, passport, or expiring ID card and proof of SSN.  As noted earlier,
it is difficult to link the birth certificate to the person applying for the ID
card.  In the United States, state ID cards are the dominant form of photo
identification.  Most private organizations rely on state ID cards for every-
FIGURE 6.1Interdependencies of foundational identity documents issued
by both governments and the private sector.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.164WHO GOES THERE?day transactionsÑfor example, paying by check, setting up a video rentalaccount, and so on.Acquiring a U.S. passport requires a birth certificate, a prior passportor other proof of citizenship, and current proof of identityÑthat is, alocal, state, or federal government-issued photo identification, including
state ID cards and U.S. passports.  An SSN must be stated on the passport

application.  The passport is sufficient to request any of the three other

forms of identification.  In addition, a passport is sufficient proof of iden-
tity and authorization to satisfy the U.S. Department of Justice I-9 form.53Alternatively, the I-9 is most commonly satisfied with a state ID card and
a Social Security card.  Other possibilities (for example, other military or

government ID cards) also exist but are used relatively rarely.  Most large

employers will issue their own photo identification, which is occasionally
used outside the place of employmentÑfor example, for verifying eligi-bility for various corporate discounts.The forms of identification described above have several noteworthyfeatures.  Birth certificates are generally issued by municipal jurisdictions,

state ID cards by states, and Social Security cards and passports by different
agencies within the federal government.  Weaknesses in various docu-

ments, particularly the difficulty of binding a birth certificate to a specific
person, are addressed by the business practices of the various agencies.  In

general, a birth certificate alone is insufficient evidence for the generation of
other identity documents.  In the United States, each agency has its own

policy for what it accepts, while in Australia this concept has been formal-
ized in the Ò100 points of proofÓ model.
54  The formalization occurs through
the assignment of points to different identifying documents.  For instance, a

current Australian passport is worth 70 points, a bank statement is worth 40
points, and employment records are worth 10 points.  Different services

require different point totals.  The decentralized nature of this arrangement

means that no single entity has a completely authoritative database.  The

evidence required for the initial government-issued identity document, the
birth certificate, is often attested to by private sector employees.  It should

also be noted that the United States is a nation of immigrantsÑdocumentsprepared overseas may introduce even more uncertainty into the system.55All of these factors contribute to the difficulty that the relying party may

have in verifying the documents.53The I-9 form is required to be completed by all new employees so that their employercan verify their employment eligibility.54See the Web sites <http://www.centrelink.gov.au/internet/internet.nsf/multifilestores/poi0110t/$File/poi0110en.pdf> and <http://www.whittlesea.vic.gov.au/enquiries/
eserv_user.asp>.55Note the case of Danny Alimonte, born in the Dominican Republic.  His eligibility to
play Little League baseball came into dispute during the 2001 Little League World Series.  It
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT165As the authentication environment changes over time, with fewerpeople to attest to the provenance of a person and more and more authen-
tication happening electronically, it will be necessary to revisit the secu-
rity of foundational identity documents.  Today
Õs systems have workedreasonably well, but it will become increasingly easy for correlated infor-
mation about an identity to be acquired by someone with malicious in-
tent.  For many purposes, birth identity becomes largely irrelevant later in

life.  Birth certificates, which are bearer credentials, suffer from the prob-

lems inherent in all bearer credentials.  However, stronger alternatives,

such as DNA, are very expensive, may be unpopular with large segments
of society, and raise new privacy and technical challenges.Recommendation 6.1: Birth certificates should not be reliedupon as the sole base identity document.  Supplemented with

supporting evidence, birth certificates can be used when proof
of citizenship is a requirement.Threats to Foundational DocumentsAs noted previously, government-issued identity documents are thefoundational documents relied upon by many authentication systems in
both the public and private sectors.  Any analysis of such systems must

therefore take into account the threat model faced in the issuance and use
of those documents.56  Only after the threats against the relied-upon docu-
ments are understood can the threat faced by the system under analysis
be considered.  As discussed in Chapter 4, in order to evaluate the secu-

rity of any authentication system, one first needs to define the threat faced
by the system.57was unclear whether Danny was 12 or 14 years old.  Dominican Republic birth records from
the mid-1980s yielded inconsistent answers.  He was eventually declared ineligible.
56This discussion applies as well to the generation of identity documents that does nottake place under governmental purview.  However, given that in practice many authentica-
tion systems do ultimately rely on a government-issued identification document, the dis-cussion is in that context.57It is worth remembering that authentication is often the first step in authorization.  The
many authorization policies that different government and private sector parties may haveare outside the scope of this report.  However, from a privacy perspective, it is often better

to handle authorization directly, rather than as a function of identity (verified throughauthentication).  For example, one can anonymously watch a movie: one goes to the ticket
window, purchases a ticket with cash, gives the ticket to the ticket collector, and enters the
theater.  No record is ever made of the identity of the moviegoer.  On the other hand, manymoviegoers voluntarily give up some amount of privacy to purchase tickets with a creditcard, possibly beforehand, over the telephone or the Internet.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.166WHO GOES THERE?The attacks against generation systems for traditional foundationaldocument fall into these general categories:¥Obtaining a fraudulent identity document,¥Passing off someone elseÕs valid identity document as oneÕs own,¥Modifying the contents of a valid identity document,¥Compromising private information stored in a back-end system,and¥Unauthorized modification of information stored in a back-endsystem.Motivations for these attacks can vary, of course, ranging from thedesire to purchase alcohol when under age to the desire to move easily
through security checkpoints in order to perpetrate a terrorist act.  At-

tacks could be aimed at individuals (as in the case of identity theft) or at
undermining public and private institutions.  The disparity of possible

motivations highlights another way (see Chapter 4) in which secondary
use is dangerous: Namely, it makes it difficult to determine the danger
posed by a security breach or attack.  Given the myriad uses of driver
Õslicenses, for example, it is incredibly difficult to ascertain the danger from
a successful attack against a motor vehicles department database.  Each of

these attacks is discussed in more detail below.¥Fraudulent identity documents. These are a major problem in today
Õssystems.  The problem includes both external threats (for example, some-

one getting a driverÕs license with someone elseÕs birth certificate andSSN) and internal threats (for example, a corrupt clerk at the DMV issuing
driverÕs licenses without checking supporting documentation).  It is worth
noting that this kind of fraud happens only on a small scale (in terms of
the percentage of identity documents issued), but it is a relatively easy
way for a determined person to circumvent the system.¥Imposters. One can attempt to pass off someone elseÕs identity docu-ment as oneÕs own.  Regardless of how the identity document is acquired
(for example, a driverÕs license  stolen in transit or a lost wallet found on
the beach), the technical problem is the binding of the document to its
holder.  Photo identification represents a primitive biometric aimed at

solving this problem.  It has the advantages of being self-contained and

requiring no infrastructure to support verification.  The problem is that

faces can change dramatically over time, and some identity documents
have long lives (for example, with automatic renewal, the picture on a
driverÕs license can easily be 8 or more years old).¥Document modification. Identity documents may be fraudulently al-
tered (for example, the substitution of photographs on a passport or
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT167driverÕs license).  Preventing this sort of attack requires technical mea-
sures in the identity document (for example, holograms on driverÕs li-censes or embossed seals on passport photos).  Document modification is

an attack on the integrity of the document itself; in the digital world,
cryptographic integrity techniques such as message authentication codes
(MACs) and digital signatures, when properly used, provide strong pro-
tection against such attacks.¥Compromising confidential information.  Many information systems
store confidential personal information that is not physically present on
the identity document.  For example, California and New Jersey driver
Õslicenses show only the mailing address of their holder, not the street
address, if two different addresses are on file.  However, an attacker

compromising the relevant database could learn the street address of any
license holder.¥Modifying computerized records.  Given that most identity authenti-
cation systems have computerized data records and that additional infor-
mation may be stored in those systems, one also must be concerned about
modification of that information.  For example, driver
Õs license suspen-sions in some states are handled by electronically marking the appropri-
ate record as having suspended driving privileges, while the license
holder is permitted to retain the physical license (and hence to continue to
use it as state-issued photo identificationÑfor example, for cashingchecks).  If an attacker changed this flag, either someone could drive who

should not be driving, or someone innocent could be caught driving with
a supposedly suspended license.  The back-end database, not the physical

document, is the arbiter of license status.Moving to digital credentials will not change these basic categories offraud.  Depending on the technology chosen for authentication, the distri-

bution of fraud among these categories may change.  For example, the use

of cryptographic integrity techniques for digital credentials would make
document modification extremely difficult, if not impossible, assuming
that the technology is properly implemented.A major change between traditional authentication and digital au-thentication is the scale of likely fraud.  With today
Õs systems, one of theprimary weaknesses relates to the validity of a specific instance of an
identity document and permeates all of the first three categories above
(fraudulent documents, imposters, and document modification).  How-

ever, controls generally work well enough to prevent the widespread
dissemination of fraudulent identity documents.  As we move forward

into a world of digital identity documents, the issuing process is still
extremely important.  All the cryptography in the world cannot overcome

weakness in this step, because cryptographic notions of trust (and valid-Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.168WHO GOES THERE?ity) are necessarily relative: They can only be as trustworthy (in the bestcase) as something else.  The hope is that this trust chain is firmly an-

chored by a statement acknowledged as true in the real world.  It is

unfortunate that all too many people (including those who should know
better) have a tendency to trust as accurate anything that the computer
says, even in situations where they would not trust a paper document.The ability to generate fraudulent identity documents on a small scaletends to have a minor impact on the overall system.  However, in a digital

world, the compromise of secret informationÑfor example, of a signingkey or other methods of accessing the document-issuing systemÑcouldopen the way to massive issuance or use of fraudulent identity docu-
ments.  The compromise of back-end systems today is already a problem

for the last two categories of threats (compromising information and
modifying records).  One has to consider the difference in speed of propa-

gation of security breaches.  Electronic issuance of identity certificates can

go orders of magnitude faster than the issuance of paper-based identity
certificates.Finding 6.3:  Many of the foundational identification documents
used to establish individual user identity are very poor from a
security perspective, often as a result of having been generated
by a diverse set of issuers that may lack an ongoing interest in
ensuring the documentsÕ validity and reliability.  Birth certifi-
cates are especially poor as base identity documents, because
they cannot be readily tied to an individual.Finding 6.4:  Scale is a major factor in the implications of au-
thentication for privacy and identity theft.  The bulk compro-

mise of private information (which is more likely to occur when
such information is accessible online) or the compromise of a
widely relied on document-issuing system can lead to massive
issuance or use of fraudulent identity documents.  The result

would adversely affect individual privacy and private- and pub-
lic-sector processes.Recommendation 6.2:  Organizations that maintain online-ac-
cessible databases containing information used to authenticate
large numbers of users should employ high-quality informa-
tion security measures to protect that information.  Wherever

possible, authentication servers should employ mechanisms
that do not require the storage of secrets.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT169GOVERNMENT AS RELYING PARTY FORAUTHENTICATION SERVICESGovernment, in addition to issuing identity documents, is also a 
rely-ing party (that is, it makes payments, allows access to records, and distrib-
utes benefits electronically based on claims made by users) for authenti-
cation systems to administer public programs at all levels (federal, state,
and local).  In fact, the government faces some unique challenges as a

relying party, owing to its large size and multifaceted nature.  It should be

noted that government revenues and expenditures are an order of magni-
tude larger than those of the largest private corporations in the United
States.  The government
Õs customer base is everything from an individualcitizen to neighborhood nonprofit organizations to large, multinational
corporations.  In some cases, the interactions between government and

varied customers are for the same purposeÑfor example, paying incometaxes.  In other cases, the interactions are very different
Ñfor example,military procurements tend to come from government contractors, not
from private citizens.Additionally, the functions of government are spread among a multi-tude of federal, state, county, and local agencies.  Government units are

organized by function (health and welfare, defense, education, public
works, and so on), regardless of how people might interact with them.
For example, a small businessperson such as a farmer probably has inter-
actions and transactions with several agencies within the federal govern-
mentÑthe Department of Agriculture, the Internal Revenue Service, theDepartment of Labor, and perhaps the Small Business Administration.
At the state level, state tax, labor, and agriculture agencies may have their
own set of reporting requirements and benefits but may also pass along
some programs funded by the federal government.There is a belief, held mainly by information technology and publicadministration professionals, that applications of technology through e-
government could reorient public organizations, causing them to become
more responsive to the needs of users.58  As discussed earlier, GPEA is
driving federal agencies to move information and transactions to the
Internet.  For many public sector organizations, though, the move to e-

government was already under way before the enactment of GPEA gave
statutory impetus to federal agency efforts.  Through the Office of Man-
58For some selected visions of e-government, see the National Association of Chief InformationOfficers, online at <http://www.nascio.org/publications/digital_government_report_2001.pdf>;
Council for Excellence in Government, online at <http://www.excelgov.org/usermedia/images/uploads/PDFs/the_next_american_revolution.pdf>; or OMB e-government strat-egy, online at <http://www.whitehouse.gov/omb/inforeg/egovstrategy.pdf>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.170WHO GOES THERE?agement and Budget, the federal government has identified 24 e-govern-ment projects that might offer a variety of electronic services between
government and citizens, government and business, and government and
government (that is, between federal, state, and local governments), and
several administrative efforts that are for internal use in federal agen-
cies.59  For the most part, the task force that recommended this list to
OMB found that these efforts had been under way for some time and
predated GPEA.Cutting through the organizational complexity, though, requires adegree of consistency in policy, management, and technology that is rarely

found in the paper-based world.  Many government agencies, most nota-

bly some leading federal agencies, are investing heavily in PKI as the
means to deploy an electronic authentication system that will work uni-
versally for users of government programs.The next three subsections describe ways in which the governmenthas tried to authenticate citizens in different contexts.  The first is a

detailed discussion of a programÑAccess Certificates for Electronic Ser-vices (ACES)Ñthat the federal government had endorsed as a way toauthenticate users across a variety of program and organizational lines,
the second describes the Internal Revenue ServiceÕs electronic tax filingprograms, and the third describes the Social Security AdministrationÕsattempt at remote authentication for access to earnings and benefits state-
ments.  Brief concluding remarks follow.
Access Certificates for Electronic ServicesACES is a program instituted by the GSA.  The program
Õs primarypurpose is to provide a PKI to facilitate secure online citizen transactions
with government agencies.60  Under ACES, a user acquires a public key
certificate by interacting with one of a small number of selected, commer-
cial CAs.  These CAs commit to certification policies and procedures con-

sistent with a model established by GSA for this purpose.  This procedure

is intended to ensure uniform quality of user authentication and status
checking for federal agencies that act as relying partiesÑthat is, that acceptthese certificates to identify users.The user employs a certificate issued by an ACES-approved CA andthe corresponding private key when engaging in transactions with par-
ticipating government agencies.  Federal agencies developing PKI-enabled
59See the Web site <http://www.egov.gov/egovreport-3.htm> for a more detailed de-scription of these 24 initiatives.60The General Services Administration is a federal management agency that sets policyin areas related to federal procurement, real estate, and information resources.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT171applications are encouraged to take advantage of the Certificate Authen-tication Module (CAM)Ña GSA-supplied and Mitretek-developed soft-wareÑto verify an ACES certificate prior to use.  The CAM is designed to
perform the requisite certificate validation checks, relieving application
developers of the need to implement this complex PKI software.  The

CAM always verifies the revocation status of an ACES certificate by con-
tacting an Online Certificate Status Protocol (OCSP) server operated by
the CA that issued the certificate.  (The rationale for adopting this specific

revocation-status-checking mechanism is described later.)To acquire a certificate, a user provides a CA with some personalinformation to verify the userÕs claimed identity.  The standards for this
verification are established by GSA and thus are uniform for all of the
CAs providing the service (within the procedural security limits that these
CAs enforce).  The form of identity established through this interaction is

a name.The identification provided by this type of interaction generally willnot be sufficient to identify the user uniquely to a government agency,
since many users may share the same nameÑfor example, John Smith.Thus, ACES certificates generally will be ambiguous relative to the ID
requirements for any government agency.  An agency may identify a user

on the basis of both a name and an SSN or other parameters (for example,
home address, age, birth date, and so on.) Thus, when the user contacts an
agency for the first time with his or her ACES certificate, the user will
need to provide this other information to an agency server to establish the
correspondence with the userÕs record in the agency database.   If this is
the userÕs first contact of any form with the agency, the agency will needto verify the supplied information as, for example, the SSA does by con-
sulting with other government records.  This procedure needs to be re-

peated by the user when he or she initially contacts an agency.  Each

agency must then find a means for binding the ACES certificate to the
user identity in that agencyÕs databaseÑfor example, on the basis of theCA name and serial number of the certificate or a hash of the public key
from the certificate.  (The CA name and serial number are unique to the

user, but they will change whenever the user renews the certificate, be-
cause a new serial number must be assigned to every certificate issued by
the CA, even if the user merely renews the certificate.  This procedure

suggests that users may have to reauthenticate themselves periodically to
each agency when the userÕs certificate expires, using whatever means theagency employed to effect the initial binding between an ACES certificate
and its records.  If the hash of the public key of the certificate is employed,

similar problems arise whenever the user changes his or her key pair.)Under the terms of the ACES program, neither the user nor the gov-ernment pays the CA for issuing an ACES certificate.  Instead, every time
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.172WHO GOES THERE?an individual uses a certificate in a transaction with a government agency,the agency pays.  The government agency pays the issuing CA for the

revocation status check (via OCSP, usually invoked by the CAM), thus
providing the financial motivation for CAs to issue these certificates.
ACES avoids the need for government agencies to make up-front invest-
ments in establishing a PKI to support this sort of e-government service.
It also avoids the need for these agencies to act as CAs.  Instead, the

agencies pay for the PKI on a sort of installment basis, indefinitely.  (This
arrangement is analogous in many ways to the government allowing a
private company to build a toll road and then collect the tolls, forever.)It has been suggested that ACES is an appropriate way to enablecitizen e-government because the technical aspects of CA operation ex-
ceed the capabilities of most government agencies.  However, since the

certificates issued by the CAs are not sufficient to identify individuals
uniquely relative to agency database records, each agency ultimately acts
as a registration authority (RA) when it establishes the correspondence
between the certificate holder and the database records.  The RA function,

while less technical than that of the CA, is usually the most security-
critical procedure of CA operation, so agencies have not avoided the need
to participate in PKI management as a result of ACES.  Arguably, the

agencies have databases that are ideal for identifying their users to the
granularity required to ensure authorized access to records and to effect
authenticated transactions.  Thus, the use of commercial CAs to issue user

certificates does not relieve government agencies of the burden of per-
forming this security-critical function.  It is true that CA operation does

require specialized technical capabilities, and the ACES program avoids
the need for agencies to acquire these capabilities.  However, it is not clear

that an agency with the IT resources needed to create and operate PKI-
enabled applications could not also operate a CA for the users that it
serves by means of these applications.The Internal Revenue ServiceÑElectronic Tax FilingThe IRS has been working to increase the volume of the electronicfiling of individual tax returns since the program began in the late 1980s.
While IRS e-file has been described as a pioneer program in electronic
government, it is interesting to note that for many years the IRS required
that electronically filed returns be accompanied by paper signature docu-
ments.  Only since 1999 has the IRS begun to make the e-file program a

totally paperless process, including electronic authentication, for some
selected taxpayers.Fortunately for the IRS, there is public law and policy that supportselectronic authentication.  The basic requirement in the Internal Revenue
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT173Code is that tax returns be signed.  However, the law does not specify
what constitutes a signing and, in fact, Treasury regulations give the IRS
commissioner broad discretion to determine what constitutes a signing.
Additionally, the IRS Reform and Restructuring Act of 1998 (PL 105-206)
speaks directly to the issue of electronic signatures and provides that they
are criminally and civilly equivalent to paper signatures.There is only one direct channel for the public to e-file with the IRS.Through the Telefile program, the IRS ÒinvitesÓ taxpayers to participate
by getting a specially designed tax package.  Invitations go to taxpayers

on the basis of expected eligibility (as a 1040EZ filer, with income less
than $50,000, or as a single filer with no dependents).  The package in-

cludes instructions for how to file using a Touch-Tone phone and a cus-
tomer service number (CSN), which is a four-digit PIN.  IRS relies on the

CSN used by the taxpayer to sign the return, but that does not authenti-
cate the transaction, since the CSN is not a unique identifier.  The IRS

authenticates the transaction by comparing data elementsÑthe CSN, dateof birth, taxpayer identification number (generally an SSN), and a name
presented by the taxpayerÑto those same data elements maintained inIRS databases.What makes authentication for IRS e-file somewhat challenging is therole of intermediaries between the IRS and the taxpayer.  In addition to

the direct provision of service through Telefile, the IRS relies extensively
on intermediaries to deliver its electronic filing products to the public.
Generally, over half the individual tax returns filed with the IRS are pre-
pared by tax preparers such as commercial services, certified public ac-
countants, and enrolled agents.  Tax preparers do an even larger percent-

age of the returns prepared for e-file, a program that emerged out of a
partnership between the IRS and H&R Block.  A subset of preparers,

authorized e-file providers, are authorized to e-file individual tax returns
for their clients.  The authorization, in this case, refers to the fact that the

IRS regulates the preparers that can e-file in some detail.Prior to 1999, the only way for a taxpayer to sign a return filed througha preparer was to fill out a paper signature document, called a jurat,
which the preparer also had to sign and then send to the IRS within 48
hours of the IRS accepting the return electronically.  The requirement for

the preparer to file the jurat with the IRS is contained in IRS Revenue
Procedures governing the behaviors of authorized e-file providers, which
also require them to exercise due diligence in verifying the identity of
taxpayers by requesting forms of identification to help validate claimed
identity.  Similarly, the taxpayer who used personal computer or Web-

based tax preparation software prior to 1999 had to complete a jurat and
send it to the IRS after the return was acknowledged as accepted.  (As a
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.174WHO GOES THERE?side note, the return can only be transmitted to the IRS through a third-party transmitter authorized by the IRS.)Beginning in 1999, the IRS built on experience of the Telefile programto issue e-file customer numbers (ECNs) to those individuals who had
filed their returns using Web-based or personal computer tax preparation
software the previous year.  Much as with Telefile, these selected taxpay-
ers got a sealed postcard that explained program eligibility and contained
one CSN (two for joint filers).As part of a parallel pilot in 1999, those taxpayers using an authorizede-file provider could avoid the use of a jurat.  In the presence of the

preparer, taxpayer(s) would select their own PIN(s), to be used to sign the
return.  IRS Revenue Procedures required that the taxpayers physically

enter their self-selected PINs on the preparerÕs keyboard.  As part of the
signing process, the preparer and taxpayers also record the PIN(s) and
other data from the return on a worksheet that both the preparer and
taxpayer(s) retain.In both of the 1999 electronic signature efforts, much as with Telefile,the IRS used the PIN-like four-digit number (for example, CSN, ECN,
self-selected PIN) to sign the returns.  This procedure meets the legal

requirement for a return to be signed.  Used in combination with other

data presented by the taxpayer(s), the IRS is able to authenticate the trans-
action to ensure that the taxpayer is who he or she claims to be.  The need

for such authentication results from the use of a four-digit PIN that is not
a unique identifier.  Additionally, authentication beyond the signing of

the return is necessary because of the business risks associated with re-
fund fraud.  The IRS refers to the need for 
Òrevenue protection.Ó61  Since
the initial offering in 1999 and 2000, the IRS has evolved its electronic
authentication efforts for taxpayers filing from home using the Web, tax
preparation software and/or the services of a tax preparer.  The primary

difference now is that the IRS no longer mails out the ECN to home filers
and instead allows taxpayers to self-select a PIN for signing purposes.
Additionally, the signing is bound to the transaction by the taxpayer(s)
providing information from the previous yearÕs tax return like AdjustedGross Income (AGI) so the IRS can validate claimed identity of the tax-
payer beyond name, address, and taxpayer identification number.61Given that over 70 percent of individual tax returns result in a refund and that there isa history of individuals trying to defraud the government by seeking refunds they are notentitled to, this is a significant business risk.  It is interesting to note that the shift from
paper-based to electronic signing altered the IRSÕs ability to prevent some refund fraud.For instance, the use of the CSN in the case of Telefile and the ECN in the first 2 years ofthat effort, in conjunction with other identifying data, provides an extra check up-front thatis not possible with paper-based signings.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT175The  Social Security Administration and PEBES
One of the more infamous cases of privacy involving authenticationcolliding with e-government capabilities comes from the SSAÕs OnlinePEBES initiative.62  In 1997, the SSA had to bring down its otherwise-
successful Web-based capability allowing individuals to request and ulti-
mately receive a Personal Earnings and Benefits Estimate Statement
(PEBES) over the Web.  The SSA took this action after a 
USA Today article
and subsequent stories in other national news outlets raised concerns
about the how the SSA authorized the release of a PEBES to an individual
electronically (described below).  Although PEBES provided dramatically

improved service through reduced cycle times and cost per transaction,
SSA yielded to congressional pressure and suspended the service owing
to the public outcry resulting from the national media coverage.Historically, SSA had provided PEBES to those who made a requestover the phone or by mail if the requester produced three identifiers
(SSN, date of birth, and name), without validating if the requester was
really the person related to that record.  For instance, it was quite possible

that a wife could have provided the requested information and obtained
her husbandÕs PEBES using that business rule.  Using this process, SSA
filled literally millions of requests per year for PEBES by mail and over
the phone.To improve service and reduce the workload of the shrinking SSAstaff, the organization launched an effort to fulfill requests for PEBES
through a self-service application over the Web.   Initially the SSA pro-

vided partial electronic service, taking the request electronically but re-
verting to paper by mailing the report to the address provided in the
request.  Over time, and after considering the results of pilot testing and

some risk analyses, the SSA launched the fully interactive version by
which the PEBES was delivered back to the requester electronically.   As

an acknowledgment that moving this kind of transaction (even just the
PEBES request portion) to the Web might entail more risk, the SSA added
more data elements to the identifiers used in the knowledge-based au-
thentication that had been used for the previous 25 years.  The Web-based

self-service application would now require the requester to provide place
of birth and motherÕs maiden name in addition to the three elementslisted above.62Zachary Tumin. ÒSocial Security on the Web: The Case of the Online PEBES.Ó StrategicComputing & Telecommunications in the Public Sector. Boston, John F. Kennedy School of
Government, Harvard University, 1998.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.176WHO GOES THERE?The fully interactive PEBES was up for approximately 1 month beforethe press depicted the offering as Òsocial insecurity.Ó  The concern ex-
pressed by the press and by several senators who wrote to the commis-
sioner of the SSA soon after the story broke was that the data elements
used in the knowledge-based authentication system might well be known
to people other than the owner of the data (and of the related PEBES
report).  More than even disgruntled spouses (or ex-spouses), close friends

or other individuals who might be able to assemble the required data
elements could gain access to the PEBES that they were not entitled to.63The three examplesÑACES, the IRS and electronic tax filing, and theSSA and PEBESÑillustrate the complexity of authentication and securityrequirements and privacy.  The IRS and the SSA have different threat

models and different security and privacy requirements, demonstrating
once again that monolithic solutions, even at the federal level, are un-
likely to be satisfactory.NATIONWIDE IDENTITY SYSTEMSThe federal government is not the only government body that plays arole in authentication and privacy considerations.  It is through local

governments that most individuals acquire identification documents.
State governments also play a key part in individual authenticationÑforexample, in the issuance of driverÕs licenses by state departments of mo-tor vehicles (DMVs).  The committee
Õs first report, IDsÑNot That Easy,64examined the concept of a nationwide identity system, raising numerouspolicy and technological questions that would need to be answered if
such a system were to be put into place.  In such a hypothetical system,

government would likely fill all three roles: regulator, issuing party, and
relying party.As noted in IDsÑNot That Easy, state driverÕs licenses already consti-tute a large-scale (nationwide and, in some cases, international) system of
identification and authentication.  Earlier in this report, it was noted that

secondary use of state driverÕs licenses and state IDs raises significantprivacy and security concerns.  Recognizing the ease with which such

documents can be fraudulently reproduced or obtained, there have been
proposals to strengthen driverÕs licenses.  The American Association of
63For SSAÕs own analysis of PEBES, see ÒPrivacy and Customer Service in the ElectronicAge:  Report to Our Customers,
Ó available online at <http://www.ssa.gov/reports/ser-
vice/>.64Computer Science and Telecommunications Board, National Research Council. IDsÑNot That Easy: Questions About Nationwide Identity Systems. Washington, D.C., National Acad-emy Press, 2002.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.AUTHENTICATION, PRIVACY, AND THE ROLES OF GOVERNMENT177Motor Vehicle Administrators is in the process of developing and propos-ing standards to do that.65  They include provisions for the use of biomet-
rics to tie a driver to his or her license; some states already require finger-
prints.  As with any system that uses biometrics, however, care must be

taken to mitigate threats against the resulting database.Finding 6.5: State-issued driverÕs licenses are a de facto nation-wide identity system.  They are widely accepted for transac-

tions that require a form of government-issued photo ID.Finding 6.6: Nationwide identity systems by definition create awidespread and widely used form of identification, which
could easily result in inappropriate linkages among nominally
independent databases.  While it may be possible to create a

nationwide identity system that would address some privacy
and security concerns, the challenges of doing so are daunting.Recommendation 6.3: If biometrics are used to uniquely iden-tify license holders and to prevent duplicate issuance, care must
be taken to prevent exploitation of the resulting centralized
database and any samples gathered.Recommendation 6.4: New proposals for improved driverÕs li-cense systems should be subject to the analysis presented in
this report by the National Research CouncilÕs Committee onAuthentication Technologies and Their Privacy Implications
and in the earlier (2002) report by the same committee: IDsÑNot That Easy: Questions About Nationwide Identity Systems.CONCLUDING REMARKS Government organizations, especially federal agencies, must live
with a plethora of legal and policy demands and guidelines in the area of
authentication and privacy, as well as provide accountability and submit
to oversight.  While citizens demand ease of use, they also expect security

and privacy protection for the information that in many cases they are
required to provide to the government.  Reconciling this tension is a

continuing challenge for any institution, but especially for the govern-
ment, owing to its unique role and requirements.  This report emphasizes

the need to avoid authentication or identification when mere authoriza-65More information is available online at <http://www.aamva.org/IDSecurity/>.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.178WHO GOES THERE?tion will suffice.  In the case of government, respecting the legitimate
function of anonymity is even more crucial.  Given the often obligatory

relationship between citizen and government, allowing anonymity and
therefore increased privacy protection when possible not only increases
efficiency (by avoiding the need to employ complicated authentication
machinery before a transaction) but also enables the many advantages of
privacy protection described in Chapter 3.  (A simple example in which

authentication is not required for interaction with the government is the
downloading of tax forms.  The identity of the person downloading cer-

tain forms does not need to be verified before the forms are made avail-
able.  The same holds true for many public records.)
Finding 6.7: Preserving the ability of citizens to interact anony-mously with other citizens, with business, and with the govern-
ment is important because it avoids the unnecessary accumula-
tion of identification data that could deter free speech and inhibit
legitimate access to public records.E-government is a driver for authentication and privacy solutionsthat place greater emphasis on government as a relying party than as an
issuer of ID documents.  Systems that depend on a common identifier in

government are subject to the privacy risks associated with the potential
for inappropriate data aggregation and (inadvertent or deliberate) infor-
mation sharing in ways that the individual providing the information did
not expect.  Care must be taken to adhere to the principles in the Privacy

Act of 1974 and the privacy principles described in Chapter 3 of this
report.Finding 6.8: Interagency and intergovernmental authenticationsolutions that rely on a common identifier create a fundamental
tension with the privacy principles enshrined in the Privacy
Act of 1974, given the risks associated with data aggregation
and sharing.Finally, while this chapter emphasizes many of the unique constraintsunder which government must operate, government is not immune to the
challenges faced by the private sector when developing authentication
systems, many of which are touched on in the preceding chapters.  Threat

models must be understood before proceeding, and the goals of any au-
thentication system should be well articulated.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.1797A Toolkit for Privacy in the
Context of Authentication
The preceding chapters provide an in-depth look at authentication,abstractly and with respect to particular technologies, as well as an
overview of privacy and a look at government-specific issues re-lated to authentication and privacy.  This concluding chapter provides a

toolkit that can aid in designing an authentication system that is sensitive
to privacy concerns.  It focuses on the three types of authentication iden-

tified in Chapter 1:¥Individual authentication is the process of establishing an under-
stood level of confidence that an identifier refers to a specific individual.¥Identity authentication is the process of establishing an understood
level of confidence that an identifier refers to an identity.  The authenti-

cated identity may or may not be linkable to an individual.¥Attribute authentication is the process of establishing an understood
level of confidence that an attribute applies to a specific individual.Authentication systems using one or more of these techniques aregenerally deployed to meet one of two goals:¥Limiting access.  Authentication may be used to limit who enters oraccesses a given area/resource and/or to control what they do once
granted entrance or access.¥Monitoring.  Authentication may be used to enable monitoring of
system use.  This may occur regardless of whether decisions about access
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.180WHO GOES THERE?to or use of resources are being made on the basis of authentication.  Such
authentication is conducted to support real-time and retrospective uses,
including audits to assess liability or blame, to mete out rewards and
praise, to provide for accountability, or to make behavior-based decisions
such as those made by marketers.Privacy issues arise in all systems that exercise control over access ormonitor behavior, regardless of the method of authentication used.  As

described in Chapter 3, the decision to authenticate, whatever the reason,
may affect decisional privacy, bodily integrity privacy, information pri-
vacy, and communications privacy.  As noted earlier, affecting privacy is
not always equivalent to violating privacy.  Without delving into the

normative decisions surrounding what is an appropriate level of sensitiv-
ity to privacy, this chapter describes how choices made at the outset of
system design and deployment can have baseline privacy implications
that should be taken into account.The choice of attribute, identity, or individual authentication is a sub-stantial determinant of how large an effect on privacy the authentication
system will have.  However, for cases in which the resource to be pro-

tected is itself private information or something else to which access must
be controlled in order to protect privacy, a privacy-invasive authentica-
tion system may be necessary and appropriate.  Such an authentication

system also may be warranted in other contexts for other reasons.  Thus,

examining the privacy consequences of authentication technology is best
done in tandem with evaluating the nature of the resource that the au-
thentication system is deployed to protect.As mentioned above, access control can be supported by proving thatone is allowed to do something or by proving that one is not on a list of
those prohibited from doing something.  This proof can be provided us-

ing attribute, identity, or individual authentication methods.  For example,

sensitive areas in a workplace are frequently limited to those individuals
who can prove that they are on the list of those permitted access.  This

proof may come in the form of an attribute authentication system (the
employee has a property that permits access), an identity authentication
system (the identification number given the employee permits access), or
an individual authentication system (the individuals on this list are per-
mitted access).  In contrast, it is common for bars and nightclubs to have

rules about those individuals who may not enter.  When individuals
present themselves for entry, those who possess certain traits (such as
being underage or being someone who is not welcome by the owner) may
not enter.  The under-21 criterion uses an attribute authentication system,

and a driverÕs license or other age-verification documents are used tomake age-based decisions about entry.  An individual authentication sys-
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.A TOOLKIT FOR PRIVACY IN THE CONTEXT OF AUTHENTICATION181tem, albeit usually a low-tech one, is used to prohibit from entering thosewhom the owner has indicated are not welcome.An attribute authentication system deployed in either of the contextsdescribed above (employment or bar) need not maintain a database.  In

each situation, the decision to permit or deny entry is based on an at-
tribute that, as far as the authentication system is concerned, any indi-
vidual may possess.  In contrast, an identity or individual authentication

system in these two contexts potentially is implemented quite differently.
1In the employment context, the identity or individual authentication sys-
tem must contain a record on everyone who is allowed to enter.  In the bar

context, the identity or individual authentication system will only contain
information on those who cannot enter (such as the list of people the
owner has indicated are unwelcome and the fact that those under 21 are
not allowed in).  An important consequence flows from this limitation.  In

the employment scenario, the system easily allows for additional controls
over the individual once he or she enters the building, and it potentially
supports monitoring of those within the system even where such moni-
toring is unrelated to decisions about access to or use of a resource.  In the

bar scenario, on the other hand, the system put in place generally will not
provide any means for controlling or monitoring those who enter based
on the way they authenticated themselves.An authentication system designed to limit the access of a specificgroup of individuals has no further privacy consequences for those not on
the initial list if the system is designed so as to limit its function to its goal.
These examples illustrate that the privacy implications of authentication
systems stem from implementation and system design choices and not
necessarily from the reasons for which the authentication system is needed
or the form of authentication technology employed.  In the next section, a

detailed toolkit is presented for thinking through how different choices in
the design of an authentication system can have an impact on privacy.PRIVACY-IMPACT TOOLKITThe choice among an attribute authentication system, an identity au-thentication system, and an individual authentication system bears sub-1In considering the distinction between identity and attribute authentication, note thatidentity authentication, which assumes the existence of an identity and a unique identifier,allows for the creation of longitudinal records.  In attribute authentication, if the attribute
chosen is sufficiently distinctive it is functionally equivalent to an identity authentication
system, in which case the attribute may be more accurately labeled an identifier, therebyeroding the protections that might otherwise be provided by an attribute authenticationsystem.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.182WHO GOES THERE?stantially on the privacy consequences of the system.  Viewed indepen-
dently of the resource they are designed to protect, attribute authentica-
tion systems present the fewest privacy problems and individual authen-
tication systems the most.  Nevertheless, in some instances individual

authentication systems may be appropriate for privacy, security, or other
reasons.Separate from the type of authentication, the overall scope of thesystem will have obvious implications for user privacy.  To limit effects

on the privacy of users, systems should collect information on the fewest
individuals possible.  Not all access-control decisions contemplate or re-

quire auditing.  While many access-control systems, particularly those

that control access to sensitive or valuable information or resources, ex-
plicitly call for auditing, it is possible to design a system that supports
access control but not auditing.2  Where auditing is not contemplated or
necessary, the scope of the system should be narrowed.  For example, if

auditing is not needed, then once a decision to permit access or action is
rendered, there may be no reason to store and maintain data about the
decision and many privacy reasons to destroy it.In general, when developing an authentication system, several ques-tions must be answered that go beyond the scope of the system and what
type of authentication will be used.  Decisions will need to be made about

which attributes to use, which identifiers will be needed, which identity
will be associated with the identifier, and how the level of confidence
needed for authentication will be reached.  The answers to each of these

questions will have implications for privacy.  Below, the four types of

privacy described in Chapter 3 (information, decisional, bodily integrity,
and communications) are discussed in the context of each of the above
questions.  The analysis proposed here is technology-independent, for the

most part, and can be applied to almost any proposed authentication
system.Attribute ChoiceAttribute authentication and, frequently, identity authentication andindividual authentication require the collection or creation of attributes
that the system uses to determine whether to grant an individual access
during the authentication phase.  In an attribute authentication system,

the attribute alone will be the thing being authenticated.  In an identity

authentication system, the identifier will correlate to some collection of2For example, auditing may not be necessary when controlling access to theaters, amuse-ment parks, or other Òone-time pay to enterÓ locales.
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.A TOOLKIT FOR PRIVACY IN THE CONTEXT OF AUTHENTICATION183information that the system considers to be an identity.  The identity may
be nothing more than an e-mail account that bears no obvious relation to
the underlying individual and the password that accesses it (in other
words not john.mulligan@example.com, but abracadabra@example.com).
In an individual authentication system, however, the identity, which po-
tentially includes attributes as well as personal information, is distinctive
to a given individual.  For example, when a driver
Õs license is initiallyissued, an effort is made to bind the driverÕs license number (nothingnecessarily individual about it at this point) to an identity that is distinct
enough to be linked, in theory, to the individual who requested the li-
cense.  Part of the identity comprises attributes such as eye and hair color,

height, weight, a photographic image of the individual, and so on.Information PrivacyTo analyze how the choice of attribute(s) may implicate informationprivacy, it is useful to consider the fair information principles detailed in
Table 3.1 in Chapter 3.Several characteristics of an attribute may be related to collecting theminimum amount of information needed for authentication.  For example,

the more distinctive the attribute is in relation to the individual, the easier
it will be to establish the necessary level of confidence that the attribute
applies to a specific individual; conversely, this may increase the poten-
tial for privacy problems.  When a large group of individuals is allowed to

access a resource, the selection of a unique attribute may inappropriately
create opportunities for revelation of the individual to whom the attribute
pertains.  The selection of an overly distinctive attribute in such a situation

would violate the minimization principle.  However, the selection of a

unique attribute may be appropriate where attribute authentication is being
used to limit access to an individual or a small set of individuals.  For

example, the use of a highly distinctive attribute to control access to per-
sonal information about an individual maintained by a third party may
meet the minimization principle and be necessary to protect against
inappropriate access to the personal information in question.Regardless of whether the choice of a highly distinctive attribute isappropriate, the more sensitive or revealing the attribute is, the greater
the information privacy problems raised.  Thus, greater attention must be

paid to protecting against misuse and disclosure.  Similarly, the more

attributes collected for authentication (regardless of whether they are ap-
propriate), the greater the information privacy problems raised.  Clearly

there are trade-offs between the privacy implications an attribute poses
and that attributeÕs security value.  Ideally, attributes should be selected
that minimize the privacy effect and maximize the security potential.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.184WHO GOES THERE?In selecting an attribute, the quality of the data represented shouldalso be examined.  The attribute should be relevant to the system.  For

example, organizational role might be an appropriate attribute in an em-
ployment context but not in a retail context; eye color might be appropri-
ate for physical access systems but not online systems.   If an attribute is

subject to change, then in some circumstances it may not be a good at-
tribute to select because its quality may be compromised.  For example,

hair color may be a poor choice of attributes if the goal is to limit access to
individuals whose hair is naturally a given shade.  In other circumstances

the changeable nature of the attribute may improve its value as an au-
thentication attribute.  For example, the use of last-deposit or last-with-

drawal information in a financial context as an attribute may meet the
data-quality standard despite its variable nature.  The fact that the value

of these attributes changes frequently means that ongoing system com-
promise is less likely if the value is guessed or stolen.The accuracy of an attribute should also be considered.  Different
systems may tolerate different levels of accuracy.  In general, to enhance

privacy protection, a system should select an attribute that is relevant,
accurate, and fresh.  If the levels of accuracy, relevance, and reliability of

an attribute are high, the number of attributes can be minimized.In selecting an attribute, system designers should also consider howwidely it is used in other systems.  If an attribute is widely used, it can

more easily facilitate secondary uses and record linkages from and to
other systems.  A less widely used attribute (ideally an attribute unique to

the system) is less likely to serve as a link between the records of disparate
systems.  In addition, if an attribute is unique to a system, and the at-

tribute space is sufficiently large and the attributes are randomly distrib-
uted in that space, then the system is less vulnerable to outside attacks
based on attribute guessing.  To limit the likelihood that its value will be

compromised, an attribute used for authentication purposes should not
be used for other system purposes.  For example, any attribute used as an

identifier in a system (perhaps an account number) is likely to be exposed
to a wide range of individuals and/or system elements and thus is a poor
choice as an authentication attribute.In order to protect privacy, the security level that the system accordsan authentication attribute should be consistent with the value of the
attribute, as well as the value of the data that can be accessed on the basis
of knowledge of the attribute.  If an attribute is sensitive or unique, its

value to the individual may go well beyond its value to the system as an
authenticator.  The data subject
Õs valuation of the attribute and the conse-quent security that it should be afforded may not be immediately obvious
to system developers or users.To better protect information privacy (and in accordance with fairWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.A TOOLKIT FOR PRIVACY IN THE CONTEXT OF AUTHENTICATION185information principles), once an attribute is selected, individuals shouldreceive clear notice about whether information regarding that attribute
will be retained in a separate authentication system of records, what the
uses of that system are, who has access to it, and what rights the indi-
vidual has with respect to accessing the system.  The system should also

specify how controls on the attribute authentication system will be en-
forced and to whom the system is accountable.Decisional PrivacyThe choice of attributes may affect decisional privacy.  In addition to
raising information privacy problems, the choice of a sensitive or reveal-
ing attribute(s) may also affect the individualÕs willingness to participatein the system for which authentication is sought and to engage in activi-
ties that might result in the collection or generation of additional sensitive
or revealing information.  Examples of attributes that are themselves re-

vealing or sensitive are political party, religion, and weight.Bodily Integrity PrivacyBodily integrity privacy may also be affected by the choice of at-tributes.  For example, the collection of blood in order to ascertain

blood type as an attribute, or of DNA in order to screen for a genetic
attribute raises two types of privacy issues that have implications for
bodily integrity.  First, the collection of the attribute may be physically

intrusive or invasive.  Second, once collected, the attribute may reveal
additional information about an individualÕs physiological or psycho-logical condition (such as a predisposition to certain diseases), as well
as information about an individualÕs recent activities (such as preg-nancy or drug use).Communications PrivacyIf identifiers such as network or communication system addresses (oreven phone numbers) are mislabeled and used as authentication at-
tributes, communications privacy can be implicated.  These addresses can

facilitate collection and analysis of information about the individual that
can be correlated with other records.Summary of Attribute Choice DiscussionThis analysis indicates that an attribute selected for an authenticationsystem that minimizes privacy implications should:Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.186WHO GOES THERE?¥Not be unique to an individual unless tightly controlled access isrequired,¥Not be widely used in other systems,¥Not be sensitive or revealing,¥Be relevant to the system, accurate, and fresh,¥Require no physical contact,¥Entail obvious (as opposed to covert) collection, and¥Not be related to communication activities.Identifier SelectionIdentity authentication and individual authentication systems bothuse identifiers to tie individuals to an identity within the system.  Both

systems require the selection or construction of an identifier, such as a
name, a random number, or a tax ID number.  The choice of or creation of

an identifier raises privacy concerns.Information PrivacyTo analyze how the choice or creation of an identifier may implicateinformation privacy, consider once again the fair information principles
in Table 3.1 in Chapter 3.The principle of limiting the collection of information is raised by theselection or construction of an identifier.  First, the minimization aspect of

the collection-limitation principle requires that efforts be made to limit
the collection of information to what is necessary to support the transac-
tion.  The selection of an identifier that in itself has meaning, is important,

or is revealing (if unnecessary to the underlying purpose of the transac-
tion) would violate this principle.  An effort should be made to use iden-

tifiers that are not themselves personal information.  Thus, randomness

and system exclusivity are valuable traits in an identifier.  As discussed

above, these traits are valuable from the perspective of system security as
well.   An identifier that is created or constructed for the purpose of

authentication in that one system will offer more protection for both pri-
vacy and security than will an identifier selected from or based upon
existing identifiers.Because the identifier is being selected for its capacity to link to theindividual in the context of an individual authentication system, the in-
formation privacy concerns are greater than they are in attribute and
identity authentication.  To best protect privacy, identifiable information

should be collected only when critical to the relationship or transaction
that is being authenticated.  The individual should consent to the collec-

tion, and the minimum amount of identifiable information should beWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.A TOOLKIT FOR PRIVACY IN THE CONTEXT OF AUTHENTICATION187collected and retained.  The relevance, accuracy, and timeliness of the
identifier should be maintained and, when necessary, updated.  Restric-

tions on secondary uses of the identifier are important in order to safe-
guard the privacy of the individual and to preserve the security of the
authentication system.  The individual should have clear rights to access

information about how data are protected and used by the authentication
system and the individual should have the right to challenge, correct, and
amend any information related to the identifier or its uses.The privacy question related to how involved an individual shouldbe in the selection or creation of an identifier is an interesting one.  It

would appear at first that allowing the individual to select an identifier
would maximize the individualÕs control and involvement and allow thatperson to establish a desired level of privacy.  Yet studies on users indi-

cate that individuals are likely to select an identifier that they can easily
remember and, in most cases, an identifier that they use elsewhere or that
is related to some personal information (see Chapter 4 for more on us-
ability).  A random identifier, created exclusively for use in that system,

will provide more protection from an information privacy perspective
but will be more cumbersome for the individual to use and remember.
However, technical mechanisms can be employed to minimize these
inconveniences.3Decisional PrivacyAn identifier that is randomly created and used exclusively for aparticular authentication system will pose fewer adverse implications for
decisional privacy than an identifier that reflects or contains personal
information.  The selection of an identifier that can be linked to the indi-

vidual is likely to pose greater risks to decisional privacy than the selec-
tion of an attribute or identifier that cannot be linked.  Such a system

would not provide for anonymous or pseudonymous participation.  In-

stead, it will be possible to associate a particular individualÕs involvementwith the activity.  Depending on the activity, it is possible that the selec-

tion of an identifier linked to the individual will cause some individuals
not to participate.3For example, in Web access contexts, a different public key certificate can be created foruse with each Web site, and browser software can automatically interact with the site toselect the right certificate when the site is visited. This affords a high degree of privacyrelative to linkage concerns, and it can provide a very convenient individual authentication
interface.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.188WHO GOES THERE?Bodily Integrity PrivacyIdentifiers, unlike attributes, generally do not represent a characteris-tic of an individual and thus are not likely to affect bodily integrity.  The

selection of an identifier that could be associated with physical character-
istics or physical activities of an individual may affect bodily integrity if
the collection of the identifier was physically intrusive, invasive, or in-
timidating.Communications PrivacyCommunications privacy is affected if the identifier is the individualÕsnetwork or communication system address or number (telephone num-
ber, e-mail address, IP address, and so on).  If the identifier is related to

the communication activities of an individual, its collection raises ques-
tions of communication privacy, because it would enable linking between
authentication and communication activities.  For example, if the identi-

fier could be linked both to the individual and to the communications
activities of the individual (phone number or e-mail address), it could
significantly compromise communications privacy.  This information

would be valuable to both the system collecting the information and also
to those outside the system, especially for law enforcement and other
investigative purposes.  To minimize privacy implications and enhance

security, it would be best if the identifier used in an authentication system
is not related to communications.  However, if the system for which ac-

cess is being authenticated is a communications system, then use of a
communications identifier would be appropriate, as it would be informa-
tion that is germane to the system.Summary of Identifier Selection DiscussionThis analysis indicates that an identifier selected for an authenticationsystem that also minimizes privacy implications should:¥Be unique to the system (possibly random),¥Not be widely used,¥Not be sensitive or revealing,¥Require little or no physical contact,¥Entail obvious (as opposed to covert) collection/assignment, and¥Not be related to communication activities.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.A TOOLKIT FOR PRIVACY IN THE CONTEXT OF AUTHENTICATION189Identity SelectionIn both identity and individual authentication systems, identifiers areoften associated with other data records.  Even in an individual authenti-
cation system that does not have as a primary goal the creation of an
identity record, these data records constitute a de facto identity of the
entity pointed to by the identifier. There are, accordingly, three types of
individual/identity authentication systems, each with different privacy
concerns:1.Purely individual authentication systems.  In these systems, the iden-
tifier itself is the only information about the entity available to the system;
no additional data records are associated with the entityÕs identifier. In
this case, the privacy analysis above regarding the selection of an identi-
fier applies directly.2.Self-contained identity authentication systems.  In these systems, an
entityÕs identifier is linked to data records that are held within the systemand contain identity information about the entity; this information may
include the history of the entityÕs access to the system.  In these systems,
an entityÕs identifier is not linked to information about the entity outsidethe system.  For example, a system-specific number might be assigned to

each entity in the system. In this case, no new privacy issues are intro-

duced.3.Non-self-contained identity authentication systems.  In these systems,
the identifier used by the system to refer to an entity is also linked to data
records that are held outside the system and contain identity information
about the entity. For example, a system of this type might maintain an

entityÕs credit card number, which is linked by credit agenciesÕ external
systems to the entityÕs transaction history and credit rating. In this case,new privacy issues arise; these issues are explored below.Information PrivacyIf the system identity is associated with a particular individual, all thefair information principles should be honored in order to best protect
privacy.  An authentication system that is organized in such a way that a

particular individualÕs privacy may be compromised requires the follow-ing:  the individual
Õs knowledge and consent; collection of the minimumamount of information and publication, including specific examples, of
the level of collection required; that the information be relevant, accurate,
timely, and complete; that information collected be used only for the
specific purpose for which it was collected, unless the individual consentsWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.190WHO GOES THERE?or a valid court order is issued; that the individual have the right toaccess, challenge, correct, and amend information; and that the system be
maintained with the highest level of security.  All of the issues related to

identifier selection and information privacy remain present in the context
of identity selection.Decisional PrivacyViewed independently of context, individual authentication systemspose the greatest risk to decisional privacy.  The creation of transactional

information about authentication events that is closely tied to a given
individual has the greatest potential to have a chilling effect on individu-
als who do not want their identity associated with an activity or organiza-
tion.  Similarly, individually identified transactional records about au-

thentication events are more likely to be reused by third parties (law
enforcement, private litigants, hackers, and so on).  Individually identi-

fied records are more easily repurposed and reused.Bodily Integrity and Communications PrivacyThe discussions in the ÒIdentifier SelectionÓ section above about issues
related to bodily integrity and communications privacy also apply here.The Authentication PhaseThis phase determines whether the attribute, identifier, or identityrefers to the individual being authenticated at the level of confidence
required by the system.  This determination is usually accomplished by

observation of the individual or by challenging the individual to produce
something supporting the claim.  (For example, requiring a card and PIN

at an ATM, requiring a badge to be swiped as the bearer enters a building,
and requiring a password at an e-commerce Web site are all authentica-
tion phases within their respective systems.)Information PrivacyWhether records of the act of authentication are kept, including, forexample, time and date logs, implicates information privacy most di-
rectly.  If such transactional records are keptÑfor example, to provide anaudit trail for security purposesÑthen the system should minimize theamount of information collected.  The individual should also be notified

that the information is being kept as part of a system, how long it will be
kept, who has access to the system, and what other uses may be made ofWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.A TOOLKIT FOR PRIVACY IN THE CONTEXT OF AUTHENTICATION191the information.  The individual should be able to access the record and
correct or amend it if necessary.  The system should limit the retention of
records containing information about authentication acts as well as sec-
ondary uses of or access to such records.Decisional PrivacyThe intrusiveness and visibility of the authentication phase may alsoaffect decisional privacy.  The attribute or identifier itself may not be

particularly revealing or sensitive, but the process for verification may be
so revealing as to inhibit someone from participating.  If the individual is

challenged to produce something supporting the attribute or identity
claimed, decisional privacy may be affected if the challenge seems to be
intimidating or if the supporting evidence is revealing or sensitive.  Deci-

sional privacy is directly affected by the creation of transactional records
of authentication events to support auditing.Bodily IntegrityThe authentication phase may also affect bodily integrity if the obser-vation of the attribute requires close or direct contact with the individual
or observation that appears intrusive.  If the authentication phase re-

quires the individual to produce physical evidence of an attribute, the
individualÕs bodily integrity may be compromised.  For example, a casual
observation that someone is 5 feet 6 inches tall is not likely to affect
someoneÕs sense of bodily integrity, while actually measuring someone islikely to affect that sense.Communications PrivacyIf the authentication phase requires the use of a communications sys-tem, communications privacy may be implicated.  Any authentication

that occurs on the Internet, for example, involves communications pri-
vacy.  The question of who has access to the content of the authentication

and to the transactional information generated during the communica-
tion should be addressed before the authentication system is imple-
mented.  Again, the creation and maintenance of transactional records of

authentication events (or the authentication events that are unrelated to
the need to control system access) may raise particularly troubling issues
of communications privacy.  If the monitoring reveals information about

whom an individual associates or communicates with, directly or indi-
rectly, the system will infringe on communications privacy.  Finally, if the

authentication phase entails use of a communications system that can beWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.192WHO GOES THERE?associated with a particular individual, then communications privacy maybe affected because the system will generate content and transactional
information linked to the individual.Summary of Authentication Phase DiscussionThis analysis indicates that in order to minimize privacy conse-quences, the following goals should be kept in mind when designing the
authentication phase of an authentication system:¥Choose the minimum level of confidence in the authentication thatsupports the system needs.  These needs could be satisfied in range of

ways:  from self-reported, to verified by the second party to the transac-

tion, to verified by a third party, to verified by multiple third parties, to
polling government sources, and so on.¥Establish processes to achieve this level of confidence and makesure that the individual being authenticated is involved in the authentica-
tion.¥Ensure that the system does only what it sets out to do (e.g., accesscontrol, monitoring, or some combination of these).¥Limit the maintenance and storage of transactional records to theminimum amount necessary.¥Set destruction policies for those records that need to be kept forlimited periods.¥Segregate authentication information from transactional data sub-sequent to authentication events.¥Create technical and procedural strategies that limit the ability toconnect authentication information with specific authentication events.¥Understand and consider the security risks of authentication activ-ity data storage, including risks of unauthorized access, unauthorized use
by those with authorized access, and legally compelled access.CONCLUDING REMARKSThe development, implementation, and broad deployment of authen-tication systems require thinking carefully about the role of identity and
privacy in a free, open, and democratic society.  Privacy, including con-

trol over the disclosure of oneÕs identity and the ability to remain anony-mous, is an essential ingredient of a functioning democracy.  It is a pre-

condition for the exercise of constitutionally protected freedoms, such as
the freedom of association.  It supports the robust exercise of freedom of

expression by, for example, creating psychological space for political dis-
sent.   It maintains social norms that protect human dignity and autonomy
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.A TOOLKIT FOR PRIVACY IN THE CONTEXT OF AUTHENTICATION193by enabling expressions of respect and intimacy and the establishment ofboundaries between oneself and oneÕs community.Information collected in or generated by authentication systems canbe valuable and revealing.  It may document where individuals have

been, what resources they have sought, what individuals or institutions
they have chosen to associate with, and so on.  It is likely to be sought by
law enforcement, commercial entities, and private parties.  If individuals

fear unchecked scrutiny, they will be less likely to vigorously participate
in the political process and in society in general.  If individuals are denied

physical and mental privacyÑfrom the government, corporations, andother individualsÑthey are less able to explore ideas, formulate personalopinions, and express and act on these beliefs.  In the context of systems

that mediate access to political, cultural or artistic information and/or
provide state and private parties with access to personal information,
identity and individual authentication mechanisms chill the free flow of
information and free association.  At the same time, 
ÒprivacyÓ is often
used as a pretext to hide illegal activities, and society has, at times, a
legitimate interest in requiring authentication or identification.  This re-

quirement may stem from the need to validate claims to rights and privi-
leges or the need to hold individuals responsible for their activities.The decision about where to deploy identity authentication systems
Ñbe it only where today confirmation of identity is already required, or in a
greater range of circumstancesÑwill shape society in both obvious andsubtle ways.  Because many privacy breaches are easy to conceal and/or

are unreported, failing to protect privacy may cost less in the short run
than the initial outlay required to establish sound procedural and techni-
cal privacy protections.  In addition, establishing practices and technical

measures that protect privacy costs money at the outset.  If individuals

whose information was compromised and agencies responsible for en-
forcing privacy laws were informed of privacy breaches, there would be
greater incentive to proactively implement technologies and policies that
protect privacy.  Even if the choice is made to institute authentication

systems only where people today attempt to discern identity, the creation
of reliable, inexpensive systems will inevitably invite function creep and
unplanned-for secondary uses unless action is taken to avoid these prob-
lems.  The role of attribute authentication in protecting privacy is

underexplored and may be a way to mitigate some of these concerns.It is critical that there be analysis of the intended context and usagemodels and thoughtful decision making about what system requirements
are.  To best protect privacy, the privacy consequences of both the in-

tended design and deployment and the potential secondary uses of au-
thentication systems must be taken into consideration by vendors, users,
policy makers, and the general public.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.APPENDIXESWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.197Appendix ABiographies of CommitteeMembers and Staff
COMMITTEE MEMBERSSTEPHEN T. KENT, Chair, is chief scientist in information security atBBN Technologies, a part of Verizon Communications.  During the past

two decades, Dr. KentÕs research and development activities have in-
cluded the design and development of user authentication and access
control systems, network layer encryption and access control systems,
secure transport layer protocols, secure e-mail technology, multilevel se-
cure (X.500) directory systems, and public key certification authority sys-
tems.  His most recent work focuses on security for Internet routing, very

high speed Internet Protocol (IP) encryption, and high-assurance crypto-
graphic modules.  Dr. Kent served as a member of the Internet Architec-

ture Board (1983-1994), and he chaired the Privacy and Security Research
Group of the Internet Research Task Force (1985-1998).  He chaired the

Privacy Enhanced Mail working group of the Internet Engineering Task
Force from 1990 to 1995 and has co-chaired the Public Key Infrastructure
Working Group since 1995. He is the primary author of the core IPsec
standards: RFCs 2401, 2402, and 2406.  He is a member of the editorial

board of the Journal of Computer Security (1995 to the present), serves onthe board of the Security Research Alliance, and served on the board of
directors of the International Association for Cryptologic Research (1982-
1989).  Dr. Kent was a member of the National Research CouncilÕs (NRCÕs)

Information Systems Trustworthiness Committee (1996-1998), which pro-
duced Trust in Cyberspace.  His other NRC service includes membership
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.198APPENDIX Aon the Committee on Rights and Responsibilities of Participants in Net-worked Communities (1993-1994), the Technical Assessment Panel for the

NIST Computer Systems Laboratory (1990-1992), and the Secure Systems
Study Committee (1988-1990).  The U.S. Secretary of Commerce appointed

Dr. Kent as chair of the Federal Advisory Committee to Develop a Federal
Information Processing Standard for Federal Key Management Infrastruc-
ture (1996-1998). The author of two book chapters and numerous techni-
cal papers on network security, Dr. Kent has served as a referee, panelist,
and session chair for a number of conferences.  Since 1977 he has lectured

on network security on behalf of government agencies, universities, and
private companies throughout the United States, Europe, Australia, and
the Far East.  Dr. Kent received the B.S. degree in mathematics, summa

cum laude, from Loyola University of New Orleans and the S.M., E.E.,
and Ph.D. degrees in computer science from the Massachusetts Institute
of Technology.  He is a fellow of the Association for Computing Machin-

ery and a member of the Internet Society and Sigma Xi.MICHAEL ANGELO is currently a staff fellow at Compaq Computer
Corporation and runs a laboratory at Compaq that assesses biometrics
and other security-enhancing technologies, such as smart cards.  He is

considered a subject-matter expert for security and its associated tech-
nologies.  His job is to provide technical guidance and input into strategic

planning and development of secure solutions.  In addition, he is respon-

sible for providing technical assistance to the corporate security team.  Dr.

Angelo possesses expertise in both biometric and token access authentica-
tion technology, including technical threat model and implementation
analysis, as well as risk reduction enhancement methodology, applied
computer system security, computer forensics, advanced data-protection
methodologies, and practical encryption techniques.  His experience com-

prises 15 years in designing, implementing, managing, and supporting
secure intra- and internets, including gateways, firewalls, and sentinels,
and 20 years working at the kernel level of numerous operating systems,
including a wide variety of hardware platforms (from personal comput-
ers to supercomputers) and software platforms (including UNIX [several
flavors], MS-DOS/Windows/NT, and VMS).  He holds several patents.

Dr. Angelo has been active in a number of trade standards organizations:
the Trusted Computing Platform Association, Americans for Computer
Privacy, the Bureau of Export Administration Technical Advisory Com-
mittee, the Information Security Exploratory Committee, the Key Recov-
ery Alliance, the Computer Systems Policy Project, the Cross-Industry
Working Team Security Working Group, and the National Institute of
Standards and TechnologyÕs Industry Key Escrow Working Group.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.APPENDIX A199STEVEN BELLOVIN is a fellow at AT&T Research.  He received a B.A.
degree from Columbia University and M.S. and Ph.D. degrees in com-
puter science from the University of North Carolina at Chapel Hill.  While

a graduate student, he helped create Netnews; for this, he and the other
collaborators were awarded the 1995 USENIX Lifetime Achievement
Award. At AT&T Laboratories, Dr. Bellovin does research in networks and
security and why the two do not get along.  He has embraced a number of

public interest causes and weighed in (e.g., through his writings) on initia-
tives (e.g., in the areas of cryptography and law enforcement) that appear to
threaten privacy.  He is currently focusing on cryptographic protocols and

network management.  Dr. Bellovin is the coauthor of the book 
Firewalls andInternet Security:  Repelling the Wily Hacker
, and he is one of the SecurityArea directors for the Internet Engineering Task Force. He was a member of
the CSTB committee that produced Trust in Cyberspace (1999) and served on
the Information Technology subcommittee of the group that produced the
NRC report Making the Nation Safer.  He has been a member of the National
Academy of Engineering since 2001.BOB BLAKLEY is chief scientist for security and privacy at IBM Tivoli
Software.   He is general chair of the 2003 Institute for Electrical and

Electronics Engineers Security and Privacy Conference and has served as
general chair of the Association for Computing MachineryÕs (ACMÕs) NewSecurity Paradigms Workshop.  He was named Distinguished Security

Practitioner by the 2002 ACM Computer Security and Applications Con-
ference and serves on the editorial board for the International Journal ofInformation Security.  Dr. Blakley was the editor of the Object Management
GroupÕs Common Object Request Broker Architecture (CORBA) securityspecification and is the author of CORBA Security: An Introduction to SafeComputing with Objects, published by Addison-Wesley.  Dr. Blakley was
also the editor of the Open GroupÕs Authorization Application Program-ming Interface specification and the OASIS Security Services Technical
CommitteeÕs Security Assertion Markup Language specification effort.He has been involved in cryptography and data security design work
since 1979 and has authored or coauthored seven papers on cryptogra-
phy, secret-sharing schemes, access control, and other aspects of com-
puter security.  He holds nine patents on security-related technologies.

Dr. Blakley received an A.B. in classics from Princeton University and a
masterÕs degree and a Ph.D. in computer and communications sciencesfrom the University of Michigan.DREW DEAN is a computer scientist at SRI International.  He joined SRIin July 2001; prior to that he was a member of the research staff at Xerox
PARC.  He pioneered the systematic study of Java security and more
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.200APPENDIX Arecently has worked across a wide range of security issues, includingdenial of service, the theory of access control, and IP traceback.  Among

his publications, he has received a Best Student Paper award from the
ACM Computer and Communications Security conference (1997), an
Outstanding Paper award from the ACM Symposium on Operating Sys-
tem Principles (1997), and a Best Paper Award from the Internet SocietyÕsNetwork and Distributed Systems Security Symposium (2001).  Dr. Dean
is a member of the editorial board of Springer-VerlagÕs International Jour-nal of Information Security. Dr. Dean holds M.A. and Ph.D. degrees fromPrinceton University and a B.S. degree from Carnegie Mellon University,
all in computer science.BARBARA FOX is a senior software architect in cryptography and digital
rights management at Microsoft Corporation and is currently a senior
fellow at the Kennedy School of Government at Harvard University.  She

serves on the technical advisory board of The Creative Commons and the
board of directors of the International Financial Cryptography Associa-
tion. Ms. Fox joined Microsoft in 1993 as director of advanced product
development and led the companyÕs electronic commerce technology de-velopment group. She has coauthored Internet standards in the areas of
Public Key Infrastructure and XML security. Her research at Harvard
focuses on digital copyright law, public policy, and privacy.STEPHEN H. HOLDEN is an assistant professor in the Department of
Information Systems at the University of Maryland, Baltimore County.
Dr. HoldenÕs research, publications, and teachings leverage his substan-tial federal government experience in government-wide policy in infor-
mation technology management and electronic government.    He left the

Internal Revenue Service (IRS) in 2000  after a 16-year career in the federal

career service. While at the IRS, he served as the program executive for
electronic tax administration (ETA) modernization, reporting to the assis-
tant commissioner (ETA).  He also served on the Federal Public Key Infra-

structure Steering Committee during his time at the IRS.  Prior to going to

the IRS, Dr. Holden worked for 10 years at the Office of Management and
Budget, doing a variety of policy, management, and budget analysis work.

His federal civil servant career began in 1983 when he was a Presidential
management intern at the Naval Sea Systems Command.   He holds a

Ph.D. in public administration and public affairs from Virginia Polytech-
nic and State University, a Master of Public Administration, and a B.A. in
public management from the University of Maine.DEIRDRE MULLIGAN was recently appointed director of the new
Samuelson Law, Technology and Public Policy Clinic at the University ofWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.APPENDIX A201California, Berkeley, School of Law (Boalt Hall).  While attending
Georgetown University Law Center, Ms. Mulligan worked at the Ameri-
can Civil Liberties UnionÕs Privacy and Technology project, where shehoned her interest in preserving and enhancing civil liberties and demo-
cratic values.  After law school, she became a founding member of the

Center for Democracy and Technology, a high-tech public interest organi-
zation for civil liberties based in Washington, D.C. For the past 6 years,
Mulligan has been staff counsel at the center.  She has worked with fed-

eral lawmakers, government agencies, the judicial system, public interest
organizations, and the high-tech business community, with the goal of
enhancing individual privacy on the Internet, thwarting threats to free
speech on the Internet, and limiting governmental access to private data.
She has testified in several settings and  has contributed to technical stan-

dards development.  Ms. Mulligan received her J.D., cum laude, from

Georgetown University Law Center in 1994 and a B.A. in architecture and
art history from Smith College in 1988.JUDITH S. OLSON is the Richard W. Pew Chair in Human Computer
Interaction at the University of Michigan.  She is also a professor in the

School of Information, Computer and Information Systems, the Business
School, and the Department of Psychology.  Her research interests in-

clude computer-supported cooperative work, human-computer interac-
tion, the design of business information systems for organizational effec-
tiveness, and cognitive psychology.  Dr. Olson
Õs recent research focuseson the nature of group work and the design and evaluation of technology
to support it.  This field combines cognitive and social psychology with

the design of information systems.  She began her career at the University

of Michigan in the Department of Psychology, served as a technical su-
pervisor for human factors in systems engineering at Bell Laboratories in
New Jersey, and returned to the University of Michigan, first to the Busi-
ness School and then the new School of Information.  She has more than

60 publications in journals and books and has served on a number of
national committees, including the National Research CouncilÕs Commit-tee on Human Factors and the council of the Association for Computing
Machinery (ACM).  She has recently been appointed to the CHI Academy

of the ACMÕs Special Interest Group for Human-Computer Interaction.Dr. Olson earned a B.A. in mathematics and psychology from Northwest-
ern University in 1965 and her Ph.D. 4 years later in the same disciplines
from the University of Michigan.JOE PATO is the principal scientist for the HP Labs Trust, Security andPrivacy research program. He has also served as chief technology officer
for Hewlett-PackardÕs Internet Security Solutions Division.  Mr. Pato
ÕsWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.202APPENDIX Acurrent research focus is the security needs of collaborative enterprises onthe Internet, addressing both interenterprise models and the needs of
lightweight instruments and peripherals directly attached to the Internet.
Specifically, he is looking at critical infrastructure protection and the
confluence of trust, e-services, and mobility. These interests have led him
to look at the preservation of Internet communication in the event of
cyberterrorism, trust frameworks for mobile environments, and how to
apply privacy considerations in complex systems.  His work in cybercrime

and homeland security recently led him to become one of the founders
and board members of the IT Sector Information Sharing and Analysis
Center. His past work included the design of delegation protocols for
secure distributed computation, key exchange protocols, interdomain

trust structures, the development of public- and secret-key-based infra-
structures, and the more general development of distributed enterprise
environments. Mr. Pato has participated on several standards or advisory
committees for the Institute for Electrical and Electronics Engineers,
American National Standards Institute, National Institute of Standards
and Technology, Department of Commerce, Worldwide Web Consortium,
Financial Services Technology Consortium, and Common Open System
Environment. He is currently the co-chair of the OASIS Security Services
Technical Committee, which is developing Security Assertions Markup
Language.RADIA PERLMAN is a Distinguished Engineer at Sun MicrosystemsLaboratories.  She is the architect for a group that does research in net-

work security issues, recently most focused on public key infrastructure
deployment. Some of the groupÕs implementation will be distributed aspart of a reference implementation for Java. Dr. Perlman is the author of
many papers in the field of network security, as well as coauthor of a
textbook on network security (and author of a textbook on lower-layer
networking protocols). She is well known for her work on sabotage-proof
routing protocols.  Her work on lower-layer protocols, also well known,

forms the basis of modern bridging, switching, and routing protocols.
This expertise is crucial to understanding the technology behind such
things as providing Internet anonymity. Dr. Perlman has about 50 issued
patents, a Ph.D. in computer science from the Massachusetts Institute of
Technology, and S.B. and S.M. degrees in mathematics from MIT. She was
recently awarded an honorary doctorate from the Royal Institute of Tech-
nology, Sweden.
PRISCILLA M. REGAN is an associate professor in the Department ofPublic and International Affairs at George Mason University.  Prior to

joining that faculty in 1989, she was a senior analyst in the congressionalWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.APPENDIX A203Office of Technology Assessment (1984-1989) and an assistant professorof politics and government at the University of Puget Sound (1979-1984).
Since the mid-1970s, Dr. ReganÕs primary research interest has been analy-sis of the social, policy, and legal implications of the organizational use of
new information and communications technologies.  She has published

more than 20 articles or book chapters, as well as Legislating Privacy: Tech-nology, Social Values, and Public Policy (University of North Carolina Press,
1995).  As a recognized researcher in this area, Dr. Regan has testified

before Congress and participated in meetings held by the Department of
Commerce, the Federal Trade Commission, the Social Security Adminis-
tration, and the Census Bureau.  She received her Ph.D. in government

from Cornell University in 1981 and her B.A. from Mount Holyoke Col-
lege in 1972.JEFFREY SCHILLER received his S.B. in electrical engineering (1979)
from the Massachusetts Institute of Technology (MIT).  As MIT network

manager, he has managed the MIT Campus Computer Network since its
inception in 1984.  Before that, he maintained MIT
Õs Multiplexed Informa-tion and Computing Service (Multics) time-sharing system during the
time of the ARPANET TCP/IP conversion.  He is an author of MIT
ÕsKerberos authentication system. Mr. Schiller is the Internet Engineering
Steering GroupÕs area director for security. He is responsible for oversee-ing security-related working groups of the Internet Engineering Task

Force.  He was responsible for releasing a U.S. legal freeware version of

the popular PGP (Pretty Good Privacy) encryption program. Mr. Schiller
is also responsible for the development and deployment of an X.509-
based public key infrastructure at MIT. He is also the technical lead for
the new Higher Education Certifying Authority being operated by the
Corporation for Research and Educational Networking.  Mr. Schiller is

also a founding member of the Steering Group of the New England Aca-
demic and Research Network (NEARnet).  NEARnet, now part of Genuity,

Inc., is a major nationwide Internet service provider.SOUMITRA SENGUPTA 
is assistant professor in the Department ofMedical Informatics at Columbia University.  Dr. Sengupta has focused

his work on the challenges of security and privacy in health care, comple-
menting his academic work by service as security officer for the New
York Presbyterian Healthcare System.  His research interests are in the

areas of distributed systems, their monitoring, management, and security
aspects, and their application in a health care environment. He is inter-
ested in the architectural design and engineering concerns of building
large, functioning systems over heterogeneous platforms and protocols.
Dr. Sengupta holds a B.E. from Birla Institute of Technology and ScienceWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.204APPENDIX A(electrical and electronics engineering), Pilani, India, and  M.S. and Ph.D.degrees from the State University of New York at Stony Brook, New York,
in computer science.   He was a member of the Association for Computing

Machinery (1984-1994), the Institute for Electrical and Electronics Engi-
neers (IEEE) Computer Society (1984-1992) and is currently a member of
the American Medical Informatics Association.JAMES L. WAYMAN has been the director of the Biometrics Test Centerat San Jose State University in California since 1995.  The Test Center is

funded by the U.S. government and other national governments to de-
velop standards and scientific test and analysis methods and to advise on
the use or nonuse of biometric identification technologies.  The test center

served as the U.S. National Biometrics Test Center from 1997 to 2000.  Dr.

Wayman received the Ph.D. degree in engineering from the University of
California at Santa Barbara in 1980 and joined the faculty of the Depart-
ment of Mathematics at the U.S. Naval Postgraduate School in 1981.  In

1986, he became a full-time researcher for the Department of Defense in
the areas of technical security and biometrics. Dr. Wayman holds three
patents in speech processing and is the author of dozens of articles in
books, technical journals, and conference proceedings on biometrics,
speech compression, acoustics, and network control. He serves on the
editorial boards of two journals and on several national and international
biometrics standards committees.  He is a senior member of the Institute

for Electrical and Electronic Engineers.DANIEL J. WEITZNER is the director of the World Wide Web Con-sortiumÕs (W3CÕs) Technology and Society activities. As such, he is re-sponsible for the development of technology standards that enable the
Web to address social, legal, and public policy concerns such as privacy,
free speech, protection of minors, authentication, intellectual property,
and identification. He is also the W3CÕs chief liaison to public policycommunities around the world and a member of the Internet Corporation
for Assigned Names and Numbers Protocol Supporting Organization Pro-
tocol Council. Mr. Weitzner holds a research appointment at the Massa-
chusetts Institute of TechnologyÕs (MITÕs) Laboratory for Computer Sci-ence and teaches Internet public policy at MIT. Before joining the W3C, he
was cofounder and deputy director of the Center for Democracy and
Technology, an Internet civil liberties organization in Washington, D.C.
He was also deputy policy director of the Electronic Frontier Foundation.
As one of the leading figures in the Internet public policy community, he
was the first to advocate user control technologies such as content filter-
ing and rating to protect children and avoid government censorship ofWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.APPENDIX A205the Internet. These arguments played a critical role in the 1997 U.S. Su-preme Court case, Reno v. ACLU
, awarding the highest free speech pro-tections to the Internet. He successfully advocated the adoption of amend-
ments to the Electronic Communications Privacy Act creating new privacy
protections for online transactional information such as Web site access
logs. Mr. Weitzner has a degree in law from Buffalo Law School and a
B.A. in Philosophy from Swarthmore College. His publications on com-
munications policy have appeared in the Yale Law Review, Global Networks,Computerworld, Wired Magazine, Social Research, Electronic Networking: Re-search, Applications and Policy, and The Whole Earth Review. He is also acommentator for National Public RadioÕs Marketplace Radio.STAFFLYNETTE I. MILLETT is a study director and program officer with the
Computer Science and Telecommunications Board (CSTB) of the National
Research Council.  She is currently involved in several CSTB projects,

including a study examining certification and dependable systems, a com-
prehensive exploration of privacy in the information age, and a look at
the fundamentals of computer science as a research discipline.  She is also

exploring possible study options for CSTB with respect to the issues of
biometrics and open source software development.  She recently com-

pleted a CSTB study that produced Embedded, Everywhere: A ResearchAgenda for Networked Systems of Embedded Computers.  Before joining CSTB,
Ms. Millett was involved in research on static analysis techniques for
concurrent programming languages as well as research on value-sensi-
tive design and informed consent online. She has an M.Sc., is ÒABDÓ in
computer science from Cornell University, and has a B.A. in mathematics
and computer science from Colby College. Her graduate work was sup-
ported by both a National Science Foundation graduate fellowship and
an Intel graduate fellowship.JENNIFER M. BISHOP has been a senior project assistant with the Com-puter Science and Telecommunications Board (CSTB) since October 2001.
She is currently supporting several projects, including Digital Archiving
and the National Archives and Records Administration; Computing Fron-
tiers: Prospects from Biology; and Telecommunications Research and De-
velopment.  She also maintains CSTB
Õs contact database, handles updatesto the CSTB Web site, and has designed book covers for several reports.
Prior to her move to Washington, D.C., Ms. Bishop worked for the City of
Ithaca, New York, coordinating the police departmentÕs transition to anew SQL-based time accrual and scheduling application. Her other workWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.206APPENDIX Aexperience includes designing customized hospitality-industry perfor-mance reports for RealTime Hotel Reports, maintaining the police records
database for the City of Ithaca, and hand-painting furniture for
Mackenzie-Childs, Ltd., of Aurora, New York. She is an artist working in
oil and mixed media. Ms. Bishop holds a B.F.A (2001) in studio art from
Cornell University.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Appendix BBriefers to the Study Committee207Although the briefers listed below provided many useful inputs tothe committee, they were not asked to endorse the conclusions or recom-
mendations, nor did they see the final draft of the report before its release.MARCH 13-14, 2001WASHINGTON, D.C.Roger Baker, Department of Commerce and CIO CouncilDaniel Chenok, Office of Information and Regulatory Affairs, Office ofManagement and BudgetSara Hamer, Social Security Administration
John Woodward, RANDMAY 30-31, 2001WASHINGTON, D.C.Lt. Col. Robert Bollig, Executive Officer, Department of Defense, Biomet-rics Management OfficeMike Green, Director, Department of Defense, PKI Program ManagementOfficeCathy Hotka, Vice President, Information Technology for the NationalRetail FederationMark MacCarthy, Senior Vice President for Public Policy, VISA USAWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.208APPENDIX BDavid Temoshok, Office of Governmentwide Policy, General ServicesAdministrationRichard Varn, Chief Information Officer, State of IowaWORKSHOPOCTOBER 3-4, 2001HERNDON, VIRGINIAMichael Aisenberg, VeriSignKim Alexander, California Voter Foundation
Brian Arbogast, Microsoft Corporation
Paul Barrett, Real User Corporation
Stefan Brands, McGill University
Roger Clarke, Xamax Consultancy Pty Ltd and the Australian NationalUniversityJohn Daugman, University of Cambridge
Mark Forman, Office of Management and Budget
Chris Hoofnagle, Electronic Privacy Information Center
Paul Van Oorschot, Entrust, Inc.
Margot Saunders, National Consumer Law Center
Judith Spencer, General Services Administration
Peter Swire, George Washington University Law School
Paul Syverson, Naval Research Laboratory
Brian Tretick, Privacy Assurance and Advisory Services, Ernst & YoungJANUARY 9, 2002PALO ALTO, CALIFORNIAChristopher Kuner, Morrison and Foerster LLPMARCH 13-14, 2002WASHINGTON, D.C.Patti Gavin, Social Security AdministrationFred Graf, Social Security Administration
Jay Maxwell, American Association of Motor Vehicles Administrators
Joe Sanders, New York State Department of Motor VehiclesWho Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Appendix CSome Key Concepts209Attribute. An attribute describes a property associated with an individual.Attribute Authentication. Attribute authentication is the process of estab-lishing an understood level of confidence that an attribute applies to a
specific individual.Authentication. Authentication is the process of establishing confidence inthe truth of some claim.Authenticator. An authenticator is evidence that is presented to supportthe authentication of a claim.  It increases confidence in the truth of the

claim.Authorization. Authorization is the process of deciding what an individualought to be allowed to do.Biometrics.  Biometrics is the automatic identification or identity verificationof individuals on the basis of behavioral or physiological characteristics.Bodily Integrity.  Bodily integrity in the context of privacy refers to those
issues involving intrusive or invasive searches and seizures.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.210APPENDIX CCertification Authority.  A certification authority is the entity that issues a
digital certificate in a public key cryptosystem.Communications Privacy.  Communications privacy is a subset of informa-tion privacy that protects the confidentiality of individualsÕ communica-
tions.Credential.  Credentials are objects that are verified when presented to the
verifier in an authentication transaction.  Credentials may be bound in

some way to the individual to whom they were issued, or they may be
bearer credentials.  The former are necessary for identification, while the

latter may be acceptable for some forms of authorization.Decisional Privacy. Decisional privacy protects the individual from inter-ference with decisions about self and family.Identification. Identification is the process of using claimed or observedattributes of an individual to infer who the individual is.Identifier. An identifier points to an individual.  An identifier can be a
name, a serial number, or some other pointer to the entity being identi-
fied.Identity. The identity of X is the set of information about individual 
X that
is associated with that individual in a particular identity system Y.  How-
ever, Y is not always named explicitly.
Identity Authentication. Identity authentication is the process of establish-ing an understood level of confidence that an identifier refers to an iden-
tity.  It may or may not be possible to link the authenticated identity to an

individual.Individual Authentication. Individual authentication is the process of estab-lishing an understood level of confidence that an identifier refers to a
specific individual.Information Privacy.  Information privacy protects the individual
Õs interestin controlling the flow of information about the self to others.Password. A sequence of characters, presumed to be secret, that is di-vulged in order to gain access to a system or resource.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.APPENDIX C211Privacy. Privacy is a multifaceted term, with many contextually depen-dent meanings.   One aspect of the right to privacy is the right of an

individual to decide for himself or herself when and on what terms his or
her attributes should be revealed.Private Key.  In public key cryptography systems, a private key is a value
(key), presumed to be secret, and typically known only to one party. The
party uses the private key to digitally sign data or to decrypt data (or
keys) encrypted for that party using the partyÕs public key.Public Key.  In public key cryptography systems, a public key is a value
used to verify a digital signature generated using a corresponding private
key, or used to encrypt data that can be decrypted using the correspond-
ing private key.Public Key Certificate.  Sometimes called a digital certificate, a public key
certificate contains attributes, typically including an identifier, that are
bound to a public key via the use of a digital signature.Public Key Infrastructure.  A public key infrastructure (PKI) consists of a set
of technical and procedural measures used to manage public keys embed-
ded in digital certificates. The keys in such certificates may be used to
enable secure communication and data exchange over potentially inse-
cure networks.Registration Authority.  A registration authority is the entity in a PKI that
establishes a correspondence between an identifier that will appear in a
certificate and an individual.Security. Security refers to a collection of safeguards that ensure the confi-dentiality of information, protect the integrity of information, ensure the
availability of information, account for use of the system, and protect the
system(s) and/or network(s) used to process the information.Threat. A threat is a motivated, capable adversary. The adversary is moti-vated to violate the security of a target (system) and has the capability to
mount attacks that will exploit vulnerabilities of the target.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.213What Is CSTB?As a part of the National Research Council, the Computer Science andTelecommunications Board (CSTB) was established in 1986 to provide
independent advice to the federal government on technical and public
policy issues relating to computing and communications.  Composed of

leaders from industry and academia, CSTB conducts studies of critical
national issues and makes recommendations to government, industry,
and academic researchers.  CSTB also provides a neutral meeting ground

for consideration of complex issues where resolution and action may be
premature. It convenes invitational discussions that bring together princi-
pals from the public and private sectors, assuring consideration of all
perspectives.  The majority of CSTB
Õs work is requested by federal agen-cies and Congress, consistent with its National Academies context.A pioneer in framing and analyzing Internet policy issues, CSTB isunique in its comprehensive scope and effective, interdisciplinary ap-
praisal of technical, economic, social, and policy issues.  Beginning with

early work in computer and communications security, cyber-assurance
and information systems trustworthiness have been a cross-cutting theme
in CSTBÕs work.  CSTB has produced several reports known as classics in
the field, and it continues to address these topics as they grow in impor-
tance.To do its work, CSTB draws on some of the best minds in the country,inviting experts to participate in its projects as a public service.  Studies

are conducted by balanced committees without direct financial interests
in the topics they are addressing.  Those committees meet, confer elec-
Who Goes There?: Authentication Through the Lens of PrivacyCopyright National Academy of Sciences. All rights reserved.214WHAT IS CSTB?tronically, and build analyses through their deliberations.  Additional
expertise from around the country is tapped in a rigorous process of
review and critique, further enhancing the quality of CSTB reports.  By

engaging groups of principals, CSTB obtains the facts and insights critical
to assessing key issues.The mission of CSTB is to¥Respond to requests from the government, nonprofit organizations,and private industry for advice on computer and telecommunications
issues and from the government for advice on computer and telecommu-
nications systems planning, utilization, and modernization;¥Monitor and promote the health of the fields of computer science
and telecommunications, with attention to issues of human resources,
information infrastructure, and societal impacts;¥Initiate and conduct studies involving computer science, computertechnology, and telecommunications as critical resources; and¥Foster interaction among the disciplines underlying computing
and telecommunications technologies and other fields, at large and within
the National Academies.As of 2003, CSTB activities with security and privacy componentsaddress privacy in the information age, critical information infrastructure
protection, authentication technologies and their privacy implications,
information technology for countering terrorism, and geospatial informa-
tion systems.  Additional studies examine broadband, digital govern-

ment, the fundamentals of computer science, limiting childrenÕs access topornography on the Internet, digital archiving and preservation, and
Internet navigation and the domain name system.  Explorations touching

on security and privacy are under way in the areas of the insider threat,
cybersecurity research, cybersecurity principles and practices, depend-
able/safe software systems, biometrics, wireless communications and
spectrum management, open source software, digital democracy, the
Òdigital divide,Ó manageable systems, information technology and jour-
nalism, supercomputing, and information technology and education.More information about CSTB can be obtained online at  <http://
www.cstb.org>.
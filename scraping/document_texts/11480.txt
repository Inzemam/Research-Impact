DETAILSDistribution, posting, or copying of this PDF is strictly prohibited without written permission of the National Academies Press.  (Request Permission) Unless otherwise indicated, all materials in this PDF are copyrighted by the National Academy of Sciences.Copyright © National Academy of Sciences. All rights reserved.THE NATIONAL ACADEMIES PRESSVisit the National Academies Press at NAP.edu and login or register to get:Œ  
Œ  10% off the price of print titles
Œ  Special offers and discountsGET THIS BOOKFIND RELATED TITLESThis PDF is available at SHARECONTRIBUTORS
http://nap.edu/11480Catalyzing Inquiry at the Interface of Computing and Biology468 pages | 8.5 x 11 | PAPERBACKISBN 978-0-309-09612-6 | DOI 10.17226/11480John C. Wooley and Herbert S. Lin, Editors; Committee on Frontiers at theInterface of Computing and Biology; Computer Science and TelecommunicationsBoard; Division on Engineering and Physical Sciences; National Research CouncilCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.INTRODUCTIONiJohn C. Wooley and Herbert S. Lin, editorsCommittee on Frontiers at the Interface of Computing and BiologyComputer Science and Telecommunications BoardDivision on Engineering and Physical SciencesCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.THE NATIONAL ACADEMIES PRESS    500 Fifth Street, N.W.    Washington, DC 20001
NOTICE:  The project that is the subject of this report was approved by the Governing Board of
the National Research Council, whose members are drawn from the councils of the NationalAcademy of Sciences, the National Academy of Engineering, and the Institute of Medicine.  The

members of the committee responsible for the report were chosen for their special competencesand with regard for appropriate balance.Support for this project was provided by the Defense Advanced Research Projects Agencyunder Contract No. MDA972-00-1-0005, the National Science Foundation under Contract No.DBI-0094528, the Department of Health and Human Services/National Institutes of Health
(including the National Institute of General Medical Sciences and the National Center forResearch Resources) under Contract No. N01-OD-4-2139, the Department of Energy underContract No. DE-FG02-02ER63336, the Department of EnergyÕs Office of Science (BER) under
Interagency Agreement No. DE-FG02-04ER63934, and National Research Council funds.  Any
opinions, findings, conclusions, or recommendations expressed in this publication are those ofthe author(s) and do not necessarily reflect the views of the organizations or agencies that
provided support for the project.International Standard Book Number 0-309-09612-XLibrary of Congress Control Number: 2005936580Cover designed by Jennifer M. Bishop.
This report is available fromComputer Science and Telecommunications Board
National Research Council500 Fifth Street, N.W.Washington, DC 20001Additional copies of this report are available from the National Academies Press, 500 FifthStreet, N.W., Lockbox 285, Washington, DC 20055; (800) 624-6242 or (202) 334-3313 (in the
Washington metropolitan area); Internet, http://www.nap.edu.Copyright 2005 by the National Academy of Sciences.  All rights reserved.
Printed in the United States of AmericaCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.The National Academy of Sciences is a private, nonprofit, self-perpetuating societyof distinguished scholars engaged in scientific and engineering research, dedicated to thefurtherance of science and technology and to their use for the general welfare. Upon theauthority of the charter granted to it by the Congress in 1863, the Academy has a mandatethat requires it to advise the federal government on scientific and technical matters.
Dr. Ralph J. Cicerone is president of the National Academy of Sciences.The National Academy of Engineering was established in 1964, under the charter of theNational Academy of Sciences, as a parallel organization of outstanding engineers. It is
autonomous in its administration and in the selection of its members, sharing with theNational Academy of Sciences the responsibility for advising the federal government. TheNational Academy of Engineering also sponsors engineering programs aimed at meeting
national needs, encourages education and research, and recognizes the superior achieve-ments of engineers. Dr. Wm. A. Wulf is president of the National Academy of Engineering.The Institute of Medicine was established in 1970 by the National Academy of Sciences tosecure the services of eminent members of appropriate professions in the examination ofpolicy matters pertaining to the health of the public. The Institute acts under the responsibil-ity given to the National Academy of Sciences by its congressional charter to be an adviser to
the federal government and, upon its own initiative, to identify issues of medical care,research, and education. Dr. Harvey V. Fineberg is president of the Institute of Medicine.The National Research Council was organized by the National Academy of Sciences in 1916to associate the broad community of science and technology with the AcademyÕs purposes offurthering knowledge and advising the federal government. Functioning in accordance withgeneral policies determined by the Academy, the Council has become the principal operat-
ing agency of both the National Academy of Sciences and the National Academy of Engi-neering in providing services to the government, the public, and the scientific and engineer-ing communities. The Council is administered jointly by both Academies and the Institute of
Medicine. Dr. Ralph J. Cicerone and Dr. Wm. A. Wulf are chair and vice chair, respectively,of the National Research Council.www.national-academies.orgCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMMITTEE ON FRONTIERS AT THE INTERFACE OFCOMPUTING AND BIOLOGYJOHN C. WOOLEY, University of California at San Diego, ChairADAM P. ARKIN, University of California at Berkeley and Lawrence BerkeleyNational LaboratoryERIC BRILL, Microsoft Research Labs
ROBERT M. CORN, University of California at Irvine
CHRIS DIORIO, University of Washington
LEAH EDELSTEIN-KESHET, University of British Columbia
MARK H. ELLISMAN, University of California at San Diego
MARCUS W. FELDMAN, Stanford University
DAVID K. GIFFORD, Massachusetts Institute of Technology
TAKEO KANADE, Carnegie Mellon University
STEPHEN S. LADERMAN, Agilent Laboratories
JAMES S. SCHWABER, Thomas Jefferson Medical CollegeStaff
Herbert Lin, Senior Scientist and Study DirectorGeoff Cohen, Consultant to CSTB
Mitchell Waldrop, Consultant to CSTB
Daehee Hwang, Consultant to Board on BiologyRobin Schoen, Senior Staff Officer
Elizabeth Grossman, Senior Staff Officer (through March 2001)
Jennifer Bishop, Program Associate
D.C. Drake, Senior Program Assistant (through March 2003)ivCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTER SCIENCE AND TELECOMMUNICATIONS BOARDJOSEPH TRAUB, Columbia University, ChairERIC BENHAMOU, Benhamou Global Ventures, LLC
DAVID D. CLARK, Massachusetts Institute of Technology, CSTB Chair EmeritusWILLIAM DALLY, Stanford University
MARK E. DEAN, IBM Almaden Research Center
DEBORAH ESTRIN, University of California, Los Angeles
JOAN FEIGENBAUM, Yale University
HECTOR GARCIA-MOLINA, Stanford University
KEVIN KAHN, Intel Corporation
JAMES KAJIYA, Microsoft Corporation
MICHAEL KATZ, University of California, Berkeley
RANDY H. KATZ, University of California, Berkeley
WENDY A. KELLOGG, IBM T.J. Watson Research Center
SARA KIESLER, Carnegie Mellon University
BUTLER W. LAMPSON, Microsoft Corporation, CSTB Member Emeritus
TERESA H. MENG, Stanford University
TOM M. MITCHELL, Carnegie Mellon University
DANIEL PIKE, GCI Cable and Entertainment
ERIC SCHMIDT, Google Inc.
FRED B. SCHNEIDER, Cornell University
WILLIAM STEAD, Vanderbilt University
ANDREW J. VITERBI, Viterbi Group, LLC
JEANNETTE M. WING, Carnegie Mellon UniversityRICHARD ROWBERG, Acting DirectorKRISTEN BATCH, Research Associate
JENNIFER M. BISHOP, Program Associate
JANET BRISCOE, Manager, Program Operations
JON EISENBERG, Senior Program Officer and Associate Director
RENEE HAWKINS, Financial Associate
MARGARET MARSH HUYNH, Senior Program Assistant
HERBERT S. LIN, Senior Scientist
LYNETTE I. MILLETT, Senior Program Officer
JANICE SABUDA, Senior Program Assistant
GLORIA WESTBROOK, Senior Program Assistant
BRANDYE WILLIAMS, Staff AssistantFor more information on CSTB, see its Web site at http://www.cstb.org, write toCSTB, National Research Council, 500 Fifth Street, N.W., Washington, DC 20001, or

call (202) 334-2605, or e-mail the CSTB at cstb@nas.edu.vCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Preface
viiIn the last decade of the 20th century, computer science and biology both emerged as fields capableof remarkable and rapid change. Moreover, they evolved as fields of inquiry in ways that draw atten-
tion to their areas of intersection. The continuing advancements in technology and the pace of scientific
research present the means for computing to help answer fundamental questions in the biological
sciences and for biology to demonstrate that new approaches to computing are possible.Advances in the power and ease of use of computing and communications systems have fueledcomputational biology (e.g., genomics) and bioinformatics (e.g., database development and analysis).
Modeling and simulation of biological entities such as cells have joined biologists and computer scien-
tists (and mathematicians, physicists, and statisticians too) to work together on activities from pharma-
ceutical design to environmental analysis.On the other side, computer scientists have pondered the significance of biology for their field. Forexample, computer scientists have explored the use of DNA as a substrate for new computing hardware
and the use of biological approaches in solving hard computing problems. Exploration of biological
computation suggests a potential for insight into the nature of and alternative processes for computa-
tion, and it also gives rise to questions about hybrid systems that achieve some kind of synergy of
biological and computational systems. And there is also the fact that biological systems exhibit charac-
teristics such as adaptability, self-healing, evolution, and learning that would be desirable in the infor-
mation technologies that humans use.Making the most of the research opportunities at the interface of computing and biologyÑwhat weare calling the BioComp interfaceÑrequires illuminating what they are and effectively engaging people
from both computing and biology. As in other contexts, the challenges of interdisciplinary education
and of collaboration are significant, and each will require attention, together with substantive work
from both policy makers and researchers. At the start of the 1990s, attempts were made to stimulate
mutual interest and collaboration among young researchers in computing and biology. Those early
efforts yielded nontrivial successes, but in retrospect represented a Version 1.0 prototype for the poten-
tial in bringing the two fields together. Circumstances today seem much more favorable for progress.
New research teams and training programs have been formed as individual investigators from the
respective communities, government agencies, and private foundations have become increasingly en-
gaged. Similarly, some larger groups of investigators from different backgrounds have been able toCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.viiiPREFACEobtain funding to work together to address cross-disciplinary research problems. It is against thisbackground that the committee sees a Version 2.0 of the BioComp interface emerging that will yield
unprecedented progress and advance.The range of possible activities at the BioComp interface is broad, and accordingly so is the range ofinterested agencies, which include the Defense Advanced Research Projects Agency (DARPA), the
National Science Foundation (NSF), the Department of Energy (DOE), and the National Institutes of
Health (NIH). These agencies have, to varying degrees, recognized that truly cross-disciplinary work
would build on both computing and biology, and they have sought to advance activities at the interface.This report by the Committee on Frontiers at the Interface of Computing and Biology seeks toestablish the intellectual legitimacy of a fundamentally cross-disciplinary collaboration between biolo-
gists and computer scientists. That is, while some universities are increasingly favorable to research at
the intersection, life science researchers at other universities are strongly impeded in their efforts to
collaborate. This report addresses these impediments and describes some strategies for overcoming
them.In addition, this report provides a wealth of well-documented examples. As a rule, these exampleshave generally been selected to illustrate the breadth of the topic in question, rather than to identify the
most important areas of activity. That is, the appropriate spirit in which to view these examples is Òlet
a thousand flowers bloom,Ó rather than one of Òfinding the prettiest flowers.Ó It is hoped that these
examples will encourage students in the life sciences to start or to continue study in computer science
that will enable them to be more effective users of computing in their future biological studies. In the
opposite direction, the report seeks to describe a rich and diverse domainÑbiologyÑwithin which
computer scientists can find worthy problems that challenge current knowledge in computing. It is
hoped that this awareness will motivate interested computer scientists to learn about biological phe-
nomena, data, experimentation, and the likeÑso that they can engage biologists more effectively.To gather information on such a broad area, the committee took input from a wide variety ofsources. The committee convened two workshops in March 2001 and May 2001, and committee mem-
bers or staff attended relevant workshops sponsored by other groups. The committee mined the pub-
lished literature extensively. It solicited input from other scientists known to be active in BioComp
research. An early draft of the report was examined by a number of reviewers far larger than usual for
National Research Council (NRC) reports, and the draft was modified in accordance with their exten-
sive input, which helped the committee to sharpen its message and strengthen its presentation.The result of these efforts is the first comprehensive NRC study that suggests a high-level intellec-tual structure for federal agencies for supporting work at the BioComp interface. Although workshop
reports have been supported by individual agencies on the subject of computing applied to various
aspects of biological inquiry, the NRC has not until now undertaken a study whose intent was to be
inclusive.Within the NRC, the lead unit on this project was the Computer Science and TelecommunicationsBoard (CSTB), and Marjory Blumenthal and Elizabeth Grossman launched the project.  The committee

also acknowledges with gratitude the contribution of the Board on BiologyÑRobin Schoen continued
work on the project after Elizabeth GrossmanÕs departure. Geoff Cohen and Mitch Waldrop, consult-
ants to CSTB, made major substantive contributions to this report. A variety of project assistants,
including D.C. Drake, Jennifer Bishop, Gloria Westbrook, and Margaret Huynh, provided research and
administrative support. Finally, grateful thanks are offered to DARPA, NIH, NSF, and DOE for their
financial support for this project as well as their patience in awaiting the final report. No single agency
can respond to the challenges and opportunities at the interface, and the committee hopes that its
analysis will facilitate agency efforts to define their own priorities, set their own path, and participate in
what will be a continuing adventure along the frontier at this exciting and promising interface, which
will continue to develop throughout the 21st century.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.PREFACEixA Personal Note from the ChairThe committee found the scope of the study and the need to achieve an adequate level of balance inboth directions around the BioComp interface to be a challenge. This challenge, I hope, has been met,
but this was only possible due to the recruitment of an outstanding physicist turned computer science
policy expert from the NRC. Specifically, after the original series of meetings, Herb Lin from the CSTB
side of the NRC joined the effort, and most notably, followed up on the committeeÕs earlier analyses by
interviewing numerous individuals engaged in both biocomputing (applications of biology to comput-
ing) and computational biology (applications of computing to biology). This was invaluable, as was
HerbÕs never ending enthusiasm, insight into the nature of the interdisciplinary discussions that are
growing, and his willingness to engage in learning a lot about biology. The report could never have
been completed without his persistence. His expertise in editing and analytical treatment of policy and
technical material allowed us to sustain a broad vision. (Even with the length and breadth of this study,
we were able to cover only selected areas at the interface.) The committeeÕs efforts were sustained and
accelerated by HerbÕs determination that we stay the course despite the size of the task, and by his
insightful comments, criticisms, and suggestions on every aspect of the study and the report.John Wooley, ChairCommittee on Frontiers at the Interfaceof Computing and BiologyCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.This report has been reviewed in draft form by individuals chosen for their diverse perspectivesand technical expertise, in accordance with procedures approved by the National Research CouncilÕs
Report Review Committee. The purpose of this independent review is to provide candid and critical
comments that will assist the institution in making its published report as sound as possible and toensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the
study charge. The review comments and draft manuscript remain confidential to protect the integrity of
the deliberative process. We wish to thank the following individuals for their review of this report:Harold Abelson, Massachusetts Institute of Technology,Eric Benhamou, Benhamou Global Ventures, LLC,
Mina Bissell, Lawrence Berkeley National Laboratory,
Gaetano Borriello, University of Washington,
Dennis Bray, University of Cambridge,
Steve Burbeck, IBM,
Andrea Califano, Columbia University,
Charles Cantor, Boston University,
David D. Clark, Massachusetts Institute of Technology,
G. Bard Ermentrout, University of Pittsburgh,
Lisa Fauci, Tulane University,
David Galas, Keck Graduate Institute,
Leon Glass, McGill University,
Mark D. Hill, University of Wisconsin-Madison,
Tony Hunter, The Salk Institute for Biological Studies,
Sara Kiesler, Carnegie Mellon University,
Isaac Kohane, ChildrenÕs Hospital,
Nancy Kopell, Boston University,
Bud Mishra, New York University,
William Noble, University of Washington,Acknowledgment of ReviewersxiCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.xiiACKNOWLEDGMENT OF REVIEWERSAlan S. Perelson, Los Alamos National Laboratory,Robert J. Robbins, Fred Hutchinson Cancer Research Center,
Lee Segel, The Weizmann Institute of Science,
Larry L. Smarr, University of California, San Diego,
Sylvia Spengler, National Science Foundation,
William Stead, Vanderbilt University,
Suresh Subramani, University of California, San Diego,
Charles Taylor, University of California, Los Angeles, and
Andrew J. Viterbi, Viterbi Group, LLC.Although the reviewers listed above have provided many constructive comments and suggestions,they were not asked to endorse the conclusions or recommendations, nor did they see the final draft of
the report before its release. The review of this report was overseen by Russ Altman, Stanford Univer-
sity. Appointed by the National Research Council, he was responsible for making certain that an inde-
pendent examination of this report was carried out in accordance with institutional procedures and that
all review comments were carefully considered. Responsibility for the final content of this report rests
entirely with the authoring committee and the institution.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY11INTRODUCTION91.1Excitement at the Interface of Computing and Biology, 9

1.2Perspectives on the BioComp Interface, 10
1.2.1From the Biology Side, 11

1.2.2From the Computing Side, 12

1.2.3The Role of Organization and Culture, 13
1.3Imagine WhatÕs Next, 14

1.4Some Relevant History in Building the Interface, 16
1.4.1The Human Genome Project, 16

1.4.2The Computing-to-Biology Interface, 16

1.4.3The Biology-to-Computing Interface, 17
1.5Background, Organization, and Approach of This Report, 19
221st CENTURY BIOLOGY23
2.1What Kind of Science?, 23
2.1.1The Roots of Biological Culture, 23

2.1.2Molecular Biology and the Biochemical Basis of Life, 24

2.1.3Biological Components and Processes in Context, and Biological Complexity, 25
2.2Toward a Biology of the 21st Century, 27

2.3Roles for Computing and Information Technology in Biology, 31
2.3.1Biology as an Information Science, 31

2.3.2Computational Tools, 33

2.3.3Computational Models, 33

2.3.4A Computational Perspective on Biology, 33

2.3.5Cyberinfrastructure and Data Acquisition, 34
2.4Challenges to Biological Epistemology, 34
ContentsxiiiCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.xivCONTENTS3ON THE NATURE OF BIOLOGICAL DATA35
3.1Data Heterogeneity, 35

3.2Data in High Volume, 37

3.3Data Accuracy and Consistency, 38

3.4Data Organization, 40

3.5Data Sharing, 44

3.6Data Integration, 47

3.7Data Curation and Provenance, 49
4COMPUTATIONAL TOOLS57
4.1The Role of Computational Tools, 57

4.2Tools for Data Integration, 58
4.2.1Desiderata, 59

4.2.2Data Standards, 60

4.2.3Data Normalization, 60

4.2.4Data Warehousing, 62

4.2.5Data Federation, 62

4.2.6Data Mediators/Middleware, 65

4.2.7Databases as Models, 65

4.2.8Ontologies, 67
4.2.8.1Ontologies for Common Terminology and Descriptions, 67

4.2.8.2Ontologies for Automated Reasoning, 69
4.2.9Annotations and Metadata, 73

4.2.10A Case Study: The Cell Centered Database, 75

4.2.11A Case Study: Ecological and Evolutionary Databases, 79
4.3Data Presentation, 81
4.3.1Graphical Interfaces, 81

4.3.2Tangible Physical Interfaces, 83

4.3.3Automated Literature Searching, 84
4.4Algorithms for Operating on Biological Data, 87
4.4.1Preliminaries: DNA Sequence as a Digital String, 87

4.4.2Proteins as Labeled Graphs, 88

4.4.3Algorithms and Voluminous Datasets, 89

4.4.4Gene Recognition, 89

4.4.5Sequence Alignment and Evolutionary Relationships, 92

4.4.6Mapping Genetic Variation Within a Species, 94

4.4.7Analysis of Gene Expression Data, 97

4.4.8Data Mining and Discovery, 100
4.4.8.1The First Known Biological Discovery from Mining Databases, 100

4.4.8.2A Contemporary Example: Protein Family Classification and Data
Integration for Functional Analysis of Proteins, 1014.4.9Determination of Three-dimensional Protein Structure, 103

4.4.10Protein Identification and Quantification from Mass Spectrometry, 106

4.4.11Pharmacological Screening of Potential Drug Compounds, 107

4.4.12Algorithms Related to Imaging, 107
4.4.12.1Image Rendering, 110

4.4.12.2Image Segmentation, 110

4.4.12.3Image Registration, 113

4.4.12.4Image Classification, 114
4.5Developing Computational Tools, 114
Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONTENTSxv5COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FORBIOLOGICAL DISCOVERY117

5.1On Models in Biology, 117

5.2Why Biological Models Can Be Useful, 119
5.2.1Models Provide a Coherent Framework for Interpreting Data, 120

5.2.2Models Highlight Basic Concepts of Wide Applicability, 120

5.2.3Models Uncover New Phenomena or Concepts to Explore, 121

5.2.4Models Identify Key Factors or Components of a System, 121

5.2.5Models Can Link Levels of Detail (Individual to Population), 122

5.2.6Models Enable the Formalization of Intuitive Understandings, 122

5.2.7Models Can Be Used as a Tool for Helping to Screen Unpromising Hypotheses, 122

5.2.8Models Inform Experimental Design, 122

5.2.9Models Can Predict Variables Inaccessible to Measurement, 123

5.2.10Models Can Link What Is Known to What Is Yet Unknown, 124

5.2.11Models Can Be Used to Generate Accurate Quantitative Predictions, 124

5.2.12Models Expand the Range of Questions That Can Meaningfully Be Asked, 124
5.3Types of Models, 125
5.3.1From Qualitative Model to Computational Simulation, 125

5.3.2Hybrid Models, 129

5.3.3Multiscale Models, 130

5.3.4Model Comparison and Evaluation, 131
5.4Modeling and Simulation in Action, 134
5.4.1Molecular and Structural Biology, 134
5.4.1.1Predicting Complex Protein Structures, 134

5.4.1.2A Method to Discern a Functional Class of Proteins, 134

5.4.1.3Molecular Docking, 136

5.4.1.4Computational Analysis and Recognition of Functional and
Structural Sites in Protein Structures, 1365.4.2Cell Biology and Physiology, 139
5.4.2.1Cellular Modeling and Simulation Efforts, 139

5.4.2.2Cell Cycle Regulation, 146

5.4.2.3A Computational Model to Determine the Effects of SNPs in
Human Pathophysiology of Red Blood Cells, 1485.4.2.4Spatial Inhomogeneities in Cellular Development, 149
5.4.2.4.1Unraveling the Physical Basis of Microtubule Structure and
Stability, 1495.4.2.4.2The Movement of 
Listeria Bacteria, 1505.4.2.4.3Morphological Control of Spatiotemporal Patterns of
Intracellular Signaling, 1515.4.3Genetic Regulation, 152
5.4.3.1Cis-regulation of Transcription Activity as Process Control Computing, 152
5.4.3.2Genetic Regulatory Networks as Finite-state Automata, 153

5.4.3.3Genetic Regulation as Circuits, 157

5.4.3.4Combinatorial Synthesis of Genetic Networks, 158

5.4.3.5Identifying Systems Responses by Combining Experimental Data with
Biological Network Information, 1595.4.4Organ Physiology, 161
5.4.4.1Multiscale Physiological Modeling, 161

5.4.4.2Hematology (Leukemia), 162
Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.xviCATALYZING INQUIRY5.4.4.3Immunology, 163
5.4.4.4The Heart, 166
5.4.5Neuroscience, 172
5.4.5.1The Broad Landscape of Computational Neuroscience, 172

5.4.5.2Large-scale Neural Modeling, 173

5.4.5.3Muscular Control, 175

5.4.5.4Synaptic Transmission, 181

5.4.5.5Neuropsychiatry, 187
5.4.6Virology, 189

5.4.7Epidemiology, 191

5.4.8Evolution and Ecology, 193
5.4.8.1Commonalities Between Evolution and Ecology, 193

5.4.8.2Examples from Evolution, 194
5.4.8.2.1Reconstruction of the 
Saccharomyces Phylogenetic Tree, 1955.4.8.2.2Modeling of Myxomatosis Evolution in Australia, 197

5.4.8.2.3The Evolution of Proteins, 198

5.4.8.2.4The Emergence of Complex Genomes, 199
5.4.8.3Examples from Ecology, 200
5.4.8.3.1Impact of Spatial Distribution in Ecosystems, 200

5.4.8.3.2Forest Dynamics, 201
5.5Technical Challenges Related to Modeling, 202
6A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY205
6.1Biological Information Processing, 205

6.2An Engineering Perspective on Biological Organisms, 210
6.2.1Biological Organisms as Engineered Entities, 210

6.2.2Biology as Reverse Engineering, 211

6.2.3Modularity in Biological Entities, 213

6.2.4Robustness in Biological Entities, 217

6.2.5Noise in Biological Phenomena, 220
6.3A Computational Metaphor for Biology, 223
7CYBERINFRASTRUCTURE AND DATA ACQUISITION227
7.1Cyberinfrastructure for 21st Century Biology, 227
7.1.1What Is Cyberinfrastructure? 227

7.1.2Why Is Cyberinfrastructure Relevant? 228

7.1.3The Role of High-performance computing, 231

7.1.4The Role of Networking, 235

7.1.5An Example of Using Cyberinfrastructure for Neuroscience Research, 235
7.2Data Acquisition and Laboratory Automation, 237
7.2.1TodayÕs Technologies for Data Acquisition, 237

7.2.2Examples of Future Technologies, 241

7.2.3Future Challenges, 245
8BIOLOGICAL INSPIRATION FOR COMPUTING247
8.1The Impact of Biology on Computing, 247
8.1.1Biology and Computing: Promise and Skepticism, 247

8.1.2The Meaning of Biological Inspiration, 249

8.1.3Multiple Roles: Biology for Computing Insight, 250
Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONTENTSxvii8.2Examples of Biology as a Source of Principles for Computing, 253
8.2.1Swarm Intelligence and Particle Swarm Optimization, 253

8.2.2Robotics 1: The Subsumption Architecture, 255

8.2.3Robotics 2: Bacterium-inspired Chemotaxis in Robots, 256

8.2.4Self-Healing Systems, 257

8.2.5Immunology and Computer Security, 259
8.2.5.1Why Immunology Might Be Relevant, 259

8.2.5.2Some Possible Applications of Immunology-based Computer Security, 259

8.2.5.3Immunological Design Principles for Computer Security, 260

8.2.5.4An Example: Immunology and Intruder Detection, 262

8.2.5.5Interesting Questions and Challenges, 263
8.2.5.5.1Definition of Self, 263

8.2.5.5.2More Immunological Mechanisms, 263
8.2.5.6Some Possible Difficulties with an Immunological Approach, 264
8.2.6Amorphous Computing, 264
8.3Biology as Implementer of Mechanisms for Computing, 265
8.3.1Evolutionary Computation, 265
8.3.1.1What Is Evolutionary Computation? 265

8.3.1.2Suitability of Problems for Evolutionary Computation, 267

8.3.1.3Correctness of a Solution, 268

8.3.1.4Solution Representation, 269

8.3.1.5Selection of Primitives, 269

8.3.1.6More Evolutionary Mechanisms, 270
8.3.1.6.1Coevolution, 270

8.3.1.6.2Development, 270
8.3.1.7Behavior of Evolutionary Processes, 271
8.3.2Robotics 3: Energy and Compliance Management, 272

8.3.3Neuroscience and Computing, 273
8.3.3.1Neuroscience and Architecture in Broad Strokes, 274

8.3.3.2Neural Networks, 274

8.3.3.3Neurally Inspired Sensors, 277
8.3.4Ant Algorithms, 277
8.3.4.1Ant Colony Optimization, 278

8.3.4.2Other Ant Algorithms, 279
8.4Biology as Physical Substrate for Computing, 280
8.4.1Biomolecular Computing, 280
8.4.1.1Description, 281

8.4.1.2Potential Application Domains, 284

8.4.1.3Challenges, 285

8.4.1.4Future Directions, 286
8.4.2Synthetic Biology, 287
8.4.2.1An Engineering Approach to Building Living Systems, 288

8.4.2.2Cellular Logic Gates, 288

8.4.2.3Broader Views of Synthetic Biology, 290

8.4.2.4Applications, 291

8.4.2.5Challenges, 291
8.4.3Nanofabrication and DNA Self-Assembly, 292
8.4.3.1Rationale, 292

8.4.3.2Applications, 296
Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.xviiiCONTENTS8.4.3.3Prospects, 297
8.4.3.4Hybrid Systems, 298
9ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OFCOMPUTING AND BIOLOGY299

9.1Why Problem-focused Research? 299

9.2Cellular and Organismal Modeling, 300

9.3A Synthetic Cell with Physical Form, 303

9.4Neural Information Processing and Neural Prosthetics, 306

9.5Evolutionary Biology, 311

9.6Computational Ecology, 313

9.7Genome-enabled Individualized Medicine, 317
9.7.1Disease Susceptibility, 318

9.7.2Drug Response and Pharmacogenomics, 320

9.7.3Nutritional Genomics, 322
9.8A Digital Human on Which a Surgeon Can Operate Virtually, 323

9.9Computational Theories of Self-assembly and Self-modification, 325

9.10A Theory of Biological Information and Complexity, 327
10CULTURE AND RESEARCH INFRASTRUCTURE331
10.1Setting the Context, 331

10.2Organizations and Institutions, 332
10.2.1The Nature of the Community, 332

10.2.2Education and Training, 333
10.2.2.1 General Considerations, 333

10.2.2.2 Undergraduate Programs, 334

10.2.2.3 The BIO2010 Report, 335
 10.2.2.3.1Engineering, 336
 10.2.2.3.2Quantitative Training, 336
 10.2.2.3.3Computer Science, 337
10.2.2.4 Graduate Programs, 341

10.2.2.5 Postdoctoral Programs, 343
 10.2.2.5.1The Sloan/DOE Postdoctoral Awards for
Computational Molecular Biology, 343 10.2.2.5.2The Burroughs-Wellcome Career Awards at the
Scientific Interface, 344 10.2.2.5.3Keck Center for Computational and
Structural Biology: The Research Training Program, 34410.2.2.6 Faculty Retraining in Midcareer, 345
10.2.3Academic Organizations, 346

10.2.4Industry, 349
10.2.4.1 Major IT Corporations, 350

10.2.4.2 Major Life Science Corporations, 350

10.2.4.3 Start-up and Smaller Companies, 351
10.2.5Funding and Support, 352
10.2.5.1General Considerations, 352
10.2.5.1.1The Role of Funding Institutions, 352

10.2.5.1.2The Review Process, 352
Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONTENTSxix10.2.5.2 Federal Support, 353
10.2.5.2.1National Institutes of Health, 353

10.2.5.2.2National Science Foundation, 356

10.2.5.2.3Department of Energy, 357

10.2.5.2.4Defense Advanced Research Projects Agency, 359
10.3Barriers, 361
10.3.1Differences in Intellectual Style, 361
10.3.1.1Historical Origins and Intellectual Traditions, 361

10.3.1.2Different Approaches to Education and Training, 362

10.3.1.3The Role of Theory, 363

10.3.1.4Data and Experimentation, 365

10.3.1.5A Caricature of Intellectual Differences, 367
10.3.2Differences in Culture, 367
10.3.2.1The Nature of the Research Enterprise, 367

10.3.2.2Publication Venue, 369

10.3.2.3Organization of Human Resources, 369

10.3.2.4Devaluing the Contributions of the Other, 369

10.3.2.5Attitudinal Issues, 370
10.3.3Barriers in Academia, 371
10.3.3.1Academic Disciplines and Departmental Structure, 371

10.3.3.2Structure of Educational Programs, 372

10.3.3.3Coordination Costs, 373

10.3.3.4Risks of Retraining and Conversion, 374

10.3.3.5Rapid But Uneven Changes in Biology, 374

10.3.3.6Funding Risk, 375

10.3.3.7Local Cyberinfrastructure, 375
10.3.4Barriers in Commerce and Business, 375
10.3.4.1Importance Assigned to Short-term Payoffs, 375

10.3.4.2Reduced Workforces, 376

10.3.4.3Proprietary Systems, 376

10.3.4.4Cultural Differences Between Industry and Academia, 376
10.3.5Issues Related to Funding Policies and Review Mechanisms, 377
10.3.5.1Scope of Supported Work, 377

10.3.5.2Scale of Supported Work, 379

10.3.5.3The Review Process, 380
10.3.6Issues Related to Intellectual Property and Publication Credit, 381
11CONCLUSIONS AND RECOMMENDATIONS383
11.1Disciplinary Perspectives, 383
11.1.1The Biology-Computing Interface, 383

11.1.2Other Emerging Fields at the BioComp Interface, 384
11.2Moving Forward, 385
11.2.1Building a New Community, 386

11.2.2Core Principles for Practitioners, 387

11.2.3Core Principles for Research Institutions, 388
11.3The Special Significance of Educational Innovation at the BioComp Interface, 389
11.3.1Content, 389

11.3.2Mechanisms, 390
Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.xxCONTENTS11.4Recommendations for Research Funding Agencies, 392
11.4.1Core Principles for Funding Agencies, 392
11.4.2National Institutes of Health, 395

11.4.3National Science Foundation, 397

11.4.4Department of Energy, 397

11.4.5Defense Advanced Research Projects Agency, 398
11.5Conclusions Regarding Industry, 398

11.6Closing Thoughts, 399
APPENDIXESAThe Secrets of Life: A MathematicianÕs Introduction to Molecular Biology403
BChallenge Problems in Bioinformatics and Computational Biology from Other Reports429
CBiographies of Committee Members and Staff437
DWorkshop Participants443What Is CSTB?445Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY11Executive SummaryDespite some apparent differences, biology and information technology (IT) have much in com-mon. They are two of the most rapidly changing fields todayÑthe former because of enormous influxes
of new, highly heterogeneous data, and the latter because of exponentially decreasing price-perfor-
mance ratios. They both deal with entities of astounding complexity (organisms in the case of biology,
networks and computer systems in the case of information technology), although in the IT context, the
significance of the constituent connections and components is much better understood than in the
biological context. Also, they both have profound and revolutionary implications for science and soci-
ety. Biological science and technology have the potential to contribute strongly to society in improving
human health and well-being. The potential impacts include earlier diagnoses and more powerful
treatments for diseases, rapid environmental cleanup, and more robust food production. Computing
and information technology enable human beings to acquire, store, process, and interpret enormous
amounts of information that continue to underpin much of modern society.Against that backdrop, this report considers potential interactions between biology and comput-ingÑthe ÒBioCompÓ interface. To understand better the potential synergies at the BioComp interface
and to facilitate the development of new collaborations between the scientific communities in both
fields that can better exploit these synergies, the National Research Council established the Committee
on Frontiers at the Interface of Computing and Biology. For simplicity, this report uses ÒcomputingÓ to
refer to the broad domain encompassed collectively by terms such as computing, computation, model-
ing and simulation, computer science, computer engineering, informatics, information technology, sci-
entific computing, and computational science. (Analytical techniques without a strong machine-as-
sisted computational dimension are generally excluded from this study, although they are mentioned
from time to time when there is an interesting relationship to computing.) Similarly, the report uses the
term Ò21st century biologyÓ to refer to all fields of endeavor in the biological, biochemical, and biomedi-
cal sciences.Obviously, the union of computing with biology results in an extraordinarily broad area of interest.Thus, this report is not intended to be comprehensive in the sense of seeing how every subfield of
biology might connect to every topic in computing. Instead, it seeks to sample the intellectual terrain in
enough places so as to give the reader a sense of the kinds of activities under way, and its spirit shouldCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.2CATALYZING INQUIRYbe understood as Òletting a thousand flowers bloomÓ rather than Òidentifying the prettiest flowers inthe landscape.ÓCOMPUTINGÕS IMPACT ON BIOLOGY Twenty-first century biology will integrate a number of diverse intellectual notions. One integra-tion is that of the reductionist and systems approachesÑa focus on components of biological systems
combined with a focus on interactions among these components. A second integration is that of many
distinct strands of biological research: taxonomic studies of many species, the enormous progress in
molecular genetics, steps toward understanding the molecular mechanisms of life, and a consideration
of biological entities in relationship to their larger environment. A third integration is that computing
will become highly relevant to both hypothesis testing and hypothesis generation in empirical work in
biology. Finally, 21st century biology will also encompass what is often called discovery scienceÑthe
enumeration and identification of the components of a biological system independently of any specific
hypothesis about how that system functions (a canonical example being the genomic sequencing of
various organisms). Twenty-first century biology will embrace the study of an inclusive set of biological
entities, their constituent components, the interactions among components, and the consequences of
those interactions, from molecules, genes, cells, and organisms to populations and even ecosystems.How will computing play in 21st century biology? Life scientists have exploited computing formany years in some form or another. Yet what is different todayÑand will increasingly be so in the
futureÑis that the knowledge of computing needed to address many interesting biological problems
can no longer be learned and exploited simply by ÒhackingÓ and reading the manuals. Indeed, the kinds
and levels of expertise needed to address the most challenging problems of 21st century biology stretch
the current state of knowledge of the fieldÑa point that illuminates the importance of real computing
research in a biological context.This report identifies four distinct but interrelated roles of computing for biology.1.Computational tools are artifactsÑusually implemented as software but sometimes hardwareÑthat enable biologists to solve very specific and precisely defined problems. Such biologically
oriented tools acquire, store, manage, query, and analyze biological data in a myriad of forms
and in enormous volume for its complexity. These tools allow biologists to move from the study
of individual phenomena to the study of phenomena in a biological context; to move across vast
scales of time, space, and organizational complexity; and to utilize properties such as evolution-
ary conservation to ascertain functional details.2.Computational models are abstractions of biological phenomena implemented as artifacts that canbe used to test insights, to make quantitative predictions, and to help interpret experimental
data. These models enable biological scientists to understand many types of biological data in
context, even in very large volume, and to make model-based predictions that can then be tested
empirically. Such models allow biological scientists to tackle difficult problems that could not
readily be posed without visualization, rich databases, and new methods for making quantita-
tive predictions. Biological modeling itself has become possible because data are available in
unprecedented richness and because computing itself has matured enough to support the analy-
sis of such complexity.3.A 
computational perspective on or metaphor for biology applies the intellectual constructs of com-puter science and information technology as ways of coming to grips with the complexity of
biological phenomena that can be regarded as performing information processing in different
ways. This perspective is a source of information and computing abstractions that can be used to
interpret and understand biological mechanisms and function. Because both computing and
biology are concerned with function, information and computing abstractions can provide well-
understood constructs that can be used to characterize the biological function of interest. Further,Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY3such abstractions may well provide an alternative and more appropriate language and set ofabstractions for representing biological interactions, describing biological phenomena, or con-
ceptualizing some characteristics of biological systems.4.Cyberinfrastructure and data acquisition are enabling support technologies for 21st century biology.CyberinfrastructureÑhigh-end general-purpose computing centers that provide supercomputing
capabilities to the community at large; well-curated data repositories that store and make avail-
able to all researchers large volumes and many types of biological data; digital libraries that
contain the intellectual legacy of biological researchers and provide mechanisms for sharing,
annotating, reviewing, and disseminating knowledge in a collaborative context; and high-speed
networks that connect geographically distributed computing resourcesÑwill become an en-
abling mechanism for large-scale, data-intensive biological research that is distributed over mul-
tiple laboratories and investigators around the world. New data acquisition technologies such as
genome sequencers will enable researchers to obtain larger amounts of data of different types
and at different scales, and advances in information technology and computing will play key
roles in the development of these technologies.Why is computing in all of these roles needed for 21st century biology? The answer, in a word, isdata. The data relevant to 21st century biology are highly heterogeneous in content and format,
multimodal in method of collection, multidimensional in time and space, multidisciplinary in creation
and analysis, multiscale in organization, international in relevance, and the product of collaborations
and sharing. Consider, for example, that biological data may consist of sequences, graphs, geometric
information, scalar and vector fields, patterns of organization, constraints, images, scientific prose, and
even biological hypotheses and evidence. These data may well be of very high dimension, since data
points that might be associated with the behavior of an individual unit must be collected for thousands
or tens of thousands of comparable units.These data are windows into structures of immense complexity. Biological entities (and systemsconsisting of multiple entities) are sufficiently complex that it may well be impossible for any human
being to keep all of the essential elements in his or her head at once; if so, it is likely that computers will
be the vessel in which biological theories are held, formed, and evaluated. Furthermore, because of
evolution and a long history of environmental accidents that have driven processes of natural selection,
biological systems are more properly regarded as engineered entities than as objects whose existence
might be predicted on the basis of the first principles of physics, although the evolutionary context
means that an artifact is never ÒfinishedÓ and rather has to be evaluated on a continuous basis. The task
of understanding thus becomes one of Òreverse engineeringÓÑattempting to understand the construc-
tion of a device about whose design little is known but from which much indicative empirical data can
be extracted.Twenty-first century biology will be an information science, and it will use computing and informa-tion technology as a language and a medium in which to manage the discrete, nonsymmetric, largely
nonreducible, unique nature of biological systems and observations. In some ways, computing and
information will have a relationship to the language of 21st century biology that is similar to the
relationship of calculus to the language of the physical sciences. Computing itself can provide biologists
with an alternative, and possibly more appropriate, language and sets of intellectual abstractions for
creating models and data representations of higher-order interactions, describing biological phenom-
ena, and conceptualizing some characteristics of biological systems.BIOLOGYÕS IMPACT ON COMPUTINGFrom the computing side (i.e., for the computer scientist), there is an as-yet-unfulfilled promise thatbiology may have significant potential to influence computer design, component fabrication, and soft-
ware. The essential premise is that biological systems possess many qualities that would be desirable inCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.4CATALYZING INQUIRYthe information technology that humans use. For example, computer and information scientists arelooking for ways to make computers more adaptive, reliable, Òsmarter,Ó faster, and resilient. Biological
systems excel at finding and learning goodÑbut not necessarily optimalÑsolutions to ill-posed prob-
lems on time scales short enough to be useful to them. They efficiently store Òdata,Ó integrate Òhard-
wareÓ and Òsoftware,Ó self-correct, and have many other properties that computing and information
science might capture in order to achieve its future goals. Especially for areas in which computer science
lacks a well-developed theory or analysis (e.g., the behavior of complex systems or robustness), biology
may have the most to contribute.The impact of biology and biological sciences on advances in computing is, however, more specula-tive than the reverse, because such considerations are, with only a few exceptions, relevant to future
outcomes and not to what has been or is already being delivered. Humans understand computing
artifacts much better than they do biological organisms, largely because humans have been responsible
for the design of computing artifacts. Absent a comparable base of understanding of biological organ-
isms, the historical and contemporary contributions from biology to computing have been largely
metaphorical and can be characterized more readily as inspiration, rather than advances having a
straightforward or linear impact.This difference may be one of time scale. Because todayÕs computing already contributes directly inan essential way to advancing biological knowledge, a path for the near-term future can be readily
described. Contemporary advances in computing provide new opportunities for understanding biol-
ogy, and this will continue to be true for the foreseeable future. Advances in biological understanding
may yet have enormous value for changing computing paradigms (e.g., as may be the case if neural
information processing is understood more fully)Ñbut these advances are themselves contingent on
work done over a considerably longer time scale.ILLUSTRATIVE PROBLEM DOMAINS AT THE BIOCOMP INTERFACEBoth life scientists and computer scientists will draw inspiration and derive utility from otherfieldsÑincluding each otherÕsÑas they see fit. Nevertheless, one way of making progress is to address
problems that emerge naturally at the BioComp interface. Problem-focused research carries the major
advantage that problems offered by nature do not respect disciplinary boundaries; hence, in making
progress against challenging problems, practitioners of different disciplines must learn to work on
problems that are shared.The BioComp interface drives many problem domains in which the expenditure of serious intellec-tual effort can reasonably be expected to generate significant new knowledge in biology and/or com-
puting. Compared to many of grand challenges in computational biology outlined over the past two
decades, making significant progress in these problem domains will call for a longer time scale, greater
resources, and more extensive basic progress in computing and in biology.Biological insight could take different formsÑthe ability to make new predictions, the understand-ing of some biological mechanism, the construction of a synthetic biological mechanism. The same is
true for computingÑinsight might take the form of a new biologically inspired approach to some
computing problem, different hardware, or novel architecture.This report discusses a number of interesting problem domains at the BioComp interface, but giventhe breadth of the cognizant scientific arenas, no attempt is made to be exhaustive. Rather, topics have
been selected to span a space of possible problem domains, and no inferences should be made concern-
ing the omission of any problem from this list. The problem domains discussed in this report include
high-fidelity cellular modeling and simulation, the development of a synthetic cell, neural information
processing and neural prosthetics, evolutionary biology, computational ecology, models that facilitate
individualized medicine, a digital human on which a surgeon can operate virtually, computational
theories of self-assembly and self-modification, and a theory of biological information and complexity.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY5THE ROLE OF ORGANIZATION AND INFRASTRUCTURE IN CREATINGOPPORTUNITIES AT THE INTERFACEThe committee believes that over time, computing will assume an increasing role in the workinglives of nearly all biologists. But given the societal benefits that accompany a fuller and more systematic
understanding of biological phenomena, it is better if the computing-enabled 21st century biology
arrives sooner rather than later.This point suggests that cultural and organizational issues have at least as much to do with thenature and scope of the biological embrace of computing as do intellectual ones. The report discusses
barriers to cooperation arising from differences in organizational culture and differences in intellectual
style.Consider organizational cultures. In many universities, for example, it is difficult for scholars work-ing at the interface between two fields to gain recognition (e.g., tenure, promotion) from eitherÑa fact
that tends to drive such individuals toward one discipline or another. The short-term goals in industrial
settings also inhibit partnerships along the interface because of the longer time frame for payoff. None-
theless, the committee believes that a synergistic cooperation between practitioners in each field, in both
basic and applied settings, will have enormous payoffs despite the real differences in intellectual style.Coordination costs are another issue, because they increase with interdisciplinary work. Computerscientists and biologists are likely to belong to different departments or universities, and when they try
to work together, the lack of physical proximity makes it harder for collaborators to meet, to coordinate
student training, and to share physical resources. In addition, bigger projects increase coordination
costs, and interdisciplinary projects are often larger than unidisciplinary projects. Such costs are re-
flected in delays in project schedules, poor monitoring of progress, and an uneven distribution of
information and awareness of what others in the project are doing. They also reduce peopleÕs willing-
ness to tolerate logistical problems that might be more tolerable in their home contexts, increase the
difficulty of developing mutual regard and common ground, and can lead to more misunderstandings.Differences of intellectual style occur because the individuals involved are first and foremost intel-lectuals. For example, for the computer scientist, the notions of modeling systems and using abstrac-
tions are central to his or her work. Using these abstractions and models, computer scientists are able to
build some of the most complex artifacts known. But manyÑperhaps mostÑbiologists today have a
deep skepticism about theory and models, at least as represented by mathematics-based theory and
computational models. And many computer scientists, mathematicians, and other theoretically inclined
researchers fail to recognize the complexity inherent in biological systems. As a result, there is often an
intellectual tension between simplification in service of understanding and capturing details in service
of fidelityÑand such a tension has both positive and negative consequences.Cooperation will require that practitioners in each field learn enough about the other to engage insubstantive conversations about hard biological problems. To take one of the most obvious examples,
the different fields place different emphases on the role of empirical data vis-‹-vis theory. Accurate data
from biological organisms impose ÒhardÓ constraints on the biologist in much the same way that results
from theoretical computer science impose hard constraints on the computer scientist. A second example
is that whereas computer scientists are trained to develop general solutions that give guarantees about
events in terms of their worst-case performance, biologists are interested in specific solutions that relate
to very particular (though voluminous) datasets.Finally, institutional difficulties often arise in academic settings for work that is not traditional ornot easily identified with existing departments. These differences derive from the structure and culture
of departments and disciplines, and they lead to scientists in different disciplines having different
intellectual and professional goals and experiencing different conditions for their career success.  Col-

laborators from different disciplines must find and maintain common ground, such as agreeing on
goals for a joint project, but must also respect one anotherÕs separate priorities, such as having to
publish in primary journals, present at particular conferences, or obtain tenure in their respectiveCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.6CATALYZING INQUIRYdepartments according to departmental criteria. Such cross-pressures and expectations from homedepartments and disciplinary colleagues remain even if the participants in a collaboration develop
similar goals for a project.FINDINGS AND RECOMMENDATIONS At the outset, the committee had hoped to identify a deep symmetry between computing andbiology. That is, it is clear that the impact of computing on biology is increasingly profound, and the
symmetrical notion would be that biology would have a comparable effect on computing. However,
this proved not to be the case. The impact of computing on biology will be deep and profound, and
indeed will span virtually all areas of life sciences research, and in this direction a focus on interesting
problem domains (some of which are illustrated above) is a reasonable way to proceed. By contrast,
research that explores the impact of biology on computing falls much more into the Òhigh-risk, high-
payoffÓ category. That is, the ultimate value of biology for changing computing paradigms in deep and
fundamental ways is as yet unproven. Nevertheless, various biological attributesÑrobustness, adapta-
tion, damage recovery, and so onÑare so desirable from a computing point of view that any intellectual
inquiry is valuable if it can contribute to human-engineered artifacts with these attributes.It is also clear that a number of other areas of inquiry are associated with the BioComp interface; inaddition to biology and computing, the interface also draws from chemistry, materials science, bioengi-
neering, and biochemistry. Three of the most important efforts, which can be loosely characterized as
different flavors of biotechnology, are (1) analytical biotechnology (which involves the application of
biotechnological tools for the creation of chemical measurement systems); (2) materials biotechnology
(which entails the use of biotechnological methods for the fabrication of novel materials with unique
optical, electronic, rheological, and selective transport properties); and (3) computational biotechnology
(which focuses on the potential replacement of silicon devices with nanoscale biomolecular-based com-
putational systems).The committee underscores the importance of building human capital and, within that enterprise,the special significance of educational innovation at the BioComp interface. The committee endorses the
call from other reports that recommend greater training in quantitative sciences (e.g., mathematics,
computer sciences) for biologists, but it also believes that students of the new biology would benefit
greatly from some study of engineering. Just as engineers must construct physical systems to operate in
the real world, so also must nature operate under these same constraintsÑphysical lawsÑto ÒdesignÓ
successful organisms. Despite this fundamental similarity, biology students rarely learn the important
analysis, modeling, and design skills common in engineering curricula. The committee believes that the
particular area of engineering (electrical, mechanical, computer, etc.) is probably much less relevant
than exposure to essential principles of engineering design: the notion of trade-offs in managing com-
peting objectives, control systems theory, feedback, redundancy, signal processing, interface design,
abstraction, and the like.Of course, more than education will have to change. Fifty years ago, academic biology had tochoose between altering the then-dominant styles of research to embrace molecular biology or risking
obsolescence. The committee believes that a new dawn is visibleÑand just as molecular biology has
become simply part of the biological sciences as a whole, so also will computational biology ultimately
become simply a part of the biological sciences. In the interim, however, considerable effort will be
required to build and sustain the infrastructure and to train a generation of biologists and computer
scientists who can choose the right collaborators to thrive at the BioComp interface.The committee believes that 21st century biology will be based on a synergistic mix of reductionistand systems biologies. For systems biology researchers, the committee emphasizes that empirical and
experimental hypothesis-testing research will continue to be central in providing experimental verifica-
tion of putative discoveriesÑand indeed, relevant as much to studies of how components interact as to
studies of components themselves. Thus, disparaging rhetoric about the inadequacies and failures ofCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.EXECUTIVE SUMMARY7reductionist biology and overheated zeal in promoting systems biology should be avoided. For re-searchers more oriented toward experimental or empirical work, the committee emphasizes that sys-
tems biology will be central in formulating novel, interesting, and in some cases counterintuitive hy-
potheses to test. The point suggests that agencies that have traditionally supported hypothesis-testing
research would do well to cast a wide ÒdiscoveryÓ net that supports the development of alternative
hypotheses as well as research that supports traditional hypothesis testing.Twenty-first century biology will require leadership from both biology and computing that linkstogether first-class research efforts in their respective domains. These efforts will necessarily cross
traditional institutional boundaries. For example, research efforts in scientific computing will have to
exist in both clinical and biological environments if they are to couple effectively to problem domains in
the life sciences. Establishment of a pervasive national infrastructure for life sciences research (includ-
ing the construction of interdisciplinary teams) and development of the requisite IT-enabled tools for
the larger community will require both sustained funding and rigorous oversight. Likewise, the depart-
mental imperatives that characterize much of academe will have to be modified if work at the BioComp
interface is to flourish.In general, the committee believes that the most important change in funding policy for the sup-porters of this area would be to broaden the kinds of work for which they offer support to include the
development of technology for data acquisition and analysis and exploratory research that results in the
generation of interesting hypotheses to be tested. That said, there is a direct relationship between the
speed with which research frontiers advance and the levels of funding allocated to them. Although it
understands the realities of a budget-constrained environment, the committee would gladly endorse an
increased flow of funding to the furtherance of a truly integrated 21st century biology.As for the support of biologically inspired computing, the committee believes that its high-risk,high-payoff nature means that supporting agencies should take a broad view of what Òbiological inspi-
rationÓ means and should support the field on a level-of-effort basis, recognizing the long-term nature
of such work and taking into account the number of researchers doing and likely to do good work in
this area and the potential availability of other avenues to improved computing.From the committeeÕs perspective, the high-level goals articulated by the agencies and programsthat support work related to biologyÕs potential contribution to computing seem generally sensible.
This is not to say that every proposal supported under the auspices of these agenciesÕ programs would
necessarily have garnered the support of the committeeÑbut that would be true of any research portfo-
lio associated with any program.One important consequence of supporting high-risk research is that it is unlikely to be successful inthe short term. ResearchÑparticularly of the high-risk varietyÑis often more ÒmessyÓ and takes longer
to succeed than managers would like. Managers understandably wish to terminate unproductive lines
of inquiry, especially when budgets are constrained. But short-term success cannot be the only metric of
the value of research, because when it is, funding managers invite hyperbole and exaggeration on the
part of proposal submitters, and unrealistic expectations begin to characterize the field. Those believing
the hyperbole (and those contributing to it as well) thus overstate the importance of the research and its
centrality to the broader goal of improving computing. When unrealistic expectations are not met (and
they will not be met, almost by definition), disillusionment sets in, and the field becomes disfavored
from both a funding and an intellectual standpoint.From this perspective, it is easy to see why support for certain fields rises rapidly and then dropsprecipitously. Wild budget fluctuations and an unpredictable funding environment that changes goals
rapidly can damage the long-term prospects of a field to produce useful and substantive knowledge.
Funding levels do matter, but programs that provide steady funding in the context of broadly stated but
consistent intellectual goals are more likely to yield useful results than those that do not.Thus, the committee believes that in the area of biologically inspired computing, funding agenciesshould have realistic expectations, and these expectations should be relatively modest in the near term.
Intellectually, their programs should continue to take a broad view of what Òbiological inspirationÓCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.8CATALYZING INQUIRYmeans. Funding levels in these areas ought to be established on a level-of-effort basis (i.e., what theagency believes is a reasonable level of effort to be expended in this area), by taking into account the
number of researchers doing and likely to do good work in an area and the potential availability of other
avenues to improved computing. In addition, programmatic continuity for biologically inspired com-
puting should be the rule, with playing rules and priorities remaining more or less constant in the
absence of profound scientific discovery or technology advances in the area.CLOSING THOUGHTSThe impact of computing on biology can fairly be considered a paradigm change as biology entersthe 21st century. Twenty-five years ago, biology saw the integration of multiple disciplines from the
physical and biological sciences and the application of new approaches to understand the mechanisms
by which simple bacteria and viruses function. The impact of the early efforts was so significant that a
new discipline, molecular biology, emerged, and many biologists, including those working at the level
of tissues or systems and whole organisms, came to adopt the approaches and even often the tech-
niques. Molecular biology has had such success that it is no longer a discipline but simply part of life
sciences research itself.Today, the revolution lies in the application of a new set of interdisciplinary tools: computationalapproaches will provide the underpinning for the integration of broad disciplines in developing a
quantitative systems approach, an integrative or synthetic approach to understanding the interplay of
biological complexes as biological research moves up in scale. Bioinformatics provides the glue for
systems biology, and computational biology provides new insights into key experimental approaches
and how to tackle the challenges of nature. In short, computing and information technology applied to
biological problems is likely to play a role for 21st century biology that is in many ways analogous to the
role that molecular biology has played across all fields of biological research for the last quarter-
centuryÑand computing and information technology will become embedded within biological re-
search itself.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION991Introduction
1.1EXCITEMENT AT THE INTERFACE OF COMPUTING AND BIOLOGY
Sustained progress across all areas of science and technology over the last half-century has trans-formed the expectations of society in many ways. Yet, even in this context of extraordinary advances,
both the biological sciences and the computer and information sciences share a number of characteris-
tics that are compelling.First, both fields have been characterized by exponential growth, with doubling times on the orderof 1-2 years. In information technology (IT), both the component density of microprocessors and theinformation storage density on hard disk drives have increased exponentially with doubling times from
9 to 18 months. In biology, the rate of growth of the biological literature is characterized by exponential
growth as well (e.g., the growth in GenBank is on the order of 60 percent per year, a rate comparable to
MooreÕs law for microprocessors). While these growth rates cannot continue indefinitely, exponential
growth is likely at least in the short term.Second, both fields deal with organisms and phenomena or artifacts of astounding complexity.Both biological organisms and sophisticated computer systems involve very large numbers of compo-
nents and interconnections between them, and out of these assemblages of components and connec-
tions emerges interesting and useful functionality. In the information technology context, the signifi-
cance of these connections and components is much better understood than in the biological context,
not least because human beings have been responsible for the design of information technology
systems such as operating systems and computer systems. Still, the capabilities of existing computing
methodologies to design or characterize large-scale information systems and networks are being
stretched, and in the biological domain, a systems-level understanding of biological or computer
networks is both highly important and difficult to achieve. In addition, information technology is a
necessary and enabling technology for the study of complex objects. Computers are the scientific
instruments that let us see genomes just as electron microscopes let us see viruses, or radio telescopes
let us see quasars.Third, both biology and information technology have profound and revolutionary implications forscience and society. From an intellectual standpoint, biology offers at least partial answers to eternal
questions such as, What is life? Also, biological science and technology have the potential for great
impact on human health and well-being, including improved disease treatments, rapid environmentalCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.10CATALYZING INQUIRYcleanup, and more robust food production. Computing and information technology enable humanbeings to acquire, store, process, and interpret enormous amounts of information, and continue to
underpin much of modern society.Finally, several important areas of interaction between the two fields have already emerged, andthere is every expectation that more will emerge in the future. Indeed, the belief of the committee that
there are many more synergies at the interface between these two fields than have been exploited to
date is the motivation for this report. Against this backdrop, it makes good sense to consider potential
interactions between the two fieldsÑwhat this report calls the ÒBioCompÓ interface.As for the nature of computing that can usefully be exploited by life scientists, there is a range ofpossibilities. For some problems encountered by biology researchers, a very rudimentary knowledge of
computing and information technology is quite sufficient. However, as problems become bigger and/or
more complex, what one may pick up by hacking and reading manuals is no longer sufficient. To
address such problems, the kinds and levels of expertise needed are more likely to require significant
formal study of computer science (e.g., as an undergraduate major in the field). And for still more
difficult, larger, or more complex problems, the kinds and levels of expertise needed stretch the current
state of knowledge of the fieldÑa point that illuminates the importance of real computer science
research in a biological context.Nor is the utility of computing limited to providing tools or modelsÑno matter how sophisti-catedÑfor biologists to use. As discussed in Chapter 6, computing can also provide intellectual abstrac-
tions that may provide insight into biological phenomena and a useful language for describing such
phenomena. As one example, notions of circuit and network and modularityÑoriginally conceptual-
ized in the world of engineering and computer scienceÑhave much applicability to understanding
biological phenomena.On the other side, biology refers to the scientific study of the activities, processes, mechanisms,and other attributes of living organisms. For the purposes of this report, biology, biomedicine, life
sciences, and other descriptions of research into how living systems work should be regarded as
synonymous. In this context, for the past decade, researchers have spoken increasingly of a new
biology, a biology of the 21st century, one that is driven by new technologies, that is more automated
with tools and methods provided by industrial models, and that often entails high-throughput data
acquisition.1 This report examines the BioComp interface from the perspective of 21st century biol-
ogy, as a science that integrates traditional empirical and experimental biology with a systems-level
biology that considers the multiscale, hierarchical, highly interwoven, or interactive aspects intrinsic
to living systems.1.2PERSPECTIVES ON THE BIOCOMP INTERFACE
This report addresses computationally inspired ways of understanding biology and biologicallyinspired ways of understanding computing. Although the committee started its work with the idea that
it would discover a single community and intellectual synthesis of biology and computing, closer
examination showed that the appropriate metaphor is one of an interface between the two fields rather
than a common, shared area of inquiry. Thus, the adventures along the frontier cannot be treated as
coming from a single community, and the different objectives have to be recognized.1For example, see National Research Council, Opportunities in Biology, National Academy Press, Washington, DC, 1989. High-throughput data acquisition is an approach that relies on the large-scale parallel interrogation of many similar biological entities.Such an approach is essential for the conduct of global biological analyses, and it is often the approach of choice for rapid andcomprehensive assessment of biological system properties and dynamics. See, for example, T. Ideker, T. Galitski, and L. Hood,ÒA New Approach to Decoding Life: Systems Biology,Ó Annual Review of Genomics and Human Genetics 2:343-372, 2001. A numberof the high-throughput data acquisition technologies mentioned in that article are discussed in Chapter 7 of his report.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION111.2.1From the Biology Side
Biologists have a long history of applying tools from other disciplines to provide more powerfulmethods to address or even solve their research problems. For example, Anton Van LeeuwenhoekÕs
invention of the optical microscope in the late 1600s opened up a previously unknown world and
ultimately brought an entirely new vista to biologyÑnamely, the existence of cells and cellular struc-
tures. This remarkable revolutionary discovery would have been impossible without the study of
opticsÑand Leeuwenhoek was a clockmaker.The biological sciences have drawn heavily from chemistry, physics, and more recently, mathemati-cal modeling. Indeed, the reductionist revolution in biological sciencesÑwhich led to the current state
of understanding of biological function and mechanism at the molecular level or of specific areas such
as neurophysiologyÑin the past five decades began as chemists, physicists, microbiologists, and others
interacted and created what is now known as molecular biology. The applications from the physical
sciences are already so well established that it is unnecessary to discuss them at length.Mathematics and statistics have at times played important roles in designing and optimizing bio-logical experiments. For example, statistical analysis of preliminary data can lead to improved data
collection and interpretation in subsequent experiments. In many cases, simple mathematical or physi-
cal ideas, accompanied by calculations or models, can suggest experiments or lead to new ideas that are
not easily identified with biological reasoning alone. An example of this category of contribution is
William HarveyÕs estimation of the volume of the blood and his finding that a closed circulatory system
would explain the anomaly in such calculations. Traditionally, biologists have resisted mathematical
approaches for various reasons discussed at length in Chapter 10. To some extent, this history is being
changed in modern biology, and it is the premise of this report that an acceleration of this change is
highly worthwhile.Approaches borrowed from another discipline may provide perspectives that are unavailable frominside the disciplinary research program itself. In some cases, these lead to a new integrative explana-
tion or to new ways of studying and appreciating the intricacies of biology. In other cases, this borrow-
ing opens an entirely new subfield of biology. The discovery of the helical structure and the ÒcodeÓ of
DNA, impossible without crystallography and innovative biological thinking, is one example. The
understanding of electrical signaling in neurons by voltage-gated channels, and the Hodgkin-Huxley
equations (based on the theory of electrical circuits), constitute another example. Both of these ap-
proaches revolutionized the way biology was conducted and required significant, skilled input from
other fields.The most dramatic scenarios arise when major subfields emerge. An example dating back somedecades, and described above in another context, is molecular biology, whose tools and techniques
(using advanced chemistry, physics, and equipment based on the above) changed the face of biology. A
more recent, current example is genomics with its indelible mark on the way that biology as a discipline
is conducted and will be conducted for years to come.The committee believes that from the perspective of the biology researcher, there is both substantiallegacy and future promise regarding the application of computing to biological problems. Some of this
legacy is manifested in a several-decade development of private-sector databases (mostly those of
pharmaceutical companies) and software for data analysis, in public-sector genetic databases, in the use
of computer-generated visualization, and in the use of computation to determine the crystal structures
of increasingly complex biomolecules.2Several life sciences research fields have begun to take computational approaches. For example,ecology and evolution were among the first subfields of biology to develop advanced computational
simulations based on theory and models of ecosystems and evolutionary pathways. Cardiovascular2See, for example, T. Head-Gordon and J.C. Wooley, ÒComputational Challenges in Structural and Functional Genomics,Ó IBMSystems Journal 40(2):265-296, 2001, available at http://www.research.ibm.com/journal/sj/402/headgordon.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.12CATALYZING INQUIRYphysiology and studies of the structure and function of heart muscle have involved bioengineeringmodels and combined experimental and computational approaches. All of these computational ap-
proaches would have been impossible without solid preexisting mathematical models that led to the
intuition and formed the basis for the emerging computational aspects.Nevertheless, genomics research is simply not possible without information technology. It is not anexaggeration to say that it was the sequencing of complete genomes, more than any other research
activity, that brought computational and informatics approaches to the forefront of life sciences re-
search, as well as identifying the need for basic underlying algorithms to tackle biological problems.
Only through computational analysis have researchers begun to uncover the implications of genomic-
scale sequence data. Apart from specific results thereby obtained, such analysis, coupled with the
availability of complete genomic sequences, has changed profoundly how many biologists think, con-
duct research, and plan strategically to address central research problems.Today, computing is essential to every aspect of molecular and cell biology, as researchers expandtheir scope of inquiry from gene sequence analysis to broader investigations of biological complexity.
This scope includes the structure and function of proteins in the context of metabolic, genetic, and
signaling networks, the sheer complexity of which is overwhelming. Future challenges include the
integration of organ physiology, catalogs of species-wide phenotypic variations, and understanding of
differences in gene expression in various states of health and disease.1.2.2From the Computing Side
From the viewpoint of the computer scientist, there is an as-yet-unfulfilled promise that biologymay have significant potential to influence computer design, component fabrication, and software.
Today, the impact of biology and biological sciences on advances in computing is more speculative than
the reverse (as described in Section 1.2.1), because such considerations are, with only a few exceptions,
relevant to future outcomes and not to what has been or is already being delivered.In one sense, this should not be very surprising. Computing is a Òscience of the artificial,Ó3 whereasbiology is a science of the natural, and in general, it is much easier for humans to understand both the
function and the behavior of a system that they have designed to fulfill a specific purpose than to
understand the internal machinery of a biological black box that evolved as a result of forms and
pressures that we can only sketchily guess.4 Thus, paths along which biology may influence computingare less clear than the reverse, and work in this area should be expected to have longer time horizons
and to take the form of many largely independent threads, rather than a hierarchy of interrelated or
intellectual thrusts.Nevertheless, exploring why the biological sciences might be relevant to computing is worthwhilein particular because biological systems possess many qualities that would be desirable in the informa-
tion technology that humans use. For example, computer and information scientists are looking for
ways to make computers more adaptive, reliable, Òsmarter,Ó faster, and resilient. Biological systems
excel at finding and learning adequateÑbut not necessarily optimalÑsolutions to ill-posed problems
on time scales short enough to be useful to them. They efficiently store Òdata,Ó integrate ÒhardwareÓ
and Òsoftware,Ó self-correct, and have many other properties that computing and information science3ÒWe speak of engineering as concerned with Ôsynthesis,Õ while science is concerned with Ôanalysis.Õ Synthetic or artificial ob-jectsÑand more specifically prospective artificial objects having desired propertiesÑare the central objective of engineering activityand skill. The engineer, and more generally the designer, is concerned with how things ought to beÑhow they ought to be in orderto attain goals, and to function.Ó H.A. Simon, Sciences of the Artificial, 3rd ed., MIT Press, Cambridge, MA, 1996, pp. 4-5.
4This is what neuroscientist Valentino Braitenberg called his law of uphill analysis and downhill synthesis, in Vehicles: Experi-ments in Synthetic Psychology, MIT Press/A Bradford Book, Cambridge, MA, 1984. Cited in Daniel C. Dennett, ÒCognitive Scienceas Reverse Engineering: Several Meanings of ÔTop-downÕ and ÔBottom-upÕ,Ó Proceedings of the Ninth International Congress of Logic,Methodology and Philosophy of Science, D. Prawitz, B. Skyrms, and D. Westerstahl, eds., Elsevier Science North-Holland, 1994.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION13might capture in order to achieve its future goals. Especially for areas in which computer science lacksa well-developed theory or analysis (e.g., the behavior of complex systems or robustness), biology may
have the most to contribute.To hint at some current threads of inquiry, some researchers envision a hybrid deviceÑa biologicalcomputerÑessentially, an organic tool for accomplishing what is now carried out in silicon. As an
information storage and processing medium, DNA itself may someday be the substance of a massively
dense memory storage device, although today the difficulties confronting the work in this area are
significant. DNA may also be the basis of nanofabrication technologies.Biomimetic devices are mechanical, electrical, or chemical systems in which an attempt has beenmade to mimic the way that a biological system solves a particular problem. Successes include robotic
locomotion (based on legged movements of arthropods), artificial blood or skin, and others. Approaches
with general-purpose applicability are less clearly successes, though they are still intriguing. These
include attempts to develop approaches to computer security that are modeled on the mammalian
immune system and approaches to programming based on evolutionary concepts.Hybrid systems are a promising new technology for measurement of or interaction with smallbiological systems. In this case, hybrid systems refer to silicon chips or other devices designed to
interact directly with a biological sample (e.g., record electrical activity in the flight muscles of a moth)
or analyze a small biological sample under field conditions. Here the applications of the technology
both to basic scientific problems and to industrial and commercially viable products are exciting.In the domain of algorithms, swarm intelligence (a property of certain systems of nonintelligent,independently acting agents that collectively exhibit intelligent behavior) and neural nets offer ap-
proaches to programming that are radically different from many of todayÕs models. Such applications of
biological principles to nonbiological computing could have much value, and Chapter 8 addresses in
greater detail some possible biological inspirations for computing. Yet it is also possible that a better
understanding of information-processing principles in biological systems will lead as well to greater
biological insight; so the dividing line between Òapplying biological principles to information process-
ingÓ and Òunderstanding biological information processingÓ is not as clear as it might appear at
first glance. Moreover, even if biology ultimately proves unhelpful in providing insight into potential
computing solutions, it is still a problem domain par excellenceÑone that offers interesting intellec-
tual challenges in which progress will require that the state of computing research be stretched
immeasurably.1.2.3The Role of Organization and Culture
The possibilityÑor even the factÑthat one field may be well positioned to make or facilitate signifi-cant intellectual contributions to the other does not, by itself, lead to harmonious interchange between
practitioners in the two fields. Cultural and organizational issues are also very much relevant to the
success or failure of collaborations across different fields. For example, one important issue is the fact
that much of todayÕs biological research is done in individual laboratories, whereas many interesting
problems of 21st century biology will require interdisciplinary teams and physical or virtual centers
with capable scientists, distributed wherever they work, involved in addressing difficult problems.Twenty-first century biology will also see the increasing importance of research programs that havea more industrial flavor and involve greater standardization of instruments and procedures. A small
example is that reagent kits are becoming more and more popular, as labs realize that the small advan-
tages that might accrue through the use of a set of customized reagents are far outweighed by the
savings in effort associated with the use of such kits. A larger example might be shared devices and
equipment of larger-scale and assembly-line-like processes that replace the craft work of individual
technicians.As biologists recognize the inherent difficulties posed by the data-intensive nature of these newresearch strategies, they will require differentÑand additionalÑtraining in quantitative methods andCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.14CATALYZING INQUIRYscience. Computing is likely to be central, but since the nature and scope of the computing required willgo far beyond what is typically taught in an introductory computing course, real advancement of the
frontier will require that computer scientists and biologists recognize and engage each other as intellec-
tual coequals. At the same time, computer scientists will have to learn enough about biology to under-
stand the nature of problems interesting to biologists and must refrain from regarding the problem
domain as a ÒmereÓ application of computing.The committee believes that such peer-level engagement happens naturally, if slowly. But accelerat-ing the cultural and organizational changes needed remains one of the key challenges facing the commu-
nities today and is one that this report addresses. Such considerations are the subject of Chapter 10.1.3Imagine WhatÕs Next
In the long term, achievements in understanding and harnessing the power of biological systemswill open the door to the development of new, potentially far-reaching applications of computing and
biologyÑfor example, the capability to use a blood or tissue sample to predict an individualÕs suscepti-
bility to a large number of afflictions and the ability to monitor disease susceptibility from birth,
factoring in genetics and aging, diet, and other environmental factors that influence the bodyÕs func-
tions over time and ultimately to treat such ailments.Likewise, 21st century biology will advance the abilities of scientists to model, before a treatment isprescribed, the likely biological response of an individual with cancer to a proposed chemotherapy
regime, including the likelihood of the effectiveness of the treatment and the side effects of the drugs.
Indeed, the promise of 21st century biology is nothing less than a system-wide understanding of bio-
logical systems both in the aggregate and for individuals. Such understanding could have dramatic
effects on health and medicine. For example, detailed computational models of cellular dynamics could
lead to mechanism-based target identification and drug discovery for certain diseases such as cancer,5to predictions of drug effects in humans that will speed clinical trials,6 and to a greater understandingof the functional interactions between the key components of cells, organs, and systems, as well as how
these interactions change in disease states.7On another scale of knowledge, it may be possible to trace the genetic variability in the worldÕshuman populations to a common ancestral set of genesÑto discover the origins of the earliest humans,
while learning, along the way, about the earliest diseases that arose in humans, and about the biological
forces that shape the worldÕs populations. Work toward all of these capabilities has already begun, as
biologists and computer scientists compile and consider vast amounts of information about the genetic
variability of humans and the role of that variability in relation to evolution, physiological functions,
and the onset of disease.At the frontiers of the interface, remarkable new devices can be pictured that draw on biology forinspiration and insight. It is possible to imagine, for example, a walking machineÑan independent set
of legs as agile, stable, and energy-efficient as those of humans or animalsÑable to negotiate unknown
terrain and recover from falls, capable of exploring and retrieving materials. Such a machine would
overcome the limitations of present-day rovers that cannot do such things. Biologists and computer
scientists have begun to examine the locomotion of living creatures from an engineering and biological
perspective simultaneously, to understand the physical and biological controls on balance, gait, speed,
and energy expended and to translate this information into mechanical prototypes.5J.B. Gibbs, ÒMechanism-Based Target Identification and Drug Discovery in Cancer Research,Ó Science 287:1969, 2000.6C. Sander, ÒGenomic Medicine and the Future of Health Care,Ó Science 287:1977, 2000.7D. Noble, ÒModeling the HeartÑFrom Genes to Cells to the Whole Organ,Ó Science 295:1678, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION15Box 1.1Illustrative Research Areas at the Interface of Computer Science and Biology¥Structure determination of biological molecules and complexes¥Simulation of protein folding¥Whole genome sequence assembly¥Whole genome modeling and annotation¥Full genome-genome comparison¥Rapid assessment of polymorphic genetic variations¥Complete construction of orthologous and paralogous groups of genes¥Relating gene sequence to protein structure¥Relating protein structure to function¥In silico drug design¥Mechanistic enzymology¥Cell network analysis-simulation of genetic networks and the sensitivity of these pathways to componentstoichiometry and kinetics¥Dynamic simulation of realistic oligomeric systems¥Modeling of cellular processes¥Modeling of physiological systems in health and disease¥Modeling behavior of schools, swarms, and their emergent behavior¥Simulation of membrane structure and dynamic function¥Integration of observations across scales of vastly different dimension and organization for modelcreation purposes¥Development of bio-inspired autonomous locomotive devices¥Development of biomimetic devices¥Bioengineering prostheticsWe can further imagine an extension of present-day bioengineering from mechanical hearts andtitanium hip joints to an entirely new level of devices, such as an implantable neural prosthetic that
could assist stroke patients in restoring speech or motor control or could enhance an individualÕs
capability to see more clearly in the dark or process complex information quickly under pressure. Such
a prosthetic would marry the speed of computing with the brainÕs capacity for intelligence and would
be a powerful tool with many applications.With the advancement of computational power and other capabilities, there is a great opportunityand challenge in whether human functions can be represented in digital computational forms. One form
of representation of a human being is how it is constructed, starting with genes and proteins. Another
form of representation is how a human being functions. Human functions can be viewed at many
different levelsÑphysioanatomical, motion-mechanical, and psychocognitive, for example. If it were
possible to represent a human being at any or all of these functional levels, then a Òdigital humanÓ could
be created inside the computer, to be used for many applications such as medical surgical training,
human-centered design of products, and societal simulation. (There are already such simulations at
varying levels of fidelity for particular organs such as the heart.)The potential breadth and depth of the interface of computing and biology are vast. Box 1.1 is arepresentative list of research areas already being pursued at the interface; Appendix B at the end of this
report provides references to more detailed discussions of these efforts. The excitement and challenge of
all of these possibilities drive the increasing interest in and enthusiasm for research at the interface.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.16CATALYZING INQUIRY1.4SOME RELEVANT HISTORY IN BUILDING THE INTERFACE
1.4.1The Human Genome Project
According to Cook-Deegan,8 the Human Genome Project resulted from the collective impact ofthree independent public airings of the idea that the human genome should be sequenced. In 1985,
Robert Sinsheimer and others convened a group of scientists to discuss the idea.9 In 1986, RenatoDulbecco noted that sequencing the genome would be an important tool in probing the genetic origins
of cancer.10 Then in 1988, Charles DeLisi developed the idea of sequencing the genome in the context ofunderstanding the biological and genetic effects of ionizing radiation on survivors of the Hiroshima and
Nagasaki atomic bombs.11In 1990, the International Human Genome Consortium was launched with the intent to map andsequence the totality of human DNA (the genome).12 On April 14, 2003, not quite 50 years to the dayafter James Watson and Francis Crick first published the structure of the DNA double helix,13 officialsannounced that the Human Genome Project was finished.14 After 13 years and $2.7 billion, the interna-tional effort had yielded a virtually complete listing of the human genetic code: a sequence some 3
billion base pairs long.151.4.2The Computing-to-Biology Interface
For most of the electronic computing age, biological computing applications have been secondarycompared to those associated with the physical sciences and the military. However, over the last two
decades, use by the biological sciencesÑin the form of applications related to protein modeling and
foldingÑwent from virtually nonexistent to being the largest user of cycles at the National Science
Foundation Centers for High Performance Computing by FY 1998. Nor has biological use of computing
capability been limited to supercomputing applicationsÑa plethora of biological computing applica-
tions have emerged that run on smaller machines.During the last two decades, federal agencies also held a number of workshops on computationalbiology and bioinformatics, but until relatively recently, there was no prospect for significant support8Cook-DeeganÕs perspective on the history of the Human Genome Project can be found in R.M. Cook-Deegan, The Gene Wars:Science, Politics, and the Human Genome, W.W. Norton and Company, New York, 1995.9R. Sinsheimer, ÒThe Santa Cruz Workshop,Ó Genomics 5(4):954-956, 1989.10R. Dulbecco, ÒA Turning Point in Cancer Research: Sequencing the Human Genome,Ó Science 231(4742):1055-1056, 1986.11C. DeLisi, ÒThe Human Genome Project,Ó American Scientist 76:488-493, 1988.12Cook-Deegan identifies three independent public airings of the idea that the human genome should be sequenced, airingsthat collectively led to the establishment of the HGP. In 1985, Robert Sinsheimer and others convened a group of scientists todiscuss the idea. (See R. Sinsheimer, ÒThe Santa Cruz Workshop,Ó Genomics 5(4):954-956, 1989.) In 1986, Renato Dulbecco notedthat sequencing the genome would be an important tool in probing the genetic origins of cancer. (See R. Dulbecco, ÒA Turning
Point in Cancer Research: Sequencing the Human Genome,Ó Science 231(4742):1055-1056, 1986.) In 1988, Charles DeLisi devel-oped the idea of sequencing the genome in the context of understanding the biological and genetic effects of ionizing radiationon survivors of the Hiroshima and Nagasaki atomic bombs. (See C. DeLisi, ÒThe Human Genome Project,Ó American Scientist76:488-493, 1988.) Cook-DeeganÕs perspective on the history of the Human Genome Project can be found in R. Cook-Deegan, TheGene Wars: Science, Politics, and the Human Genome, W.W. Norton and Company, New York, 1995.13J.D. Watson and F.H. Crick, ÒMolecular Structure of Nucleic Acids: A Structure for Deoxyribose Nucleic Acid,Ó Nature171(4356):737-738, 1953.14The ÒcompletionÓ of the project had actually been announced once before, on June 26, 2000, when U.S. President Bill Clintonand British Prime Minister Tony Blair jointly hailed the release of a preliminary, draft version of the sequence with loud mediafanfare. However, while that draft sequence was undoubtedly useful, it contained multiple gaps and had an error rate of onemistaken base pair in every 10,000. The much-revised sequence released in 2003 has an error rate of only 1 in 100,000, and gaps inonly those very rare segments of the genome that cannot reliably be sequenced with current technology. See http://
www.genome.gov/11006929.15Various histories of the Human Genome Project can be found at http://www.ornl.gov/sci/techresources/Human_Genome/project/hgp.shtml.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION17for academic work at the interface. The Keck Foundation and the Sloan Foundation supported training,and numerous database activities have been supported by federal agencies. As the impact of the Human
Genome Project and comparative genomics began to reach the community as a whole, the situation
changed. An important step came from the Howard Hughes Medical Institute, which in 1999 held a
special competition to select professors in bioinformatics and thus provided a strong endorsement of
the role of computing in biology.In 1999, the National Institutes of Health (NIH) also took a first step toward integrating ad hocsupport by requesting an analysis of the opportunities, requirements, and challenges from computing
for biomedicine. In June 1999, the Botstein-Smarr Working Group on Biomedical Computing presented
a report to the NIH entitled The Biomedical Information Science and Technology Initiative.16 Specificallytasked with investigating the needs of NIH-supported investigators for computing resources, including
hardware, software, networking, algorithms, and training, the working group made recommendations
for NIH actions to support the needs of NIH-funded investigators for biomedical computing.That report embraces a vision of computing as the hallmark of tomorrowÕs biomedicine. To acceler-ate the transition to this new world of biomedicine, the working group sought to find ways Òto discover,
encourage, train, and support the new kinds of scientists needed for tomorrowÕs science.Ó Much of the
report focuses on national programs to create Òthe best opportunities that can be created for doing and
learning at the interfaces among biology, mathematics, and computation,Ó and argues that Òwith such
new and innovative programs in place, scientists [would] absorb biomedical computing in due course,
while supporting the mission of the NIH.Ó The report also identifies a variety of barriers to the full
exploitation of computation for biological needs.In the intervening 4 years, the validity of the Botstein-Smarr Working Group report vision has notbeen in question; if anything, the expectations, opportunities, and requirements have grown. Computa-
tion in various forms is rapidly penetrating all aspects of life sciences research and practice.¥State-of-the-art radiology (and along with it other fields dependent on imagingÑneurology, forexample) is highly dependent on information technology: the images are filtered, processed reconstruc-
tions that are acquired, stored, and analyzed computationally.¥Genomics and proteomics are completely dependent on computation.¥Integrative biology aimed at predictive modeling is not just computationally enabledÑit literallycannot occur in a noncomputational environment.Biomedical scientists of all stripes are increasingly using public resources and computational toolsat high levels of intensity such that very significant fractions of the overall effort are in this domain, and
it is highly likely that these trends will continue. Yet many of the barriers to full exploitation of compu-
tation in the biological sciences that were identified in the Botstein-Smarr report still remain. One
primary focus of the present report is accordingly to consider the intellectual, organizational, and
cultural barriers that impede or even prevent the full benefits of computation from being realized for
biomedical research.1.4.3The Biology-to-Computing Interface
The application of biological ideas to the design of computing systems appears through much of thehistory of electronic computers, in most cases as an outgrowth of attempts to model or simulate a
biological system. In the early 1970s, John H. Holland (the first person in the United States to be
awarded a Ph.D. in computer science) pioneered the idea of genetic algorithms, which use simulatedgenetic processes (crossover, mutation, and inversion) to search a large solution space of algorithms.1716Available at http://www.nih.gov/about/director/060399.htm.17J.H. Holland, Adaptation in Natural and Artificial Systems, University of Michigan Press, Ann Arbor, 1975.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.18CATALYZING INQUIRYThis work grew out of research in the 1950s and 1960s to simulate just such processes in the naturalworld. A second wave of popularity of this technique came after John Koza described genetic program-
ming, which used similar techniques to modify symbolic expressions that comprised entire programs.18Both of these approaches are in use today, especially in research and academic settings.The history of artificial neural networks also shows a strong relationship between attempts tosimulate biology and attempts to construct a new software tool. This research predates even the modern
electronic digital computers, since Warren McCulloch and Walter Pitts published a model of a neuron
that incorporated analog weights into a binary logic scheme in 1943.19 This was meant to be used as amodel of biological neurons, not merely as an abstract computational processing approach. Research on
neural nets continued throughout the next decades, focusing on network architectures (particularly
random and layered), mechanisms of self-assembly, and pattern recognition and classification. Signifi-
cant among this research was RosenblattÕs work on perceptrons.20 However, lack of progress caused aloss of interest in neural networks in the late 1970s and early 1980s. Hopfield revived interest in the field
in 1982,21 and progress throughout the 1980s and 1990s established neural networks as a standard toolfor learning and classifying patterns.A similar pattern characterizes research into cellular automata. John von NeumannÕs attempts toprovide a theory of biological self-assembly inspired him to apply traditional automata theory to a two-
dimensional grid;22 similar work was being done at the same time by Stanislaw Ulam (who may havesuggested the approach to von Neumann). Von Neumann also showed that cellular automata could
simulate a Turing machine, meaning that they were a system that could provide universal computation.
A boom of popularity for cellular automata followed the publication of the details of John ConwayÕs
Game of Life.23 In the early 1980s, Stephen Wolfram made important contributions to formalizingcellular automata, especially in their role in computational theory,24 and Toffoli and Margolus stressedthe general applicability of automata as systems for modeling.25At a more metaphorical level, IBM has taken initiatives in biologically inspired computing. Specifi-cally, IBM launched its Autonomic Computing initiative in 2001. Autonomic computing is inspired by
biology in the sense that biological systemsÑand in particular the autonomic nervous systemÑare
capable of doing many things that would be desirable in complex computing systems. Autonomic
computing is conceived as a way to manage increasingly complex and distributed computing environ-
ments as traditional approaches to system management reach their limits. IBM takes special note of the
fact that Òthe autonomic nervous system frees our conscious brain from the burden of having to deal
with vital but lower-level functions.Ó26 Autonomic computing, by IBMÕs definition, requires that asystem be able to configure and reconfigure itself under varying and unpredictable conditions, to
continually optimize its workings, to recover from routine and extraordinary events that might cause18J.R. Koza, ÒGenetically Breeding Populations of Computer Programs to Solve Problems in Artificial Intelligence,Ó pp. 819-827in Proceedings of the Second International Conference on Tools for Artificial Intelligence, IEEE Computer Society Press, Los Alamitos,CA, 1990.19W.S. McCulloch and W.H. Pitts, ÒA Logical Calculus of the Ideas Immanent in Nervous Activity,Ó 
Bulletin of MathematicalBiophysics 5:115-137, 1943.20R. Rosenblatt, Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms
, Spartan Books, Washington, DC,1962.21J.J. Hopfield, ÒNeural Networks and Physical Systems with Emergent Collective Computational Abilities,Ó 
Proceedings of theNational Academy of Sciences
 (USA) 79(8):2554-2558, 1982.22J. von Neumann, Theory of Self-reproducing Automata (edited and completed by A. W. Burks), University of Illinois Press, 1966.23M. Gardner, ÒMATHEMATICAL GAMES: The Fantastic Combinations of John ConwayÕs New Solitaire Game ÔLifeÕ,Ó Scien-tific American 223(October):120-123, 1970.24S. Wolfram, ÒComputation Theory of Cellular Automata,Ó Communications in Mathematical Physics 96:15-57, 1984.25T. Toffoli and N. Margolus, Cellular Automata Machines: A New Environment for Modeling, MIT Press, Cambridge, MA, 1987.26G. Ganek and T.A. Corbi, ÒThe Dawning of the Autonomic Computing Era,Ó IBM Systems Journal 42(1):5-18, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION19some parts to malfunction in a manner analogous to the healing of a biological system, and to protectitself against dangers in its (open) environment.1.5BACKGROUND, ORGANIZATION, AND APPROACH OF THIS REPORT
To better understand potential synergies at the BioComp interface and to facilitate the developmentof collaborations between scientific communities in both fields that can better exploit these synergies,
the National Research Council established the Committee on Frontiers at the Interface of Computing
and Biology. The committee hopes that this report will be valuable and important to a variety of
interested parties and constituencies and that scientists who read it will be attracted by the excitement
of research at the interface. To researchers in computer science, the committee hopes to demonstrate
that biology represents an enormously rich problem domain in which their skills and talents can be of
enormous value in ways that go far beyond their value as technical consultants and also that they may
in turn be able to derive inspiration for solving computing problems from biological phenomena and
insights. To researchers in the biological sciences, the committee hopes to show that computing and
information technology have enormous value in changing the traditional intellectual paradigms of
biology and allowing interesting new questions to be posed and answered. To academic administrators,
the committee hopes to provide guidance and principles that facilitate the conduct of research and
education at the BioComp interface. Finally, to funding agencies and organizations, the committee
hopes to provide both a rationale for broadening the kinds of work they support at the BioComp
interface and practices that can enhance and create links between computing and biology.A note on terminology and scope is required for this report. Within the technology domain are anumber of interconnecting aspects implied by terms such as computing, computation, modeling, com-
puter science, computer engineering, informatics, information technology, scientific computing, and
computational science. Today, there is no one term that defines the breadth of the science and technol-
ogy within the computing and information sciences and technologies. The intent is to use any of these
terms with a broad rather than narrow construction and connotation and to consider the entire domain
of inquiry in terms of an interface to life science. For simplicity, this report uses the term ÒcomputingÓ
to refer to intellectual domains characterized by roots in the union of the terms above.Although the words ÒcomputingÓ and ÒcomputationÓ are used throughout this report, biology inthe new millennium connects with a number of facets of the exact sciences in a way that cannot be
separated from computer science per se. In particular, biology has a synergistic relationship with math-
ematics, statistics, physics, chemistry, engineering, and theoretical methodsÑincluding modeling and
analysis as well as computation and simulation. In this relationship, blind computation is no surrogate
for insight and understanding. In many cases, the fruits of computation are reaped only after careful
and deliberate theoretical analysis, in which the physics, biology, and mathematics underlying a given
system are carefully considered. Although much of the focus of this report is on the exchange between
biology and computing, the reader should consider how the same ideas may be extended to encompass
these other aspects.Consider, for example, the fact that mathematics plays an essential role in the interpretation ofexperimental data and in developing algorithms for machine-assisted computing. Computing is implic-
itly mathematical, and as techniques for mathematical analysis evolve and develop, so will new oppor-
tunities for computing.These points suggest that any specific limits on the range of coverage of this report are artificial andsomewhat forced. Yet practicality dictates that some limits be set, and thus the committee leaves sys-
tematic coverage of certain important dimensions of the biology-computing interface to other reports.
For example, a 2005 report of the Board on Mathematical Sciences (BMS) of the National Research
Council (NRC) recommends a mathematical sciences research program that allows biological scientists
to make the most effective use of the large amount of existing genomic information and the much larger
and more diverse collections of structural and functional genomic information that are being created,Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.20CATALYZING INQUIRYcovering both current research needs and some higher-risk research that might lead to innovativeapproaches for the future.27 The BMS study takes a very broad look at what will be required forbioinformatics, biophysics, pattern matching, and almost anything related to the mathematical founda-
tions of computational biology; thus, it is that BMS report, rather than the present report, that addresses
analytical techniques.Similar comments apply to the present reportÕs coverage of medical devices based on embeddedinformation technologies and medical informatics. Medical devices such as implanted defibrillators rely
on real-time analysis of biological data to decide when to deliver a potentially lifesaving shock. Medical
informatics can be regarded as computer science applied directly to problems of medicine and health
care, focusing on the management of medical information, data, and knowledge for medical problem
solving and decision making. Medical devices and medical informatics have many links and similarities
to the subject matter of this report, but they, too, are largely outside its scope, although from time to
time issues and challenges from the medical area are mentioned. Comprehensive studies describing
future needs in medical informatics and medical devices must await future NRC work.Yet another area of concern unaddressed in this report is the area of ethics associated with the issuesdiscussed here. To ask just a few questions: Who will own DNA data? What individual biomedical data
will be collected and retained? What are the ethics involved in using this data? What should individuals
be told about their genetic futures? What are the ethical implications of creating new biological organ-
isms or of changing the genetics of already living individuals? All of these questions are important, and
philosophers and ethicists have begun to address some of them, but they are outside the scope of this
report or the expertise of the committee.In developing this report, the committee chose to characterize the overarching opportunities at theinterface of biology and the computer and information sciences, and to highlight several diverse ex-
amples of activities at the interface. These points of intersection broadly represent and illustrate charac-
teristics of research along the interface and include promising areas of exploration, some exciting from
a basic science perspective and others from the point of view of novel applications.Chapter 2 presents perspectives on 21st century biology, a synthesis among a variety of differentintellectual approaches to biological research. Chapter 3 is a discussion of the nature of biological data
and the requirements that biologists put on data.Chapter 4 discusses computational tools for biology that help to solve specific and precisely definedproblems. Chapter 5 focuses on models and simulations in biology as approaches for exploring and
predicting biological phenomena.Chapter 6 describes the value of a computational and engineering perspective in characterizingbiological functionality of interest. Chapter 7 addresses roles in biological research for cyberinfrastruc-
ture and technologies for data acquisition.Chapter 8 describes the potential of computer science applications and processes to utilize biologi-cal systemsÑto emulate, mimic, or otherwise draw inspiration from the organization, behavior, and
structure of living things or to make use of the physical substrate of biological material in hybrid
systems or other information-processing applications.Chapter 9 presents a number of illustrative problem domains. These are technical challenges, poten-tial future applications, and specific research questions that exemplify points along the interface of
computing and biology. They illustrate the two overarching themes described in Chapter 2, and de-
scribe in detail the specific technological goals that must be met in order to successfully meet the
challenge.Chapter 10 is a discussion of the research infrastructureÑpeople and resources need to vitalize theinterface. The chapter examines the requisite scientific expertise, the false starts of the past, cultural and
other barriers that must be addressed, and the coordinated effort needed to move research at the
interface forward.27National Research Council, Mathematics and 21st Century Biology, The National Academies Press, Washington, DC, 2005.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.INTRODUCTION21Finally, Chapter 11 summarizes key findings about opportunities and barriers to progress at theinterface and provides recommendations for priority areas of research, tools, education, and resources
that will propel progress at the interface.Appendix A is a reprint of a chapter from a 1995 NRC report entitled Calculating the Secrets of Life.The chapter, ÒThe Secrets of Life: A MathematicianÕs Introduction to Molecular Biology,Ó is essentially
a short primer on the fundamentals of molecular biology for nonbiologists. Appendix B lists some of the
research challenges in computational biology discussed in other reports. Short biographies of commit-
tee members, staff, and the review coordinator are given in Appendix C.Throughout this report, examples of relevant work are provided quite liberally where they arerelevant to the topic at hand. The reader should note that these examples have generally been selected
to illustrate the breadth of the topic in question, rather than to identify the most important areas of
activity. That is, the appropriate spirit in which to view these examples is Òletting a thousand flowers
bloom,Ó rather than one of Òfinding the prettiest flowers.ÓCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.21ST CENTURY BIOLOGY2323221st Century BiologyBiology, like any science, changes when technology introduces new tools that extend the scope andtype of inquiry. Some changes, such as the use of the microscope, are embraced quickly and easily,
because they are consonant with existing values and practices. Others, such as the introduction of multi-
variate statistics as performed by computers in the 1960s, are resisted, because they go against traditions of
intuition, visualization, and conceptions of biology that separate it clearly from mathematics.This chapter attempts to frame the challenges and opportunities created by the introduction ofcomputation to the biological sciences. It does so by first briefly describing the existing threads of
biological culture and practice, and then by showing how different aspects of computational science
and technology can support, extend, or challenge the existing framework of biology.Computing is only one of a large number of fields playing a role in the transformation of biology,from advanced chemistry to new fields of mathematics. And yet, in many ways, computers have proven
the most challenging and the most transformative, rooted as they are in a tradition of design and
abstraction so different from biology. Just as computers continue to radically change society at large,
however, there is no doubt that they will change biology as well. As it has done so many times before,
biology will change with this new technology, adopting new techniques, redefining what makes good
science and good training, and changing which inquiries are important, valued, or even possible.2.1WHAT KIND OF SCIENCE?
2.1.1The Roots of Biological Culture
Biology is a science with a deep history that can be linked to the invention of agriculture at the verydawn of civilization and, even earlier, to the first glimmerings of oral culture: ÒIs that safe to eat?Ó As
such, it is a broad field, rich with culture and tradition, that encompasses many threads of observa-
tional, empirical, and theoretical research and spans scales from single molecules to continents. Such a
broad field is impossible to describe simply; nevertheless, this section attempts to identify a number of
the main threads of the activity and philosophy of biology.First, biology is an empirical and a descriptive science. It is rooted in a tradition of qualitative obser-vation and description dating back at least to Aristotle. Biological researchers have long sought toCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.24CATALYZING INQUIRYcatalog the characteristics, behaviors, and variations of individual biological organisms or populationsthrough the direct observation of organisms in their environments, rather than trying to identify general
principles through mathematical or abstract modeling. For this reason, the culture of biology is both
strongly visual and specific. Identifying a new species, and adequately describing its physical appear-
ance, environment, and life cycle, remains a highly considered contribution to biological knowledge.It is revealing to contrast this philosophy with that of modern physics, where the menagerie of newsubatomic particles discovered in the 1960s and 1970s was a source of faint embarrassment and discom-
fort for physicists. Only with the introduction of quarks, and the subsequent reduction in the number of
fundamental particles, did physicists again feel comfortable with the state of their field. Biology, in
strong contrast, not only prizes and embraces the enormous diversity of life, but also considers such
diversity a prime focus of study.Second, biology is an ontological science, concerned with taxonomy and classification. From the timeof Linnaeus, biologists have attempted to place their observations into a larger framework of knowl-
edge, relating individual species to the identified span of life. The methodology and basis for this
catalog is itself a matter of study and controversy, and so research activity of this type occurs at two
levels: specific species are placed into the tree of life (or larger taxa are relocated), still a publishable
event, and the science of taxonomy itself is refined.Biology is a historical science. Life on Earth apparently arose just once, and all life today is derivedfrom that single instance. A complete history of life on EarthÑwhich lineage arose from which, and
whenÑis one of the great, albeit possibly unachievable, goals of biology. Coupled to this inquiry, but
separate, are the questions, How? and Why? What are the forces that cause species to evolve in certain
ways? Are there secular trends in evolution, for example, as is often claimed, toward increasing com-
plexity? Does evolution proceed smoothly or in bursts? If we were to Òreplay the tapeÓ of evolution,
would similar forms arise? Just as with taxonomy (and closely related to it), there are two levels here:
what precisely happened and what the forces are that cause things to happen.These three strandsÑempirical observations of a multitude of life forms, the historical facts ofevolution, and the ordering of biological knowledge into an overarching taxonomy of lifeÑserved to
define the central practices of biology until the 1950s and still in many ways affect the attitudes, training,
philosophy, and values of the biological sciences. Although biology has expanded considerably with
the advent of molecular biology, these three strands continue as vital areas of biological research and
interest.These three intellectual strands have been reflected in biological research that has been qualitativeand descriptive throughout much of its early history. For example, empirical and ontological research-
ers have sought to catalog the characteristics, behaviors, and variations of individual biological organ-
isms or populations through the direct observation of organisms in their environments.Yet as important and valuable as these approaches have been for biology, they have not providedÑand cannot provideÑvery much detail about underlying mechanisms. However, in the last half-cen-
tury, an intellectual perspective provided by molecular biology and biochemistry has served as the
basis for enormous leaps forward.2.1.2Molecular Biology and the Biochemical Basis of Life
In the past 50 years, biochemical approaches to analyzing biological questions and the overallapproaches now known as molecular biology have led to the increased awareness, identification, and
knowledge of the central role of certain mechanisms, such as the digital code of DNA as the mechanism
underlying heredity, the use of adenosine triphospate (ATP) for energy storage, common protein sig-
naling protocols, and many conserved genetic sequences, some shared by species as distinct as humans,
sponges, and even single-cell organisms such as yeast.This new knowledge both shaped and was shaped by changes in the practice of biology. Twoimportant threads of biological inquiry, both existing long before the advent of molecular biology, cameCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.21ST CENTURY BIOLOGY25to the forefront in the second half of the 20th century. These threads were biological experimentationand the search for the underlying mechanics of life.Biological experimentation and the collection of data are not new, but they acquired a new impor-tance and centrality in the late 20th century. The identification of genes and mutations exemplified by
experiments on Drosophila became an icon of modern biological science, and with this a new focusemerged on collecting larger amounts of quantitative data.Biologists have always been interested in how organisms live, a question that ultimately comesdown to the very definition of life. A great deal of knowledge regarding anatomy, circulation, respira-
tion, and metabolism was gathered in the 18th and 19th centuries, but without access to the instruments
and knowledge of biochemistry and molecular biology, there was a limit to what could be discovered.
With molecular biology, some of the underlying mechanisms of life have been identified and analyzed
quantitatively.The effort to uncover the basic chemical features of biological processes and to ascertain all aspectsof the components by way of experimental design will continue to be a major aspect of basic biological
research, and much of modern biology has sought to reduce biological phenomena to the behavior of
molecules.However, biological researchers are also increasingly interested in a systems-level view in whichcompletely novel relationships among system components and processes can be ascertained. That is, a
detailed understanding of the components of a biological organism or phenomenon inevitably leads to
the question of how these components interact with each other and with the environment in which the
organism or phenomenon is embedded.2.1.3Biological Components and Processes in Context, and Biological Complexity
There is a long tradition of studying certain biological systems in context. For example, ecology hasalways focused on ecosystems. Physiology is another example of a life science that has generally consid-
ered biological systems as whole entities. Animal behavior and systematics science also considers
biological phenomena in context. However, data acquisition technologies, computational tools, and
even new intellectual paradigms are available today that enable a significantly greater degree of in-
context understanding of many more biological components and processes than was previously pos-
sible, and the goal today is to span the space of biological entities from genes and proteins to networks
and pathways, from organelles to cells, and from individual organisms to populations and ecosystems.Following Kitano,1 a systems understanding of a biological entity is based on insights regardingfour dimensions: (1) system structures (e.g., networks of gene interactions and biochemical pathways
and their relationship to the physical properties of intracellular and multicellular structures), (2) system
dynamics (e.g., how a system behaves over time under various conditions and the mechanisms under-
lying specific behaviors), (3) control mechanisms (e.g., mechanisms that systematically control the state
of the cell), and (4) design principles (e.g., principles underlying the construction and evolution of
biological systems that have certain desirable properties).2As an example, consider advances in genomic sequencing. Sequence genomics has created a pathfor establishing the Òparts listÓ for living cells, but to move from isolated molecular details to a compre-
hensive understanding of phenomena from cell growth up to the level of homeostasis is widely recog-1H. Kitano, ÒSystems Biology: A Brief Overview,Ó Science 295(5560):1662-1664, 2002.2For example, such principles might occur as the result of convergent evolution, that is, the evolution of species with differentorigins toward similar forms or characteristics, and an understanding of the likely ways that evolution can take to solve certainproblems. Alternatively, principles might be identified that can explain the functional behavior of some specific biologicalsystem under a wide set of circumstances without necessarily being an accurate reflection of what is going on inside the system.Such principles may prove useful from the standpoint of being able to manipulate the behavior of a larger system in which thesmaller system is embedded, though they may not be useful in providing a genuine understanding of the system with whichthey are associated.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.26CATALYZING INQUIRYnized as requiring a very different approach. In the highly interactive systems of living organisms, themacromolecular, cellular, and physiological processes, themselves at different levels of organizational
complexity, have both temporal and spatial components. Interactions occur between sets of similar
objects, such as two genes, and between dissimilar objects, such as genes and their environment.A key aspect of biological complexity is the role of chance. One of the most salient instances ofchance in biology is evolution, in which chance events affect the fidelity of genetic transmission from
one generation to the next. The hand of chance is also seen in the development of an organismÑchance
events affect many of the details of development, though generally not the broad picture or trends. But
perhaps the most striking manifestation is that individual biological organismsÑeven as closely related
as sibling cellsÑare unlikely to be identical because of stochastic events from environmental input to
thermal noise that affect molecular-level processes. If so, no two cells will have identical macromolecu-
lar content, and the dynamic structure and function of the macromolecules in one cell will never be the
same as even a sibling cell. This fact is one of the largest distinctions between living systems and most
silicon devices or almost any other manufactured or human-engineered artifact.Put differently, the digital Òcode of lifeÓ embedded in DNA is far from simple. For example, thebiological Òparts listÓ that the genomic sequence makes available in principle may be unavailable in
practice if all of the parts cannot be identified from the sequence. Segments of the genome once assumed
to be evolutionary ÒjunkÓ are increasingly recognized as the source of novel types of RNA molecules that
are turning out to be major actors in cellular behavior. Furthermore, even a complete parts list provides a
lot less insight into a biological system than into an engineered artifact, because human conventions for
assembly are generally well understood, whereas natureÕs conventions for assembly are not.A second example of the complexity is that a single gene can sometimes produce many proteins. Ineukaryotes, for example, mRNA cannot be used as a blueprint until special enzymes first cut out the
introns, or noncoding regions, and splice together the exons, the fragments that contain useful code.3 Insome cases, however, the cell can splice the exons in different ways, producing a series of proteins with
various pieces added or subtracted but with the same linear ordering (these are known as splice vari-
ants). A process known as RNA editing can alter the sequence of nucleotides in the RNA after transcrip-
tion from DNA but before translation into a protein, resulting in different proteins. An individual
nucleotide can be changed into a different one (Òsubstitution editingÓ), or nucleotides can be inserted or
deleted from the RNA (Òinsertion-deletion editingÓ). In some cases (however rare), the cellÕs translation
machinery might introduce an even more radical change by shifting its Òreading frame,Ó meaning that
it starts to read the three-base-pair genetic code at a point displaced by one or two base pairs from the
original. The result will be a very different sequence of amino acids and, thus, a very different protein.Furthermore, even after the proteins are manufactured at the ribosome, they undergo quite a lot ofpostprocessing as they enter the various regulatory networks. Some might have their shapes and activity
levels altered by the attachment, for example, of a phosphate group, a sugar molecule, or any of a variety
of other appendages, while others might come together to form a multiprotein structure. In short, know-
ing the complete sequence of base pairs in a genome is like knowing the complete sequence of 1s and 0sthat make up a computer program: by itself, that information does not necessarily yield insight into what
the program does or how it may be organized into functional units such as subroutines.4A third illustration of biological complexity is that few, if any, biological functions can be assignedto a single gene or a single protein. Indeed, the unique association between the hemoglobin molecule
and the function of oxygen transport in the bloodstream is by far the exception rather than the rule.3Virtually all introns are discarded by the cell, but in a few cases, an intron has been found to codeÑby itselfÑfor anotherprotein.4A meaningful analogy can be drawn to the difference between object code and source code in a computer. Object code,consisting of binary digits, is what runs on the computer. Source code, usually written in a high-level programming language, iscompiled into object code so that a program will run, but source codeÑand therefore program structure and logicÑis much
more comprehensible to human beings. Source code is also much more readily changed.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.21ST CENTURY BIOLOGY27Much more common is the situation in which biological function depends on interactions among manybiological components. A cellÕs metabolism, its response to chemical and biological signals from the
outside, its cycle of growth and cell divisionÑall of these functions and more are generally carried out
and controlled by elaborate webs of interacting molecules.Fran“ois Jacob and Jacque Monod won the 1965 Nobel Prize in medicine for the discovery that DNAcontained regulatory regions that governed the expression of individual genes.5 (They further empha-sized the importance of regulatory feedback and discussed these regulatory processes using the lan-
guage of circuits, a point of relevance in Section 5.4.3.3.) Since then, it has become understood that
proteins and other products of the genome interact with the DNA itself (and with each other) in a
regulatory web.For example, RNA molecules have a wide range of capabilities beyond their roles as messengersfrom DNA to protein. Some RNA molecules can selectively silence or repress gene transcription; others
operate as a combination chemoreceptor-gene transcript (ÒriboswitchÓ) that gives rise to a protein at
one end of the molecule when the opposite end comes in contact with the appropriate chemical target.
Indeed, it may even be that a significant increase in the number of regulatory RNAs on an evolutionary
time scale is largely responsible for the increase in eukaryotic complexity without a large increase in the
number of protein-coding genes. Understanding the role of RNA and other epigenetic phenomena that
result in alternative states of gene expression, molecular function, or organizationÑÒsystems [that] are
far more complex than any problem that molecular biology, genetics or genomics has yet approached,Ó6is critical to realizing genomicsÕ promise.A fourth example of biological complexity is illustrated by the fact that levels of biological complex-ity extend beyond the intricacies of the genome and protein structures through supramolecular com-
plexes and organelles to cellular subsystems and assemblies of these to form often functionally polar-
ized cells that together contribute to tissue form and function and, thereby to an organismÕs properties.
Although the revolution of the last half of the last century in biochemistry and molecular biology has
contributed significantly to our knowledge of the building blocks of life, we have only begun to scratch
the surface of a data-dense and Gordian knot-like puzzle of complex and dynamic molecular interac-
tions that give rise to the complex behaviors of organisms. In short, little is known about how the
complexities of physiological processes are governed by molecular, cellular, and transcellular signaling
systems and networks. Available information is deep only in limited spatial or temporal domains, and
scarce in other key domains, such the middle spatial scales (e.g., 10 †-10 µm), and there are no tools thatmake intelligent links between relatable pieces of scientific knowledge across these scales.Complexity, then, appears to be an essential aspect of biological phenomena. Accordingly, thedevelopment of a coherent intellectual approach to biological complexity is required to understand
systems-level interactionsÑof molecules, genes, cells, organisms, populations, and even ecosystems. In
this intellectual universe, both Ògenome syntaxÓ (the letters, words, and grammar associated with the
DNA code) and Ògenome semanticsÓ (what the DNA code can express and do) are central foci for
investigation. Box 2.1 describes some of the questions that will arise in cell biology.2.2TOWARD A BIOLOGY OF THE 21
ST CENTURYA biology of the 21st century will integrate a number of diverse intellectual themes.7 One integra-tion is that of the reductionist and systems approaches. Where the component-centered reductionist5F. Jacob and J. Monod, ÒGenetic Regulatory Mechanisms in the Synthesis of Proteins,Ó Journal of Molecular Biology 3:318-356,1961.6F.S. Collins et al., ÒA Vision for the Future of Genomic Research,Ó Nature 422:835-847, 2003.7What this report calls 21st century biology has also been called Òbringing the genome to life,Ó an intentional biology, anintegrative biology, synthetic biology, the new biology or even the next new biology, Biology 21, beyond the genome, postgenomicbiology, genome-enabled science, and industrialized biology.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.28CATALYZING INQUIRYBox 2.1Some Questions for Cell Biology in the 21st CenturyIn the Human Genome InstituteÕs recently published agenda for research in the postgenome era, FrancisCollins and his coauthors repeatedly emphasized how little biologists understand about the data already in
hand. Collins et al. argue that biologists are a very long way from knowing everything there is to know abouthow genes are structured and regulated, for example, and they are virtually without a clue as to whatÕs goingon in the other 95 percent of the genome that does not code for genes. This is why the agendaÕs very first grand
challenge was to systematically endow those data with meaningÑthat is, to Òcomprehensively identify thestructural and functional components encoded in the human genome.Ó1The challenge, in a nutshell, is to understand the cellular information processing systemÑall of itÑfrom the

genome on up. Weng et al. suggest that the essential defining feature of a cell, which makes the system as awhole extremely difficult to analyze, is the following:2[The cell] is not a machine (however complex) drawn to a well-defined design, but a machine that can and doesconstantly rebuild itself within a range of variable parameters. For a systematic approach, what is needed is arelatively clear definition of the boundary of this variability. In principle, these boundaries are determined by an as-
yet-unknown combination of intrinsic capability and external inputs. The balance between intrinsic capability andthe response to external signals is likely to be a central issue in understanding gene expression. . . . A large body ofemerging data indicates that early development occurs through signaling interactions that are genetically pro-
grammed, whereas at the later stages, the development of complex traits is dependent on external inputs as well. Aquantitative description of this entire process would be a culmination and synthesis of much of biology.Some of the questions raised by this perspective include the following:¥What is the proteome of any given cell? How do these individual protein molecules organize themselvesinto functional subnetworksÑand how do these subnetworks then organize themselves into higher- andhigher-level networks?3 What are the functional design principles of these systems? And how, precisely, dothe products of the genome react back on the genome to control their own creation?¥To what extent are active elements (such as RNA) present in the noncoding portions of the genome? Whatis the inventory of epigenetic mechanisms (e.g., RNA silencing, DNA methylation, histone hypoacetylation,
chromatin modifications, imprinting) that cells use to control gene expression? These mechanisms play impor-tant roles in controlling an organismÕs development and, in some lower organisms, are defense responsesagainst viruses and transposable elements. However, epigenetic phenomena have also been implicated in
several human diseases, particularly cancer development due to the repression of tumor suppressor genes.What activates these mechanisms?¥How do these dynamically self-organizing networks vary over the course of the cell cycle (even thoughmost cells in an organism are not proliferating and have exited from the cell cycle)? How do they change asthe cell responds to its surroundings? How do they encode and process information?  Also, what accounts for
lifeÕs robustnessÑthe ability of these networks to adapt, maintain themselves, and recover from a wide varietyof environmental insults?1F.S. Collins, E.D. Green, A.E. Guttmacher, and M.S. Guyer, ÒA Vision for the Future of Genomic Research,Ó Nature 422(6934):835-847,2003. To help achieve this grand challenge, the institute has launched the ENCODE project, a public research consortium dedicated tobuilding an annotated encyclopedia of all known functional DNA elements. See http://www.genome.gov/10005107.2G. Weng, U.S. Bhalla, and R. Iyengar, ÒComplexity in Biological Signaling Systems,Ó Science 284(5411):92-96, 1999.3The hierarchy of levels obviously doesnÕt stop at the cell membrane. Although deciphering the various cellular regulatory networks is ahuge challenge in itself, systems biology ultimately has to deal as well with how cells organize themselves into tissues, organs, and the wholeorganism. One group that is trying to lay the groundwork for such an effort is the Physiome Project at the University of Auckland in NewZealand. See http://www.webopedia.com/TERM/W/Web_services.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.21ST CENTURY BIOLOGY29approach is based on identifying the constituent parts of an organism and understanding the behaviorof the organism in terms of the behavior of those parts (in the limit, a complete molecular-level charac-
terization of the biological phenomena in question), systems biology aims to understand the mecha-
nisms of a living organism across all relevant levels of hierarchy.8 These different fociÑa focus oncomponents of biological systems versus a focus on interactions among these componentsÑare comple-
mentary, and both will be essential for intellectual progress in the future.Twenty-first century biology will bring together many distinct strands of biological research: taxo-nomic studies of many species, the enormous progress in molecular genetics, steps towards under-
standing the molecular mechanisms of life, and an emerging systems biology that will consider biologi-
cal entities in relationship to their larger environment. Twenty-first century biology aims to understand
fully the mechanisms of a living cell and the increasingly complex hierarchy of cells in metazoans, up to¥How do cells develop spatial structure? The cytoplasm is far from a uniform mixture of all of the biomol-ecules that exist in a cell; proteins and other macromolecules are often bound to membranes or isolated insidevarious cellular compartments (especially eukaryotes). A full account of the regulatory networks has to takethis compartmentalization into account, along with such spatial factors as diffusion and the transport of vari-
ous species through the cytoplasm and across membranes.¥How do the networks organize and reorganize themselves over the course of embryonic development, aseach cell decides whether its progeny are going to become skin, muscle, brain, or whatever?4 Then, once thecells are through differentiating, how do the networks actually vary from one cell type to the next? Whatconstitutes the difference, and what happens to the networks as cells age or are damaged? How do flaws in thenetworks manifest themselves as maladies such as cancer?
¥How do the networks vary between individuals? How do those variations account for differences in mor-phology and behavior? AlsoÑespecially in humansÑhow do those variations account for individual differ-ences in the response to drugs and other therapies?
¥How do multicellular organisms operate? A full account of multicellular organisms will have to include anaccount of signaling (in all its varieties, including cell-cell; cell-substratum; autocrine, paracrine, and exocrinesignaling), cellular differentiation, cell motility, tissue architecture, and many other ÒcommunityÓ issues.
¥How do the networks vary between species? To put it another way, how have they changed over thecourse of evolution? Since the ÒblueprintÓ genes for proteins and RNA seem to be quite highly conserved fromone species to the next, is it possible that most of evolution is the result of rearrangements in the genetic
regulatory system?54Physiological processes such as metabolism, signal transduction, and the cell cycle take place on a time scale that ranges frommilliseconds to days and are reversible in the sense that an activity flickers on, gene expression is adjusted as needed, and then everythingreturns to some kind of equilibrium. But the commitments that the cell makes during development are effectively irreversible. Becoming aparticular cell line means that the genetic regulatory networks in each successive generation of cells have to go through a cascade ofdecisions that end up turning genes on and off by the thousands. Unless there is some drastic intervention, as in the cloning experiments thatcreated Dolly the Sheep, those genes are locked in place for the life span of the organism. Of course, the developmental program does notproceed in an isolated, Òopen-loopÓ fashion, as a computer scientist might say. Very early in the process, for example, the growing embryolays out its basic body planÑfront versus back, top versus bottom, and so onÑby establishing embryo-wide chemical gradients, so that theconcentration of the appropriate compound tells each cell what to do. Similar tricks are used at every stage thereafter: each cell is alwaysreceiving copious feedback from its neighbors, with chemical signals providing a constant stream of instructions and course corrections.5After all, even very small changes in the timing of events during development, and in the rates at which various tissues grow, can havea profound impact on the final outcome.
8As a philosophical matter, the notion of reductionist explanation has had a long history in the philosophy of science. Life iscomposed of matter, and matter is governed by the laws of physics. So, the ultimate in reductionist explanation would suggest
that life can be explained by the properties of SchrıdingerÕs equation.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.30CATALYZING INQUIRYprocesses operating at the level of the organism and even populations and ecosystems. However, thiskind of understanding is fundamentally dependent on synergies between a systems understanding as
described above and the reductionist tradition.Twenty-first century biology also brings together empirical work in biology with computationalwork. Empirical work is undertaken in laboratory experiments or field observations and has led to both
hypothesis testing and hypothesis generation. Hypothesis testing relies on the data provided by empiri-
cal work to accept or reject a candidate hypothesis. However, data collected in empirical work can also
suggest new hypotheses, leading to work that is exploratory in nature. In 21st century biology, compu-
tational work provides a variety of tools that support empirical work, but also enables much of systems
biology through techniques such as simulation, data mining, and microarray analysisÑand thus under-
lies the generation of plausible candidate hypotheses that will have to be tested. Note also that hypoth-
esis testing is relevant to both reductionist and systems biology, in the sense that both types of biology
are formulated around hypotheses (about components or about relationships between components)
that mayÑor may notÑbe consistent with empirical or experimental results.In this regard, a view expressed by Walter Gilbert in 1991 seems prescient. Gilbert noted that Òin thecurrent paradigm [i.e., that of 1991], the attack on the problems of biology is viewed as being solely
experimental. The ÔcorrectÕ approach is to identify a gene by some direct experimental procedureÑ
determined by some property of its product or otherwise related to its phenotypeÑto clone it, to
sequence it, to make its product and to continue to work experimentally so as to seek an understanding
of its function.Ó He then argued that Òthe new paradigm [for biological research], now emerging [i.e., in
1991], is that all the genes will be known (in the sense of being resident in databases available electroni-
cally), and that the starting point of a biological investigation will be theoretical. An individual scientist
will begin with a theoretical conjecture, only then turning to experiment to follow or test that hypoth-
esis. The actual biology will continue to be done as Ôsmall scienceÕÑdepending on individual insight
and inspiration to produce new knowledge but the reagents that the scientist uses will include a
knowledge of the primary sequence of the organism, together with a list of all previous deductions from
that sequence.Ó9Finally, 21st century biology encompasses what is often called discovery science. Discovery sciencehas been described as Òenumerat[ing] the elements of a system irrespective of any hypotheses on how
the system functionsÓ and is exemplified by genome sequencing projects for various organisms.10 Asecond example of discovery science is the effort to determine the transcriptomes and proteomes of
individual cell types (e.g., quantitative measurements of all of the mRNAs and protein species).11 Suchefforts could be characterized as providing the building blocks or raw materials out of which hypoth-
eses can be formulatedÑmetaphorically, words of a biological ÒlanguageÓ for expressing hypotheses.
Yet even here, the Human Genome Project, while unprecedented in its scope, is comfortably part of a
long tradition of increasingly fine description and cataloging of biological data.All told, 21st century biology will entail a broad spectrum of research, from laboratory work di-rected by individual principal investigators, to projects on the scale of the human genome that generate
large amounts of primary data, to the ÒmesoscienceÓ in between that involves analytical or synthetic
work conducted by multiple collaborating laboratories. For the most part, these newer research strate-
gies involving discovery science and analytical work will complement rather than replace the tradi-
tional, relatively small laboratory focusing on complementary empirical and experimental methods.9W. Gilbert, ÒTowards a Paradigm Shift in Biology,Ó Nature 349(6305):99, 1991.10R. Aebersold, L.E. Hood, and J.D. Watts, ÒEquipping Scientists for the New Biology,Ó Nature Biotechnology 18:359, 2000.11These examples are taken from T. Ideker, T. Galitski, and L. Hood, ÒA New Approach to Decoding Life: Systems Biology,ÓAnnual Review of Genomics and Human Genetics 2:343-372, 2001. The transcriptome is the complete collection of transcribedelements of the genome, including all of the genetic elements that code for proteins, all of the mRNAs, and all noncoding RNAsthat are used for structural and regulatory purposes. The proteome is the complete collection of all proteins involved in a
particular pathway, organelle, cell, tissue, organ, or organism that can be studied in concert to provide accurate and comprehen-sive data about that system.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.21ST CENTURY BIOLOGY31Grand questions, such as those concerning origins of life, the story of evolution, the architecture ofthe brain, and the interactions of living things with each other in populations and ecosystems, are up for
grabs in 21st century biology, and the applications to health, agriculture, and industry are no less
ambitious. For example, 21st century biology may enable the identification of individuals who are likely
to develop cancer, AlzheimerÕs, or other diseases, or who will respond to or have a side effect from a
particular disease treatment. Pharmaceutical companies are making major investments in
transcriptomics to screen for plausible drug targets. Forward-thinking companies want to develop more
nutritious plants and animals, commandeer the machinery of cells to produce materials and drugs, and
build interfaces to the brain to correct impaired capabilities or produce enhanced abilities. Agencies
interested in fighting bioterrorism want to be able to rapidly identify the origins and ancestry of patho-
gen outbreaks, and stewards of natural systems would like to make better predictions about the impacts
of introduced species or global change.2.3ROLES FOR COMPUTING AND INFORMATION TECHNOLOGY IN BIOLOGY
To manage biological data, 21st century biology will integrate discovery science, systems biology,and the empirical tradition of biological science and provide a quantitative framework within which the
results of efforts in each of these areas may be placed. The availability of large amounts of biological
data is expected to enable biological questions to be addressed globally, for example, examining the
behavior of all of the genes in a genome, all of the proteins produced in a cell type, or all of the
metabolites created under particular environmental conditions. However, enabling the answering of
biological questions by uncovering the raw data is not the same as answering those questionsÑthe data
must be analyzed and used in intellectually meaningful and significant ways.2.3.1Biology as an Information Science
The data-intensive nature of 21st century biology underlies the dependence of biology on informa-tion technology (IT). For example, even in 1990 it was recognized that IT would play a central role in the
International Human Genome Consortium for the storage and retrieval of biological gene sequence
dataÑrecording the signals, storing the sequence data, processing images of fluorescent traces specific
to each base, and so on. Also, as biology unfolds in the 21st century, it is clear that the rate of production
of biological data will not abate. Data acquisition opportunities will emerge in most or all life science
subdisciplines and fields, and life scientists will have to cope with the coming deluge of highly multi-
variate, largely nonreducible data, including high-resolution imaging and time series data of complex
dynamic processes.Yet beyond data management issues, important and challenging though they are, it has also becomeclear that computing and information technology will play crucial roles in identifying meaningful
structures and patterns in the genome (e.g., genes, genetic regulatory elements), in understanding the
interconnections between various genomic elements, and in uncovering functional biological informa-
tion about genes, proteins, and their interactions. This focus on informationÑon acquiring, processing,
structuring, and representing informationÑplaces genomic studies squarely in the domain of comput-
ing and information science.Of course, genomic studies are not the whole of modern biology. For life sciences ranging fromecology, botany, zoology, and developmental biology to cellular and molecular biologyÑall of which
can be characterized as science with diverse data types and high degrees of data heterogeneity and
hierarchyÑIT is essential to collect key information and organize biological data in methodical ways in
order to draw meaningful observations. Massive computing power, novel modeling approaches, new
algorithms and mathematical or statistical techniques, and systematic engineering approaches will
provide biologists with vital and essential tools for managing the heterogeneity and volume of the data
and for extracting meaning from those data.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.32CATALYZING INQUIRYUltimately, what calculus is to the language of the physical sciences, computing and informationwill be to the language of 21st century biology, or at least to its systems biology thread.12 The processesof biology, the activities of living organisms, involve the usage, maintenance, dissemination, transfor-
mation or transduction, replication, and transmittal of information across generations. Biological sys-
tems are characterized by individuality, contingency, historicity, and high digital information contentÑ
every living thing is unique. Furthermore, the uniqueness and historical contingency of life means that
for population-scale problems, the potential state space that the population actually inhabits is huge.13As an information science, the life sciences use computing and information technology as a language
and a medium in which to manage the discrete, asymmetric, largely irreducible, unique nature of
biological systems and observations.In the words above, those even marginally familiar with the history of biology will recognize hintsof what was once called theoretical biology or mathematical biology, which in earlier days meant
models and computer simulations based on such then-fashionable ideas as cybernetics and general
systems theory.14 The initial burst of enthusiasm waned fairly quickly, as it became clear that theavailable experimental data were not sufficient to keep the mathematical abstractions tethered to real-
ity. Indeed, reliable models are impossible when many or most of the quantitative values are missing.
Moreover, experience since then has indicated that biological systems are much more complex and
internally interlinked than had been imaginedÑa fact that goes a long way towards explaining why the
models of that era were not very successful in driving productive hypothesis generation and research.The story is radically different today. High-throughput data acquisition technologies (themselvesenabled and made practical by todayÕs information technologies), change a paucity of data into a deluge
of it, as illustrated by the use of these technologies for sequencing of many eukaryotic organisms. This
is not to say that more data are not needed, merely that the acquisition of necessary data now seems to
be possible in reasonable amounts of time.The same is true for the information technologies underpinning 21st century biology. In the past,even if data had been available, the IT then available would have been inadequate to make sense out of
those data. But todayÕs information technologies are vastly more powerful and hold considerable prom-
ise for enabling the kinds of data management and analytical capabilities that are necessary for a
systems-level approach. Moreover, information technology as an underlying medium has the advan-
tage of growing ever more capable over time at exponential rates. As information technology becomes
more capable, biological applications will have an increasingly powerful technology substrate on which
to draw.12Biological Sciences Advisory Committee on Cyberinfrastructure for the Biological Sciences, Building a Cyberinfrastructure forthe Biological Sciences (CIBIO): 2005 and Beyond: A Roadmap for Consolidation and Exponentiation, July 2003. Available from http://research.calit2.net/cibio/archived/CIBIO_FINAL.pdf. This is not to deny that calculus also has application in systems biology(mostly through its relevance to biochemistry and thermodynamics), but calculus is not nearly as central to systems biology as itis to the physical sciences nor as central as computing and information technology are to systems biology.13The number of possible different 3-billion-base-pair genomes, assuming only simple base substitution mutations, is 4 to the3-billionth power. ThatÕs a big number. In fact, it is so big that the ratio of that number (big) to the number of particles in theknown universe (small) is much greater than the ratio of the diameter of the universe to the diameter of a carbon atom. Thus,
exhaustive computer modeling of that state space is effectively precluded. Even more tractable state spaces, such as the numberof different possible human haploid genotypes, still produce gigantic numbers. For example, if we assume that the entire humanpopulation is heterozygous at just 500 locations throughout the genome (a profound underestimate of existing diversity), with
each site having only two states, then the number of possible human haplotypes is 2 to the 500th power, which also exceeds thenumber of electrons in the known universe. These back-of-the-envelope calculations also show that it is impossible for the statespace of existing human genotypes to exist in anything approaching linkage equilibrium.14N. Wiener, Cybernetics, or Control and Communication in the Animal and the Machine, 2nd ed., MIT Press, Cambridge, MA, 1961;L. von Bertalanffy, General Systems Theory: Foundations, Development, Applications, George Braziller, New York, 1968. This historywas recently summarized in O. Wolkenhauer, ÒSystems Biology: The Reincarnation of Systems Theory Applied in Biology?Ó
Briefings in Bioinformatics 2(3):258-270, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.21ST CENTURY BIOLOGY33In short, the introduction of computing into biology has transformed, and continues to transform,the practice of biology. The most straightforward, although often intellectually challenging, way in-
volves computing tools with which to acquire, store, process, and interpret enormous amounts of
biological data. But computing (when used wisely and in combination with the tools of mathematics
and physics) will also provide biologists with an alternative and possibly more appropriate language
and set of abstractions for creating models and data representations of higher-order interactions, de-
scribing biological phenomena, and conceptualizing some characteristics of biological systems.Finally, it should be noted that although computing and information technology will become anincreasingly important part of life science research, researchers in different subfields of biology are
likely to understand the role of computing differently. For example, researchers in molecular biology or
biophysics may focus on the ability of computing to make more accurate quantitative predictions about
enzyme behavior, while researchers in ecology may be more interested in the use of computing to
explore relationships between ecosystem behavior and perturbations in the ambient environment. These
perspectives will become especially apparent in the chapters of this report dealing with the impact of
computing and IT on biology (see Chapter 4 on tools and Chapter 5 on models).This report distinguishes between computational tools, computational models, information abstrac-tions and a computational perspective on biology, and cyberinfrastructure and data acquisition tech-
nologies. Each of these is discussed in Chapters 4 through 7, respectively, preceded by a short chapter
on the nature of biological data (Chapter 3).2.3.2Computational Tools
In the lexicon of this report, computational tools are artifactsÑusually implemented as software,but sometimes as hardwareÑthat enable biologists to solve very specific and precisely defined prob-
lems. For example, an algorithm for gene finding or a database of genomic sequences is a computational
tool. As a rule, these tools reinforce and strengthen biological research activities, such as recording,
managing, analyzing, and presenting highly heterogeneous biological data in enormous quantity. Chap-
ter 4 focuses on computational tools.2.3.3Computational Models
Computational models apply to specific biological phenomena (e.g., organisms, processes) and areused for several purposes. They are used to test insight; to provide a structural framework into which
observations and experimental data can be coherently inserted; to make hypotheses more rigorous,
quantifiable, and testable; to help identify key or missing elements or important relationships; to help
interpret experimental data; to teach or present system behavior; and to predict dynamical behavior of
complex systems. Predictive models provide some confidence that certain aspects of a given biological
system or phenomenon are understood, when their predictions are validated empirically. Chapter 5
focuses on computational models and simulations.2.3.4A Computational Perspective on Biology
Coming to grips with the complexity of biological phenomena demands an array of intellectualtools to help manage complexity and facilitate understanding in the face of such complexity. In recent
years, it has become increasingly clear that many biological phenomena can be understood as perform-
ing information processing in varying degrees; thus, a computational perspective that focuses on infor-
mation abstractions and functional behavior has potentially large benefit for this endeavor. Chapter 6
focuses on viewing biological phenomena through a computational lens.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.34CATALYZING INQUIRY2.3.5Cyberinfrastructure and Data Acquisition
Cyberinfrastructure for science and engineering is a term coined by the National Science Founda-tion to refer to distributed computer, information, and communication technologies and the associated
organizational facilities to support modern scientific and engineering research conducted on a global
scale. Cyberinfrastructure for the life sciences is increasingly an enabling mechanism for a large-scale,
data-intensive biological research effort, inherently distributed over multiple laboratories and investi-
gators around the world, that facilitates the integration of experimental data, enables collaboration, and
promotes communication among the various actors involved.Obtaining primary biological data is a separate question. As noted earlier, 21st century biology isincreasingly a data-intensive enterprise. As such, tools that facilitate acquisition of the requisite data
types in the requisite amounts will become ever more important in the future. Although they are not by
any means the whole story, advances in IT and computing will play key roles in the development of
new data acquisition technologies that can be used in novel ways.Chapter 7 focuses on the roles of cyberinfrastructure and data acquisition for 21st century biology.2.4CHALLENGES TO BIOLOGICAL EPISTEMOLOGY
The forthcoming integration of computing into biological research raises deep epistemologicalquestions about the nature of biology itself. For many thousands of years, a doctrine known as vitalism
held that the stuff of life was qualitatively different from that of nonlife and, consequently, that living
organisms were made of a separate substance than nonliving things or that some separate life force
existed to animate the materials that composed life.While this belief no longer holds sway today (except perhaps in bad science fiction movies), thequestion of how biological phenomena can be understood has not been fully settled. One stance is based
on the notion that the behavior of a given system is explained wholly by the behaviors of the compo-
nents that make up that systemÑa view known as reductionism in the philosophy of science. A con-
trasting stance, known as autonomy in the philosophy of science, holds that in addition to understand-
ing its individual components, understanding of a biological system must also include an understanding
of the specific architecture and arrangement of the systemÕs components and the interactions among
them.If autonomy is accepted as a guiding worldview, introducing the warp of computing into the weftof biology creates additional possibilities for intellectual inquiry. Just as the invention of the microscope
extended biological inquiry into new arenas and enlarged the scope of questions that were reasonable to
ask in the conduct of biological research, so will the computer. Computing and information technology
will enable biological researchers to consider heretofore inaccessible questions, and as the capabilities of
the underlying information technologies increase, such opportunities will continue to open up.New epistemological questions will also arise. For example, as simulation becomes more pervasiveand common in biology, one may ask, Are the results from a simulation equivalent to the data output of
an experiment? Can biological knowledge ever arise from a computer simulation? (A practical example
is the following: As large-scale clinical trials of drugs become more and more expensive, under what
circumstances and to what extent might a simulation based on detailed genomic and pharmacological
knowledge substitute for a large-scale trial in the drug approval process?) As simulations become more
and more sophisticated, pre-loaded with more and more biological data, these questions will become
both more pressing and more difficult to answer definitively.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA35353On the Nature of Biological Data
Twenty-first century biology will be a data-intensive enterprise. Laboratory data will continue tounderpin biologyÕs tradition of being empirical and descriptive. In addition, they will provide confirming
or disconfirming evidence for the various theories and models of biological phenomena that researchers
build. Also, because 21st century biology will be a collective effort, it is critical that data be widely
shareable and interoperable among diverse laboratories and computer systems. This chapter describes the
nature of biological data and the requirements that scientists place on data so that they are useful.3.1DATA HETEROGENEITY
An immense challengeÑone of the most central facing 21st century biologyÑis that of managingthe variety and complexity of data types, the hierarchy of biology, and the inevitable need to acquire
data by a wide variety of modalities. Biological data come in many types. For instance, biological data
may consist of the following:1¥Sequences.Sequence data, such as those associated with the DNA of various species, have grownenormously with the development of automated sequencing technology. In addition to the human
genome, a variety of other genomes have been collected, covering organisms including bacteria, yeast,
chicken, fruit flies, and mice.2 Other projects seek to characterize the genomes of all of the organismsliving in a given ecosystem even without knowing all of them beforehand.3  Sequence data generally1This discussion of data types draws heavily on H.V. Jagadish and F. Olken, eds., Data Management for the Biosciences, Report ofthe NSF/NLM Workshop of Data Management for Molecular and Cell Biology, February 2-3, 2003, Available at http://www.eecs.umich.edu/~jag/wdmbio/wdmb_rpt.pdf. A summary of this report is published as H.V. Jagadish and F. Olken,ÒDatabase Management for Life Science Research,Ó OMICS: A Journal of Integrative Biology 7(1):131-137, 2003.2See http://www.genome.gov/11006946.3See, for example, J.C. Venter, K. Remington, J.F. Heidleberg, A.L. Halpern, D. Rusch, J.A. Eisen, D. Wu, et al., ÒEnvironmentalGenome Shotgun Sequencing of the Sargasso Sea,Ó Science 304(5667):66-74, 2004. VenterÕs team collected microbial populationsen masse from seawater samples originating in the Sargasso Sea near Bermuda. The team subsequently identified 1.045 billion
base pairs of nonredundant sequence, which they estimated to derive from at least 1,800 genomic species based on sequencerelatedness, including 148 previously unknown bacterial phylotypes. They also claimed to have identified more than 1.2 millionpreviously unknown genes represented in these samples.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.36CATALYZING INQUIRYconsist of text strings indicating appropriate bases, but when there are gaps in sequence data, gaplengths (or bounds on gap lengths) must be specified as well.¥Graphs.Biological data indicating relationships can be captured as graphs, as in the cases ofpathway data (e.g., metabolic pathways, signaling pathways, gene regulatory networks), genetic maps,
and structured taxonomies. Even laboratory processes can be represented as workflow process model
graphs and can be used to support formal representation for use in laboratory information management
systems.¥High-dimensional data.Because systems biology is highly dependent on comparing the behaviorof various biological units, data points that might be associated with the behavior of an individual unit
must be collected for thousands or tens of thousands of comparable units. For example, gene expression
experiments can compare expression profiles of tens of thousands of genes, and since researchers are
interested in how expression profiles vary as a function of different experimental conditions (perhaps
hundreds or thousands of such conditions), what was one data point associated with the expression of
one gene under one set of conditions now becomes 106 to 107 data points to be analyzed.¥Geometric information.Because a great deal of biological function depends on relative shape (e.g.,the ÒdockingÓ behavior of molecules at a potential binding site depends on the three-dimensional
configuration of the molecule and the site), molecular structure data are very important. Graphs are one
way of representing three-dimensional structure (e.g., of proteins), but ball-and-stick models of protein
backbones provide a more intuitive representation.¥Scalar and vector fields.Scalar and vector field data are relevant to natural phenomena that varycontinuously in space and time. In biology, scalar and vector field properties are associated with chemi-
cal concentration and electric charge across the volume of a cell, current fluxes across the surface of a
cell or through its volume, and chemical fluxes across cell membranes, as well as data regarding charge,
hydrophobicity, and other chemical properties that can be specified over the surface or within the
volume of a molecule or a complex.¥Patterns.Within the genome are patterns that characterize biologically interesting entities. Forexample, the genome contains patterns associated with genes (i.e., sequences of particular genes) and
with regulatory sequences (that determine the extent of a particular geneÕs expression). Proteins are
characterized by particular genomic sequences. Patterns of sequence data can be represented as regular
expressions, hidden Markov models (HMMs), stochastic context-free grammars (for RNA sequences),
or other types of grammars. Patterns are also interesting in the exploration of protein structure data,
microarray data, pathway data, proteomics data, and metabolomics data.¥Constraints.Consistency within a database is critical if the data are to be trustworthy, and bio-logical databases are no exception. For example, individual chemical reactions in a biological pathway
must locally satisfy the conservation of mass for each element involved. Reaction cycles in thermody-
namic databases must satisfy global energy conservation constraints. Other examples of nonlocal con-
straints include the prohibition of cycles in overlap graphs of DNA sequence reads for linear chromo-
somes or in the directed graphs of conceptual or biological taxonomies.¥Images.Imagery, both natural and artificial, is an important part of biological research. Electronand optical microscopes are used to probe cellular and organ function. Radiographic images are used to
highlight internal structure within organisms. Fluorescence is used to identify the expressions of genes.
Cartoons are often used to simplify and represent complex phenomena. Animations and movies are
used to depict the operation of biological mechanisms over time and to provide insight and intuitive
understanding that far exceeds what is available from textual descriptions or formal mathematical
representations.¥Spatial information.Real biological entities, from cells to ecosystems, are not spatially homoge-neous, and a great deal of interesting science can be found in understanding how one spatial region is
different from another. Thus, spatial relationships must be captured in machine-readable form, and
other biologically significant data must be overlaid on top of these relationships.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA37¥Models.As discussed in Section 5.3.4, computational models must be compared and evaluated.As the number of computational models grows, machine-readable data types that describe computa-
tional modelsÑboth the form and the parameters of the modelÑare necessary to facilitate comparison
among models.¥Prose.The biological literature itself can be regarded as data to be exploited to find relationshipsthat would otherwise go undiscovered. Biological prose is the basis for annotations, which can be
regarded as a form of metadata. Annotations are critical for researchers seeking to assign meaning to
biological data. This issue is discussed further in Chapter 4 (automated literature searching).¥Declarative knowledge such as hypotheses and evidence.As the complexity of various biologicalsystems is unraveled, machine-readable representations of analytic and theoretical results as well as the
underlying inferential chains that lead to various hypotheses will be necessary if relationships are to be
uncovered in this enormous body of knowledge. This point is discussed further in Section 4.2.8.1.In many instances, data on some biological entity are associated with many of these types: forexample, a protein might have associated with it two-dimensional images, three-dimensional struc-
tures, one-dimensional sequences, annotations of these data structures, and so on.Overlaid on these types of data is a temporal dimension. Temporal aspects of data types such asfields, geometric information, high-dimensional data, and even graphsÑimportant for understanding
dynamical behaviorÑmultiply the data that must be managed by a factor equal to the number of time
steps of interest (which may number in the thousands or tens of thousands). Examples of phenomena
with a temporal dimension include cellular response to environmental changes, pathway regulation,
dynamics of gene expression levels, protein structure dynamics, developmental biology, and evolution.
As noted by Jagadish and Olken,4 temporal data can be taken absolutely (i.e., measured on an absolutetime scale, as might be the case in understanding ecosystem response to climate change) or relatively
(i.e., relative to some significant event such as division, organism birth, or environmental insult). Note
also that in complex settings such as disease progression, there may be many important events against
which time is reckoned. Many traditional problems in signal processing involve the extraction of signal
from temporal noise as well, and these problems are often found in investigating biological phenomena.All of these different types of data are needed to integrate diverse witnesses of cellular behavior intoa predictive model of cellular and organism function. Each data source, from high-throughput
microarray studies to mass spectroscopy, has characteristic sources of noise and limited visibility into
cellular function. By combining multiple witnesses, researchers can bring biological mechanisms into
focus, creating models with more coverage that are far more reliable than models created from one
source of data alone. Thus, data of diverse types including mRNA expression, observations of in vivo
protein-DNA binding, protein-protein interactions, abundance and subcellular localization of small
molecules that regulate protein function (e.g., second messengers), posttranslational modifications, and
so on will be required under a wide variety of conditions and in varying genetic backgrounds. In
addition, DNA sequence from diverse species will be essential to identify conserved portions of the
genome that carry meaning.3.2DATA IN HIGH VOLUME
Data of all of the types described above contribute to an integrated understanding of multiple levelsof a biological organism. Furthermore, since it is generally not known in advance how various compo-
nents of an organism are connected or how they function, comprehensive datasets from each of these4H.V. Jagadish and F. Olken, ÒDatabase Management for Life Science Research,Ó OMICS: A Journal of Integrative Biology 7(1):131-137, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.38CATALYZING INQUIRYtypes are required. In cellular analysis, data comprehensiveness includes three aspects, as noted byKitano: 51.Factor comprehensiveness, which reflects the numbers of mRNA transcripts and proteins that canbe measured at once;2.Time-line comprehensiveness, which represents the time frame within which measurements aremade (i.e., the importance of high-level temporal resolution); and3.Item comprehensivenessÑthe simultaneous measurement of multiple items, such as mRNA andprotein concentrations, phosphorylation, localization, and so forth.For every one of the many proteins in a given cell type, information must be collected about proteinidentity, abundance, processing, chemical modifications, interactions, turnover time, and so forth. Spa-
tial localization of proteins is particularly critical. To understand cellular function in detail, proteins
must be localized on a scale finer than that of cell compartments; moreover, localization of specific
protein assemblies to discrete subcellular sites through anchoring and scaffolding proteins is important.All of these considerations suggest that in addition to being highly heterogeneous, biological datamust be voluminous if they are to support comprehensive investigation.3.3DATA ACCURACY AND CONSISTENCY
All laboratories must deal with instrument-dependent or protocol-dependent data inconsistencies.For example, measurements must be calibrated against known standards, but calibration methods and
procedures may change over time, and data obtained under circumstances of heterogeneous calibration
may well not be comparable to each other. Experiments done by multiple independent parties almost
always result in inconsistencies in datasets.6 Different experimental runs with different technicians andprotocols in different labs inevitably produce data that are not entirely consistent with each other, and
such inconsistencies have to be noted and reconciled. Also, the absolute number of data errors that must
be reconciledÑboth within a single dataset and across datasetsÑincreases with the size of the dataset.
For such reasons, statistical data analysis becomes particularly important in analyzing data acquired via
high-throughput techniques.To illustrate these difficulties, consider the replication of microarray experiments. Experience withmicroarrays suggests that such replication can be quite difficult. In principle, a microarray experiment
is simple. The raw output of a microarray experiment is a listing of fluorescent intensities associated
with spots in an array; apart from complicating factors, the brightness of these spots is an indication of
the expression level of the transcript associated with them.On the other hand, the complicating factors are many, and in some cases ignoring these factors canrender oneÕs interpretation of microarray data completely irrelevant. Consider the impact of the following:¥Background effects, which are by definition contributions to spot intensity that do not originatewith the biological material being examined. For example, an empty microarray might result in some5H. Kitano, ÒSystems Biology: A Brief Overview,Ó Science 295(5560):1662-1664, 2002.6As an example, there is only limited agreement between the datasets generated by multiple methods regarding protein-protein interactions in yeast. See, for example, the following set of papers: Y. Ho, A. Gruhler, A. Heilbut, G.D. Bader, L. Moore,S.L. Adams, A. Miller, et al., ÒSystematic Identification of Protein Complexes in Saccharomyces cerevisiae by Mass Spectrometry,ÓNature 415(6868):180-183, 2002; A.C. Gavin, M. Bosche, R. Krause, P. Grandi, M. Marzioch, A. Bauer, J. Schultz, et al., ÒFunctionalOrganization of the Yeast Proteome by Systematic Analysis of Protein Complexes,Ó Nature 415(6868):141-147, 2002; T. Ito, T.Chiba, R. Ozawa, M. Yoshida, M. Hattori, and Y. Sakaki, ÒA Comprehensive Two Hybrid Analysis to Explore the Yeast Protein
Interactome,Ó Proceedings of the National Academy of Sciences 98(8):4569-4574, 2001; P. Uetz, L. Giot, G. Cagney, T.A. Mansfield,R.S. Judson, J.R. Knight, D. Lockshon, et al., ÒA Comprehensive Analysis of Protein-Protein Interactions in Saccharomycescerevisiae,Ó Nature 403(6770):623-627, 2000.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA39background level of fluorescence and even some variation in background level across the entire surfaceof the array.¥Noise dependent on expression levels of the sample. For example, Tu et al. found that hybridizationnoise is strongly dependent on expression level, and in particular the hybridization noise is mostly
Poisson-like for high expression levels but more complex at low expression levels.7¥Differential binding strengths for different probe-target combinations. The brightness of a spot is deter-mined by the amount of target present at a probe site and the strength of the binding between probe and
target. Held et al. found that the strength of binding is affected by the free energy of hybridization,
which is itself a function of the specific sequence involved at the site, and they developed a model to
account for this finding.8¥Lack of correlation between mRNA levels and protein levels. The most mature microarray technologymeasures mRNA levels, while the quantity of interest is often protein level. However, in some cases of
interest, the correlation is small even if overall correlations are moderate.  One reason for small correla-

tions is likely to be the fact that some proteins are regulated after translation, as noted in Ideker et al.9¥Lack of uniformity in the underlying glass surface of a microarray slide. Lee et al. found that the specificlocation of a given probe on the surface affected the expression level recorded.10Other difficulties arise when the results of different microarray experiments must be compared.11¥Variations in sample preparation. A lack of standardized procedure across experiments is likely toresult in different levels of random noiseÑand procedures are rarely standardized very well when they
are performed by humans in different laboratories. Indeed, sample preparation effects may dominate
effects that arise from the biological phenomenon under investigation.12¥Insufficient spatial resolution. Because multiple cells are sampled in any microarray experiment,tissue inhomogeneities may result in more of a certain kind of cell being present, thus throwing off the
final result.¥Cell-cycle starting times. Identical cells are likely to have more-or-less identical clocks, but there isno assurance that all of the clocks of all of the cells in a sample are started at the same time. Because
expression profile varies over time, asynchrony in cell cycles may also throw off the final result.13To deal with these difficulties, the advice offered by Lee et al. and Novak et al., among others, isfairly straightforwardÑrepeat the experiment (assuming that the experiment is appropriately struc-7Y. Tu, G. Stolovitzky, and U. Klein, ÒQuantitative Noise Analysis for Gene Expression Microarray Experiments,Ó Proceedingsof the National Academy of Sciences  99(22):14031-14036, 2002.8G.A. Held, G. Grinstein, and Y. Tu, ÒModeling of DNA Microarray Data by Using Physical Properties of Hybridization,ÓProceedings of the National Academy of Sciences 100(13):7575-7580, 2003.9T. Ideker, V. Thornsson, J.A. Ranish, R. Christmas, J. Buhler, J.K. Eng, R. Bumgarner, et al., ÒIntegrated Genomic and ProteomicAnalyses of a Systematically Perturbed Metabolic Network,Ó Science 292(5518):929-934, 2001. (Cited in Rice and Stolovitzky,ÒMaking the Most of It,Ó 2004, Footnote 11.)10M.L. Lee, F.C. Kuo, G.A. Whitmore, and J. Sklar, ÒImportance of Replication in Microarray Gene Expression Studies: Statisti-cal Methods and Evidence from Repetitive cDNA Hybridizations,Ó Proceedings of the National Academy of Sciences 97(18):9834-9839, 2000.11J.J. Rice and G. Stolovitzky, ÒMaking the Most of It: Pathway Reconstruction and Integrative Simulation Using the Data atHand,Ó Biosilico 2(2):70-77, 2004.12J.P. Novak, R. Sladek, and T.J. Hudson, ÒCharacterization of Variability in Large-scale Gene Expression Data: Implicationsfor Study Design,Ó Genomics 79(1):104-113, 2002.13R.J. Cho, M.J. Campbell, E.A. Winzeler, L. Steinmetz, A. Conway, L. Wodicka, T.G. Wolfsberg, et al., ÒA Genome-wideTranscriptional Analysis of the Mitotic Cell Cycle,Ó Molecular Cell 2(1):65-73, 1998; P.T. Spellman, G. Sherlock, M.Q. Zhang, V.R.Iyer, K. Anders, M.B. Eisen, P.O. Brown, et al., ÒComprehensive Identification of Cell Cycle-regulated Genes of the Yeast Saccha-romyces cerevisiae by Microarray Hybridization,Ó Molecular Biology of the Cell 9(12):3273-3297, 1998. (Cited in Rice and Stolovitzky,ÒMaking the Most of It,Ó 2004, Footnote 11.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.40CATALYZING INQUIRYtured and formulated in the first place). However, the expense of microarrays may be an inhibitingfactor in this regard.3.4DATA ORGANIZATION
The acquiring of experimental data by some researcher is only the first step in making them usefulto the wider biological research community. Data are useless if they are inaccessible or incomprehen-
sible to others, and given the heterogeneity and large volumes of biological data, appropriate data
organization is central to extracting useful information from the data. Indeed, it would not be an
exaggeration to identify data management and organization issues as a key rate-limiting step in doing
science for the small to medium-sized laboratory, where ÒscienceÓ covers the entire intellectual water-
front from laboratory experiment to data that are useful to the community at large. This is especially
true in laboratories using high-throughput data acquisition technologies.In recent years, biologists have taken significant steps in coming to terms with the need to thinkcollectively about databases as research tools accessible to the entire community. In the field of molecu-
lar biology, the first widely recognized databases were the international archival repositories for DNA
and genomic sequence information, including GenBank, the European Molecular Biology Laboratory
(EMBL) Nucleotide Sequence Database, and the DNA Databank of Japan (DDJ). Subsequent databases
have provided users with information that annotated the genomic sequence data, connecting regions of
a genome with genes, identifying proteins associated with those genes, and assigning function to the
genes and proteins. There are databases of scientific literature, such as PubMed; databases on single
organisms, such as FlyBase (the Drosophila research database); and databases of protein interactions,such as the General Repository for Interaction Datasets (GRID). In their research, investigators typically
access multiple databases (from the several hundred Web-accessible biological databases). Table 3.1
provides examples of key database resources in bioinformatics.Data organization in biology faces significant challenges for the foreseeable future, given the levelsof data being produced. Each year, workshops associated with major conferences in computational
biology are held to focus on how to apply new techniques from computer science into computational
biology. These include the Intelligent Systems for Molecular Biology (ISMB) Conference and the Confer-
ence on Research in Computational Biology (RECOMB), which have championed the cause of creating
tools for database development and integration.14 The long-term vision for biology is for a decentral-ized collection of independent and specialized databases that operate as one large, distributed informa-
tion resource with common controlled vocabularies, related user interfaces, and practices. Much re-
search will be needed to achieve this vision, but in the short term, researchers will have to make do with
more specialized tools for the integration of diverse data types as described in Section 4.2.What is the technological foundation for managing and organizing data? In 1998, Jeff Ullman notedthat Òthe common characteristic of [traditional business databases] is that they have large amounts of
data, but the operations to be performed on the data are simple,Ó and also that under such circum-
stances, Òthe modification of the database scheme is very infrequent, compared to the rate at which
queries and other data manipulations are performed.Ó15The situation in biology is the reverse. Modern information technologies can handle the volumes ofdata that characterize 21st century biology, but they are generally inadequate to provide a seamless
integration of biological data across multiple databases, and commercial database technology has proven
to have many limitations in biological applications.16 For example, although relational databases haveoften been used for biological data management, they are clumsy and awkward to use in many ways.14T. Head-Gordon and J. Wooley, ÒComputational Challenges in Structural and Functional Genomics,Ó IBM Systems Journal40(2):265-296, 2001.15J.D. Ullman, Principles of Database and Knowledge-Base Systems, Vols. I and II, Computer Science Press, Rockville, MD, 1988.16H.V. Jagadish and F. Olken, ÒDatabase Management for Life Science Research,Ó OMICS: A Journal of Integrative Biology7(1):131-137, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA41TABLE 3.1Examples of Key Database Resources in Bioinformatics
CategoryDatabases and URLs
Comprehensive dataNCBI (National Center for Biotechnology and Information):
center: broad contenthttp://www.ncbi.nlm.nih.gov/
including sequence,
structure, function, etc.EBI (European Bioinformatics Institute): http://www.ebi.ac.uk/
European Molecular Biology Laboratory (EMBL):http://www.emblheidelberg.de/TIGR (the Institute of Genome Research): http://www.tigr.org/
Whitehead/Massachusetts Institute of Technology Genome Center:http://www-genome.wi.mit.edu/DNA or protein sequenceGenBank: http://www.ncbi.nlm.nih.gov/Genbank
DDBJ (DNA Data Bank of Japan): http://www.ddbj.nig.acjp/EMBL Nucleotide Sequence Databank:http://www.ebi.ac.uk/embl/index.htmlPIR (Protein Information Resource): http://pir.georgetown.edu/
Swiss-Prot: http://www.expasy.ch/sprot/sprot-top.htmlBiomolecular interactionsBIND (Biomolecular Interaction Network Database):
http://www.blueprint.org/bind/bind.phpThe contents of BIND include high-throughput data submissions and
hand-curated information gathered from the scientific literature.Genomes: completeEntrez complete genomes:
genome sequences andhttp://www.ncbi.nlm.nih.gov/Entrez/Genome/org.html
related information forspecific organismsComplete genome at EBI: http://www.ebi.ac.uk/genomes/
University of California, Santa Cruz, Human Genome Working Draft:http://genome.ucsc.edu/MGD (Mouse Genome Database): http://www.informaticsjax.org/SGD (Saccharomyces Genome Database):http://genomewww.stanford.edu/Saccharomyces/FlyBase (a database of the Drosophila genome):http://flybase.bio.indiana.edu/WormBase (the genome and biology of Caenorhabditis elegans):http://www.wormbase.org/Genetics: gene mapping,GDB (Genome Database): http://gdbwww.gdb.org/gdb/
mutations, and diseasesOMIM (Online Mendelian Inheritance in Man):http://www3.ncbi.nlm.nih.gov/Omim/searchomim.htmlHGMD (Human Gene Mutation Database):http://archive.uwcm.ac.uk/uwcm/mg/hgmdO.htmlcontinuedCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.42CATALYZING INQUIRYTABLE 3.1Continued
CategoryDatabases and URLs
Gene expression:Unigene: http://www.ncbi.nlm.nih.gov/UniGene/
microarray and cDNA
gene expressiondbEST (Expression Sequence Tag Database):
http://www.ncbi.nlm.nih.gov/dbEST/index.htmlBodyMap: http://bodymap.ims.u-tokyo.ac.jp/GEO (Gene Expression Omnibus): http://www.ncbi.nlm.nih.gov/geo/Structure: three-PDB (Protein Data Bank): http://www.rcsb.org/pdb/index.html
dimensional structures ofsmall molecules, proteins,NDB (Nucleic Acid Database):
nucleic acids (both RNAhttp://ndbserver.irutgers.edu/NDB/ndb.html

and DNA) foldingpredictionsCSD (Cambridge Structural Database):
http://www.ccdc. cam. ac.uk/prods/csd/csd.htmlClassification of proteinSCOP (Structure Classification of Proteins):
family and proteinhttp://scop.mrc-Imb.cam.ac.uk/scop/

domainsCATH (Protein Structure Classification Database):http://www.biochem.ucl.ac.uk/bsm/cath-new/index.htmlPfam: http://pfam.wustl.edu/PROSITE database for protein family and domains:http://www.expasy.ch/prosite/BLOCK: http://www.blocks.fhcrc.org/Protein pathwayKEGG (Kyoto Encyclopedia of Genes and Genomes):
Protein-proteinhttp://www.genome.ad.jp/kegg/kegg2.html#pathway
interactions andmetabolic pathwayBIND (Biomolecular Interaction Network Database):
http://www.binddb.org/DIP (Database of Interacting Proteins): http: Hdip.doe-mbi.ucla.edu/
EcoCyc (Encyclopedia of Escherichia coli Genes and Metabolism):http://ecocyc.org/ecocyc/ecocyc.htmlWIT (Metabolic Pathway): http://Hwit.mcs.anl.gov/WIT2/Proteomics: proteins,AFCS (Alliance for Cellular Signaling): http://cellularsignaling.org/
protein familyJCSG (Joint Center for Structure Genomics):http://www.jcsg.org/scripts/prod/home.htmlPKR (Protein Kinase Resource): http://pkr.sdsc.edu/html/index.shtmlCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA43The size of biological objects is often not constant. More importantly, relational databases presume theexistence of well-defined and known relationships between data records, whereas the reality of biologi-
cal research is that relationships are imprecisely knownÑand this imprecision cannot be reduced to
probabilistic measures of relationship that relational databases can handle.Jagadish and Olken argue that without specialized life sciences enhancements, commercial rela-tional database technology is cumbersome for constructing and managing biological databases, and
most approximate sequence matching, graph queries on biopathways, and three-dimensional shape
similarity queries have been performed outside of relational data management systems. Moreover, the
relational data model is an inadequate abstraction for representing many kinds of biological data (e.g.,
pedigrees, taxonomies, maps, metabolic networks, food chains). Box 3.1 provides an illustration of how
business database technology can be inadequate.Object-oriented databases have some advantages over relational databases since the natural foci ofstudy are in fact biological objects. Yet Jagadish and Olken note that object-oriented databases have also
had limited success in providing efficient or extensible declarative query languages as required for
specialized biological applications.Because commercial database technology is of limited help, research and development of databasetechnology that serves biological needs will be necessary. Jagadish and Olken provide a view of require-
ments that will necessitate further advances in data management technology, requirements that includePharmacogenomics,PharmGKB (Pharmacogenetics Knowledge Base):
pharmaco genetics, singlehttp://pharmgkb.org
nucleotide polymorphism(SNP), genotypingSNP Consortium: http://snp.cshl.org
dbSNP (Single Nucleotide Polymorphism Database):http://www.ncbi.nlm.nih.gov/SNP/LocusLink: http://www.ncbi.nlm.nih.gov/LocusLinkAFRED (Allele Frequency Database):http://alfred.med.yale. edu/alfred/index.aspCEPH Genotype Database: http://www.cephb.fr/cephdb/Tissues, organs, andVisible Human Project Database:
organismshttp://www.nlm.nih.gov/research/visible/visible-human.htmlBRAID (Brain Image Database): http://Hbraid.rad.jhu.edu/interface.html
NeuroDB (Neuroscience Federated Database):http://www.npaci.edu/DICE/Neuro/The Whole Brain Atlas:http://www.med.harvard.edu/AANLIB/home.htmlLiterature referencePubMed MEDLINE:
http://www.ncbi.nlm.nih.gov/entrez/query.fcgiUSPTO (U.S. Patent and Trademark Office): http://www.uspto.gov/TABLE 3.1Continued
CategoryDatabases and URLs
Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.44CATALYZING INQUIRYa great diversity of data types: sequences, graphs, three-dimensional structures, images; unconven-tional types of queries: similarity queries, (e.g., sequence similarity), pattern-matching queries, pattern-
finding queries; ubiquitous uncertainty (and sometimes even inconsistency) in the data; data curation
(data cleaning and annotation); large-scale data integration (hundreds of databases); detailed data
provenance; extensive terminology management; rapid schema evolution; temporal data; and manage-
ment for a variety of mathematical and statistical models of organisms and biological systems.Data organization and management present major intellectual challenges in integration and presen-tation, as discussed in Chapter 4.3.5DATA SHARING
There is a reasonably broad consensus among scientists in all fields that reproducibility of findingsis central to the scientific enterprise. One key component of reproducibility is thus the availability of
data for community examination and inspection. In the words of the National Research Council (NRC)
Committee on Responsibilities of Authorship in the Biological Sciences, Òan authorÕs obligation is notBox 3.1Probabilistic One-to-Many Database Entry LinkingOne purpose of database technology is the creation and maintenance of links between items in differentdatabases. Thus, consider the problem in which a primary biological database of genes contains an object(call it A) that subsequent investigation and research reveal to be two objects. For example, what was thought
to be a single gene might upon further study turn out to be two closely linked genes (A1 and A2) with anoncoding region in between (A3). Another database (e.g., a database of clones known to hybridize to variousgenes) may have contained a link to AÑcall the clone in question C. Research reveals that it is impossible for
C to hybridize to both A1 and A2 individually, but that it does hybridize to the set taken collectively (i.e., A1,A2, and A3).How should this relationship now be represented? Before the new discovery, the link was simple: C to A. Nowthat new knowledge requires that the primary database (or at least the entry for A) be restructured, how shouldthis new knowledge be reflected in the original simple link? That is, what should one do with links connected
to the previously single object, now that that single object has been divided into two?The new information in the primary database has three components, A1, A2, and A3. To which of these, ifany, should the original link be attached?  If the link is discarded entirely, the database loses the fact that C
hybridizes to the collection. If the link from C is now attached to all three equally, that link represents infor-mation contrary to fact, since experiment shows that C does not hybridize to both A1 and A2. The necessary
relationship that must be reflected calls for the clone entry C to link to A1, A2, and A3 simultaneously but alsoprobabilistically. That is, what must be represented is that the probability of the match in the set of three is oneand that the probability of match for two or one in the set is zero.As a general rule, such relationships (i.e., one-to-many relationships that are probabilistic) are not supportedby business database technology. However, they are required in scientific databases once this kind of splitting
operation has occurred on a hypothetical biological objectÑand such splitting is commonplace in scientificliterature. As indicated, it can occur in the splitting of a gene, or in other cases, it can occur in the splitting ofa species on the basis of additional findings on the biology of what was believed to be one species.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA45only to release data and materials to enable others to verify or replicate published findings but also toprovide them in a form on which other scientists can build with further research.Ó17However, in practice, this ethos is not uniformly honored. An old joke in the life science researchcommunity comments on data mining in biologyÑÒthe data are mine, mine, mine.Ó For a field whose
roots are in empirical description, it is not hard to see the origins of such an attitude. For most of its
history, the life sciences research community has granted primary intellectual credit to those who have
collected data, a stance that has reinforced the sentiment that those that collect the data are its rightful
owners. While some fields such as evolutionary biology generally have an ethos of data sharing, the
data-sharing ethos is honored with much less uniformity in many other fields of biology. Requests for
data associated with publications are sometimes (even often) denied, ignored, or fulfilled only after
long delay or with restrictions that limit how the data may be used.18The reasons for this state of affairs are multiple. The UPSIDE report called attention to the growingrole of the for-profit sector (e.g., the pharmaceutical, biotechnology, research-tool, and bioinformatics
companies) in basic and applied research over the last two decades, and the resulting circumstance that
increasing amounts of data are developed by and held in private hands. These for-profit entitiesÑ
whose primary responsibilities are to their investorsÑhope that their data will provide competitive
advantages that can be exploited in the marketplace.Nor are universities and other nonprofit research institutions immune to commercial pressures. Anincreasing amount of life sciences research in the nonprofit sector is supported directly by funds from
the for-profit sector, thus increasing the prospect of potentially conflicting missions that can impede
unrestricted data sharing as nonprofit researchers are caught up in commercial concerns. Universities
themselves are encouraged as a matter of public law (the Bayh-Dole Act of 1980) to promote the use,
commercialization, and public availability of inventions developed through federally funded research
by allowing them to own the rights to patents they obtain on these inventions. University researchers
also must confront the publish-or-perish issue. In particular, given the academic premiums on being
first to publish, researchers are strongly motivated to take steps that will preserve their own ability to
publish follow-up papers or the ability of graduate students, postdoctoral fellows, or junior faculty
members to do the same.Another contributing factor is that the nature of the data in question has changed enormously sincethe rise of the Human Genome Project. In particular, the enormous volumes of data collected are a
continuing resource that can be productively ÒminedÓ for a long time and yield many papers. Thus,
scientists who have collected such data can understandably view relinquishing control of them as a stiff
penalty in light of the time, cost, and effort needed to do the research supporting the first publication.19Although some communities (notably the genomics, structural biology, and clinical trials communities)
have established policies and practices to facilitate data sharing, other communities (e.g., those working
in brain imaging or gene and protein expression studies) have not yet done so.17National Research Council, Sharing Publication-Related Data and Materials: Responsibilities of Authorship in the Life Sciences,National Academies Press, Washington, DC, 2003. Hereafter referred to as the UPSIDE report. Much of the discussion in Section3.5 is based on material found in that report.18For example, a 2002 survey of geneticists and other life scientists at 100 U.S. universities found that of geneticists who hadasked other academic faculty for additional information, data, or materials regarding published research, 47 percent reportedthat at least one of their requests had been denied in the preceding 3 years. Twelve percent of geneticists themselves acknowl-
edged denying a request from another academic researcher. See E.G. Campbell, B.R. Clarridge, M. Gokhale, L. Birenbaum, S.Hilgartner, N.A. Holtzen, and D. Blumenthal, ÒData Withholding in Academic Genetics: Evidence from a National Survey,ÓJournal of the American Medical Association 287(4):473-480, 2002. (Cited in the UPSIDE report; see Footnote 17.)19Data provenance (the concurrent identification of the source of data along with the data itself as discussed in Section 3.7) hasan impact on the social motivation to share data. If data sources are always associated with data, any work based on that datawill automatically have a link to the original source; hence proper acknowledgment of intellectual credit will always be possible.Without automated data provenance, it is all too easy for subsequent researchers to lose the connection to the original source.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.46CATALYZING INQUIRYFinally, raw biological data are not the only commodities in question. Computational tools andmodels are increasingly the subject of publication in the life sciences (see Chapters 4 and 5), and it is
inevitable that similar pressures will arise (indeed, have arisen) with respect to sharing the software and
algorithms that underlie these artifacts. When software is at issue, a common concern is that the release
of softwareÑespecially if it is released in source codeÑcan enable another party to commercialize that
code. Some have also argued that mandatory sharing of source code prevents universities from exercis-
ing their legal right to develop commercial products from federally funded research.Considering these matters, the NRC Committee on Responsibilities of Authorship in the BiologicalSciences concluded:The act of publishing is a quid pro quo in which authors receive credit and acknowledgment in ex-change for disclosure of their scientific findings. All members of the scientific communityÑwhether
working in academia, government, or a commercial enterpriseÑhave equal responsibility for uphold-ing community standards as participants in the publication system, and all should be equally able toderive benefits from it.The UPSIDE report also explicated three principles associated with sharing publication-related dataand software:20¥Authors should include in their publications the data, algorithms, or other information that is centralor integral to the publicationÑthat is, whatever is necessary to support the major claims of the paper andwould enable one skilled in the art to verify or replicate the claims.¥If central or integral information cannot be included in the publication for practical reasons (for exam-ple, because a dataset is too large), it should be made freely (without restriction on its use for researchpurposes and at no cost) and readily accessible through other means (for example, on line). Moreover,when necessary to enable further research, integral information should be made available in a form that
enables it to be manipulated, analyzed, and combined with other scientific data. . . . [However, m]akingdata that is central or integral to a paper freely obtainable does not obligate an author to curate andupdate it. While the published data should remain freely accessible, an author might make available an
improved, curated version of the database that is supported by user fees. Alternatively, a value-addeddatabase could be licensed commercially.¥If publicly accessible repositories for data have been agreed on by a community of researchers and arein general use, the relevant data should be deposited in one of these repositories by the time of publica-tion. . . . [T]hese repositories help define consistent policies of data format and content, as well as accessi-bility to the scientific community. The pooling of data into a common format is not only for the purpose
of consistency and accessibility. It also allows investigators to manipulate and compare datasets, synthe-size new datasets, and gain novel insights that advance science.When a publication explicitly involves software or algorithms to solve biological problems, theUPSIDE report pointed out that the principle enunciated for data should also apply: software or algo-
rithms that are central or integral to a publication Òshould be made available in a manner that enables its
use for replication, verification, and furtherance of science.Ó The report also noted that one option is to
provide in the publication a detailed description of the algorithm and its parameters. A second option is
to make the relevant source code available to investigators who wish to test it, and either option
upholds the spirit of the researcherÕs obligation.Since the UPSIDE report was released in 2003, editors at two major life science journals, Science andNature, have agreed in principle with the idea that publication entails a responsibility to make datafreely available to the larger research community.21 Nevertheless, it remains to be seen how widely theUPSIDE principles will be adopted in practice.20The UPSIDE report contained five principles, but only three were judged relevant to the question of data sharing per se. Theprinciples described in the text are quoted directly from the UPSIDE report.21E. Marshall, ÒThe UPSIDE of Good Behavior: Make Your Data Freely Available,Ó Science 299(5609):990, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA47As for the technology to facilitate the sharing of data and models, the state of the art today is thateven when the will to share is present, data or model exchange between researchers is generally a
nontrivial exercise. Data and models from one laboratory or researcher must be accompanied by enough
metadata that other researchers can query the data and use the model in meaningful ways without a lot
of unproductive overhead in Òfutzing around doing stupid things.Ó Technical dimensions of this point
are discussed further in Section 4.2.3.6DATA INTEGRATION
 As noted in Chapter 2, data are the sine qua non of biological science. The ability to share datawidely increases the utility of those data to the research community and enables a higher degree of
communication between researchers, laboratories, and even different subfields. Data incompatibilities
can make data hard to integrate and to relate to information on other variables relevant to the same
biological system. Further, when inquiries can be made across large numbers of databases, there is an
increased likelihood that meaningful answers can be found. Large-scale data integration also has the
salutary virtue that it can uncover inconsistencies and errors in data that are collected in disparate ways.In digital form, all biological data are represented as bits, which are the underlying electronicrepresentation of data. However, for these data to be useful, they must be interpretable according to
some definitions. When there is a single point of responsibility for data management, the definitions are
relatively easy to generate. When responsibility is distributed over multiple parties, they must agree on
those definitions if the data of one party are to be electronically useful to another party. In other words,
merely providing data in digital form does not necessarily mean that they can be shared readilyÑthe
semantics of differing data sets must be compatible as well.Another complicating factor is the fact that nearly all databasesÑregardless of scaleÑhave theirorigins in small-scale experimentation. Researchers almost always obtain relatively small amounts of
data in their first attempts at experimentation. Small amounts of data can usually be managed in flat
filesÑtypically, spreadsheets. Flat files have the major advantage that they are quick and easy to
implement and serve small-scale data management needs quite well.However, flat files are generally impractical for large amounts of data. For example, queries involv-ing multiple search criteria are hard to make when a flat-file database is involved. Relationships be-
tween entries are concealed in a flat-file format. Also, flat files are quite poor for handling heteroge-
neous data types.There are a number of technologies and approaches, described below, that address such issues. Inpractice, however, the researcher is faced with the problem of knowing when to abandon the small-
scale flat file in favor of a more capable and technically sophisticated arrangement that will inevitably
entail higher overhead, at least initially.The problem of large-scale data integration is extraordinarily complex and difficult to solve. In2003, Lincoln Stein noted that Òlife would be much simpler if there was a single biological database, but
this would be a poor solution. The diverse databases reflect the expertise and interests of the groups that
maintain them. A single database would reflect a series of compromises that would ultimately impov-
erish the information resources that are available to the scientific community. A better solution would
maintain the scientific and political independence of the databases, but allow the information that they
contain to be easily integrated to enable cross-database queries. Unfortunately, this is not trivial.Ó22Consider, for example, what might be regarded as a straightforward problemÑthat of keepingstraight vocabularies and terminologies and their associated concepts. In reality, when new biological
structures, entities, and events have been uncovered in a particular biological context, they are often22Reprinted by permission from L.D. Stein, ÒIntegrating Biological Databases,Ó Nature Reviews Genetics 4(5):337-345, 2003.Copyright 2005 Macmillan Magazines Ltd.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.48CATALYZING INQUIRYdescribed with novel terminology or measurements that do not reveal much about how they might berelated to similar entities in other contexts or how they quantitatively function in the contexts in which
they exist, for example:¥Biological concepts may clash as users move from one database to another. Stein discussesseveral examples:231.To some research communities, Òa pseudogene is a gene-like structure that contains in-framestop codons or evidence of reverse transcription. To others, the definition of a pseudogene is
expanded to include gene structures that contain full open reading frames (ORFs) but are not
transcribed. Some members of the Neisseria gonorrhea research community, meanwhile, usepseudogene to mean a transposable cassette that is rearranged in the course of antigenic variation.Ó
2.ÒThe human genetics community uses the term allele to refer to any genomic variant, includ-

ing silent nucleotide polymorphisms that lie outside of genes, whereas members of many model-
organism communities prefer to reserve the term allele to refer to variants that change genes.Ó
3.ÒEven the concept of the gene itself can mean radically different things to different research

communities. Some researchers treat the gene as the transcriptional unit itself, whereas others
extend this definition to include up- and downstream regulatory elements, and still others use
the classical definitions of cistron and genetic complementation.Ó¥Evolving scientific understandings may drive changes in terminology. For example, diabeteswas once divided into the categories of juvenile and adult onset. As the role of insulin became clearer,
the relevant categories evolved into Òinsulin dependentÓ and Ònon-insulin dependent.Ó The relation-
ship is that almost all juvenile cases of diabetes are insulin dependent, but a significant fraction of adult-
onset cases are as well.¥Names of the same biological object may change across databases. ÒFor example, consider theDNA-damage checkpoint-pathway gene that is named Rad24 in Saccharomyces cerevisiae (budding yeast).[Schizo]saccharomyces pombe (fission yeast) also has a gene named rad24 that is involved in the check-point pathway, but it is not the orthologue of the S. cerevisiae Rad24. Instead, the correct S. pombeorthologue is rad17, which is not to be confused with the similarly named Rad17 gene in S. cerevisiae.Meanwhile, the human checkpoint-pathway genes are sometimes named after the S. cerevisiaeorthologues, sometimes after the S. pombe orthologues, and sometimes have independently derivednames. In C. elegans, there are a series of rad genes, none of which is orthologous to S. cerevisiae Rad17.The closest C. elegans match to Rad17 is, in fact, a DNA-repair gene named mrt-2.Ó24¥Implicit meanings can be counterintuitive. For example, the International Classification of Dis-ease (ICD) code for ÒanginaÓ means Òangina occurring in the past.Ó25 A condition of current angina isindicated by the code for Òchest pain not otherwise specified.Ó¥Data transformations from one database to another may destroy useful information. For ex-ample, a clinical order in a hospital may call for a ÒPA [posterior-anterior] and lateral chest X-ray.Ó
When that order is reflected in billing, it may be collapsed into Òchest X-ray: 2 views.Ó¥Metadata may change when databases originally created for different purposes are conceptuallyjoined. For example, MEDLINE was developed to facilitate access to the printed paper literature by23Reprinted by permission from L.D. Stein, ÒIntegrating Biological Databases,Ó  Nature Reviews Genetics 4(5):337-345, 2003.Copyright 2005 Macmillan Magazines Ltd.24Reprinted by permission from L.D. Stein, ÒIntegrating Biological Databases,Ó  Nature Reviews Genetics 4(5):337-345, 2003.Copyright 2005 Macmillan Magazines Ltd.25ICD codes refer to a standard international classification of diseases. For more information, see http://www.cdc.gov/nchs/about/otheract/icd9/abticd9.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA49scientists. The data were assembled in MEDLINE to help users find citations. As a result, authors inMEDLINE were originally treated as text strings, not as people. There was no effort, to identify indi-
vidual people, so ÒSmith, JÓ could be John Smith, Jim Smith, or Joan Smith. However, the name of an
individual is not necessarily constant over his or her professional lifetime. Thus, one cannot use

MEDLINE to search for all papers authored by an individual who has undergone a name change
without independent knowledge of the specifics of that change.Experience suggests that left to their own devices, designers of individual databases generally makelocally optimal decisions about data definitions and formats for entirely rational reasons, and local deci-
sions are almost certain to be incompatible in some ways with other such decisions made in other labora-
tories by other researchers.26 Nearly 10 years ago, Robbins noted that Òa crisis occurred in the [biological]databases in the mid 1980s, when the data flow began to outstrip the ability of the database to keep up. A
conceptual change in the relationship of databases to the scientific community, coupled with technical
advances, solved the problem. . . . Now we face a data-integration crisis of the 1990s. Even if the various
separate databases each keep up with the flow of data, there will still be a tremendous backlog in the
integration of information in them. The implication is similar to that of the 1980s: either a solution will
soon emerge or biological databases collectively will experience a massive failure.Ó27 Box 3.2 describessome of the ways in which community-wide use of biological databases continues to be difficult today.Two examples of research areas requiring a large degree of data integration are cellular modeling andpharmacogenomics. In cellular modeling (discussed further in Section 5.4.2), researchers need to integrate
the plethora of data available today about cellular function; such information includes the chemical,
electrical, and regulatory features of cells; their internal pathways; mechanisms of cell motility; cell shape
changes; and cell division. Box 3.3 provides an example of a cell-oriented database. In pharmacogenomics
(the study of how an individualÕs genetic makeup affects his or her specific reaction to drugs, discussed in
Section 9.7), databases must integrate data on clinical phenotypes (including both pharmacokinetic and
pharmacodynamic data) and profiles (e.g., pulmonary, cardiac, and psychological function tests, and
cancer chemotherapeutic side effects); DNA sequence data, gene structure, and polymorphisms in se-
quence (and information to track haploid, diploid, or polyploid alleles, alternative splice sites, and poly-
morphisms observed as common variants); molecular and cellular phenotype data (e.g., enzyme kinetic
measurements); pharmacodynamic assays; cellular drug processing rates; and homology modeling of
three-dimensional structures. Box 3.4 illustrates the Pharmacogenetics Research Network and Knowledge
Base (PharmGKB), an important database for pharmacogenetics and pharmacogenomics.3.7DATA CURATION AND PROVENANCE
28Biological research is a fast-paced, quickly evolving discipline, and data sources evolve with it: newexperimental techniques produce more and different types of data, requiring database structures to
change accordingly; applications and queries written to access the original version of the schema must26In particular, a scientist working on the cutting edge of a problem almost certainly requires data representations and modelswith more subtlety and more degrees of resolution in the data relevant to the problem than someone who has only a passinginterest in that field. Almost every dataset collected has a lot of subtlety in some areas of the data model and less subtlety
elsewhere. Merging these datasets into a common-denominator model risks throwing away the subtlety, where much of thevalue resides. Yet, merging these datasets into a uniformly data-rich model results in a database so rich that it is not particularlyuseful for general use. An exampleÑbiomedical databases for human beings may well include coding for gender as a variable.
However, in a laboratory or medical facility that does a lot of work on transgendered individuals who may have undergone sex-change operations, the notion of gender is not necessarily as simple as ÒmaleÓ or Òfemale.Ó27R.J. Robbins, ÒComparative Genomics: A New Integrative Biology,Ó in Integrative Approaches to Molecular Biology, J. Collado-Vides, B. Magasanik, and T.F. Smith, eds., MIT Press, Cambridge, MA, 1996.28Section 3.7 embeds excerpts from S.Y. Chung and J.C. Wooley, ÒChallenges Faced in the Integration of Biological Informa-tion,Ó Bioinformatics: Managing Scientific Data, Z. Lacroix and T. Critchlow, eds., Morgan Kaufmann, San Francisco, CA, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.50CATALYZING INQUIRYBox 3.2Characteristics of Biological DatabasesBiological databases have several characteristics that make them particularly difficult to use by the communityat large. Biological databases are¥Autonomous. As a point of historical fact, most biological databases have been developed and maintainedby individual research groups or research institutions. Initially, these databases were developed for individualuse by these groups or institutions, and even when they proved to have value to the larger community, datamanagement practices peculiar to those groups remained. As a result, biological databases almost alwayshave their own governing body and infrastructure.¥Inconsistent in format (syntax). In addition to the heterogeneity of data types discussed in Section 3.1,databases that contain the same types of data still may be (and often are) syntactically heterogeneous. Forexample, the scientific literature, images, and other free-text documents are commonly stored in unstructuredor semistructured formats (plain text files, HTML or XML files, binary files). Genomic, microarray gene expres-sion, and proteomic data are routinely stored in conventional spreadsheet programs or in structured relationaldatabases (Oracle, Sybase, DB2, Informix, etc.). Major data depository centers have also adopted differentstandards for data formats. For example, the U.S. National Center for Biotechnology Information (NCBI) hasadopted the highly nested data ASN.1 (Abstract Syntax Notation) for the general storage of gene, protein, andgenomic information, while the U.S. Department of AgricultureÕs Plant Genome Data and Information Centerhas adopted the object-oriented ACEDB data management systems and interface.¥Inconsistent in meaning (semantics). Biological databases containing the same types of data are also oftensemantically inconsistent. For example, in the database of biological literature known as MEDLINE, multiplealiases for genes are the norm, rather than the exception. There are cases in which the same name refers todifferent genes that have no relationship to each other. A gene that codes for an enzyme might be namedaccording to its mutant phenotype by a geneticist and its enzymatic function by a biochemist. A vector to amolecular biologist refers to a vehicle, as in a cloning vector, whereas vector to a parasitologist is an organismthat is an agent in the transmission of disease. Research groups working with different organisms will oftengive the same molecule a different name. Finally, biological knowledge is often represented only implicitly, inthe shared assumptions of the community that produced the data source, and not explicitly via metadata thatcan be used either by human users or by integration software.¥Dynamic and subject to continual change. As biological research progresses and better understandingemerges, it is common that new data are obtained that contradict old data. Often, new data organizationalschemes become necessary, even new data types or entirely new databases may become necessary.¥Diverse in the query tools they support. The queries supported by a database are what give the database itsutility for a scientist, for only through the making of a query can the appropriate data be returned. Yet databas-es vary widely in the kinds of query they supportÑor indeed that they can support. User interfaces to queryengines may require specific input and output formats. For example, BLAST (the basic local alignment searchtool), the most frequently used program in the molecular biology community, requires a specific format(FASTA) for input sequence and outputs a list of pairwise sequence alignments to the end users. Output fromone database query often is not suitable as direct input for a query on a different database. Finally, applicationsemantics vary widely. Leaving aside the enormous variety of different applications for different biologicalproblems (e.g., applications for nucleic and protein sequence analysis, genome comparison, protein structureprediction, biochemical pathway and genetic network analysis, construction of phylogenetic trees, modelingand simulation of biological systems and processes), even applications nominally designed for the sameproblem domain can make different assumptions about the underlying data and the meaning of answers toqueries. At times, they require nontrivial domain knowledge from different fields. For example, protein foldingcan be approached using ab initio prediction based on first principles (physics) or using knowledge-based(computer science) threading methods.¥Diverse in the ways they allow users to access data. Some databases provide large text dumps of theircontents, others offer access to the underlying database management system and still others provide only Webpages as their primary mode of access.SOURCE: Derived largely from S.Y. Chung and J.C. Wooley, ÒChallenges Faced in the Integration of Biological Information,ÓBioinformatics: Managing Scientific Data, Z. Lacroix and T. Critchlow, eds., Morgan Kaufmann, San Francisco, CA, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA51be rewritten to match the new version. Incremental updates to data warehouses (as opposed to whole-sale rebuilding of the warehouse from scratch) are difficult to accomplish efficiently, particularly when
complex transformations or aggregations are involved.A most important point is that most broadly useful databases contain both raw data and data thatare either the result of analysis or derived from other databases. In this environment, databases become
interdependent. Errors due to data acquisition and handling in one database can be propagated quickly
into other databases. Data updated in one database may not be propagated immediately to related
databases.Thus, data curation is essential. Curation is the process through which the community of users canhave confidence in the data on which they rely. So that these data can have enduring value, information
related to curation must itself be stored within the database; such information is generally categorized
as annotation data. Data provenance and data accuracy are central concerns, because the distinctions
between primary data generated experimentally, data generated through the application of scientificBox 3.3The Alliance for Cellular SignalingThe Alliance for Cellular Signaling (AfCS), partly supported by the National Institute of General MedicalSciences and partly by large pharmaceutical companies, seeks to build a publicly accessible, comprehensive
database on cellular signaling that makes available virtually all significant information about molecules ofinterest. This database will also be one enabler for pathway analysis and facilitate an understanding of howmolecules coordinate with one another during cellular responses. The database seeks to identify all of the
proteins that constitute the various signaling systems, assess time-dependent information flow through thesystems in both normal and pathological states, and reduce the mass of detailed data into a set of interactingtheoretical models that describe cellular signaling. To the maximum extent possible, the information con-
tained in the database is intended to be machine-readable.The complete database is intended to enable researchers to:
¥Query the database about complex relationships between molecules;¥View phenotype-altering mutations or functional domains in the context of protein structure;¥View or create de novo signaling pathways assembled from knowledge of interactions between moleculesand the flow of information among the components of complex pathways;¥Evaluate or establish quantitative relationships among the components of complex pathways;¥View curated information about specific molecules of interest (e.g., names, synonyms, sequence informa-tion, biophysical properties, domain and motif information, protein family details, structure and gene data, theidentities of orthologues and paralogues, BLAST results) through a Òmolecule home pageÓ devoted to each
molecule of interest, and¥Read comprehensive, peer-reviewed, expert-authored summaries, which will include highly structuredinformation on protein states, interactions, subcellular localization, and function, together with references to
the relevant literature.The AFCS is motivated by a desire to understand as completely as possible the relationships between sets ofinputs and outputs in signaling cells that vary both temporally and spatially. Yet because there are many re-searchers engaged in signaling research, the cultural challenge faced by the alliance is the fact that informationin the database is collected by multiple researchers in different laboratories and from different organizations.
Today, it involves more than 50 investigators from 20 academic and industrial institutions. However, as of thiswriting, it is reported that the NIGMS will reduce funding sharply for the Alliance following a mid-project reviewin early 2005 (see Z. Merali and J. Giles, ÒDatabases in Peril,Ó 
Nature 435:1010-1011, 23 June 2005).Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.52CATALYZING INQUIRYBox 3.4The Pharmacogenetics Research Network and Knowledge BaseSupported by the National Institute of General Medical Sciences (NIGMS) of the National Institutes of Health,the Pharmacogenetics Research Network and Knowledge Base (PharmGKB) is intended as a national resourcecontaining high-quality structured data linking genomic information, molecular and cellular phenotype infor-mation, and clinical phenotype information. The ultimate aim of this project is to produce a knowledge base
that provides a public infrastructure for understanding how variations in the human genome lead to variationsin clinical response to medications.Sample inquiries to this database might include the following:1. For gene X, show all observed polymorphisms in its sequence;2. For drug Y, show the variability in pharmacokinetics; and3. For phenotype Z, show the variability in association with drug Y and/or gene X.Such queries require a database that can model key elements of the data, acquire data efficiently, providequery tools for analysis, and deliver the resulting system to the scientific community.A central challenge for PharmGKB is that data contained it must be cross-referenced and integrated with avariety of other Web-accessible databases. Thus, PharmGKB provides mechanisms for surveillance of andintegration with these databases, allowing users to submit one query with the assurance that other relevant
databases are being accessed at the same time. For example, PharmGKB monitors dbSNP, the National Centerfor BioTechnology Information (NCBI)-supported repository for single nucleotide polymorphisms and shortdeletion and insertion polymorphisms. These monitoring operations search for new information about the
genes of interest to the various research groups associated with the Pharmacogenetics Research Network. Inaddition, PharmGKB provides users with a tool for comparative genomic analysis between human and mousethat focuses on long-range regulatory elements. Such elements can be difficult to find experimentally, but are
often conserved in syntenic regions between mice and humans, and may be useful in focusing polymorphismstudies on noncoding areas that are more likely to be associated with detectable phenotypes.Another important issue for the PharmGKB database is that because it contains clinical data derived fromindividual patients, it must have functionality that enforces the rights of those individuals to privacy andconfidentiality. Thus, data flow must be limited both into and out of the knowledge base, based on evolving
rules defining what can be stored in PharmGKB and what can be disseminated. No identifying informationabout an individual patient can be accepted into the knowledge base, and the data must be ÒmassagedÓ sothat patient identity cannot be reconstructed from publicly available data records.29P. Buneman, S. Khanna, and W.C. Tan, ÒWhy and Where: A Characterization of Data Provenance,Ó 8th International Confer-ence on Database Theory (ICDT), pp. 316-330, 2001. Cited in Chung and Wooley, ÒChallenges Faced in the Integration of BiologicalInformation,Ó 2003, Footnote 28.analysis programs, and data derived from database searches are blurred. Users of databases containingthese kinds of data must be concerned about where the data come from and how they are generated. A
database may be a potentially rich information resource, but its value is diminished if it fails to keep an
adequate description of the provenance of the data it contains.29 Although proponents of online accessCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA53to databases frequently tout it as an advantage that Òthe user does not need to know where the datacame from or where the data are located,Ó in fact it is essential for quality assurance reasons that the user
be able to ascertain the source of all data accessed in such databases.Data provenance addresses questions such as the following: Where did the characterization of agiven GenBank sequence originate? Has an inaccurate legacy annotation been ÒtransitivelyÓ propa-
gated to similar sequences? What is the evidence for this annotation?A complete record of a datumÕs history presents interesting intellectual questions. For example, it isdifficult to justify filling a database with errata notices correcting simple errors when the actual entriesGenomicGenomicInformationInformationMolecular &Molecular &CellularCellularPhenotypePhenotypeClinicalClinicalPhenotypePhenotypeAllelesAllelesMoleculesMoleculesIndividualsIndividualsDrugDrugResponseResponseSystemsSystemsDrugsDrugsEnvironmentEnvironmentIsolated Isolated functional functional measuresmeasuresCodingCodingrelationshiprelationshipPharmacologicPharmacologicactivitiesactivitiesProteinProteinproductsproductsRole inRole inorganismorganismVariationsVariationsin genomein genomeMolecularMolecularvariationsvariationsTreatmentTreatmentprotocolsprotocolsObservableObservablephenotypesphenotypesGeneticGeneticmakeupmakeupPhysiologyPhysiologyNonNon--geneticgeneticfactorsfactorsIntegratedIntegratedfunctional functional measuresmeasuresObservableObservablephenotypesphenotypesFIGURE 3.4.1Complexity of relationships in pharmacogenetics.
SOURCE: Figure reprinted and text adapted by permission from T.E. Klein, J.T. Chang, M.K. Cho, K.L. Easton, R. Fergerson, M. Hewett, Z.Lin, Y. Liu, S. Liu, D.E. Oliver, D.L. Rubin, F. Shafa, J.M. Stuart, and R.B. Altman, ÒIntegrating Genotype and Phenotype Information: AnOverview of the PharmGKB Project,Ó The Pharmacogenomics Journal 1:167-170, 2001. Copyright 2001 Macmillan Publishers Ltd.PharmGKB integrates data on clinical phenotypes (including both pharmacokinetic and pharmacodynamicdata) and profiles (e.g., pulmonary, cardiac, and psychological function tests; cancer chemotherapeutic side
effects), DNA sequence data, gene structure, and polymorphisms in sequence (and information to track hap-loid, diploid, or polyploid alleles; alternative splice sites; and polymorphisms observed as common variants),molecular and cellular phenotype data (e.g., enzyme kinetic measurements), pharmacodynamic assays, cellu-
lar drug processing rates, and homology modeling of three-dimensional structures. Figure 3.4.1 illustrates thecomplex relationships that are of interest for this knowledge base.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.54CATALYZING INQUIRYcan be updated. However, the original data themselves might be important, because subsequent re-search might have been based on them. One view is that once released, electronic database entries, like
the pages of a printed journal, must stand for all time in their original condition, with errors and
corrections noted only by the additional publication of errata and commentaries. However, this might
quickly lead to a situation in which commentary outweighs original entries severalfold. On the other
hand, occasional efforts to ÒimproveÓ individual entries might inadvertently result in important infor-
mation being mistakenly expunged. A middle ground might be to require that individual released
entries be stable, no matter what the type of error, but that change entries be classified into different
types (correction of data entry error, resubmission by original author, correction by different author,
etc.), thus allowing the user to set filters to determine whether to retrieve all entries or just the most
recent entry of a particular type.To illustrate the need for provenance, consider that the output of a program used for scientificanalysis is often highly sensitive to the parameters used and the specifics of the input datasets. In the
case of genomic analysis, a finding that two sequences are ÒsimilarÓ or not may depend on the specific
algorithms used and the different cutoff values used to parameterize matching algorithms, in which
case other evidence is needed. Furthermore, biological conclusions derived by inference in one database
will be propagated and may no longer be reliable after numerous transitive assertions. Repeated transi-
tive assertions inevitably degrade data, whether the assertion is a transitive inference or the result of a
simple ÒjoinÓ operation. In the absence of data perfection, additional degradation occurs with each
connection.For a new sequence that does not match any known sequence, gene prediction programs can beused to identify open reading frames, to translate DNA sequence into protein sequence, and to charac-
terize promoter and regulatory sequence motifs. Gene prediction programs are also parameter-depen-
dent, and the specifics of parameter settings must be retained if a future user is to make sense of the
results stored in the database.Neuroscience provides a good example of the need for data provenance. Consider the response ofrat cortical cells to various stimuli. In addition to the ÒprimaryÓ data themselvesÑthat is, voltages as a
function of timeÑit is also important to record information about the rat: where the rat came from, how
the rat was killed, how the brain was extracted, how the neurological preparation was made, what
buffers were present, the temperature of the preparation, how much time elapsed between the sacrifice
of the rat and the actual experiment being done, and so on. While all of this ÒextraÓ information seems
irrelevant to the primary question, neuroscience has not advanced to the point where it is known which
of these variables might have an effect on the response of interestÑthat is, on the evoked cortical
potential.Box 3.5 provides two examples of well-characterized and well-curated data repositories.
Finally, how far curation can be carried is an open question. The point of curation is to providereliable and trustworthy dataÑwhat might be called biological truths. But the meaning of such ÒtruthsÓ
may well change as more data is collected and more observations are madeÑsuggesting a growing
burden of constant editing to achieve accuracy and internal consistency. Indeed, every new entry in the
database would necessarily trigger extensive validity checks of all existing entries individually and
perhaps even for entries taken more than one at a time. Moreover, assertions about the real world may
be initially believed, then rejected, then accepted again, albeit in a modified form. Catastrophism in
geology is an example. Thus, maintaining a database of all biological truths would be an editorial
nightmare, if not an outright impossibilityÑand thus the scope of any single database will necessarily
be limited.A database of biological observations and experimental results provides different challenges. Anindividual datum or result is a stand-alone contribution. Each datum or result has a recognized party
responsible for it, and inclusion in the database means that it has been subject to some form of editorial
review, which presumably assures its adherence to current scientific practices (and does not guaranteeCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ON THE NATURE OF BIOLOGICAL DATA55Box 3.5Two Examples of Well-Curated Data RepositoriesGenBankGenBank is a public database of all known nucleotide and protein sequences, distributed by the NationalCenter for Biotechnology Information (NCBI), a division of the National Library of Medicine (NLM). As ofJanuary 2003, GenBank contained over 20 billion nucleotide bases in sequences from more than 55,000
speciesÑhuman, mice, rat, nematode, fruit fly, and the model plant Arabidopsis are the most represented.GenBank and its collaborating European (EMBL) and Japanese (JPPL) databases are built with data submittedelectronically by individual investigators (using BankIt or Sequin submission programs) and large-scale se-
quencing centers (using batch procedures). Each submission is reviewed for quality assurance and assigned anaccession number; sequence updates are designated as new versions. The database is organized by a se-quence-based taxonomy into divisions (e.g., bacteria, viruses, primates) and categories (e.g., expressed se-
quence tags, genome survey sequences, high-throughput genomic data). GenBank makes available derivativedatabases, for example of putative new genes, from these data.Investigators use the Entrez retrieval system for cross-database searching of GenBankÕs collections of DNA,protein, and genome mapping sequence data, population sets, the NCBI taxonomy, protein structures fromthe Molecular Modeling Database (MMDB), and MEDLINE references (from the scientific literature). A popu-
lar tool is BLAST, the sequence alignment program, for finding GenBank sequences similar to a query se-quence. The entire database is available by anonymous FTP in compressed flat-file format, updated every 2months. NCBI offers its ToolKit to software developers creating their own interfaces and specialized analytical
tools.The Research Resource for Complex Physiologic SignalsThe Research Resource for Complex Physiologic Signals was established by the National Center for ResearchResources of the National Institutes of Health to support the study of complex biomedical signals. The creation
of this three-part resource (PhysioBank, PhysioToolkit, and PhysioNet) overcomes long-standing barriers tohypothesis-testing research in this field by enabling access to validated, standardized data and software.1PhysioBank comprises databases of multiparameter, cardiopulmonary, neural, and other biomedical signals
from healthy subjects and patients with pathologies such as epilepsy, congestive heart failure, sleep apnea,and sudden cardiac death. In addition to fully characterized, multiply reviewed signal data, PhysioBank
provides online access to archival data that underpin results reported in the published literature, significantlyextending the contribution of that published work. PhysioBank provides theoreticians and software develop-ers with realistic data with which to test new algorithms.The PhysioToolkit includes software for the detection of physiologically significant events using both classicmethods and novel techniques from statistical physics, fractal scaling analysis, and nonlinear dynamics; the
analysis of nonstationary processes; interactive display and characterization of signals; the simulation of phys-iological and other signals; and the quantitative evaluation and comparison of analysis algorithms.PhysioNet is an online forum for the dissemination and exchange of recorded biomedical signals and thesoftware for analyzing such signals; it provides facilities for the cooperative analysis of data and the evaluationof proposed new algorithms. The database is available at http://www.physionet.org/physiobank.1A.L. Goldberger, L.A. Amaral, L. Glass, J.M. Hausdorff, P.C. Ivanov, R.G. Mark, J.E. Mietus, G.B. Moody, C.K. Peng, and H.E. Stanley,ÒPhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals,Ó Circulation101(23):E215-E220, 2000.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.56CATALYZING INQUIRYits absolute truth value). Without the existence of databases with differing editorial policies, someimportant but iconoclastic data or results might never be published. On the other hand, there is no
guarantee of consistency among these data and results, which means that progress at the frontiers will
depend on expert judgment in deciding which data and results will constitute the foundation from
which to build.In short, reconciling the tension between truth and diversityÑboth desirable, but for differentreasonsÑis implicitly a part of the construction of every large-scale database.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS57574Computational Tools
As a factual science, biological research involves the collection and analysis of data from potentiallybillions of members of millions of species, not to mention many trillions of base pairs across different
species. As data storage and analysis devices, computers are admirably suited to the task of supporting
this enterprise. Also, as algorithms for analyzing biological data have become more sophisticated and
the capabilities of electronic computers have advanced, new kinds of inquiries and analyses have
become possible.4.1THE ROLE OF COMPUTATIONAL TOOLS
Today, biology (and related fields such as medicine and pharmaceutics) are increasingly data-intensiveÑa trend that arguably began in the early 1960s.1 To manage these large amounts of data, andto derive insight into biological phenomena, biological scientists have turned to a variety of computa-
tional tools.As a rule, tools can be characterized as devices that help scientists do what they know they must do.That is, the problems that tools help solve are problems that are known by, and familiar to, the scientists
involved. Further, such problems are concrete and well formulated.  As a rule, it is critical that compu-
tational tools for biology be developed in collaboration with biologists who have deep insights into the
problem being addressed.The discussion below focuses on three generic types of computational tools: (1) databases and datamanagement tools to integrate large amounts of heterogeneous biological data, (2) presentation tools
that help users comprehend large datasets, and (3) algorithms to extract meaning and useful informa-
tion from large amounts of data (i.e., to find meaningful a signal in data that may look like noise at first
glance). (Box 4.1 presents a complementary view of advances in computer sciences needed for next-
generation tools for computational biology.)1The discussion in Section 4.1 is derived in part from T. Lenoir, ÒShaping Biomedicine as an Information Science,Ó Proceedingsof the 1998 Conference on the History and Heritage of Science Information Systems, M.E. Bowden, T.B. Hahn, and R.V. Williams, eds.,ASIS Monograph Series, Information Today, Inc., Medford, NJ, 1999, pp. 27-45.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.58CATALYZING INQUIRYThese examples are drawn largely from the area of cell biology. The reason is not that these are theonly good examples of computational tools, but rather that a great deal of the activity in the field has
been the direct result of trying to make sense out of the genomic sequences that have been collected to
date. As noted in Chapter 2, the Human Genome ProjectÑcompleted in draft in 2000Ñis arguably the
first large-scale project of 21st century biology in which the need for powerful information technology
was manifestly obvious. Since then, computational tools for the analysis of genomic data, and by
extension data associated with the cell, have proliferated wildly; thus, a large number of examples are
available from this domain.4.2TOOLS FOR DATA INTEGRATION
2As noted in Chapter 3, data integration is perhaps the most critical problem facing researchers asthey approach biology in the 21st century.Box 4.1Tool Challenges for Computer ScienceData Representation¥Next-generation genome annotation system with accuracy equal to or exceeding the best humanpredictions¥Mechanism for multimodal representation of dataAnalysis Tools¥Scalable methods of comparing many genomes¥Tools and analyses to determine how molecular complexes work within the cell¥Techniques for inferring and analyzing regulatory and signaling networks¥Tools to extract patterns in mass spectrometry datasets¥Tools for semantic interoperabilityVisualization¥Tools to display networks and clusters at many levels of detail¥Approaches for interpreting data streams and comparing high-throughput data with simulation outputStandards¥Good software-engineering practices and standard definitions (e.g., a common component architecture)¥Standard ontology and data-exchange format for encoding complex types of annotationDatabases¥Large repository for microbial and ecological literature relevant to the ÒGenomes to LifeÓ effort.¥Big relational database derived by automatic generation of semantic metadata from the biological literature¥Databases that support automated versioning and identification of data provenance¥Long-term support of public sequence databasesSOURCE: U.S. Department of Energy, Report on the Computer Science Workshop for the Genomes to Life Program, Gaithersburg, MD,March 6-7, 2002; available at http://DOEGenomesToLife.org/compbio/.2Sections 4.2.1, 4.2.4, 4.2.6, and 4.2.8 embed excerpts from S.Y. Chung and J.C. Wooley, ÒChallenges Faced in the Integration ofBiological Information,Ó in Bioinformatics: Managing Scientific Data, Z. Lacroix and T. Critchlow, eds., Morgan Kaufmann, SanFrancisco, CA, 2003. (Hereafter cited as Chung and Wooley, 2003.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS594.2.1Desiderata
If researcher A wants to use a database kept and maintained by researcher B, the Òquick and dirtyÓsolution is for researcher A to write a program that will translate data from one format into another. For
example, many laboratories have used programs written in Perl to read, parse, extract, and transform
data from one form into another for particular applications.3 Depending on the nature of the datainvolved and the structure of the source databases, writing such a program may require intensive
coding.Although such a fix is expedient, it is not scalable. That is, point-to-point solutions are not sustain-able in a large community in which it is assumed that everyone wants to share data with everyone else.
More formally, if there are N data sources to be integrated, and point-to-point solutions must bedeveloped, N (N Ð 1)/2 translation programs must be written. If one data source changes (as is highlylikely), N Ð 1 programs must be updated.A more desirable approach to data integration is scalable. That is, a change in one database shouldnot necessitate a change on the part of every research group that wants to use those data. A number of
approaches are discussed below, but in general, Chung and Wooley argue that robust data integration
systems must be able to1.Access and retrieve relevant data from a broad range of disparate data sources;
2.Transform the retrieved data into a common data model for data integration;

3.Provide a rich common data model for abstracting retrieved data and presenting integrated data
objects to the end-user applications;4.Provide a high-level expressive language to compose complex queries across multiple data
sources and to facilitate data manipulation, transformation, and integration tasks; and5.Manage query optimization and other complex issues.
Sections 4.2.2, 4.2.4, 4.2.5, 4.2.6, and 4.2.8 address a number of different approaches to dealing withthe data integration problem. These approaches are not, in general, mutually exclusive, and they may be
usable in combination to improve the effectiveness of a data integration solution.Finally, biological databases are always changing, so integration is necessarily an ongoing task. Notonly are new data being integrated within the existing database structure (a structure established on the
basis of an existing intellectual paradigm), but biology is a field that changes quicklyÑthus requiring
structural changes in the databases that store data. In other words, biology does not have some Òclassi-
cal core frameworkÓ that is reliably constant. Thus, biological paradigms must be redesigned from time
to time (on the scale of every decade or so) to keep up with advances, which means that no Ògold
standardsÓ to organize data are built into biology. Furthermore, as biology expands its attention to
encompass complexes of entities and events as well as individual entities and events, more coherent
approaches to describing new phenomena will become necessaryÑapproaches that bring some com-
monality and consistency to data representations of different biological entitiesÑso that relationships
between different phenomena can be elucidated.As one example, consider the potential impact of Ò-omicÓ biology, biology that is characterized bya search for data completenessÑthe complete sequence of the human genome, a complete catalog of
proteins in the human body, the sequencing of all genomes in a given ecosystem, and so on. The
possibility of such completeness is unprecedented in the history of the life sciences and will almost
certainly require substantial revisions to the relevant intellectual frameworks.3The Perl programming language provides powerful and easy-to-use capabilities to search and manipulate text files. Becauseof these strengths, Perl is a major component of much bioinformatics programming. At the same time, Perl is regarded by manycomputer scientists as an unsafe language in which it is easy to make programs do dangerous things. In addition, many regardthe syntax and structure of most Perl programs to be of a nature that is hard to understand much after the fact.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.60CATALYZING INQUIRY4.2.2  Data Standards
One obvious approach to data integration relies on technical standards that define representations ofdata and hence provide an understanding of data that is common to all database developers. For obvious
reasons, standards are most relevant to future datasets. Legacy databases, which have been built around
unique data definitions, are much less amenable to a standards-driven approach to data integration.Standards are indeed an essential element of efforts to achieve data integration of future datasets,but the adoption of standards is a nontrivial task. For example, community-wide standards for data
relevant to a certain subject almost certainly differ from those that might be adopted by individual
laboratories, which are the focus of the Òsmall-instrument, multi-data-sourceÓ science that characterizes
most public-sector biological research.Ideally, source data from these projects flow together into larger national or international dataresources that are accessible to the community. Adopting community standards, however, entails local
compromises (e.g., nonoptimal data structuring and semantics, greater expense), and the budgets that
characterize small-instrument, single-data-source science generally do not provide adequate support
for local data management and usually no support at all for contributions to a national data repository.If data from such diverse sources are to be maintained centrally, researchers and laboratories must haveincentives and support to adopt broader standards in the name of the communityÕs greater good. In this
regard, funding agencies and journals have considerable leverage and through techniques such as requiring
researchers to deposit data in conformance to community standards may be able to provide such incentives.At the same time, data standards cannot resolve the integration problem by themselves even forfuture datasets. One reason is that in some fast-moving and rapidly changing areas of science (such as
biology), it is likely that the data standards existing at any given moment will not cover some new
dimension of data. A novel experiment may make measurements that existing data standards did not
anticipate. (For example, sequence databasesÑby definitionÑdo not integrate methylation data; and yet
methylation is an essential characteristic of DNA that falls outside primary sequence information.) As
knowledge and understanding advance, the meaning attached to a term may change over time.  A second

reason is that standards are difficult to impose on legacy systems, because legacy datasets are usually very
difficult to convert to a new data standard and conversion almost always entails some loss of information.As a result, data standards themselves must evolve as the science they support changes. Becausestandards cannot be propagated instantly throughout the relevant biological community, database A
may be based on Version 12.1 of a standard, and database B on Version 12.4 of the ÒsameÓ standard. It
would be desirable if the differences between Versions 12.1 and 12.4 were not large and a basic level of
integration could still be maintained, but this is not ensured in an environment in which options vary
within standards, different releases and versions of products, and so on. In short, much of the devil of
ensuring data integration is in the detail of implementation.Experience in the database world suggests that standards gaining widespread acceptance in thecommercial marketplace tend to have a long life span, because the marketplace tends to weed out weak
standards before they become widely accepted. Once a standard is widely used, industry is often moti-
vated to maintain compliance with this accepted standard, but standards created by niche players in the
market tend not to survive. This point is of particular relevance in a fragmented research environment and
suggests that standards established by strong consortia of multiple players are more likely to endure.4.2.3  Data Normalization
4An important issue related to data standards is data normalization. Data normalization is the processthrough which data taken on the ÒsameÓ biological phenomenon by different instruments, procedures, or
researchers can be rendered comparable. Such problems can arise in many different contexts:4Section 4.2.3 is based largely on a presentation by C. Ball, ÒThe Normalization of Microarray Data,Ó presented at the AAAS2003 meeting in Denver, Colorado.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS61¥Microarray data related to a given cell may be taken by multiple investigators in different labo-ratories.¥Ecological data (e.g., temperature, reflectivity) in a given ecosystem may be taken by differentinstruments looking at the system.¥Neurological data (e.g., timing and amplitudes of various pulse trains) related to a specificcognitive phenomenon may be taken on different individuals in different laboratories.The simplest example of the normalization problem is when different instruments are calibrateddifferently (e.g., a scale in GeorgeÕs laboratory may not have been zeroed properly, rendering mass
measurements from GeorgeÕs laboratory noncomparable to those from MaryÕs laboratory). If a large
number of readings have been taken with GeorgeÕs scale, one possible fix (i.e., one possible normaliza-
tion) is to determine the extent of the zeroing required and to add or subtract that correction to the
already existing data. Of course, this particular procedure assumes that the necessary zeroing was
constant for each of GeorgeÕs measurements. The procedure is not valid if the zeroing knob was jiggled
accidentally after half of the measurements had been taken.Such biases in the data are systematic. In principle, the steps necessary to deal with systematic biasare straightforward. The researcher must avoid it as much as possible. Because complete avoidance is
not possible, the researcher must recognize it when it occurs and then take steps to correct for it.
Correcting for bias entails determining the magnitude and effect of the bias on data that have been taken
and identifying the source of the bias so that the data already taken can be modified and corrected
appropriately. In some cases, the bias may be uncorrectable, and the data must be discarded.However, in practice, dealing with systematic bias is not nearly so straightforward. Ball notes thatin the real world, the process goes something like this:1.Notice something odd with data.
2.Try a few methods to determine magnitude.

3.Think of many possible sources of bias.

4.Wonder what in the world to do next.
There are many sources of systematic bias, and they differ depending on the nature of the datainvolved. They may include effects due to instrumentation, sample (e.g., sample preparation, sample
choice), or environment (e.g., ambient vibration, current leakage, temperature). Section 3.3 describes a
number of the systematic biases possible in microarray data, as do several references provided by Ball.5There are many ways to correct for systematic bias, depending on the type of data being corrected.In the case of microarray studies, these ways include use of dye swap strategies, replicates and reference
samples, experimental controls, consistent techniques, and sensible array and experiment design. Yet all5BallÕs AAAS presentation includes the following sources: T.B. Kepler, L. Crosby, and K.T. Morgan, ÒNormalization andAnalysis of DNA Microarray Data by Self-consistency and Local Regression,Ó Genome Biololgy 3(7), RESEARCH0037.1- RE-SEARCH0037.12, 2002. Available at http://genomebiology.org/2002/3/7/research/0037.1; R. Hoffmann, T. Seidl, M. Dugas.
ÒProfound Effect of Normalization on Detection of Differentially Expressed Genes in Oligonucleotide Microarray Data Analy-sis,Ó Genome Biolology 3(7):RESEARCH0033.1-RESEARCH0033.1-11. Available at http://genomebiology.com/2002/3/7/re-search/0033; C. Colantuoni, G. Henry, S. Zeger, and J. Pevsner, ÒLocal Mean Normalization of Microarray Element Signal
Intensities Across an Array Surface: Quality Control and Correction of Spatially Systematic Artifacts,Ó Biotechniques 32(6):1316-1320, 2002; B.P. Durbin, J.S. Hardin, D.M. Hawkins, and D.M. Rocke, ÒA Variance-Stabilizing Transformation for Gene-Expres-sion Microarray Data,Ó Bioinformatics 18 (Suppl. 1):S105-S110, 2002; P.H. Tran, D.A. Peiffer, Y. Shin, L.M. Meek, J.P. Brody, andK.W. Cho, ÒMicroarray Optimizations: Increasing Spot Accuracy and Automated Identification of True Microarray Signals,ÓNucleic Acids Research 30(12):e54, 2002, available at http://nar.oupjournals.org/cgi/content/full/30/12/e54; M. Bilban, L.K.Buehler, S. Head, G. Desoye, and V. Quaranta, ÒNormalizing DNA Microarray Data,Ó Current Issues in Molecular Biology 4(2):57-64, 2002; J. Quackenbush, ÒMicroarray Data Normalization and Transformation,Ó Nature Genetics Supplement 32:496-501, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.62CATALYZING INQUIRYof these approaches are labor-intensive, and an outstanding challenge in the area of data normalizationis to develop approaches to minimize systematic bias that demand less labor and expense.4.2.4  Data Warehousing
Data warehousing is a centralized approach to data integration. The maintainer of the data ware-house obtains data from other sources and converts them into a common format, with a global data
schema and indexing system for integration and navigation. Such systems have a long track record of
success in the commercial world, especially for resource management functions (e.g., payroll, inven-
tory). These systems are most successful when the underlying databases can be maintained in a con-
trolled environment that allows them to be reasonably stable and structured. Data warehousing is
dominated by relational database management systems (RDBMS), which offer a mature and widely
accepted database technology and a standard high-level standard query language (SQL).However, biological data are often qualitatively different from the data contained in commercialdatabases. Furthermore, biological data sources are much more dynamic and unpredictable, and few
public biological data sources use structured database management systems. Data warehouses are often
troubled by a lack of synchronization between the data they hold and the original database from which
those data derive because of the time lag involved in refreshing the data warehouse store. Data ware-
housing efforts are further complicated by the issue of updates. Stein writes:6One of the most ambitious attempts at the warehouse approach [to database integration] was the Inte-grated Genome Database (IGD) project, which aimed to combine human sequencing data with the multi-
ple genetic and physical maps that were the main reagent for human genomics at the time. At its peak,IGD integrated more than a dozen source databases, including GenBank, the Genome Database (GDB)and the databases of many human genetic-mapping projects. The integrated database was distributed to
end-users complete with a graphical front end. . . . The IGD project survived for slightly longer than ayear before collapsing. The main reason for its collapse, as described by the principal investigator on theproject (O. Ritter, personal communication, as relayed to Stein), was the database churn issue. On aver-
age, each of the source databases changed its data model twice a year. This meant that the IGD dataimport system broke down every two weeks and the dumping and transformation programs had to berewrittenÑa task that eventually became unmanageable.Also, because of the breadth and volume of biological databases, the effort involved in maintaininga comprehensive data warehouse is enormousÑand likely prohibitive. Such an effort would have to
integrate diverse biological information, such as sequence and structure, up to the various functions of
biochemical pathways and genetic polymorphisms.Still, data warehousing is a useful approach for specific applications that are worth the expense ofintense data cleansing to remove potential errors, duplications, and semantic inconsistency.7 Two cur-rent examples of data warehousing are GenBank and the International Consortium for Brain Mapping
(ICBM) (the latter is described in Box 4.2).4.2.5  Data Federation
The data federation approach to integration is not centralized and does not call for a ÒmasterÓdatabase. Data federation calls for scientists to maintain their own specialized databases encapsulating
their particular areas of expertise and retain control of the primary data, while still making it available
to other researchers. In other words, the underlying data sources are autonomous. Data federation often6Reprinted by permission from L.D. Stein, ÒIntegrating Biological Databases,Ó Nature Reviews Genetics 4(5)337-345, 2003. Copy-right 2005 Macmillan Magazines Ltd.7R. Resnick, ÒSimplified Data Mining,Ó pp. 51-52 in Drug Discovery and Development, 2000. (Cited in Chung and Wooley, 2003.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS63Box 4.2The International Consortium for Brain Mapping (ICBM):A Probabilistic Atlas and Reference System for the Human BrainIn the human population, the brain varies structurally and functionally in currently undefined ways. It is clearthat the size, shape, symmetry, folding pattern, and structural relationships of the systems in the human brainvary from individual to individual. This has been a source of considerable consternation and difficulty in
research and clinical evaluations of the human brain from both the structural and the functional perspective.Current atlases of the human brain do not address this problem. Cytoarchitectural and clinical atlases typicallyuse a single brain or even a single hemisphere as the reference specimen or target brain to which other brains
are matched, typically with simple linear stretching and compressing strategies. In 1992, John Mazziotta andArthur Toga proposed the concept of developing a probabilistic atlas from a large number of normal subjectsbetween the ages of 18 and 90. This data acquisition has now been completed, and the value of such an atlas
is being realized for both research and clinical purposes. The mathematical and software machinery requiredto develop this atlas of normal subjects is now also being applied to patient populations including individualswith AlzheimerÕs disease, schizophrenia, autism, multiple sclerosis, and others.Talairach AtlasTo date, more than 7,000 normal subjects have been entered into the Talairach atlas project and a wide rangeof datasets. These datasets contain detailed demographic histories of the subjects, results of general medicaland neurological examinations, neuropsychiatric and neuropsychological evaluations, quantitative Òhanded-
ness measurementsÓ, and imaging studies. The imaging studies include multispectra 1 mm3 voxel-size mag-netic resonance imaging (MRI) evaluations of the entire brain (T1, T2, and proton density pulse sequences). Asubset of individuals also undergo functional MRI, cerebral blood flow position emission tomography (PET)
and electroencephalogram (EEG) examinations (evoked potentials). Of these subjects, 5,800 individuals havealso had their DNA collected and stored for future genotyping. As such, this database represents the mostcomprehensive evaluation of the structural and functional imaging phenotypes of the human brain in the
normal population across a wide age span and very diverse social, economic, and racial groups. Participatinglaboratories are widely distributed geographically from Asia to Scandinavia, and include eight laboratories, inseven countries, on four continents.World Map of SitesA component of the World Map of Sites project involves the post mortem MRI imaging of individuals whohave willed their bodies to science. Subsequent to MRI imaging, the brain is frozen and sectioned at a reso-lution of approximately 100 microns. Block face images are stored, and the sectioned tissue is stained for
cytoarchitectural, chemoarchitectural, and differential myelin to produce microscopic maps of cellular anat-omy, neuroreceptor or transmitter systems, and white matter tracts. These datasets are then incorporated intoa target brain to which the in vivo brain studies are warped in three dimensions and labeled automatically.
The 7,000 datasets are then placed in the standardized space, and probabilistic estimates of structural bound-aries, volumes, symmetries, and shapes are computed for the entire population or any subpopulation (e.g.,age, gender, race). In the current phase of the program, information is being added about in vivo chemoarchi-
tecture (5-HT2A [5-hydroxytryptamine-2A] in vivo PET receptor imaging), in vivo white matter tracts (MRI-diffusion tensor imaging), vascular anatomy (magnetic resonance angiography and venography), and cerebralconnections (transcranial magnetic stimulation-PET cerebral blood flow measurements).Target BrainThe availability of 342 twin pairs in the dataset (half monozygotic and half dizygotic) along with DNA forgenotyping provides the opportunity to understand structure-function relationships related to genotype and,therefore, provides the first large-scale opportunity to relate phenotype-genotype in behavior across a wide
range of individuals in the human population.continuedCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.64CATALYZING INQUIRYcalls for the use of object-oriented concepts to develop data definitions, encapsulating the internaldetails of the data associated with the heterogeneity of the underlying data sources.8 A change in therepresentation or definition of the data then has minimal impact on the applications that access those
data.An example of a data federation environment is BioMOBY, which is based on two ideas.9 The firstis the notion that databases provide bioinformatics services that can be defined by their inputs and
outputs. (For example, BLAST is a service provided by GenBank that can be defined by its inputÑthat
is, an uncharacterized sequenceÑand by its output, namely, described gene sequences deposited in
GenBank.) The second idea is that all database services would be linked to a central registry (MOBY
Central) of services that users (or their applications) would query. From MOBY Central, a user could
move from one set of input-output services to the nextÑfor example, moving from one database that,
given a sequence (the input), postulates the identity of a gene (the output), and from there to a database
that, given a gene (the input), will find the same gene in multiple organisms (the output), and so on,
picking up information as it moves through database services. There are limitations to the BioMOBY
systemÕs ability to discriminate database services based the descriptions of inputs and outputs, and
MOBY Central must be up and running 24 hours a day.10Box 4.2 ContinuedThe development of similar atlases to evaluate patients with well-defined disease states allows the opportunityto compare the normal brain with brains of patients having cerebral pathological conditions, thereby poten-tially leading to enhanced clinical trials, automated diagnoses, and other clinical applications. Such exampleshave already emerged in patients with multiple sclerosis and epilepsy. An example in AlzheimerÕs disease
relates to a current hotly contested research question. Individuals with AlzheimerÕs disease have a greaterlikelihood of having the genotype ApoE 4 (as opposed to ApoE 2 or 3). Having this genotype, however, isneither sufficient nor required for the development of AlzheimerÕs disease. Individuals with AlzheimerÕs dis-
ease also have small hippocampi, presumably because of atrophy of this structure as the disease progresses.The question of interest is whether individuals with the high-risk genotype (ApoE 4) have small hippocampi tobegin with. This would be a very difficult hypothesis to test without the dataset described above. With the
ICBM database, it is possible to study individuals from, for example, ages 20 to 40 and identify those with thesmallest (lowest 5 percent) and largest (highest 5 percent) hippocampal volumes. This relatively small numberof subjects could then be genotyped for ApoE alleles. If individuals with small hippocampi all had the geno-
type ApoE 4 and those with large hippocampi all had the genotype ApoE 2 or 3, this would be strong supportfor the hypothesis that individuals with the high-risk genotype for the development of AlzheimerÕs diseasehave small hippocampi based on genetic criteria as a prelude to the development of AlzheimerÕs disease.
Similar genotype-imaging phenotype evaluations could be undertaken across a wide range of human condi-tions, genotypes, and brain structures.SOURCE: Modified from John C. Mazziotta and Arthur W. Toga, Department of Neurology, David Geffen School of Medicine, Universityof California, Los Angeles, personal communication to John Wooley, February 22, 2004.8R.G.G. Cattell, Object Data Management: Object-Oriented and Extended Relational Database Systems, revised edition, Addison-Wiley, Reading, MA, 1994. (Cited in Chung and Wooley, 2003.)9M.D. Wilkinson and M. Links, ÒBioMOBY: An Open-Source Biological Web Services Proposal,Ó Briefings In Bioinformatics3(4):331-341, 2002.10L.D. Stein, ÒIntegrating Biological Databases,Ó Nature Reviews Genetics 4(5):337-345, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS654.2.6  Data Mediators/Middleware
In the middleware approach, an intermediate processing layer (a ÒmediatorÓ) decouples the under-lying heterogeneous, distributed data sources and the client layer of end users and applications.11 Themediator layer (i.e., the middleware) performs the core functions of data transformation and integra-
tion, and communicates with the database ÒwrappersÓ and the user application layer. (A ÒwrapperÓ is
a software component associated with an underlying data source that is generally used to handle the
tasks of access to specified data sources, extraction and retrieval of selected data, and translation of
source data formats into a common data model designed for the integration system.)The common model for data derived from the underlying data sources is the responsibility of themediator. This model must be sufficiently rich to accommodate various data formats of existing biologi-
cal data sources, which may include unstructured text files, semistructured XML and HTML files, and
structured relational, object-oriented, and nested complex data models. In addition, the internal data
model must facilitate the structuring of integrated biological objects to present to the user application
layer. Finally, the mediator also provides services such as filtering, managing metadata, and resolving
semantic inconsistency in source databases.There are many flavors of mediator approaches in life science domains. IBMÕs DiscoveryLink for thelife sciences is one of the best known.12 The Kleisli system provides an internal nested complex datamodel and a high-power query and transformation language for data integration.13 K2 shares manydesign principles with Kleisli in supporting a complex data model, but adopts more object-oriented
features.14 OPM supports a rich object model and a global schema for data integration.15 TAMBISprovides a global ontology (see Section 4.2.8 on ontologies) to facilitate queries across multiple data
sources.16 TSIMMIS is a mediation system for information integration with its own data model (Object-Exchange Model, OEM) and query language.174.2.7  Databases as Models
A natural progression for databases established to meet the needs and interests of specializedcommunities, such as research on cell signaling pathways or programmed cell death, is the evolution of11G. Wiederhold, ÒMediators in the Architecture of Future Information Systems,Ó IEEE Computer 25(3):38-49, 1992; G.Wiederhold and M. Genesereth, ÒThe Conceptual Basis for Mediation Services,Ó IEEE Expert, Intelligent Systems and Their Applica-tions 12(5):38-47, 1997. (Both cited in Chung and Wooley, 2003.)12L.M. Haas et al., ÒDiscoveryLink: A System for Integrated access to Life Sciences Data Sources,Ó IBM Systems Journal 40(2):489-511, 2001.13S. Davidson, C. Overton, V. Tannen, and L. Wong, ÒBioKleisli: A Digital Library for Biomedical Researchers,Ó InternationalJournal of Digital Libraries 1(1):36-53, 1997; L. Wong, ÒKleisli, a Functional Query System,Ó Journal of Functional Programming10(1):19-56, 2000. (Both cited in Chung and Wooley, 2003.)14J. Crabtree, S. Harker, and V. Tannen, ÒThe Information Integration System K2,Ó available at http://db.cis.upenn.edu/K2/K2.doc; S.B. Davidson, J. Crabtree, B.P. Brunk, J. Schug, V. Tannen, G.C. Overton, and C.J. Stoeckert, Jr., ÒK2/Kleisli and GUS:Experiments in Integrated Access to Genomic Data Sources,Ó IBM Systems Journal 40(2):489-511, 2001. (Both cited in Chung andWooley, 2003.)15I-M.A. Chen and V.M. Markowitz, ÒAn Overview of the Object-Protocol Model (OPM) and OPM Data Management Tools,ÓInformation Systems 20(5):393-418, 1995; I-M.A. Chen, A.S. Kosky, V.M. Markowitz, and E. Szeto, ÒConstructing and MaintainingScientific Database Views in the Framework of the Object-Protocol Model,Ó Proceedings of the Ninth International Conference onScientific and Statistical Database Management, Institute of Electrical and Electronic Engineers, Inc., New York, 1997, pp. 237Ð248.(Cited in Chung and Wooley, 2003.)16N.W. Paton, R. Stevens, P. Baker, C.A. Goble, S. Bechhofer, and A. Brass, ÒQuery Processing in the TAMBIS BioinformaticsSource Integration System,Ó Proceedings of the 11th International Conference on Scientific and Statistical Database Management, IEEE,New York 1999, pp. 138-147; R. Stevens, P. Baker, S. Bechhofer, G. Ng, A. Jacoby, N.W. Paton, C.A. Goble, and A. Brass,ÒTAMBIS: Transparent Access to Multiple Bioinformatics Information Sources,Ó Bioinformatics 16(2):184-186, 2000. (Both cited inChung and Wooley, 2003.)17Y. Papakonstantinou, H. Garcia-Molina, and J. Widom, ÒObject Exchange Across Heterogeneous Information Sources,ÓProceedings of the IEEE Conference on Data Engineering, IEEE, New York, 1995, pp. 251-260. (Cited in Chung and Wooley, 2003.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.66CATALYZING INQUIRYdatabases into models of biological activity. As databases become increasingly annotated with func-tional and other information, they lay the groundwork for model formation.In the future, such Òdatabase modelsÓ are envisioned as the basis of informed predictions anddecision making in biomedicine. For example, physicians of the future may use biological information
systems (BISs) that apply known interactions and causal relationships among proteins that regulate cell
division to changes in an individualÕs DNA sequence, gene expression, and proteins in an individual
tumor.18 The physician might use this information together with the BIS to support a decision onwhether the inhibition of a particular protein kinase is likely to be useful for treating that particular
tumor.Indeed, a major goal in the for-profit sector is to create richly annotated databases that can serve astestbeds for modeling pharmaceutical applications. For example, Entelos has developed PhysioLab, a
computer model system consisting of a large set (more than 1,000) of ordinary nonlinear differential
equations.19 The model is a functional representation of human pathophysiology based on currentgenomic, proteomic, in vitro, in vivo, and ex vivo data, built using a top-down, disease-specific systems
approach that relates clinical outcomes to human biology and physiology. Starting with major organ
systems, virtual patients are explicit mathematical representations of a particular phenotype, based on
known or hypothesized factors (genetic, life-style, environmental). Each model simulates up to 60
separate responses previously demonstrated in human clinical studies.In the neuroscience field, Bower and colleagues have developed the ModelerÕs Workspace,20 whichis based on a notion that electronic databases must provide enhanced functionality over traditional
means of distributing information if they are to be fully successful. In particular, Bower et al. believe
that computational models are an inherently more powerful medium for the electronic storage and
retrieval of information than are traditional online databases.The ModelerÕs Workspace is thus designed to enable researchers to search multiple remote data-bases for model components based on various criteria; visualize the characteristics of the components
retrieved; create new components, either from scratch or derived from existing models; combine com-
ponents into new models; link models to experimental data as well as online publications; and interact
with simulation packages such as GENESIS to simulate the new constructs.The tools contained in the Workspace enable researchers to work with structurally realistic biologi-cal models, that is, models that seek to capture what is known about the anatomical structure and
physiological characteristics of a neural system of interest. Because they are faithful to biological
anatomy and physiology, structurally realistic models are a means of storing anatomical and physi-
ological experimental information.For example, to model a part of the brain, this modeling approach starts with a detailed descriptionof the relevant neuroanatomy, such as a description of the three-dimensional structure of the neuron
and its dendritic tree. At the single-cell level, the model represents information about neuronal mor-
phology, including such parameters as soma size, length of interbranch segments, diameter of branches,
bifurcation probabilities, and density and size of dendritic spines. At the neuronal network level, the
model represents the cell types found in the network and the connectivity among them. The model must
also incorporate information regarding the basic physiological behavior of the modeled structureÑfor
example, by tuning the model to replicate neuronal responses to experimentally derived data.With such a framework in place, a structural model organizes data in ways that make manifestlyobvious how those data are related to neural function. By contrast, for many other kinds of databases it
is not at all obvious how the data contained therein contribute to an understanding of function. Bower18R. Brent and D. Endy, ÒModelling Cellular Behaviour,Ó Nature 409:391-395, 2001.19See, for example, http://www.entelos.com/science/physiolabtech.html.20M. Hucka, K. Shankar, D. Beeman, and J.M. Bower, ÒThe ModelerÕs Workspace: Making Model-Based Studies of the NervousSystem More Accessible,Ó Computational Neuroanatomy: Principles and Methods, G.A. Ascoli, ed., Humana Press, Totowa, NJ, 2002,pp. 83-103.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS67and colleagues argue that Òas models become more sophisticated, so does the representation of the data.As models become more capable, they extend our ability to explore the functional significance of the
structure and organization of biological systems.Ó214.2.8  Ontologies
Variations in language and terminology have always posed a great challenge to large-scale, com-prehensive integration of biological findings. In part, this is due to the fact that scientists operate, with
a data- and experience-driven intuition that outstrips the ability of language to describe. As early as
1952, this problem was recognized:Geneticists, like all good scientists, proceed in the first instance intuitively and . . . their intuition hasvastly outstripped the possibilities of expression in the ordinary usages of natural languages. They know
what they mean, but the current linguistic apparatus makes it very difficult for them to say what theymean. This apparatus conceals the complexity of the intuitions. It is part of the business of geneticalmethodology first to discover what geneticists mean and then to devise the simplest method of saying
what they mean. If the result proves to be more complex than one would expect from the current exposi-tions, that is because these devices are succeeding in making apparent a real complexity in the subjectmatter which the natural language conceals.22In addition, different biologists use language with different levels of precision for different pur-poses. For instance, the notion of ÒidentityÓ is different depending on context.23 Two geneticists maylook at a map of human chromosome 21. A year later, they both want to look at the same map again. But
to one of them, ÒsameÓ means exactly the same map (same data, bit for bit); to the other, it means the
current map of the same biological object, even if all of the data in that map have changed. To a protein
chemist, two molecules of beta-hemoglobin are the same because they are composed of exactly the same
sequence of amino acids. To a biologist, the same two molecules might be considered different because
one was isolated from a chimpanzee and the other from a human.To deal with such context-sensitive problems, bioinformaticians have turned to ontologies. Anontology is a description of concepts and relationships that exist among the concepts for a particular
domain of knowledge.24  Ontologies in the life sciences serve two equally important functions. First,they provide controlled, hierarchically structured vocabularies for terminology that can be used to
describe biological objects. Second, they specify object classes, relations, and functions in ways that
capture the main concepts of and relationships in a research area.4.2.8.1Ontologies for Common Terminology and Descriptions
To associate concepts with the individual names of objects in databases, an ontology tool mightincorporate a terminology database that interprets queries and translates them into search terms consis-
tent with each of the underlying sources. More recently, ontology-based designs have evolved from
static dictionaries into dynamic systems that can be extended with new terms and concepts without
modification to the underlying database.21M. Hucka, K. Shankar, D. Beeman, and J.M. Bower, ÒThe ModelerÕs Workspace,Ó 2002.22J.H. Woodger, Biology and Language, Cambridge University Press, Cambridge, UK, 1952.23R.J. Robbins, ÒObject Identity and Life Science Research,Ó position paper submitted for the Semantic Web for Life SciencesWorkshop, October 27-28 2004, Cambridge, MA, available at http://lists.w3.org/Archives/Public/public-swls-ws/2004Sep/att-0050/position-01.pdf.24The term ÒontologyÓ is a philosophical term referring to the subject of existence. The computer science community borrowedthe term to refer to Òspecification of a conceptualizationÓ for knowledge sharing in artificial intelligence. See, for example, T.R.Gruber, ÒA Translation Approach to Portable Ontology Specification,Ó Knowledge Acquisition 5(2):199-220, 1993. (Cited in Chungand Wooley, 2003.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.68CATALYZING INQUIRYA feature of ontologies that facilitates the integration of databases is the use of a hierarchicalstructure that is progressively specialized; that is, specific terms are defined as specialized forms of
general terms. Two different databases might not extend their annotation of a biological object to the
same level of specificity, but the databases can be integrated by finding the levels within the hierarchy
that share a common term.The naming dimension of ontologies has been common to research in the life sciences for much ofits history, although the term itself has not been widely used. Chung and Wooley note the following, for
example:¥The Linnaean system for naming of species and organisms in taxonomy is one of the oldestontologies.¥The nomenclature committee for the International Union of Pure and Applied Chemistry (IUPAC)and the International Union of Biochemistry and Molecular Biology (IUBMB) make recommendations
on organic, biochemical, and molecular biology nomenclature, symbols, and terminology.¥The National Library of Medicine Medical Subject Headings (MeSH) provides the most compre-hensive controlled vocabularies for biomedical literature and clinical records.¥A division of the College of American Pathologists oversees the development and maintenanceof a comprehensive and controlled terminology for medicine and clinical information known as
SNOMED (Systematized Nomenclature of Medicine).¥The Gene Ontology Consortium25 seeks to create an ontology to unify work across many ge-nomic projectsÑto develop controlled vocabulary and relationships for gene sequences, anatomy, physi-
cal characteristics, and pathology across the mouse, yeast, and fly genomes.26 The consortiumÕs initialefforts focus on ontologies for molecular function, biological process, and cellular components of gene
products across organisms and are intended to overcome the problems associated with inconsistent
terminology and descriptions for the same biological phenomena and relationships.Perhaps the most negative aspect of ontologies is that they are in essence standards, and hence takea long time to developÑand as the size of the relevant community affected by the ontology increases, so
does development time. For example, the ecological and biodiversity communities have made substan-
tial progress in metadata standards, common taxonomy, and structural vocabulary with the help of
National Science Foundation and other government agencies.27 By contrast, the molecular biologycommunity is much more diverse, and reaching a community-wide consensus has been much harder.An alternative to seeking community-wide consensus is to seek consensus in smaller subcommuni-ties associated with specific areas of research such as sequence analysis, gene expression, protein path-
ways, and so on.28 These efforts usually adopt a use-case and open-source approach for communityinput. The ontologies are not meant to be mandatory, but instead to serve as a reference framework
from which further development can proceed.25See www.geneontology.org.26M. Ashburner, C.A. Ball, J.A. Blacke, D. Botstein, H. Butler, J.M. Cherry, A.P. Davis, et al., ÒGene Ontology: Tool for theUnification of Biology,Ó Nature Genetics 25(1):25Ð29, 2000. (Cited in Chung and Wooley, 2003.)27J.L. Edwards, M.A. Lane, and E.S. Nielsen, ÒInteroperability of Biodiversity Databases: Biodiversity Information on EveryDesk,Ó Science 289(5488):2312-2314, 2000; National Biological Information Infrastructure (NBII), available at http://www.nbii.gov/disciplines/systematics.html; Federal Geographic Data Committee (FGDC), available at http://www.fgdc.gov/.
(All cited in Chung and Wooley, 2003.)28Gene Expression Ontology Working Group, see http://www.mged.org/; P.D. Karp, M. Riley, S.M. Paley, and A. Pellegrini-Toole, ÒThe MetaCyc Database,Ó Nucleic Acids Research 30(1):59-61, 2002; P.D. Karp, M. Riley, M. Saier, I.T. Paulsen, J. Collado-Vides, S.M. Paley, A. Pellegrini-Toole, et al., ÒThe EcoCyc Database,Ó Nucleic Acids Research 30(1):56-58, 2002; D.E. Oliver, D.L.Rubin, J.M. Stuart, M. Hewett, T.E. Klein, and R.B. Altman, ÒOntology Development for a Pharmacogenetics Knowledge Base,ÓPacific Symposium on Biocomputing 65-76, 2002. (All cited in Chung and Wooley, 2003.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS69An ontology developed by one subcommunity inevitably leads to interactions with related ontolo-gies and the need to integrate. For example, consider the concept of homology. In traditional evolution-
ary biology, ÒanalogyÓ is used to describe things that are identical by function and ÒhomologyÓ is used
to identify things that are identical by descent. However, in considering DNA, function and descent are
both captured in the DNA sequence, and therefore to molecular biologists, homology has come to mean
simply similarity in sequence, regardless of whether this is due to convergence or ancestry. Thus, the
term ÒhomologousÓ means different things in molecular biology and evolutionary biology.29  Morebroadly, a brain ontology will inevitably relate to ontologies of other anatomic structures or at the
molecular level sharing ontologies for genes and proteins.30Difficulties of integrating diverse but related databases thus are transformed into analogous diffi-culties in integrating diverse but related ontologies, but since each ontology represents the integration
of multiple databases relevant to the field, the integration effort at the higher level is more encompass-
ing. At the same time, it is also more difficult, because the implications of changes in fundamental
conceptsÑwhich will be necessary in any integration effortÑare much more far-reaching than analo-
gous changes in a database. That is, design compromises in the development of individual ontologies
might make it impossible to integrate the ontologies without changes to some of their basic components.
This would require undoing the ontologies, then redoing them to support integration.These points relate to semantic interoperability, which is an active area of research in computerscience.31 Information integration across multiple biological disciplines and subdisciplines would de-pend on the close collaborations of domain experts and information technology professionals to de-
velop algorithms and flexible approaches to bridge the gaps between multiple biological ontologies. In
recent years, a number of life science researchers have come to believe in the potential of the Semantic
Web for integrating biological ontologies, as described in Box 4.3.A sample collection of ontology resources for controlled vocabulary purposes in the life sciences islisted in Table 4.1.4.2.8.2Ontologies for Automated Reasoning
Today, it is standard practice to store biological data in databases; no one would deny that thevolume of available data is far beyond the capabilities of human memory or written text. However, even
as the volume of analytic and theoretical results drawn from these data (such as inferred genetic
regulatory, metabolic, and signaling network relationships) grows, it will become necessary to store
such information as well in a format suitable for computational access.The essential rationale underlying automated reasoning is that reasoning oneÕs way through all ofthe complexity inherent in biological organisms is very difficult, and indeed may be, for all practical
purposes, impossible for the knowledge bases that are required to characterize even the simplest organ-
isms. Consider, for example, the networks related to genetic regulation, metabolism, and signaling of an
organism such as Escherichia coli. These networks are too large for humans to reason about in theirtotality, which means that it is increasingly difficult for scientists to be certain about global network
properties. Is the model complete? Is it consistent? Does it explain all of the data? For example, the
database of known molecular pathways in E. coli contains many hundreds of connections, far more thanmost researchers could remember, much less reason about.29For more on the homology issue, see W.M. Fitch, ÒHomology: A Personal View on Some of the Problems,Ó Trends in Genetics16(5):227-231, 2000.30A. Gupta, B. Lud−scher, and M.E. Martone, ÒKnowledge-Based Integration of Neuroscience Data SourcesÓ Conference onScientific and Statistical Database Management, Berlin, IEEE Computer Society, July 2000. (Cited in Chung and Wooley, 2003.)31P. Mitra, G. Wiederhold, and M. Kersten, ÒA Graph-oriented Model for Articulation of Ontology Interdependencies,Ó Pro-ceedings of Conference on Extending Database Technology Konstanz, Germany, March 2000. (Cited in Chung and Wooley, 2003.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.70CATALYZING INQUIRYBox 4.3Biological Data and the Semantic WebThe Semantic Web seeks to create a universal medium for the exchange of machine-understandabledata of all types, including biological data. Using Semantic Web technology, programs can share andprocess data even when they have been designed totally independently. The semantic web involves a
Resource Description Framework (RDF), an RDF Schema language, and the Web Ontology language(OWL). RDF and OWL are Semantic Web standards that provide a framework for asset management,enterprise integration and the sharing and reuse of data on the Web. Furthermore, a standardized query
language for RDF enables the ÒjoiningÓ of decentralized collections of RDF data. The underlying tech-nology foundation of these languages is that of URLs, XML, and XML name spaces.Within the life sciences, the notion of a life sciences identifier (LSID) is intended to provide a straight-forward approach to naming and identifying data resources stored in multiple, distributed data stores ina manner that overcomes the limitations of naming schemes in use today. LSIDs are persistent, location-
independent, resource identifiers for uniquely naming biologically significant resources including butnot limited to individual genes or proteins, or data objects that encode information about them.The life sciences pose a particular challenge for data integration because the semantics of biologicalknowledge are constantly changing. For example, it may be known that two proteins bind to each other.But this fact could be represented at the cellular level, the tissue level, and the molecular level depend-
ing on the context in which that fact was important.The Semantic Web is intended to allow for evolutionary change in the relevant ontologies as newscience emerges without the need for consensus. For example, if Researcher A states (and encodesusing Semantic Web technology) a relationship between a protein and a signaling cascade withwhich Researcher B disagrees, Researcher B can instruct his or her computer to ignore (perhaps
temporarily) the relationship encoded by Researcher A in favor (perhaps) of a relationship that isdefined only locally.An initiative coordinated by the World Wide Web Consortium seeks to explore how Semantic Webtechnologies can be used to reduce the barriers and costs associated with effective data integration,analysis, and collaboration in the life sciences research community, to enable disease understanding,
and to accelerate the development of therapies. A meeting in October 2004 on the Semantic Web andthe life sciences concluded that work was needed in two high-priority areas.¥In the area of ontology development, collaborative efforts were felt required to define core vocabu-laries that can bridge data and ontologies developed by individual communities of practice. Thesevocabularies would address provenance and context (e.g., identifying data sources, authors, publica-
tions names, and collection conditions), terms for cross-references in publication and other reporting ofexperimental results, navigation, versioning, and geospatial/temporal quantifiers.¥With respect to LSIDs, the problem of sparse implementation was regarded as central, and partici-pants believed that work should focus on how to implement LSIDs in a manner that leverages existingWeb resource resolution mechanisms such as http servers.SOURCES: The Semantic Web Activity Statement, available at http://www.w3.org/2001/sw/Activity; Life Sciences Identifiers RFPResponse, OMG Document lifesci/2003-12-02, January 12, 2004, available at http://www.omg.org/docs/lifesci/03-12-02.doc#_Toc61702471; John Wilbanks, Science Commons, Massachusetts Institute of Technology, personal communication, April
4, 2005.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS71TABLE 4.1Biological Ontology Resources
OrganizationDescriptions
Human Genome Organization (HUGO) GeneHGNC is responsible for the approval of a unique symbol
Nomenclature Committee (HGNC):for each gene and designate description of genes. Aliases
http://www.gene.ucl.ac.uk/nomenclature/for genes are also listed in the database.
Gene Ontology Consortium (GO):The purpose of GO is to develop ontologies describing the
http://www.geneontology.orgmolecular function, biological process, and cellular
component of genes and gene products for eukaryotes.
Members include genome databases of fly, yeast, mouse,worm, and Arabidopsis.Plant Ontology Consortium:This consortium will produce structured, controlled
http://www.plantontology.orgvocabularies applied to plant-based database information.
Microarrey Gene Expression Data (MGED)The MGED group facilitates the adoption of standards for
Society Ontology Working Group:DNA-microarray experiment annotation and data
http://www.mged.org/representation, as well as the introduction of standard
expertmental controls and data normalization methods.NIBII (National Biological InformationNBII provides links to taxonomy sites for all biological
Infrastructure):disciplines.
http://www.nbii.gov/disciplines/systematics.htmlITIS (Integrated Taxonomic Information System):ITIS provides taxonomic information on plants, animals,
http://www.itis.usda.gov/and microbes of North America and the world.
MeSH (Medical Subject Headings):MeSH is a controlled vocabulary established by the
http://www.nlm.nih.gov/mesh/National Library of Medicine (NLM) and used for indexing
meshhome.htmlarticles, cataloging books and 
other holdings, and searchingMeSH-indexed databases, including MEDLINE.SNOMED (Systematized Nomenclature ofSNOMED is recognized globally as a comprehensive,
Medicine):multiaxial, 
controlled terminology created for the indexinghttp://www.snomed.org/of the entire medical record.
International Classification of Diseases,ICD-9-CM is the official system of assigning codes to
Ninth Revision, Clinical Modificationdiagnoses and procedures associated with hospital
(ICD-9-CM):utilization in the 
United States. It is published by the U.S.http://www.cdc.gov/nchs/about/National Center for Health Statistics.
otheract/lcd9/abtlcd9.htmInternational Union of Pure and AppliedIUPAC and IUBMB make recommendations on organic,
Chemistry (IUPAQ)biochemical, and molecular biology nomenclature,
symbols, and terminology.International Union of Biochemistry andMolecular Biology (IUBMB) Nomenclature
Committee:http://www.chem.q-mul.ac.uk/iubmb/PharmGKB ( Pharmacogenetics Knowledge Base:PharmGKB, develops ontologies for pharmacogenetics and
http://pharmgkb.org/pharmacogenomics.
continuedCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.72CATALYZING INQUIRYBy representing working hypotheses, derived results, and the evidence that supports and refutesthem in machine-readable representations, researchers can uncover correlations in and make inferences
about independently conducted investigations of complex biological systems that would otherwise
remain undiscovered by relying simply on serendipity or their own reasoning and memory capaci-
ties.32 In principle, software can read and operate on these representations, determining properties in a
way similar to human reasoning, but able to consider hundreds or thousands of elements simulta-
neously. Although automated reasoning can potentially predict the response of a biological system to a
particular stimulus, it is particularly useful for discovering inconsistencies or missing relations in the
data, establishing global properties of networks, discovering predictive relationships between elements,
and inferring or calculating the consequences of given causal relationships.33 As the number of discov-ered pathways and molecular networks increases and the questions of interest to researchers become
more about global properties of organisms, automated reasoning will become increasingly useful.Symbolic representations of biological knowledgeÑontologiesÑare a foundation for such efforts.Ontologies contain names and relationships of the many objects considered by a theory, such as genes,
enzymes, proteins, transcription, and so forth. By storing such an ontology in a symbolic machine-TABLE 4.1Continued
OrganizationDescriptions
mmCEF (Macromolecular CrystallographicThe information file mmCEF is sponsored by IUCr
Information File):(International Union of Crystallography) to provide a
http://pdb.rutgers.edu/mmcif/dictionary for data items relevant to macromolecular
http://www.iucr.ac.ukliucr-top/cif/index.htmlcrystallographic experiments.
LocusLink:LocusLink contains gene-centered resources, including
http://www.ncbi.nlm.nih.gov/LocusLink/nomenclature and aliases for genes.
Prot”g”-2000:Prot”g”-2000 is a tool that allows the user to construct a
http://protege.stanford.edudomain ontology that can be extended to access embedded
applications in other knowledge-based systems. A numberof biomedical ontologies have been constructed with thissystem, but it can be applied to other domains as well.TAMBIS:TAMBIS aims to aid researchers in the biological sciences
http://imgproj.cs.man.ac.uk/tambis/by providing a single access point for biological
information sources around the world. The access point willbe a single Web-based interface that acts as a singleinformation source. It will find appropriate sources of
information for user queries and phrase the user questionsfor each source, returning the results in a consistent mannerwhich will include details of the information source.32L. Hunter, ÒOntologies for Programs, Not People,Ó Genome Biology 3(6):1002.1-1002.2, 2002.33As shown in Chapter 5, simulations are also useful for predicting the response of a biological system to various stimuli. Butsimulations instantiate procedural knowledge (i.e., how to do something), whereas the automated reasoning systems discussedhere operate on declarative knowledge (i.e., knowledge about something). Simulations are optimized to answer a set of questionsthat is narrower than those that can be answered by automated reasoning systemsÑnamely, predictions about the subsequent
response of a system to a given stimulus. Automated reasoning systems can also answer such questions (though more slowly),but in addition they can answer questions such as, What part of a network is responsible for this particular response?, presumingthat such (declarative) knowledge is available in the database on which the systems operate.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS73readable form and making use of databases of biological data and inferred networks, software based onartificial intelligence research can make complex inferences using these encoded relationships, for ex-
ample, to consider statements written in that ontology for consistency or to predict new relationships
between elements.34  Such new relationships might include new metabolic pathways, regulatory rela-tionships between genes, signaling networks, or other relationships. Other approaches rely on logical
frameworks more expressive than database queries and are able to reason about explanations for a
given feature or suggest plans for intervention to reach a desired state.35Developing an ontology for automated reasoning can make use of many different sources. Forexample, inference from gene-expression data using Bayesian networks can take advantage of online
sources of information about the likely probabilistic dependencies among expression levels of various
genes.36 Machine-readable knowledge bases can be built from textbooks, review articles, or even theOxford Dictionary of Molecular Biology. The rapidly growing volume of publications in the biologicalliterature is another important source, because inclusion of the knowledge in these publications helps to
uncover relationships among various genes, proteins, and other biological entities referenced in the
literature.An example of ontologies for automated reasoning is the ontology underlying the EcoCyc database.The EcoCyc Pathway Database (http://ecocyc.org) describes the metabolic transport, and genetic regu-
latory networks of E. coli. EcoCyc structures a scientific theory about E. coli within a formal ontology sothat the theory is available for computational analysis.37 Specifically, EcoCyc describes the genes andproteins of E. coli as well as its metabolic pathways, transport functions, and gene regulation. Theunderlying ontology encodes a diverse array of biochemical processes, including enzymatic reactions
involving small molecule substrates and macromolecular substrates, signal transduction processes,
transport events, and mechanisms of regulation of gene expression.384.2.9  Annotations and Metadata
Annotation is auxiliary information associated with primary information contained in a database.Consider, for example, the human genome database. The primary database consists of a sequence of
some 3 billion nucleotides, which contains genes, regulatory elements, and other material whose func-
tion is unknown. To make sense of this enormous sequence, the identification of significant patterns
within it is necessary. Various pieces of the genome must be identified, and a given sequence might be
annotated as translation (e.g., ÒstopÓ), transcription (e.g., ÒexonÓ or ÒintronÓ), variation (ÒinsertionÓ),
structural (ÒcloneÓ), similarity, repeat, or experimental (e.g., Òknockout,Ó ÒtransgenicÓ). Identifying a
particular nucleotide sequence as a gene would itself be an annotation, and the protein corresponding
to it, including its three-dimensional structure characterized as a set of coordinates of the proteinÕs
atoms, would also be an annotation. In short, the sequence database includes the raw sequence data,
and the annotated version adds pertinent information such as gene coded for, amino acid sequence, or
other commentary to the database entry of raw sequence of DNA bases.3934P.D. Karp, ÒPathway Databases: A Case Study in Computational Symbolic Theories,Ó Science 293(5537):2040-2044, 2001.35C. Baral, K. Chancellor, N. Tran, N.L. Tran, A. Joy, and M. Berens, ÒA Knowledge Based Approach for Representing andReasoning About Signaling Networks,Ó Bioinformatics 20(Suppl. 1):I15-I22, 2004.36E. Segal, B. Taskar, A. Gasch, N. Friedman, and D. Koller, ÒRich Probabilistic Models for Gene Expression,Ó Bioinformatics17(Supp. 1):S243-S252, 2001. (Cited in Hunter, ÒOntologies for Programs, Not People,Ó 2002, Footnote 32.)37P.D. Karp, ÒPathway Databases: A Case Study in Computational Symbolic Theories,Ó Science 293(5537):2040-2044, 2001; P.D.Karp, M. Riley, M. Saier, I.T. Paulsen, J. Collado-Vides, S.M. Paley, A Pellegrini-Toole, et al., ÒThe EcoCyc Database,Ó NucleicAcids Research 30(1):56-58, 2002.38P.D. Karp, ÒAn Ontology for Biological Function Based on Molecular Interactions,Ó Bioinformatics 16(3):269Ð285, 2000.39See http://www.biochem.northwestern.edu/holmgren/Glossary/Definitions/Def-A/Annotation.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.74CATALYZING INQUIRYAlthough the genomic research community uses annotation to refer to auxiliary information thathas biological function or significance, annotation could also be used as a way to trace the provenance
of data (discussed in greater detail in Section 3.7). For example, in a protein database, the utility of an
entry describing the three-dimensional structure of a protein would be greatly enhanced if entries also
included annotations that described the quality of data (e.g., their precision), uncertainties in the data,
the physical and chemical properties of the protein, various kinds of functional information (e.g., what
molecules bind to the protein, location of the active site), contextual information such as where in a cell
the protein is found and in what concentration, and appropriate references to the literature.In principle, annotations can often be captured as unstructured natural language text. But formaximum utility, machine-readable annotations are necessary. Thus, special attention must be paid to
the design and creation of languages and formats that facilitate machine processing of annotations. To
facilitate such processing, a variety of metadata tools are available. MetadataÑor literally Òdata about
dataÓÑare anything that describes data elements or data collections, such as the labels of the fields, the
units used, the time the data were collected, the size of the collection, and so forth. They are invaluable
not only for increasing the life span of data (by making it easier or even possible to determine the
meaning of a particular measurement), but also for making datasets comprehensible to computers. The
National Biological Information Infrastructure (NBII)40 offers the following description:Metadata records preserve the usefulness of data over time by detailing methods for data collection anddata set creation. Metadata greatly minimize duplication of effort in the collection of expensive digital
data and foster sharing of digital data resources. Metadata supports local data asset management such aslocal inventory and data catalogs, and external user communities such as Clearinghouses and websites. Itprovides adequate guidance for end-use application of data such as detailed lineage and context. Metada-
ta makes it possible for data users to search, retrieve, and evaluate data set information from the NBIIÕsvast network of biological databases by providing standardized descriptions of geospatial and biologicaldata.A popular tool for the implementation of controlled metadata vocabularies is the extensible markuplanguage (XML).41 XML offers a way to serve and describe data in a uniform and automatically parsable
format and provides an open-source solution for moving data between programs. Although XML is a
language for describing data, the descriptions of data are articulated in XML-based vocabularies.Such vocabularies are useful for describing specific biological entities along with experimentalinformation associated with those entities. Some of the vocabularies have been developed in association
with specialized databases established by the community. Because of their common basis in XML,
however, one vocabulary can be translated to another using various tools, for example, the XML style
sheet language transformation, or XSLT.42Examples of such XML-based dialects include the BIOpolymer Markup Language (BIOML),43 de-signed for annotating the sequences of biopolymers (e.g., genes, proteins), in such a way that all infor-
mation about a biopolymer can be logically and meaningfully associated with it. Much like HTML, the
language uses tags such as <protein>, <subunit>, and <peptide> to describe elements of a biopolymer
along with a series of attributes.The Microarray Markup Language (MAML) was created by a coalition of developers(www.beahmish.lbl.gov) to meet community needs for sharing and comparing the results of gene
expression experiments. That community proposed the creation of a Microarray Gene Expression Data-
base and defined the minimum information about a microarray experiment (MIAME) needed to enable40See http://www.nbii.gov/datainfo/metadata/.41H. Simon, Modern Drug Discovery, American Chemical Society, Washington, DC, 2001, pp. 69-71.42See http://www.w3c./TR/xslt.43See http://www.bioml.com/BIOML.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS75sharing. Consistent with the MIAME standards proposed by microarray users, MAML can be used todescribe experiments and results from all types of DNA arrays.The Systems Biology Markup Language, (SBML) is used to represent and model information insystems simulation software, so that models of biological systems can be exchanged by different soft-
ware programs (e.g., E-Cell, StochSim). The SBML language, developed by the Caltech ERATO Kiranto
systems biology Project,44 is organized around five categories of information: model, compartment,geometry, specie, and reaction.A downside of XML is that only a few of the largest and most used databases (e.g., a GenBank)support an XML interface. Other databases whose existence predates XML keep most of their data in
flat files. But this reality is changing, and database researchers are working to create conversion tools
and new database platforms based on XML. Additional XML-based vocabularies and translation tools
are needed.The data annotation process is complex and cumbersome when large datasets are involved, andsome efforts have been made to reduce the burden of annotation. For example, the Distributed Annota-
tion System (DAS) is a Web service for exchanging genome annotation data from a number of distrib-
uted databases. The system depends on the existence of a Òreference sequenceÓ and gathers ÒlayersÓ of
annotation about the sequence that reside on third-party servers and are controlled by each annotation
provider. The data exchange standard (the DAS XML specification) enables layers to be provided in real
time from the third-party servers and overlaid to produce a single integrated view by a DAS client.
Success in the effort depends on the willingness of investigators to contribute annotation information
recorded on their respective servers, and on usersÕ learning about the existence of a DAS server (e.g.,
through ad hoc mechanisms such as link lists). DAS is also more or less specific to sequence annotation
and is not easily extended to other biological objects.Today, when biologists archive a newly discovered gene sequence in GenBank, for example, theyhave various types of annotation software at their disposal to link it with explanatory data. Next-
generation annotation systems will have to do this for many other genome features, such as transcrip-
tion-factor binding sites and single nucleotide polymorphisms (SNPs), that most of todayÕs systems
donÕt cover at all. Indeed, these systems will have to be able to create, annotate, and archive models of
entire metabolic, signaling, and genetic pathways. Next-generation annotation systems will have to be
built in a highly modular and open fashion, so that they can accommodate new capabilities and new
data types without anyoneÕs having to rewrite the basic code.4.2.10  A Case Study: The Cell Centered Database
45To illustrate the notions described above, it is helpful to consider an example of a database effortthat implements many of them. Techniques such as electron tomography are generating large amounts
of exquisitely detailed data on cells and their macromolecular organization that have to be exposed to
the greater scientific community. However, very few structured data repositories for community use
exist for the type of cellular and subcellular information produced using light and electron microscopy.
The Cell Centered Database (CCDB) addresses this need by developing a database for three-dimen-
sional light and electron microscopic information.4644See http://www.cds.caltech.edu/erato.45Section 4.2.10 is adapted largely from M.E. Martone, S.T. Peltier, and M.H. Ellisman, ÒBuilding Grid Based Resources forNeurosciences,Ó National Center for Microscopy and Imaging Research, Department of Neurosciences, University of California,San Diego, unpublished and undated working paper.46M.E. Martone, A. Gupta, M. Wong, X. Qian, G. Sosinsky, B. Ludascher, and M.H. Ellisman, ÒA Cell-Centered Database forElectron Tomographic Data,Ó Journal of Structural Biology 138(1-2):145-155, 2002; M.E. Martone, S. Zhang, S. Gupta, X. Qian, H.He, D.A. Price, M. Wong, et al., ÒThe Cell Centered Database: A Database for Multiscale Structural and Protein Localization Datafrom Light and Electron Microscopy,Ó Neuroinformatics 1(4):379-396, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.76CATALYZING INQUIRYThe CCDB contains structural and protein distribution information derived from confocal, mul-tiphoton, and electron microscopy, including correlated microscopy. Its main mission is to provide a
means to make high-resolution data derived from electron tomography and high-resolution light mi-
croscopy available to the scientific community, situating itself between whole brain imaging databases
such as the MAP project47 and protein structures determined from electron microscopy, nuclear mag-netic resonance (NMR) spectroscopy, and X-ray crystallography (e.g., the Protein Data Bank and EMBL).The CCDB serves as a research prototype for investigating new methods of representing imagingdata in a relational database system so that powerful data-mining approaches can be employed for the
content of imaging data. The CCDB data model addresses the practical problem of image management
for the large amounts of imaging data and associated metadata generated in a modern microscopy
laboratory. In addition, the data model has to ensure that data within the CCDB can be related to data
taken at different scales and modalities.The data model of the CCDB was designed around the process of three-dimensional reconstructionfrom two-dimensional micrographs, capturing key steps in the process from experiment to analysis.
(Figure 4.1 illustrates the schema-entity relationship for the CCDB.) The types of imaging data stored in
the CCDB are quite heterogeneous, ranging from large-scale maps of protein distributions taken by
confocal microscopy to three-dimensional reconstruction of individual cells, subcellular structures, and
organelles. The CCDB can accommodate data from tissues and cultured cells regardless of tissue of
origin, but because of the emphasis on the nervous system, the data model contains several features
specialized for neural data. For each dataset, the CCDB stores not only the original images and three-
dimensional reconstruction, but also any analysis products derived from these data, including seg-
mented objects and measurements of quantities such as surface area, volume, length, and diameter.
Users have access to the full resolution imaging data for any type of data, (e.g., raw data, three-
dimensional reconstruction, segmented volumes), available for a particular dataset.For example, a three-dimensional reconstruction is viewed as one interpretation of a set of raw datathat is highly dependent on the specimen preparation and imaging methods used to acquire it. Thus, a
single record in the CCDB consists of a set of raw microscope images and any volumes, images, or data
derived from it, along with a rich set of methodological details. These derived products include recon-
structions, animations, correlated volumes, and the results of any segmentation or analysis performed
on the data. By presenting all of the raw data, as well as reconstructed and processed data with a
thorough description of how the specimen was prepared and imaged, researchers are free to extract
additional content from micrographs that may not have been analyzed by the original author or employ
additional alignment, reconstruction, or segmentation algorithms to the data.The utility of image databases depends on the ability to query them on the basis of descriptiveattributes and on their contents. Of these two types of query, querying images on the basis of their
contents is by far the most challenging. Although the development of computer algorithms to identify
and extract image features in image data is advancing,48 it is unlikely that any algorithm will be able tomatch the skill of an experienced microscopist for many years.The CCDB project addresses this problem in two ways. One currently supported way is to store theresults of segmentations and analyses performed by individual researchers on the data sets stored in the
CCDB. The CCDB allows each object segmented from a reconstruction to be stored as a separate object
in the database along with any quantitative information derived from it. The list of segmented objects
and their morphometric quantities provides a means to query a dataset based on features contained in
the data such as object name (e.g., dendritic spine) or quantities such as surface area, volume, and
length.47A. MacKenzie-Graham, E.S. Jones, D.W. Shattuck, I. Dinov, M. Bota, and A.W. Toga, ÒThe Informatics of a C57BL/6 MouseBrain Atlas,Ó Neuroinformatics 1(4):397-410, 2003.48U. Sinha, A. Bui, R. Taira, J. Dionisio, C. Morioka, D. Johnson, and H. Kangarloo, ÒA Review of Medical Imaging Informatics,ÓAnnals of the New York Academy of Sciences 980:168-197, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS77It is also desirable to exploit information in the database that is not explicitly represented in theschema.49 Thus, the CCDB project team is developing specific data types around certain classes of seg-mented objects contained in the CCDB. For example, the creation of a Òsurface data typeÓ will enable users
to query the original surface data directly. The properties of the surfaces can be determined through very
general operations at query time that allow the user to query on characteristics not explicitly modeled in
the schema (e.g., dendrites from striatal medium spiny cells where the diameter of the dendritic shaft
shows constrictions of at least 20 percent along its length). In this example, the schema does not contain
explicit indication of the shape of the dendritic shaft, but these characteristics can be computed as part of
the query processing. Additional data types are being developed for volume data and protein distribution
data. A data type for tree structures generated by Neurolucida has recently been implemented.The CCDB is being designed to participate in a larger, collaborative virtual data federation. Thus, anapproach to reconciling semantic differences between various databases must be found.50 ScientificProjectExperiment
SubjectTissueProductTissueProcessingFixationProteinLocalizationStainingEmbedding
Microscopy
ProductMicroscopy
Image DetailsAnatomicalDetailsRegion ofInterestReconstruction
Reconstruction
Image DetailsSegmentationTree Tracking
Morphometrics
FIGURE 4.1The schema and entity relationship in the Cell Centered Database.
SOURCE: See http://ncmir.ucsd.edu/CCDB.49Z. Lacroix, ÒIssues to Address While Designing a Biological Information System,Ó pp. 4-5 in Bioinformatics: Managing Scien-tific Data, Z.T. Lacroix , ed., Morgan Kaufmann, San Francisco, 2003.50Z. Lacroix, ÒIssues to Address While Designing a Biological Information System,Ó pp. 4-5 in Bioinformatics: Managing Scien-tific Data, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.78CATALYZING INQUIRYterminology, particularly neuroanatomical nomenclature, is vast, nonstandard, and confusing. Ana-tomical entities may have multiple names (e.g., caudate nucleus, nucleus caudates), the same term mayhave multiple meanings (e.g., spine [spinal cord] versus spine [dendritic spine]), and worst of all, the
same term may be defined differently by different scientists (e.g., basal ganglia). To minimize semantic
confusion and to situate cellular and subcellular data from the CCDB in a larger context, the CCDB is
mapped to several shared knowledge sources in the form of ontologies.Concepts in the CCDB are being mapped to the Unified Medical Language System (UMLS), a largemetathesaurus and knowledge source for the biomedical sciences.51 The UMLS assigns each concept inthe ontology a unique identifier (ID); thus, all synonymous terms can then be assigned the same ID. For
example, the UMLS ID number for the synonymous terms Purkinje cell, cerebellar Purkinje cell, and
PurkinjeÕs corpuscle is C0034143. Thus, regardless of which term is preferred by a given individual, if
they share the same ID, they are asserted to be the same. Conversely, even if two terms share the same
name, they are distinguishable by their unique IDs. In the example given above, spine (spinal cord) =
C0037949, whereas spine (dendritic spine) = C0872341.In addition, an ontology can support the linkage of concepts by a set of relationships. These rela-tionships may be simple Òis aÓ and Òhas aÓ relationships (e.g., Purkinje cell is a neuron, neuron has a
nucleus), or they may be more complex.52 From the above statements, a search algorithm could inferthat ÒPurkinje cell has a nucleusÓ if the ontology is encoded in a form that would allow such reasoning
to be performed. Because the knowledge required to link concepts is contained outside of the source
database, the CCDB is relieved of the burden of storing exhaustive taxonomies for individual datasets,
which may become obsolete as new knowledge is discovered.The UMLS has recently incorporated the NeuroNames ontology53 as a source vocabulary.NeuroNames is a comprehensive resource for gross brain anatomy in the primate. However, for the
type of cellular and subcellular data contained in the CCDB, the UMLS does not contain sufficient
detail. Ontologies for areas such as neurocytology and neurological disease are being built on top of the
UMLS, utilizing existing concepts wherever possible and constructing new semantic networks and
concepts as needed.54In addition, imaging data in the CCDB is mapped to a higher level of brain organization by register-ing their location in the coordinate system of a standard brain atlas. Placing data into an atlas-based
coordinate systems provides one method by which data taken across scales and distributed across
multiple resources can reliably be compared.55Through the use of computer-based atlases and associated tools for warping and registration, it ispossible to express the location of anatomical features or signals in terms of a standardized coordinate
system. While there may be disagreement among neuroscientists about the identity of a brain area
giving rise to a signal, its location in terms of spatial coordinates is at least quantifiable. The expression
of brain data in terms of atlas coordinates also allows them to be transformed spatially to offer alterna-
tive views that may provide additional information (such as flat maps or additional parcellation51B.L. Humphreys, D.A. Lindberg, H.M. Schoolman, and G.O. Barnett, ÒThe Unified Medical Language System: An InformaticsResearch Collaboration,Ó Journal of the American Medical Informatics Association 5(1):1-11, 1998.52A. Gupta, B. Ludascher, J.S. Grethe, and M.E. Martone, ÒTowards a Formalization of a Disease Specific Ontology forNeuroinformatics,Ó Neural Networks 16(9):1277-1292, 2003.53D.M. Bowden and M.F. Dubach, ÒNeuroNames 2002,Ó Neuroinformatics 1:43-59, 2002.54A. Gupta, B. Ludascher, J.S. Grethe, and M.E. Martone, ÒTowards a Formalization of a Disease Specific Ontology forNeuroinformatics,Ó Neural Networks 6(9):1277-1292, 2003.55A. Brevik, T.B. Leergaard M. Svanevik, J.G. Bjaalie, ÒThree-dimensional Computerised Atlas of the Rat Brain StemPrecerebellar System: Approaches for Mapping, Visualization, and Comparison of Spatial Distribution Data,Ó Anatomy andEmbryology 204(4):319-332, 2001; J.G. Bjaalie, ÒOpinion: Localization in the Brain: New Solutions Emerging,Ó Nature Reviews:Neuroscience 3(4):322-325, 2003; D.C. Van Essen, H.A. Drury, J. Dickson, J. Harwell, D. Hanlon, and C.H. Anderson, ÒAn Inte-grated Software Suite for Surface-based Analyses of Cerebral Cortex,Ó Journal of the American Medical Informatics Association8(5):443-459, 2001; D.C. Van Essen, ÒWindows on the Brain: The Emerging Role of Atlases and Databases in Neuroscience,ÓCurrent Opinion in Neurobiology 12(5):574-579, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS79schemes).56 Finally, because individual experiments can study only a few aspects of a brain region atone time, a standard coordinate system allows the same brain region to be sampled repeatedly to allow
data to be accumulated over time.4.2.11  A Case Study: Ecological and Evolutionary Databases
Although genomic databases such as GenBank receive the majority of attention, databases andalgorithms that operate on databases are key tools in research into ecology and biodiversity as well.
These tools can provide researchers with access to information regarding all identified species of a given
type, such as AlgaeBase57 or FishBase;58 they also serve as a repository for submission of new informa-tion and research. Other databases go beyond species listings to record individuals: for example, the
ORNIS database of birds seeks to provide access to nearly 5 million individual specimens held in
natural history collections, which includes data such as recordings of vocalizations and egg and nest
holdings.59The data associated with ecological research are gathered from a wide variety of sources: physicalobservations in the wild by both amateurs and professionals; fossils; natural history collections; zoos,
botanical gardens, and other living collections; laboratories; and so forth. In addition, these data must
placed into contexts of time, geographic location, environment, current and historical weather and
climate, and local, regional, and global human activity. Needless to say, these data sources are scattered
throughout many hundreds or thousands of different locations and formats, even when they are in
digitally accessible format. However, the need for integrated ecological databases is great: only by being
able to integrate the totality of observations of population and environment can certain key questions be
answered. Such a facility is central to endangered species preservation, invasive species monitoring,
wildlife disease monitoring and intervention, agricultural planning, and fisheries management, in addi-
tion to fundamental questions of ecological science.The first challenge in building such a facility is to make the individual datasets accessible bynetworked query. Over the years, hundreds of millions of specimens have been recorded in museum
records. In many cases, however, the data are not even entered into a computer; they may be stored as
a set of index cards dating from the 1800s. Natural history collections, such as a museumÕs collection of
fossils, may not even be indexed, and they are available to researchers only by physically inspecting the
drawers. Very few specimens have been geocoded.Museum records carry a wealth of image and text data, and digitizing these records in a mean-ingful and useful way remains a serious challenge. For this reason, funding agencies such as the
National Science Foundation (NSF) are emphasizing integrating database creation, curation, and
sharing into the process of ecological science: for example, the NSF Biological Databases and
Informatics program60 (which includes research into database algorithms and structures, as well asdeveloping particular databases) and the Biological Research Collections program, which provides
around $6 million per year for computerizing existing biological data. Similarly, the NSF Partner-
ships for Enhancing Expertise in Taxonomy (PEET) program,61 which emphasizes training in tax-onomy, requires that recipients of funding incorporate collected data into databases or other shared
electronic formats.56D.C. Van Essen, ÒWindows on the Brain: The Emerging Role of Atlases and Databases in Neuroscience,Ó Current Opinion inNeurobiology 12:574-579, 2002.57See http://www.algaebase.org.58See http://www.fishbase.org.59See http://www.ornisnet.org.60NSF Program Announcement NSF 02-058; see http://www.nsf.gov/pubsys/ods/getpub.cfm?nsf02058.61See http://web.nhm.ku.edu/peet/.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.80CATALYZING INQUIRYEcological databases also rely on metadata to improve interoperability and compatibility amongdisparate data collections.62 Ecology is a field that demands access to large numbers of independentdatasets such as geographic information, weather and climate records, biological specimen collections,
population studies, and genetic data. These datasets are collected over long periods of time, possibly
decades or even centuries, by a diverse set of actors for different purposes. A commonly agreed-upon
format and vocabulary for metadata is essential for efficient cooperative access.Furthermore, as data increasingly are collected by automated systems such as embedded systemsand distributed sensor networks, the applications that attempt to fuse the results into formats amenable
to algorithmic or human analysis must deal with high (and always on) data rates, likely contained in
shifting standards for representation. Again, early agreement on a basic system for sharing metadata
will be necessary for the feasibility of such applications.In attempting to integrate or cross-query these data collections, a central issue is the naming ofspecies or higher-level taxa. The Linnean taxonomy is the oldest such effort in biology, of course, yet
because there is not yet (nor likely can ever be) complete agreement on taxa identification, entries in
different databases may contain different tags for members of the same species, or the same tag for
members that were later determined to be of different species. Taxa are often moved into different
groups, split, or merged with others; names are sometimes changed. A central effort to manage this is
the Integrated Taxonomic Information System (ITIS),63 which began life as a U.S. interagency task force,but today is a global cooperative effort between government agencies and researchers to arrive at a
repository for agreed-upon species names and taxonomic categorization. ITIS data are of varying qual-
ity, and entries are tagged with three different quality indicators: credibility, which indicates whether or
not data have been reviewed; latest review, giving the year of the last review; and global completeness,
which records whether all species belonging to a taxon were included at the last review. These measure-
ments allow researchers to evaluate whether the data are appropriate for their use.In constructing such a database, many data standards questions arise. For example, ITIS uses namingstandards from the International Code of Botanical Nomenclature and the International Code of Zoologi-
cal Nomenclature. However, for the kingdom Protista, which at various times in biological science has
been considered more like an animal and more like a plant, both standards might apply. Dates and date
ranges provide another challenge: while there are many international standards for representing a calen-
dar date, in general these did not foresee the need to represent dates occurring millions or billions of years
ago. ITIS employs a representation for geologic ages, and this illustrates the type of challenge encountered
when stretching a set of data standards to encompass many data types and different methods of collection.For issues of representing observations or collections, an important element is the Darwin Core, aset of XML metadata standards for describing a biological specimen, including observations in the wild
and preserved items in natural history collections. Where ITIS attempts to improve communicability by
achieving agreement on precise name usage, Darwin Core64 (and similar metadata efforts) concentratesthe effort on labeling and markup of data. This allows individual databases to use their own data
structures, formats, and representations, as long as the data elements are labeled by Darwin Core
keywords. Since the design demands on such databases will be substantially different, this is a useful
approach. Another attempt to standardize metadata for ecological data is the Access to Biological
Collections Data (ABCD) Schema,65 which is richer and contains more information. These two ap-proaches indicate a common strategic choice: simpler standards are easier to adopt, and thus will likely
be more widespread, but are limited in their expressiveness; more complex standards can successfully62For a more extended discussion of the issues involved in maintaining ecological data, see W.K. Michener and J.W. Brunt,eds., Ecological Data: Design, Management and Processing, Methods in Ecology, Blackwell Science, Maryland, 2000. A useful onlinepresentation can be found at http://www.soest.hawaii.edu/PFRP/dec03mtg/michener.pdf.63See http://www.itis.usda.gov.64See http://speciesanalyst.net/docs/dwc/.65See http://www.bgbm.org/TDWG/CODATA/Schema/default.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS81support a wider variety of queries and data types, but may be slower to gain adoption. Another effort toaccomplish agreement on data and metadata standards is the National Biological Information Initiative
(NBII), a program of the U.S. Geological SurveyÕs Center for Biological Informatics.Agreement on standard terminology and data labeling would accomplish little if the data sourceswere unknown. The most significant challenge in creating large-scale ecological information is the
integration and federation of the potentially vast number of relevant databases. The Global Biodiversity
Information Facility (GBIF)66 is an attempt to offer a single-query interface to cooperating data provid-ers; in December of 2004, it consisted of 95 providers totaling many tens of millions of individual
records. GBIF accomplishes this query access through the use of data standards (such as the Darwin
Core) and Web services, an information technology (IT) industry standard way of requesting informa-
tion from servers in a platform-independent fashion. A similar international effort is found at the
Clearinghouse Mechanism (CHM),67 an instrumentality of the Convention on Biodiversity. The CHM isintended as a way for information on biodiversity to be shared among signatory states and made
available as a way to monitor compliance and as a tool for policy.Globally integrated ecological databases are still in embryonic form, but as more data becomedigitized and made available by the Internet in standard fashions, their value will increase. Integration
with phylogenetic and molecular databases will add to their value as research tools, in both the ecologi-
cal and the evolutionary fields.4.3  DATA PRESENTATION
4.3.1  Graphical Interfaces
Biological processes can take place over a vast array of spatial scales, from the nanoscale inhabitedby individual molecules, to the everyday, meter-sized human world. They can take place over an even
vaster range of time scales, from the nanosecond gyrations of a folding protein molecule to the seven
decade (or so) span of a human lifeÑand far beyond, if evolutionary time is included. They also can be
considered at many levels of organization, from the straightforward realm of chemical interaction to the
abstract realm of, say, signal transduction and information processing.Much of 21st century biology must deal with these processes at every level and at every scale,resulting in data of high dimensionality. Thus, the need arises for systems that can offer vivid and easily
understood visual metaphors to display the information at each level, showing the appropriate amount
of detail. (Such a display would be analogous to, say, a circuit diagram, with its widely recognized icons
for diodes, transistors, and other such components.) A key element of such systems is easily understood
metaphors that present signals containing multiple colors over time on more than one axis. As an
empirical matter, these metaphors are hard to find. Indeed, the problem of finding a visually (or
intellectually!) optimal display layout for high-dimensional data is arguably combinatorially hard,
because in the absence of a well-developed theory of display, it requires exploring every possible
combination of data in a multitude of arrangements.The system would likewise offer easy and intuitive ways to navigate between levels, so that the usercould drill down to get more detail or pop up to higher abstractions as needed. Also, it would offer good
ways to visualize the dynamical behavior of the system over timeÑwhatever the appropriate time scale
might be. Current-generation visualization systems such as those associated with BioSPICE68 andCytoscape69 are a good beginningÑbut, as their developers themselves are the first to admit, only abeginning.66See http://www.gbif.org/.67See http://www.biodiv.org/chm/default.aspx.68See http://biospice.lbl.gov/home.html.69See http://www.cytoscape.org/.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.82CATALYZING INQUIRYBiologists use a variety of different data representations to help describe, examine, and understanddata. Biologists often use cartoons as conceptual, descriptive models of biological events or processes. A
cartoon might show a time line of events: for example, the time line of the phosphorylation of a receptor
that allows a protein to bind to it. As biologists take into account the simultaneous interactions of larger
numbers of molecules, events over time become more difficult to represent in cartoons. New ways to
ÒseeÓ interactions and associations are therefore needed in life sciences research.The most complex data visualizations are likely to be representations of networks. The completegraph in Figure 4.2 contains 4,543 nodes of approximately 6,000 proteins encoded by the yeast genome,
along with 12,843 interactions. The graph was developed using the Osprey network visualization system.FIGURE 4.2From genomics to proteomics. Visualization of combined, large-scale interaction data sets in yeast. A
total of 14,000 physical interactions obtained from the GRID database were represented with the Osprey network
visualization system (see http://biodata.mshri.on.ca/grid). Each edge in the graph represents an interaction be-tween nodes, which are colored according to Gene Ontology (GO) functional annotation. Highly connected com-plexes within the dataset, shown at the perimeter of the central mass, are built from nodes that share at least three
interactions within other complex members. The complete graph contains 4,543 nodes of ~6,000 proteins encodedby the yeast genome, 12,843 interactions and an average connectivity of 2.82 per node. The 20 highly connectedcomplexes contain 340 genes, 1,835 connections, and an average connectivity of 5.39.
SOURCE: Reprinted by permission from M. Tyers and M. Mann, ÒFrom Genomics to Proteomics,Ó Nature 422:193-197, 2003. Copyright 2003 Macmillan Magazines Ltd.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS83Other diagrammatic simulations of complex cell networks use tools such as the Diagrammatic CellLanguage (DCL) and Visual Cell. These software tools are designed to read, query, and edit cell path-
ways, and to visualize data in a pathway context. Visual Cell creates detailed drawings by compactly
formatting thousands of molecular interactions. The software uses DCL, which can visualize and simu-
late large-scale networks such as interconnected signal transduction pathways and the gene expression
networks that control cell proliferation and apoptosis. DCL can visualize millions of chemical states and
chemical reactions.A second approach to diagrammatic simulation has been developed by Efroni et al.70 These re-searchers use the visual language of Statecharts, which makes specification of the simulation precise,
legible, and machine-executable. Behavior in Statecharts is described by using states and events that
cause transitions between states. States may contain substates, thus enabling description at multiple
levels and zooming in and zooming out between levels. States may also be divided into orthogonal
states, thus modeling concurrency, allowing the system to reside simultaneously in several different
states. A cell, for example, may be described orthogonally as expressing several receptors, no receptors,
or any combination of receptors at different stages of the cell cycle and in different anatomical compart-
ments. Furthermore, transitions take the system from one state to another. In cell modeling, transitions
are the result of biological processes or the result of user intervention. A biological process may be the
result of an interaction between two cells or between a cell and various molecules. Statecharts provide
a controllable way to handle the enormous dataset of cell behavior by enabling the separation of that
dataset into orthogonal states and allowing transitions.Still another kind of graphical interface is used for molecular visualization. Interesting biomoleculesusually consist of thousands of atoms. A list of atomic coordinates is useful for some purposes, but an
actual image of the molecule can often provide much more insight into its propertiesÑand an image
that can be manipulated (e.g., viewed from different angles) is even more useful. Virtual reality tech-
niques can be used to provide the viewer with a large field of view, and to enable the viewer to interact
with the virtual molecule and compare it to other molecules. However, many problems in biomolecular
visualization tax the capability of current systems because of the diversity of operations required and
because many operations do not fit neatly into the current architectural paradigm.4.3.2  Tangible Physical Interfaces
As useful as graphical visualizations are, even in simulated three-dimensional virtual reality
they are still two-dimensional. Tangible, physical models that a human being can manipulate di-
rectly with his or her hands are an extension of the two-dimensional graphical environment. A
project at the Molecular Graphics Laboratory at the Scripps Research Institute is developing tan-
gible interfaces for molecular biology.71 These interfaces use computer-driven autofabrication tech-nology (i.e., three-dimensional printers) and result in physical molecular representations that one
can hold in oneÕs hand.These efforts have required the development and testing of software for the representation ofphysical molecular models to be built by autofabrication technologies, linkages between molecular
descriptions and computer-aided design and manufacture approaches for enhancing the models with
additional physical characteristics, and integration of the physical molecular models into augmented-
reality interfaces as inputs to control computer display and interaction.70S. Efroni, D. Harel, and I.R. Cohen, ÒToward Rigorous Comprehension of Biological Complexity: Modeling, Execution, andVisualization of Thymic T-Cell Maturation,Ó Genome Research 13(11):2485-2497, 2003.71A. Gillet, M. Sanner, D. Stoffler, D. Goodsell, and A. Olson, ÒAugmented Reality with Tangible Auto-Fabricated Models forMolecular Biology Applications,Ó Proceedings of the IEEE Visualization 2004 (VISÕ04), October 10-15, 2004, Austin, pp. 235-242.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.84CATALYZING INQUIRY4.3.3  Automated Literature Searching
72Still another form of data presentation is journal publication. It has not been lost on the scientificbioinformatics community that vast amounts of functional information that could be used to annotate
gene and protein sequences are embedded in the written literature. Rice and Stolovitzky go so far as to
say that mining the literature on biomolecular interactions can assist in populating a network model of
intracellular interaction (Box 4.4).73So far, however, the availability of full-text articles in digital formats such as PDF, HTML, or TIFfiles has limited the possibilities for computer searching and retrieval of full text in databases. In the
future, wider use of structured documents tagged with XML will make intelligent searching of full text
feasible, fast, and informative and will allow readers to locate, retrieve, and manipulate specific parts of
a publication.In the meantime, however, natural language provides a considerable, though not insurmountable,challenge for algorithms to extract meaningful information from natural text. One common application
of natural language processing involves the extraction from the published literature of information
about proteins, drugs, and other molecules. For example, Fukuda et al. (1998) pioneered identificationof protein names using properties of the text such as the occurrence of uppercase letters, numerals, and
special endings to pinpoint protein names.74Other work has investigated the feasibility of recognizing interactions between proteins and othermolecules. One approach is based on simultaneous occurrences of gene names and their use to predict
their connections based on their occurrence statistics.75 A second approach to pathway discovery was72The discussion in Section 4.3.3 is based on excerpts from L. Hirschman, J.C. Park, J. Tsujii, L. Wong, and C.H. Wu, ÒAccom-plishments and Challenges in Literature Data Mining for Biology,Ó Bioinformatics Review 18(12):1553-1561, 2002. Available athttp://pir.georgetown.edu/pirwww/aboutpir/doc/data_mining.pdf.73J.J. Rice and G. Stolovitzky, ÒMaking the Most of It: Pathway Reconstruction and Integrative Simulation Using the Data atHand,Ó Biosilico 2(2):70-77, 2004.74K. Fukuda, et al., ÒToward Information Extraction: Identifying Protein Names from Biological Papers,Ó Pacific Symposium onBiocomputing 1998, 707-718. (Cited in Hirschman et al., 2002.)75B. Stapley and G. Benoit, ÒBiobibliometrics: Information Retrieval and Visualization from Co-occurrences of Gene Names inMEDLINE Abstracts,Ó Pacific Symposium on Biocomputing 2000, 529-540; J. Ding et al., ÒMining MEDLINE: Abstracts, Sentences,or Phrases?Ó Pacific Symposium on Biocomputing 2002, 326-337. (Cited in Hirschman et al., 2002.)Box 4.4Text Mining and Populating a Network Model of Intracellular InteractionOther methods [for the construction of large-scale topological maps of cellular networks] have sought to mineMEDLINE/PubMed abstracts that are considered to contain concise records of peer-reviewed published results. Thesimplest methods, often called Ôguilt by association,Õ seek to find co-occurrence of genes or protein names in ab-stracts or even smaller structures such as sentences or phrases. This approach assumes that co-occurrences are
indicative of functional links, although an obvious limitation is that negative relations (e.g., A does not regulate B) arecounted as positive associations. To overcome this problem, other natural language processing methods involvesyntactic parsing of the language in the abstracts to determine the nature of the interactions. There are obvious
computation costs in these approaches, and the considerable complexity in human language will probably renderany machine-based method imperfect. Even with limitations, such methods will probably be required to makeknowledge in the extant literature accessible to machine-based analyses. For example, PreBIND used support vector
machines to help select abstracts likely to contain useful biomolecular interactions to ÔbackfillÕ the BIND database.SOURCE: Reprinted by permission from J.J. Rice and G. Stolovitzky, ÒMaking the Most of It: Pathway Reconstruction and IntegrativeSimulation Using the Data at Hand,Ó Biosilico 2(2):70-77. Copyright 2004 Elsevier. (References omitted.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS85based on templates that matched specific linguistic structures to recognize and extract of protein inter-action information from MEDLINE documents.76 More recent work goes beyond the analysis of singlesentences to look at relations that span multiple sentences through the use of co-reference. For example,
Putejovsky and Castano focused on relations of the word inhibit and showed that it was possible toextract biologically important information from free text reliably, using a corpus-based approach to
develop rules specific to a class of predicates.77 Hahn et al. described the MEDSYNDIKATE system foracquiring knowledge from medical reports, a system capable of analyzing co-referring sentences and
extracting new concepts given a set of grammatical constructs.78Box 4.5 describes a number of other information extraction successes in biology. In a commen-tary in EMBO Reports on publication mining, Les Grivell, manager of the European electronicpublishing initiative, E-BioSci, sums up the challenges this way:79The detection of gene symbols and names, for instance, remains difficult, as researchers have seldomfollowed logical rules. In some organismsÑthe fruit fly Drosophila is an exampleÑscientists have enjoyedapplying gene names with primary meaning outside the biological domain. Names such as vamp, eve,disco, boss, gypsy, zip or ogre are therefore not easily recognized as referring to genes.80Also, both synonymy (many different ways to refer to the same object) and polysemy (multiple mean-ings for a given word) cause problems for search algorithms. Synonymy reduces the number of recalls ofa given object, whereas polysemy causes reduced precision. Another problem is ambiguities of a wordÕssense. The word insulin, for instance, can refer to a gene, a protein, a hormone or a therapeutic agent,
depending on the context. In addition, pronouns and definite articles and the use of long, complex ornegative sentences or those in which information is implicit or omitted pose considerable hurdles for full-text processing algorithms.Grivell points out that algorithms exist (e.g., the Vector Space Model) to undertake text analysis,theme generation, and summarization of computer-readable texts, but adds that Òapart from the consid-
erable computational resources required to index terms and to precompute statistical relationships for
several million articles,Ó an obstacle to full-text analysis is the fact that scientific journals are owned by
a large number of different publishers, so computational analysis will have to be distributed across
multiple locations.76S.K. Ng and M. Wong, ÒToward Routine Automatic Pathway Discovery from Online Scientific Text Abstracts,Ó GenomeInformatics 10:104-112, 1999. (Cited in Hirschman et al., 2002.)77J. Putejovsky and J. Castano, ÒRobust Relational Parsing over Biomedical Literature: Extracting Inhibit Relations,Ó PacificSymposium on Biocomputing 2002, 362-373. (Cited in Hirschman et al., 2002.)78U. Hahn, et al., ÒRich Knowledge Capture from Medical Documents in the MEDSYNDIKATE System,Ó Pacific Symposium onBiocomputing 2002, 338-349. (Cited in Hirschman et al., 2002.)79L. Grivell, ÒMining the Bibliome: Searching for a Needle in a Haystack? New Computing Tools Are Needed to EffectivelyScan the Growing Amount of Scientific Literature for Useful Information,Ó EMBO Report 3(3):200-203, 2002.80D. Proux, F. Rechenmann, L. Julliard, V. Pillet. and B. Jacq, ÒDetecting Gene Symbols and Names in Biological Texts: A FirstStep Toward Pertinent Information Extraction,Ó Genome Informatics 9:72-80, 1999. (Cited in Grivell, 2002.) Note also that whilegene names are often italicized in print (so that they are more readily recognized as genes), neither verbal discourse nor textsearch recognizes italicization. In addition, because some changes of name are made for political rather than scientific reasons,and because these political revisions are done quietly, even identifying the need for synonym tracking can be problematic. Anexample is a gene mutation, discovered in 1963, that caused male fruit flies to court other males. Over time, the assigned genename of ÒfruityÓ came to be regarded as offensive, and eventually the genes name was changed to ÒfruitlessÓ after much public
disapproval. A similar situation arose more recently, when scientists at Princeton University found mutations in flies that causedthem to be learning defective or, in the vernacular of the investigators, Òvegged out.Ó They assigned names such as cabbage,rutabaga, radish, and turnipÑwhich some other scientists found objectionable. See, for example, M. Vacek, ÒA Gene by Any
Other Name,Ó American Scientist 89(6), 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.86CATALYZING INQUIRYBox 4.5Selected Information Extraction Successes in BiologyBesides the recognition of protein interactions from scientific text, natural language processing has been applied toa broad range of information extraction problems in biology.Capturing of Specific Relations in Databases.. . . We begin with systems that capture specific relations in databases. Hahn et al. (2002) used natural languagetechniques and nomenclatures of the Unified Medical Language System (UMLS) to learn ontological relations for a
medical domain. Baclawski et al. (2000) is a diagrammatic knowledge representation method called keynets. TheUMLS ontology was used to build keynets.Using both domain-independent and domain-specific knowledge, keynets parsed texts and resolved references tobuild relationships between entities. Humphreys et al. (2000) described two information extraction applications inbiology based on templates: EMPathIE extracted from journal articles details of enzyme and metabolic pathways;
PASTA extracted the roles of amino acids and active sites in protein molecules. This work illustrated the importanceof template matching, and applied the technique to terminology recognition. Rindflesch et al. (2000) describedEDGAR, a system that extracted relationships between cancer-related drugs and genes from biomedical literature.
EDGAR drew on a stochastic part-of-speech tagger, a syntactic parser able to produce partial parses, a rule-basedsystem, and semantic information from the UMLS. The metathesaurus and lexicon in the knowledge base were usedto identify the structure of noun phrases in MEDLINE texts. Thomas et al. (2000) customized an information extrac-tion system called Highlight for the task of gathering data on protein interactions from MEDLINE abstracts. Theydeveloped and applied templates to every part of the texts and calculated the confidence for each match. Theresulting system could provide a cost-effective means for populating a database of protein interactions.Information Retrieval and Clustering.The next papers [in this volume] focus on improving retrieval and clustering in searching large collections. Chang etal. (2001) modified PSI-BLAST to use literature similarity in each iteration of its search. They showed that supple-menting sequence similarity with information from biomedical literature search could increase the accuracy ofhomology search result. Illiopoulos et al. (2001) gave a method for clustering MEDLINE abstracts based on a statis-tical treatment of terms, together with stemming, a Ògo-list,Ó and unsupervised machine learning. Despite the mini-mal semantic analysis, clusters built here gave a shallow description of the documents and supported conceptdiscovery.Wilbur (2002) formalized the idea of a ÒthemeÓ in a set of documents as a subset of the documents and a subset ofthe indexing terms so that each element of the latter had a high probability of occurring in all elements of the former.
An algorithm was given to produce themes and to cluster documents according to these themes.Classification.. . . text processing has been used for classification. Stapley et al. (2002) used a support vector machine to classifyterms derived by standard term weighting techniques to predict the cellular location of proteins from description inabstracts. The accuracy of the classifier on a benchmark of proteins with known cellular locations was better than
that of a support vector machine trained on amino acid composition and was comparable to a handcrafted rule-based classifier (Eisenhaber and Bork, 1999).SOURCE: Reprinted by permission from L. Hirschman, J.C. Park, J. Tsujii, L. Wong, and C.H. Wu, ÒAccomplishments and Challenges inLiterature Data Mining for Biology, Bioinformatics Review 18(12):1553-1561, 2002, available at http://pir.georgetown.edu/pirwww/aboutpir/doc/data_mining.pdf. Copyright 2002 Oxford University Press.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS874.4  ALGORITHMS FOR OPERATING ON BIOLOGICAL DATA
4.4.1  Preliminaries: DNA Sequence as a Digital String
 The digital nature of DNA is a central evolutionary innovation for many reasonsÑthat is, theÒvaluesÓ of the molecules making up the polymer are discrete and indivisible units. Just as an electronic
digital computer abstracts various continuous voltage levels as 0 and 1, DNA abstracts a three-dimen-
sional organization of atoms as A, T, G, and C. This has important biological benefits, including very
high-accuracy replication, common and simplified ways for associated molecules to bind to sites, and
low ambiguity in coding for proteins.For human purposes in bioinformatics, however, the use of the abstraction of DNA as a digitalstring has had other equally significant and related benefits. It is easy to imagine the opposite case, in
which DNA is represented as the three-dimensional locations of each atom in the macromolecule, and
comparison of DNA sequences is a painstaking process of comparing the full structures. Indeed, this is
very much the state of the art in representing proteins (which, although they can be represented as a
digital string of peptides, are more flexible than DNA, so the digital abstraction leaves out the critically
important features of folding). The digital abstraction includes much of the essential information of the
system, without including complicating higher- and lower-order biochemical properties.81 The com-parison of the state of the art in computational analysis of DNA sequences and protein sequences speaks
in part to the enormous advantage that the digital string abstraction offers when appropriate.The most basic feature of the abstraction is that it treats the arrangement of physical matter asinformation. An important advantage of this is that information-theoretic techniques can be applied to
specific DNA strings or to the overall alphabet of codon-peptide associations. For example, computer
science-developed concepts such as Hamming distance, parity, and error-correcting codes can be used
to evaluate the resilience of information in the presence of noise and close alternatives.82A second and very practical advantage is that as strings of letters, DNA sequences can be storedefficiently and recognizably in the same format as normal text.83 An entire human genome, for example, canbe stored in about 3 gigabytes, costing a few dollars in 2003. More broadly, this means that a vast array of
tools, software, algorithms, and software packages that were designed to operate on text could be adapted
with little or no effort to operate on DNA strings as well. More abstract examples include the long history of
research into algorithms to efficiently search, compare, and transform strings. For example, in 1974, an
algorithm for identifying the Òedit distanceÓ of two strings was discovered,84 measuring the minimumnumber of changes, transpositions, and insertions necessary to transform one string into another. Although
this algorithm was developed long before the genome era, it is useful to DNA analysis nonetheless.85Finally, the very foundation of computational theory is the Turing machine, an abstract model ofsymbolic manipulation. Some very innovative research has shown that the DNA manipulations of some
single-celled organisms are Turing-complete,86 allowing the application of a large tradition of formallanguage analysis to problems of cellular machinery.81A. Regev and E. Shapiro, ÒCellular Abstractions: Cells as Computation,Ó Nature 419(6905): 343, 2002.82D.A. MacDonaill, ÒA Parity Code Interpretation of Nucleotide Alphabet Composition,Ó Chemical Communications 18:2062-2063, 2002.83Ideally, of course, a nucleotide could be stored using only two bits (or three to include RNA nucleotides as well). ASCIItypically uses eight bits to represent characters.84R.A. Wagner and M.J. Fischer, ÒThe String-to-String Correction Problem,Ó Journal of the Association for Computing Machinery21(1):168-173, 1974.85See for example, American Mathematical Society, ÒMathematics and the Genome: Near and Far (Strings),Ó April 2002.Available at http://www.ams.org/new-in-math/cover/genome5.html; M.S. Waterman, Introduction to Computational Biology:Maps, Sequences and Genomes, Chapman and Hall, London, 1995; M.S. Waterman, ÒSequence Alignments,Ó Mathematical Methodsfor DNA Sequences, CRC, Boca Raton, FL, 1989, pp. 53-92.86L.F. Landweber and L. Kari, ÒThe Evolution of Cellular Computing: NatureÕs Solution to a Computational Problem,ÓBiosystems 52(1-3):3-13, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.88CATALYZING INQUIRYThese comments should not be taken to mean that the abstraction of DNA into a digital string is cost-free. Although digital coding of DNA is central to the mechanisms of heredity, the nucleotide sequence
cannot deal with nondigital effects that also play important roles in protein synthesis and function.
Proteins do not necessarily bind only to one specific sequence; the overall proportions of AT versus CG in
a region affect its rate of transcription; and the state of methylation of a region of DNA is an important
mechanism for the epigenetic control of gene expression (and can indeed be inherited just as the digital
code can be inherited).87 There are also numerous posttranslational modifications of proteins by processessuch as acetylation, glycosylation, and phosphorylation, which by definition are not inherent in the
genetic sequence.88 The digital abstraction also cannot accommodate protein dynamics or kinetics. Be-cause these nondigital properties can have important effects, ignoring them puts a limit on how far the
digital abstraction can support research related to gene finding and transcription regulation.Last, DNA is often compared to a computer program that drives the functional behavior of a cell.Although this analogy has some merit, it is not altogether accurate. Because DNA specifies which
proteins the cell must assemble, it is at least one step removed from the actual behavior of a cell, since
the proteinsÑnot the DNAÑthat determine (or at least have a great influence on) cell behavior.4.4.2  Proteins as Labeled Graphs
A significant problem in molecular biology is the challenge of identifying meaningful substructuralsimilarities among proteins. Although proteins, like DNA, are composed of strings made from a se-
quence of a comparatively small selection of types of component molecules, unlike DNA, proteins can
exist in a huge variety of three-dimensional shapes. Such shapes can include helixes, sheets, and other
forms generally referred to as secondary or tertiary structure.Since the structural details of a protein largely determine its functions and characteristics, determin-ing a proteinÕs overall shape and identifying meaningful structural details is a critical element of protein
studies. Similar structure may imply similar functionality or receptivity to certain enzymes or other
molecules that operate on specific molecular geometry. However, even for proteins whose three-dimen-
sional shape has been experimentally determined through X-ray crystallography or nuclear magnetic
resonance, finding similarities can be difficult due to the extremely complex geometries and large
amount of data.A rich and mature area of algorithm research involves the study of graphs, abstract representationsof networks of relationships. A graph consists of a set of nodes and a set of connections between nodes
called Òedges.Ó In different types of graphs, edges may be one-way (a Òdirected graphÓ) or two-way
(ÒundirectedÓ), or edges may also have ÒweightsÓ representing the distance or cost of the connection.
For example, a graph might represent cities as nodes and the highways that connect them as edges
weighted by the distance between the pair of cities.Graph theory has been applied profitably to the problem of identifying structural similarities amongproteins.89 In this approach, a graph represents a protein, with each node representing a single aminoacid residue and labeled with the type of residue, and edges representing either peptide bonds or close
spatial proximity. Recent work in this area has combined graph theory, data mining, and information
theoretic techniques to efficiently identify such similarities.9087For more on the influence of DNA methylation on genetic regulation, see R. Jaenisch and A. Bird, ÒEpigenetic Regulation ofGene Expression: How the Genome Integrates Intrinsic and Environmental Signals,Ó Nature Genetics 33 (Suppl):245-254, 2003.88Indeed, some work even suggests that DNA methylation and histone acetylation may be connected. See J.R. Dobosy and E.U.Selker, ÒEmerging Connections Between DNA Methylation and Histone Acetylation,Ó Cellular and Molecular Life Sciences 58(5-6):721-727, 2001.89E.M. Mitchell, P.J. Artymiuk, D.W. Rice, and P. Willet, ÒUse of Techniques Derived from Graph Theory to Compare Second-ary Structure Motifs in Proteins,Ó Journal of Molecular Biology 212(1):151-166, 1989.90J. Huan, W. Wang, A. Washington, J. Prins, R. Shah, and A. Tropsha, ÒAccurate Classification of Protein Structural FamiliesUsing Coherent Subgraph Analysis,Ó Pacific Symposium on Biocomputing 2004:411-422, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS89A significant computational aspect of this example is that since the general problem of identifyingsubgraphs is NP-complete,91 the mere inspiration of using graph theory to represent proteins is insuffi-cient; sophisticated algorithmic research is necessary to develop appropriate techniques, data representa-
tions, and heuristics that can sift through the enormous datasets in practical times. Similarly, the problem
involves subtle biological detail (e.g., what distance represents a significant spatial proximity, which
amino acids can be classified together), and could not be usefully attacked by computer scientists alone.
4.4.3Algorithms and Voluminous Datasets
Algorithms play an increasingly important role in the process of extracting information from largebiological datasets produced by high-throughput studies. Algorithms are needed to search, sort, align,
compare, contrast, and manipulate data related to a wide variety of biological problems and in support
of models of biological processes on a variety of spatial and temporal scales. For example, in the
language of automated learning and discovery, research is needed to develop algorithms for active and
cumulative learning; multitask learning; learning from labeled and unlabeled data; relational learning;
learning from large datasets; learning from small datasets; learning with prior knowledge; learning
from mixed-media data; and learning causal relationships.92The computational algorithms used for biological applications are likely to be rooted in mathematicaland statistical techniques used widely for other purposes (e.g., Bayesian networks, graph theory, principal
component analysis, hidden Markov models), but their adaptation to biological questions must address
the constraints that define biological events. Because critical features of many biological systems are not
known, algorithms must operate on the basis of working models and must frequently contend with a lack
of data and incomplete information about the system under study (though sometimes simulated data
suffices to test an algorithm). Thus, the results they provide must be regarded as approximate and provi-
sional, and the performance of algorithms must be tested and validated by empirical laboratory studies.
Algorithm development, therefore, requires the joint efforts of biologists and computer scientists.Sections 4.4.4 through 4.4.9 describe certain biological problems and the algorithmic approaches tosolving them. Far from giving a comprehensive description, these sections are intended to illustrate the
complex substrate on which algorithms must operate and, further, to describe areas of successful and
prolific collaboration between computer scientists and biologists.Some of the applications described below are focused on identifying or measuring specific at-tributes, such as the identity of a gene, the three-dimensional structure of a protein, or the degree of
genetic variability in a population. At the heart of these lines of investigation is the quest to understand
biological function, (e.g., how genes interact, the physical actions of proteins, the physiological results
of genetic differences). Further opportunities to address biological questions are likely to be as diverse
as biology itself, although work on some of those questions is only nascent at this time.4.4.4  Gene Recognition
Although the complete genomic sequences of many organisms have been determined, not all of the geneswithin those genomes have been identified. Difficulties in identifying genes from sequences of uncharacterized
DNA stem mostly from the complexity of gene organization and architecture. Just a small fraction of the
genome of a typical eukaryote consists of exons, that is, blocks of DNA that, when arranged according to their
sequence in the genome, constitute a gene; in the human genome, the fraction is estimated at less than 3 percent.91The notion of an NP-complete problem is rooted in the theory of computational complexity and has a precise technicaldefinition. For purposes of this report, it suffices to understand an NP-complete problem as one that is very difficult and wouldtake a long time to solve.92S. Thurn, C. Faloutsos, T. Mitchell, and L. Wasseterman, ÒAutomated Learning and Discovery: State-of-the-Art and ResearchTopics in a Rapidly Growing Field,Ó Summary of a Conference on Automated Learning and Discovery, Center for Automated Learn-ing and Discovery, Carnegie Mellon University, Pittsburgh, PA, 1998.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.90CATALYZING INQUIRYRegions of the genome that are not transcribed from DNA into RNA include biological signals (such aspromoters) that flank the coding sequence and regulate the geneÕs transcription. Other untranscribed regions of
unknown purpose are found between genes or interspersed within coding sequences.Genes themselves can occasionally be found nested within one another, and overlapping genes havebeen shown to exist on the same or opposite DNA strands.93 The presence of pseudogenes (nonfunctionalsequences resembling real genes), which are distributed in numerous copies throughout a genome, further
complicates the identification of true protein-coding genes.94 Finally, it is known that most genes are ulti-
mately translated into more than one protein through a process that is not completely understood. In the
process of transcription, the exons of a particular gene are assembled into a single mature mRNA. However,
in a process known as alternate splicing, various splicings omit certain exons, resulting in a family of variants
(Òsplice variantsÓ) in which the exons remain in sequence, but some are missing. It is estimated that at least
a third of human genes are alternatively spliced,95 with certain splicing arrangements occurring morefrequently than others. Protein splicing and RNA editing also play an important role. To understand gene
structures completely, all of these sequence features have to be anticipated by gene recognition tools.Two basic approaches have been established for gene recognition: the sequence similarity search, orlookup method, and the integrated compositional and signal search, or template method (also known as
ab initio gene finding).96 Sequence similarity search is a well-established computational method for generecognition based on the conservation of gene sequences (called homology) in evolutionarily related
organisms. A sequence similarity search program compares a query sequence (an uncharacterized se-
quence) of interest with already characterized sequences in a public sequence database (e.g., databases of
the Institute of Genomic Research (TIGR)97) and then identifies regions of similarity between the se-quences. A query sequence with significant similarity to the sequence of an annotated (characterized) gene
in the database suggests that the two sequences are homologous and have common evolutionary origin.
Information from the annotated DNA sequence or the protein coded by the sequence can potentially be
used to infer gene structure or function of the query sequence, including promoter elements, potential
splice sites, start and stop codons, and repeated segments. Alignment tools, such as BLAST, 98 FASTA, andSmith-Waterman, have been used to search for the homologous genes in the database.Although sequence similarity search has been proven useful in many cases, it has fundamentallimitations. Manning et al. note in their work on the protein kinase complement of the human genome93I. Dunham, L.H. Matthews, J. Burton, J.L. Ashurst, K.L. Howe, K.J. Ashcroft, D.M. Beare, et al., ÒThe DNA Sequence ofHuman Chromosome 22,Ó Nature 402(6982):489-495, 1999.94A mitigating factor is that pseudogenes are generally not conserved between species (see, for example, S. Caenepeel, G.Charydezak, S. Sudarsanam, T. Hunter, and G. Manning, ÒThe Mouse Kinome: Discovery and Comparative Genomics of All
Mouse Protein Kinases,Ó Proceedings of the National Academy of Sciences 101(32):11707-11712, 2004). This fact provides another cluein deciding which sequences represent true genes and which represent pseudogenes.95D. Brett, J. Hanke, G. Lehmann, S. Haase, S. Delbruck, S. Krueger, J. Reich, and P. Bork, ÒEST Comparison Indicates 38% ofHuman mRNAs Contain Possible Alternative Splice Forms,Ó FEBS Letters 474(1):83-86, 2000.96J.W. Fickett, ÒFinding Genes by Computer: The State of the Art,Ó Trends in Genetics 12(8):316-320, 1996.97See http://www.tigr.org/tdb/.98The BLAST 2.0 algorithm, perhaps the most commonly used tool for searching large databases of gene or protein sequences, isbased on the idea that sequences that are truly homologous will contain short segments that will match almost perfectly. BLAST wasdesigned to be fast while maintaining the sensitivity needed to detect homology in distantly related sequences. Rather than aligningthe full length of a query sequence against all of the sequences in the reference database, BLAST fragments the reference sequences intosub-sequences or ÒwordsÓ (11 nucleotides long for gene search) 
constituting a 
dictionary against which a query sequence is matched.The program creates a list of all the reference words that show up in the query sequence and then looks for pairs of those words thatoccur at adjacent positions on different sequences in the reference database. BLAST uses these ÒseedÓ positions to narrow candidatematches and to serve as the starting point for the local alignment of the query sequence. In local alignment, each nucleotide position inthe query receives a score relative to how well the query and reference sequence match; perfect matches score highest, substitutions ofdifferent nucleotides incur different penalties. Alignment is continued outward from the seed positions until the similarity of queryand reference sequences drops below a predetermined threshold. The program reports the highest scoring alignments, described by anE-value, the probability that an alignment with this score would be observed by chance. See, for example, S.F. Altschul, T.L. Madden,A.A. Schaffer, J. Zhang, Z. Zhang, W. Miller, and D.J. Lipman, ÒGapped BLAST and PSI-BLAST: A New Generation of ProteinDatabase Search Programs,Ó Nucleic Acids Research 25(17):3389-3402, 1997.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS91that although Òall 518 [kinase] genes are covered by some EST [Expressed Sequence Tag] sequence, and~90% are present in gene predictions from the Celera and public genome databases, . . . those predic-
tions are often fragmentary or inaccurate and are frequently misannotated.Ó99There are several reasons for these limitations. First, only a fraction of newly discovered sequences haveidentifiable homologous genes in the current databases.100 The proportion of vertebrate genes with no detect-able similarity in other phyla is estimated to be about 50 percent,101 and this is supported by a recent analysisof human chromosome 22, where only 50 percent of the proteins are found to be similar to previously known
proteins.102 Also, the most prominent vertebrate organisms in GenBank have only a fraction of their genomespresent in finished (versus draft, error-prone) sequences. Hence, it is obvious that sequence similarity search
within vertebrates is currently limited. Second, sequence similarity searches are computationally expensive
when query sequences have to be matched against a large number of sequences in the databases.To resolve this problem, a dictionary-based method, such as Identifier of Coding Exons (ICE), is oftenemployed. In this method, gene sequences in the reference database are fragmented into subsequences of
length k, and these subsequences make up the dictionary against which a query sequence is matched. If thesubsequences corresponding to a gene have at least m consecutive matches with a query sequence, the gene isselected for closer examination. Full-length alignment techniques are then applied to the selected gene se-
quences. The dictionary-based approach significantly reduces the processing time (down to seconds per gene).In compositional and signal search, a model (typically a hidden Markov model) is constructed thatintegrates coding statistics (measures indicative of protein coding functions) with signal detection into
one framework. An example of a simple hidden Markov model for a compositional and signal search
for a gene in a sequence sampled from a bacterial genome is shown in Figure 4.3. The model is first
ÒtrainedÓ on sequences from the reference database and generates the probable frequencies of different
nucleotides at any given position on the query sequence to estimate the likelihood that a sequence is in
a different ÒstateÓ (such as a coding region). The query sequence is predicted to be a gene if the product
of the combined probabilities across the sequence exceeds a threshold determined by probabilities
generated from sequences in the reference database.The discussion above has presumed that biological understanding does not play a role in generecognition. This is often untrueÑgene-recognition algorithms make errors of omission and commis-
sion when run against genomic sequences in the absence of experimental biological data. That is, they
fail to recognize genes that are present, or misidentify starts or stops of genes, or mistakenly insert or
delete segments of DNA into the putative genes. Improvements in algorithm design will help to reduce
these difficulties, but all the evidence to date shows that knowledge of some of the underlying science
helps even more to identify genes properly.10399G. Manning, D.B. Whyte, R. Martinez, T. Hunter, and S. Sudarsanam, ÒThe Protein Kinase Complement of the HumanGenome,Ó Science 298(5600):1912-1934, 2002.100I. Dunham, N. Shimizu, B.A. Roe, S. Chissoe, A.R. Hunt, J.E. Collins, R. Bruskiewich, et al. ÒThe DNA Sequence of HumanChromosome 22,Ó Nature 402(6761):489-495, 1999.101J.M. Claverie, ÒComputational Methods for the Identification of Genes in Vertebrate Genomic Sequences,Ó Human MolecularGenetics 6(10):1735-1744, 1999.102I. Dunham, N. Shimizu, B.A. Roe, S. Chissoe, A.R. Hunt, J.E. Collins, R. Bruskiewich, et al., ÒThe DNA Sequence of HumanChromosome 22,Ó Nature 402(6761):489-495, 1999.103This discussion is further complicated by the fact that there is no scientific consensus on the definition of a gene. RobertRobbins (Vice President for Information Technology at the Fred Hutchinson Cancer Research Center in Seattle, Washington, per-sonal communication, December 2003) relates the following story: ÒSeveral times, IÕve experienced a situation where something likethe following happens. First, you get biologists to agree on the definition of a gene so that a computer could analyze perfect dataand tell you how many genes are present in a region. Then you apply the definition to a fairly complex region of DNA to determinethe number of genes (letÕs say the result is 11). Then, you show the results to the biologists who provided the rules and you say,ÔAccording to your definition of a gene there are eleven genes present in this region.Õ The biologists respond, ÔNo, there are justthree. But they are related in a very complicated way.Õ When you then ask for a revised version of the rules that would provide aresult of three in the present example, they respond, ÔNo, the rules I gave you are fine.ÕÓ  In short, Robbins argues with cons
iderablepersuasion that if biologists armed with perfect knowledge and with their own definition of a gene cannot produce rules that willalways identify how many genes are present in a region of DNA, computers have no chance of doing so.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.92CATALYZING INQUIRY4.4.5  Sequence Alignment and Evolutionary Relationships
A remarkable degree of similarity exists among the genomes of living organisms.104 Informationabout the similarities and dissimilarities of different types of organisms presents a picture of relatedness
between species (i.e., between reproductive groups), but also must provide useful clues to the impor-
tance, structure, and function of genes and proteins carried or lost over time in different species.
ÒComparative genomicsÓ has become a new discipline within biology to study these relationships.IntergenicRegionStart Codon            (ATG)Coding RegionStop Codon
(TAA)Emission Probability
A   
C   T    G   TP = 1.0TP = 1.0Emission ProbabilityA  
C  
T  
G  TP =.9TP =.9TP =.1TP = .1.25.25.25.25.9.03.03.04FIGURE 4.3Hidden Markov model of a compositional signal and search approach for finding a gene in a bacterial
genome.The model has four features: (1) state of the sequence, of which four states are possible (coding, intergenic, start,and stop); (2) outputs, defined as the possible nucleotide(s) that can exist at any given state (A, C, T, G at codingand intergenic states; ATG and TAA at start and stop states, respectively); (3) emission possibilitiesÑthe probabil-
ity that a given nucleotide will be generated in any particular state; and (4) transition probability (TP)Ñthe proba-bility that the sequence is in transition between two states.To execute the model, emission and transition probabilities are obtained by training on the characterized genesin the reference database. The set of all possible combinations of states for the query sequence is then generated,and an overall probability for each combination of states is calculated. If the combination having the highestoverall probability exceeds a threshold determined using gene sequences in the reference database, the query
sequence is concluded to be a gene.104For example, 9 percent of E. coli genes, 9 percent of rice genes, 30 percent of yeast genes, 43 percent of mosquito genes, 75percent of zebrafish genes, and 94 percent of rat genes have homologs in humans. See http://iubio.bio. Indiana.edu:8089/all/hgsummary.html (Summary Table August 2005).Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS93Alignments of gene and protein sequences from many different organisms are used to find diagnosticpatterns to characterize protein families; to detect or demonstrate homologies between new sequences
and existing families of sequences; to help predict the secondary and tertiary structures of new se-
quences; and to serve as an essential prelude to molecular evolutionary analysis.To visualize relationships between genomes, evolutionary biologists develop phylogenetic treesthat portray groupings of organisms, characteristics, genes, or proteins based on their common ances-
tries and the set of common characters they have inherited. One type of molecular phylogenetic tree, for
example, might represent the amino acid sequence of a protein found in several different species. The
tree is created by aligning the amino acid sequences of the protein in question from different species,
determining the extent of differences between them (e.g., insertions, deletions, or substitutions of amino
acids), and calculating a measure of relatedness that is ultimately reflected in a drawing of a tree with
nodes and branches of different lengths.The examination of phylogenetic relationships of sequences from several different species gen-erally uses a method known as progressive sequence alignment, in which closely related sequences
are aligned first, and more distant ones are added gradually to the alignment. Attempts at tackling
multiple alignments simultaneously have been limited to small numbers of short sequences be-
cause of the computational power needed to resolve them. Therefore, alignments are most often
undertaken in a stepwise fashion. The algorithm of one commonly used program (ClustalW) con-
sists of three main stages. First, all pairs of sequences are aligned separately in order to calculate a
distance matrix giving the divergence of each pair of sequences; second, a guide tree is calculated
from the distance matrix; and third, the sequences are progressively aligned according to the
branching order in the guide tree.Alignment algorithms that test genetic similarity face several challenges. The basic premise of amultiple sequence alignment is that, for each column in the alignment, every residue from every se-
quence is homologous (i.e., has evolved from the same position in a common ancestral sequence). In the
process of comparing any two amino acid sequences, the algorithm must place gaps or spaces at points
throughout the sequences to get the sequences to align. Because inserted gaps are carried forward into
subsequent alignments with additional new sequences, the cumulative alignment of multiple sequences
can become riddled with gaps that sometimes result in an overall inaccurate picture of relationships
between the proteins. To address this problem, gap penalties based on a weight matrix of different
factors are incorporated into the algorithm. For example, the penalty for introducing a gap in aligning
two similar sequences is greater that that for aligning two dissimilar sequences. Gap penalties differ
depending on the length of the sequence, the types of sequence, and different regions of sequence.
Based on the weight matrix and rules for applying penalties, the algorithm compromises in the place-
ment of gaps to obtain the lowest penalty score for each alignment.The placement of a gap in a protein sequence may represent an evolutionary changeÑif a gap,reflecting the putative addition or subtraction of an amino acid to a proteinÕs structure, is introduced,
the function of the protein may change, and the change may have evolutionary benefit. However, the
change may also be insignificant from a functional point of view. Today, it is known that most insertions
and deletions occur in loops on the surface of the protein or between domains of multidomain proteins,
which means that knowledge of the three-dimensional structure or the domain structure of the protein
can be used to help identify functionally important deletions and insertions.As the structures of different protein domains and families are increasingly determined by othermeans, alignment algorithms that incorporate such information should become more accurate. More
recently, stochastic and iterative optimization methods are being used to refine individual alignments.
Also, some algorithms (e.g., Bioedit) allow users to manually edit the alignment when other information
or ÒeyeballingÓ suggests logical placement of gaps.Exploitation of complete genomic knowledge across closely related species can play an importantrole in identifying the functional elements encoded in a genome. Kellis et al. undertook a comparative
analysis of the yeast Saccharomyces cerevisiae based on high-quality draft sequences of three relatedCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.94CATALYZING INQUIRYspecies (S. paradoxus, S. mikatae, and S. bayanus).105 This analysis resulted in significant revisions of theyeast gene catalogue, affecting approximately 15 percent of all genes and reducing the total count by
about 500 genes. Seventy-two genome-wide elements were identified, including most known regula-
tory motifs and numerous new motifs, and a putative function was inferred for most of these motifs.
The power of the comparative genomic approach arises from the fact that sequences that are positively
selected (i.e., confer some evolutionary benefit or have some useful function) tend to be conserved as a
species evolves, while other sequences are not conserved. By comparing a given genome of interest to
closely related genomes, conserved sequences become much more obvious to the observer than if the
functional elements had to be identified only by examination of the genome of interest. Thus, it is
possible, at least in principle, that functional elements can be identified on the basis of conservation
alone, without relying on previously known groups of co-regulated genes or without using data from
gene expression or transcription factor binding experiments.Molecular phylogenetic trees that graphically represent the differences between species are usuallydrawn with branch lengths proportional to the amount of evolutionary divergence between the two
nodes they connect. The longer the distance between branches, the more relatively divergent are the
sequences they represent. Methods for calculating phylogenetic trees fall into two general categories: (1)
distance-matrix methods, also known as clustering or algorithmic methods, and (2) discrete data meth-
ods. In distance-matrix methods, the percentage of sequence difference (or distance) is calculated for
pairwise combinations of all points of divergence; then the distances are assembled into a tree. In
contrast, discrete data methods examine each column of the final alignment separately and look for the
tree that best accommodates all of the information, according to optimality criteriaÑfor example, the
tree that requires the fewest character state changes (maximum parsimony), the tree that best fits an
evolutionary model (maximum likelihood), or the tree that is most probable, given the data (Bayesian
inference). Finally, ÒbootstrappingÓ analysis tests whether the whole dataset supports the proposed tree
structure by taking random subsamples of the dataset, building trees from each of these, and calculating
the frequency with which the various parts of the proposed tree are reproduced in each of the random
subsamples.Among the difficulties facing computational approaches to molecular phylogeny is the fact thatsome sequences (or segments of sequences) mutate more rapidly than others.106 Multiple mutations atthe same site obscure the true evolutionary difference between sequences. Another problem is the
tendency of highly divergent sequences to group together when being compared regardless of their true
relationships. This occurs because of a background noise problemÑwith only a limited number of
possible sequence letters (20 in the case of amino acid sequences), even divergent sequences will not
infrequently present a false phylogenetic signal due strictly to chance.4.4.6  Mapping Genetic Variation Within a Species
The variation that occurs between different species represents the product of reproductive isolationand population fission over very long time scales during which many mutational changes in genes and
proteins occur. In contrast, variation within a single species is the result of sexual reproduction, genetic105M. Kellis, N. Patterson, M. Endrizzi, B. Birren, and E.S. Lander, ÒSequencing and Comparison of Yeast Species to IdentifyGenes and Regulatory Elements,Ó Nature 423(6937):241-254, 2003.106A number of interesting references to this problem can be found in the following: M.T. Holder and P.O. Lewis, ÒPhylogenyEstimation: Traditional and Bayesian Approaches,Ó Nature Reviews Genetics 4:275-284, 2003; I. Holmes and W.J. Bruno, ÒEvolu-tionary HMMs: A Bayesian approach to multiple alignment,Ó Bioinformatics 17(9):803-820, 2001; A. Siepel and D. Haussler,ÒCombining Phylogenetic and Hidden Markov Models in Biosequence Analysis,Ó in Proceedings of the Seventh Annual Interna-tional Conference on Computational Molecular Biology, Berlin, Germany, pp. 277-286, 2003; R. Durbin, S. Eddy, A. Krogh, and G.Mitchison, Biological Sequence AnalysisÑProbabilistic Models of Proteins and Nucleic Acids, Cambridge University Press, New York,1998.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS95recombination, and smaller numbers of relatively recent mutations.107 Examining the variation of geneor protein sequences between different species helps to draw a picture of the pedigree of a particular
gene or protein over evolutionary time, but scientists are also interested in understanding the practical
significance of such variation within a single species.Geneticists have been trying for decades to identify the genetic variation among individuals in thehuman species that result in physical differences between them. There is an increasing recognition of
the importance of genetic variation for medicine and developmental biology and for understanding the
early demographic history of humans.108 In particular, variation in the human genome sequence isbelieved to play a powerful role in the origins of and prognoses for common medical conditions.109The total number of unique mutations that might exist collectively in the entire human populationis not known definitively and has been estimated at upward of 10 million,110 which in a 3 billion base-pair genome corresponds to a variant every 300 bases or less. Included in these are single-nucleotide
polymorphisms (SNPs), that is, single-nucleotide sites in the genome where two or more of the four
bases (A, C, T, G) occur in at least 1 percent of the population. Many SNPs were discovered in the
process of overlapping the ends of DNA sequences used to assemble the human genome, when these
sequences came from different individuals or from different members of a chromosome pair from the
same individual. The average number of differences observed between the DNA of any two unrelated
individuals represented at 1 percent or more in the population is one difference in every 1,300 bases; this
leads to the estimation that individuals differ from one another at 2.4 million places in their genomes.111In rare cases, a single SNP has been directly associated with a medical condition, such as sickle cellanemia or cystic fibrosis. However, most common diseases such as diabetes, cancer, stroke, heart dis-
ease, depression, and arthritis (to name a few) appear to have complex origins and involve the partici-
pation of multiple genes along with environmental factors. For this reason there is interest in identifying
those SNPs occurring across the human genome that might be correlated with common medical condi-
tions. SNPs found within exons that contain genes are of greatest interest because they are believed to be
potentially related to changes in proteins that affect a predisposition to disease, but because most of the
genome does not code for proteins (and indeed a number of noncoding SNPs have been found112), thefunctional impact of many SNPs is unknown.Armed with rapid DNA sequencing tools and the ability to detect single-base differences, an inter-national consortium looked for SNPs in individuals over the last several years, ultimately identifying
more than 3 million unique SNPs and their locations on the genome in a public database. SNP maps of
the human genome with a density of about one SNP per thousand nucleotides have been developed. An
effort under way in Iceland known as deCODE seeks to correlate SNPs with human diseases.113 How-ever, determining which combinations of the 10 million SNPs are associated with particular disease
states, predisposition to disease, and genes that contribute to disease remains a formidable challenge.Some research on this problem has recently on focused on the discovery that specific combinationsof SNPs on a chromosome (called ÒhaplotypesÓ) occur in blocks that are inherited together; that is, they107D. Posada and K.A. Crandall, ÒIntraspecific Gene Genealogies: Trees Grafting into Networks,Ó Trends in Ecology and Evolu-tion 16(1):37-45, 2001.108L.L. Cavalli-Sforza and M.W. Feldman, ÒThe Application of Molecular Genetic Approaches to the Study of Human Evolu-tion,Ó Nature Genetics 33 (Suppl.):266-275, 2003.109S.B. Gabriel, S.F. Schaffner, H. Nguyen, J.M. Moore, J. Roy, B. Blumenstiel, J. Higins, et al., ÒThe Structure of HaplotypeBlocks in the Human Genome,Ó Science 296(5576):2225-2229, 2002.110L. Kruglyak and D.A. Nickerson, ÒVariation Is the Spice of Life,Ó Nature Genetics 27(3):234-236, 2001, available at http://nucleus.cshl.edu/agsa/Papers/snp/Kruglyak_2001.pdf.111The International SNP Map Working Group, ÒA Map of Human Genome Sequence Variation Containing 1.42 Million SingleNucleotide Polymorphisms,Ó Nature 409:928-933, 2001.112See, for example, D. Trikka, Z. Fang, A. Renwick, S.H. Jones, R. Chakraborty, M. Kimmel, and D.L. Nelson, ÒComplex SNP-based Haplotypes in Three Human Helicases: Implications for Cancer Association Studies,Ó Genome Research 12(4):627-639, 2002.113See www.decode.com.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.96CATALYZING INQUIRYare unlikely to be separated by recombination that takes place during reproduction. Further, only arelatively small number of haplotype patterns appear across portions of a chromosome in any given
population.114 This discovery potentially simplifies the problem of associating SNPs with disease be-cause a much smaller number of ÒtagÓ SNPs (500,000 versus the estimated 10 million SNPs) might be
used as representative markers for blocks of variation in initial studies to find correlations between
parts of the genome and common diseases. In October 2002, the National Institutes of Health (NIH)
launched the effort to map haplotype patterns (the HapMap) across the human genome.Developing a haplotype map requires determination of all of the possible tag SNP combinationsthat are common in a population, and therefore relies on data from high-throughput screening of SNPs
from a large number of individuals. A difficulty is that a haplotype represents a specific group of SNPs
on a single chromosome. However, with the exception of gametes (sperm and egg), human cells contain
two copies of each chromosome (one inherited from each parent). High-throughput studies generally
do not permit the separate, parallel examination of each SNP site on both members of an individualÕs
pair of chromosomes. SNP data obtained from individuals represent a combination of information
(referred to as the genotype) from both of an individualÕs chromosomes. For example, genotyping an
individual for the presence of a particular SNP will result in two data values (e.g., A and T). Each value
represents an SNP at the same site on both chromosomes, and recently it has become possible to
determine the specific chromosomes to which A and T belong.115There are two problems in creating a HapMap. The first is to extract haplotype informationcomputationally from genotype information for any individual. The second is to estimate haplotype
frequencies in a population. Although good approaches to the first problem are known,116 the secondremains challenging. Algorithms such as the expectation-maximization approach, Gibbs sampling
method, and partition-ligation methods have been developed to tackle this problem.Some algorithmic programs rely on the concept of evolutionary coalescence or a perfect phylog-enyÑthat is, a rooted tree whose branches describe the evolutionary history of a set of sequences (or
haplotypes) in sample individuals. In this scenario, each sequence has a single ancestor in the previous
generation, under the presumption that the haplotype blocks have not been subject to recombination,
and takes as a given that only one mutation will have occurred at any one SNP site. Given a set of
genotypes, the algorithm attempts to find a set of haplotypes that fit a perfect phylogeny (i.e., could
have originated from a common ancestor). The performance of algorithms for haplotype prediction
generally improves as the number of individuals sampled and the number of SNPs included in the
analysis increases. This area of algorithm development will continue to be a robust area of research in
the future as scientists and industry seek to associate genetic variation with common diseases.Direct haplotyping is also possible, and can circumvent many of the difficulties and ambiguitiesencountered when a statistical approach is used.117 For example, Ding and Cantor have developed atechnique that enables direct molecular haplotyping of several polymorphic markers separated by as
many as 24 kb.118 The haplotype is directly determined by simultaneously genotyping several polymor-phic markers in the same reaction with a multiplex PCR and base extension reaction. This approach
does not rely on pedigree data and does not require previous amplification of the entire genomic region
containing the selected markers.114E.S. Lander, L.M. Linton, B. Birren, C. Nusbaum, M.C. Zody, J. Baldwin, et al., ÒInitial Sequencing and Analysis of theHuman Genome,Ó Nature 409(6822):860-921, 2001.115C. Ding and C.R. Cantor, ÒDirect Molecular Haplotyping of Long-range Genomic DNA with M1-PCR,Ó Proceedings of theNational Academy of Sciences 100(13):7449-7453, 2003.116See, for example, D. Gusfield, ÒInference of Haplotypes from Samples of Diploid Populations: Complexity and Algo-rithms,Ó Journal of Computational Biology 8(3):305-323, 2001.117J. Tost, O. Brandt, F. Boussicault, D. Derbala, C. Caloustian, D. Lechner, and I.G. Gut, ÒMolecular Haplotyping at HighThroughput,Ó Nucleic Acids Research 30(19):e96, 2002.118C. Ding and C.R. Cantor, ÒDirect Molecular Haplotyping of Long-range Genomic DNA with M1-PCR,Ó Proceedings of theNational Academy of Sciences 100(13):7449-7453, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS97Finally, in early 2005, National Geographic and IBM announced a collaboration known as the theGenographic Project to probe the migratory history of the human species.119 The project seeks to collect 100,000blood samples from indigenous populations, with the intent of analyzing DNA in these samples. Ultimately,
the project will create a global database of human genetic variation and associated anthropological data
(language, social customs, etc.) that provides a snapshot of human genetic variation before the cultural context
of indigenous populations is lostÑa context that is needed to make sense of the variations in DNA data.4.4.7  Analysis of Gene Expression Data
Although almost all cells in an organism contain the same genetic material (the genomic blueprint forthe entire organism), only about one-third of a given cellÕs genes are expressed or Òswitched onÓÑthat is,
are producing proteinsÑat a given time. Expressed genes account for differences in cell types; for ex-
ample, DNA in skin cells produces a different set of proteins than DNA in nerve cells. Similarly, a
developing embryo undergoes rapid changes in the expression of its genes as its body structure unfolds.
Differential expression in the same types of cells can represent different cellular ÒphenotypesÓ (e.g.,
normal versus diseased), and modifying a cellÕs environment can result in changed levels of expression of
a cellÕs genes. In fact, the ability to perturb a cell and observe the consequential changes in expression is a
key to understanding linkages between genes and can be used to model cell signaling pathways.A powerful technology for monitoring the activity of all the genes in a cell is the DNA microarray(described in Box 7.5 in Chapter 7). Many different biological questions can be asked with microarrays,
and arrays are now constructed in many varieties. For example, instead of DNA across an entire
genome, the array might be spotted with a specific set of genes from an organism or with fabricated
sequences of DNA (oligonucleotides) that might represent, for example, a particular SNP or a mutated
form of a gene. More recently, protein arrays have been developed as a new tool that extends the reach
of gene expression analysis.The ability to collect and analyze massive sets of data about the transcriptional states of cells is anemerging focus of molecular diagnostics as well as drug discovery. Profiling the activation or suppres-
sion of genes within cells and tissues provides telling snapshots of function. Such information is critical
not only to understand disease progression, but also to determine potential routes for disease interven-
tion. New technologies that are driving the field include the creation of ÒdesignerÓ transcription factors
to modulate expression, use of laser microdissection methods for isolation of specific cell populations,
and technologies for capturing mRNA. Among the questions asked of microarrays (and the computa-
tional algorithms to decipher the results) are the discrimination of genes with significant changes in
expression relative to the presence of a disease, drug regimen, or chemical or hormonal exposure.To illustrate the power of large-scale analysis of gene data, an article in Science by Gaudet andMango is instructive.120 A comparison of microarray data taken from Caenorhabditis elegans embryoslacking a pharynx with microarray data from embryos having excess pharyngeal tissue identified 240
genes that were preferentially expressed in the pharynx, and further identified a single gene as directly
regulating almost all of the pharynx-specific genes that were examined in detail. These results suggest
the possibility that direct transcriptional regulation of entire gene networks may be a common feature of
organ-specification genes.121119More information on the project can be found at http://www5.nationalgeographic.com/genographic/.120J. Gaudet and S.E. Mango, ÒRegulation of Organogenesis by the Caenorhabditis elegans FoxA Protein PHA-4,Ó Science295(5556):821-825, 2002.121For example, it is known that a specific gene activates other genes that function at two distinct steps of the regulatoryhierarchy leading to wing formation in Drosophila (K.A. Guss, C.E. Nelson, A. Hudson, M. E. Kraus and S. B. Carroll, ÒControl ofa Genetic Regulatory Network by a Selector Gene,Ó Science 292(5519):1164-1167, 2001), and also that the presence of specificfactor is both necessary and sufficient for specification of eye formation in Drosophila imaginal discs, where it directly activatesthe expression of both early- and late-acting genes (W.J. Gehring and K. Ikeo, ÒPax 6: Mastering Eye Morphogenesis and Evolu-tion,Ó Trends in Genetics 15(9):371-377, 1999).Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.98CATALYZING INQUIRYMany analytic techniques have been developed and applied to the problem of revealing biologicallysignificant patterns in microarray data. Various statistical tests (e.g., t-test, F-test) have been developed
to identify genes with significant changes in expression (out of thousands of genes); such genes have
had widespread attention as potential diagnostic markers or drug targets for disease, stages of develop-
ment, and other cellular phenotypes. Many classification tools (e.g., FisherÕs Discriminant Analysis,
Bayesian classifier, artificial neural networks, tools from signal processing) have also been developed to
build a phenotype classifier with the genes differentially expressed. These classification tools are gener-
ally used to discriminate known sample groups from each other using differentially expressed genes
selected by statistical testing.Other algorithms are necessary because data acquired through microarray technology often haveproblems that must be managed prior to use. For example, the quality of microarray data is highly
dependent on the way in which a sample is prepared. Many factors can affect the extent to which a dot
fluoresces, of which the transcription level of the particular gene involved is only one. Such extraneous
factors include the sampleÕs spatial homogeneity, its cleanliness (i.e., lack of contamination), the sensi-
tivity of optical detectors in the specific instrument, varying hybridization efficiency between clones,
relative differences between dyes, and so forth. In addition, because different laboratories (and different
technicians) often have different procedures for sample preparation, datasets taken from different labo-
ratories may not be strictly comparable. Statistical methods of analysis of variance (ANOVA) have been
applied to deal with these problems, using models to estimate the various contributions to relative
signal from the many potential sources. Importantly, these models not only allow researchers to attach
measures of statistical significance to data, but also suggest improved experimental designs.122An important analytical task is to identify groups of genes with similar expression patterns. Thesegroups of genes are more likely to be involved in the same cellular pathways, and many data-driven
hypotheses about cellular regulatory mechanisms (e.g., disease mechanisms) have been drawn under
this assumption. For this purpose, various clustering methods, such as hierarchical clustering methods,
self-organizing maps (trained neural networks), and COSA (Clustering Objects on Subsets of Attributes),
have been developed. The goal of cluster analysis is to partition a dataset of N objects into subgroupssuch that these objects are more similar to those in their subgroups than to those in other groups.
Clustering tools are generally used to identify groups of genes that have similar expression pattern
across samples; thus, it is reasonable to suppose that the genes in each group (or cluster) are involved in
the same biological pathway. Most clustering methods are iterative and involve the calculation of a
notional distance between any two data points; this distance is used as the measure of similarity. In
many implementations of clustering, the distance is a function of all of the attributes of each sample.Agglomerative hierarchical clustering begins with assigning N clusters for N samples, where allsamples are defined as different individual clusters. Potential clusters are arranged in a hierarchy
displayed as a binary tree or Òdendrogram.Ó Euclidian distance or Pearson correlation is used with
Òaverage linkingÓ to develop the dendrogram. For example, two clusters that are closest to each other in
terms of Euclidean distance are combined to form a new cluster, which is represented as the average of
two groups combined (average linkage). This process is continued until there is one cluster to which all
samples belong. In the process of forming the single cluster, the overall structure of clusters is evaluated
for whether the merging of two clusters into one new cluster decreases both the sum of the similarity
within all of the clusters and the sum of differences between all of the clusters. The clustering procedure
stops at the level at which these are equal.Self-organizing maps (SOMs)123 are another form of cluster analysis. With SOMs, a number ofdesired clusters is decided in advance, and a geometry of nodes (such as an N × M grid) is created,where each node represents a single cluster. The nodes are randomly placed in the data space. Then, in122M. Kerr, M. Martin, and G. Churchill, ÒAnalysis of Variance for Gene Expression Microarray Data,Ó Journal of ComputationalBiology 7(6):819-837, 2000.123T. Kohonen, Self-Organizing Maps, Second Edition, Springer, Berlin, 1997.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS99a random order, each data point is selected. At each iteration, the nodes move closer to the selected datapoint, with the distance moved influenced by the distance from the data point to the node and the
iteration number. Thus, the closest node will move the most. Over time, the initial geometry of the
nodes will deform and each node will represent the center of an identified cluster. Experimentation is
often necessary to arrive at a useful number of nodes and geometry, but since SOMs are computationally
tractable, it is feasible to run many sessions. The properties of SOMsÑpartially structured, scalable to
large datasets, unsupervised, easily visualizableÑmake them well suited for analysis of microarray
data, and they have been used successfully to detect patterns of gene expression.124In contrast to the above two methods, COSA is based on the assumption that better clustering canbe achieved if only relevant genes are used in individual clusters. This is consistent with the idea of
identifying differentially expressed genes (relevant genes) and then using only those genes to build a
classifier. The search algorithm in COSA identifies an optimal set of variables that should be used to
group individual clusters and which clusters should be merged when their similarity is assessed using
the optimal set of variables identified. This idea was implemented by adding weights reflecting contri-
butions of all genes to producing a particular set of sample clusters, and the search algorithm is then
formulated as an optimization problem. The clustering results by COSA indicate that a subset of genes
makes a greater contribution to a particular sample cluster than to other clusters.125Clustering methods are being used in many types of studies. For example, they are particularlyuseful in modeling cell networks and in clustering disparate kinds of data (e.g., RNA data and non-
RNA data; sequence data and protein data). Clustering can be applied to evaluate how feasible a given
network structure is. Also, clustering is often combined with perturbation analysis to explore a set of
samples or genes for a particular purpose. In general, clustering can be useful in any study in which
local analyses with groups of samples or genes identified by clustering improve the understanding of
the overall system.Biclustering is an alternate approach to revealing meaningful patterns in the data.126 It seeks toidentify submatrices in which the set of values has a low mean-squared residue, meaning that the each
value is reasonably coherent with other members in its row and column. (However, excluding meaning-
less solutions with zero area, this problem is unfortunately NP-complete.) Advantages of this approach
include that it can reveal clusters based on a subset of attributes, it simultaneously clusters genes with
similar expression patterns and conditions with similar expression patterns, and most importantly,
clusters can overlap. Since genes are often involved in multiple biological pathways, this can be used to
reveal linkages that otherwise would be obscured by traditional cluster analysis.While many analyses of microarray data consider a single snapshot in time, of course expressionlevels vary over time, especially due to the cellular life cycle. A challenge in analyzing microarray time-
series data is that cell cycles may be unsynchronized, making it difficult to correctly identify correla-
tions between data samples that have similar expression behavior. Statistical techniques can identify
periodicity in series and look for phase-shifted correlations between pairs of samples,127 as well as moretraditional clustering analysis.A separate set of analytic techniques is referred to as supervised methods, in contrast to clusteringand similar methods that run with no incoming assumptions. Supervised methods, in contrast, use
existing knowledge of the dataset to classify data into one of a set of classes. In general, these techniques124P. Tamayo, D. Slonim, J. Mesirov, Q. Zhu, S. Kitareewwan, E. Dmitrovsky, E.S. Lander, and T.R. Golub, ÒInterpretingPatterns of Gene Expression with Self-organizing maps: Methods and Application to Hematopoietic Differentiation,Ó Proceedingsof the National Academy of Sciences 96(6):2907-2912, 1999.125J.H. Friedman and J.J. Meulman, ÒClustering Objects on Subsets of Attributes,Ó Journal of the Royal Statistical Society Series B66(4):815-849(34), 2004.126Y. Cheng and G.M. Church, ÒBiclustering of Expression Data,Ó Proceedings of the Eighth International Conference on IntelligentSystems for Molecular Biology 8:93-103, 2000.127V. Filkov, S. Skiena, and J. Zhi, ÒAnalysis Techniques for Microarray Time-Series Data,Ó Journal of Computational Biology9(2):317-330. Available at http://www.cs.ucdavis.edu/~filkov/papers/spellmananalysis.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.100CATALYZING INQUIRYrely on training sets provided by the researchers, where the class membership of data is provided. Then,when presented with experimental data, supervised methods apply the learning from the training set to
perform similar classifications. One such technique is support vector machines (SVMs), which are
useful for highly multidimensional data. SVMs map the data into a Òfeature spaceÓ and then create
(through one of a large number of possible algorithms) a hyperplane that separates the classes. Another
common method is Artificial Neural Nets (see XREF), which train on a dataset with defined class
membership; if the neural network classifies a member of the training set incorrectly, the error back-
propagates through the system and updates the weightings. Unsupervised and supervised methods can
be combined for ÒsemisupervisedÓ learning methods, in which heterogeneous training data can be both
classified and unclassified.128However, there is no analytic method optimal to any dataset. Thus, it would be useful to develop ascheme that can guide users to choose an appropriate method (e.g., in hierarchical clustering, an appro-
priate set of similarity measure, linkage method, and the measure used to determine the number of
clusters) to achieve a reasonable analysis of their own datasets.Ultimately, it is desirable to go beyond correlations and associations in the analysis of gene expres-sion data to seek causal relationships. It is an elementary truism of statistics that indications of correla-
tion are not by themselves indicators of causalityÑan experimental manipulation of one of more vari-
ables is always necessary to conclude a causal relationship. Nevertheless, analysis of microarray data
can be helpful in suggesting experiments that might be particularly fruitful in uncovering causal rela-
tionships. Bayesian analysis allows one to make inferences about the possible structure of a genetic
regulatory pathway on the basis of microarray data, but even advocates of such analysis recognize the
need for experimental test. One work goes so far as to suggest that it is possible that automated
processing of microarray data can suggest interesting experiments that will shed light on causal rela-
tionships, even if the existing data themselves donÕt support causal inferences.1294.4.8  Data Mining and Discovery
4.4.8.1  The First Known Biological Discovery from Mining Databases
130By the early 1970s, the simian sarcoma virus had been determined to cause cancer in certain speciesof monkeys. In 1983, the responsible oncogene within the virus was sequenced. At around the same
time, and entirely independently, a partial amino acid sequence of an important growth factor in
humansÑthe platelet-derived growth factor (PDGF) was also determined. PDGF was known to cause
cultured cells to proliferate in a cancer-like manner. Russell Doolittle compared the two sequences and
found a high degree of similarity between them, indicating a possible connection between an oncogene
and a normal human gene. In this case, the indication was that the simian sarcoma virus acted on cells
in monkeys in a manner similar to the action of PDGF on human cells.128T. Li, S. Zhu, Q. Li, and M. Ogihara, ÒGene Functional Classification by Semisupervised Learning from HeterogeneousData,Ó pp. 78-82 in Proceedings of the ACM Symposium on Applied Computing, ACM Press, New York, 2003.129C. Yoo and G. Cooper, ÒAn Evaluation of a System That Recommends Microarray Experiments to Perform to DiscoverGene-regulation Pathways,Ó Artificial Intelligence in Medicine 31(2):169-182, 2004, available at http://www.phil.cmu.edu/projects/genegroup/papers/yoo2003a.pdf.130Adapted from S.G.E. Andersson and L. Klasson, ÒNavigating Through the Databases,Ó available at http://artedi.ebc.uu.se/course/overview/navigating_databases.html. The original Doolittle article was published as R.F. Doolittle, M.W. Hunkapiller,L.E. Hood, S.G. Davare, K.C. Robbins, S.A. Aaronson, and H.N. Antoniades, ÒSimian Sarcoma Virus onc Gene, v-sis, Is Derived
from the Gene (or Genes) Encoding a Platelet-derived Growth Factor,Ó Science 221(4607):275-277, 1983.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS1014.4.8.2  A Contemporary Example: Protein Family Classification and
Data Integration for Functional Analysis of ProteinsNew bioinformatics methods allow inference of protein function using associative analysis (Òguiltby associationÓ) of functional properties to complement the traditional sequence homology-based meth-
ods.131 Associative properties that have been used to infer function not evident from sequence homol-ogy include co-occurrence of proteins in operons or genome context; proteins sharing common domains
in fusion proteins; proteins in the same pathway, subcellular network, or complex; proteins with corre-
lated gene or protein expression patterns; and protein families with correlated taxonomic distribution
(common phylogenetic or phyletic patterns).Coupling protein classification and data integration allows associative studies of protein family,function, and structure.132 An example is provided in Figure 4.4, which illustrates how the collectiveuse of protein family, pathway, and genome context in bacteria helped researchers to identify a long-
sought human gene associated with the methylmalonic aciduria disorder.Domain-based or structural classification-based searches allow identification of protein familiessharing domains or structural fold classes. Functional convergence (unrelated proteins with the same
activity) and functional divergence are revealed by the relationships between the enzyme classification
and protein family classification. With the underlying taxonomic information, protein families that
occur in given lineages can be identified. Combining phylogenetic pattern and biochemical pathway
information for protein families allows identification of alternative pathways to the same end product
in different taxonomic groups, which may present attractive potential drug targets. The systematic
approach for protein family curation using integrative data leads to novel prediction and functional
inference for uncharacterized ÒhypotheticalÓ proteins, and to detection and correction of genome anno-
tation errors (a few examples are listed in Table 4.2). Such studies may serve as a basis for further
analysis of protein functional evolution, and its relationship to the coevolution of metabolic pathways,
cellular networks, and organisms.Underlying this approach is the availability of resources that provide analytical tools and data. Forexample, the Protein Information Resource (PIR) is a public bioinformatics resource that provides an
advanced framework for comparative analysis and functional annotation of proteins. PIR recently
joined the European Bioinformatics Institute and Swiss Institute of Bioinformatics to establish
UniProt,133 an international resource of protein knowledge that unifies the PIR, Swiss-Prot, and TrEMBL
databases. Central to the PIR-UniProt functional annotation of proteins is the PIRSF (SuperFamily)
classification system134 that provides classification of whole proteins into a network structure to reflecttheir evolutionary relationships. This framework is supported by the iProClass integrated database of
protein family, function, and structure,135 which provides value-added descriptions of all UniProt pro-teins with rich links to more than 50 other databases of protein family, function, pathway, interaction,
modification, structure, genome, ontology, literature, and taxonomy. As a core resource, the PIR envi-
ronment is widely used by researchers to develop other bioinformatics infrastructures and algorithms
and to enable basic and applied scientific research, as shown by examples in Table 4.3.131E.M. Marcotte, M. Pellegrini, M.J. Thompson, T.O. Yeates, and D. Eisenberg, ÒCombined Algorithm for Genome-widePrediction of Protein Function,Ó Nature 402(6757):83-86, 1999.132C.H. Wu, H. Huang, A. Nikolskaya, Z. Hu, and W.C. Barker, ÒThe iProClass Integrated Database for Protein FunctionalAnalysis,Ó Computational Biology and Chemistry 28(1):87-96, 2004.133R. Apweiler, A. Bairoch, C.H. Wu, W.C. Barker, B. Boeckmann, S. Ferro, E. Gasteiger, et al., ÒUniProt: Universal ProteinKnowledgebase,Ó Nucleic Acids Research 32(Database issue):D115-D119, 2004.134C.H. Wu, A. Nikolskaya A, H. Huang, L.S. Yeh, D.A. Natale, C.R. Vinayaka, Z.Z. Hu, et al., ÒPIRSF Family ClassificationSystem at the Protein Information Resource,Ó Nucleic Acids Research 32(Database issue):D112-D114, 2004.135C.H. Wu, H. Huang, A. Nikolskaya, Z. Hu, and W.C. Barker, ÒThe iProClass Integrated Database for Protein FunctionalAnalysis,Ó Computational Biology and Chemistry 28(1):87-96, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.102CATALYZING INQUIRYFIGURE 4.4Integration of protein family, pathway, and genome context data for disease gene identification.
The ATR enzyme (EC 2.5.1.17) converts inactive cobalamins to AdoCbl (A), a cofactor for enzymes in severalpathways, including diol/glycerol dehydratase (EC 4.2.1.28) (B) and methylmalonyl-CoA mutase (MCM) (EC5.4.99.2) (C). Many prokaryotic ATRs are predicted to be required for EC 4.2.1.28 based on the genome context of
the corresponding genes. However, in at least one organism (Archaeoglobus fulgidus), the ATR gene is adjacent tothe MCM gene, which provided a clue for cloning the human and bovine ATRs.SOURCE: Courtesy of Cathy Wu, Georgetown University.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS1034.4.9  Determination of Three-dimensional Protein Structure
One central problem of proteomics is that of protein folding. Protein folding is one of the mostimportant cellular processes because it produces the final conformation required for a protein to attain
biological activity. Diseases such as AlzheimerÕs disease or bovine spongiform encephalopathy (BSE, or
ÒMad CowÓ disease) are associated with the improper folding of proteins. For example, in BSE the
protein (called the scrapie prion), which is soluble when it folds properly, becomes insoluble when one
of the intermediates along its folding pathway misfolds and forms an aggregation that damages nerve
cells.136Due to the importance of the functional conformation of proteins, many efforts have been at-tempted to predict computationally a three-dimensional structure of a protein from its amino acid
sequence. Although experimental determination of protein structure based on X-ray crystallography
and nuclear magnetic resonance yields protein structures in high resolution, it is slow, labor-intensive,
and expensive and thus not appropriate for large-scale determination. Also, it can apply only to al-
ready-synthesized or isolated proteins, while an algorithm could be used to predict the structure of a
great number of potential proteins.TABLE 4.2Protein Family Classification and Integrative Associative Analysis for Functional
AnnotationSuperfamily ClassificationDescription
A. Functional inference of uncharacterized hypothetical proteinsSF034452TIM-barrel signal transduction protein
SF004961Metal-dependent hydrolase
SF005928Nucleotidyltransferase

SF005933ATPase with chaperone activity
and inactive LON protease domainSF005211alpha/beta hydrolase

SF014673Lipid carrier protein
SF005019[Ni,Fe]-Hydrogenase-3-type complex,
membrane protein EhaAB. Correction or improvement of genome annotationsSF025624Ligand-binding protein with an ACT domain

SF005003Inactive homologue of metal-dependent
proteaseSF000378Glycyl radical cofactor protein YfiD

SF000876Chemotaxis response regulator
methylesterase CheBSF000881Thioesterase, type II

SF002845Bifunctional tetrapyrrole methylase and
MazG NTPaseC. Enhanced understanding of structure, function, evolutionary relationshipsSF005965Chorismate mutase, AroH class
SF001501Chorismate mutase, AroQ class,
prokaryotic typeNOTE: PIRSF protein family reports detail supporting evidence for both experimentally validated and computationally pre-
dicted annotations.136See, for example, C.M. Dobson, ÒProtein Misfolding, Evolution and Disease,Ó Trends in Biochemical Science 24(9):329-332,1999; C.M. Dobson, ÒProtein Folding and Its Links with Human Disease.Ó Biochemical Society Symposia 68:1-26, 2001; C.M. Dob-son, ÒProtein Folding and Misfolding,Ó Nature 426(6968):884-890, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.104CATALYZING INQUIRYProtein structures predicted in high resolution can help characterize the biological functions ofproteins. Biotechnology companies are hoping to accelerate their efforts to discover new drugs that
interact with proteins by using structure-based drug design technologies. By combining computational
and combinatorial chemistry, researchers expect to find more viable leads. Algorithms create molecular
structure built de novo to optimize interactions within the proteinÕs active sites. The use of so-called
virtual screening in combination with studies of co-crystallized drugs and proteins could be a powerful
tool for drug development.A number of tools for protein structure prediction have been developed, and progress in predictionby these methods has been evaluated by the Critical Assessment of Protein Structure Prediction (CASP)
experiment held every two years since 1994.137 In a CASP experiment, the amino acid sequences ofproteins whose experimentally determined structures have not yet been released are published, and
computational research groups are then invited to predict structures of these target sequences using
their methods and any other publicly available information (e.g., known structures that exist in the
Protein Data Bank (PDB), the data repository for protein structures). The methods used by the groupsTABLE 4.3Algorithms, Databases, Analytical Systems, and Scientific Research Enabled by the PIR
ResourceResourceTopicReference
AlgorithmBenchmarking for sequence similarity searchPearson,
 J. Mol. Biol. 276:71-84,statistics1998
PANDORA keyword-based analysis of proteinsKaplan, 
Nucleic Acids Research31:5617-5626, 2003Computing motif correlations for structureHorng et al., 
J. Comp. Chem.prediction24(16):2032-2043, 2003
DatabaseNESbase database of nuclear export signalsla Cour et al., 
Nucleic AcidsResearch 31(l):393-396, 2003TMPDB database of transmembrane topologiesIkeda et al., 
Nucleic AcidsResearch 31:406-409, 2003SDAP database and tools for allergenic proteinsIvanciuc et al., 
Nucleic AcidsResearch 31:359-362, 2003SystemSPINE 2 system for collaborative structuralGoh et al., 
Nucleic AcidsproteomicsResearch 31:2833-2838, 2003ERGOTM genome analysis and discovery systemOverbeek et al., 
Nucleic AcidsResearch 31(l):164-171, 2003Automated annotation pipeline and cDNAKasukawa et al., 
Genome Res.annotation system13(6B):1542-1551, 2003
Systers, GeneNest, SpliceNest from genome toKrause et al., 
Nucleic AcidsproteinResearch 30(l):299-300, 2002ResearchIntermediate filament proteins duringPrasad et al., 
Int. J. Oncol.carcinogenesis or apoptosis14(3):563-570, 1999
Conserved pathway by global protein networkKelley et al., 
PNASalignment100(20):11394-11399, 2003
Membrane targeting of phospholipase CSingh and Murray, 
Protein Sci.pleckstrin12:1934-1953, 2003
Analysis of human and mouse cDNA sequencesStrausberg et al., 
PNAS99(26):16899-16903, 2002A novel Schistosoma mansoni G protein-coupledHamdan et al.,
 Mol. Biochem.receptorParasitol. 119(l):75-86, 2002Proteomics reveals open reading frames (ORFs)Jungblut et al., 
Infect. Immunol.in Mycobacterium tuberculosis69(9):5905-5907, 2001137See http://predictioncenter.llnl.gov/.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS105can be divided into three areas depending on the similarity of the target protein to proteins of knownstructure: comparative (also known as homology) modeling, fold recognition (also known as thread-
ing), and de novo/new fold methods (also known as ab initio). This traditional division of prediction
methods has become blurred as the methods in each category incorporate detailed information used by
methods in the other categories.In comparative (or homology) modeling, one or more template proteins of known structure withhigh sequence homology (greater than 25 percent sequence identity) to the target sequence are identi-
fied. The target and template sequences are aligned through multiple sequence alignment (similar to
comparative genomics), and a three-dimensional structure of the target protein is generated from the
coordinates of the aligned residues of the template proteins. Finally, the model is evaluated using a
variety of criteria, and if necessary, the alignment and the three-dimensional model are refined until a
satisfactory model is obtained.If no reliable template protein can be identified from sequence homology alone, the predictionproblem is denoted as a fold recognition (or threading) problem. The primary goal is to identify one or
more folds in the template proteins that are consistent with the target sequence. In the classical thread-
ing methods, known as Òrigid body assembly,Ó a model is constructed from a library of known core
regions, loops, side chains, and folds, and the target sequence is then threaded onto the known folds.
After evaluating how well the model fits the known folds, the best fit is chosen. The assumption in fold
recognition is that only a finite number of folds exist and most existing folds can be identified from
known structures in the PDB. Indeed, as new sequences are deposited and more protein structures are
solved, there appear to be fewer and fewer unique folds. When two sequences share more than 25
percent similarity (or sequence identity), their structures are expected to have similar folds. However,
there are still remaining issues such as the high rate of false positives in fold recognition, and therefore,
the resulting alignment with the fold structure is poor. At 30 percent sequence identity, the fraction of
incorrectly aligned residues is about 20 percent, and the number rises sharply with further decreases in
sequence similarity. This limits the usefulness of comparative modeling.138If no template structure (or fold) can be identified with confidence by sequence homology methods,the target sequence may be modeled using new fold prediction methods. The goal in this prediction
method rests on the biological assumption that proteins adopt their lowest free energy conformation as
their functional state. Thus, computational methods to predict structure ab initio comprise three ele-
ments: (1) protein geometry, (2) potential energy functions, and (3) an energy space search method
(energy minimization method). First, setting protein geometry involves determining the number of
particles to be used to represent the protein structure (for example, all-atom, united-atom, or virtual-
atom model) and the nature of the space where atoms can be allocated (e.g., continuous (off-lattice) or
discrete (lattice) model). In a simple ab initio folding such as a virtual-atom lattice model, one virtual
atom represents a number of atoms in a protein (i.e., the backbone is represented as a sequence of alpha
carbons) and an optimization method searches only the predetermined lattice points for positions of the
virtual atoms to minimize the energy functions. Second, the potential energy functions in ab initio
models include covalent terms, such as bond stretching, bond angle stretching, improper dihedrals, and
torsional angles, and noncovalent terms, such as electrostatic and van der Waals forces. The use of
molecular mechanics for refinement in comparative modeling is equivalent to ab initio calculation using
all atoms in an off-lattice model. Third, many optimizations tools, such as genetic algorithms, Monte
Carlo, simulated annealing, branch and bound, and successive quadratic programming (SQP), have
been used to search for the global minimum in the energy (or structure) spaces with a number of local
minima. These approaches have provided encouraging results, although the performance of each
method may be limited by the shape of the energy space.138T. Head-Gordon and J. Wooley, ÒComputational Challenges in Structural and Functional Genomics,Ó IBM Systems Journal40(2):265-296, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.106CATALYZING INQUIRYBeyond studies of protein structure is the problem of describing a solvent environment (such aswater) and its influence on a proteinÕs conformational behavior. The importance of hydration in protein
stability and folding is widely accepted. Models are needed to incorporate the effects of solvents in
protein three-dimensional structure.4.4.10  Protein Identification and Quantification from Mass Spectrometry
A second important problem in proteomics is protein identification and quantification. That is,given a particular biological sample, what specific proteins are present and in what quantities? This
problem is at the heart of studying proteinÐprotein interactions at proteomic scale, mapping various
organelles, and generating quantitative protein profiles from diverse species. Making inferences about
protein identification and abundance in biological samples is often challenging, because cellular
proteomes are highly complex and because the proteome generally involves many proteins at relatively
low abundances. Thus, highly sensitive analytical techniques are necessary.Today, techniques based on mass spectrometry increasingly fill this need. The mass spectrometerworks on a biological sample in ionized gaseous form. A mass analyzer measures the mass-to-charge
ratio (m/z) of the ionized analytes, and a detector measures the number of ions at each m/z value. In
the simplest case, a procedure known as peptide mass fingerprinting (PMF) is used. PMF is based on the
fact that a protein is composed of multiple peptide groups, and identification of the complete set of
peptides will with high probability characterize the protein in question. After enzymatically breaking
up the protein into its constituent peptides, the mass spectrometer is used to identify individual pep-
tides, each of which has a known mass. The premise of PMF is that only a very few (one in the ideal case)
proteins will correspond to any particular set of peptides, and protein identification is effected by
finding the best fit of the observed peptide masses to the calculated masses derived from, say, a se-
quence database. Of course, the Òbest fitÓ is an algorithmic issue, and a variety of approaches have been
taken to determine the most appropriate algorithms.The applicability of PMF is limited when samples are complex (that is, when they involve largenumbers of proteins at low abundances). The reason is that only a small fraction of the constituent
peptides are typically ionized, and those that are observed are usually from the dominant proteins in
the mixture. Thus, for complex samples, multiple (tandem) stages of mass spectrometry may be neces-
sary. In a typical procedure, peptides from a database are scored on the likelihood of their generating a
tandem mass spectrum, and the top scoring peptide is chosen. This computational approach has shown
great success, and contributed to the industrialization of proteomics.However, much remains to be done. First, the generation of the spectrum is a stochastic processgoverned by the peptide composition, and the mass spectrometer. By mining data to understand these
fragmentation propensities, scoring and identification can be further improved. Second, if the peptide is
not in the database, de novo or homology-based methods must be developed for identification. Many
proteins are post-translationally modified, with the modifications changing the mass composition.
Enumeration and scoring of all modifications leads to a combinatorial explosion that must be addressed
using novel computational techniques. It is fair to say that computation will play an important role in
the success of mass spectrometry as the tool of choice for proteomics.Mass spectrometry is also coming into its own for protein expression studies. The major problem hereis that the intensity of a peak depends not only on the peptide abundance, but also on the physico-
chemical properties of the peptide. This makes it difficult to measure expression levels directly. However,
relative abundance can be measured using the proven technique of stable-isotope dilution. This method
makes use of the facts that pairs of chemically identical analytes of different stable-isotope composition
can be differentiated in a mass spectrometer owing to their mass difference, and that the ratio of signal
intensities for such analyte pairs accurately indicates the abundance ratio for the two analytes.This approach shows great promise. However, computational methods are needed to correlate dataacross different experiments. If the data were produced using liquid chromatography coupled withCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS107mass spectrometry, a peptide pair could be approximately labeled by its retention time in the column,and its mass-to-charge ratio. Such pairs can be matched across experiments using geometric matching.
Combining the relative abundance levels from different experiments using statistical methods will
greatly help in improving the reliability of this approach.4.4.11Pharmacological Screening of Potential Drug Compounds
139The National Cancer Institute (NCI) has screened more than 60,000 compounds against a panel of60 human cancer cell lines. The extent to which any single compound inhibits growth in any given cell
line is simply one data point relevant to that compound-cell line combinationÑnamely the concentra-
tion associated with a 50 percent inhibition in the growth of that cell line. However, the pattern of such
values across all 60 cell lines can provide insight into the mechanisms of drug action and drug resis-
tance. Combined with molecular structure data, these activity patterns can be used to explore the NCI
database of 460,000 compounds for growth-inhibiting effects in these cell lines, and can also provide
insight into potential target molecules and modulators of activity in the 60 cell lines. Based on this
approach, five compounds have been screened in this manner and selected for entry into clinical trials.This approach to drug discovery and molecular pharmacology serves a number of useful functions.According to Weinstein et al.,(i)It suggests novel targets and mechanisms of action or modulation.
(ii)It detects inhibition of integrated biochemical pathways not adequately represented by any single

molecule or molecular interaction. (This feature of cell-based assays is likely to be more important in thedevelopment of therapies for cancer than it is for most other diseases; in the case of cancer, one is fightingthe plasticity of a poorly controlled genome and the selective evolutionary pressures for development of
drug resistance.)(iii)It provides candidate molecules for secondary testing in biochemical assays; conversely, it provides a
well-characterized biological assay in vitro for compounds emerging from biochemical screens.
(iv)It ÔÔfingerprintsÕÕ tested compounds with respect to a large number of possible targets and modula-
tors of activity.(v)It provides such fingerprints for all previously tested compounds whenever a new target is assessed

in many or all of the 60 cell lines. (In contrast, if a battery of assays for different biochemical targets wereapplied to, for example, 60,000 compounds, it would be necessary to retest all of the compounds for anynew target or assay.)
(vi)It links the molecular pharmacology with emerging databases on molecular markers in microdissect-
ed human tumorsÑwhich, under the rubric of this article, constitute clinical (C) databases.(vii)It provides the basis for pharmacophore development and searches of an S [structure] database for

additional candidates. If an agent with a desired action is already known, its fingerprint patterns ofactivity can be used by . . . [various] pattern-recognition technologies to find similar compounds.Box 4.6 provides an example of this approach.4.4.12  Algorithms Related to Imaging
Biological science is rich in images. Most familiar are images taken through optical microscopes, butthere are many other imaging modalitiesÑelectron microscopes, computed tomography scans, X-rays,
magnetic resonance imaging, and so on. For most of the history of life science research, images have139Section 4.4.11 is based heavily on J.N. Weinstein, T.G. Myers, P.M. OÕConnor, S.H. Friend, A.J. Fornace, Jr., K.W. Kohn, T.Fojo, et al., ÒAn Information-Intensive Approach to the Molecular Pharmacology of Cancer,Ó Science 275(5298):343-349, 1997.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.108CATALYZING INQUIRYbeen a source of qualitative insight.140 While this is still true, there is growing interest in using imagedata more quantitatively.Consider the following applications:¥Automated identification of fungal spores in microscopic digital images and automated estima-tion of spore density;141¥Automated analysis of liver MRI images from patients with putative hemochromatosis to deter-mine the extent of iron overload, avoiding the need for an uncomfortable liver biopsy;142Box 4.6An Information-intensive Approach to Cancer Drug DiscoveryGiven one compound as a Òseed,Ó [an algorithm known as] COMPARE searches the database of screened agents forthose most similar to the seed in their patterns of activity against the panel of 60 cell lines. Similarity in pattern oftenindicates similarity in mechanism of action, mode of resistance, and molecular structure. . . .A formulation of this approach in terms of three databases [includes databases for] the activity patterns [A], . . .molecular structural features of the tested compounds [S], and . . . possible targets or modulators of activity in the
cells [T]. . . . The (S) database can be coded in terms of any set of two-dimensional (2D) or 3D molecular structuredescriptors. The NCIÕs Drug Information System (DIS) contains chemical connectivity tables for approximately
460,000 molecules, including the 60,000 tested to date. 3-D structures have been obtained for 97% of the DIS
compounds, and a set of 588 bitwise descriptors has been calculated for each structure by use of the Chem-Xcomputational chemistry package. This data set provides the basis for pharmacophoric searches; if a tested com-pound, or set of compounds, is found to have an interesting pattern of activity, its structure can be used to search for
similar molecules in the DIS database.In the target (T) database, each row defines the pattern (across 60 cell lines) of a measured cell characteristic that maymediate, modulate, or otherwise correlate with the activity of a tested compound. When the term is used in this generalshorthand sense, a ÒtargetÓ may be the site of action or part of a pathway involved in a cellular response. Among thepotential targets assessed to date are oncogenes, tumor-suppressor genes, drug resistance-mediating transporters, heat
shock proteins, telomerase, cytokine receptors, molecules of the cell cycle and apoptotic pathways, DNA repair en-zymes, components of the cytoarchitecture, intracellular signaling molecules, and metabolic enzymes.In addition to the targets assessed one at a time, others have been measured en masse as part of a protein expressiondatabase generated for the 60 cell lines by 2D polyacrylamide gel electrophoresis.Each compound displays a unique ÒfingerprintÓ pattern, defined by a point in the 60D space (one dimension for eachcell line) of possible patterns. In information theoretic terms, the transmission capacity of this communication chan-nel is very large, even after one allows for experimental noise and for biological realities that constrain the com-
pounds to particular regions of the 60D space. Although the activity data have been accumulated over a 6-yearperiod, the experiments have been reproducible enough to generate . . . patterns of coherence.SOURCE: Reprinted by permission from J.N. Weinstein, T.G. Myers, P.M. OÕConnor, S.H. Friend, A.J. Fornace, Jr., K.W. Kohn, T. Fojo, etal., ÒAn Information-intensive Approach to the Molecular Pharmacology of Cancer,Ó Science 275(5298):343-349, 1997. Copyright 1997AAAS.140Note also that biological imaging itself is a subset of the intersection between biology and visual techniques. In particular, otherbiological insight can be found in techniques that consider spectral information, e.g., intensity as a function of frequency and perhaps afunction of time. Processing microarray data (discussed further in Section 7.2.1) ultimately depends on the ability to extract interestingsignals from patterns of fluorescing dots, as does quantitative comparison of patterns obtained in two-dimensional polyacrylamide gelelectrophoresis. (See S. Veeser, M.J. Dunn, and G.Z. Yang, ÒMultiresolution Image Registration for Two-dimensional Gel Electrop
hore-sis,Ó Proteomics 1(7):856-870, 2001, available at http://vip.doc.ic.ac.uk/2d-gel/2D-gel-final-revision.pdf.)
141T. Bernier and J.A. Landry, ÒAlgorithmic Recognition of Biological Objects,Ó Canadian Agricultural Engineering 42(2):101-109, 2000.142George Reeke, Rockefeller University, personal communication to John Wooley, October 8, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS109¥Fluorescent speckle microscopy, a technique for quantitatively tracking the movement, assem-bly, and disassembly of macromolecules in vivo and in vitro, such as those involved in cytoskeletondynamics;143and¥Establishing metrics of similarity between brain images taken at different times.144These applications are only an infinitesimal fraction of those that are possible. Several researchareas associated with increasing the utility of biological images are discussed below. Box 4.7 describes
the open microscopy environment, an effort intended to automate image analysis, modeling, and min-
ing of large sets of biological images obtained from optical microscopy.145As a general rule, biologists need to develop better imaging methods that are applicable across theentire spatial scale of interest, from the subcellular to the organismal. (In this context, ÒbetterÓ means
imaging that occurs in real time (or nearly so) with the highest possible spatial and temporal resolution.)
These methods will require new technologies (such as the multiphoton microscope) and also new
protein and nonprotein reporter molecules that can be expressed or introduced into cells or organisms.143C.M. Waterman-Storer and G. Danuser, ÒNew Directions for Fluorescent Speckle Microscopy,Ó Current Biology 12(18):R633-R640, 2002.144M.I. Miller, A. Trouve, and L. Younes, ÒOn the Metrics and Euler-Lagrange Equations of Computational Anatomy,Ó AnnualReview of Biomedical Engineering 4:375-405, 2002, available at http://www.cis.jhu.edu/publications/papers_in_database/EulerLagrangeEqnsCompuAnatomy.pdf.145J.R. Swedlow, I. Goldberg, E. Brauner, and P.K. Sorger, ÒInformatics and Quantitative Analysis in Biological Imaging,ÓScience 300(5616):100-102, 2003.Box 4.7The Open Microscopy Environment1Responding to the need to manage a large number of multispectral movies of mitotic cells in the late 1990s,Sorger and Swedlow began work on the open microscopy environment (OME). The OME is designed asinfrastructure that manages optical microscopy images, storing both the primary image data and appropriatemetadata on those images, including data on the optics of the microscope, the experimental setup and sam-
ple, and information derived by analysis of the images. OME also permits data federation that allows informa-tion from multiple sources (e.g., genomic or chemical databases) to be linked to image records.In addition, the OME provides an extensible environment that enables users to write their own applications forimage analysis. Consider, for example, the task of tracking labeled vesicles in a time-lapse movie. As noted bySwedlow et al., this problem requires the following: a segmentation algorithm to find the vesicles and to
produce a list of centroids, volumes, signal intensities, and so on; a tracker to define trajectories by linkingcentroids at different time points according to a predetermined set of rules; and a viewer to display the analyticresults overlaid on the original movie.2OME provides a mechanism for linking together various analytical modules by specifying data semantics that
enable the output of one module to be accepted as input to another. These semantic data types of OME
describe analytic results such as Òcentroid,Ó Òtrajectory,Ó and Òmaximum signa,Ó and allow users, rather thana predefined standard, to define such concepts operationally, including in the machine-readable definitionand the processing steps that produce it (e.g., the algorithm and the various parameter settings used).1See www.openmicroscopy.org.2J.R. Swedlow, I. Goldberg, E. Brauner, and P.K. Sorger, ÒInformatics and Quantitative Analysis in Biological Imaging,Ó Science300(5616):100-102, 2003.SOURCE: Based largely on the paper by Swedlow et al. cited in Footnote145 and on the OME Web page at www.openmicroscopy.org.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.110CATALYZING INQUIRYThe discussion below focuses only on a narrow slice of the very general problem of biological imaging,as a broader discussion would go beyond the scope of this report.4.4.12.1  Image Rendering
146Images have been central to the study of biological phenomena ever since the invention of themicroscope. Today, images can be obtained from many sources, including tomography, MRI, X-rays,
and ultrasound. In many instances, biologists are interested in the spatial and geometric properties of
components within a biological entity. These properties are often most easily understood when viewed
through an interactive visual representation that allows the user to view the entity from different angles
and perspectives. Moreover, a single analysis or visualization session is often not sufficient, and pro-
cessing across many image volumes is often required.The requirement that a visual representation be interactive places enormous demands on thecomputational speed of the imaging equipment in use. Today, the data produced by imaging equip-
ment are quickly outpacing the capabilities offered by the image processing and analysis software
currently available. For example, the GE EVS-RS9 CT scanner is able to generate image volumes with
resolutions in the 20-90 mm range, which results in a dataset size of multiple gigabytes.  Datasets of

such size require software tools specifically designed for the imaging datasets of today and tomorrow
(see Figure 4.5) so that researchers can identify subtle features that can otherwise be missed or misrep-
resented. Also with increasing dataset resolution comes increasing dataset size, which translates di-
rectly to lengthening dataset transfer, processing, and visualization times.New algorithms that take advantage of state-of-the-art hardware in both relatively inexpensiveworkstations and multiprocessor supercomputers must be developed and moved into easy-to-access
software systems for the clinician and researcher. An example is ray-tracing, a method commonly used
in computer graphics that supports highly efficient implementations on multiple processors for interac-
tive visualization.  The resulting volume rendition permits direct inspection of internal structures,

without a precomputed segmentation or surface extraction step, through the use of multidimensional
transfer functions. As seen in the visualizations in Figure 4.6, the resolution of the CT scan allows
subtleties such as the definition of the cochlea, the modiolus, the implanted electrode array, and the lead
wires that connect the array to a head-mounted connector. The co-linear alignment of the path of the
cochlear nerve with the location of the electrode shanks and tips is the necessary visual confirmation of
the correct surgical placement of the electrode array.In both of the studies described in Figure 4.5 and Figure 4.6, determination of three-dimensionalstructure and configuration played a central role in biological inquiry. Volume visualization created
detailed renderings of changes in bone morphology due to a Pax3 mutation in mice, and it provided
visual confirmation of the precise location of an electrode array implanted in the feline skull. The
scientific utility of volume visualization will benefit from further improvements in its interactivity and
flexibility, as well as simultaneous advances in high-resolution image acquisition and the development
of volumetric image-processing techniques for better feature extraction and enhancement.4.4.12.2  Image Segmentation
147An important problem in automated image analysis is image segmentation. Digital images arerecorded as a set of pixels in a two- or three-dimensional array. Images that represent natural scenes
usually contain different objects, so that, for example, a picture of a park may depict people, trees, and146Section 4.4.12.1 is based on material provided by Chris Johnson, University of Utah.147Section 4.4.11.2 is adapted from and includes excerpts from National Research Council, Mathematics and Physics of EmergingBiomedical Imaging, National Academy Press, Washington, DC, 1996.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS111FIGURE 4.5Visualizations of mutant (
left) and normal (right) mice embryos.CT values are inspected by maximum intensity projection in (a) and with standard isosurface rendering in (b).Volume rendering (c) using multidimensional opacity functions allows more accurate bone emphasis, depth cue-
ing, and curvature-based transfer functions to enhance bone contours in image space. In this case, Drs. Keller andCapecchi are investigating the birth defects caused by a mutation in the Pax3 gene, which controls musculoskeletaldevelopment in mammalian embryos. In their model, they have activated a dominantly acting mutant Pax3 gene
and have uncovered two of its effects: (1) abnormal formation of the bones of the thoracolumbar spine andcartilaginous rib cage and (2) cranioschisis, a more drastic effect in which the dermal and skeletal covering of thebrain is missing. Imaging of mutant and normal mouse embryos was performed at the University of Utah Small
Animal Imaging Facility, producing two 1.2 GB 16-bit volumes of 769 × 689 × 1173 samples, with resolution of 21 ×21 × 21 microns.SOURCE: Courtesy of Chris Johnson, University of Utah; see also http://www.sci.utah.edu/stories/2004/
spr_imaging.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.112CATALYZING INQUIRYbenches. Similarly, a scanned image of a magazine page may contain text and graphics (e.g., a picture ofa park). Segmentation refers to the process by which an object (or characteristics of the object) in an
image is extracted from image data for purposes of visualization and measurement. (Extraction means
that the pixels associated with the object of interest are isolated.)  In a biological context, a typical

problem in image segmentation might involve extracting different organs in a CT scan of the body.
Segmentation research involves the development of automatic, computer-executable rules that can
isolate enough of these pixels to produce an acceptably accurate segmentation. Segmentation is a central
problem of image analysis because segmentation must be accomplished before many other interesting(a)  (b)(c)
FIGURE 4.6Volume renderings of electrode array implanted in feline skull.
In this example, scanning produced a 131 MB 16-bit volume of 425 × 420 × 385 samples, with resolution of 21 ×21 × 21 microns. Renderings of the volume were generated using a ray-tracing algorithm across multiple proces-sors allowing interactive viewing of this relatively large dataset. The resolution of the scan allows definition of the
shanks and tips of the implanted electrode array. Volumetric image processing was used to isolate the electrodearray from the surrounding tissue, highlighting the structural relationship between the implant and the bone.There are distinct CT values for air, soft tissue, bone, and the electrode array, enabling the use of a combination of
ray tracing and volume rendering to visualize the array in the context of the surrounding structures, specificallythe bone surface. The volume is rotated gradually upward in columns (a), (b), and (c), from seeing the side of thecochlea exterior in (a), to looking down the path of the cochlear nerve in (c). From top to bottom, each row uses
different rendering styles: (1), summation projections of CT values (green) and gradients (magenta); (2), volumerenderings with translucent bone, showing the electrode leads in magenta.SOURCE: Courtesy of Chris Johnson, University of Utah; see also http://www.sci.utah.edu/stories/2004/
spr_imaging.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS113problems in image analysis can be solved, including image registration, shape analysis, and volume andarea estimation. A specific laboratory example would be the segmentation of spots on two-dimensional
electrophoresis gels.There is no common method or class of methods applicable to even the majority of images. Segmen-tation is easiest when the objects of interest have intensity or edge characteristics that allow them to be
separated from the background and noise, as well as from each other. For example, an MRI image of the
human body would be relatively easy to segment for bones: all pixels with intensity below a given
threshold would be eliminated, leaving mostly the pixels associated with high-signal-intensity bone.Generally, edge detection depends on a search for intensity gradients. However, it is difficult to findgradients when, as is usually the case in biomedical images, intensities change only gradually between
the structure of interest and the surrounding structure(s) from which it is to be extracted. Continuity
and connectivity are important criteria for separating objects from noise and have been exploited quite
widely.A number of different approaches to image segmentation are described in more detail by Pham et al.1484.4.12.3  Image Registration
149Different modes of imaging instrumentation may be used on the same object because they aresensitive to different object characteristics. For example, an X-ray of an individual will produce different
information than a CT scan. For various purposes, and especially for planning surgical and radiation
treatment, it can be important for these images to be aligned with each other, that is, for information
from different imaging modes to be displayed in the same locations. This process is known as image
registration.There are a variety of techniques for image registration, but in general they can be classified basedon the features that are being matched. For example, such features may be external markers that are
fixed (e.g., on a patientÕs body), internal anatomic markers that are identifiable on all images, the center
of gravity for one or more objects in the images, crestlines of objects in the images, or gradients of
intensity. Another technique is minimization of the distance between corresponding surface points of a
predefined object. Image registration often depends on the identification of similar structures in the
images to be registered. In the ideal case, this identification can be performed through an automated
segmentation process.Image registration is well defined for rigid objects but is more complicated for deformable objects orfor objects imaged from different angles. When soft tissue deforms (e.g., because a patient is lying on his
side rather than on his back), elastic warping is required to transform one dataset into the other. The
difficulty lies in defining enough common features in the images to enable specifying appropriate local
deformations.An example of an application in which image registration is important is the Cell-Centered Database(CCDB).150 Launched in 2002, the CCDB contains structural and protein distribution information derivedfrom confocal, multiphoton, and electron microscopy for use by the structural biology and neuroscience
communities. In the case of neurological images, most of the imaging data are referenced to a higher levelof brain organization by registering their location in the coordinate system of a standard brain atlas.
Placing data into an atlas-based coordinate system provides one method by which data taken across scales148D.L. Pham, C. Xu, and J.L. Prince, ÒCurrent Methods in Medical Image Segmentation,Ó Annual Review of Biomedical Engineer-ing 2:315-338, 2000.149Section 4.4.12.3 is adapted from National Research Council, Mathematics and Physics of Emerging Biomedical Imaging, NationalAcademy Press, Washington, DC, 1996.150See M.E. Martone, S.T. Peltier, and M.H. Ellisman, ÒBuilding Grid Based Resources for Neurosciences,Ó unpublished paper2003, National Center for Microscopy and Imaging Research, Department of Neurosciences, University of California, San Diego,San Diego, CA,  and http://ccdb.ucsd.edu/CCDB/about.shtml.
Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.114CATALYZING INQUIRYand distributed across multiple resources can be compared reliably. Through the use of atlases and toolsfor surface warping and image registration, it is possible to express the location of anatomical features or
signals in terms of a standardized and quantitative coordinate system, rather by using terms that describe
objects in the field of view. The expression of brain data in terms of atlas coordinates also allows it to be
transformed spatially to provide alternative views that may offer additional information (e.g., flat maps or
additional parcellation schemes). Finally, a standard coordinate system allows the same brain region to be
sampled repeatedly to allow data to be accumulated over time.4.4.12.4 Image ClassificationImage classification is the process through which a set of images can be sorted into meaningfulcategories. Categories can be defined through low-level features such as color mix and texture patterns or
through high-level features such as objects depicted. As a rule, low-level features can be computed with
little difficulty, and a number of systems have been developed that take advantage of such features.151However, users are generally much more interested in semantic content that is not easily repre-sented in such low-level features. The easiest method to identify interesting semantic content is simply
to annotate an image manually with text, although this process is quite tedious and is unlikely to
capture the full range of content in an image. Thus, automated techniques hold considerable interest.The general problem of automatic identification of such image content has not been solved. Oneapproach described by Huang et al. relies on supervised learning to classify images hierarchically.152This approach relies on using good low-level features and then performing feature-space reconfiguration
using singular value decomposition to reduce noise and dimensionality. A hierarchical classification
tree can be generated from training data and subsequently used to sort new images into categories.A second approach is based on the fact that biological images often contain branching struc-tures. (For example, both muscle and neural tissue contain blood vessels and dendrites that are
found in branching structures.) The fractal dimensionality of such structures can then be used as a
measure of similarity, and images that contain structures of similar fractal dimension can be
grouped into categories.1534.5  DEVELOPING COMPUTATIONAL TOOLS
The computational tools described above were once gleams in the eye of some researcher. Despitethe joy and satisfaction felt when a prototype program supplies the first useful results to its developer,
it is a long, long way to converting that program into a genuine product that is general, robust, and
useful to others. Indeed, in his classic text The Mythical Man-Month (Addison-Wesley, Reading, MA,1995), Frederick P. Brooks, Jr., estimates the difference in effort necessary to create a programming
systems product from a program as an order of magnitude.Some of the software engineering considerations necessary to turn a program into a product includethe following:¥Quality. The program, of course, must be as free of defects as possible, not only in the sense ofrunning without faults, but also of precisely implementing the stated algorithm. It must be tested for all151See, for example, M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani, J. Hafner, D. Lee, D.Petkovic, D. Steele, and P. Yanker, ÒQuery by Image and Video Content: The QBIC System,Ó IEEE Computer 28(9):23-32, 1995,available at http://wwwqbic.almaden.ibm.com/.152J. Huang, S.R. Kumar, and R. Zabih, ÒAn Automatic Hierarchical Image Classification Scheme,Ó ACM Conference onMultimedia, Bristol, England, September 1998. A revised version appears in EURASIP Journal on Applied Signal Processing
, 2003,available at http://www.cs.cornell.edu/rdz/Papers/Archive/mm98.pdf.153D. Cornforth, H. Jelinek, and L. Peich, ÒFractop: A Tool for Automated Biological Image Classification,Ó available at http://csu.edu.au/~dcornfor/Fractop_v7.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL TOOLS115potential inputs, and combinations of factors, and must be robust even in the face of invalid usage. Theprogram should have well-understood and bounded resource demands, including memory, input-
output, and processing time.¥Maintenance. When bugs are discovered, they must be tracked, patched, and provided to users.This often means that the code should be structured for maintainability; for example, Perl, which is
extremely powerful, is often written in a way that is incomprehensible to programmers other than the
author (and often even to the author). Differences in functionality between versions must be docu-
mented carefully.¥Documentation. If the program is to be usable by others, all of the functionality must be clearlydocumented, including data file formats, configuration options, output formats, and of course program
usage. If the source code of the program is made available (as is often the case with scientific tools), the
code must be documented in such a way that users can check the validity of the implementation as well
as alter it to meet their needs.¥User interface. The program must have a user interface, although not necessarily graphical, that isunambiguous and able to access the full range of functions of the program. It should be easy to use,
difficult to make mistakes, and clear in its instructions and display of state.¥System integration and portability. The program must be distributed to users in a convenient way,and be able to run on different platforms and operating systems in a way that does not interfere with
existing software or system settings. It should be easily configurable and customizable for particular
requirements, and should install easily without access to specialized software, such as nonstandard
compilers.¥General. The program should accept a wide selection of data types, including common formats,units, precisions, ranges, and file sizes. The internal coding interfaces should have precisely defined
syntax and semantics, so that users can easily extend the functionality or integrate it into other tools.Tool developers address these considerations to varying degrees, and users may initially be moretolerant of something that is more program than product if the functionality it confers is essential and
unique. Over time, however; such programs will eventually become more product-like because users
will not tolerate significant inconvenience.Finally, there is an issue of development methodology. A proprietary approach to development canbe adopted for a number of competitive reasons, ranging from the ultimate desire to reap financial
benefit to staying ahead of competing laboratories. Under a proprietary approach, source code for the
tools would be kept private, so that potential competitors would be unable to exploit the code easily for
their own purposes. (Source code is needed to make changes to a program.) An open approach to
development calls for the source code to be publicly available, on the theory that broad community
input strengthens the utility of the tools being made available and better enables one team to build on
another teamÕs work.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY1171175Computational Modeling and Simulation asEnablers for Biological DiscoveryWhile the previous chapter deals with the ways in which computers and algorithms could supportexisting practices of biological research, this chapter introduces a different type of opportunity. The
quantities and scopes of data being collected are now far beyond the capability of any human, or team
of humans, to analyze. And as the sizes of the datasets continue to increase exponentially, even existing
techniques such as statistical analysis begin to suffer. In this data-rich environment, the discovery of
large-scale patterns and correlations is potentially of enormous significance. Indeed, such discoveries
can be regarded as hypotheses asserting that the pattern or correlation may be importantÑa mode of
Òdiscovery scienceÓ that complements the traditional mode of science in which a hypothesis is gener-
ated by human beings and then tested empirically.For exploring this data-rich environment, simulations and computer-driven models of biologicalsystems are proving to be essential.5.1ON MODELS IN BIOLOGY
In all sciences, models are used to represent, usually in an abbreviated form, a more complex anddetailed reality. Models are used because in some way, they are more accessible, convenient, or familiar
to practitioners than the subject of study. Models can serve as explanatory or pedagogical tools, repre-
sent more explicitly the state of knowledge, predict results, or act as the objects of further experiments.
Most importantly, a model is a representation of some reality that embodies some essential and interest-
ing aspects of that reality, but not all of it.Because all models are by definition incomplete, the central intellectual issue is whether the essen-tial aspects of the system or phenomenon are well represented (the term ÒessentialÓ has multiple
meanings depending on what aspects of the phenomenon are of interest). In biological phenomena,
what is interesting and significant is usually a set of relationshipsÑfrom the interaction of two mol-
ecules to the behavior of a population in its environment. Human comprehension of biological systems
is limited, among other things, by that very complexity and by the problems that arise when attempting
to dissect a given system into simpler, more easily understood components. This challenge is com-
pounded by our current inability to understand relationships between the components as they occur in
reality, that is, in the presence of multiple, competing influences and in the broader context of time and
space.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.118CATALYZING INQUIRYDifferent fields of science have traditionally used models for different purposes; thus, the nature ofthe models, the criteria for selecting good or appropriate models, and the nature of the abbreviation or
simplification have varied dramatically. For example, biologists are quite familiar with the notion of
model organisms.1 A model organism is a species selected for genetic experimental analysis on thebasis of experimental convenience, homology to other species (especially to humans), relative simplic-
ity, or other attractive attributes. The fruit fly Drosophila melanogaster is a model organism attractive atleast in part because of its short generational time span, allowing many generations in the course of an
experiment.At the most basic level, any abstraction of some biological phenomenon counts as a model. Indeed,the cartoons and block diagrams used by most biologists to represent metabolic, signaling, or regulatory
pathways are modelsÑqualitative models that lay out the connectivity of elements important to the
phenomenon. Such models throw away details (e.g., about kinetics) implicitly asserting that omission of
such details does not render the model irrelevant.A second example of implicit modeling is the use of statistical tests by many biologists. All statisti-cal tests are based on a null hypothesis, and all null hypotheses are based on some kind of underlying
model from which the probability distribution of the null hypothesis is derived. Even those biologists
who have never thought of themselves as modelers are using models whenever they use statistical tests.Mathematical modeling has been an important component of several biological disciplines formany decades. One of the earliest quantitative biological models involved ecology: the Lotka-Volterra
model of species competition and predator-prey relationships described in Section 5.2.4. In the context
of cell biology, models and simulations are used to examine the structure and dynamics of a cell or
organismÕs function, rather than the characteristics of isolated parts of a cell or organism.2 Such modelsmust consider stochastic and deterministic processes, complex pleiotropy, robustness through redun-
dancy, modular design, alternative pathways, and emergent behavior in biological hierarchy.In a cellular context, one goal of biology is to gain insight into the interactions, molecular orotherwise, that are responsible for the behavior of the cell. To do so, a quantitative model of the cell
must be developed to integrate global organism-wide measurements taken at many different levels of
detail.The development of such a model is iterative. It begins with a rough model of the cell, based onsome knowledge of the components of the cell and possible interactions among them, as well as prior
biochemical and genetic knowledge. Although the assumptions underlying the model are insufficient
and may even be inappropriate for the system being investigated, this rough model then provides a
zeroth-order hypothesis about the structure of the interactions that govern the cellÕs behavior.Implicit in the model are predictions about the cellÕs response under different kinds of perturbation.Perturbations may be genetic (e.g., gene deletions, gene overexpressions, undirected mutations) or
environmental (e.g., changes in temperature, stimulation by hormones or drugs). Perturbations are
introduced into the cell, and the cellÕs response is measured with tools that capture changes at the
relevant levels of biological information (e.g., mRNA expression, protein expression, protein activation
state, overall pathway function). Box 5.1 provides some additional detail on cellular perturbations.The next step is comparison of the modelÕs predictions to the measurements taken. This comparisonindicates where and how the model must be refined in order to match the measurements more closely.
If the initial model is highly incomplete, measurements can be used to suggest the particular compo-
nents required for cellular function and those that are most likely to interact. If the initial model is
relatively well defined, its predictions may already be in good qualitative agreement with measure-
ment, differing only in minor quantitative ways. When model and measurement disagree, it is often1See, for example, http://www.nih.gov/science/models for more information on model organisms.2Section 5.1 draws heavily on excerpts from T. Ideker, T. Galitski, and L. Hood, ÒA New Approach to Decoding Life: SystemsBiology,Ó Annual Review of Genomics and Human Genetics 2:343-372, 2001; and H. Kitano, ÒSystems Biology: A Brief Overview,ÓScience 295(5560):1662-1664, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY119necessary to create a number of more refined models, each incorporating a different mechanism under-lying the discrepancies in measurement.With the refined model(s) in hand, a new set of perturbations can be applied to the cell. Note thatnew perturbations are informative only if they elicit different responses between models, and they are
most useful when the predictions of the different models are very different from one another. Neverthe-
less, a new set of perturbations is required because the predictions of the refined model(s) will generally
fit well with the old set of measurements.The refined model that best accounts for the new set of measurements can then be regarded as theinitial model for the next iteration. Through this process, model and measurement are intended to
converge in such a way that the modelÕs predictions mirror biological responses to perturbation. Mod-
eling must be connected to experimental efforts so that experimentalists will know what needs to be
determined in order to construct a comprehensive description and, ultimately, a theoretical framework
for the behavior of a biological system. Feedback is very important, and it is this feedback, along with
the globalÑor, loosely speaking, genomic-scaleÑnature of the inquiry that characterizes much of 21st
century biology.5.2WHY BIOLOGICAL MODELS CAN BE USEFUL
In the last decade, mathematical modeling has gained stature and wider recognition as a useful toolin the life sciences. Most of this revolution has occurred since the era of the genome, in which biologists
were confronted with massive challenges to which mathematical expertise could successfully be brought
to bear. Some of the success, though, rests on the fact that computational power has allowed scientists to
explore ever more complex models in finer detail. This means that the mathematicianÕs talent for
abstraction and simplification can be complemented with realistic simulations in which details not
amenable to analysis can be explored. The visual real-time simulations of modeled phenomena giveBox 5.1Perturbation of Biological SystemsPerturbation of biological systems can be accomplished through a number of genetic mechanisms, such as thefollowing:¥High-throughput genomic manipulation. Increasingly inexpensive and highly standardized tools are avail-able that enable the disruption, replacement, or modification of essentially any genomic sequence. Further-more, these tools can operate simultaneously on many different genomic sequences.
¥Systematic gene mutations. Although random gene mutations provide a possible set of perturbations, therandom nature of the process often results in nonuniform coverage of possible genotypesÑsome genes aretargeted multiple times, others not at all. A systematic approach can cover all possible genotypes and the
coverage of the genome is unambiguous.¥Gene disruption. While techniques of genomic manipulation and systematic gene mutation are often use-ful in analyzing the behavior of model organisms such as yeast, they are not practical for application to
organisms of greater complexity (i.e., higher eukaryotes). On the other hand, it is often possible to inducedisruptions in the function of different genes, effectively silencing (or deleting) them to produce a biologicallysignificant perturbation.SOURCE: Adapted from T. Ideker, T. Galitski, and L. Hood, ÒA New Approach to Decoding Life: Systems Biology,Ó Annual Review ofGenomics and Human Genetics 2:343-372, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.120CATALYZING INQUIRYmore compelling and more accessible interpretations of what the models predict.3 This has made iteasier to earn the recognition of biologists.On the other hand, modelingÑespecially computational modelingÑshould not be regarded as anintellectual panacea, and models may prove more hindrance than help under certain circumstances. In
models with many parameters, the state space to be explored may grow combinatorially fast so that no
amount of data and brute force computation can yield much of value (although it may be the case that
some algorithm or problem-related insight can reduce the volume of state space that must be explored
to a reasonable size). In addition, the behavior of interest in many biological systems is not characterized
as equilibrium or quasi-steady-state behavior, and thus convergence of a putative solution may never
be reached. Finally, modeling presumes that the researcher can both identify the important state vari-
ables and obtain the quantitative data relevant to those variables.4Computational models apply to specific biological phenomena (e.g., organisms, processes) and areused for a number of purposes as described below.5.2.1Models Provide a Coherent Framework for Interpreting Data
A biologist surveys the number of birds nesting on offshore islands and notices that the numberdepends on the size (e.g., diameter) of the island: the larger the diameter d, the greater is the number ofnests N. A graph of this relationship for islands of various sizes reveals a trend. Here the mathematicallyinformed and uninformed part ways: simple linear least-squares fit of the data misses a central point.
A trivial Ònull modelÓ based on an equal subdivision of area between nesting individuals predicts that
N~ d2, (i.e., the number of nests should be roughly proportional to the square of island area). This simplegeometric property relating area to population size gives a strong indication of the trend researchers
should expect to see. Departures from this trend would indicate that something else may be important.
(For example, different parts of islands are uninhabitable, predators prefer some islands to others, and
so forth.)Although the above example is elementary, it illustrates the idea that data are best interpretedwithin a context that shapes oneÕs expectations regarding what the data ÒoughtÓ to look like; often a
mathematical (or geometric) model helps to create that context.5.2.2Models Highlight Basic Concepts of Wide Applicability
Among the earliest applications of mathematical ideas to biology are those in which populationlevels were tracked over time and attempts were made to understand the observed trends. Malthus
proposed in 1798 the fitting of population data to exponential growth curves following his simple
model for geometric growth of a population.5 The idea that simple reproductive processes produce3As one example, Ramon Felciano studied the use of Òdomain graphicsÓ by biologists. Felciano argued that certain visualrepresentations (known as domain graphics) become so ingrained in the discourse of certain subdisciplines of biology that theybecome good targets for user interfaces to biological data resources. Based on this notion, Felciano constructed a reusableinterface based on the standard two-dimensional layout of RNA secondary structure. See R. Felciano, R. Chen, and R. Altman,
ÒRNA Secondary Structure as a Reusable Interface to Biological Information Resources,Ó Gene 190:59-70, 1997.4In some cases, obtaining the quantitative data is a matter of better instrumentation and higher accuracy. In other cases, thedata are not available in any meaningful sense of practice. For example, Richard Lewontin notes that the probability of survivalPs of a particular genotype is an ensemble property, rather than the property of a single individual who either will or will notsurvive. But if what is of interest is Ps as a function of the alternative genotypes deriving from a single locus, the effects of theimpacts deriving from other loci must be randomized. However, in sexually reproducing organisms, there is no way known to
produce an ensemble of individuals that are all identical with respect to a single locus but randomized over other loci. Thus, aquantitative characterization of Ps is in practice not possible, and no alternative measurement technologies will be of much valuein solving this problem. See R. Lewontin, The Genetic Basis of Evolutionary Change, Columbia University Press, New York, 1974.5T.R. Malthus, An Essay on the Principle of Population, First Edition, E.A. Wrigley and D. Souden, eds., Penguin Books,Harmondsworth, England, 1798.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY121exponential growth (if birth rates exceed mortality rates) or extinction (in the opposite case) is a funda-mental principle: its applicability in biology, physics, chemistry, as well as simple finance, is central.An important refinement of the Malthus model was proposed in 1838 to explain why most popula-tions do not experience exponential growth indefinitely. The refinement was the idea of the density-
dependent growth law, now known as the logistic growth model.6 Though simple, the Verhulst modelis still used widely to represent population growth in many biological examples. Both Malthus and
Verhulst models relate observed trends to simple underlying mechanisms; neither model is fully accu-
rate for real populations, but deviations from model predictions are, in themselves, informative, be-
cause they lead to questions about what features of the real systems are worthy of investigation.More recent examples of this sort abound. Nonlinear dynamics has elucidated the tendency ofexcitable systems (cardiac tissue, nerve cells, and networks of neurons) to exhibit oscillatory, burst, and
wave-like phenomena. The understanding of the spread of disease in populations and its sensitive
dependence on population density arose from simple mathematical models. The same is true of the
discovery of chaos in the discrete logistic equation (in the 1970s). This simple model and its mathemati-
cal properties led to exploration of new types of dynamic behavior ubiquitous in natural phenomena.
Such biologically motivated models often cross-fertilize other disciplines: in this case, the phenomenon
of chaos was then found in numerous real physical, chemical, and mechanical systems.5.2.3Models Uncover New Phenomena or Concepts to Explore
Simple conceptual models can be used to uncover new mechanisms that experimental science hasnot yet encountered. The discovery of chaos mentioned above is one of the clearest examples of this
kind. A second example of this sort is TuringÕs discovery that two chemicals that interact chemically in
a particular way (activate and inhibit one another) and diffuse at unequal rates could give rise to Òpeaks
and valleysÓ of concentration. His analysis of reaction-diffusion (RD) systems showed precisely what
ranges of reaction rates and rates of diffusion would result in these effects, and how properties of the
pattern (e.g., distance between peaks and valleys) would depend on those microscopic rates. Later
research in the mathematical community also uncovered how other interesting phenomena (traveling
waves, oscillations) were generated in such systems and how further details of patterns (spots, stripes,
etc.) could be affected by geometry, boundary conditions, types of chemical reactions, and so on.TuringÕs theory was later given physical manifestation in artificial chemical systems, manipulatedto satisfy the theoretical criteria of pattern formation regimes. And, although biological systems did not
produce simple examples of RD pattern formation, the theoretical framework originating in this work
motivated later more realistic and biologically based modeling research.5.2.4Models Identify Key Factors or Components of a System
Simple conceptual models can be used to gain insight, develop intuition, and understand Òhowsomething works.Ó For example, the Lotka-Volterra model of species competition and predator-prey7 islargely conceptual and is recognized as not being very realistic. Nevertheless, this and similar models
have played a strong role in organizing several themes within the discipline: for example, competitive
exclusion, the tendency for a species with a slight advantage to outcompete, dominate, and take over
from less advantageous species; the cycling behavior in predator-prey interactions; and the effect of6P.F. Verhulst, ÒNotice sur la loi que la population suit dans son accroissement,Ó Correspondence Math”matique et Physique, 1838.7A.J. Lotka, Elements of Physical Biology, Williams & Wilkins Co., Baltimore, MD, 1925; V. Volterra, ÒVariazioni e fluttuazionidel numero dÕindividui in specie animali conviventi,Ó Mem. R. Accad. Naz. dei Lincei., Ser. VI, Vol. 2, 1926. The Lotka-Volterramodel is a set of coupled differential equations that relate the densities of prey and predator given parameters involving thepredator-free rate of prey population increase, the normalized rate at which predators can successfully remove prey from thepopulation, the normalized rate at which predators reproduce, and the rate at which predators die.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.122CATALYZING INQUIRYresource limitations on stabilizing a population that would otherwise grow explosively. All of theseconcepts arose from mathematical models that highlighted and explained dynamic behavior within the
context of simple models. Indeed, such models are useful for helping scientists to recognize patterns
and predict system behavior, at least in gross terms and sometimes in detail.5.2.5Models Can Link Levels of Detail (Individual to Population)
Biological observations are made at many distinct hierarchies and levels of detail. However, thelinks between such levels are notoriously difficult to understand. For example, the behavior of single
neurons and their response to inputs and signaling from synaptic connections might be well known.
The behavior of a large assembly of such neurons in some part of the central nervous system can be
observed macroscopically by imaging or electrode recording techniques. However, how the two levels
are interconnected remains a massive challenge to scientific understanding. Similar examples occur in
countless settings in the life sciences: due to the complexity of nonlinear interactions, it is nearly impos-
sible to grasp intuitively how collections of individuals behave, what emergent properties of these
groups arise, or the significance of any sensitivity to initial conditions that might be magnified at higher
levels of abstraction. Some mathematical techniques (averaging methods, homogenization, stochastic
methods) allow the derivation of macroscopic statements based on assumptions at the microscopic, or
individual, level. Both modeling and simulation are important tools for bridging this gap.5.2.6Models Enable the Formalization of Intuitive Understandings
Models are useful for formalizing intuitive understandings, even if those understandings are partialand incomplete. What appears to be a solid verbal argument about cause and effect can be clarified and
put to a rigorous test as soon as an attempt is made to formulate the verbal arguments into a mathemati-
cal model. This process forces a clarity of expression and consistency (of units, dimensions, force
balance, or other guiding principles) that is not available in natural language. As importantly, it can
generate predictions against which intuition can be tested.Because they run on a computer, simulation models force the researcher to represent explicitlyimportant components and connections in a system. Thus, simulations can only complement, but never
replace, the underlying formulation of a model in terms of biological, physical, and mathematical
principles. That said, a simulation model often can be used to indicate gaps in oneÕs knowledge of some
phenomenon, at which point substantial intellectual work involving these principles is needed to fill the
gaps in the simulation.5.2.7Models Can Be Used as a Tool for Helping to Screen Unpromising Hypotheses
In a given setting, quantitative or descriptive hypotheses can be tested by exploring the predictionsof models that specify precisely what is to be expected given one or another hypothesis. In some cases,
although it may be impossible to observe a sequence of biological events (e.g., how a receptor-ligand
complex undergoes sequential modification before internalization by the cell), downstream effects may
be observable. A model can explore the consequences of each of a variety of possible sequences can and
help scientists to identify the most likely candidate for the correct sequence. Further experimental
observations can then refine oneÕs understanding.5.2.8Models Inform Experimental Design
Modeling properly applied can accelerate experimental efforts at understanding. Theory embeddedin the model is an enabler for focused experimentation. Specifically, models can be used alongside
experiments to help optimize experimental design, thereby saving time and resources. Simple modelsCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY123give a framework for observations (as noted in Section 5.2.1) and thereby suggest what needs to bemeasured experimentally and, indeed, what need not be measuredÑthat is how to refine the set of
observations so as to extract optimal knowledge about the system. This is particularly true when models
and experiments go hand-in-hand. As a rule, several rounds of modeling and experimentation are
necessary to lead to informative results.Carrying these general observations further, Selinger et al.8 have developed a framework for un-derstanding the relationship between the properties of certain kinds of models and the experimental
sampling required for ÒcompletenessÓ of the model. They define a model as a set of rules that maps a set
of inputs (e.g., possible descriptions of a cellÕs environment) to a set of outputs (e.g., the resulting
concentrations of all of the cellÕs RNAs and proteins). From these basic properties, Selinger et al. are able
to determine the order of magnitude of the number of measurements needed to populate the space of all
possible inputs (e.g., environmental conditions) with enough measured outputs (e.g., transcriptomes,
proteomes) to make prediction feasible, thereby establishing how many measurements are needed to
adequately sample input space to allow the rule parameters to be determined.Using this framework, Salinger et al. estimate the experimental requirements for the completenessof a discrete transcriptional network model that maps all N genes as inputs to all N genes as outputs inwhich the genes can take on three levels of expression (low, medium, and high) and each gene has, at
most, K direct regulators. Applying this model to three organismsÑMycoplasma pneumoniae, Escherichiacoli, and Homo sapiensÑthey find that 80, 40,000, and 700,000 transcriptome experiments, respectively,are necessary to fill out this model. They further note that the upper-bound estimate of experimental
requirements grows exponentially with the maximum number of regulatory connections K per gene,although genes tend to have a low K, and that the upper-bound estimate grows only logarithmicallywith the number of genes N, making completeness feasible even for large genetic networks.5.2.9Models Can Predict Variables Inaccessible to Measurement
Technological innovation in scientific instrumentation has revolutionized experimental biology.However, many mysteries of the cell, of physiology, of individual or collective animal behavior, and of
population-level or ecosystem-level dynamics remain unobservable. Models can help link observations
to quantities that are not experimentally accessible. At the scale of a few millimeters, Mar”e and
Hogeweg recently developed9 a computational model based on a cellular automaton for the behavior ofthe social amoeba Dictyostelium discoideum. Their model is based on differential adhesion between cells,cyclic adenosine monophosphate (cAMP) signaling, cell differentiation, and cell motion. Using detailedtwo- and three-dimensional simulations of an aggregate of thousands of cells, the authors showed how
a relatively small set of assumptions and ÒrulesÓ leads to a fully accurate developmental pathway.
Using the simulation as a tool, they were able to explore which assumptions were blatantly inappropri-
ate (leading to incorrect outcomes). In its final synthesis, the Mar”e-Hogeweg model predicts dynamic
distributions of chemicals and of mechanical pressure in a fully dynamic simulation of the culminating
Dictyostelium slug. Some, but not all, of these variables can be measured experimentally: those that aremeasurable are well reproduced by the model. Those that cannot (yet) be measured are predicted inside
the evolving shape. What is even more impressive: the model demonstrates that the system has self-
correcting properties and accounts for many experimental observations that previously could not be
explained.8D.W. Selinger, M.A. Wright, and G.M. Church, ÒOn the Complete Determination of Biological Systems,Ó Trends in Biotechnol-ogy 21(6):251-254, 2003.9A.F.M. Mar”e and P. Hogeweg, ÒHow Amoeboids Self-organize into a Fruiting Body: Multicellular Coordination inDictyostelium discoideum,Ó Proceedings of the National Academy of Sciences 98(7):3879-3883, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.124CATALYZING INQUIRY5.2.10Models Can Link What Is Known to What Is Yet Unknown
In the words of Pollard, ÒAny cellular process involving more than a few types of molecules is toocomplicated to understand without a mathematical model to expose assumptions and to frame the
reactions in a rigorous setting.Ó10 Reviewing the state of the field in cell motility and the cytoskeleton,he observes that even with many details of the mechanism as yet controversial or unknown, modeling
plays an important role. Referring to a system (of actin and its interacting proteins) modeled by Mogilner
and Edelstein-Keshet,11 he points to advantages gained by the mathematical framework: ÒA math-ematical model incorporating molecular reactions and physical forces correctly predicts the steady-state
rate of cellular locomotion.Ó The model, he notes, correctly identifies what limits the motion of the cell,
predicts what manipulations would change the rate of motion, and thus suggests experiments to per-
form. While details of some steps are still emerging, the model also distinguishes quantitatively be-
tween distinct hypotheses for how actin filaments are broken down for purposes of recycling their
components.5.2.11Models Can Be Used to Generate Accurate Quantitative Predictions
Where detailed quantitative information exists about components of a system, about underlyingrules or interactions, and about how these components are assembled into the system as a whole,
modeling may be valuable as an accurate and rigorous tool for generating quantitative predictions.
Weather prediction is one example of a complex model used on a daily basis to predict the future. On
the other hand, the notorious difficulties of making accurate weather predictions point to the need for
caution in adopting the conclusions even of classical models, especially for more than short-term pre-
dictions, as one might expect from mathematically chaotic systems.5.2.12Models Expand the Range of Questions That Can Meaningfully Be Asked
12For much of life science research, questions of purpose arise about biological phenomena. For
instance, the question, Why does the eye have a lens? most often calls for the purpose of the lensÑto
focus light raysÑand only rarely for a description of the biological mechanism that creates the lens.
That such an answer is meaningful is the result of evolutionary processes that shape biological entities
by enhancing their ability to carry out fitness-enhancing functions. (Put differently, biological entities
are the result of natureÕs engineering of devices to perform the function of survival; this perspective is
explored further in Chapter 6.)Lander points out that molecular biologists traditionally have shied away from teleological matters,and that geneticists generally define function not in terms of the useful things a gene does, but by what
happens when the gene is altered. However, as the complexity of biological mechanism is increasingly
revealed, the identification of a purpose or a function of that mechanism has enormous explanatory
power. That is, what purpose does all this complexity serve?As the examples in Section 5.4 illustrate, computational modeling is an approach to exploring theimplications of the complex interactions that are known from empirical and experimental work. Lander
notes that one general approach to modeling is to create models in which networks are specified in
terms of elements and interactions (the network ÒtopologyÓ), but the numerical values that quantify
those interactions (the parameters) are deliberately varied over wide ranges to explore the functionality
of the networkÑwhether it acts as a Òswitch,Ó Òfilter,Ó Òoscillator,Ó Òdynamic range adjuster,Ó Òpro-
ducer of stripes,Ó and so on.10T.D. Pollard, ÒThe Cytoskeleton, Cellular Motility and the Reductionist Agenda,Ó Nature 422(6933):741-745, 2003.11A. Mogilner and L. Edelstein-Keshet, ÒRegulation of Actin Dynamics in Rapidly Moving Cells: A Quantitative Analysis,ÓBiophysical Journal 83(3):1237-1258, 2002.12Section 5.2.12 is based largely on A.D. Lander, ÒA Calculus of Purpose,Ó PLoS Biology 2(6):e164, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY125Lander explains the intellectual paradigm for determining function as follows:By investigating how such behaviors change for different parameter setsÑan exercise referred to asÒexploring the parameter spaceÓÑone starts to assemble a comprehensive picture of all the kinds of
behaviors a network can produce. If one such behavior seems useful (to the organism), it becomes acandidate for explaining why the network itself was selected; i.e., it is seen as a potential purpose for thenetwork. If experiments subsequently support assignments of actual parameter values to the range of
parameter space that produces such behavior, then the potential purpose becomes a likely one.5.3TYPES OF MODELS
135.3.1From Qualitative Model to Computational Simulation
Biology makes use of many different types of models. In some cases, biological models are qualita-tive or semiquantitative. For example, graphical models show directional connections between compo-
nents, with the directionality indicating influence. Such models generally summarize a great deal of
known information about a pathway and facilitate the formation of hypotheses about network function.
Moreover, the use of graphical models allows researchers to circumvent data deficiencies that might be
encountered in the development of more quantitative (and thus data-intensive) models. (It has also
been argued that probabilistic graphical models provide a coherent, statistically sound framework that
can be applied to many problems, and that certain models used by biologists, such as hidden Markov
models or Bayesian Networks), can be regarded as special cases of graphical models.14)On the other hand, the forms and structures of graphical models are generally inadequate to expressmuch detail, which might well be necessary for mechanistic models. In general, qualitative models do not
account for mechanisms, but they can sometimes be developed or analyzed in an automated manner.
Some attempts have been made to develop formal schemes for annotating graphical models (Box 5.2).
15Qualitative models can be logical or statistical as well. For example, statistical properties of a graphof protein-protein interaction have been used to infer the stability of a networkÕs function against most
ÒdeletionsÓ in the graph.16 Logical models can be used when data regarding mechanism are unavail-able and have been developed as Boolean, fuzzy logical, or rule-based systems that model complex
networks17 or genetic and developmental systems.In some cases, greater availability of data (specifically, perturbation response or time-series data)enables the use of statistical influence models. Linear,18 neural network-like,19 and Bayesian20 modelshave all been used to deduce both the topology of gene expression networks and their dynamics. On the13Section 5.3 is adapted from A.P. Arkin, ÒSynthetic Cell Biology,Ó Current Opinion in Biotechnology 12(6):638-644, 2001.14See, for example, Y. Moreau, P. Antal, G. Fannes, and B. De Moor, ÒProbabilistic Graphical Models for ComputationalBiomedicine, Methods of Information in Medicine 42(2):161-168, 2003.15K.W. Kohn, ÒMolecular Interaction Map of the Mammalian Cell Cycle: Control and DNA Repair Systems,Ó Molecular Biologyof the Cell 10(8):2703-2734, 1999; I. Pirson, N. Fortemaison, C. Jacobs, S. Dremier, J.E. Dumont, and C. Maenhaut, ÒThe VisualDisplay of Regulatory Information and Networks,Ó Trends in Cell Biology 10(10):404-408, 2000. (Both cited in Arkin, 2001.)16H. Jeong, S.P. Mason, A.L. Barabasi, and Z.N. Oltvai, ÒLethality and Centrality in Protein Networks,Ó Nature 411(6833):41-42,2001; H. Jeong, B. Tombor, R. Albert, Z.N. Oltvai, and A.L. Barabasi, ÒThe Largescale Organization of Metabolic Networks,Ó
Nature 407(6804):651-654, 2000. (Cited in Arkin, 2001.)17D. Thieffry and R. Thomas, ÒQualitative Analysis of Gene Networks,Ó pp. 77-88 in Pacific Symposium on Biocomputing, 1998.(Cited in Arkin, 2001.)18P. DÕHaeseleer, X. Wen, S. Fuhrman, and R. Somogyi, ÒLinear Modeling of mRNA Expression Levels During CNS Develop-ment and Injury,Ó pp. 41-52 in Pacific Symposium on Biocomputing, 1999. (Cited in Arkin, 2001.)19E. Mjolsness, D.H. Sharp, and J. Reinitz, ÒA Connectionist Model of Development,Ó Journal of Theoretical Biology 152(4):429-453, 1999. (Cited in Arkin, 2001.)20N. Friedman, M. Linial, I. Nachman, and D. PeÕer, ÒUsing Bayesian Networks to Analyze Expression Data,Ó Journal ofComputational Biology 7(3-4):601-620, 2000. (Cited in Arkin, 2001.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.126CATALYZING INQUIRYother hand, statistical influence models are not causal and may not lead to a better understanding ofunderlying mechanisms.Quantitative models make detailed statements about biological processes and hence are easier tofalsify than more qualitative models. These models are intended to be predictive and are useful for
understanding points of control in cellular networks and for designing new functions within them.Some models are based on power law formalisms.21 In such cases, the data are shown to fit genericpower laws, and the general theory of power law scaling (for example) is used to infer some degree of
causal structure. They do not provide detailed insight into mechanism, although power law models
form the basis for a large class of metabolic control analyses and dynamic simulations.Computational modelsÑsimulationsÑrepresent the other end of the modeling spectrum. Simula-tion is often necessary to explore the implications of a model, especially its dynamical behavior, becauseBox 5.2On Graphical ModelsA large fraction of todayÕs knowledge of biochemical or genetic regulatory networks is represented either astext or as cartoon-like diagrams. However, text has the disadvantage of being inherently ambiguous, andevery reader must reinterpret the text of a journal article. Diagrams are usually informal, often confusing, and
thus fail to present all of the information that is available to the presenter of the research. For example, themeanings of nodes and arcs within a diagram are inconsistentÑone arrow may mean activation, but anotherarrow in the same diagram may mean transition of the state or translocation of materials.To remedy this state of affairs, a system of graphical representation should be powerful enough to expresssufficient information in a clearly visible and unambiguous way and should be supported by software tools.
There are several criteria for a graphical notation system, including the following:1.Expressiveness.The notation system should be able to describe every possible relationship among theentities in a systemÑfor example, those between genes and proteins in a biological model.2.Semantical unambiguity.Notation should be unambiguous. Different semantics should be assigned todifferent symbols that are clearly distinguishable.
3.Visual unambiguity.Each symbol should be identified clearly and not be mistaken with other symbols.This feature should be maintained with low-resolution displays, using only black and white.4.Extension capability.The notation system should be flexible enough to add new symbols and relationshipsin a consistent manner. This may include the use of color coding to enhance expressiveness and readability,but information should not be lost even with black-and-white displays.5.Mathematical translation.The notation should be able to convert itself into mathematical formalisms, suchas differential equations, so that it can be applied directly for numerical analysis.6.Software support.The notation should be supported by software for its drawing, viewing, editing, andtranslation into mathematical formalisms.No current graphical notation system satisfies all of these criteria fully, although a number of systems satisfysome of them.1SOURCE: Adapted by permission from H. Kitano, ÒA Graphical Notation for Biochemical Networks,Ó Biosilico 1(5):159-176. Copyright2003 Elsevier.1See, for example, K.W. Kohn, ÒMolecular Interaction Map of the Mammalian Cell Cycle Control and DNA Repair Systems,Ó MolecularBiology of the Cell 10(8):2703-2734, 1999; K. Kohn, ÒMolecular Interaction Maps as Information Organizers and Simulation Guides,Ó Chaos11(1):84-97, 2001.21E.O. Voit and T. Radivoyevitch, ÒBiochemical Systems Analysis of Genomewide Expression Data,Ó Bioinformatics 16(11):1023-1037, 2000. (Cited in Arkin, 2001.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY127human intuition about complex nonlinear systems is often inadequate.22 Lander cites two examples.The first is that Òintuitive thinking about MAP [mitogen-activated protein] kinase pathways led to the
long-held view that the obligatory cascade of three sequential kinases serves to provide signal amplifi-
cation. In contrast, computational studies have suggested that the purpose of such a network is to
achieve extreme positive cooperativity, so that the pathway behaves in a switch-like, rather than a
graded, fashion.Ó23 The second example is that while intuitive interpretations of experiments in thestudy of morphogen gradient formation in animal development led to the conclusion that simple
diffusion is not adequate to transport most morphogens, computational analysis of the same experi-
mental data led the opposite conclusion.24Simulation, which traces functional biological processes through some period of time, generatesresults that can be checked for consistency with existing data (ÒretrodictionÓ of data) and can also
predict new phenomena not explicitly represented in but nevertheless consistent with existing datasets.
Note also that when a simulation seeks to capture essential elements in some oversimplified and
idealized fashion, it is unrealistic to expect the simulation to make detailed predictions about specific
biological phenomena. Such simulations may instead serve to make qualitative predictions about ten-
dencies and trends that become apparent only when averaged over a large number of simulation runs.
Alternatively, they may demonstrate that certain biological behaviors or responses are robust and do
not depend on particular details of the parameters involved within a very wide range.Simulations can also be regarded as a nontraditional form of scientific communication. Tradition-ally, scientific communications have been carried by journal articles or conference presentations. Though

articles and presentations will continue to be important, simulationsÑin the form of computer pro-
gramsÑcan be easily shared among members of the research community, and the explicit knowledge
embedded in them can become powerful points of departure for the work of other researchers.With the availability of cheap and powerful computers, modeling and simulation have becomenearly synonymous. Yet, a number of subtle differences should be mentioned. Simulation can be used
as a tool on its own or as a companion to mathematical analysis.In the case of relatively simple models meant to provide insight or reveal a concept, analyticaland mathematical methods are of primary utility. With simple strokes and pen-and-paper compu-
tations, the dependence of behavior on underlying parameters (such as rate constants), conditions
for specific dynamical behavior, and approximate connections between macroscopic quantities
(e.g., the velocity of a cell) and underlying microscopic quantities (such the number of actin fila-
ments causing the membrane to protrude) can be revealed. Simulations are not as easily harnessed
to making such connections.Simulations can be used hand-in-hand with analysis for simple models: exploring slight changes inequations, assumptions, or rates and gaining familiarity can guide the best directions to explore with
simple models as well. For example, G. Bard Ermentrout at the University of Pittsburgh developed XPP
software as an evolving and publicly available experimental modeling tool for mathematical biolo-
gists.25 XPP has been the foundation of computational investigations in many challenging problems inneurophysiology, coupled oscillators, and other realms.Mathematical analysis of models, at any level of complexity, is often restricted to special cases that
have simple properties: rectangular boundaries, specific symmetries, or behavior in a special class. Simu-
lations can expand the repertoire and allow the modeler to understand how analysis of the special cases22A.D. Lander, ÒA Calculus of Purpose,Ó PLoS Biology 2 (6):e164, 2004.23C.Y. Huang and J.E. Ferrell, ÒUltrasensitivity in the Mitogen Activated Protein Kinase Cascade,Ó Proceedings of the NationalAcademy of Sciences 93(19):10078-10083, 1996. (Cited in Lander, ÒA Calculus of Purpose,Ó 2004.)24A.D. Lander, Q. Nie, and F.Y. Wan, ÒDo Morphogen Gradients Arise by Diffusion?Ó Developmental Cell 2(6):785-796, 2002.(Cited in Lander, 2004.)25See http://www.math.pitt.edu/~bard/xpp/xpp.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.128CATALYZING INQUIRYrelates to more realistic situations. In this case, simulation takes over where analysis ends.26 Some systemsare simply too large or elaborate to be understood using analytical techniques. In this case, simulation is a
primary tool. Forecasts requiring heavy Ònumber-crunchingÓ (e.g., weather prediction, prediction of cli-
mate change), as well as those involving huge systems of diverse interacting components (e.g., cellular
networks of signal transduction cascades), are only amenable to exploration using simulation methods.More detailed models require a detailed consideration of chemical or physical mechanisms involved(i.e., these models are mechanistic27). Such models require extensive details of known biology and havethe largest data requirements. They are, in principle, the most predictive. In the extreme, one can imagine
a simulation of a complete cellÑan Òin silicoÓ cell or cybercellÑthat provides an experimental framework
in which to investigate many possible interventions. Getting the right format, and ensuring that the in
silico cell is a reasonable representation of reality, has been and continues to be an enormous challenge.No reasonable model is based entirely on a bottom-up analysis. Consider, for example, that solvingSchrıdingerÕs equation for the millions of atoms in a complex molecule in solution would be a futile
exercise, even if future supercomputers could handle this task. The question to ask is how and why such
work would be contemplated: finding the correct level of representation is one of the key steps to good
scientific work. Thus, some level of abstraction is necessary to render any model both interesting
scientifically and feasible computationally. Done properly, abstractions can clarify the sources of con-
trol in a network and indicate where more data are necessary. At the same time, it may be necessary to
construct models at higher degrees of biophysical realism and detail in any event, either because
abstracted models often do not capture the essential behavior of interest or to show that indeed the
addition of detail does not affect the conclusions drawn from the abstracted model.28It is also helpful to note the difference between a computational artifact that reproduces somebiological behavior (a task) and a simulation. In the former case, the relevant question is: ÒHow well
does the artifact accomplish the task?Ó In the latter case, the relevant question is: ÒHow closely does the
simulation match the essential features of the system in question?ÓMost computer scientists would tend to assign higher priority to performance than to simulation.The computer scientist would be most interested in a biologically inspired approach to a computer
science problem when some biological behavior is useful in a computational or computer systems
context and when the biologically inspired artifact can demonstrate better performance than is possible
through some other way of developing or inspiring the artifact. A model of a biological system then
becomes useful to the computer scientist only to the extent that high-fidelity mimicking of how nature
accomplishes a task will result in better performance of that task.By contrast, biologists would put greater emphasis on simulation. Empirically tested and validatedsimulations with predictive capabilities would increase their confidence that they understood in some
fundamental sense the biological phenomenon in question. However, it is important to note that be-
cause a simulation is judged on the basis of how closely it represents the essential features of a biologicalsystem, the question ÒWhat counts as essential?Ó is central (Box 5.3). More generally, one fundamental
focus of biological research is a determination of what the ÒessentialÓ features of a biological system are,26At times, it is also desirable to employ a mix of analysis and simulation. Analysis would be used to generate the basicequations underlying a complex phenomenon. Solutions to these equations would then be explored and with luck, considerablysimplified. The simplified models can then be simulated. See, for example, E.A. Ezrachi, R. Levi, J.M. Camhi, and H. Parnas,
ÒRight-Left Discrimination in a Biologically Oriented Model of the Cockroach Escape System,Ó Biological Cybernetics 81(2):89-99,1999.27Note that mechanistic models can be stochasticÑthe term ÒmechanisticÓ should not be taken to mean deterministic.28Tensions between these perspectives were apparent even in reviews of the draft of this report. In commenting on neuro-science topics in this report, advocates of the first point of view argued that ultrarealistic simulations accomplish little to furtherour understanding about how neurons work. Advocates of the second point of view argued that simple neural models could not
capture the implications of the complex dynamics of each neuron and its synapses and that these models would have to besupplemented by more physiological ideas. From the committeeÕs perspective, both points of view have merit, and the scientificchallenge is to find an appropriate simplification or abstraction that does capture the interesting behavior at reasonable fidelity.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY129recognizing that what is ÒessentialÓ cannot be determined once and for all, but rather depends on theclass of questions under consideration.5.3.2Hybrid Models
Hybrid models are models composed of objects with different mathematical representations. Theseallow a model builder the flexibility to mix modeling paradigms to describe different portions of a
complex system. For example, in a hybrid model, a signal transduction pathway might be described by
a set of differential equations, and this pathway could be linked to a graphical model of the genetic
regulatory network that it influences. An advantage of hybrid models is that model components can
evolve from high-level abstract descriptions to low-level detailed descriptions as the components are
better characterized and understood.An example of hybrid model use is offered by McAdams and Shapiro,29 who point out that geneticnetworks involving large numbers of genes (more than tens) are difficult to analyze. Noting the Òmany
parallels in the function of these biochemically based genetic circuits and electrical circuits,Ó they
propose Òa hybrid modeling approach that integrates conventional biochemical kinetic modeling within

the framework of a circuit simulation. The circuit diagram of the bacteriophage lambda lysislysogeny
decision circuit represents connectivity in signal paths of the biochemical components. A key feature of
the lambda genetic circuit is that operons function as active integrated logic components and introduce
signal time delays essential for the in vivo behavior of phage lambda.ÓThere are good numerical methods for simulating systems that are formulated in terms of ordinarydifferential equations or algebraic equations, although good methods for analysis of such models are
still lacking. Other systems, such as those that mix continuous with discrete time or Markov processes
with partial differential equations, are sometimes hard to solve even by numerical methods. Further, a
particular model object may change mathematical representation during the course of the analysis. For
example, at the beginning of a biosynthetic process there may be very small amounts of product so itsBox 5.3An Illustration of ÒEssentialÓConsider the following modeling task. The phenomenon of interest is a monkey learning to fetch a bananafrom behind a transparent conductive screen. The first time, the monkey sees the banana, goes straight ahead,
bumps into the screen, and then goes around the screen to the banana. The second time, the monkey, havingdiscovered the existence of the screen that blocks his way, goes directly around the screen to the banana.To model this phenomenon, a system is constructed, consisting of a charged ball and a metal sheet. Thecharged metal ball is hung from a string above the banana and then held at an angle so the screen separatesthe ball and the banana. The first time the ball is released, the ball swings toward the screen, and then touches
it, transferring part of its charge to the screen. The similar charges on the screen and the ball now repel eachother, and the ball swings around the screen. The second time the ball is released, the ball sees a similarlycharged screen and goes around the screen directly.This model reproduces the behavior of the monkey in the first instance. However, no one would claim that itis an accurate model of the learning that takes place in the monkeyÕs brain, even though the model replicates
the most salient feature of the monkeyÕs learning consistently: both the ball and the monkey dodge the screenon the second attempt. In other words, even though it demonstrates the same behavior, the model does notrepresent the essential features of the biological system in question.29See H.H. McAdams and L. Shapiro, ÒCircuit Simulation of Genetic Networks,Ó Science 269(5224):650-656, 1994.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.130CATALYZING INQUIRYconcentration would have to be modeled discretely. As more of it is synthesized, the concentrationbecomes high enough that a continuous approximation is justified and is then more efficient for simu-
lation and analysis.The point at which this switch is made is dependent not just on copy number but also on where in thedynamical state space the system resides. If the system is near a bifurcation point, small fluctuations may be
significant. Theories of how to accomplish this dynamic switching are lacking. As models grow more
complex, different parts of the system will have to be modeled with different mathematical representations.
Also, as models from different sources begin to be joined, it is clear that different representations will be
used. It is critical that the theory and applied mathematics of hybrid dynamical systems be developed.5.3.3Multiscale Models
Multiscale models describe processes occurring at many time and length scales. Depending on thebiological system of interest, the data needed to provide the basis for a greater understanding of the
system will cut across several scales of space and time. The length dimensions of biological interest
range from small organic molecules to multiprotein complexes at 100 angstroms to cellular processes at
1,000 angstroms to tissues at 1-10 microns, and the interaction of human populations with the environ-
ment at the kilometer scale. The temporal domain includes the femtosecond chemistry of molecular
interactions to the millions of years of evolutionary time, with protein folding in seconds and cell and
developmental processes in minutes, hours, and days. In turn, the scale of the process involved (e.g.,
from the molecular scale to the ecosystem scale) affects both the complexity of the representation (e.g.,
molecule base, concentration based, at equilibrium or fully dynamic) and the modality of the represen-
tation (e.g., biochemical, genetic, genomic, electrophysiological, etc.).Consider the heart as an example. The macroscopic unit of interest is the heartbeat, which lastsabout a second and involves the whole heart of 10 cm scale. But the cardiac action potential (the
electrical signal that initiates myocellular contractions) can change significantly on time scales of milli-
seconds as reflected in the appropriate kinetic equations. In turn, the molecular interactions that under-
lie kinetic flows occur on time scales on the order of femtoseconds. Across such variation in time scales,
it is not feasible to model 1015 molecular interactions in order to model a complete heartbeat. Fortu-nately, in many situations the response with the shorter time scale will converge quickly to equilibrium
or quasi-steady-state behavior, obviating the need for a complete lower-level simulation.30For most biological problems, the scale at which data could provide a central insight into theoperation of the whole system is not known, so multiple scales are of interest. Thus, biological models
have to allow for transition among different levels of resolution. A biologist might describe a protein as
a simple ellipsoid and then in the next breath explain the effect of a point mutation by the atomic-level
structural changes it causes in the active site.31Identifying the appropriate ranges of parameters (e.g., rate constants that govern the pace of chemi-cal reactions) remains one of the difficulties that every modeler faces sooner or later. As modelers know
well, even qualitative analysis of simple models depends on knowing which Òleading-order termsÓ are
to be kept on which time scales. When the relative rates are entirely unknownÑtrue of many biochemi-
cal steps in living cellsÑit is hard to know where to start and how to assemble a relevant model, a point
that underscores the importance of close dialogue between the laboratory biologist and the mathemati-
cal or computational modeler.Finally, data obtained at a particular scale must be sufficient to summarize the essential biologicalactivity at that scale in order to be evaluated in the context of interactions at greater scales of complexity.
The challenge, therefore, is one of understanding not only the relationship of multiple variables operat-
ing at one scale of detail, but also the relationship of multivariable datasets collected at different scales.30A.D. McCulloch and G. Huber, ÒIntegrative Biological Modelling in Silico,Ó pp. 4-25 in ÔIn SilicoÕ Simulation of BiologicalProcesses No. 247,
 Novartis Foundation Symposium, G. Bock and J.A. Goode, eds., John Wiley & Sons Ltd., Chichester, UK, 2002.31D. Endy and R. Brent, ÒModeling Cellular Behavior,Ó Nature 409(6818):391-395, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY1315.3.4Model Comparison and Evaluation
Models are ultimately judged by their ability to make predictions. Qualitative models predict trendsor types of dynamics that can occur, as well as thresholds and bifurcations that delineate one type of
behavior from another. Quantitative models predict values that can be compared to actual experimental
data. Therefore, the selection of experiments to be performed can be determined, at least in part, by their
usefulness in constraining a model or selecting one model from a set of competing models.The first step in model evaluation is to replicate and test a computational model of biologicalsystems that has been published. However, most papers contain typographical errors and do not pro-
vide a complete specification of the biological properties that were represented in the model. One
should be able to extract the specification from the modelÕs source code, but for a whole host of reasons
it is not always possible to obtain the actual files that were used for the published work.In the neuroscience field, ModelDB (http://senselab.med.yale.edu/senselab/modeldb/) is beingdeveloped to answer the need for a database of published models used in neuroscience research.32 It ispart of the SenseLab project (http://senselab.med.yale.edu/), which is supported through the Human
Brain Project by the National Institute of Mental Health (NIMH), the National Institute of Neurologist
disorders and Stroke (NINDS), and the National Cancer Institute (NCI).ModelDB is a curated database that is designed for convenient entry, search, and retrieval of modelswritten for any programming language or simulation environment. As of December 10, 2004, it con-
tained 141 downloadable models. Most of these are for NEURON, but 40 of them are for MATLAB,
GENESIS, SNNAP, or XPP, and there are also some models in C/C++ and FORTRAN. Database entries
are linked to the published literature so that users can more easily determine the Òscientific contextÓ of
any given model.Although ModelDB is still in a developmental or research stage, it has already begun to have a positiveeffect on computational modeling in neuroscience. Database logs indicate that it is seeing heavy usage, and
from personal communications the committee has learned that even experienced programmers who write
their own code in C/C++ are regularly examining models written for NEURON and other domain-specific
simulators, in order to determine key parameter values and other important details. Recently published
papers are beginning appear that cite ModelDB and the models it contains as sources of code, equations, or
parameters. Furthermore, a leading journal has adopted a policy that requires authors to make their source
code available as a condition of publication and encourages them to use ModelDB for this purpose.As for model comparison, it is not possible to ascertain in isolation whether a given model is correctsince contradictory data may become available later, and indeed even ÒincorrectÓ models may make
correct predictions. Suitably complex models can be made to fit to any dataset, and one must guard
against ÒoverfittingÓ a model. Thus, the predictions of a model must be viewed in the context of the
number of degrees of freedom of the model, and one measure that one model is better than another is a
judgment about which model best explains experimental data with the least model complexity. In some
cases, measures of the statistical significance of a model can be computed using a likelihood distribution
over predicated state variables taking into account the number of degrees of freedom present in the model.At the same time, lessons learned over many centuries of scientific investigation regarding the use ofOccamÕs Razor may have limited applicability in this context. Because biological phenomena are the result
of an evolutionary process that simply uses what is available, many biological phenomena are simply
cobbled together and in no sense can be regarded as the ÒsimplestÓ way to accomplish something.As noted in Footnote 28, there is a tension between the need to capture details faithfully in a modeland the desire to simplify those details so as to arrive at a representation that can be analyzed, understood
fully, and converted into scientific Òknowledge.Ó There are numerous ways of reducing models that are
well known in applied mathematics communities. These include dimensional analysis and multiple time-
scale analysis (i.e., dissecting a system into parts that evolve rapidly versus those that change on a slower32M.L. Hines, T. Morse, M. Migliore, N.T. Carnevale, and G.M. Shepherd, ÒModelDB: A Database to Support ComputationalNeuroscience,Ó Journal of Computational Neuroscience 17(1):7-11, 2004; B.J. Richmond, ÒEditorial Commentary,Ó Journal of Computa-tional Neuroscience 17(1):5, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.132CATALYZING INQUIRYtime scale). In some cases, leaving out some of the interacting components (e.g., those whose interactionsare weakest or least significant) may be a workable method. In other cases, lumping together families or
groups of substances to form aggregate components or compartments works best. Sensitivity analysis of
alternative model structures and parameters can be performed using likelihood and significance mea-
sures. Sensitivity analysis is important to inform a model builder of the essential components of the model
and to attempt to reduce model complexity without loss of explanatory power.Model evaluation can be complicated by the robustness of the biological organism being repre-sented. Robustness generally means that the organism will endure and even prosper under a wide
range of conditionsÑwhich means that its behavior and responses are relatively insensitive to varia-
tions in detail.33 That is, such differences are unlikely to matter much for survival. (For example, themodeling of genetic regulatory networks can be complicated by the fact that although the data may
show that a certain gene is expressed under certain circumstances, the biological function being served
may not depend on the expression of that gene.) On the other hand, this robustness may also mean that
a flawed understanding of detailed processes incorporated into a model that does explain survival
responses and behavior will not be reflected in the modelÕs output.34Simulation models are essentially computer programs and hence suffer from all of the problemsthat plague software development. Normal practice in software development calls for extensive testing
to see that a program returns the correct results when given test data for which the appropriate results
are known independently of the program as well as for independent code reviews. In principle, simula-
tion models of biological systems could be subject to such practices. Yet the fact that a given simulation
model returns results that are at variance with experimental data may be attributable to an inadequacy
of the underlying model or to an error in programming.35 Note also that public code reviews areimpossible if the simulation models are proprietary, as they often are when they are created by firms
seeking to obtain competitive advantage in the marketplace.These points suggest a number of key questions in the development of a model.¥How much is given up by looking at simplified versions?¥How much poorer, and in what ways poorer, is a simplified model in its ability to describe the system?¥Are there other, new ways of simplifying and extracting salient features?¥Once the simplified representation is understood, how can the details originally left out bereincorporated into a model of higher fidelity?Finally, another approach to model evaluation is based on notions of logical consistency. Thisapproach uses program verification tools originally developed by computer scientists to determine
whether a given program is consistent with a given formal specification or property. In the biological
context, these tools are used to check the consistency and completeness of a modelÕs description of the
biological systemÕs processes. These descriptions are dynamic and thus permit ÒrunningÓ a model to
observe developments in time. Specifically, Kam et al. have demonstrated this approach using the
languages, methods, and tools of scenario-based reactive system design and applied it to modeling the
well-characterized process of cell fate acquisition during Caenorhabditis elegans vulval development.(Box 5.4 describes the intellectual approach in more detail.36)33L.A. Segel, ÒComputing an Organism,Ó Proceedings of the National Academy of Sciences 98(7):3639-3640, 2001.34On the basis of other work, Segel argues that a biological model enjoys robustness only if it is ÒcorrectÕÕ in certain essential features.35Note also the well-known psychological phenomenon in programmingÑbeing a captive of oneÕs test data. Programmingerrors that prevent the model from accounting for the data tend to be hunted down and fixed. However, if the model doesaccount for the data, there is a tendency to assume that the program is correct.36N. Kam, D. Harel, H. Kugler, R. Marelly, A. Penueli, J. Hubbard, et al., ÒFormal Modeling of C. elegans Development: AScenario-based Approach,Ó pp. 4-20 in Proceedings of the First International Workshop on Computational Methods in Systems Biology(CMSB03; Rovereto, Italy, February 2003), Vol. 2602, Lecture Notes in Computer Science, Springer-Verlag, Berlin, Heidelberg,2003. This material is scheduled to appear in the following book: G. Ciobanu, ed., Modeling in Molecular Biology,
 Natural Comput-ing Series, Springer, available at http://www.wisdom.weizmann.ac.il/~kam/CelegansModel/Publications/MMB_Celegans.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY133Box 5.4Formal Modeling of Caenorhabditis elegans DevelopmentOur understanding of biology has become sufficiently complex that it is increasingly difficult to integrate all therelevant facts using abstract reasoning alone. [Formal modeling presents] a novel approach to modeling biological
phenomena. It utilizes in a direct and powerful way the mechanisms by which raw biological data are amassed, andsmoothly captures that data within tools designed by computer scientists for the design and analysis of complexreactive systems.A considerable quantity of biological data is collected and reported in a form that can be called Òcondition-resultÓdata. The gathering is usually carried out by initializing an experiment that is triggered by a certain set of circum-
stances (conditions), following which an observation is made and the results recorded. The condition is most oftena perturbation, such as mutating genes or exposing cells to an altered environment. . . . [and] a large proportion ofbiological data is reported as stories, or Òscenarios,Ó that document the results of experiments conducted under
specific conditions.The challenge of modeling these aspects of biology is to be able to translate such Òcondition-resultÓ phenomenafrom the ÒscenarioÓ-based natural language format into a meaningful and rigorous mathematical language. Sucha translation process will allow these data to be integrated more comprehensively by the application of high-levelcomputer-assisted analysis. In order for it to be useful, the model must be rigorous and formal, and thus amenable
to verification and testing.We have found that modeling methodologies originating in computer science and software engineering, and createdfor the purpose of designing complex reactive systems, are conceptually well suited to model this type of condition-result biological data. Reactive systems are those whose complexity stems not necessarily from complicated compu-tation but from complicated reactivity over time. They are most often highly concurrent and time-intensive, and
exhibit hybrid behavior that is predominantly discrete in nature but has continuous aspects as well. The structure ofa reactive system consists of many interacting components, in which control of the behavior of the system is highlydistributed amongst the components. Very often the structure itself is dynamic, with its components being repeatedly
created and destroyed during the systemÕs life span.The most widely used frameworks for developing models of such systems feature visual formalisms, which are bothgraphically intuitive and mathematically rigorous. These are supported by powerful tools that enable full modelexecutability and analysis, and are linkable to graphical user interfaces (GUIs) of the system. This enables realisticsimulation prior to actual implementation. At present, such languages and toolsÑoften based on the object-orientedparadigmÑare being strengthened by verification modules, making it possible not only to execute and simulate thesystem models (test and observe) but also to verify dynamic properties thereof (prove). . . .[M]any kinds of biological systems exhibit characteristics that are remarkably similar to those of reactive systems.The similarities apply to many different levels of biological analysis, including those dealing with molecular, cellular,organ-based, whole organism, or even population biology phenomena. Once viewed in this light, the dramatic
concurrency of events, the chain-reactions, the time-dependent patterns, and the event-driven discrete nature oftheir behaviors, are readily apparent. Consequently, we believe that biological systems can be productively modeledas reactive systems, using languages and tools developed for the construction of man-made systems. . . .SOURCE: N. Kam et al., ÒFormal Modeling of C. elegans Development: A Scenario-based Approach,Ó pp. 4-20 in Proceedings of the FirstInternational Workshop on Computational Methods in Systems Biology (CMSB03; Rovereto, Italy, February 2003), Vol. 2602, Lecture Notesin Computer Science, Springer-Verlag, Berlin, Heidelberg, 2003, available at http://www.wisdom.weizmann.ac.il/~kam/CelegansModel/Publications/MMB_Celegans.pdf. Reprinted with permission from Springer-Verlag.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.134CATALYZING INQUIRY5.4MODELING AND SIMULATION IN ACTION
The preceding discussion has been highly abstract. This section provides some illustrations of howmodeling and simulation have value across a variety of subfields in biology. No claim is made to
comprehensiveness, but the committee wishes to illustrate the utility of modeling and simulations at
levels of organization from gene to ecosystem.5.4.1Molecular and Structural Biology
5.4.1.1Predicting Complex Protein Structures
Interactions between proteins are crucial to the functioning of all cells. While there is much experi-mental information being gathered regarding protein structures, many interactions are not fully under-
stood and have to be modeled computationally. The topic of computational prediction of protein-
protein structure remains to be solved and is one of the most active areas of research in bioinformatics
and structural biology.ZDOCK and RDOCK are two computer programs that address this problem, also known as proteindocking.37 ZDOCK is an initial stage protein docking program that performs a full search of the relativeorientations of two molecules (referred to by convention as the ligand and receptor) to determine their
best fit based on surface complementarity, electrostatics and desolvation. The efficiency of the algo-
rithm is enhanced by discretizing the molecules onto a grid and performing a fast Fourier transform
(FFT) to quickly explore the translational degrees of freedom.RDOCK takes as input the ZDOCK predictions and improves them using two steps. The first step isto improve the energetics of the prediction and remove clashes by performing small movements of the
predicted complex, using a program known as CHARMM. The second step is to rescore these mini-
mized predictions with more detailed scoring functions for electrostatics and desolvation.The combination of these two algorithms has been tested and verified with a benchmark set ofproteins collected for use in testing docking algorithms. Now at version 2.0, this benchmark is publicly
available and contains 87 test cases. These test cases cover a breadth of interactions, such as antibody-
antigen, and cases involving significant conformational changes.The ZDOCK-RDOCK programs have consistently performed well in the international dockingcompetition CAPRI (Figure 5.1). Some notable predictions were for the Rotavirus VP6/Fab (50 of 52contacting residues correctly predicted), and SAG-1/Fab complex (61 of 70 contacts correct), and the
cellulosome cohesion-dockerin structure (50 of 55 contacts correct). In the first two cases, the number of
contacts in the ZDOCK-RDOCK predictions were the highest among all participating groups.5.4.1.2A Method to Discern a Functional Class of Proteins
The DNA-binding helix-turn-helix structural motif plays an essential role in a variety of cellularpathways that include transcription, DNA recombination and repair, and DNA replication. Current
methods for identifying the motif rely on amino acid sequence, but since members of the motif belong
to different sequence families that have no sequence homology to each other, these methods have been
unable to identify all motif members.A new method based on three-dimensional structure was created that involved the followingsteps:38 (1) choosing a conserved component of the motif, (2) measuring structural features relative37For more information, see http://zlab.bu.edu.38W.A. McLaughlin and H.M. Berman, ÒStatistical Models for Discerning Protein Structures Containing the DNA-bindingHelix-Turn-Helix Motif,Ó Journal of Molecular Biology 330(1):43-55, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY135to that component, and (3) creating classification models by comparing measurements of structuresknown to contain the motif to measurements of structures known not to contain the motif. In this
case, the conserved component chosen was the recognition helix (i.e., the alpha helix that makes
sequence-specific contact with DNA), and two types of relevant measurements were the hydropho-
bic area of interaction between secondary structure elements (SSEs) and the relative solvent acces-
sibility of SSEs.With a classification model created, the entire Protein Data Bank of experimentally measured struc-tures was searched and new examples of the motif were found that have no detected sequence homol-
ogy with previously known examples. Two such examples are Esa1 histone acetyltransferase and
isoflavone 4-O-methyltransferase. The result emphasizes an important utility of the approach: sequence-
based methods used to discern a functional class of proteins may be supplemented through the use of a
classification model based on three-dimensional structural information.FIGURE 5.1The ZDOCK/RDOCK prediction for dockerin (in red) superposed on the crystal structure for CAPRI
Target 13, cohesin/dockerin. SOURCE: Courtesy of Brian Pierce and Zhiping Weng, Boston University.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.136CATALYZING INQUIRY5.4.1.3Molecular Docking
Using a simple, uniform representation of molecular surfaces that requires minimal parameteriza-tion, Jain39 has constructed functions that are effective for scoring protein-ligand interactions, quantita-tively comparing small molecules, and making comparisons of proteins in a manner that does not
depend on protein backbone. These methods rely on computational approaches that are rooted in
understanding the physics of molecular interactions, but whose functional forms do not resemble thoseused in physics-based approaches. That is, this problem can be treated as a pure computer science
problem that can be solved using combinations of scoring and search or optimization techniques pa-
rameterized with the use of domain knowledge. The approach is as follows:¥Molecules are approximated as collections of spheres with fixed radii: H = 1.2; C = 1.6; N = 1.5; O =1.4; S = 1.95; P = 1.9; F = 1.35; Cl = 1.8; Br = 1.95; I = 2.15.¥A labeling of the features of polar atoms is superimposed on the molecular representation:polarity, charge, and directional preference (Figure 5.2, subfigures A and B).¥A scoring function is derived that, given a protein and a ligand in some relative alignment, yieldsa prediction of the energy of interaction.¥The function is parameterized in terms of the pairwise distances between molecular surfaces.¥The dominant terms are a hydrophobic term that characterizes interactions between nonpolaratoms and a polar term that captures complementary polar contacts with proper directionality.¥The parameters of the function were derived from empirical binding data and 34 protein-ligandcomplexes that were experimentally determined.¥The scoring function is described in Figure 5.2, Subfigure C. The hydrophobic term peaks atapproximately 0.1 unit with a slight surface interpenetration. The hydrophobic term for an ideal hydro-
gen bond peaks at 1.25 units, and a charged interaction (tertiary amine proton (+1.0) to a charged
carboxylate (Ð0.5)) peaks at about 2.3 units. Note that this scoring function looks nothing like a force
field derived from molecular mechanics.¥Figure 5.2, Subfigure D compares eight docking methods on screening efficiency using thymi-dine kinase as a docking target. For the test, 10 known ligands and 990 random ligands were used.
Particularly at low false-positive rates (low database coverage), the scoring function approach shows
substantial improvements over the other methods.5.4.1.4Computational Analysis and Recognition of Functional and
Structural Sites in Protein Structures40Structural genomics initiatives are producing a great increase in protein three-dimensional struc-tures determined by X-ray and nuclear magnetic resonance technologies as well as those predicted by
computational methods. A critical next step is to study the relationships between protein structures and
functions. Studying structures individually entails the danger of identifying idiosyncratic rather than
conserved features and the risk of missing important relationships that would be revealed by statisti-39See A.N. Jain, ÒScoring Noncovalent Protein Ligand Interactions: A Continuous Differentiable Function Tuned to ComputeBinding Affinities,Ó Journal of Computer-Aided Molecular Design 10(5):427-440, 1996; W. Welch, J. Ruppert, and A.N. Jain, ÒHam-merhead: Fast, Fully Automated Docking of Flexible Ligands to Protein Binding Sites,Ó Chemistry & Biology 3(6):449-462, 1996; J.Ruppert, W. Welch, and A.N. Jain, ÒAutomatic Identification and Representation of Protein Binding Sites for Molecular Dock-ing,Ó Protein Science 6(3):524-533, 1997; A.N. Jain, ÒSurflex: Fully Automatic Flexible Molecular Docking Using a MolecularSimilarity-based Search Engine,Ó Journal of Medicinal Chemistry 46(4):499-511, 2003; A.N. Jain, ÒLigand-Based Structural Hypoth-eses for Virtual Screening.Ó Journal of Medicinal Chemistry 47(4):947-961, 2004.40Section 5.4.1.4 is based on material provided by Liping Wei, Nexus Genomics, Inc., and Russ Altman, Stanford University,personal communication, December 4, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY137cally pooling relevant data. The expected surfeit of protein structures provides an opportunity to de-velop computational methods for collectively examining multiple biological structures and extracting
key biophysical and biochemical features, as well as methods for automatically recognizing these fea-
tures in new protein structures.Wei and Altman have developed an automated system known as FEATURE that statistically stud-ies the important functional and structural sites in protein structures such as active sites, binding sites,
disulfide bonding sites, and so forth. FEATURE collects all known examples of a type of site from the
Protein Data Bank (PDB) as well as a number of control ÒnonsiteÓ examples. For each of them, FEA-
TURE computes the spatial distributions of a large set of defined biophysical and biochemical proper-
ties spanning multiple levels of details in order to capture conserved features beyond basic amino acid
sequence similarity. It then uses a nonparametric statistical test, the Wilcoxin Rank Sum Test, to find the
features that are characteristic of the sites, in the context of control nonsites. Figure 5.3 shows the
statistical features of calcium binding sites.By using a Bayesian scoring function that recognizes whether a local region within a three-dimen-sional structure is likely to be any of the sites and a scanning procedure that searches the whole
structure for the sites, FEATURE can also provide an initial annotation of new protein structures.
FEATURE has been shown to have good sensitivity and specificity in recognizing a diverse set of site
types, including active sites, binding sites, and structural sites and is especially useful when the sites do
not have conserved residues or residue geometry. Figure 5.4 shows the result of searching for ATP
(adenosine triphosphate) binding sites in a protein structure.FIGURE 5.2A Computational Approach to Molecular Docking. SOURCE: Courtesy of A.N. Jain, University of
California, San Francisco.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.138CATALYZING INQUIRYVOLUMECalcium Model012345
ATOM-NAME-IS-ANY<>>>
ATOM-NAME-IS-C<<>>>
ATOM-NAME-IS-N<<>
ATOM-NAME-IS-O<>>>>>
AMIDE<<>
AMINE<
CARBONYL<>>>>
RING-SYSTEM<
PEPTIDE<>>>
VDW-VOLUME<<>>
CHARGE>>>
NEG-CHARGE>>>
CHARGE-WITH-HIS>>>
HYDROPHOBICITY<<<<
MOBILITY<>>
SOLVENT-ACCESSIBILITY<>
RESIDUE_NAME_IS_ASN>>>>
RESIDUE_NAME_IS_ASP>>>>>
RESIDUE_NAME_IS_GLU>>>>>
RESIDUE_NAME_IS_GLY>>>
RESIDUE_NAME_IS_ILE>
RESIDUE_NAME_IS_LEU>
RESIDUE_NAME_IS_LYS>
RESIDUE_NAME_IS_SER>>
RESIDUE_NAME_IS_VAL<<
RESIDUE_NAME_IS_HOH>
RESIDUE_CLASS1_IS_HYDROPHOBIC<<
RESIDUE_CLASS1_IS_CHARGED>>>>>
RESIDUE_CLASS1_IS_POLAR<>>>
RESIDUE_CLASS1_IS_UNKNOWN>>
RESIDUE_CLASS2_IS_NONPOLAR<<
RESIDUE_CLASS2_IS_POLAR<>>
RESIDUE_CLASS2_IS_BASIC<
RESIDUE_CLASS2_IS_ACIDIC>>>>>
RESIDUE_CLASS2_IS_UNKNOWN>>
SECONDARY_STRUCTURE1_IS_TURN>
SECONDARY_STRUCTURE1_IS_BEND>>>>>
SECONDARY_STRUCTURE1_IS_COIL>>>>>
SECONDARY_STRUCTURE1_IS_HET>>
SECONDARY_STRUCTURE2_IS_BETA<>>
SECONDARY_STRUCTURE2_IS_COIL>>>>>
SECONDARY_STRUCTURE2_IS_HET>>
FIGURE 5.3Statistical features of calcium binding sites determined by FEATURE. The volumes in this case corre-
spond to concentrate radial shells 1 † in thickness around the calcium ion or a control nonsite location. Thecolumn shows properties that are statistically significantly different (at p-value cutoff of 0.01) in at least onevolume between known examples of calcium binding sites and those of control nonsites. A Ò>Ó (greater than sign)
indicates that the calcium binding sites have significantly higher value for that property at that volume comparedto control nonsites. A Ò<Ó (less than sign) indicates the opposite. An empty box indicates the lack of statisticallysignificant difference. SOURCE: Courtesy of Liping Wei, Nexus Genomics, Inc., and Russ Altman, Stanford Uni-
versity, personal communication, December 4, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY1395.4.2Cell Biology and Physiology
5.4.2.1Cellular Modeling and Simulation Efforts
Cellular simulation requires a theoretical framework for analyzing the interactions of molecularcomponents, of modules made up of those components, and of systems in which such modules are
linked to carry out a variety of functions. The theoretical goal is to quantitatively organize, analyze, and
interpret complex data on cell biological processes, and experiments provide images, biochemical and
electrophysiological data on the initial concentrations, kinetic rates, and transport properties of the
molecules and cellular structures that are presumed to be the key components of a cellular event.41 Asimulation embeds the relevant rate laws and rate constants for the biochemical transformations being
modeled. Based on these laws and parameters, the model accepts as initial conditions the initial concen-
trations, diffusion coefficients, and locations of all molecules implicated in the transformation, and
generates predictions for the concentration of all molecular species as a function of time and space.
These predictions are compared against experiment, and the differences between prediction and experi-
ment are used to further refine the model. If the system is perturbed by the addition of a ligand,
electrical stimulus, or other experimental intervention, the model should be capable of predicting

changes as well in the relevant spatiotemporal distributions of the molecules involved.FIGURE 5.4Results of automatic scanning for ATP binding sites in the structure of casein kinase (PDB ID 1csn)
using WebFEATURE, a freely available, Web-based server of FEATURE. The solid red dots show the prediction ofFEATURE, they correspond correctly with the true location of the ATP binding site, shown as white cloud.
SOURCE: Courtesy of Liping Wei, Nexus Genomics, Inc., and Russ Altman, Stanford University, personal commu-nication, December 4, 2003.41A brief introduction to the rationale underlying cellular modeling can be found at the National Resource for Cell Analysisand Modeling (http://www.nrcam.uchc.edu/applications/applications.html).Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.140CATALYZING INQUIRYThere are many different tools for simulating and analyzing models of cellular systems (Table 5.1).More general tools, such as Mathematica and MATLAB or other systems that can be used for solving
systems of differential or stochastic-differential equations, can be used to develop simulations, and
because these tools are commonly used by many researchers, their use facilitates the transfer of models
among different researchers. Another approach is to link data gathering and biological information
systems to software that can integrate and predict behavior of interacting components (currently, re-
searchers are far from this goal, but see Box 5.5 and Box 5.6). Finally, several platform-independent
model specification languages are under development that will facilitate greater sharing and
interoperability. For example, SBML,42 Gepasi,43 and CellML44 are specialized systems for biologicaland biochemical modeling. Madonna45 is a general-purpose system for solving a variety of equations(differential equations, integral equations, and so on).Rice and Stolovitzky describe the task of inferring signaling, metabolic, or gene regulatory path-ways from experimental data as one of reverse engineering.46 They note that automated, high-through-put methods that collect species- and tissue-specific datasets in large volume can help to deal with the
risks in generalizing signaling pathways from one organism to another. At the same time, fully detailed
kinetic models of intracellular processes are not generally feasible. Thus, one step is to consider models
that describe network topology (i.e., that identify the interactions between nodes in the systemÑgenes,
proteins, metabolites, and so on). A model with more detail would describe network topology that is
causally directional (i.e., that specifies which entities serve as input to others). Box 5.7 provides more
detail.TABLE 5.1Sample Simulation Programs
NameDescriptors
aWeb SiteGepasi/CopasifkFWhttp://gepasi.dbs.aber.ac.uk/softw/gepasi.html
BioSimqWMUhttp://www.molgen.mpg.de/~biosim/BioSim/BioSimHome.html

JarnackrfbFWShttp://members.tripod.co.uk/sauro/Jarnac.htm
MCELLrsUhttp://www.mcell.cnl.salk.edu/
Virtual CellksDFWMUhttp://www.nrcam.uchc.edu/

E-CellkWUShttp://www.e-cell.org/
NeuronksFWMUShttp://neuron.duke.edu/
GenesisksUShttp://www.bbb.caltech.edu/GENESIS/genesis.html

PlaskfbFWhttp://correio.cc.fc.ul.pt/~aenf/plas.html
IngeneueqkFMWUShttp://www.ingeneue.org/
DynaFitkfWhttp://www.biokin.com/dynafit/

StochsimrShttp://www.zoo.cam.ac.uk/comp-cell/StochSim.html
T7 SimulatorkUShttp://virus.molsci.org/t7/
Molecularizer/StochastiratorkrUShttp://opnsrcbio.molsci.org/alpha/comps/sim.html
NOTE: All packages have facilities for chemical kinetic simulation of one sort or another.  Some are better designed for metabo
licsystems, others for electrochemical systems, and still others for genetic systems.aThe descriptors are as follows: b, bifurcation analyses and steady-state calculation; f, flux balance or metabolic control andrelated analyses; k, deterministic kinetic simulation; q, qualitative simulation; r, stochastic process models; s, spatial processes; D,database connectivity; F, fitting, sensitivity, and optimization code; M, runs on Macintosh; S, source code available; U, runs onLinux or Unix; W, runs on windows.42See http://www.cds.caltech.edu/erato/sbml/.43See http://www.gepasi.org/.44See http://www.cellml.org/.45See http://www.berkeleymadonna.com/.46 J.J. Rice and G. Stolovitzky, ÒMaking the Most of It: Pathway Reconstruction and Integrative Simulation Using the Data atHand,Ó Biosilico 2:70-77, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY141An example of a cellular simulation environment is E-CELL, an open-source system for modelingbiochemical and genetic processes. Organizationally, E-CELL is an international research project aimed
at developing theoretical and functioning technologies to allow precise Òwhole cellÓ simulation; it is
supported by the New Energy and Industrial Technology Development Organization (NEDO) of Japan.E-CELL simulations allow a user to model hypothetical virtual cells by defining functions of pro-teins, protein-protein interactions, protein-DNA interactions, regulation of gene expression, and other
features of cellular metabolism.47 Based on reaction rules that are known through experiment and
assumed concentrations of various molecules in various locations, E-CELL numerically integrates dif-
ferential equations implicitly described in these reaction rules, resulting in changes over time in the
concentrations of proteins, protein complexes, and other chemical compounds in the cell.Developers hope E-CELL will ultimately allow investigators a cheap, fast way to screen drugcandidates, study the effects of mutations or toxins, or simply probe the networks that govern cell
behavior. One application of E-CELL has been to construct a model of a hypothetical cell capable ofBox 5.5BioSPICEBioSPICE, the Biological Simulation Program for Intra-Cellular Evaluation, is in essence a modeling frameworkthat provides users with model components, tools, databases, and infrastructure to develop predictive dynam-
ical models of cellular function. BioSPICE seeks to promote a synergy between experiment and model, inwhich model predictions drive experiment and experimental results identify areas in which a given modelneeds to be improved, and the intent is that researchers go from data to models to analysis and hypothesis
generation, iteratively refining their understanding of the biological processes.An important component of BioSPICE is a library of experimentally validated (and hence trusted) modelcomponents that can be used as starting points in larger-scale simulations, as elements from this library arecomposed in new ways or adapted to investigate other biological systems. Many biological parts and process-es are represented as components, including phosphorylization events, chemotaxis, and conserved elements
of various pathways. Also, because BioSPICE is designed as an open-source environment, it is hoped that theuser community itself will make available a repertoire of model components that span a wide range of spatial,temporal, and functional scales, including those that simulate a single chemical reaction with high fidelity,
those that simulate entire pathways, and those that simulate more abstract higher-order motifs.BioSPICE tools are intended to enable researchers to use public databases and local resources to formulate aqualitative description of the cellular process of interest (e.g., models of networks or pathways), to annotatethe links between entities with biochemical interactions, and finally to convert this annotated qualitativedescription to a set of equations that can be analyzed and simulated. In addition, BioSPICE provides a number
of simulation engines with the capability to simulate ordinary, stochastic, and partial differential equationsand other tools that support stability and bifurcation analysis and qualitative reasoning that combines proba-bilistic and temporal logic.SOURCE: Sri Kumar, Defense Advanced Research Projects Agency, June 30, 2003.47See http://www.e-cell.org/project/. For a view of the computer science challenges, see also K. Takahashi, K. Yugi, K.Hashimoto, Y. Yamada, C.J.F. Pickett, and M. Tomita, ÒComputational Challenges in Cell Simulation: A Software EngineeringApproach,Ó IEEE Intelligent Systems 17(5):64-71, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.142CATALYZING INQUIRYtranscription, translation, energy production, and phospholipid synthesis with only 127 genes. Most ofthese genes were taken from Mycoplasma genitalium, the organism with the smallest known chromo-some (the complete genome sequence is 580 kilobases).48 E-CELL has also been used to construct acomputer model of the human erythrocyte,49 to estimate a gene regulatory network and signalingBox 5.6CytoscapeA variety of computer-aided models has been developed to simulate biological networks, typically focusingon specific cellular processes or single pathways.1  Cytoscape is a modeling environment particularly suitedto the analysis of global data on network interactions (from high-throughput screens for protein-protein, pro-
tein-DNA, and gene interactions) and on network states (including data on gene expression, protein abun-dance, and metabolite concentrations.) The Java-based, open-source software uses plug-ins to incorporateanalyses of individual processes and pathways.2A model in Cytoscape is organized as a network graph, with molecular species represented as nodes and
interactions represented as edges between nodes. Nodes and edges are mapped to specific data values called
attributes that can be text strings, discrete or continuous numbers, URLs, or lists, either loaded from a datarepository or generated dynamically. Layered onto attributes are annotations, which represent a hierarchicalclassification of progressively more specific descriptions (such as functions) of groups of nodes and edges. It is
possible to have many levels of annotation active simultaneously, each displayed as a different attribute of anode or edge. To visualize the network, Cytoscape supports several layout algorithms that fix the relativelocations of specific nodes and edges in the graphical window. An attribute-to-visual mapping facility allowsattributes to determine the appearance (color, shape, size) of their associated nodes and edges. Graph selec-tion and filtering reduces the complexity of the network by selectively displaying subsets of nodes and edgesaccording to a variety of criteria.CytoscapeÕs plug-in extensibility addresses the challenge of bridging high-level information (relationships amongnetwork components) with lower-level information (reaction rates, binding constants) of specific processes. A
plug-in that organizes the network layout according to putative functional attributes of genes was used to studyenergy transduction pathways in Halobacterium.3 Another plug-in allows Cytoscape to simulate stochasticSBML-biochemical models.4 The authors hope a community will further develop and enhance Cytoscape.1A. Gilman and A.P. Arkin, ÒGenetic ÔCodeÕ: Representations and Dynamical Models of Genetic Components and Networks,Ó AnnualReview of Genomics and Human Genetics 3:341-369, 20022P. Shannon, A. Markiel, O. Ozier, N.S. Baliga, J.T. Wang, D. Ramage, N. Amin, et al., ÒIntegrated Models of Biomolecular InteractionNetworks,Ó Genome Research 13:2498-2504, 2003.3N.S. Baliga, M. Pan, Y.A. Goo, E.C. Yi, D.R. Goodlett, K. Dimitrov, P. Shannon, et al., ÒCoordinate Regulation of Eenergy TransductionModules in Halobacterium species Analyzed by a Global Systems Approach,Ó Proceedings of the National Academy of Sciences99(23):14913-14918, 2002.4M. Hucka, A. Finney, H.M. Sauro, H. Bolouri, J. Doyle, and H. Kitano, ÒThe ERATO Systems Biology Workbench: Enabling Interactionand Exchange Between Software Tools for Computational Biology,Ó Pacific Symposium in Biocomputing, 450-461, 2002.SOURCE: Adapted from P. Shannon, A. Markiel, O. Ozier, N.S. Baliga, J.T. Wang, D. Ramage, N. Amin et al., ÒCytoscape: A SoftwareEnvironment for Integrated Models of Biomolecular Interaction Networks,Ó Genome Research 13(11):2498-2504, 2003.48M. Tomita, K. Hashimoto, K. Takahashi, Y. Matsuzaki, R. Matsushima, K. Saito, K. Yugi, et al., ÒE-CELL Project Overview:Towards Integrative Simulation of Cellular Processes,Ó Genome Informatics 9:242-243, 1998, available at http://giw.ims.u-tokyo.ac.jp/giw98/cdrom/Poster-pdf/poster02.pdf.49M. Tomita et al., ÒIn Silico Analysis of Human Erythrocyte Using E-Cell System,Ó poster session, The Future of Biology in the21st Century: 2nd International Conference on Systems Biology, California Institute of Technology, Pasadena, November 4-7,2001, available at http://www.icsb2001.org/Posters/032_kinoshita.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY143pathway involved in the circadian rhythm of Synechococcus sp. PCC 7942,50 and to model mitochondrialenergy metabolism and metabolic pathways in rice.51Another cellular simulation environment is the Virtual Cell, developed at the University of Connecti-cut Health Center.52 The Virtual Cell is a tool for experimentalists and theoreticians for computationallytesting hypotheses and models. To address a particular question, these mechanisms (chemical kinetics,
membrane fluxes and reactions, ionic currents, and diffusion) are combined with a specific set of experi-
mental conditions (geometry, spatial scale, time scale, stimuli) and applicable conservation laws to specifyBox 5.7Pathway Reconstruction: A Systems ApproachOn Topology.In this level, we are only concerned with identifying the interaction between nodes (genes, proteins, metabolites,etc.) in the system. The goal is the generation of a diagram of non-directional connections between all interactingnodes. For example, many have sought to develop large-scale maps of proteinÐprotein interactions derived from
various sources. Two-hybrid studies have produced genome-wide interaction maps for E. coli bacteriophage T7,yeast, Drosophila, and C. elegans. Although this approach can be comprehensive in regard to being genome wide,many interactions are not reproducible (a potential source of false negatives) and putative interactions occur be-
tween unlikely protein combinations (a potential source of false positives). . . . Another approach to constructinglarge-scale connection maps is by mining databases. Specific databases of protein interactions are being developed,the largest of which are DIP and BIND. These databases combine data from many high-throughput experiments
along with data from other sources, such as published literature. . . . Along other lines, investigators have attemptedto identify topological links by analyzing the dynamic behavior of networks. Pioneering work in this area shows thatmetabolic network topologies can be derived from correlation of time-series measurements of species concentra-
tions. The method is further refined to better identify connections in non-linear systems using mutual informationinstead of correlation. In another method, pair-wise correlation of gene expression data is used to predict functionalconnections that could then be combined into Òrelevance networksÓ of linked genes. Other methods may seek to use
some combination of data sources, although this may not be completely straightforward.On Inferring Qualitative Connections.In this level, we include not only associations between cellular entities but also the causal relations of such associ-ations, such as which entities serve as input to others. . . . Researchers have proposed methods that infer connectiv-ities from the estimations of the Jacobian matrix for metabolic, signaling, and genetic networks. Ross and co-workershave proposed a method based on propagated perturbations of chemical species that can reconstruct causal se-
quences of reactions from synthetic and experimental data. To reconstruct gene regulatory systems, methods includefuzzy logic analysis of facilitator/repressor groups in the yeast cell cycle and reconstruction of binary networks.However, the wide application of such methods is often limited because the continuous nature of many biological
systems prevents easy abstractions into coarser signals. Recently, there has been considerable work using Bayesiannetwork inference. Examples include inferring gene regulation using gene expression data from the yeast cell cycleor using data from synthetic gene networks.SOURCE: Reprinted by permission from J.J. Rice and G. Stolovitzky, ÒMaking the Most of It: Pathway Reconstruction and IntegrativeSimulation Using the Data at Hand,Ó Biosilico 2(2):70-77. Copyright 2004 Elsevier. (References omitted.)50F. Miyoshi et al., ÒEstimation of Genetic Networks of the Circadian Rhythm in Cyanobacterium Using the E-CELL system,Óposter session, presented at US-Japan Joint Workshop on Systems Biology of Useful Microorganisms, September 6-18, 2002, Keio
University, Yamagata, Japan, available at http://nedo-doe.jtbcom.co.jp/abstracts/35.pdf.51E. Wang et al., Òe-Rice Project: Reconstructing Plant Cell Metabolism Using E-CELL System,Ó poster session presented atSystems Biology: The Logic of LifeÑ3rd International Conference on Systems Biology, December 13-15, 2002, Karolinska
Institutet, Stockholm, available at http://www.ki.se/icsb2002/pdf/ICSB_222.pdf.52L.M. Loew and J.C. Schaff, ÒThe Virtual Cell: A Software Environment for Computational Cell Biology,Ó Trends in Biotechnol-ogy 19(10):401-406, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.144CATALYZING INQUIRYa concrete system of differential and algebraic equations. This experimental geometry may assume well-mixed compartments or a one-, two-, or three-dimensional spatial representation (e.g., experimental im-
ages from a microscope). Models are constructed from biochemical and electrophysiological data mapped
to appropriate subcellular locations in images obtained from a microscope. A variety of modeling approxi-
mations are available including pseudo-steady state in time (infinite kinetic rates) or space (infinite diffu-
sion or conductivity). In the case of spatial simulations, the results are mapped back to experimental
images and can be analyzed by applying the arsenal of image-processing tools that is familiar to a cell
biologist. Section 5.4.2.4 describes a study undertaken within the Virtual Cell framework.Simulation models can be useful for many purposes. One important use is to facilitate an understand-ing of what design properties of an intracellular network are necessary for its function. For example, von
Dassow et al.53 used a simulation model of the gap and pair-rule gene network in Drosophila melanogasterto show that the structure of the network is sufficient to explain a great deal of the observed cellular
patterning. In addition, they showed that the network behavior was robust to parameter variation upon
the addition of hypothetical (but reasonable) elements to the known network. Thus, simulations can also
be used to formally propose and justify new hypothetical mechanisms and predict new network elements.Another use of simulation models is in exploring the nature of control in networks. An example ofexploring network control with simulation is the work of Chen et al.
.54 in elucidating the control of different
phases of mitosis and explaining the impact of 50 different mutants on cellular decisions related to mitosis.Simulations have also been used to model metabolic pathways. For example, Edwards and Palssondeveloped a constraint-based genome-scale simulation of Escherichia coli metabolism (Box 5.8). By ap-plying successive constraints (stoichiometric, thermodynamic, and enzyme capacity constraints) to the
metabolic network, it is possible to impose limits on cellular, biochemical, and systemic functions,
thereby identifying all allowable solutions (i.e., those that do not violate the applicable constraints).
Compared to the detailed theory-based models, such an approach has the major advantage that it does
not require knowledge of the kinetics involved (since it is concerned only with steady-state function).
(On the other hand, it is impossible to implement without genome-scale knowledge, because only
genome-scale knowledge can bound the system in question.)  Within the space of allowable solutions, a

particular solution corresponds to the maximization of some selected function, such as cellular growth
or a response to some environmental change. A more robust model accounting for a larger number of
pathways is also described in Box 5.8.The Edwards and Palsson model has been used to predict the evolution of E. coli metabolism undera variety of environmental conditions. In the words of Ibarra et al., ÒWhen placed under growth
selection pressure, the growth rate of E. coli on glycerol reproducibly evolved over 40 days, or about 700generations, from a sub-optimal value to the optimal growth rate predicted from a whole-cell in silico
model. These results open the possibility of using adaptive evolution of entire metabolic networks to
realize metabolic states that have been determined a priori based on in silico analysis.Ó55Simulation models can also be used to test design ideas for engineering networks in cells. Forexample, very simple models have been used to provide insight into a genetic oscillator and a switch in
E. coli.56 Models have also been used to test designs for the control of cellular networks, as illustrated by53G. Von Dassow, E. Meir, E.M. Munro, and G.M. Odell, ÒThe Segment Polarity Network Is a Robust Developmental Module,ÓNature 406(6792):188-192, 2000.54K.C. Chen, A. Csikasz-Nagy, B. Gyorffy, J. Val, B. Novak, and J.J. Tyson, ÒKinetic Analysis of a Molecular Model of theBudding Yeast Cell Cycle,Ó Molecular Biology of the Cell 11(1):369-391, 2000.55R.U. Ibarra, J.S. Edwards, and B.O. Palsson, ÒEscherichia coli K-12 Undergoes Adaptive Evolution to Achieve in Silico Pre-dicted Optimal Growth,Ó Nature 420(6912):186-189, 2002.56M.B. Elowitz and S. Leibler, ÒA Synthetic Oscillatory Network of Transcriptional Regulators,Ó Nature 403(6767):335-338,2000; T.S. Gardner, C.R. Cantor, and J.J. Collins, ÒConstruction of a Genetic Toggle Switch in Escherichia coli,Ó Nature403(6767):339-342, 2000.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY145Endy and Yin in using their T7 model to propose a pharmaceutical strategy for preventing both T7propagation and the development of drug resistance through mutation.57Given observed cell behavior, simulation models can be used to suggest the necessity of a givenregulatory motif or the sufficiency of known interactions to produce the phenomenon. For example, Qi
et al. demonstrate the sufficiency of membrane energetics, protein diffusion, and receptor-binding
kinetics to generate a particular dynamic pattern of protein location at the synapse between two im-
mune cells.58The following sections describe several simulation studies in more detail.Box 5.8Escherichia coli Constraint-based ModelsA. In Silico Model1The Escherichia coli MG1655 genome has been completely sequenced. The annotated sequence, biochemical infor-mation, and other information were used to reconstruct the E. coli metabolic map. The stoichiometric coefficients foreach metabolic enzyme in the E. coli metabolic map were assembled to construct a genome-specific stoichiometricmatrix. The E. coli stoichiometric matrix was used to define the systemÕs characteristics and the capabilities of E. colimetabolism. The effects of gene deletions in the central metabolic pathways on the ability of the in silico metabolicnetwork to support growth were assessed, and the in silico predictions were compared with experimental observa-
tions. It was shown that based on stoichiometric and capacity constraints the in-silico analysis was able to qualita-tively predict the growth potential of mutant strains in 86% of the cases examined. Herein, it is demonstrated that thesynthesis of in silico metabolic genotypes based on genomic, biochemical, and strain-specific information is possi-
ble, and that systems analysis methods are available to analyze and interpret the metabolic phenotype.B. Genome-scale Model2An expanded genome-scale metabolic model of E. coli (iJR904 GSM/GPR) has been reconstructed which includes904 genes and 931 unique biochemical reactions. The reactions in the expanded model are both elementally andcharge balanced. Network gap analysis led to putative assignments for 55 open reading frames (ORFs). Gene to
protein to reaction associations (GPR) are now directly included in the model. Comparisons between predictionsmade by iJR904 and iJE660a models show that they are generally similar but differ under certain circumstances.Analysis of genome-scale proton balancing shows how the flux of protons into and out of the medium is important
for maximizing cellular growth. . . . E. coli iJR904 has improved capabilities over iJE660a [a model that accounted for660 genes and 627 unique biochemical reactions and was itself a slight modification of the original model describedin the above paragraph]. iJR904 is a more complete and chemically accurate description of E. coli metabolism thaniJE660a. Perhaps most importantly, iJR904 can be used for analyzing and integrating the diverse datasets. iJR904 willhelp to outline the genotype-phenotype relationship for E. coli K-12, as it can account for genomic, transcriptomic,proteomic and fluxomic data simultaneously.1Reprinted from J.S. Edwards and B.O. Palsson, ÒThe Escherichia coli MG1655 in Silico Metabolic Genotype: Its Definition, Characteristics, andCapabilities,Ó Proceedings of the National Academy of Sciences 97(10): 5528-5533, 2000. Copyright 2000 National Academy of Sciences.2J.L. Reed, T.D. Vo, C.H. Schilling, and B.O. Palsson, ÒAn Expanded Genome-scale Model of Escherichia coli K-12 (iJR904 GSM/GPR),ÓGenome Biology 4(9): Article R54, 2003, available at http://genomebiology.com/2003/4/9/R54. Reprinted by permission of the authors.57D. Endy and J. Yin, ÒToward Antiviral Strategies That Resist Viral Escape,Ó Antimicrobial Agents and Chemotherapy 44(4):1097-1099, 2000.58S.Y. Qi, J.T. Groves, and A.K. Chakraborty, ÒSynaptic Pattern Formation During Cellular Recognition,Ó Proceedings of theNational Academy of Sciences 98(12):6548-6553, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.146CATALYZING INQUIRY5.4.2.2Cell Cycle Regulation
Biological growth and reproduction depend ultimately on the cycle of DNA synthesis and physicalseparation of the replicate DNA molecules within individual cells. In eukaryotes, these processes are
triggered by cyclin-dependent protein kinases (CDKs). In fission yeast, CDK activity (cdc2 = kinase
subunit, cdc13 = cyclin subunit) is regulated by a network of protein interactions (Figure 5.5), including
cyclin synthesis and degradation, phosphorylation of cdc2, and binding to an inhibitor.A network of such complexity, with multiple feedback loops, cannot be understood thoroughly bycasual intuition. Instead, the network is converted into a set of nonlinear differential equations, and the
physiological implications of these equations are studied.59 Numerical simulation of the equations(Figure 5.6) provides complete time courses of every component and can be interpreted in terms of
observable events in the cell cycle. Simulations can be run, not only of wild-type cells but also of dozens
of mutants constructed by deleting or overexpressing each component singly or in multiple combina-
tions. From the observed phenotypes of these mutants it is possible to reverse-engineer the regulatory
network and the set of kinetic constants associated with the component reactions.FIGURE 5.5The cell-cycle control system in fission yeast. This system can be divided into three modules, which
regulate the transitions from G1 into S phase, from G2 into M phase, and exit from mitosis. SOURCE: J.J. Tyson, K.Chen, and B. Novak, ÒNetwork Dynamics and Cell Physiology,Ó Nature Reviews of Molecular Cell Biology 2(12):908-916, 2001. Figure and caption reproduced with permission from Nature Reviews of Molecular Cell Biology. Copyright2001 Macmillan Magazines Ltd.59J.J. Tyson, K. Chen, and B. Novak, ÒNetwork Dynamics and Cell Physiology,Ó Nature Reviews: Molecular Cell Biology 2(12):908-916, 2001; J.J. Tyson, A. Csikasz-Nagy, and B. Novak, ÒThe Dynamics of Cell Cycle Regulation,Ó BioEssays 24(12):1095-1109, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY147For understanding the dynamics of molecular regulatory systems, bifurcation theory is a powerfulcomplement to numerical simulation. The bifurcation diagram in Figure 5.7 presents recurrent solutions
(steady states and limit cycle oscillations) of the differential equations as functions of cell size. The
control system has three characteristic steady states: low cdc2 activity (G1 = pre-replication), medium
cdc2 activity (S/G2 = replication and post-replication), and high cdc2 activity (M = separation of repli-
cated DNA molecules). G1 and S/G2 are stable steady states; M is unstable because of a negative
feedback loop as shown in Figure 5.5 (cdc2-cdc13 activates Slp1, which degrades cdc13).When the time courses of size and cdc2 activity from Figure 5.6 are superimposed on the bifurcationdiagram (curve labeled ÒsizeÓ), one sees how progress through the cell cycle is governed by the bifurca-
tions that turn stable steady states into unstable steady states and/or stable oscillations. A mutation
changes a specific rate constant, which changes the locations of the bifurcation points in Figure 5.7,
which changes how cells progress through (or halt in) the cell cycle. By this route one can trace the
dynamical consequences of genetic information all the way to observable cell behavior.FIGURE 5.6Simulated time courses of cdc2 and related proteins during the cell cycle of fission yeast. Numerical
integration of the full set of differential equations that describe the wiring diagram in Figure 5.5 yields these time
courses. Time is expressed in minutes; all other variables are given in arbitrary units. ÒSizeÓ refers to the number of
ribosomes per nucleus. Notice the brief G1 phase, when ste9 is active and rum1 is abundant. After a long S/G2 phase,during which cdc2 is tyrosine phosphorylated, the cell enters M phase, when cdc25 removes the inhibitory phosphategroup. After some delay, slp1 activates and degrades cdc13. As cdc2Ðcdc13 activity falls, the cell exits mitosis. Size
decreases twofold at nuclear division. SOURCE: J.J. Tyson, K. Chen, and B. Novak, ÒNetwork Dynamics and Cell
Physiology,Ó Nature Reviews of Molecular Cell Biology 2(12):908-916, 2001. Figure and caption reproduced with permis-sion from Nature Reviews of Molecular Cell Biology. Copyright 2001 Macmillan Magazines Ltd.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.148CATALYZING INQUIRY5.4.2.3A Computational Model to Determine the Effects of SNPs in
Human Pathophysiology of Red Blood CellsThe completion of the Human Genome Project has led to the construction of single nucleotidepolymorphism (SNP) maps. Single nucleotide polymorphisms are common DNA sequence variations
among individuals. A result of the construction of SNP maps is to determine the effects of SNPs on the
development of disease(s) since sequence variations can lead to altered biological function or disease.Currently, it is difficult to determine the causal relationship between the variations in sequence,SNPs, and the physiological function. One way to analyze this relationship is to create computational
models or simulations of biological processes. Since erythrocyte (red blood cell) metabolism has been
studied extensively over the years and many SNPs have been characterized, Jamshidi et al. used this
information to build their computational models.60Two important metabolic enzymes, glucose-6-phosphate dehydrogenase (G6PD) and pyruvate ki-nase (PK), were studied for alterations in their kinetic properties in an in silico model to calculate the
overall effect of SNPs on red blood cell function. Defects in these enzymes cause hemolytic anemia.FIGURE 5.7Bifurcation diagram for the full cell-cycle control network. . . . [T]he full diagram is not a simple sum of the
bifurcation diagrams of its modules. In particular, oscillations around the M state are greatly modified in the compositecontrol system. Superimposed on the bifurcation diagram is a Òcell-cycle orbitÓ (line on 
the right with arrows): 
fromthe time courses in Figure 5.6, we plot size on the abscissa and cdc2Ðcdc13 activity on the ordinate for representative
times between birth and division. Notice that, at small cell size, all three modules support stable steady states. Noticehow the cell-cycle orbit follows the attractors of the control system. SOURCE: J.J. Tyson, K. Chen and B. Novak,
ÒNetwork Dynamics and Cell Physiology,Ó Nature Reviews Molecular Cell Biology 2(12):908-916, 2001. Figure and caption,reproduced with permission from Nature Reviews Molecular Cell Biology. Copyright 2001 Macmillan Magazines Ltd.60N. Jamshidi, S.J. Wiback, and B.O. Palsson, ÒIn Silico Model-driven Assessment of the Effects of Single Nucleotide Polymor-phisms (SNPs) on Human Red Blood Cell Metabolism,Ó Genome Research 12(11):1687-1692, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY149Clinical data taken from the published literature were used for the measured values of the kineticparameters. These values were then used in model simulations to determine whether a direct link could
be established between the SNP and the disease (anemia).The computational modeling revealed two results. For the G6PD and PK variants analyzed, thereappeared to be no clear relationship between their kinetic properties as a function of sequence variation
or SNP. However, upon assessment of overall biological function, a correlation was found between the
sequence variation of G6PD and the severity of the clinical disease. Thus, in silico modeling of biological
processes may aid in analysis and prediction of SNPs and pathophysiological conditions.5.4.2.4Spatial Inhomogeneities in Cellular Development
Simulation models can be used to provide insight into the significance of spatial inhomogeneities.For example, the interior of living cells does not resemble at all a uniform aqueous solution of dissolved
chemicals, and yet this is the implicit assumption underlying many views of the cell. This assumption
serves traditional biochemistry and molecular biology reasonably well, but research increasingly dem-
onstrates that the physical locations of specific molecules are crucial. Multiprotein complexes act as
machines for internal movements or as integrated circuits in signaling. Messenger RNA molecules are
transported in a highly directed fashion to specific regions of the cell (in nerve axons, for example). Cells
adopt highly complex shapes and undergo complex movements thanks to the matrix of protein fila-
ments and associated proteins within their cytoplasm.5.4.2.4.1Unraveling the Physical Basis of Microtubule Structure and Stability
Microtubules are cylin-drical polymers found in every eukaryotic cell. Microtubles play a role in cellular architecture and as
molecular train tracks used to transport everything from chromosomes to drug molecules. An under-
standing of microtubule structure and function is key not just to unraveling fundamental mechanisms
of the cell, but also to opening the way to the discovery of new antiparasitic and anticancer drugs.Until now, researchers have known that the microtubules, constructed of units called protofilamentsin a hollow, helical arrangement, are rigid but not static, and undergo periods of growth and sudden
collapse. Yet the mechanism for this construction-destruction had eluded researchers.Over the past several years, McCammon and his colleagues have pioneered the use of a combina-tion of an atomically detailed model for a microtubule and large-scale computations using the adaptive
Poisson-Boltzmann Solver to create a high-resolution, 1.25-million-atom map of the electrostatic inter-
actions within the microtubule.61More recently, David Sept and Nathan Baker of Washington University and McCammon used thesame technique to successfully predict the helicity of the tubule with a striking correspondence to
experimental observation.62 Based on the lateral interactions between protofilaments, they determinedthat the microtubule prefers to be in a configuration in which the protofilaments assemble with a seam
at each turn, rather than spiraling smoothly upward with alpha and beta monomers wrapping the
microtubule as if it were a barberÕs pole. At the end of each turn, a chain of alphas is trailed by a chain
of betas, then after that turn, a chain of alphas, and so on. It is as if the red and white stripes on the
barberÕs pole traded places with every twist (Figure 5.8).61N.A. Baker, D. Sept, S. Joseph, M.J. Holst, and J.A. McCammon, ÒElectrostatics of Nanosystems: Application to Microtubulesand the Ribosome,Ó Proceedings of the National Academy of Sciences 98(18):10037-10041, 2001.62D. Sept, N.A. Baker, and J.A. McCammon, ÒThe Physical Basis of Microtubule Structure and Stability,Ó Protein Science12(10):2257-2261, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.150CATALYZING INQUIRY5.4.2.4.2The Movement of 
Listeria BacteriaAlberts and Odell have developed a computational modelof Listera monocytogenes based on an explicit simulation of a large number of monomer-scale biochemi-cal and mechanical interactions,63 representing all protein-protein binding interactions with on-rate andoff-rate kinetic equations. These equations characterize individual actin filaments: the bulk properties of
the actin ÒgelÓ arise from the contributions of the many individual filaments of the actin network; and
the growth of any particular filament depends on that filamentÕs precise location, orientation, and
biochemical state, all of which change through time. Mechanical interactions, which resolve collisions
and accommodate the stretching of protein-protein linkages, follow NewtonÕs laws.The model is based on a large set of differential equations that determine how the relevant statevariables change with time. These equations are solved numerically, taking into account the fact that
discontinuities in time occur frequently as objects suddenly collide and as objects suddenly spring into
existence or disappear (due to new filament nucleation and depolymerization). The model accommo-FIGURE 5.8The binding free energy between two protofilaments as a function of the subunit rise between adja-
cent dimmers. Sept et al. used electrostatic calculations to determine the binding energy between two protofila-ments as a function of the subunit rise between adjacent dimers. Viewed from the growing (+) end of the tubule,
the graph demonstrates the most favorable configuration at various points during assembly. SOURCE: Reprintedby permission from D. Sept, N.A. Baker, and J.A. McCammon, ÒThe Physical Basis of Microtubule Structure andStability,Ó Protein Science 12:2257-2261, 2003. Copyright 2003 by the Protein Society.63J.B. Alberts and G.M. Odell, ÒIn Silico Reconstitution of Listeria Propulsion Exhibits Nano-Saltation,Ó PLoS Biology 2(12):e412,2004, available at http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=532387.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY151dates arbitrary geometries, explicit stochastic input, and specific small-scale events. Because the modelis built from the ground up, it can predict emergent behavior that would not be apparent from intuition
or qualitative description of the behavior of individual parts. On the other hand, the simulation requires
multiple runs of its stochastic, individual molecule-based model, and parametric relationships emerge
not from closed-form equations that demonstrate qualitative functional dependencies but from en-
sembles of many repeated simulations.The trajectories generated by this model of L. monocytogenes motility display repeated runs andpauses that closely resemble the actual nanoscale measurements of bacterial motion.64 Further analysisof the simulation state at the beginning and ends of simulated pauses indicate that there is no character-
istic step-size or pause duration in these simulated trajectories and that pauses can be caused both by
correlated Brownian motion and by synchronously strained sets of ActA-actin filament mechanical
links.5.4.2.4.3Morphological Control of Spatiotemporal Patterns of Intracellular Signaling
Fink andSlepchenko studied calcium waves evoked by activation of the bradykinin receptor in the plasma
membrane of a neuronal cell.65 The neuromodulator bradykinin applied to the cells produced a calciumwave that starts in the neurite and spreads to the soma and growth cones. The calcium wave was
monitored with digital microscope imaging of a fluorescent calcium indicator. The hypothesis was that
interaction of bradykinin with its receptor on the plasma membrane activated production of inositol-
1,4,5-trisphosphate (InsP3) that diffused to its receptor on the endoplasmic reticulum, leading to calciumrelease.Using the Virtual Cell software environment, they assembled a simulation model of this phenom-enon.66 The model contained details of the relevant receptor distributions (via immunofluorescence)within the cell geometry, the kinetics of InsP3 production (via biochemical analysis of InsP3 in cellpopulations and photorelease of caged InsP3 in individual cells), the transport of calcium through theInsP3 receptor calcium channel and the sarcoplasmic/endoplasmic reticulum calcium ATPase (SERCA)pump (from literature studies of single-channel kinetics and radioligand flux), and calcium buffering by
both endogenous proteins and the fluorescent indicator (from confocal measurements of indicator
concentrations).The mathematical equations generated by this combination of molecular distributions and reactionand membrane transport kinetics were then solved to produce a simulation of the spatiotemporal pattern
of calcium that could be directly compared to the experiment. The characteristic calcium dynamics re-
quires rapid, high-amplitude production of [InsP3]cyt in the neurite. This requisite InsP3 spatiotemporalprofile is provided, in turn, as an intrinsic consequence of the cellÕs morphology, demonstrating how
geometry can locally and dramatically intensify cytosolic signals that originate at the plasma membrane.
In addition, the model predicts and experiments confirm that stimulation of just the neurite, but not the
soma or growth cone, is sufficient to generate a calcium response throughout the cell.64S.C. Kuo and J.L. McGrath, ÒSteps and Fluctuations of Listeria Monocytogenes During Actin-based Motility,Ó Nature407(6807):1026-1029, 2000; J. McGrath, N. Eungdamrong, C. Fisher, F. Peng, L. Mahadevan, T.J. Mitchison, and S.C. Kuo, ÒTheForce-Velocity Relationship for the Actin-based Motility of Listeria Moncytogenes,Ó Current Biology 13(4):329-332, 2003. (Bothcited in Alberts and Odell, 2004.)65C.C. Fink, B. Slepchenko, I.I. Moraru, J. Schaff, J. Watras, and L.M. Loew, ÒMorphological Control of Inositol-1,4,5-Trisphosphate-dependent Signals.Ó Journal of Cell Biology 147(5):929-935, 1999; C.C. Fink, B. Slepchenko, I.I. Moraru, J. Watras,J.C. Schaff, and L.M. Loew, ÒAn Image-based Model of Calcium Waves in Differentiated Neuroblastoma Cells,Ó BiophysicalJournal 79(1):163-183, 2000.66B.M. Slepchenko, J.C. Schaff, I. Macara, and L.M. Loew, ÒQuantitative Cell Biology with the Virtual Cell,Ó Trends in CellBiology 13(11):570-576, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.152CATALYZING INQUIRY67For purposes of the discussion in this subsection (Section 5.4.3.1), regulation refers to cis-regulation.68C.H. Yuh, H. Bolouri, and E.H. Davidson, ÒGenomic Cis-regulatory Logic: Experimental and Computational Analysis of aSea Urchin Gene,Ó Science 128(5):617-629, 1998. Some of this discussion is also adapted from commentary on this article: G.A.Wray, ÒPromoter Logic,Ó Science 279(5358):1871-1872, 1998.69In this context, a multifunctional organization of the regulatory system means that the protein associated with the endo16gene is differentially expressed in various cells in the sea urchin.5.4.3Genetic Regulation
The problem of genetic regulationÑhow and under what circumstances and the extent to whichgenes are expressed as proteinsÑis a central problem of modern biology. The issue originates in an
apparent paradoxÑevery cell in a complex organism contains the same DNA sequences, and yet there
are many cell types in such organisms (blood cells, skin cells, and so on). In particular, the proteins that
comprise any given cell type are different from those of other cell types, even though the genomic
information is the same in both. Nor is genomic information the whole story in developmentÑcells also
respond to their environment, and external signals coming into a cell from neighboring cells influence
which proteins the cell makes.Genetic regulation is an extraordinarily complex problem. Molecular biologists distinguish be-tween cis-regulation and trans-regulation. Cis-regulatory elements for a given gene are segments of thegenome that are located in the vicinity of the structural portion of a gene and regulate the expression of
the gene. Trans-regulatory elements for a given gene refer to proteins not structurally associated with agene that nevertheless regulate its expression. The sections below provide examples of several con-
structs that help shed some light on both kinds of regulation.5.4.3.1Cis-regulation of Transcription Activity as Process Control ComputingIt has been known for some time that the genome contains both genes and cis-regulatory ele-ments.67 The presence or absence of particular combinations of these regulatory elements determinesthe extent to which specific genes are expressed (i.e., transcribed into specific proteins). In pioneering
work undertaken by Davidson et al.,68 it was shown that cis-regulation couldÑin the case of a specificgeneÑbe viewed as a logical process analogous to a computer program that connected various inputs to
a single output determining the precise level of transcription for that gene.In particular, Davidson and his colleagues developed a high-level computer simulation of the cis-regulatory system governing the expression of the endo16 gene in the sea urchin (endo16 is a gut-specificgene of the sea urchin embryo). In this context, the term Òhigh-levelÓ means a highly abstracted repre-
sentation, consisting at its core of 18 lines of code. This simulation enabled them to make predictions
about the effect of specific manipulations of the various regulatory factors on endo16 transcription levelsthat could be tested against experiment.Some of the inputs to the simulation were binary values. The value 1 indicated that a binding sitewas both present and productively occupied by the appropriate cis-regulatory factor. A 0 indicated thatthe site was mutationally destroyed or inactive because its factor was not present or was inactive. The
other inputs to the simulation were continuous and varied with time, and represented outputs (protein
concentrations) in other parts of the system. The output of this process in some cases was a continuous
time-varying variable that regulated the extent to which the specific gene in question was transcribed.Davidson et al. were able to confirm the predictions made by their computational model, conclud-ing that all of the regulatory functions in question (and the resulting system properties) were encoded in
the DNA sequence, and that the regulatory system described is capable of processing complex informa-
tional inputs and hence indicates the presence of a multifunctional organization of the endo16 cis-regulatory system.69Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY153The computational model undoubtedly provides a compact representation of the relationshipsbetween different inputs and different outputs.70 Perhaps a more interesting question, however, is theextent to which it is meaningful to ascribe a computational function to the biochemical substrate under-
lying the regulatory system. Davidson et al. argue that the DNA sequence in this case specifies Òwhat is
essentially a hard-wired, analog computational device,Ó resulting in system properties that are Òall
explicitly specified in the genomic DNA sequence.Ó71It is highly unlikely that the precise computational structure of endo16Õs regulatory system willgeneralize to the regulatory systems of other genes. From the perspective of the biologist, the reason is
clearÑorganisms are not designed as general-purpose devices. Indeed, the evolutionary process virtu-
ally guarantees that individualized solutions and architectures will be abundant, because specific adap-
tations are the rule of the day. Nevertheless, insight into the computational behavior of the endo16 cis-regulatory system provides a new way of looking at biological behavior.Can the regulatory systems of some other genes be cast in similar computational terms? If and whenfuture work demonstrates that such casting is possible, it will become increasingly meaningful to view
the genome as thousands of simple computational devices operating in tandem. DavidsonÕs work
suggests the possibility that a class of regulatory mechanisms, complex though they might be with
respect to their behavior, may be governed by what are in essence hard-wired devices whose essential
functionality can be understood in computational terms through a logic of operation that is in fact
relatively simple at its core. Prior to DavidsonÕs work and despite extensive research, the literature had
not revealed any apparent regularity in the organization of regulatory elements or in the ways in which
they interact to regulate gene expression.Indeed, while many promoters appear either to have a simpler organization or to operate lesslogically than that of endo16, few promoters have been examined with the many precise quantitativeassays that were carried out by Davidson et al., and nonquantitative assays would have completely
missed most of the functions that the majority of the regulatory systemÕs elements encode.72 So, it is atthis point an open question whether this computational view has applicability beyond the specific case
of endo16. 5.4.3.2Genetic Regulatory Networks as Finite-state Automata
Trans-regulation (as contrasted to cis-regulation) is based on the notion that some genes can haveregulatory effects on others.73 In reality, the network of connections between genes that regulate andgenes that are regulated is highly complex. In an attempt to gain insight into genetic regulatory net-
works from a gross oversimplification, Kaufmann proposed that actual genetic regulatory networks
might be modeled as randomly connected Boolean networks.74KaufmannÕs model made several simplifying assumptions:70E.F. Keller, Making Sense of Life: Explaining Biological Development with Models, Metaphors, and Machines, Harvard UniversityPress, Cambridge, MA, 2002, p. 241.71This is not to argue that DNA sequence alone is responsible for the specification of system properties. Epigenetic controlmechanisms also influence system properties as do environmental conditions and cell state that are not specified in DNA. Ananalogy might be that although a memory dump of a computer specifies the state of the computer, many contingent activitiesmay affect the actual execution path. For example, the behavior (and timing) of specific input-output activities are likely to berelevant.72G.A. Wray, ÒPromoter Logic,Ó Science 279(5358):1871-1872, 1998.73For purposes of the discussion in this subsection (Section 5.4.3.2), regulation refers to trans-regulation.74Much of this work is due to the pioneering work of Stuart Kauffman. See for example, S.A. Kauffman, The Origins of Order:Self-Organization and Selection in Evolution, Oxford University Press, New York, 1993. An alternative discussion of this materialcan be found at http://www.smi.stanford.edu/projects/helix/bmi214/ (May 13); lecture notes of Russell Altman.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.154CATALYZING INQUIRY¥The total number of genes involved is N, a number of order 30,000.¥The number of genes that regulate a given target is a constant (call it K) for all regulated genes; Kis a small integer.¥The regulatory signal associated with a connection or the expression of a gene is either on or off.(In fact, almost certainly it is not just the fact of a connection between genes that influences regulation,
but rather the nature of that connection as a continuous time-varying value such as a molecular concen-
tration over time.)¥Every gene is governed by the same transition rule (i.e., a Boolean function) that specifies its state(on or off) as a function of the activities of its K inputs at the immediately earlier time.¥The regulatory network operates synchronously (and, by implication, kinetics are unimportant).¥Secondary effects on genetic regulation arising from the nondigital characteristics of DNA (suchas methylation) can be neglected.¥The genes that regulate and genes that are regulated (which may overlap) are connected atrandom.Box 5.9 provides more details about this model. Because the model treats all genes as identical (i.e.,all obey the same transition rule) and assigns connections between genes at random, it obviously lacks
fidelity to any specific genome and cannot predict the biological phenomenology of any specific organ-
ism. Yet, it may provide insight into biological order that emerges from the structure of the genetic
regulatory network itself.Simulations of the operation of this model yielded interesting behavior, which depends on thevalues of N and K. For K = 1 or K > 5, the behavior of the network exhibits little interesting order, whereorder is defined in terms of fixed cycles known as attractors. If K = 1, the networks are static, with thenumber of attractors exponential in the size of the network and the cycle length approaching unity. If
K > 5, there are few attractors, and it is the cycle length that is exponential in the size of the network.However, for K = 2, the network does exhibit order that has potential biological significanceÑboth thenumber of attractors and the cycle length are proportional to N1/2.75What might be the biological significance of these results?¥The trajectory of an attractor through its successive states would reflect the fact that, over time,different genes are expressed in a biological organism.¥The fact that there are multiple attractors within the same genome suggests that multiple biologi-cal structures might exist, even within the same organism, corresponding to the genome being in one of
these attractor states. An obvious candidate for such structures would be multiple cell types. That is,
this analysis suggests that a cell type corresponds to a given state cycle attractor, and the different
attractors to the different cell types of the organism. Another possibility is that different but similar
attractors correspond to cells in different states (e.g., disease state, resting state, perturbed state).¥The fact that an attractor is cyclic suggests that it may be related to cyclic behavior in a biologicalorganism. If cell types can be identified with attractors, the cyclic trajectory in phase space of an
attractor may correspond to the cell cycle in which a cell divides.¥States that can be moved from one trajectory (for one attractor) to another trajectory (and anotherattractor) by changing a single state variable are not robust and may represent the phenomenon that
small, apparently minor perturbations to a cellÕs environment may kick it into a different state.¥The square root of the number of genes in the human genome (around 30,000) is 173. Under theassumption of K = 2 scaling, this would correspond to the number of cyclic attractors and thus to thenumber of cell types in the human body. This is not far from the number of cell types actually observed75A. Bhattacharjya and S. Liang, ÒPower-Law Distributions in Some Random Boolean Networks,Ó Physical Review Letters77(8):1644, 1996.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY155(about 200). Such a result may be numerological coincidence or rooted in the fact that nearly all cells ina given organism (even across eukaryotes) share the same basic housekeeping mechanisms (metabo-
lism, cell-cycle control, cytoskeletal construction and deconstruction, and so on), or it may reflect phe-
notypic structure driven by the large-scale connectivity in the overall genetic regulatory network. More
work will be needed to investigate these possibilities.76 Box 5.10 provides one view on experimentalwork that might be relevant.To illustrate the potential value of Boolean networks as a model for genetic regulatory net-works, consider their application to understanding the etiology of cancer.77 Specifically, cancer isBox 5.9Finite-state Automata and a Comparison of Genetic Networks and Boolean NetworksIn KaufmannÕs Boolean representation of a genetic regulatory network, there are N genes, each with two statesof activity (expressed or inhibited), and hence 2N possible states (i.e., sets of activities) in the network. Thenumber of possible connections is combinatorial in N and K. Starting at time t, each gene makes a transitionto a new state at time t + 1 in accord with the transition rule and the K inputs that it receives. Thus, the stateof the network at a time t + 1 is uniquely determined from its state at time t. The trajectory of the network ast changes (i.e., the sequence of states that the network assumes) is analogous to the process by which genesare expressed.This network is an instantiation of a finite-state automaton. Since there are a finite number of states (2N), thesystem must eventually find itself in a state previously encountered. Since the system is deterministic, thenetwork then cycles repeatedly through a fixed cycle, called an attractor. Every possible system state eitherleads to some attractor or is part of an attractor.Different initial conditions may or may not lead to different attractors. All of the initial conditions that lead tothe same attractor constitute what is known as a ÒbasinÓ for that attractor. Any state within a basin can be
exchanged with any other state in the same basin without changing the behavior of the network in the longrun. In addition, given a set of attractors, no attractor can intersect with another (i.e., pass through even onestate that is contained in another attractor). Thus, attractors are intrinsically stable and are analogous to the
genetic expression pattern in a mature cell.An attractor may be static or dynamic. A static attractor involves a cycle length of one (i.e., the automatonnever changes state). A dynamic attractor has a cycle length greater than one (i.e., a sequence of states repeatsafter some finite number of time increments). Attractors that have extremely long cycle lengths are regarded aschaotic (i.e., they do not repeat in any amount of time that would be biologically interesting).Two system states differing in only a small number of state variables (i.e., having only a few bits that differ outof the entire set of N variables) often lie on dynamical trajectories that converge closer to one another in statespace. In other words, their attractors are robust under small perturbations. However, there can be stateswithin a basin of attraction that differ in only one state variable from a trajectory that can lead to a differentattractor.76This point is discussed further in Section 5.4.2.2 and the references therein.77Z. Szallasi and S. Liang, ÒModeling the Normal and Neoplastic Cell Cycle with ÔRealistic Boolean Genetic NetworksÕ: TheirApplication for Understanding Carcinogenesis and Assessing Therapeutic Strategies,Ó Pacific Symposium on Biocomputing, pp. 66-76, 1998.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.156CATALYZING INQUIRYwidely believed to be a pathology of the hereditary apparatus. However, it has been clear for sometime that single-cause, single-effect etiologies cannot account for all or nearly all occurrences of
cancer.78Box 5.10Testing the Potential Relevance of the Boolean Network ModelBecause of the extreme simplifications embedded in the Boolean network model, detailed predictions (e.g.,genes A and B turn on gene C) are unlikely to be possible. Instead, the utility of this approach as a way oflooking at genetic regulation will depend on its ability to make qualitative predictions about large-scalestructure and trends. Put differently, can Boolean networks behave in biologically plausible ways?Under certain circumstances, Boolean networks do exhibit certain regularities. Thus, the operative question iswhether these features have reasonable biological interpretations that afford insight into the integrated behav-
ior of the genomic system. Consider the following:1.A large fraction of the genes in Boolean networks converge to fixed states of activity, on or off, that contain
the same genes on all cell-type attractors. The existence of this Òstable coreÓ predicts that most genes will bein the same state of activity on all cell types of an organism. Direct experimental testing of this prediction ispossible using DNA chip technology today.
2.Nearby states in the state space of the system typically lie on trajectories that converge on each other in
state space. This might be tested by cloning exogenous promoters upstream of a modest number of randomlychosen genes to transiently activate them, or by using inhibitory RNA to transiently inactivate a geneÕs RNA
products, and following the trajectory of gene activities in unperturbed cells over time and perturbed cellswhere the geneÕs activity is transiently altered, using DNA chips to assess whether the states of activity be-come more similar.
3.The Boolean model predicts that if randomly chosen genes are transiently reversed in their activity, a
downstream avalanche of gene activities will ensue. The size distribution of these avalanches is predicted tobe a power law, with many small avalanches and few large ones. There is a rough maximum size avalanche
that scales as about three times the square root of the number of genes, hence about 500 for human cells. Thisis testable, again by cloning upstream controllable promoters to transiently activate random genes, or inhib-itory RNA to transiently inactivate random genes, and following the resulting avalanche of changes in gene
activities over time using DNA chips.4.The Boolean model assumes cell types are attractors. As such, cell-type attractors are stable to about 95
percent of the single gene perturbationsÑthe system returns to the attractor from which it was perturbed.
Similarly, it is possible to test whether cell types are stable in the same homeostatic way by perturbing theactivity of many choices of single genes, one at a time.5.The stable core leaves behind Òtwinkling islandsÓ of genes that are functionally isolated from one another.

These are the subcircuits that determine differentiation, since each island has its own attractors, and theattractors of the network as a whole are unique choices of attractor from each of the twinkling islands in a kindof combinatorial epigenetic code. Current techniques can test for such islands by starting avalanches from
different single genes. Two genes in the same island should have overlapping downstream members of theavalanches they set off. Genes in different islands should not. The caveat here is that there may be genesdownstream from more than one island, affected by avalanches started in each.SOURCE: Stuart Kauffman, Santa Fe Institute, personal communication, September 20, 2002.78See, for example, T. Hunter, ÒOncoprotein Networks,Ó Cell 88(3):333, 1997; B. Vogelstein and K.W. Kinzler, ÒThe MultistepNature of Cancer,Ó Trends in Genetics 9(4):138, 1993. (Cited in Szallasi and Liang, 1998.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY157If the correspondence between attractor and cell is assumed, malignancy can be viewed as anattractor similar in most ways to that associated with a normal cell,79 and the transition from normal tomalignant is represented by a Òphase transitionÓ from one attractor to another. Such a transition might
be induced by an external event (radiation, chemical exposure, lack of nutrients, and so on).As one illustration, Szallasi and Liang argue that changes in large-scale gene expression patternsassociated with conversion to malignancy depend on the nature of attractor transition in the underlying
genetic network in three ways:1.A specific oncogene can induce changes in the state of downstream genes (i.e., genes for which
the oncogene is part of their regulatory network) and transition rules for those genes without driving
the system from one attractor to another one. If this is true, inhibition of the oncogene will result a
reversion of those downstream changes and a consequent normal phenotype. In some cases, just such
phenomenology has been suggested,80 although whether or not this mechanism is the basis of someforms of human cancer is unknown as yet.2.A specific oncogene could force the system to leave one attractor and flow into another one. The
new attractor might have a much shorter cycle time (implying rapid cell division and reproduction)
and/or be more resistant to outside perturbations (implying difficulty in killing those cells). In this case,
inhibition of the oncogene would not result in reversion to a normal cellular state.3.A set of ÒpartialÓ oncogenes may force the system into a new attractor. In this case, no individual
partial oncogene would induce a phenotypical change by itselfÑhowever, the phenomenology associ-
ated with a new attractor would be similar.These different scenarios have implications for both research and therapy. From a research perspec-tive, the operation of the second and third mechanisms implies that the networkÕs trajectory through
state space is entirely different, a fact that would impede the effectiveness of traditional methodologies
that focus on one or a few regulatory pathways or oncogenes. From a therapeutic standpoint, the
operation of the latter two mechanisms implies that a focus on Òknocking out the causal oncogeneÓ is
not likely to be very effective.5.4.3.3Genetic Regulation as Circuits
Genetic networks can also be modeled as electrical circuits.81 
In some ways, the electrical circuitanalogy is almost irresistible, as can be seen from a glance at any of the known regulatory pathways: the
tangle of links and nodes could easily pass for a circuit diagram of IntelÕs latest Pentium chip. For
example, McAdams and Shapiro described the regulatory network that governs the course of a -phageinfection in E. coli as a circuit, and included factors such as time delays, which are critical in biologicalnetworks (gene transcription and translation are not instantaneous, for example) and indeed, in electri-
cal networks, as well.More generally, natureÕs designs for the cellular circuitry seems to draw on any number of tech-niques that are very familiar from engineering: ÒThe biochemical logic in genetic regulatory circuits
provides real-time regulatory control [via positive and negative feedback loops], implements a branch-79S.A. Kauffman, ÒDifferentiation of Malignant to Benign Cells,Ó Journal of Theoretical Biology 31:429, 1971. (Cited in Szallasi andLiang, 1998.)80S. Baasner, H. von Melchner, T. Klenner, P. Hilgard, and T. Beckers, ÒReversible Tumorigenesis in Mice by ConditionalExpression of the HER2/c-erbB2 Receptor Tyrosine Kinase,Ó Oncogene 13(5):901, 1996. (Cited in Szallasi and Liang, 1998.)81H.H. McAdams and L. Shapiro, ÒCircuit Simulation of Genetic Networks,Ó Science 269(5224):650-656, 1995.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.158CATALYZING INQUIRYing decision logic, and executes stored programs [in the DNA] that guide cellular differentiation ex-tending over many cell generations.Ó82 Table 5.2 describes some of the similarities.Of course, taking an engineering view of biological circuits does not make understanding themtrivial. For example, consider that cellular regulatory circuits implement a complex adaptive control
system. Understanding this system is greatly complicated by the fact that at the biochemical implemen-
tation level, the distinction between the controlling mechanisms and the controlled processes is not as
clear as it is when such control is engineered into a human-designed artifact. In a biochemical environ-
ment, control reactions and controlled functions are composed of intermingled molecules interacting in
ways that make identification of roles much more complex.Nor does the analogy to electrical circuits always carry over perfectly. Because critical molecules areoften present in the cell in extremely small quantities, to take the most notable example, certain critical
reactions are subject to large statistical fluctuations, meaning that they proceed in fits and starts, much
more erratically than their electrical counterparts.5.4.3.4Combinatorial Synthesis of Genetic Networks
83Guet et al. have demonstrated the feasibility of creating synthetic networks, composed of well-characterized genetic elements, that provide a framework for understanding how diverse phenotypi-TABLE 5.2Points of Similarity Between Genetic Logic and Electronic Digital Logic in Computer
ChipsCharacteristicElectronic LogicGenetic Logic
SignalsElectron concentrationsProtein concentrations
distributionPoint-to-point (by wiresDistributed volumetrically by
or by electrically encodeddiffusion or compartment-to-
addresses)compartment 
by activetransport mechanismsOrganizationHierarchicalHierarchical
logic typeDigital, clocked, sequentialAnalog, unclocked (can
logicapproximate asynchronoussequential logic; dependent onrelative timing of signals)NoiseInherent noise due to discreteInherent noise due to discrete
electron events andchemical events and
   environmental effectsenvironmental effects
Signal-to-noise ratioSignal-to-noise ratio high inSignal-to-noise ratio low in
most circuits most circuits
Switching speedFast (>10
Ð9 sÐ1)Slow (<10Ð2 sÐ1)SOURCE: Excerpted with permission from H. McAdams and A. Arkin, ÒSimulation of Prokaryotic Genetic Circuits,Ó AnnualReview of Biophysics and Biomolecular Structure 27:199-224, 1998, available at http://caulo.stanford.edu/usr/hm/pdf/1998_McAdams_simulation_genetic_circuits.pdf. Originally published by Annual Review of Biophysics and Biomolecular Structure.82H.H. McAdams and A. Arkin, ÒSimulation of Prokaryotic Genetic Circuits,Ó Annual Reviews of Biophysical and BiomolecularStructure 27:199-224, 1998.83Section 5.4.3.4 is based on C.C. Guet, M.B. Elowitz, W. Hsing, and S. Leibler, ÒCombinatorial Synthesis of Genetic Net-works,Ó Science 296(5572):1466-1470, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY159cal functionality can (but does not always) arise from changes in network topology rather thanchanges in the elements themselves. This functionality includes networks that exhibit the behavior
associated with negative and positive feedback loops, oscillators, and toggle switches. By showing
that functionality can change dramatically due to changes in topology, Guet et al. argue that once a
simple set of genes and regulatory elements is in place, it is possible to jump discontinuously from
one functional phenotype to another using the same ÒtoolkitÓ of genes simply by modifying the
regulatory connections. Such discontinuous changes are different from the more gradual effects driven
by successive point mutations.Such discontinuities reflect the nonlinear nature of genetic networks. Furthermore, the topology ofconnectivity of a network does not necessarily determine its behavior uniquely, and the behavior of
even simple networks built out of a few well-characterized components cannot always be inferred from
connectivity diagrams alone. Because genetic networks are nonlinear (and stochastic as well), the un-
known details of interactions between components might be of crucial importance to understanding
their functions. Combinatorially developed libraries of simple networks may thus be useful in uncover-
ing the existence of additional regulatory mechanisms and exploring the limits of quantitative modeling
of cellular systems.The system of Guet et al. uses a small number of elements restricted to a single type of interaction(transcriptional regulation), but the range of biochemical interactions can be extended by including
other modular genetic elements. For example, the approach can be extended to include linking input
and output through cell-cell signaling molecules, such as those involved in quorum sensing. Also, this
combinatorial strategy can be used to search for other dynamic behaviors such as switches, sensors,
oscillators, and amplifiers, as well as for high-level structural properties such as robustness or noise
resistance.5.4.3.5Identifying Systems Responses by Combining Experimental
Data with Biological Network InformationMawuenyega et al. have developed a method to identify specific subnetworks in large biologicalnetworks.84 A biological network is constructed by identifying components (genes, proteins, transcrip-tion factors, chemicals) and interactions between components (protein-protein, protein-DNA, signal
transduction, gene expression, catalysis) from genome context information as well as from external
sources (databases, literature, and direct interaction with experimentalists). By superimposing experi-
mental data such as expression values or identified proteins, it is possible to identify a best-scored
subnetwork in the large biological network. This subnetwork is known as the response network, identify-ing a systemÕs response with respect to the experimental scenario and data used.Proteomic mass spectroscopy (MS) analysis was used to identify and characterize 1,044 Mycobacte-rium tuberculosis (TB) proteins and their corresponding cellular locations. From these 1,044 identified, 70proteins were selected that are known to function in lipid biosynthesis (20) and fatty acid degradation
(50). It is striking that the identified proteins involved in fatty acid degradation were distributed be-
tween the different cellular compartments in an almost exclusive fashion (e.g., in the subnetwork
centered on fadB2 and fadB3) (Figure 5.9).In addition, Forst and colleagues performed a response network analysis of mycobacterium tubercu-losis to isoniazid (INH) drug treatment.85 The entirety of the FAS-II fatty acid synthase group (except84K.G. Mawuenyega, C.V. Forst, K.M. Dobos, J.T. Belisle, J. Chen, M.E. Bradbury, A.R. Bradbury, and X. Chen, ÒMycobacte-rium Tuberculosis Functional Network Analysis by Global Subcellular Protein Profiling,Ó Molecular Biology of the Cell 16:396-404,2005.85L. Cabusora, E. Sutton, A. Fulmer, and C.V. Forst, ÒDifferential Network Expression During Drug and Stress Response,ÓBioinformatics 21:2898-2905, 2005, available at http://bioinformatics.oupjournals.org/cgi/content/abstract/bti440v1.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.160CATALYZING INQUIRYacpM, which was not included in the interaction data used to construct the original, whole network)showed up in the INH response network, all with significant up-regulation (Figure 5.10). Furthermore,
the specific removal of these genes (kasA, kasB, fabD, accD6) from the initial set of genes did not affecttheir presence in the INH response subnetwork: the newly calculated network continued to contain
each of them. Forst concluded that INH does directly interfere with the FAS-II fatty acid production
pathway, in confirmation of earlier results.Fatty Acid Degradation Network
Fatty acid degradation
Lipid biosynthesisNeitherCytoplasmCell wall
Membrane
Membrane and cell wall
Membrane and cytoplasm
FIGURE 5.9Fatty acid degradation network. SOURCE: Courtesy of Christian Forst, Los Alamos National Labora-
tories, December 8, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY1615.4.4Organ Physiology
Sydney Brenner has noted that Ògenes can only specify the properties of the proteins they code for,and any integrative properties of the system must be ÔcomputedÕ by their interactions.Ó86 In this context,subcellular behavior and function represents a first level of ÒcomputedÓ interaction; cellular behavior and
function, a second level. Organization of cells into organs provides a context for cellular behavior, and in
the words of Denis Noble, Òsuccessful physiological analysis requires an understanding of the functional
interactions between the key components of cells, organs, and systems, as well as how these interactions
change in disease states. This information resides neither in the genome nor even in the individual
proteins that genes code for. It lies at the level of protein interactions within the context of subcellular,
cellular, tissue, organ, and system structures. There is therefore no alternative to copying nature and
computing these interactions to determine the logic of healthy and diseased states. The rapid growth in
biological databases; models of cells, tissues, and organs; and the development of powerful computing
hardware and algorithms have made it possible to explore functionality in a quantitative manner all the
way from the level of genes to the physiological function of whole organs and regulatory systems.Ó875.4.4.1Multiscale Physiological Modeling
88Physiological modeling is the modeling of biological units at a level of aggregation larger than thatof an individual cell. Biological units can be successively decomposed into subunits (e.g., an organism
may consist of subsystems for circulatory, pulmonary, digestive, and cognitive function; a digestiveFIGURE 5.10The isoniazid (INH) response network. Red nodes indicate up-regulated genes. Blue nodes indicate
down-regulated genes. SOURCE: Courtesy of Christian Forst, Los Alamos National Laboratories, December 8,2004.86S. Brenner, ÒBiological Computation,Ó The Limits of Reductionism in Biology. Wiley, Chichester, UK, 1998, pp. 106-116.87D. Noble, ÒModeling the HeartÑfrom Genes to Cells to the Whole Organ,Ó Science 295(5560):1678-1682, 2002.88Much of the material in Section 5.4.4.1 is based on excerpts from A.D. McCulloch and G. Huber, ÒIntegrative BiologicalModelling in Silico,Ó Novartis Foundation Symposium 247:4-19, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.162CATALYZING INQUIRYsystem may consist of esophagus, stomach, and intestines; and so on down to the level of organelleswithin cells and molecular functions within organelles), and every unit depends on the coordinated
interaction of its subunits.Given the complexity of physiological modeling, it makes sense to replicate this natural organiza-tion. Thus, models of tissue, organs, and even entire organisms are relevant subjects of physiological
modeling. Functional behavior in each of these entities depends on activity at all spatial and temporal
scales associated with structure from protein to cell to tissue to organ to whole organism (Box 5.11) and
requires the integration of interacting physiological processes such as regulation, growth, signaling,
metabolism, excitation, contraction, and transport processes. One term sometimes used for work that
involves such integration is ÒphysiomeÓ (or by analogy to genomics,ÒphysiomicsÓ).89Integration of such models presents many intellectual challenges. Following McCulloch andHuber,90 it is helpful to consider two different types of integration. Structural integration implies integra-tion across physical scales of biological organization from protein to whole organism, while functionalintegration refers to the integrated representation of interacting physiological processes. Structurallyintegrative models (e.g., models of molecular dynamics and other strategies that predict protein func-
tion from structure) are driven by first principles and hence tend to be computation-intensive. Because
they are based on first principles, they impose constraints on the space of possible organismic models.
Functionally integrative models are strongly data-driven and therefore data-intensive, and are needed
to bridge the multiple time and space scales of substructures within an organism without leaving the
problem computationally intractable. Box 5.12 provides a number of examples of intersection between
structurally and functionally integrated models.Predictive simulations of subcomponents at various levels of the hierarchy of complexity are gener-ally based on physicochemical first principles. Integrating such simulations, of which micromechanical
tissue models and molecular dynamics models are examples, with each other across scales of biological
organization is highly computationally intensive (and requires a computational infrastructure that
enables distributed and heterogeneous computational resources to participate in the integration and
facilitates the modular addition of new models and levels of organization).5.4.4.2Hematology (Leukemia)
Childhood acute lymphoblastic leukemia (ALL) is a lethal but highly treatable disease. However,successful treatment depends on the ability to deliver the correct intensity of therapy. Improper inten-
sity can result in an excess of deaths caused by toxicity, decreased mental function over the long term,
and undertreatment for high-risk cases.The appropriate intensity is determined today through an extensiveÑand expensiveÑrange ofprocedures including morphology, immunophenotyping, cytogenetics, and molecular diagnostics.
However, Limsoon Wong has developed a relatively inexpensive single-platform microarray test that
uses gene expression profiling to identify each of the known clinically important subgroups of child-
hood ALL (Figure 5.11) and hence the appropriate intensity of treatment.91 This is confirmed usingcomputer-assisted supervised learning algorithms, in which an overall diagnostic accuracy of 96 per-
cent was achieved in a blinded test sample. To determine whether expression profiling at diagnosis89J.B. Bassingthwaighte, ÒToward Modeling the Human Physionome,Ó pp. 331-339 in Molecular and Subcellular Cardiology:Effects on Structure and Function, S. Sideman and R. Beyar, eds., Plenum Press, New York, Volume 382 in Advanced Experimentsin Medical Biology, 1995; http://www.physiome.org/.90A.D. McCulloch and G. Huber, ÒIntegrative Biological Modelling in Silico,Ó pp. 4-25 in ÔIn SilicoÕ Simulation of BiologicalProcesses No. 247, Novartis Foundation Symposium, G. Bock and J.A. Goode, eds., John Wiley & Sons Ltd., Chichester, UK, 2002.91L. Wong, ÒDiagnosis of Childhood Acute Lymphoblastic Leukemia and Optimization of Risk-Benefit Ratio of Therapy,ÓPowerPoint presentation presented at the Institute for Infocomm Research, 2003, Singapore, available at http://sdmc.lit.org.sg:8080/~limsoon/psZ/wls-aasbi03.ppt.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY163might further help identify those patients who are likely to relapse up to 4 years later, the expressionprofiles of four groups of leukemic samples with different outcomes were compared. Distinct gene
expression profiles for each of these groups were identified.5.4.4.3Immunology
The immune system provides protection for human beings from pathogens. (For purposes of thisdiscussion, the immune system of interest here refers to the adaptive immune system. The human bodyalso has an innate immune system that provides a first response to pathogens that is essentially inde-
pendent of the specific pathogenÑin essence, its role is to give the adaptive immune system time to
build a more specific response.) To do so, the immune system must first identify an entity within the
body as a harmful pathogen that it should attack or eliminate and then mount a response that does so.In principle, the identification of harmful pathogens might be based on a list of known pathogens.If an entity is found within the human body that is sufficiently similar to a known pathogen, it could be
marked for later attack and destruction. However, a list-based approach to pathogen identification
suffers from two major weaknesses. First, any such list would have to be large enough to include most
of the possible pathogens that an organism might encounter in its lifetime; some estimates of the
number of different foreign molecules that the human immune system is capable of recognizing are as
high as 1016.92 Second, because pathogens evolve (and, thus, new pathogens are created), an a priori listcould never be complete.Accordingly, nature has developed an alternative mechanism for pathogen identification based onthe notion of ÒselfÓ versus Ònonself.Ó In this paradigm, entities or substances that are recognized as self
are deemed harmless, while those that are nonself are regarded as potentially dangerous. Thus, the
immune system has developed a variety of mechanisms to differentiate between these two categories.
Note that this distinction is highly simplistic, as not all nonself entities are bad for the human body (e.g.,
transplanted organs that replace original organs damaged beyond repair). Nevertheless, the self-non-
self distinction is not a bad point of departure for understanding the human immune system.The immune system relies on a process that generates detectors for a subset of possible pathogensand constantly turns over those detectors for new detectors capable of identifying a different set of
pathogens. When the immune system identifies a pathogen, it selects one of several immunological
mechanisms (e.g., those associated with the different immunoglobulin [Ig] groups) to eliminate it.
Furthermore, the immune system retains memory of the pathogen, in the form of detectors that are
specifically configured for high affinity to that pathogen. Such memory enables the immune system to
confer long-lasting resistance (immunity) to pathogens that may be encountered in the future and to
mount a stronger response to such future encounters.Many of the broad outlines of the immune system are believed to be understood, and computa-tional modeling of the immune system has shed important light on its detailed workings, as described
in Box 5.13. A medical application of simulation models in immunology has been to evaluate the effects
of revaccinating someone yearly for influenza. Because of the phenomenon of immune memory, a
vaccine that is too similar to a prior yearÕs vaccine will be eliminated rapidly by the immune response (a
negative interference effect). A simulation model by Smith et al. has examined this effect and suggests
some circumstances under which individuals who are vaccinated annually will have greater or less
protection than those with a first-time vaccination.93 The Smith et al. results also suggested that in theproduction of flu vaccine, a choice among otherwise equivalent strains (i.e., strains thought to be92J. Inman, ÒThe Antibody Combining RegionÑSpeculations on the Hypothesis of General Multispecificity,Ó Theoretical Immu-nology, G. Bell, ed., Dekker, New York, 1978.93D.J. Smith, S. Forrest, D.H. Ackley, and A.S. Perelson, ÒVariable Efficacy of Repeated Annual Influenza Vaccination,Ó Pro-ceedings of the National Academy of Sciences 96(24):14001-14006, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.164CATALYZING INQUIRYBox 5.11Levels of Biological OrganizationOne helpful approach is to consider a set of different, but interrelated, levels of biological organization:¥Organ system, in which the entire organ can be represented by a lumped-parameter systems model thatcan be used to predict the gross behavior of the organ. In the case of the heart, one model can be based on thenotion of arterial impedance, which can be used to generate the dynamic pressure boundary conditions actingon the cardiac chambers.
¥Whole organ continuum, in which the physical behavior and dynamical responses of the organ can becalculated from finite element methods that solve the continuum equations for the mechanics of the organ. Inthe case of the heart, boundary conditions such as ventricular cavity pressures are computed from the lumped
parameter model in the top level. Detailed parametric models of three-dimensional cardiac geometry andmuscle fiber orientations have been used to represent the detailed structure of the whole organ with submil-limeter resolution.1¥Tissue, in which constitutive laws for the continuum models are evaluated at each point in the whole organcontinuum model and obtained by homogenizing the results of multicellular network models. That is, homog-enization theory can be used to re-parameterize the results of a micromechanical analysis into a form suitable
for continuum-scale stress analysis. In the case of tissue mechanics for the heart, the basic functional units oftissue are represented, such as laminar myocardial sheets as ensembles of cell and matrix micromechanicsmodels and, in some cases, the microvascular blood vessels as well.2 A variety of approaches for these modelshave been used, including stochastic models based on measured statistical distributions of myofiber orienta-tions.3 In cardiac electrophysiology, the tissue level is typically modeled as resistively coupled networks ofdiscrete cellular models interconnected in three dimensions.4¥Single cell, in which different types of cells are represented. As a rule, single-cell models bridge to stochas-tic state-transition models of macromolecular function through subcellular compartment models of represen-tative tissue structures (e.g., the sarcomere in the case of the heart). Heart cells of different types to be modeled
are representative cells from different regions of the heart, such as epicardial cells, midventricular M-cells, andendocardial cells. For mechanical models, individual myofibrils and cytoskeletal structures are modeled bylattices and networks of rods, springs, and dashpots in one, two, or three dimensions.
¥Macromolecular complex, in which representative populations of cross-bridges or ion channels are mod-eled. Such complexes are typically described by Markov models of stochastic transitions between discretestates of, for example, channel gating, actin-myosin binding, or nucleotide bound to myosin.
¥Molecular model, in which single cross-bridges and ion channels are represented. Cross-bridges moveaccording to Brownian dynamics, and it is necessary to use weighted-ensemble dynamics to allow the simu-lation to clear the energy barriers. (For example, a weighted-ensemble Brownian dynamics simulation of ion
transport through a single channel can be used to compute channel gating properties from the results of ahierarchical collective motion (HCM) simulation of the channel complex.) The flexibility of the cross-bridgesthemselves can be derived from the HCM method, and the interactions with other molecules can be comput-
ed using continuum solvent approximations.¥Atomic model, in which molecules are represented in terms of the positions of their constituent atoms incrystallographic structures. (Such data can be found in public repositories such as the Protein Data Bank.)
Such data feed molecular dynamics simulations in order to build the HCM model.The approach described aboveÑof integrating models across structural and functional linesÑis generallyadaptable to other tissues and organs, especially those with physical functions, such as lung and cartilage.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY165Box 5.12Examples of Intersection Between Structurally and Functionally Integrated ModelsThere are a number of examples of intersection between structurally and functionally integrated models,including the following:¥Linkage of biochemical networks and spatially coupled processes, such as calcium diffusion in structurallybased models of cell biophysics;1¥Use of physicochemical constraints to optimize genomic systems models of cell metabolism;2¥Integration of genomic or cellular system models into multicellular network models of memory and learn-ing,3 developmental pattern formation,4 or action potential propagation;5¥Integration of structure-based predictions of protein function into systems models of molecular networks;¥Development of kinetic models of cell signaling and coupling them to physiological targets such as energymetabolism, ionic currents or cell motility;6¥Use of empirical constraints to optimize protein folding predictions;7 and¥Integration of systems models of cell dynamics into continuum models of tissue and organ physiology.81L.M. Loew, ÒThe Virtual Cell Project,Ó Novartis Foundation Symposium 247:151-161, 2002; L.M. Loew and J.C. Schaff, ÒThe Virtual Cell:A Software Environment for Computational Cell Biology,Ó Trends in Biotechnology 19(10):401-406, 2001.2B.O. Palsson, ÒWhat Lies Beyond Bioinformatics?Ó Nature Biotechnology 15:3-4, 1997; C.H. Schilling, J.S. Edwards, D. Letscher, andB.O. Palsson, ÒCombining Pathway Analysis with Flux Balance Analysis for the Comprehensive Study of Metabolic Systems,Ó Biotechnologyand Bioengineering 71(4):286-306, 2000-2001.3D. Durstewitz, J.K. Seamans, and T.J. Sejnowski, ÒNeurocomputational Models of Working Memory,Ó Nature Neuroscience3(Supplement):S1184-S1191, 2000; P.H. Tiesinga, J.M. Fellous, J.V. Jose, and T.J. Sejnowski, ÒInformation Transfer in Entrained CorticalNeurons,Ó Network: Computation in Neural Systems 13(1):41-66, 2002.4E.H. Davison, J.P. Rast, P. Oliveri, A. Ransick, C. Calestani, C.H. Yuh, T. Minokawa, et al., ÒA Genomic Regulatory Network forDevelopment,Ó Science 295(5560):1669-1678, 2002.5R.M. Shaw and Y. Rudy, ÒElectrophysiologic Effects of Acute Myocardial Ischemia: A Mechanistic Investigation of Action PotentialConduction and Conduction Failure,Ó Circulation Research 80(1):124-138, 1997.6J.M. Levin, R.C. Penland, A.T. Stamps, and C.R. Cho, ÒUsing in Silico Biology to Facilitate Drug Development.,Ó in Novartis FoundationSymposium 247: 222-238, 2002.7L. Salwinski and D. Eisenberg, ÒMotif-based Fold Assignment,Ó Protein Science 10(12):2460-2469, 2001.8R.L. Winslow, D.F. Scollan, A. Holmes, C.K. Yung, J. Zhang, M.S. Jafri, ÒElectrophysiological Modeling of Cardiac Ventricular Function:From Cell to Organ,Ó Annual Reviews of Biomedical Engineering 2: 119-155, 2002; N.P. Smith, P.J. Mulquiney, M.P. Nash, C.P. Bradley,D.P. Nickerson, and P.J. Hunter, ÒMathematical Modelling of the Heart: Cell to Organ,Ó Chaos, Solitons and Fractals 13:1613-1621, 2002.SOURCE: A.D. McCulloch and G. Huber, ÒIntegrative Biological Modelling in Silico,Ó pp. 4-19 in ÔIn SilicoÕ Simulation of BiologicalProcesses No. 247, Novartis Foundation Symposium, G. Bock and J.A. Goode, eds., John Wiley & Sons Ltd., Chichester, UK, 2002.Reproduced with permission from John Wiley & Sons Ltd.1F.J. Vetterand A.D. McCulloch, ÒThree-dimensional Analysis of Regional Cardiac Function: A Model of Rabbit Ventricular Anatomy,ÓProgress in Biophysics and Molecular Biology 69(2-3):157-183, 1998.2K. May-Newman and A.D. McCulloch, ÒHomogenization Modelling for the Mechanics of Perfused Myocardium,Ó Progress in Biophysicsand Molecular Biology 69(2-3):463-481, 1998.3T.P. Usyk, J.H. Omens, and A.D. McCulloch, ÒRegional Septal Dysfunction in a Three-dimensional Computational Model of FocalMyofiber Disarray,Ó American Journal of Physiology 281(2):H506-H514, 2001.4L.J. Leon and F.A. Roberge, ÒDirectional Characteristics of Action Potential Propagation in Cardiac Muscle: A Model Study,Ó CirculationResearch 69: 378-395, 1991.SOURCE: Adapted from A.D. McCulloch and G. Huber, ÒIntegrative Biological Modelling in Silico,Ó pp. 4-25 in ÔIn SilicoÕ Simulation ofBiological Processes No. 247, Novartis Foundation Symposium, G. Bock and J.A. Goode, eds., John Wiley & Sons Ltd., Chichester, UK,2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.166CATALYZING INQUIRY94A.D. McCulloch and G. Huber, ÒIntegrative Biological Modelling in Silico,Ó pp. 4-25 in ÔIn SilicoÕ Simulation of BiologicalProcesses No. 247,
 Novartis Foundation Symposium, G. Bock and J.A. Goode, eds., John Wiley & Sons Ltd., Chichester, UK, 2002.equally good guesses of the upcoming epidemic strain and equally appropriate for manufacture) shouldbe resolved in favor of the strain that is most different from the one used in the previous year, because
this choice would reduce the effects of negative interference and thus potentially increase vaccine
efficacy in recipients of repeat vaccines.5.4.4.4The Heart
The heart is an organ of primary importance in vertebrates, and heart disease is one of the primarycauses of death in the Western world. At the same time, the heart is an organ of high complexity.
Although it is in essence an impulsive pump, it is a pump that must operate continuously and repair
itself if necessary while in operation. Its output must be regulated according to various physiological
conditions in the body, and its performance is affected by the characteristics of the arterial and vein
networks to which it is connected.The heart brings together many subsystems that interact mutually through fundamental physi-ological processes. As a general rule, physiological processes have both functional and structural di-
mensions. For example, cells are functionally specializedÑblood cells and myocytes (heart cells) do
different things. Furthermore, blood cells and heart cells are themselves part of a collective of other
blood cells and heart cells; thus, the structure within which an individual cell is embedded is relevant.An integrated computational model of the heart would bring together all of the relevant physiologi-cal processes (Box 5.14).94 Were such a model available, it would be possible to investigate commonFIGURE 5.11Microarray expression groupings indicating known clinically important subgroups of childhood
acute lymphoblastic leukemia (ALL). Note in particular the second column from the right, labeled Ònovel.Ó In thisinstance, the hierarchical clustering of gene expression reveals a novel subtype of childhood ALL. SOURCE:
Courtesy of L. Wong, Institute for Infocomm Research, Singapore, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY167heart diseases and to probe cardiac structure and function in different places in the heartÑa point ofsome significance in light of the fact that heart failure is usually regional and nonhomogeneous. The
graphic in Box 5.14 emphasizes functional integration in the heart, and the majority of functional
interactions take place at the scale of the single cell. However, an organismÕs behavior depends on
interactions that span many orders of magnitude of space and time (from molecular structures and
events to whole-organ anatomy and physiology). Thus, high-fidelity modeling of an organism or organ
system within an organism demands the integration of information across similar scales.An example of a functional model of a single cell is the work of Winslow et al. in modeling thecardiac ventricular cell, and specifically the relationship between various current flows in the cell andBox 5.13Modeling in ImmunologyIn basic immunology, issues related to mutation also have been the focus of mathematical modeling and intenseexperimentation. . . . [For example,] during the course of an immune response, B lymphocytes within germinal
centers can rapidly mutate the genes that code for antibody variable regions. The immune system thus provides anenvironment in which evolution occurs on a time scale of weeks. Among the large number of mutant B cells that aregenerated, selection chooses for survival those B cells that have increased binding affinity for the antigen that
initiated the response. After 2 to 3 weeks, antibodies can have improved their equilibrium binding constant forantigen by one to two orders of magnitude, and may have sustained as many as 10 point mutations. How can theimmune system generate and select variants with higher fitness this rapidly and this effectively? An optimal control
model has suggested that mutation should be turned on and off episodically in order to allow new variants time toexpand without being subjected to the generally deleterious effects of mutation. Time-varying mutation could beimplemented by having cells recycle through one region of the germinal center, mutating while there, and prolifer-
ating in a different region of the germinal center. This suggestion has generated new experimental investigations ofevents that occur within germinal centers. Opportunities exist for a range of models that address basic questionsabout in vivo cell population dynamics and evolution, as well as more detailed questions involving the immunolog-
ical mechanisms underlying affinity maturation.Control of the immune response is another area ripe for modeling. What determines the intensity of a response? Howis the response shut off when the antigen is eliminated? Feedback mechanisms may exist to control the responseintensity, response length, and type of response (cellular or antibody). Some models of a basic feedback mechanisminvolving two types of helper T cells, TH1 and TH2, have been developed; others are needed. Regulatory mechanismsinvolve interactions among many cell populations that communicate by direct cell-cell contact and through thesecretion of cytokines. Diagrams representing the elements of regulatory schemes commonly have scores of ele-ments. Because of the complexities involved, theorists have an opportunity to lead experimentation by providing
suggestions as to what needs to be measured and how such measurements can be used to provide an insightful viewof possible control mechanisms.A fundamental feature of the immune system is its diversity. Successful recognition of antigens appears to require arepertoire of at least 105 different lymphocyte clones. The diversity of the immune system has challenged experimen-talists, and many recent advances have come from developing experimental models with limited immune diversity.
However, models based on ecological concepts may provide insights into the control of clonal diversity, and mod-ern computational methods now make it practical to consider models with tens of thousands of clones. Thus, it ispossible to develop models that start to approach the size of small immune systems. Simulations have suggested that
from simple rules of cell response, emergent phenomena arise that may have immunological significance. Thechallenge in using computation is to develop models that address important questions, are realistic enough tocapture the relevant immunology, and yet are simple enough to be revealing.SOURCE: Reprinted by permission from S.A. Levin, B. Grenfell, A. Hastings, and A.S. Perelson, ÒMathematical and ComputationalChallenges in Population Biology and Ecosystems Science,Ó Science 275(5298):334-343. Copyright 1997 AAAS. (References omitted.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.168CATALYZING INQUIRYBox 5.14Computational Modeling of the Heart. . . [Integrative cardiac modelling has sought] to integrate data and theories on the anatomy and structure, hemody-namics and metabolism, mechanics and electrophysiology, regulation and control of the normal and diseased heart.The challenge of integrating models of many aspects of such an organ system, including its structure and anatomy,biochemistry, control systems, hemodynamics, mechanics and electrophysiology, has been the theme of severalworkshops over the past decade or so.Some of the major components of an integrative cardiac model that have been developed include [models of]ventricular anatomy and fiber structure, coronary network topology and hemodynamics, oxygen transport and sub-
strate delivery, myocyte metabolism, ionic currents, impulse propagation, excitation-contraction coupling, neuralcontrol of heart rate and blood pressure, cross-bridge cycling, tissue mechanics, cardiac fluid dynamics and valvemechanics, and ventricular growth and remodelling. . . .. . . . [T]hese models can be extended and integrated with others [by considering the role in] several major functionalmodules . . . as shown in the figure below. . . . They include:¥Coronary artery anatomy and regional myocardial flows for substrate and oxygen delivery.¥Metabolism of the substrate for energy metabolism, fatty acid and glucose, the tricarboxylic acid (TCA) cycle, andoxidative phosphorylation.¥Purine nucleoside and purine nucleotide metabolism, describing the formation of ATP and the regulation of itsdegradation to adenosine in endothelial cells and myocytes, and its effects on coronary vascular resistance.
¥The transmembrane ionic currents and their propagation across the myocardium.¥Excitation-contraction coupling: calcium release and reuptake, and the relationships between these and thestrength and extent of sarcomere shortening.
¥Sarcomere dynamics of myofilament activation and cross-bridge cycling, and the three-dimensional mechanicsof the ventricular myocardium during the cardiac cycle.¥Cell signalling and the autonomic control of cardiac excitation and contraction.. . . While [Figure 5.14.1] does show different scales in the structural hierarchy, it emphasizes functional integration, andthus it is not surprising that the majority of functional interactions take place at the scale of the single cell. . . . [A
functionally integrated] model of functionally interacting networks in the cell can be viewed as a foundation for structur-ally coupled models that extend to multicellular networks, tissue, organ and organ system. But it can also be viewed as afocal point into which feed structurally based models of protein function and subcellular anatomy and physiology.. . . Predictive computational models of various processes at almost every individual level of the hierarchy have beenbased on physicochemical first principles. Although important insight has been gained from empirical models of
living systems, models become more predictive if the number of adjustable parameters is reduced by making use ofdetailed structural data and the laws of physics to constrain the solution. These models, such as molecular dynamicssimulations, spatially coupled cell biophysical simulations, tissue micromechanical models and anatomically based
continuum models are usually computationally intensive in their own right. . . . This will require a computationalinfrastructure that will allow us to integrate physically based biological models that span the hierarchy from thedynamics of individual protein molecules up to the regional physiological function of the beating heart. . . .Investigators have developed large-scale numerical methods for ab initio simulation of biophysical processes at thefollowing levels of organization: molecular dynamics simulations based on the atomic structure of biomolecules;
hierarchical models of the collective motions of large assemblages of monomers in macromolecular structures;biophysical models of the dynamics of cross-bridge interactions at the level of the cardiac contractile filaments;whole-cell biophysical models of the regulation of muscle contraction; microstructural constitutive models of the
mechanics of multicellular tissue units; continuum models of myocardial tissue mechanics and electrical impulsepropagation; and anatomically detailed whole organ models.They have also investigated methods to bridge some of the boundaries between the different levels of organization.We [McCulloch and Huber] and others have developed finite-element models of the whole heart, incorporatingmicrostructural constitutive laws and the cellular biophysics of thin filament activation. Recently, these mechanicsCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY169models have been coupled with a non-linear reaction-diffusion equation model of electrical propagation incorporat-ing an ionic cellular model of the cardiac action potential and its regulation by stretch. At the other end of thehierarchy, Huber has recently developed a method, the Hierarchical Collective Motions method, for integrating
molecular dynamics simulation results from small sections of a large molecule into a quasi-continuum model of theentire molecule.FIGURE 5.14.1Some major functional subsystems of an integrated heart model and their hierarchical relationships from
cell to tissue to organ and cardiovascular system.SOURCE: A.D. McCulloch and G. Huber, ÒIntegrative Biological Modelling in Silico,Ó pp. 4-19 in ÔIn SilicoÕ Simulation ofBiological Processes No. 247, Novartis Foundation Symposium, G. Bock and J.A. Goode, eds., John Wiley & Sons Ltd.,Chichester, UK, 2002. Text and figure reproduced with permission from John Wiley & Sons Ltd. (References omitted.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.170CATALYZING INQUIRY95R.L. Winslow, D.F. Scollan, A. Holmes, C.K. Yung, J. Zhang, and M.S. Jafri, ÒElectrophysiological Modeling of CardiacVentricular Function: From Cell to Organ,Ó Annual Reviews of Biomedical Engineering 2:119-155, 2002.96P.J. Hunter, ÒThe IUPS Physiome Project: A Framework for Computational Physiology,Ó Progress in Biophysics and MolecularBiology 85(2-3):551-569, 2004.Box 5.15Illustrations of Functional Models of Cellular BehaviorExample 1: Results from Single-cell ModelingWinslow et al. have developed and applied a model of the normal and failing canine ventricular myocyte toanalysis of the functional significance of changes in gene expression during tachycardia pacing-induced heartfailure. Using the data on mRNA and protein expression levels cited above, these investigators defined a minimal
model of end-stage heart failure as (1) 33 percent reduction of I
K1; (2) 66 percent reduction of I
to1; (3) 68 percent
reduction of the SR [sacroplasmic reticulum] Ca2+-ATPase; and (4) 75 percent upregulation of the Na
+-Ca2+exchanger. They incorporated these changes sequentially into the computational model and used the model topredict the functional consequences of each alteration of gene expression in this disease. Results show that theminimal HF [heart failure] model can reproduce the increased APD [action potential duration] observed infailing myocytes relative to normal myocytes. The minimal model can also account for the reduced amplitude
and slowed relaxation of the Ca2+ transients observed in failing versus normal myocytes. Most importantly,model simulations reveal that reduced expression of the outward potassium currents Ito1 and IK1 have relativelylittle impact on APD, whereas altered expression of the Ca2+ handling proteins has a profound impact on APD.These results suggested a strong interplay between APD and the properties of Ca2+ handling in canine myo-cytes. The nature of this interplay was examined in the model. The model indicated that reductions in expres-
sion level of the SR Ca2+-ATPase and increased expression of the Na+-Ca2+ exchanger both contribute to areduction of JSR Ca2+ load. This reduction in the junctional SR (JSR) Ca2+ load in turn produces a smaller Ca2+release from SR, reduced subspace Ca2+ levels, and therefore reduced Ca2+-mediated inactivation of the Ca2+current. The enhanced Ca2+ current then contributes to prolongation of APD. This is an important insight,because identifies the heart failure-induced reduction in JSR Ca2+ load as a critical factor in APD prolongationand in the accompanying increased risk of arrhythmias related to repolarization abnormalities.Analyses of the type described above are likely to become increasingly important in determining the function-al role of altered gene and protein expression in various disease states as more comprehensive large-scale data
on genome and protein expression in disease become available.its contractile behavior.95 In particular, Winslow has used this model to show that the reduced contrac-tility (i.e., reduction in the strength with which a ventricular muscle contracts, which is associated with
heart failure) is caused largely by changes in the calcium ion currents in those cells, rather than changes
in potassium ion currents as was widely speculated before this work (Example 1 in Box 5.15). Such an
insight suggests that the development of drugs to cope with heart failure would thus be better focused
on those that can regulate calcium flow. Examples 2 and 3 in Box 5.15 illustrate some of the scientific
insights that can be gained with a computational model integrated across functional and structural
lines.Integrating these various perspectives on the heart (and other organs as well) is the mission of thePhysiome Project, which seeks to construct models that incorporate the detailed anatomy and tissue
structure of an organ in a way that allows the inclusion of cell-based models and spatial structure and
distribution of proteins. The Physiome project has developed a computational framework for integrat-
ing the electrical, mechanical, and biochemical functions of the heart:96Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY171¥The underlying anatomical descriptions are based on finite element techniques, and orthotropicconstitutive laws based on the measured fiber-sheet structure of myocardial tissue drive the dynamics
of the large deformation soft-tissue mechanics involved.¥Patterns of electrical current flow in the heart are computed using reaction-diffusion equationson a grid of deforming material points which access systems of ordinary differential equations repre-
senting the cellular processes underlying the cardiac action potential; these result in representations of
the activation wavefronts that spread around the heart and initiate contraction.¥Coronary blood flow is computed based on the Navier-Stokes equations in a system of branchingblood vessels embedded in the deforming myocardium and the delivery of oxygen and metabolites is
coupled to the energy-dependent cellular processes.These models of different cardiac phenomena have been been implemented with ÒhorizontalÓintegration of mechanics, electrical activation and metabolism, together with ÒverticalÓ integration from
cell to tissue to organ. Thus, these models can be said to deconstruct an organ into a set of (submodels
for) constituent functions, with explicit feedback and connection between them represented in the
overall model of the whole organ.Results from Integrated Modeling (Examples 2 and 3)In the clinical arrhythmogenic disorder long-QT syndrome, a mutation in a gene coding for a cardiomyocytesodium or potassium-selective ion channel alters its gating kinetics. This small change at the molecular level
affects the dynamics and fluxes of ions across the cell membrane and thus affects the morphology of therecorded electrocardiogram (prolonging the QT interval and increasing the vulnerability to life-threateningcardiac arrhythmia). Such an understanding could not be derived by considering only the single gene, chan-
nel, or cell; it is an integrated response across scales of organization. A hierarchical integrative simulationcould be used to analyze the mechanism by which this genetic defect can lead to sudden cardiac death, forexample, by exploring the effects of altered repolarization on the inducibility and stability of reentrant activa-
tion patterns in the whole heart. A recent study made excellent progress in spanning some of these scales byincorporating a Markov model of altered channel gating, based on the structural consequences of the geneticdefect in the cardiac sodium channel, into a whole-cell kinetic model of the cardiac action potential that
included all the major ionic currents.. . . [It] is becoming clearer that mutations in specific proteins of the cardiac muscle contractile filament systemlead to structural and developmental abnormalities of muscle cells, impairment of tissue contractile function,and the eventual pathological growth (hypertrophy) of the whole heart as a compensatory response. In thiscase, the precise physical mechanisms at each level remain speculative, although much detail has been
elucidated recently, so an integrative model will be useful for testing various hypotheses regarding the mech-anisms. The modeling approach could be based on the same integrative paradigm commonly used by exper-imental biologists, in which the integrated effect of a specific molecular defect or structure can be analysed
using techniques such as in vivo gene targeting.SOURCE: R.L. Winslow, D.F. Scollan, A. Holmes, C.K. Yung, J. Zhang, and M.S. Jafri, ÒElectrophysiological Modeling of CardiacVentricular Function: From Cell to Organ,Ó Annual Review of Biomedical Engineering 2:119-156, 2000. Adapted by permission from AnnualReview of Biomedical Engineering. (References and citations omitted.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.172CATALYZING INQUIRY5.4.5Neuroscience
In recent years, neuroscience has expanded its horizons beyond the microstructure of the brainÑneurons, synapses, neurotransmitters, and the likeÑto focus on the brainÕs large-scale cognitive archi-
tecture. Drawing on dramatic advances in mapping techniques, such as functional magnetic resonance
imaging (MRI) and magnetoencephalography, neuroscientists hope to give a computational account of
precisely what each specialized region of the brain is doing and how it interacts with all the other active
regions to produce high-level thought and behavior.5.4.5.1The Broad Landscape of Computational Neuroscience
Neuroscience seeks to probe the details of the brain and the mechanisms by which the nervoussystems develops, is organized, processes information, and establishes mental abilities. Research in
neuroscience spans many levels of organization, from atomic and molecular events on the order of one-
tenth to one nanometer, up to the entire nervous system on the order of a meter or more. In addition,
there are on the order of 1011 neurons and thousands to tens of thousands of synapses per neuron.Information processing in the brain occurs through the interactions and spread of chemical andelectrical signals both within and among neurons. Acting within the extensive but intricate architecture
of the neurons and their interconnections, the mechanisms are nonlinear and span a wide range of
spatial and temporal scales.97 Understanding how the nervous system and brain work thus requires aninterdisciplinary approach to the challenging multiscale integration of experimental data, computa-
tional data, and theory.It is helpful to describe the nervous systemÕs functional processes and their mechanisms at severaldifferent levels of detail, depending on the goal of a given effort. Table 5.3 and Figure 5.12 describe the
numerous spatial and temporal scales relevant to neuroscience research, and provide some indication
of the complexity of such research.To illustrate, a low level of analysis might involve consideration of individual neurons. In thisanalysis, functional properties of neurons such as electronic structure, nerve cell connections (syn-
apses), and voltage-gated ion channels are important. At a higher level, it is recognized that individual
neurons connect in networksÑan analysis at this level examines how individual neurons interact to
form functioning circuits. The mathematics of dynamic systems and visual neuroscience are notably
relevant at this level. At a still higher level, individual networksÑeach with its own specific architecture
and information-processing capabilitiesÑinteract to form neural nets and carry out cognition, speechTABLE 5.3Scales of Neuroscience Research
Spatial ScaleComponent
1 meterCentral nervous system
10 centimetersSystems

1 centimeterMaps
1 millimeterNetworks
100 micronsNeurons

1 micronSynapses
10 angstromsMolecules
97N.T. Carnevale and S. Rosenthal, ÒKinetics of Diffusion in a Spherical Cell: I. No Solute Buffering,Ó Journal of NeuroscienceMethods 41(3):205-216, 1992.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY173FIGURE 5.12Temporal and spatial scales of neuroscience research. SOURCE: Courtesy of Christof Koch, Caltech.
perception, and imaging. At this level, computational analysis of nervous system networks and(connectionist) modeling of psychological processes is the primary focus.Computational neuroscience provides the basis for testing models of the nervous systemÕs func-tional processes and their mechanisms, and computational modeling at several levels of detail is impor-
tant, depending on the purposes of a given effort. Box 5.16 describes simulators that operate at different
levels of detail for different purposes.5.4.5.2Large-scale Neural Modeling
98To better understand a system as complex as the human brain, it is necessary to develop techniquesand tools for supporting large-scale, similarly complex simulations. Recent advances in understanding
how single neurons represent the world,99 how large populations of neurons cooperate to build morecomplex representations,100 and how neurobiological systems compute functions over their representa-tions make large-scale neural modeling a highly anticipated next step.98Section 5.4.5.2 is based largely on material supplied by Chris Eliasmith, University of Waterloo, September 7, 2004.99F. Rieke, D. Warland, R. de Ruyter van Steveninick, and W. Bialek, Spikes: Exploring the Neural Code, MIT Press, Cambridge,MA, 1997; D. Warland, M. Landolfa, J. Miller, and W. Bialek, ÒReading Between the Spikes in the Cercal Filiform Hair Receptorsof the Cricket,Ó Analysis and Modeling of Neural Systems, F. Eeckman, ed., Kluwer Academic Publishers, Boston, MA, 1992.100L. Abbott and T. Sejnowski, Neural Codes and Distributed Representations: Foundations of Neural Computation, MIT Press,Cambridge, MA, 1999; R.S. Zemel, P. Dayan, and A. Pouget, ÒProbabilistic Interpretation of Population Codes,Ó Neural Computa-tion 10, 1998.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.174CATALYZING INQUIRYBox 5.16 Simulators for Computational NeuroscienceThe nervous system is extraordinarily complex. A single cubic centimeter in the brainÕs cerebral cortex con-tains on the order of 5 billion synapses, and these differ in size and shape. The transmission of chemicalsignals is very complex, with many molecules involved, and is an area of intense study. With the introductionof more powerful computer hardware and advances in algorithms, quantitative modeling and realistic simu-
lation in three-dimensions of the interplay of biological ultrastructure and neuron physiology have becomepossible and have provided insight into the variability in signaling and plasticity of the system.To deal with the complexity, multiscale range of space and time, and nonlinearity of neural phenomena, anumber of specialized computational tools have been developed.MCell (a Monte Carlo simulator of cellular microphysiology) simulates individual connections or synapsesbetween neurons and groups of synapses. MCell simulations provide insights into the behavior and variabilityof real systems comprising finite numbers of molecules interacting in spatially complex environments. MCell
incorporates high-resolution physical structure into models of ligand diffusion and signaling and thus can takeinto account the large complexity and diversity of neural tissue at the subcellular level. Monte Carlo algo-rithms are used to simulate ligand diffusion using three-dimensional random walk movements for individual
molecules. Effector sites and surface positions are mapped spatially, and the encounters during ligand diffu-sion are detected. Bulk solution rate constants are converted into Monte Carlo probabilities so that the diffus-ing ligands can undergo stochastic chemical interactions with individual binding sites such as receptor pro-
teins, enzymes, and transporters.GENESIS (the General Neural Simulation System) is a tool for building structurally realistic simulations ofbiological neural systems that quantitatively embed what is known about the anatomical structure and phys-Recent theoretical work has suggested that it is possible to generally characterize the dynamics,representation, and computational properties of any neural population (Figure 5.13).101 Applications ofthese methods have been used successfully to generate models of working memory, rodent naviga-
tional tasks (path integration; see Figure 5.14), eye position control, representation of self-motion, lam-
prey and fish motor control, and deductive reasoning (Figure 5.15).Box 5.17 illustrates the use of computational modeling to understand how dopamine functions inthe prefrontal cortex. The box also illustrates the often-present tension between those who believe that
simple models (in this case, advocates of a connectionist model) can provide useful insight and those
who believe that simple models cannot capture the implications of the complex dynamics of individual
neurons and their synapses and that the addition of considerable biophysical and physiological detail is
needed for real understanding. Many of these models require large numbers of individual, spiking
neurons to be simulated concurrently, which results in significant computational demands. In addition,
calculating the necessary connection weights requires the inversion of extremely large matrices. Thus,
high-performance computing resources are essential for expanding these simulations to include more
neural tissue, and hence more complex neural function.101C. Eliasmith and C.H. Anderson, Neural Engineering: Computation, Representation and Dynamics in Neurobiological Systems,MIT Press, Cambridge, MA, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY175iological characteristics of the neural system of interest. GENESIS reflects the modeling perspective that spatialorganization and structure are important for understanding neural function. GENESIS is organized around
neurons constructed out of components such as compartments (short sections of cellular membrane) andvariable conductance ion channels that receive inputs, perform calculations on them, and then generateoutputs. Neurons in turn can be linked to form neural circuits. GENESIS originally was used largely for realistic
simulations of cortical networks and of the cerebellar Purkinje cell and, more recently, to interconnect celland network properties to biochemical signaling pathways.NEURON is similar to GENESIS in many ways, but contains optimizations that enable it to run very fast onnetworks in which cable properties play a crucial role, that involve system sizes ranging from parts of singlecells to small numbers of cells, and that involve complex branched connections. Furthermore, the perfor-
mance of NEURON degrades very slowly with increasing complexity of morphology and membrane mecha-nisms, and it has been applied to very large network models (104 cells with six compartments each and a totalof 106 synapses in the network. Using a high-level language known as NMODL, NEURON has also beenextended to investigate new kinds of membrane channels. The morphology and membrane properties ofneurons are defined with an object-oriented interpreter, allowing for voltage control, manipulation of currentstimuli, and other biological parameters.SOURCES: For more information, see http://www.mcell.cnl.salk.edu; J.R. Stiles and T.M. Bartol, Jr., ÒMonte Carlo Methods for SimulatingRealistic Synaptic Microphysiology Using MCell,Ó pp. 87-127 in Computational Neuroscience: Realistic Modeling for Experimentalists, E. deShutter, ed., Boca Raton, FL, CRC Press, 2000; J.R. Stiles, T.M. Bartol, Jr., E.E. Salpeter, M.M. Salpeter, T.J. Sejnowski, ÒSynaptic Variability:New Insights from Reconstructions and Monte Carlo Simulations with MCell,Ó pp. 681-731 in Synapses, W.M. Cowan, T.C. Sudhof, C.F.Sudhof, eds., Johns Hopkins University Press, Baltimore, 2001; J.M. Bower, D. Beeman, and M. Hucka, ÒThe GENESIS Simulation System,ÓThe Handbook of Brain Theory and Neural Networks, Second Edition, M.A. Arbib, ed., MIT Press, Cambridge, MA, 2003, pp. 475-478,available at http://www.genesis-sim.org/GENESIS/hbtn2e-bower-etal/hbtn2e-bower-etal.html; M.L. Hines and N.T. Carnevale, ÒThe NEU-RON Simulation Environment,Ó Neural Computation 9(6):1179-1209, 1997, available at www.neuron.yale.edu/neuron/papers/nc97/nsimenv.pdf.5.4.5.3Muscular Control
Muscles are controlled by action potentialsÑbrief, rapid depolarizations of membranes in nervesand muscles. The timing of action potentials transmitted from motor neurons coordinates the contrac-
tion of the muscles they innervate. Rhythmic activity of the nervous system often takes the form of
complex bursting oscillations in which intervals of action potential firing and quiescent intervals of
membrane activity alternate. The relative timing of action potentials generated by different neurons is a
key ingredient in the function of the nervous system.Changes in the electrical potential of membranes are mediated by ion channels that selectivelypermit the flow of ions such as sodium, calcium, and potassium across the membrane. Individual
channels are protein complexes containing membrane-spanning pores that open and close randomly at
rates that depend on many factors. Cellular and network models of membrane potential represent these
systems as electrical circuits in which voltage gated channels function as ÒnonlinearÓ resistors whose
conductance depends on membrane potential. Information is transmitted from one neuron to another
through synapses where action potentials trigger the release of neurotransmitters that bind to channels
of adjacent cells, stimulating changes in the ionic currents of these cells. (The action potential is the basic
neuronal signaling ÒpacketÓ of ionic flow through a cell membrane.)The most basic model of this mechanism is the Hodgkin-Huxley model, which refers to a set ofdifferential equations that describe the action potential.102 Specifically, the Hodgkin-Huxley equations102A.L. Hodgkin and A.F. Huxley, ÒA Quantitative Description of Membrane Current and Its Application to Conduction andExcitation in Nerve,Ó Journal of Physiology 117(4):500-544, 1952.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.176CATALYZING INQUIRYexpress quantitatively the feedback entailed in the relationship between changing ionic flows andchanging membrane potential. Originally based on data collected from experiments on the giant axon of
the squid, the physical model used is that of a membrane separating two infinite regions, each of which
is homogeneous on its side of the membrane.In the nervous system, different kinds of ions pass through the membrane, and the flow of ionsthrough these channels is voltage dependent. In the model, a circuit is used to represent the ion flows
and potential differences that drive ion flow. The semipermeable cell membrane separating the interior
of the cell from the extracellular liquid is modeled as a capacitor, and each ion channel is modeled as a
separately variable resistor. In series with each variable resistor is a battery representing the Nernst
potential arising from the difference in ion concentration on each side of the membrane. All of these
components are connected in parallel and are driven by a time-varying current source to ground. If a
time-varying input current is injected into the cell, it may add further charge on the capacitor, or the
added charge may leak through the channels in the cell membrane. Because of active ion transport
through the cell membrane, the ion concentration inside the cell is different from that in the extracellular
liquid. The potential generated by the difference in ion concentration is represented by a battery.Elementary circuit theory allows the construction of a set of differential equations relating thedifferent ion currents to the potential difference across the membrane. Using this set of differential
equations, certain essential features of neural behavior can be modeled. For example, assuming appro-synaptic weights PSCs decoding recurrent connections neuron soma dendrites input spikes h syn ( s ) h syn ( s ) recurrent matrix output spikes encoding dynamics matrices FIGURE 5.13A generic neural subsystem. The diagram depicts the mathematical analysis of a neural subsystem
and its mapping onto the biological systemÑa population of neurons. Labels outside the gray boxes indicate therelevant biological structures and processes. Neural action potentials (spikes) coming from a previous neuralpopulation generate weighted post-synaptic currents (PSCs) in the dendrites of the neurons to which they are
connected. The subsequent voltage changes travel to the neural somata, where action potentials are generated,resulting in output spikes. Because the input and output are neural spikes, this kind of subsystem can be linked toothers like it, permitting the construction of larger, more complex neural circuits (see Figure 5.15 for an example).
Note that labels inside the gray boxes are generated based on understanding of the purpose of the neural systembeing modeled and on current understanding of neural representation (encoding), computation (decoding), anddynamics (dynamics matrices and hsyn). Building simulations using these methods leads to a better understandingof how neural systems perform the complex functions they do. SOURCE: Courtesy of Chris Eliasmith, Universityof Waterloo.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY177priate parameter values, a constant input current larger than a certain critical value and turned on at agiven instant of time results in the potential difference across the membrane taking the form of a regular
spike trainÑwhich is reminiscent of how a real neuron fires. More realistic current inputs (e.g., stochas-
tic ones) result in a much more realistic-looking output.Despite lack of information about much of the cellular and molecular basis of neuronal excitation atthe time, Hodgkin and Huxley were able to provide a relatively accurate quantitative description of
how an action potential was generated by voltage-dependent ionic conductivities. The Hodgkin-Huxley
model provided the basis for research for more than five decades, spinning off a new field of neuro-
physiology: in large part, this field rests on the foundation created by their model. Recent research on
membrane ion channels can be related directly to the seminal ideas and (more importantly) precise
mechanism that their model described.The Òplain vanillaÓ Hodgkin-Huxley model is still interesting today. For example, a recent studydemonstrated previously unobserved dynamics in the Hodgkin-Huxley model, namely, the existence of
chaotic solutions in the model with its original parameters.103 The significance of chaos in this context isthat the excitability of a neural membrane with respect to firing is likely to be more complex than can be
explained by a simple sub- or super-threshold potential.Simulation and mathematical analysis of models have become essential tools in investigations ofthe complicated processes underlying rhythm generation in the nervous system. There are many types
of channels and synapses. The number of channels and synapses and their locations distinguish differ-
ent types of neurons from one another. Simulation of networks consisting of model neurons withFIGURE 5.14Rodent navigation. These figures depict the behavior of a neurally realistic simulation of the path
integrator in a rat. The simulation was generated by using a single (recurrent) generic neural subsystem. (A) Whenthe simulation is given random noise, it spontaneously generates a stable, localized bump of neural activity over the
neural sheet, which represents the ratÕs current location. This demonstrates that a stable attractor (a widely acceptedmodel of how the ratÕs path integrator is organized) has been implemented. (B) This model also implements control(i.e., updating of the current location based on the ratÕs motion) of the path integrator in a neurally plausible way.
Here, straight-line motion in a rightward direction is shown. (C) The model correctly integrates the circular path ofthe rat, demonstrating that it can path integrate in any direction that the rat might move. This simulation has verylittle error compared to the simulations of past models. SOURCE: Chris Eliasmith, University of Waterloo, personalcommunication, September 7, 2004, and A. Samsonovich and B.L. McNaughton, ÒPath Integration and CognitiveMapping in a Continuous Attractor Model,Ó Journal of Neuroscience 17(15):5900-5920, 1997.ABC
103J. Guckenheimer and R.A. Oliva, ÒChaos in the Hodgkin-Huxley Model,Ó SIAM Journal on Applied Dynamical Systems 1(1):105-114, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.178CATALYZING INQUIRYspecified conductances and synapses enables researchers to test their intuitions regarding how thesenetworks function. Simulations also lead to predictions of the effects of neuromodulators and disorders
that affect the electrical excitability of the systems. Nonetheless, simulation alone is not sufficient to
determine the information we would like to extract from these models. The models have large numbers
of parameters, many of which are difficult or impossible to measure, and the goal is to determine how
the system behavior depends on the values of all of these parameters.Dynamical systems theory provides a conceptual framework for characterizing rhythms. This theoryexplains why there are only a small number of dynamical mechanisms that initiate or terminate bursts
of action potentials, and it provides the foundations for algorithms that compute parameter space maps
delineating regions with different dynamical behaviors. The presence of multiple time scales is an
important ingredient of this analysis because the rates at which different families of channels respond to
changes in membrane potential or ligand concentration vary over several orders of magnitude.Figure 5.16 illustrates this type of analysis using a model for bursting in the pre-Bıtzinger complex,a neural network in the brain stem that controls respiration. The first panel shows voltage recordings
from intracellular recordings of a medullar slice from neonatal rats. Butera and colleagues measured
conductances in this preparation and constructed a model for this system.104 Simulations of the burst-a b c d f eg k x T R A=TR i h A* V j l m n R«A* A¥A *
=<V,A¥A*> FIGURE 5.15System for learning and performing deductive reasoning. The graphic describes the proposed sys-
tem used during solution of the Wason card selection task; see P.C. Wason and P.N. Johnson-Laird, Psychology ofReasoning: Structure and Content, Harvard University Press, Cambridge, MA, 1972. This task requires determiningwhen a logical rule is valid or invalid, and so is a form of deductive reasoning. Humans perform notoriously badlyon many versions of this task, but well on other versions. This kind of context/content sensitivity is captured bythis model; see C. Eliasmith, ÒLearning Context Sensitive Logical Inference in a Neurobiolobical Simulation,Ó pp.
17-19 in Compositional Connectionism in Cognitive Science: Papers from the AAAI Fall Symposium, S.D. Levy and R.Gayler, Program Co-chairs, October 21-24, 2004, The AAAI Press, Arlington, VA, Technical Report FS-04-03, 2004.The depicted large-scale circuit consists of 14 neural subsystems, distributed across frontal and ventral areas of the
brain. This is a good example of the degree of complexity that can be built into a neurally realistic simulation usingthese new techniques. Populations a-d learn and apply the appropriate context for interpretation of the rule (R)encoded by population e. Populations f and g apply the relevant transformation (T) to the rule, giving the currentanswer (A). Populations h, k, and l determine the degree of correctness or incorrectness of the suggested answer(either given the correct answer, or given a reward or punishment signal), resulting in an error signal e. Popula-tions m and n provide a guess at the best possible transformation. This guess and the error signal are integratedinto the learning algorithm. SOURCE: Courtesy of Chris Eliasmith, University of Waterloo.104R.J. Butera, Jr., J. Rinzel, and J.C. Smith, ÒModels of Respiratory Rhythm Generation in the Pre-Bıtzinger Complex. I.Bursting Pacemaker Neurons,Ó Journal of Neurophysiology 82(1):382-397, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY179Box 5.17Computational Perspectives on Dopamine Function in the Prefrontal CortexConnectionist Models of Dopamine NeuromodulationA long-held hypothesis suggests that catecholamine neurotransmitters, including dopamine (DA), modulate targetneuron responses, by increasing their signal-to-noise (SNR) ratio (i.e. by increasing the differentiation between back-ground or baseline firing rates and those that are evoked by afferent stimulation). For example, studies in the striatumshowed that DA potentiated the response of target neurons to the effect of both excitatory and inhibitory signals.
However, the precise biophysical mechanisms underlying these effects were not well understood. Moreover, theview that DA acts as a modulator in the pre-frontal cortex (PFC) has been controversial, because, for many years, DAapplication or stimulation of DA neurons reliably inhibited spontaneous PFC activity. Thus, many investigators
argued that DA served as an inhibitory transmitter in PFC.The first explicit computational models of the neuromodulatory function of catecholamines, and DA in particular,were developed within the connectionist framework, and focused on their effects on information processing. Al-though such models do not typically incorporate biophysical detail, by virtue of their simplicity they have theadvantage of simulating system level function and performance in a wide variety of cognitive tasks. Within this
framework, DA effects were simulated as a change in the slope (or gain) of the sigmoidally shaped input-outputactivation function of processing units. Thus, in the presence of DA, both the excitatory and inhibitory influences ofafferent inputs are potentiated. Computational analyses showed that this modulatory function would not improve the
SNR characteristics of single neurons, but could do so at the network level. Models implementing these ideas proveduseful for accounting for a wide range of phenomena, including the pharmacological effects of DA on performancein tasks thought to rely on PFC and the effects of disturbances of DA in schizophrenia.Biophysically Detailed ModelsIn recent work, computational studies have focused on more biophysically detailed accounts of DA action withinPFC. Models by Durstewitz et al. and Brunel and Wang, all include data on the different biophysical effects of DA on
specific cellular processes. These models have been used to simulate the dynamics of activity in networks thatclosely parallel the patterns observed in vivo within PFC. . . .These models synthesize the rapidly growing, but often confusing literature on the neurophysiology of DA withinPFC. For example, the biophysical effects of DA are shown to produce a suppressive influence on spontaneousactivity, explaining its apparent inhibitory actions, while at the same time causing an enhanced excitability in
response to afferent drive. Furthermore, the selective enhancement of inputs from recurrent versus external afferentsprovides a mechanism for stabilizing sustained activity patterns within PFC that are resistant to interference fromexternal inputs. These computational analyses support the characterization of DA as a modulatory neurotransmitter,
rather than a classical excitatory or inhibitory one, and explain its role in support sustained activity within PFC.Strikingly, these models are remarkably consistent with the original hypothesis that DA increases SNR within thePFC, and the expression of this idea in earlier connectionist models. The underlying assumption in both types ofmodels is that short-term storage of information in PFC occurs through recirculating activity within local recurrentnetworks, which can be described as fixed-point attractor systems. DA activity helps to stabilize attractor states, both
by making high activity states more stable (active maintenance), and low activity states (spontaneous backgroundactivity) less likely to spuriously transition to high activity states in the absence of strong afferent input. This isaccomplished by the concurrent potentiation of excitatory and inhibitory transmission, implemented as changes in
ion channel properties in biophysically detailed models and ÒsummarizedÓ as a change in the gain of the sigmoidalactivation function in connectionist models.These mechanisms can be used to simulate the effects of DA on performance in cognitive tasks that rely on PFCfunction. For example, in a task emphasizing the role of PFC in working memory, increased DA activation in theDurstewitz et al. model enhanced the stability of PFC working memory representations by making them less suscep-
tible to interference from the intervening distractors. Within connectionist models, similar effects have been demon-strated by changing the gain of the activation function, and simulating human performance in tasks known to rely onPFC, tasks similar to those simulated by Durstewitz et al. and Brunel and Wang.SOURCE: Reprinted by permission from J.D. Cohen, T.S. Braver, and J.W. Brown, ÒComputational Perspectives on Dopamine Function inPrefrontal Cortex,Ó Current Opinion in Neurobiology 12(2):223-229. Copyright 2002 Elsevier. (References omitted.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.180CATALYZING INQUIRYFIGURE 5.16Bursting in the pre-Botzinger complex.
Panel 1: Example of voltage-dependent properties of pre-Bıtzinger complex (pre-BıtC) inspiratory bursting neu-rons. Traces show whole-cell patch-clamp recordings from a single candidate pacemaker neuron in the pre-BıtC ofa 400-µm-thick neonatal rat transverse medullary slice with rhythmically active respiratory network. Recordingsin A and B were obtained respectively before and after block of synaptic transmission by low Ca2+ conditionsidentical to those described in Johnson et al. (1994) (i.e., 0.2 mM Ca2+, 4 mM Mg2+, 9 mM K+ in slice bathingsolution). Patch pipette solution and procedure for whole-cell recording were as described previously (Smith et al.
1991, 1992). Before block of synaptic transmission, the neuron bursts in synchrony with the inspiratory phase ofnetwork activity as monitored by the inspiratory discharge recorded on the hypoglossal (XII) nerve (Smith et al.1991). After block of synaptic activity (30 minutes under low-Ca2+ conditions), the cell exhibits intrinsic voltage-dependent oscillatory behavior. As the cell is depolarized by constant applied current, it undergoes a transitionfrom silence (baseline potential below 65 mV, left) to oscillatory bursting to beating (baseline potential above 45mV, right). In the bursting regime, the burst period and duration decreases (see expanded time-base traces in B) as
the baseline membrane potential is depolarized. SOURCE: Reprinted by permission from R.J. Butera, Jr., J. Rinzel,and J.C. Smith, ÒModels of Respiratory Rhythm Generation in the Pre-Bıtzinger Complex. I. Bursting PacemakerNeurons,Ó Journal of Neurophysiology 82(1):382-397, 1999. Copyright 1999 American Physiological Society.continuedPANEL 1Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY181PANEL 2FIGURE 5.16Continued
Panel 2: Gating and I-V characteristics of components of models 1 and 2. (A) spike-generating kinetics: m3(V) andh(V) of INa and n(V) and n(V) of IK; note that h=1− n. (B1) gating characteristics of INaP: m(V), h(V), and h(V)(bold); left: y-axis scale for steady-state gating functions; right: y-axis scale for h(V). (B2) I-V plots of INaP forh=h(V) and h=1. First case results in a small window current at subthreshold potentials; second case corre-sponds to INaP-h with complete removal of inactivation. (C1) gating characteristics of IKS: k(V) and k(V) (bold);left: y-axis scale for activation function; right: y-axis scale for k(V). (C2) I-V plots of INaP+IKS for k=k(V) andk=0. First case results in a small current at subthreshold potentials; second case corresponds to INaP with com-plete removal of the opposing IKS.SOURCE: Reprinted by permission from R.J. Butera, Jr., J. Rinzel, and J.C. Smith, ÒModels of Respiratory Rhythm
Generation in the Pre-Bıtzinger Complex. I. Bursting Pacemaker Neurons,Ó Journal of Neurophysiology 82(1):382-397, 1999. Copyright 1999 American Physiological Society.ing rhythms displayed by this model are shown in the second panel. The third panel shows a map of thesimulated trajectory that illustrates the relationship of the bursting to slow and fast variables in the
system.5.4.5.4Synaptic Transmission
The intercellular signaling process of synaptic transmission is a much-studied problem. Much hasbeen learned about synaptic structure and function through the classical techniques of neuropharma-
cology, electron microscopy (EM) neuroanatomy, and electrophysiology, and correlation of the obser-
vations made through these various techniques has led to the development of computational models of
synaptic microphysiology. However, the scope of previous modeling attempts has been limited by
available computing power, modeling framework, and lack of high-resolution three-dimensional ultra-
structural data in an appropriate machine representation.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.182CATALYZING INQUIRYPANEL 3FIGURE 5.16Continued
Panel 3: Projection of trajectory onto fixed points of fast subsystem. The axes are v: membrane potential; h: inacti-vation of the HH sodium channel (there is also a persistent sodium channel in the model); and n: activation of theHH Òdelayed rectifierÓ potassium channel. The voltage traces show the changes of voltage as a function of time.
The values of h and n also change with time. Think of v, n, h as the three coordinates of a point moving throughspace. This plot depicts the path taken by this point in a bursting oscillation of the model. The curves are states atwhich the motion through this space is particularly slow, becoming zero in the limit so that the slower currents in
the model are not allowed to change at all. SOURCE: Derived from Figure 4, Panel A3, in R.J. Butera Jr., J. Rinzel,and J.C. Smith, ÒModels of Respiratory Rhythm Generation in the Pre-Bıtzinger Complex. I. Bursting PacemakerNeurons,Ó Journal of Neurophysiology 82(1):382-397, 1999.  Copyright 1999 American Physiological Society. Used bypermission.What has been missing is an appropriate set of tools for acquiring, building, simulating, and analyz-ing biophysically realistic models of subcellular microdomains. Coggan et al. have developed and used
a suite of such computational tools to build a realistic computational model of nicotinic synaptic trans-
mission based on serial electron tomograms of a chick ciliary ganglion somatic spine mat.105The chick ciliary ganglion somatic spine mat is a complex system with more than one type ofneurotransmitter receptor, possible alternative locations for transmitter release, and a tortuous synaptic
geometry that includes a spine mat and calyx-type nerve terminal. Highly accurate models of the
synaptic ultrastructure are obtained through large-scale, high-resolution electron tomography; com-105J.S. Coggan, T.M. Bartol, E. Esquenazi, J.R. Stiles, S. Lamont, M.E. Martone, D.K. Berg, M.H. Ellisman, and T.J. Sejnowski,ÒEvidence for Ectopic Neurotransmission at a Neuronal Synapse,Ó Science 309(5733):446-451, 2005.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY183puter-aided methods for extracting accurate surfaces and defining in-silico representations of theirmolecular properties; and physiological underpinnings from a variety of studies conducted by the
involved laboratories and from the literature.These data are then used as the framework for advanced simulations using MCell running on high-performance supercomputers as well as distributed or grid-based computational resources. This project
pushes development of tools for acquisition of improved large-scale tomographic reconstructions of
cellular interfaces down to supramolecular scales. It also drives improvements in the software tools
both for the distribution of molecular components within the surface models extracted from the tomo-
graphic reconstructions and for the deposition and retrieval of relevant information for the MCell
simulator (Box 5.18) in the tomography and Cell-Centered Database (CCDB) environment.Realistic modeling of synaptic microphysiology (as illustrated in Figure 5.17) requires the following:1.Acquisition of high-resolution, three-dimensional synaptic ultrastructureÑthis is accomplished
with serial EM tomography.2.Segmentation of pre- and postsynaptic membrane from the tomographic volumeÑthis is accom-
plished using the tracing tool in Xvoxtrace.3.Three-dimensional reconstruction of the membrane surface topology to form a triangle meshÑ
this is accomplished using the marching cubes isosurface extraction tool in Xvoxtrace.4.Subdivision of the membrane surface meshes into physiologically relevant regions (e.g., spine
versus nonspine membrane and PSD [phosphorylation site domain] versus non-PSD regions)Ñthis is
accomplished using the mesh tagging tool in DReAMM.5.Placement of effector molecules (e.g., receptors, enzymes, reuptake transporters) onto membrane
surfaces with the desired distribution and densityÑthis is accomplished using the MCell model de-
scription language (MDL). Effector distribution and density may be determined by labeling and imag-
ing studies.6.Specification of the diffusion constant, quantity, and location of neurotransmitter releaseÑthis is
accomplished using MCell MDL.7.Specification of the reaction mechanisms and kinetic rate constants governing the mass action
kinetics interaction of neurotransmitter and effector moleculesÑthis is accomplished using MCell MDL.
8.Specification of what quantitative measures should be made during the simulationÑthis is ac-
complished using MCell MDL.9.Simulation of the defined systemÑthis is accomplished using the MCell compute kernel.
10.Analysis of the results at various points in the parameter space defined by the systemÑthis is
accomplished using analysis tools of the investigatorÕs discretion.Analysis of miniature excitatory postsynaptic currents (mEPSCs) recorded in electrophysiologicalexperiments shows that mEPSCs in the CG somatic spine mat occur in a broad spectrum of amplitudes,
rise times, and fall times. The differential kinetics and complementary distributions of 3 and 7nAChRs are expected to lead to mEPSCs whose characteristics are highly dependent on the location of
neurotransmitter release within the spine mat. Realistic simulation makes it possible to explore and
quantify the degree to which this hypothesis is true and to make quantitative comparisons of the
simulation and electrophysiological results. Figure 5.18 summarizes the results of simulations designed
to explore the limits of mEPSC behavior by virtue of the choice of neurotransmitter release locations.
The results not only confirm the qualitative expectations at each site but also predict their quantitative
behavior, allowing fine discriminations to be made.The process briefly outlined above represents a significant advance in the ability to create realisticcomputational models of subcellular microdomains from actual cellular ultrastructure. The preliminary
results presented are just the beginning of exciting computational experiments that can now be per-
formed on the CG model in an effort to illuminate and inform further bench experiments. Among all of
the things learned, perhaps the most important is which of the physical characteristics of the CG are theCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.184CATALYZING INQUIRYBox 5.18The MCell SimulatorMCell is a general Monte Carlo simulator of cellular microphysiology. MCell simulations provide insights intothe behavior and variability of real systems comprising finite numbers of molecules interacting in spatially
complex environments. MCell incorporates high-resolution physical structure into models of ligand diffusionand signaling, and thus can take into account the large complexity and diversity of neural tissue at the subcel-lular level.MCell is based on the use of rigorously validated Monte Carlo algorithms to track the evolution of biochemicalevents in time and three-dimensional space for individual ligand and effector molecules. That is, the Monte
Carlo approach is based on the use of random numbers and probabilities to effect the simulation of individualcases of the systemÕs behavior.In the MCell models used in neural signaling employing a Brownian dynamics random walk algorithm, indi-vidual ligand molecules move according to a three-dimensional Brownian dynamics random walk and en-counter membrane boundaries and effector molecules as they diffuse. Bulk solution rate constants are con-
verted into Monte Carlo probabilities so that the diffusing ligands can undergo stochastic chemical interactionswith individual binding sites such as receptor proteins, enzymes, and transporters. These interactions aregoverned by user-specified reaction mechanisms.The diffusion algorithms are grid-free, and the reaction algorithms are at the level of interactions betweenindividual molecules and thus do not involve solving systems of differential equations. Membrane boundaries
are represented as triangle meshes and may be of arbitrary complexity.The Monte Carlo approach has certain important advantages over the finite element (FE) approach often usedto include spatial information in kinetic modeling. The FE approach divides three-dimensional space into aregular grid of contiguous subcompartments, or voxels. It assumes well-mixed conditions within each voxeland uses differential equations to compute fluxes between, and reactions within, each voxel. Mass action
equations are based on continuum processes and predict average concentrations. In large, simple volumeswith great numbers of a few types of molecules (e.g., reactions in a test tube), fluctuations are relatively small,and knowledge of average concentrations accounts most of the interesting phenomena. However, synaptic
signaling is inherently discrete and stochastic because the number of molecules involved is small; hence, theFE method will fail to describe accurately the biochemistry of synaptic signaling because these methodsprovide only averaged data. Furthermore, complex cellular structuresÑsuch as the structures that characterize
the synapseÑrequire that the voxel grid be very fine and irregular in shape, making an FE approach bothcomputationally expensive and difficult to implement.MCell is very general because it includes a high-level model description language (MDL), which allows theuser to build subcellular structures and signaling pathways of virtually any configuration. MCellÕs algorithmsscale smoothly from typical workstations to shared-memory multiprocessor machines to massively parallel
supercomputers.SOURCE: For more information, see http://www.mcell.cnl.salk.edu; J.R. Stiles and T.M. Bartol, Jr., ÒMonte Carlo Methods for SimulatingRealistic Synaptic Microphysiology Using MCell,Ó pp. 87-127 in Computational Neuroscience: Realistic Modeling for Experimentalists, E. deSchutter, ed., CRC Press, Boca Raton, FL, 2000; J.R. Stiles, T.M. Bartol, Jr., E.E. Salpeter, M.M. Salpeter, and T.J. Sejnowski, ÒSynapticVariability: New Insights from Reconstructions and Monte Carlo Simulations with MCell,Ó pp. 681-731 in Synapses, W. Cowan, T.C. Sudhof,and C.F. Stevens, eds., Johns Hopkins University Press, Baltimore, MD, 2001. Discussion of the pros and cons of FE versus MC is from K.M.Franks and T.J. Sejnowski, ÒComplexity of Calcium Signaling in Synaptic Spines,Ó BioEssays 24(12):1130-1144, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY185FIGURE 5.17Constructing the geometry of a chick ciliary ganglion (CG) somatic spine mat model. A serial EM
tomogram of a CG spine mat was obtained at ~4 nm per voxel resolution. The serial tomogram encompassed avolume of ~27 mm3 (~3 µm × 3 µm × 3 µm).(A) A typical slice through the tomographic volume together with hand-traced contours of the pre- and postsynap-
tic membranes. Tracing and segmentation of presynaptic (cyan) and postsynaptic (red) membrane contours gener-ated using Xvoxtrace.(B) Three-dimensional reconstruction of pre- and postsynaptic membrane surfaces as triangle meshesÑview look-
ing down onto intracellular face of presynaptic membrane (visualized using DReAMM). The presynaptic mesh iscomposed of 100,000 triangles and the postsynaptic mesh is composed of 300,000 triangles.(C) Postsynaptic membrane surfaceÑview of extracellular face of membrane (presynaptic membrane invisible).
(D) Completed model including postsynaptic membrane subdivided into distinct spines, PSD areas (black regionswith yellow borders), receptor molecules (tiny blue particles on membrane surface), and several neurotransmitterrelease sites (red spheres). The membrane was subsequently populated with the desired distributions and densi-
ties of nicotinic acetylcholine receptor (nAChR) types and acetylcholine esterase (AChE) enzyme. Also visible in(D) are several acetylcholine (ACh) vesicular release sites whose locations are most clearly illustrated in Figure5.18A.
(E) Magnified view of the state of a simulation of synaptic transmission model as simulated by MCell. State ofsystem 300 ms after release of 5,000 molecules of acetylcholine (small green ellipsoids) is shown. 7 nAChR typesare shown in blue, and 3* nAChR types are shown in yellow (inactive receptors are semitransparent, and openreceptors are opaque).SOURCE: Courtesy of Tom Bartol, Salk Institute, San Diego, California.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.186CATALYZING INQUIRYFIGURE 5.18Summary of synaptic transmission simulations.
(A) Location of selected transmitter release sites and their associated simulated mEPSC traces, each decomposedinto their 3 and 7 nAChR components. Each trace is the average of 100 simulations using MCell. Site 1 is locatedat a PSD on nonspine membrane. This site is expected to have a large 3 response and a very small 7 response. Atthe other extreme of behavior, sites 3 and 5 are placed over non-PSD spine membrane. Rich in 7 receptors andpoor in 3 receptors, these sites are expected to have large 7 responses and minimal 3 responses. The other sitesare placed at locations expected to give rise to mEPSCs of mixed nAChR origin.
(B) mEPSC amplitudes (decomposed into their 3 and 7 nAChR components) at each of 550 distinct vesicularrelease sites. The mEPSC amplitudes are indicated by the diameter of the yellow spherical glyph and demonstratea strong dependence on location and underlying geometry.
SOURCE: Courtesy of Tom Bartol, Salk Institute, San Diego, CA.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY187least constrained, are least understood, and have the greatest impact on synaptic function. Specifically,the results clearly demonstrate that synaptic geometry, receptor distribution, and vesicle release loca-
tion each have a profound quantitative impact on the efficacy of the postsynaptic response. This means
that attention to accuracy in the model-building process must be a prime concern.5.4.5.5Neuropsychiatry
106The field of computational neuropsychiatry has been exploding with applications of large-deforma-tion brain mapping technology that provide mechanisms for discovering neuropsychiatric disorders of
many types. The hippocampus is a region of the brain (depicted in green in Figure 5.19) that has been
implicated in schizophrenia and other neurodegenerative diseases such as AlzheimerÕs. Using large-
deformation brain mapping tools in computational anatomy, researchers can define, visualize, and
measure the volume and shape of the hippocampus. These methods allow for precise assessment of
changes in hippocampal formation.Researchers at the Center for Imaging Science (CIS) used mapping tools to compare the left andright hippocampi (Figure 5.20) in 15 pairs of schizophrenic and control subjects. In the schizophrenicFIGURE 5.19The hippocampus in situ. SOURCE: Courtesy of Michael Miller, Johns Hopkins University.
106Section 5.4.5.5 is based on L. Wang, S.C. Joshi, M.I. Miller, and J.G. Csernansky, ÒStatistical Analysis of HippocampalAsymmetry in Schizophrenia,Ó Neuroimage 14(3):531-545, 2001; J.G. Csernansky, L. Wang, S. Joshi, J.P. Miller, M. Gado, D. Kido,D. McKeel, et al., ÒEarly DAT Is Distinguished from Aging by High-dimensional Mapping of the Hippocampus,Ó Neurology55(11):1636-1643, 2000.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.188CATALYZING INQUIRYsubjects, deformations were localized to hippocampal subregions that send projections to the prefrontalcortex. The deformations strongly distinguish schizophrenic subjects from control subjects. The pictures
indicate inward deformations by cooler colors, outward deformations by warmer colors, and little
deformation by a neutral green color. These results support the current hypothesis that schizophrenia
involves a disturbance of hippocampal-prefrontal connections.In a separate study, CIS researchers also compared asymmetry between the left and right hippoc-ampi. The left and the right side of normal brains develop at different rates. Structures on both sides of
the brain are similar, but not identical. This is normal brain asymmetry. If a different asymmetry pattern
exists in schizophrenic subjects, it may indicate a disturbance of the left-right balance during early
stages of brain development. Researchers found that the left hippocampus was narrower along the
outside edge than the right hippocampus. This asymmetry was similar in schizophrenic and normal
subjects (Figure 5.21, left image). However, further comparison revealed a significant difference in
asymmetry patterns of the hippocampal area called the subiculum (Figure 5.21, right image). People
with schizophrenia tend to have a more pronounced depression and a downward bend in the surface of
that structure.As part of Washington UniversityÕs Healthy Aging and Senile Dementia (HASD) program, CISresearchers have also applied brain mapping tools to assess the structure of the hippocampus in older
human subjects (depicted in Figure 5.22). They compared measurements of hippocampal volume and
shape in 18 subjects with early dementia of the Alzheimer type (DAT) with 18 healthy elderly and 15
younger control subjects. Hippocampal volume loss and shape deformities observed in subjects with
DAT distinguished them from both elderly and younger control subjects. The pattern of hippocampalFIGURE 5.20Left and right hippocampuses. SOURCE: Courtesy of Michael Miller, Johns Hopkins University.
Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY189deformities in subjects with DAT was largely symmetric and suggested damage to the CA1 hippocam-pal subfield.Hippocampal shape changes were also observed in healthy elderly subjects, which distinguishedthem from healthy younger subjects. These shape changes occurred in a pattern distinct from the
pattern seen in DAT and were not associated with substantial volume loss. These assessments indicate
that hippocampal volume and shape derived from computational anatomy large deformation brain
mapping tools may be useful in distinguishing early DAT from healthy aging.5.4.6Virology
Mathematical and computational methods are increasingly important to virology. For example, aprimary and surprising phenomenological aspect of HIV infection is that progression to AIDS usuallyFIGURE 5.21Asymmetry in schizophrenia. SOURCE: Michael Miller, Johns Hopkins University.
FIGURE 5.22Hippocampal structure in normal aging (left) versus in AlzheimerÕs disease patients (right). SOURCE:
Courtesy of Michael Miller, Johns Hopkins University.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.190CATALYZING INQUIRYBox 5.19Modeling the In Vivo Dynamics of HIV-1 InfectionMathematical models of HIV infection and treatment have provided quantitative insights into the major bio-logical processes that underlie HIV pathogenesis and helped establish the treatment of patients with combina-tion therapy. This in turn has changed HIV from a fatal disease to a treatable one. The models successfully
describe the changes in viral load in patients under therapy and have yielded estimates of how rapidly HIV isproduced and cleared in vivo, how long HIV-infected cells survive while producing HIV, and how fast HIVmutates and evolves drug resistance. They have also provided clues into the process of T-cell depletion that
characterizes AIDS. The models have also provided means to rapidly screen antiviral drug candidates forpotency in vivo, thus hastening the introduction of new antiretroviral therapies.On average, HIV takes about 10 years to advance from initial infection to immune dysfunction (or AIDS).During this period the amount of virus measured in a personÕs blood hardly changes. Because of this slowprogression and the unchanging level of virus it was initially thought that this infection was slow and it was
unclear whether treating this disease early, when symptoms were not apparent, was worthwhile.Recognizing that constant levels of virus meant only that the rates of viral production and clearance were inbalance, but not necessarily slow, Perelson and David Ho from Rockefeller University used experimental drugtherapy to ÒperturbÓ the viral steady state. Mathematically modeling the response to this perturbation using asystem of ordinary differential equations that kept track of the concentrations of infected cells and HIV, and
fitting the experimental data to the model, revealed a plethora of new features of HIV infection.Figure 5.19.1 shows that after therapy is initiated at time 0, levels of HIV RNA (a surrogate for virus) fall ten-to a hundredfold in the first week or two of therapy. This suggested that HIV has a half-life (t1/2) of 1-2 days,and thus maintaining the pre-therapy constant level of virus requires enormous virus productionÑin fact, theamount of virus in the body must double every 1-2 days.Detailed analysis showed that this viral decay was governed by two processes, clearance of free virus particles(t1/2 < 6 hours) and loss of productively infected cells (t1/2 < 1.6 days). From this rapid clearance of virus onecould compute that at steady state, ~1010 virions are produced daily and given the mutation rate of HIV, thateach single and most double mutations of the HIV genome are produced daily. Thus, effective drug therapyFIGURE 5.19.1Model predictions (lines) of the biphasic decay of HIV viral load compared with typical patient data
(symbols). SOURCE: Courtesy of A.S. Perelson, Los Alamos National Laboratory.continuedDaysHIV RNA Copies (per ml)104103105106DaysHIV RNA Copies (per ml)1041031051061022030400010
12345678Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY191takes a very long time, and individuals who have not progressed to full-blown AIDS are asymptomatic.As Box 5.19 suggests, computational models have been able to shed considerable light on this phenom-

enology, and these insights have altered the view of AIDS from a static picture in which the virus is
essentially dormant and does not do very much for a long time to a much more dynamic picture of a rough
balance between the virus and the immune system, both working very hard, for that period of time. These
findings have had tangible impact, because they have affected drug treatment regimes considerably.More specifically, the average rate of HIV production in the human body is on the order of 1010copies per day as noted in Box 5.19. Empirical data indicate that errors in HIV replication occur at a rate
on the order of 10Ð4 to 10Ð5 per base per generation, and since the HIV genome is 10,000 base pairs long,the likelihood that a replicated genome will contain at least one error is 10 percent to nearly unity (and
the vast majority of these errors are errors in a single base). Because there are only four possible bases in
DNA (and hence each base can change into only one of three other bases), there are only 30,000 possible
single-base mutations of a given genome. An error rate of 10Ð4 to 10Ð5 per base per generation distrib-uted among 1010 copies each with 104 bases means that each generation produces 109 to 1010 mutations,which are distributed over the set of 30,000 possible mutations. Put differently, every new day brings to
life on the order of 105 instances of every possible single-base variant of HIV.Thus, a drug known to bind to a particular sequence of amino acids at a certain location in a proteintoday will face 105 to 106 new variants tomorrow against which its effectiveness will be questionable.This fact suggests that drug treatment regimes must target multiple binding sites, and hence combina-
tion drug therapy is likely to be more effective because drug-resistant variants must then be the result of
multiple errors in the replication process (which occur much less frequently). This in fact reflects recent
experience with combination drug regimes.1075.4.7Epidemiology
Epidemiology is the study of the dynamics of disease in a population of individuals. Of particularinterest is the epidemiology of infectious diseases, which arise from contact between an environmentalBox 5.19 Continuedwould require drug combinations that can sustain at least three mutations before resistance arises, and thisengendered the idea of triple combination therapy. Other analyses showed that the slope of viral decay was
proportional to the drug combinationsÕ antiviral efficacy, providing a means of comparing therapies.Following the rapid 1-2 week Òfirst phaseÓ loss, the rate of HIV RNA decline slows. Models of this ÒsecondphaseÓ of decline, when fitted to the kinetic data, suggested that a small fraction of infected cells might live aperiod of weeks while infected (t1/2 ~ 14 days).Following upon the success of these joint modeling and experimental efforts, many similar studies wereundertaken and revealed a fourth, much longer time-scale for the decay of latently infected cells of 6-44months. Latently infected cells, which harbor the HIV genome but do not produce virus, can hide from the
immune system and reignite infection when the cells are stimulated into proliferation. Clearing latently infect-ed cells is one of the last remaining obstacles to eradicating HIV from the body.107For further discussion, see A.G. Rodrigo, ÒHIV Evolutionary Genetics,Ó Proceedings of the National Academy of Sciences96(19):10559-10561, 1999; B.A. Cipra, ÒWill Viruses Succumb to Mathematical Models?Ó SIAM News 32(2), 1999, available athttp://www.siam.org/siamnews/03-99/viruses.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.192CATALYZING INQUIRYagent and an individual (e.g., an insect that bites an individual) or between individuals (e.g., an indi-vidual who sneezes in a room filled with people) that leads to the transmission of disease. The dynamics
of infectious diseases depend on many things, such as the likelihood of transmission between carrier
agent and infected individual given that contact has been made, the geographical distribution of carrier
agents and individuals, and the susceptibility of individuals to the disease.A central problem in epidemiology is how the dynamics of disease play out across geographicalspace.108 Problems of spatial heterogeneity play out at many different levels of aggregation: individu-als, families, work groups and firms, neighborhood, and cities. Box 5.20 provides an example taken
from the study of measles.At the same time, spatial heterogeneity is not the only inhomogeneity of interest. For example, theepidemiology of sexually transmitted diseases (STDs) cannot be separated from a consideration of their
dynamics in different social groups. For example, patterns of STDs in prostitutes and intravenous drugBox 5.20Spatial Heterogeneity in Epidemiology: An ExampleOne of the best illustrations of [the significance of spatial heterogeneity] is provided by the highly dynamic spa-tiotemporal epidemic pattern of measles. An important set of analyses of simple, homogeneous models predicted thepossibility of chaotic dynamics; however, the resulting large-amplitude [predicted] epidemics generate unrealistical-
ly low persistence of infection in small communities. Adding successive layers of social and geographical spaceÑand moving from deterministic to stochastic modelsÑimproves spatial realism and may reduce the propensity forchaos.The major computational challenge in these highly nonlinear stochastic systems is to represent hierarchical spatialcomplexity and especially its impact on vaccination strategies. Depending on the problem, all scalesÑfrom the
individual level to big citiesÑmay be important, both in terms of social space [family and school infection dynamics]and in terms of geographic spread and coherency.. . . [A] central question is: How spatially aggregated and parsimonious a model can provide useful results in a givencontext? This is particularly important in comparisons between directly transmitted human infectionsÑwhere long-range movements may bring infection dynamics comparatively close to mean field behavior (in which every individ-
ual is assumed to have equal contact with every other individual, thus experiencing the mean or average field)Ñandthe equivalent infections in natural populations, where more restricted movements and host population dynamicsadd extra complexities.It is risky to model at a given level of detail without having data at the relevant spatial grain. Notifiable infectiousdiseases are unusually well [documented], with large and often as yet uncomputerized spatiotemporal data sets.
These data provide a huge potential testbed for developing methods for characterizing spatiotemporal dynamics innonlinear, nonstationary stochastic systems. An encouraging development is that the current, generally nonparamet-ric, approaches to characterizing chaos and other nonlinear behaviors are increasingly incorporating lessons from
mechanistic epidemiological models.SOURCE: Reprinted by permission from S.A. Levin, B. Grenfell, A. Hastings, and A.S. Perelson, ÒMathematical and ComputationalChallenges in Population Biology and Ecosystems Science,Ó Science 275(5298):334-343, 1997. Copyright 1997 AAAS. (References omitted.)108K. Dietz, ÒThe Estimation of the Basic Reproduction Number for Infectious Diseases,Ó Statistical Methods in Medical Research2(1):23-41, 1993; A.D. Cliff and P. Haggett, Atlas of Disease Distributions: Analytic Approaches to Epidemiologic Data, Blackwell LTD,Oxford, UK, 1988; D. Mollison and S.A. Levin, ÒSpatial Dynamics of Parasitism,Ó pp. 384-398 in Ecology of Infectious Diseases inNatural Populations, B.T. Greenfell and A.P. Dobson, eds., Cambridge University, Cambridge, UK, 1995.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY193users exhibit different dynamical patterns than those in the general population because of factors suchas rates of sexual contact with others (both inside and outside the individualÕs own social group) and
different sexual practices of individuals in each group (e.g., use of condoms). Box 5.21 elaborates on this
notion in greater detail.5.4.8Evolution and Ecology
5.4.8.1Commonalities Between Evolution and Ecology
No two fields in biology encompass such a broad range of levels of biological organization asecology and evolutionary biology. Although the two fields ask different questions, they both contend
with factors of space and time, and share common theories about relationships between individuals,
populations, and communities. The two intertwined fields view these relationships in different ways.
Evolutionary biologists want to understand and quantify the effect of environment (e.g., natural selec-
tion) on individuals and populations; ecologists want to understanding the role of individuals and
populations in shaping their environment (ecological inheritance, niche construction).The two fields encompass a diverse assemblage of topics with applications in resource manage-ment, epidemiology, and global change. In these fields, data have been relatively difficult to collect in
ways that relate directly to mathematical or computational models, although this has been changing
over the past 10 years. Thus, both fields have relied heavily on theory to advance their insights. In fact,
ecology and evolution have been the substrate for the development of important mathematical con-
cepts. The quantitative study of biological inheritance and evolution provided the context for statistics,
probability theory, stochasticity, and dynamical systems theory.Box 5.21Social Heterogeneity in Epidemiology: An ExampleThe main focus for modeling social space (the space of social interactions) and disease is, of course, on AIDS andother sexually transmitted infections. Simple models illustrated clearly that heterogeneities in contact rates cansubstantially alter the predicted course of epidemics. This area has seen an explosion of research, both in data
analysis of contact structures and in graph-theoretic and other approaches to modeling. Models and data analysis aremost productive when combined, especially in allowing the observations to limit the universe of possible networks.The major computational challenge is how to deal with the complexity of networks, where concurrency of partner-ships often means that closure to a few moments of the distribution is difficult. This problem is especially acute giventhe sensitivity of obtaining data for STD networks, in that the nature of the network is generally only partially and
imperfectly known. The use of mathematical models for human immunodeficiency virus (HIV) transmission will beespecially important in assessing the impact of potential vaccines. Another major computational challengeÑwhichdeveloped with the AIDS epidemic and is currently being applied to another pathogen, the bovine spongiform
encephalopathy agentÑis to estimate the parameters of transmission models from disease incidence and otherdemographic data.One hope for the future for both of these areas is network information embedded in viral genomes. A body of recentwork indicates exciting possibilities for estimating epidemiological parameters from the birth and death processes ofpathogen evolutionary trees. More generally, new mathematical and computational techniques will be needed to
understand the epidemiological implications of the rapidly accumulating data on pathogen sequences, especially inthe context of parasite genetic diversity and the host immunological response to it.SOURCE: Reprinted by permission from S.A. Levin, B. Grenfell, A. Hastings, and A.S. Perelson, ÒMathematical and ComputationalChallenges in Population Biology and Ecosystems Science,Ó Science 275(5298):334-343,1997. Copyright 1997 AAAS. (References omitted.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.194CATALYZING INQUIRYAmong the fundamental questions in the study of evolution are those that seek to know the relativestrengths of natural selection, genetic drift, dispersal processes, and genetic recombination in shaping
the genome of a populationÑessentially the forces that provide genetic variability in a species. Both
ecologists and evolutionary biologists want to know how these forces lead to morphological changes,
speciation, and ultimately, survival over time. The fields seek theory, models, and data that can account
for genetic changes over time in large heterogeneous populations in which genetic information is
exchanged routinely in an environment that also exerts its influence and changes over time.In addition to interest in genetic variability and fitness within a single species, the two fields areinterested in relationships between multiple species. In ecology, this manifests itself in questions of how
the individual forces of variability within and between species affect their relative ability to compete for
resources and space that leads to their survival or extinctionÑin other words, forces that determines the
biodiversity of an ecosystem (i.e., a set of biological organisms interacting among themselves and their
environment). Ecologists want to understand what determines the minimum viable population size for
a given population, the role of keystone species in determining the diversity of the ecosystem, and the
role of diversity in preservation of the ecosystem.For evolutionary biologists, questions regarding relationships between species focus on trying tounderstand the flow of genetic information over long periods of time as a measure of the relatedness of
different species and the effects of selection on the genetic contribution to phenotypes. Among the great
mysteries for evolutionary biologists is whether and how evolution relates to organismal development,
an interaction for which no descriptive language currently exists.How will ecologists and evolutionary biologists answer these questions?  These fields have had few
tools to monitor interactions in real time. But new opportunities have emerged in areas from genomics
to satellite imaging and in new capabilities for the computer simulation of complex models.5.4.8.2Examples from Evolution
A plethora of genomic data is beginning to help untangle the relationship between traits, genes,developmental processes, and environments. The data will serve as the substrate from which new
statistical conclusions can be drawn, for example, new methods for identifying inherited gene se-
quences such as those related to disease. To answer question about the process of genome rearrange-
ment, the possibility of comparing gene sequences from multiple organisms provides the basis for
testing tools that discern repeatable patterns and elucidate linkages.As more detailed DNA and protein sequence information is compiled for more genes in moreorganisms, computational algorithms for estimating parameters of evolution have become extremely
complex. New techniques will be needed to handle the likelihood functions and produce satisfactory
statistics in a reasonable amount of time. Studies of the role of environmental and genetic plasticity in
trait development will involve large-scale simulations of networks of linked genes and their interacting
products. Such simulations may well suggest new approaches to such old problems as the nature-
nurture dichotomy for human behaviors.New techniques and the availability of more powerful computers have also led to the developmentof highly detailed models in which a wide variety of components and mechanisms can be incorporated.
Among these are individual unit models that attempt to follow every individual in a population over
time, thereby providing insight into dynamical behavior (Box 5.22).Levin argues that such models are Òimitation[s] of reality that represent at best individual realiza-tion of complex processes in which stochasticity, contingency, and nonlinearity underlie a diversity of
possible outcomes.Ó109 From the collective behaviors of individual units arise the observable dynamics109S.A. Levin, B. Grenfell, A. Hastings, and A.S. Perelson, ÒMathematical and Computational Challenges in Population Biologyand Ecosystems Science,Ó Science 275(5298):334-343, 1997.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY195of the system. ÒThe challenge, then, is to develop mechanistic models that begin from what is under-stood about the interactions of the individual units, and to use computation and analysis to explain
emergent behavior in terms of the statistical mechanics of ensembles of such units.Ó Such models must
extrapolate from the effects of change on individual plants and animals to changes in the distribution of
individuals over longer time scales and broader space scales and hence in community-level patterns
and the fluxes of nutrients.5.4.8.2.1Reconstruction of the 
Saccharomyces Phylogenetic TreeAlthough the basic structure andmechanisms underlying evolution and genetics are known in principle, there are many complexities
that force researchers into computational approaches in order to gain insight. Box 5.23 addresses com-
plexities such as multiple loci, spatial factors, and the role of frequency dependence in evolution, and
discusses a computational perspective on the evolution of altruism, a behavioral characteristic that is
counterintuitive in the context of individual organisms doing all that they can to gain advantage in the
face of selection pressures.Box 5.22The Dynamics of EvolutionAvida is a simulation software system developed at the Digital Life Laboratory at the California Institute ofTechnology.1 In it, digital organisms have genomes comprised of a sequence of instructions that operate on avirtual machine. These instructions include the ability to perform simple mathematical operations, copy val-
ues from memory location to memory location, provide input and output, and check conditions. Through asequence of instructions, these organisms can copy their genome, thereby reproducing asexually. Since thesoftware can simulate many hundreds of thousands of generations of evolution for thousands of organisms,
their digital evolution not only can be observed in reasonable lengths of time, but also can be preciselyinspected (since there are no inconvenient gaps in the fossil record). Moreover, alternate scenarios can beexplored by going back into evolutionary history and reversing the effects of mutations, for example. At a
minimum, this can be seen as experiment by analogy, revealing potential avenues for investigation or hypoth-eses to test in actual biological evolution. A stronger argument holds that evolution is an abstract mathemat-ical process and will operate under similar dynamics whether embodied in DNA in the physical world or in
digital simulations of it.Avida has been used to explore how complex features can arise through mutation, competition, and selectivepressure.2 In a series of experiments, organisms were provided with a limited supply of energy units necessaryfor the execution of their genome of instructions. However, organisms that performed any of a set of complexlogical operations were rewarded with an increased allowance and thus increased opportunities to reproduce.
More complicated logical operations provided proportionally greater rewards.The experiment was seeded with an ancestral form that could perform none of those operations, containingonly the instructions to reproduce. Mutation arose through imperfect copying of the genome during reproduc-tion. EQU, the most complex logical operation checked for [representing the logical statement (A and B) or(~A and ~B)], arose in 23 out of 50 populations studied where the simpler operations also provided rewards.
The sequence of instructions that evolved to perform the operation varied widely in length and implementa-tion. However, in other simulations where only EQU was rewarded, no lineages ever evolved it. This evi-dence agrees with the standard theory of biological evolutionÑstated as early as DarwinÑthat complex
structures arise through the combination and modification of useful intermediate forms.1C. Adami, Introduction to Artificial Life, Springer-Verlag, New York, 1998.2R.E. Lenski, C. Ofria, R.T. Pennock, and C. Adami, ÒThe Evolutionary Origin of Complex Features,Ó Nature 423:139-144, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.196CATALYZING INQUIRYBox 5.23Genetic Complexities in Evolutionary ProcessesThe dynamics of alleles at single loci are well understood, but the dynamics of alleles at two loci are still notcompletely understood, even in the deterministic case. As a rule, two-locus models require the use of a variety
of computational approaches, from straightforward simulation to more complex analyses based on optimiza-tion or the use of computer algebra systems. Three-locus models can be understood only through numericalapproaches, except for some very special cases.Compare these analytical capabilities to the fact that the number of loci exhibiting genetic variation in popu-lations of higher organisms is well into the thousands. Thus, the number of possible genotypes can be much
larger than the population. In such a situation, the detailed population simulation (i.e., a detailed consider-ation of events at each locus) leads to problems of substantial computational difficulty.An alternative is to represent the population as phenotypesÑthat is, in terms of traits that can be directlyobserved and described. For example, certain traits of individuals are quantitative in the sense that theyrepresent the sum of multiple small effects. Efforts have been undertaken to integrate statistical models of the
dynamics of quantitative traits with more mechanistic genetic approaches, though even under simplifyingassumptions concerning the relation between genotype and phenotype, further approximations are requiredto obtain a closed system of equations.Frequency dependence in evolution refers to the phenomenon in which the fitness of an individualdepends both on its own traits and on the traits of other individuals in the populationÑthat is, selection is
dependent on the frequency with which certain traits appear in the population, not just on pressures fromthe environment.This point arises most strongly in understanding how cooperation (altruism) can evolve through individualselection. The simplest model is the game of prisonerÕs dilemma, in which the game-theoretic solution for asingle encounter between parties is unconditional noncooperation. However, in the iterated prisonerÕs dilem-
ma, the game theoretic solution is a strategy known as Òtit-for-tat,Ó which begins with cooperation and thenuses the strategy employed by the other player in the previous interaction. (In other words, the iterated prison-erÕs dilemma stipulates repeated interactions over time between players.)Although the iterated prisonerÕs dilemma yields some insight into how cooperative behavior might emergeunder some circumstances, it is a highly and perhaps oversimplified model. Most importantly, it does not
account for possible spatial localizations of individualsÑa point that is important in light of the fact thatindividuals who are spatially separated have low probabilities of interacting. Because the evolution of traitsdependent on population frequency requires knowledge of which individuals are interacting, more realistic
models introduce some explicit spatial distribution of individualsÑand, for these, simulations are required todynamical understanding. These more realistic models suggest that spatial localization affects the evolution ofboth cooperative and antagonistic behaviors.SOURCE: Adapted from S.A. Levin, B. Grenfell, A. Hastings, and A.S. Perelson, ÒMathematical and Computational Challenges inPopulation Biology and Ecosystems Science,Ó Science 275(5298):334-343, 1997. (References in the original.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY197Along these lines, a particularly interesting work on the reconstruction of phylogenies was reportedin 2003 by Rokas et al.110 One of the primary goals of evolutionary research has been understanding thehistorical relationships between living organismsÑreconstruction of the phylogenetic tree of life. A
primary difficulty in phylogenetic reconstruction is that different single-gene datasets often result in
different and incongruent phylogenies. Such incongruences occur in analyses at all taxonomic levels,
from phylogenies of closely related species to relationships between major classes or phyla and higher
taxonomic groups.Many factors, both analytical and biological, may cause incongruence. To overcome the effect ofsome of these factors, analysis of concatenated datasets has been used. However, phylogenetic analyses
of different sets of concatenated genes do not always converge on the same tree, and some studies have
yielded results at odds with widely accepted phylogenies.Rokas et al. exploited genome sequence data for seven Saccharomyces species and for the outgroupfungus Candida albicans to construct a phylogenetic tree. Their results suggested that datasets consisting ofa single gene or a small number of concatenated genes had a significant probability of supporting conflict-
ing topologies, but that use of the entire dataset of concatenated genes resulted in a single, fully resolved
phylogeny with the maximum likelihood. In addition, all alternative topologies resulting from single-gene
analyses were rejected with high probability. In other words, even though the individual genes examined
supported alternative trees, the concatenated data exclusively supported a single tree. They concluded
that Òthe maximum support for a single topology regardless of method of analysis is strongly suggestive
of the power of large data sets in overcoming the incongruence present in single-gene analyses.Ó5.4.8.2.2Modeling of Myxomatosis Evolution in Australia
Evolution also provides a superb and easy-to-understand example of time scales in biological phenomena. Around 1860, a nonindigenous rabbit
was introduced into Australia as part of British colonization of that continent. Since this rabbit had no
indigenous foe, it proliferated wildly in a short amount of time (about 20 years). Early in the 1950s
Australian authorities introduced a particular strain of virus that was deadly to the rabbit.The data indicated that in the short term (say, on a time scale of a few months), the mostvirulent strains of the virus were dominant (i.e., the virus had a lethality of 99.8 percent). This is not
surprising, in the sense that one might expect virulence to be a measure of viral fitness. However, in
the longer term (on a scale of decades), similar measurements indicate that these more virulent
strains were no longer dominant, and the dominant niche was occupied by less virulent strains
(lethality of 90 percent or less). The evolutionary explanation for this latter phenomenon is that an
excessively virulent virus would run the risk of killing off its hosts at too rapid a rate, thereby
jeopardizing its own survival. The underlying mechanism responsible for this counterintuitive
phenomenon is that transmission of the virus depended on mosquitoes feeding from live rabbits.
Rabbits that were infected with the more virulent variant died quickly, and thus, fewer were avail-

able as sources of that variant.The above system was modeled in closed form based on a set of coupled differential equations; thismodel was successful in reproducing the essential qualitative features described above.111 In 1990, thismodel was extended by Dwyer et al. to incorporate more biologically plausible features.112 For ex-ample, the evolution of rabbit and virus reacting to each other was modeled explicitly. A multiplicity of110A. Rokas, B.L. Williams, N. King, and S.B. Carroll, ÒGenome-scale Approaches to Resolving Incongruence in MolecularPhylogenies,Ó Nature 425(6960):798-804, 2003.111S. Levin and D. Pimentel, ÒSelection of Intermediate Rates of Increase in Parasite-Host Systems,Ó The American Naturalist117(3), 1981.112G. Dwyer, S.A. Levin, and L.A. Buttel, ÒA Simulation Model of the Population Dynamics and Evolution of Myxomatosis,ÓEcological Monographs 60(4):423-447, 1990.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.198CATALYZING INQUIRYvirus vectors was modeled, each with different transmission efficiencies, rather than assuming a singlevector. The inclusion of such features, coupled with exploitation of a wealth of data available on this

system, allowed Dwyer et al. to investigate questions that could not be addressed in the earlier model.
These questions included whether the system will continue to evolve antagonistically and whether the
virus will be able to control the rabbit population in the future.More broadly, this example illustrates the important lesson that both time scales are equally signifi-cant from an evolutionary perspective, and one is not more ÒfundamentalÓ than the other when it comes
to understanding the dynamical behavior of the system. Furthermore, it demonstrates that pressures for
natural selection can operate at many different levels of complexity.5.4.8.2.3The Evolution of Proteins 
By making use of simple physical models of proteins, it is possibleto model evolution under different evolutionary, structural, and functional scenarios. For example,
cubic lattice models of proteins can be used to model enzyme evolution involving binding to two
hydrophobic substrates. Gene duplication coupled to subfunctionalization can be used to predict en-
zyme gene duplicate retention patterns and compare with genomic data.113 This type of physical mod-eling can be expanded to other evolutionary models, including those that incorporate positive selective
pressures or that vary population genetic parameters. At a structural level, they can be used to address
issues of protein surface-area-to-volume ratios or the evolvability of different folds. Ultimately, such
models can be extended to real protein shapes and can be correlated to the evolution of different folds
in real genomes.114The role of structure in evolution during potentially adaptive periods can also be analyzed. Asubset of positive selection will be dictated by structural parameters and intramolecular coevolution.
Common interactions, like RKDE ionic interactions can be detected in this manner. Similarly, less
common interactions, like cation-p interactions, can also be detected and the interconversion between
different modes of interactions can be assessed statistically.One important tool underlying these efforts is the Adaptive Evolution Database (TAED), a phyloge-netically organized database that gathers information related to coding sequence evolution.115 Thisdatabase is designed to both provide high-quality gene families with multiple sequence alignments and
phylogenetic trees for chordates and embryophytes and to enable answers to the question, ÒWhat
makes each species unique at the molecular genomic level?ÓStarting with GenBank, genes have been grouped into families, and multiple sequence alignmentsand phylogenetic trees have been calculated. In addition to multiple sequence alignments and phyloge-
netic trees for all families of chordate and embryophyte sequences, TAED includes the ratio of
nonsynonymous to synonymous nucleotide substitution rates (Ka/Ks) for each branch of every phyloge-netic tree. This ratio, when significantly greater than 1, is an indicator of positive selection and poten-
tially a change of function of the encoded protein in closely related species, and has been useful in the
construction of phylogenetic trees with probabilistic reconstructed ancestral sequences calculated using
both parsimony and maximum likelihood approaches. With a mapping of gene tree to species tree, the
branches whose ratio is significantly greater than 1 are collated together in a phylogenetic context.113F.N. Braun and D.A. Liberles, ÒRetention of Enzyme Gene Duplicates by Subfunctionalization;Ó International Journal ofBiological Macromolecules 33(1-3):19-22, 2003.114H. Hegyi, J. Lin, D. Greenbaum, and M. Gerstein, ÒStructural Genomics Analysis: Characteristics of Atypical, Common, andHorizontally Transferred Folds,Ó Proteins 47(2):126-141, 2002.115D.A. Liberles, ÒEvaluation of Methods for Determination of a Reconstructed History of Gene Sequence Evolution.Ó Molecu-lar Biology and Evolution 18(11):2040-2047, 2001; D.A. Liberles, D.R. Schreiber, S. Govindarajan, S.G. Chamberlin, and S.A. Benner,ÒThe Adaptive Evolution Database (TAED),Ó Genome Biology 2(8):research0028.1-0028.6, 2001; C. Roth, M.J. Betts, P. Steffansson,G. S¾lensminde, and D.A. Liberles, ÒThe Adaptive Evolution Database (TAED): A Phylogeny-based Tool for ComparativeGenomics,Ó Nucleic Acids Research 33(Database issue):D495-D497, 2005.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY199116E.V. Koonin, N.D. Fedorova, J.D. Jackson, A.R. Jacobs, D.M. Krylov, K.S. Makarova, R. Mazumder, et al., ÒA Comprehen-sive Evolutionary Classification of Proteins Encoded in Complete Eukaryotic Genomes,Ó Genome Biology 5(2):R7, 2004. (Cited inRoth et al., ÒThe Adaptive Evolution Database,Ó 2005.)117R. Rossnes, ÒPhylogenetic Reconstruction of Ancestral Character States for Gene Expression and mRNA Splicing Data,ÓM.Sc. thesis, Universtiy of Bergen, Norway, 2004. (Cited in Roth et al., 2005.)118See, for example, G.F. Joyce, ÒThe Antiquity of RNA-based Evolution,Ó Nature 418(6894):214-221, 2002.119M. Eigen, ÒSelforganization of Matter and the Evolution of Biological Macromolecules,Ó Naturwissenschaften 58(10):465-523,1971.120P. SzabŠ, I Scheuring, T. Czaran, and E. Szathmary, ÒIn Silico Simulations Reveal That Replicators with Limited DispersalEvolve Towards Higher Efficiency and Fidelity,Ó Nature 420(6913):340-343, 2002. A very helpful commentary on this article canbe found in G.F. Joyce, ÒMolecular Evolution: Booting Up Life,Ó Nature 420(6894):278Ð279, 2002. The discussion in Section5.4.8.2.4 is based largely on this article.121W.K. Johnston, P.J. Unrau, M.S. Lawrence, M.E. Glasner, and D.P. Bartel, ÒRNA-catalyzed RNA Polymerization: Accurateand General RNA-Templated Primer Extension,Ó Science 292(5520):1319-1325, 2001.The TAED framework is expandable to incorporate other genomic-scale information in a phyloge-netic context. This is important because coding sequence evolution (e.g., as reflected in the Ka/Ks ratio)is only one part of the molecular evolution of genomes driving phenotypic divergence. Changes in gene
content116and phylogenetic reconstructions of changes in gene expression and alternative splicingdata117 can indicate where other significant lineage-specific changes have occurred. Altogether, phylo-genetic indexing of genomic data presents a powerful approach to understanding the evolution of
function in genomes.5.4.8.2.4The Emergence of Complex Genomes
How did life get started on Earth? Today, life is based onDNA genomes and protein enzymes. However, biological evidence exists to suggest that in a previous
era, life was based on RNA, in the sense that genetic information was contained in RNA sequences and
phenotypes were expressed as catalytic properties of RNA.118An interesting and profound issue is therefore to understand the transition from the RNA to theDNA world, one element of which is the fact that DNA genomes are complex structures. In 1971, Eigen
found an explicit relationship between the size of a stable genome and the error rate inherent in its
replication, specifically that the size of the genome was inversely proportional to the per-nucleotide
replication error rate.119 Thus, for a genome of length L to be reasonably stable over successive genera-tions, the maximum tolerable error rate in replication could be no more than 1/L per nucleotide.However, more precise replication mechanisms tend to be more complex. Given that the replication
mechanism must itself be represented in the genome, the puzzle is that a precise replication mecha-
nism is needed to maintain a complex genome, but a complex genome is required to encode such a
mechanism.The only possible answer to this puzzle is that complex genomes evolved from simpler ones. SzabŠet al. investigated this possibility through computer simulations.120 They constructed a population ofdigital genomes subject to evolutionary forces and found that under a certain set of circumstances, both
genome size and replication fidelity increased with the run time of the simulation. However, such
behavior was dependent on the existence of a sufficient amount of spatial isolation of the evolving
population. In the absence of separation (i.e., in the limit of very rapid diffusion of genomes across the
two-dimensional surface to which they were confined), genome complexity and replication fidelity
were both limited. However, if diffusion is slow (i.e., the characteristic time constant of diffusion is less
than the time scale of replication), both complexity and fidelity increase.In addition, Johnston et al. have synthesized in the laboratory a catalytic RNA molecule that con-tains about 200 nucleotides and synthesizes RNA molecules of up to 14 nucleotides, with an error rate
of about 3 percent per residue.121 This laboratory demonstration, coupled with the computationalfinding described above, suggest that a small RNA genome that operates as an RNA replicase withCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.200CATALYZING INQUIRYmodest efficiency and fidelity could evolve a succession of ever-larger genomes and ever-higher repli-cation efficiencies.5.4.8.3Examples from Ecology
122Simulation-based study of an ecosystem considers the dynamic behavior of systems of individualorganisms as they respond to each other and to environmental stimuli and pressures (e.g., climate) and
examines the behavior of the ecosystem in aggregate terms. However, no individual run of such a
simulation can be expected to predict the detailed behavior of each individual organism within an
ecosystem. Rather, the appropriate test of a simulationÕs fidelity is the extent to which it can, through a
process of judicious averaging of many runs, predict features that are associated with aggregation at
many levels of spatial and/or temporal detail. These more qualitative features provide the basis for
descriptions of ecosystem dynamics that are robust across a variety of dynamical scenarios that are
different at a detailed level and also provide high-level descriptions that can be more readily interpreted
by researchers.Because of the general applicability of the approach described above, simulations of dynamicalbehavior can be developed for aggregations of any organisms as long as they can be informed by
adequate understandings of individual-level behavior and the implications of such behavior for interac-
tions with other individuals and with the environment.Note also the key role played by ecosystem heterogeneity. Spatial heterogeneity is one obvious wayin which nonuniform distributions play a role. But in biodiversity, functional heterogeneity is also
important. In particular, essential ecosystem functions such as the maintenance of fluxes of certain
nutrients and pollutants, the mediation of climate and weather, and the stabilization of coastlines may
depend not on the behavior of all species within the ecosystem but rather on a limited subset of these
species. If biodiversity is to be maintained, the most fragile and functionally critical subsets species must
be identified and understood.The mathematical and computational challenges range from techniques for representing and ac-cessing datasets, to algorithms for simulation of large-scale spatially stochastic, multivariate systems, to
the development and analysis of simplified description. Novel data acquisition tools (e.g., a satellite-
based geographic information system that records changes for insertion in the simulations) would be
welcome in a field that is relatively data poor.5.4.8.3.1Impact of Spatial Distribution in Ecosystems
An important dimension of ecological environ-ments is how organisms interact with each other. One often-made computationally simple assumption is
that an organism is equally likely to interact with every other organism in the environment. Although this
is a pragmatic assumption, actual ecosystems are physical and organisms interact only with a very small
number of other organismsÑnamely, the ones that are nearby in a spatial sense. Moreover, localized
selectionÑin which a fitness evaluation is undertaken only under nearest neighborsÑis also operative.Introducing these notions increases the speciation rate tremendously, and the speculation is that ina nonlocalized environment, the pressures on the population tend toward population uniformityÑ
everything looks similar, because each entity faces selection pressure from every other entity. When
localization occurs, different species emerge in different spatial areas. Further, the individuals that are
evolving will start to look quite different from each other, even though they have (comparably) high122Section 5.4.8.3 is based largely on material taken from S.A. Levin, B. Grenfell, A. Hastings, and A.S. Perelson, ÒMathematicaland Computational Challenges in Population Biology and Ecosystems Science,Ó Science 275(5298):334-343, 1997.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY201fitness ratings. (This phenomenon is known as convergent evolution, in which a given environmentmight evolve several different species that are in some sense equally well adapted to that environment.)As an example of spatial localization, Kerr et al. developed a computational model to examine thebehavior of a community consisting of three strains of E. coli,123 based on a modification of the lattice-based simulation of Durrett and Levin.124 One of the strains carried a gene that created an antibioticcalled colicin. (The colicin-producing strain, C, was immune to the colicin it produced.) A second strain
was sensitive to colicin (S), while a third strain was resistant to colicin (R). Furthermore, the factors that
make the S strain sensitive also facilitate its consumption of certain nutrients, and the R strain is less able
to consume these nutrients. However, because the R strain does not have to produce colicin, it avoids a
metabolic cost incurred by the C strain. The result is that C bacteria kill S bacteria, S bacteria thrive
where R bacteria do not, and R bacteria thrive where C bacteria do not. The community thus satisfies a
Òrock-paper-scissorsÓ relationship.The intent of the simulation was to explore the spatial scale of ecological processes in a communityof these three strains. It was found found (and confirmed experimentally) that when dispersal and
interaction were local, patches of different strains formed, and these patches chased one another over
the latticeÑtype C patches encroached on S patches, S patches displaced R patches and R patche
invaded C patches. Within this mosaic of patches, the local gains made by any one type were soon
enjoyed by another type; hence the diversity of the system was maintained. However, dispersal and
interaction were no longer exclusively local (i.e., in the Òwell-mixedÓ case in which all three strains are
allowed to interact freely with each other): continual redistribution of C rapidly drove S extinct, and R
then came to dominate the entire community5.4.8.3.2Forest Dynamics
125To simulate the growth of northeastern forests, a stochastic and mecha-nistic model known as SORTIE has been developed to follow the fates of individual trees and theiroffspring. Based on species-specific information on growth rates, fecundity, mortality, and seed dis-
persal distances, as well as detailed, spatially explicit information about local light regimes, SORTIE
follows tens of thousands of trees to generate dynamic maps of distributions of nine dominant or
subdominant species of tree that look like real forests and match data observed in real forests at
appropriate levels of spatial resolution. SORTIE predicts realistic forest responses to disturbances (e.g.,
small circles within the forest boundaries within which all trees are destroyed), clear-cuts (i.e., large
disturbances), and increased tree mortality.SORTIE consists of two units that account for local light availability and species life history for eachof nine tree species. Local light availability refers to the availability of light at each individual tree. This
is a function of all of the neighboring trees that shade the tree in question. Information on the spatial
relations among these neighboring tree crowns is combined with the movement of the sun throughout
the growing season to determine the total, seasonally averaged light expressed as a percentage of full
sun. In other words, the growth of any given tree depends on the growth of all neighboring trees.The species life history (available for each of nine tree species) provides the relationship betweenradial growth rates as a function of its local light environment and is based on empirically estimated
life-history information. Radial growth predicts height growth, canopy width, and canopy depth in
accordance with estimated allometric relations. Fecundity is estimated as an increasing power function
of tree size, and seeds are dispersed stochastically according to a relation whereby the probability of123B. Kerr, M.A. Riley, M.W. Feldman, and B.J. Bohannan, ÒLocal Dispersal Promotes Biodiversity in a Real-life Game of Rock-Paper-Scissors,Ó Nature 418(6894):171-174, 2002.124R. Durrett and S. Levin, ÒAllelopathy in Spatially Distributed Populations,Ó Journal of Theoretical Biology 185(2):165-171, 1997.125Section 5.4.8.3.2 is based largely on D.H. Deutschman, S.A. Levin, C. Devine, and L.A. Buttel, ÒScaling from Trees to Forests:Analysis of a Complex Simulation Model,Ó Science Online supplement to Science 277(5332), 1997, available at http://www.sciencemag.org/content/vol277/issue5332. Science Online article available at http://www.sciencemag.org/feature/data/deutschman/home.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.202CATALYZING INQUIRYdispersal declines with distance. Mortality risk is also stochastic and has two elements: random mortal-ity and mortality associated with suppressed growth.Because SORTIE is intended to aggregate statistical properties of forests, an ensemble of simulation runsis necessary, in which different degrees of smoothing and aggregation are used to determine how much
information is lost by averaging and to find out where error is compressed and where it is enlarged in the
course of this process. SORTIE is a computation-intensive simulation even for individual simulations, be-
cause multiple runs are needed to generate the necessary ensembles for statistical analysis. In addition,
simulations carried out for heterogeneous environments require an interface between large dynamic simula-
tions and geographic information systems, providing real-time feedbacks between the two.5.5TECHNICAL CHALLENGES RELATED TO MODELING
A number of obstacles and difficulties must be overcome if modeling is to be made useful to lifescientists more broadly than is the case today. The development of a sophisticated computational model
requires both a conceptual foundation and implementation. Challenges related to conceptual founda-
tions can be regarded as mathematical and analytical; challenges related to implementation can be
regarded as computational or, more precisely, as related to computer science (Box 5.24).TodayÕs mathematical tools for modeling are limited. Nonlinear dynamics and bifurcation theoryprovide some of the most well-developed applied mathematical techniques and offer great successes in
illuminating simple nonlinear systems of differential equations. But they are inadequate in many situa-
tions, as illustrated by the fact that understanding global stability in systems larger than four equations
is prohibitively hard, if not unrealistic. Visualization of high-dimensional dynamics is still problematic
in computational as well as analytical frameworks; the question remains as to how to represent such
complex dynamics in the best, most easily understood ways. Moreover, many high-dimensional sys-
tems have effectively low-dimensional dynamics. A challenge is to extract the dynamical behavior from
the equations without first knowing what the low-dimensional subspace is. Box 5.25 describes one new
and promising approach to dealing with high-dimensional multiscale problems.Other mathematical methods and new theory will be needed to find solutions that apply not only tobiological problems, but to other scientific and engineering applications as well. These include methods
for global optimization and for reverse engineering of structure (of any Òblack box,Ó be it a network of
genes, a signal transduction pathway, or a neuronal system) based on data elicited in response to
stimuli and perturbations.Identification of model structure and parameters in nonlinear systems is also nontrivial. This isespecially true in biological systems due to incomplete knowledge and essentially limitless types of
interactions. Decomposition of complex systems into simpler subsystems (ÒmodulesÓ) is an important
challenge to our ability to analyze and understand such systems (a point discussed in Chapter 6).
Development of frameworks to incorporate moving boundaries and changing geometries or shapes is
essential to describing biological systems. This is traditionally a difficult area. Ideally, it would be
desirable to be able to synthesize and analyze models that have nonlinear deterministic as well as
stochastic elements, and continuous as well as discrete, algebraic constraints, with other more tradi-
tional nonlinear dynamics. (See Section 5.3.2 for greater detail.) All of these can be viewed as challenges
in nonlinear dynamics aspects of modeling.Further developing both computational (numerical simulation) methods and analytical methods(bifurcation, perturbation methods, asymptotic analysis) for large nonlinear systems will invariably
mean great progress in the ability to build more elaborate and detailed models. However, with these
large models come large challenges. One is how to find methodical ways of organizing parameter space
exploration for systems that have numerous parameters. Another is the development of ways to codify
and track assumptions that have gone into the construction of a model. Understanding these assump-
tions (or simplifications) is essential to understanding the limitations of a model and when its predic-
tions are no longer biologically relevant.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.COMPUTATIONAL MODELING AND SIMULATION AS ENABLERS FOR BIOLOGICAL DISCOVERY203Box 5.24Modeling Challenges for Computer ScienceIntegration Methods¥Methods for integrating dissimilar mathematical models into complex and integrated overall models¥Tools for semantic interoperabilityModels¥High-performance, scalable algorithms for network analyses and cell modeling¥Methods to propagate measures of confidence from diverse data sources to complex modelsValidation¥Robust model and simulation-validation techniques (e.g., sensitivity analyses of systems with huge num-bers of parameters, integration of model scales)¥Methods for assessing the accuracy of genome-annotation systemsSOURCE: U.S. Department of Energy, Report on the Computer Science Workshop for the Genomes to Life Program, Gaithersburg, MD,March 6-7, 2002, available at http://DOEGenomesToLife.org/compbio/.Box 5.25Equation-free Multiscale Computation:Enabling Microscopic Simulators to Perform System-level TasksYannis Kevrikides of Princeton University and his colleagues have developed a framework for computer-aidedmultiscale analysis. This framework enables models at a ÒfineÓ (microscopic, stochastic) level of description to
perform modeling tasks at a ÒcoarseÓ (macroscopic, systems) level. These macroscopic modeling tasks, yieldinginformation over long time and large space scales, are accomplished through appropriately initialized calls to themicroscopic simulator for only short times and small spatial domains: ÒpatchesÓ in macroscopic space-time.In general, traditional modeling approaches require the derivation of macroscopic equations that govern thetime evolution of a system. With these equations in hand (usually partial differential equations (PDEs)), a variety
of analytical and numerical techniques for their solution is available. The framework of Kevrikides and col-leagues, known as the equation-free (EF) approach can, when successful, bypass the derivation of the macro-scopic evolution equations when these equations conceptually exist but are not available in closed form.The advantage of this approach is that the long-term behavior of the system bypasses the computationallyintensive calculations needed to solve the PDEs that describe the system. That is, the EF approach enables an
alternative description of the physics underlying the system at the microscopic scale (i.e., its behavior onrelatively short time and space scales) provide information about the behavior of the system over relativelylarge time and space scales directly without expensive computations. In effect, the EF approach constitutes a
systems identification-based, Òclosure on demandÓ computational toolkit, bridging microscopic-stochasticsimulation with traditional continuum scientific computation and numerical analysis.SOURCE: The EF approach was first introduced by Yannis Kevrikides and colleagues in K. Theodoropoulos et al., ÒCoarse Stability andBifurcation Analysis Using Timesteppers: A Reaction Diffusion Example,Ó Proceedings of the National Academy of Sciences 97:9840, 2000,available at http://www.pnas.org/cgi/reprint/97/18/9840.pdf. The text of this box is based on excerpts from an abstract describing a presen-tation by Kevrikides on April 16, 2003, to the Singapore-MIT Alliance program on High Performance Computation for Engineered Systems(HPCES); abstract available at http://web.mit.edu/sma/events/seminar/kevrekidis.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.204CATALYZING INQUIRYIn the second category, issues related to implementing the model arise. Often such issues involvethe actual code used to implement the model. Computational models are, in essence, large computer
programs; issues of software development come to the fore. As the desire for and utility of computa-
tional modeling increase, the needs for software are growing rather than diminishing as hardware
becomes more capable. On the other hand, progress in software development and engineering over the
last several decades has not been nearly as dramatic as progress in hardware capability, and there
appears to be no magic bullets on the horizon that will revolutionize the software development process.This is not to say that good software engineering does not or should not play a role in the developmentof computational models. Indeed, the Biomedical Information Science and Technology Initiative (BISTI)
Planning Workshop of January 15-16, 2003, explicitly recommended that NIH require grant applications,
proposing research in bioinformatics or computational biology to adopt as appropriate, accepted practices
of software engineering.126
 Section 4.5 describes some of the elements of good software engineering in thecontext of tool development, and the same considerations apply to model development.A second important challenge as large simulation models become more prevalent is a standardspecification language to unambiguously specify the model, its parameters, annotations, and even the
means by which it is to be scored against data. The challenge will be to provide a language flexible
enough to capture all interesting biological processes and incorporate models at different levels of
abstraction and in different mathematical paradigms, including stochastic differential, partial differen-
tial, algebraic, and discrete equations. It may prove necessary to develop a set of nested languagesÑfor
example, a language that specifies the biological process at a very high level and a linked language that
specifies the mathematical representation of each process. There are some current attempts at these
languages based on the XML framework. SBML and CellML are attempts in this direction.Finally, many biological modeling applications involve a problem space that is not well understoodand may even be intended to explore queries that are not well formulated. Thus, there is a high
premium on reducing the labor and time involved to produce an application that does something
useful. In this context, technologies for Òrapid prototypingÓ of biological models have considerable
interest.127126See http://www.bisti.nih.gov/2003meeting/report.cfm.127Note, however, that in the rapid prototyping process often used to create commercial applications, there is a dialoguebetween developer and user that reveals what the user would find valuable: once the developer knows what the user really
wants, the software development effort is straightforward. By contrast, in biological applications, it is nature that determines theappropriate structuring and formulation of a problem, and a problem cannot be structured in a certain way simply because it isconvenient to do so. Thus, technologies for the rapid prototyping of biological models must afford the ability to rearrange modelcomponents and connections between components with ease.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY2052056A Computational and Engineering View of Biology
Because 21st century biology is very concerned with function, it is helpful to have abstractionsavailable that characterize the functionality of interest. By doing so, insights derived from study of those
abstractions in other contexts become available for biological use. In addition, because biological sys-
tems are the products of eons of evolutionary history and decision making, viewing them through the
lens of engineering yields insights that are not otherwise available from an analysis that might be based
on first principles.6.1  BIOLOGICAL INFORMATION PROCESSING
1As noted in Chapter 2, biological systems are extraordinarily complexÑand partly as a conse-quence, poorly understood. Yet it is clear that biological systems demonstrate and exemplify function-
ality at different levels.Artifacts such as computer hardware and software also exhibit functionality and multiple levels. Tofacilitate the understanding and construction of such artifacts, computer science has developed infor-
mation abstractions that seek to capture and encapsulate certain kinds of functional behavior in ma-
nipulating and managing information; such abstractions are a primary focus of study of the computer
scientist (Box 6.1).One key connection to 21st century biology is that many biological problems now require thesimultaneous consideration of phenomena at different scales. For example, biologists can think of
genetics at the level of individual nucleotides, at the level of chromosomes, at the level of genomes, and
at the level of populations. From nucleotide to population is a span of many orders of magnitude, and
it is difficult to conceptualize such a range without moving seamlessly between different levels of
abstraction.Section 6.1 describes several such abstractions and their specific biological applications already inuse, but the description is not intended to be exhaustive, and there are likely many more such abstrac-
tions capable of providing biological insight, including new or as yet undiscovered techniques or
concepts. As such, this area represents opportunities for both biologists and computer scientists.
1Much of the discussion in Section 6.1 about cells as information-processing devices is adapted from R. Aviv and E. Shapiro,ÒCellular Abstractions: Cells as Computation,Ó Nature 419:343, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.206CATALYZING INQUIRYConsider that biological processes, such as catalysis, protein synthesis, and other metabolic sys-tems, are consumers, processors, or creators of information. As Loewenstein puts it, in biological sys-
tems, Òin addition to flows of matter and energy, there is also flow of information. Biological systems
are information-processing systems and this must be an essential part of any theory we may con-
struct.Ó2 Sydney Brenner goes farther, arguing that Ò. . . this information flow, not energy per se, is theprime mover of lifeÑthat molecular information flowing in circles brings forth the organization we call
ÔorganismÕ and maintains it against the ever-present disorganizing pressures in the physics universe.
So viewed, the information circle becomes the unit of life.Ó3The current state of intellectual affairs with respect to biological information and complexity mayhave some historical analogy with the concept of energy at the beginning of the 19th century. Although
the concept was intuitively obvious, it was not formally defined or measured at that time. CarnotÕs
analysis of the performance of steam engines formalized the meaning of energy, creating the basis forBox 6.1On the Abstractions of the Computer Scientist and EngineerAbstraction is a generic technique that allows the scientist or engineer to focus only on certain features of asystem while hiding others. Scientists in all disciplines typically use abstractions as a way to simplify calcula-tions for purposes of analysis, but computer scientists also use abstractions for purposes of design: to buildworking computer systems. Because building systems is the central focus of much work in computer science,
the use of abstractions to cope with complexity over a wide range of scale, size, and levels of detail is centralto a computer scientistÕs way of thinking.The focus of the computer scientist in creating an abstraction is to hide the complexity of operation Òunderneaththe abstractionÓ while offering a simple and useful set of services Òon top of it.Ó Using such abstractions is the
principal technique for organizing and constructing very sophisticated computer systems, and they enable com-
puter scientists to deal with large differences of scale. For example, one particularly useful abstraction useshardware, system software, and application software as successive layers on which useful computer systems canbe built. This illustrates one very important use of abstraction in computer systems: each layer provides the
capability to specify that a certain task be carried out without specifying how it should be carried out. In general,computing artifacts embody many different abstractions that capture many different levels of detail.A good abstraction is one that captures the important features of an artifact and allows the user to ignore theirrelevant ones. (The features decided to be important collectively constitute the interface of the artifact to theoutside world.) By hiding details, an abstraction can make working with an artifact easier and less subject to
error. But hiding details is not cost-freeÑin a particular programming problem, access to a hidden detail mightin fact be quite helpful to the person who will use that abstraction. Thus, deciding how to construct an abstrac-tion (i.e., deciding what is important or irrelevant) is one of the most challenging intellectual issues in computer
science. A second challenging issue is how to manage all of the details that are hidden. The fact that they arehidden beneath the interface does not mean that they are irrelevant, only that the computer scientist must designand implement approaches to handle these details ÒautomaticallyÓ (i.e., without external specification).SOURCE: Adapted from Computer Science and Telecommunications Board, National Research Council, Computing the Future: A BroaderAgenda for Computer Science and Engineering, National Academy Press, Washington, D.C., 1991.2W. Loewenstein, The Touchstone of Life: Molecular Information, Cell Communication, and the Foundations of Life, Oxford UniversityPress, New York, 1998, p. xiv.3S. Brenner, ÒTheoretical Biology in the Third Millennium,Ó Philosophical Transactions of the Royal Society B 354(1392):1963-1965,1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY207the science of thermodynamics. Only after energy had been identified and studied in the artificial realmof steam engines was it recognized as a prime aspect of natural systems as well.Similarly, the existing state of the theory of biological information (or, indeed, information of anysort) is based on the work of Claude Shannon, who studied the processing of information in human
technological channels of communication, and the field of computational complexity, which was cre-
ated to analyze the performance characteristics of algorithms running on human-built computers. How-
ever, just as thermodynamics successfully widened its scope to the natural world from steam engines,
information and computation theory may become a powerful lens for describing, measuring, and
understanding processes in the natural world.Biological information is likely to have a close relationship to information in the Shannon sense ofthe term, if only because biological entities depend on information to coordinate their internal activity.
Cells coordinate their internal activity because they have harnessed intracellular Shannon information
channels. Multicellular organisms coordinate their internal activity because they have harnessed inter-
cellular Shannon information channels. These channels are the conduits through which genes transfer
their information content to proteins, proteins serve as signaling agents, and nervous systems work.
Also, ShannonÕs insight about the nature of information transmission allows us to understand how
signals can reliably be sent through a noisy unpredictable environment (whether cell telephone signals,
Internet packets, or hormone signaling proteins) and received accurately at the other end.On the other hand, Shannon information applies in the strict sense only when it is possible toidentify a sender and receiver connected by a channel. There are some places in which this applies, such
as the projection of the retina to the brain. Yet in the context of information feedback and loops rather
than channels, it is not clear that Shannon information continues to have a well-defined meaning.There have been a number of attempts to generalize Shannon information to problems at thecellular and subcellular levels, of which the conceptualization by Manfred Eigen of hypercycles, quasi-
species, and sequence space is one of the most notable.4 But whether these concepts are the right onesis not as important as the recognition that new concepts are needed.A more specific connection between biology and computation can be seen in the biological use ofinformation to enhance the survival and reproductive functions of an organism. That is, biological
organisms use information about the environment to stimulate or drive responses that boost the likeli-
hood of survival and successful reproduction. This process is effectively a computation that transforms
the inputs (which describe environmental conditions) into the appropriate outputs (the organismÕs
behavior).5 For example, Hartwell et al. note that signals from the environment entrain circadian bio-logical clocks to produce responses to predicted fluctuations in light intensity and temperature.6Embedded within cells are complex signaling mechanisms that transfer information from one partof a cell to another and intercellular mechanisms that transfer information from one part of a multicel-
lular organism to another. Indeed, signal transduction pathwaysÑand the proteins associated with
themÑappear to serve the functions of information processing and transfer,7 rather than those of moreÒtraditionalÓ biology (e.g., chemical transformation of metabolic intermediates or the building of cellu-
lar structures).4M. Eigen, ÒThe Origin of Biological Information,Ó presented at the Seventh International Conference on Intelligent Systemsfor Molecular Biology, August 6-10, 1999; Heidelberg, Germany, available at http://bioinf.mpi-sb.mpg.de/conferences/ismb99/WWW/abstracts/abs-eigen.html.5Indeed, it has been asserted that the history of life can be described as the evolution of systems that manipulate one set ofsymbols representing inputs into another set of symbols that represent outputs. J.J. Hopfield, ÒPhysics, Computation, and WhyBiology Looks So Different,Ó Journal of Theoretical Biology 171:53-60, 1994.6L.H. Hartwell, J.J. Hopfield, S. Leibler, and A.W. Murray, ÒFrom Molecular to Modular Cell Biology,Ó Nature 402(6761Suppl):C47-C52, 1999.7D. Bray, ÒProtein Molecules as Computational Elements in Living Cells,Ó Nature 376(6538):307-312, 1995. The examples in thenext paragraph are also BrayÕs.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.208CATALYZING INQUIRYFor example, a simple enzyme protein could be viewed as a computational element that takes aninputÑthe concentration of its Òsubstrate,Ó the molecule with which it interactsÑand produces an
output: a concentration of the catalyzed reaction product. An enzyme that becomes active only when it
binds to two separate regulator molecules will function something like a Boolean AND gate, and so on.
Circuits formed from these elements can be as simple as a switch or an oscillator, or as complex as to
drive a bacteriumÕs chemotaxis response. Indeed, the cell even possesses a kind of short-term, Òrandom-
accessÓ memory, in the sense that events in its environment have profoundly shaped the concentration
and activity of many thousands of molecules in the cell. In short, these protein-based circuits constitute
a kind of nervous system for the cell, providing it with much of what it needs to control its behavior.
Box 6.2 provides some additional perspective on this subject.Additional insights can be gained from the notion that both computational processes and biologicalpathways can be viewed as processes that affect the state of a system according to well-defined (though
possibly probabilistic) rules. Thus, it is possible to describe regulatory, metabolic, and signaling path-
ways, as well as multicellular processes such as immune responses, as systems of interacting computa-
tions operating in parallel. In particular, languages such as Petrinets, Statecharts (discussed in Section
4.3.1), and the Pi-calculus, originally developed for the specification and study of systems of interacting
computations, can be used to represent such systems.8 Such representations enable researchers tosimulate their behavior, and to support qualitative and quantitative reasoning on the properties of these
systems.To cite two prominent researchers in this area:Processes, the basic interacting computational entities of these languages, have an internal state andinteraction capabilities. Process behavior is governed by reaction rules specifying the response to an input
message based on its content and the state of the process. The response can include state change, a changein interaction capabilities, and/or sending messages. Complex entities are described hierarchicallyÑforexample, if a and b are abstractions of two molecular domains of a single molecule, then (a parallel b) is
an abstraction of the corresponding two-domain molecule. Similarly, if a and b are abstractions of the twopossible behaviors of a molecule in one of two conformational states, depending on the ligand it binds,then (a choice b) is an abstraction of the molecule, with the choice between a and b determined by its
interaction with a ligand process.9Abstractions of the cell as a computing or information-processing device allow one to distinguishbetween two conceptual levels: a Òlow-levelÓ view that focuses on implementation (i.e., how the system
is builtÑwhere the wires go or the detailed molecular processes involved) and a Òhigh-levelÓ view that
focuses on functionality (what the system doesÑanalogous to a logic gate or a computational device).10For example, one might distinguish between the pathways involved in regulating the circadian rhythm
of an organism and its functional behavior as an oscillator.The difference between these levels of abstraction enables biologically significant comparisons to bemade. For example, it would be instructive if two different organisms implemented the same function
in different ways. In other words, functional equivalence between related implementations in different
organisms could be regarded as a measure of the behavioral similarity of entire systems. (In the litera-
ture of evolutionary biology, the implementation of the same function in different ways is called Òanalo-
gousÓ implementation.) Perhaps more importantly, a functional perspective is an enabler for the inte-
gration of knowledge about the function, activity, and interaction of cellular molecular systems.8R. Aviv and E. Shapiro, ÒCellular Abstractions: Cells as Computation,Ó Nature 419:343, 2002.9R. Aviv and E. Shapiro, ÒCellular Abstractions,Ó 2002.10In many circumstances, different parts of a biological system may play different roles at different times or even differentroles at different time scales at the same time. This is especially true in splicing variants, where the expression of a gene mayproduce proteins with quite different functions according to the behavior of the splicing mechanism. Indeed, in some cases,different splicings have opposite functions. Nevertheless, in understanding a given role at a given time and time scale, the high-level abstraction focused on functionality is meaningful and scientifically significant.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY209This perspective on cells as computational devices should not be taken as an argument that cellsprocess information the way a digital computer does. The organizations are radically different. To name
just a few differences, in a cell there is no clean separation between the data store and the central
processing unit: the cellÕs memory is the same protein reaction network that does its processing. Real
proteins rarely respond or act in a completely binary fashionÑthe levels of concentration matter. Apart
from DNA, few portions of a cellÕs internal machinery are explicitly digital in natureÑwith the result
that signaling in a cell must take place in a highly noisy environment.Box 6.2Role of Computation in Complex Regulatory NetworksComputation . . . [is] a crucial ingredient when dealing with the description of biocomplexity and its evolution,because it turns out to be much more relevant than the underlying physics. Its dynamics is governed mainly by thetransmission, storage and manipulation of information, a process which is highly nonlinear. This nonlinearity is wellillustrated by the nature of signaling in cells: local events involving a few molecules can produce a propagating
cascade of signals through the whole system to yield a global response. . . . If we try to make predictions about theoutcomes of these signaling events in general, we are faced with the inherent unpredictability of computationalsystems. It is at this level where computation becomes central and where idealized models of regulatory networks
seem appropriate enough to capture the essential features at the global scale.Cells are probably the most complete example of this traffic of signals at all levels. . . . The cellular network can bedivided into three major self-regulated sub-webs:¥The genome, in which genes can affect each otherÕs level of expression;¥The proteome, defined by the set of proteins and their interactions by physical contact; and¥The metabolic network (or the metabolome), integrated by all metabolites and the pathways that link each other.All these subnetworks are very much intertwined since, for instance, genes can only affect other genes throughspecial proteins, and some metabolic pathways, regulated by proteins themselves, may be the very ones to catalyzethe formation of nucleotides, in turn affecting the process of translation. . . . It is not difficult to appreciate theenormous complexity that these networks can achieve in multicellular organisms, where large genomes have struc-
tural genes associated with at least one regulatory element and each regulatory element integrates the activity of atleast two other genes. . . .Luckily, all this extraordinary complexity can be abstracted, at least at some levels, to simplified models which canhelp in the study of the inner-workings of cellular networks. Overall, irrespective of the particular details, biologicalsystems show a common pattern: some low-level units produce complex, high-level dynamics coordinating their
activity through local interactions. Thus, despite the many forms of interaction found at the cellular level, all comedown to a single fact: the state of the elements in the system is a function of the state of the other elements it interactswith. What models of network functioning try, therefore, is to understand the basic properties of general systems
composed of units whose interactions are governed by nonlinear functions. These models, being simplifications, donot allow one to make predictions at the level of the precise state of particular units. Their average overall behavior,however, can shed light into the way real cells behave as a system. . . .. . . [M]any entities in cellular networks can be identified as the basic units of regulation, mainly distinguished bytheir unique roles with respect to interaction with other units. These basic units are genes, each of the proteins that
the genes can produce, each of the forms of a protein, protein complexes, and all related metabolites. These unitshave associated values that either represent concentrations or levels of activation. Their values depend on the valuesof the units that affect them due to the mechanisms discussed, plus some parameters that govern each special form
of interaction. . . . Computer modeling of [the] network [the segment polarity network of Drosophila melanogaster]has provided insight into various questions. A very important result is the fact that this network seems to be aconserved module. Evidence for this has been obtained by simulations demonstrating its robustness against thechange of parameters. . . .SOURCE: Reprinted from P. Fernandez and R.V. Sole, ÒThe Role of Computation in Complex Regulatory Networks,Ó Santa Fe InstituteWorking Paper, 2003, available at http://www.santafe.edu/sfi/publications/Working-Papers/03-10-055.pdf; to appear in a chapter inPower Laws, Scale-Free Networks and Genome Biology, Landes Bioscience. Reprinted with permission.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.210CATALYZING INQUIRYIt is also interesting that biological function often relies on what might be called exploration withselectionÑthe production of many intermediate products resulting from stochastic subprocesses that
are then refined to unique and appropriate solutions.11 Taken across the entire population, explora-
tion with selection exploits the difference between creating a solution and testing a solution for
correctnessÑthe first being in general a much more difficult computational task than the second.12Random processes are used to explore the space of possible solutions,13 and other machinery cullsthese possible solutions. As Hartwell et al. argue, ÒSimilar messy and probabilistic intermediates
appear in engineering systems based on artificial neural networksÑmathematical characterizations
of information processing that are directly inspired by biology. A neural network can usefully de-
scribe complicated deterministic input-output relationships, even though the intermediate calcula-
tions through which it proceeds lack any obvious meaning and their choice depends on random noise
in a training process.Ó146.2  AN ENGINEERING PERSPECTIVE ON BIOLOGICAL ORGANISMS
6.2.1  Biological Organisms as Engineered Entities
Engineering insights can be useful in understanding biological organisms as engineered entities,and the rationale for seeking insights from engineering is based on three notions. First, although the
physical scales may differ in some cases, human technology and natural systems operate in the same
world and must obey the same physical rules. Knowledge that engineering fields have accumulated
about what techniques work and the limits of those techniques can serve as a potentially valuable guide
in investigating the physical basis of the operations of natural systems. This is especially true for
biomechanical feats, such as structural support, locomotion, circulation, and so on.The second rationale is that because evolution and a long history of environmental accidents havedriven processes of natural selection, biological systems are more properly regarded as engineered
artifacts than as objects whose existence might be predicted on the basis of the first principles of physics,
although the evolutionary context means that an artifact is never ÒfinishedÓ and is rather evaluated on
a continuous basis.15 Both engineered artifacts and biological organisms demonstrate function, embody11For example, the immune system relies on the random generation of pathogen detectors, which are then eliminated whenthey match some definition of Òself.Ó In single molecules, kinetic funnels direct different molecules of the same protein throughmultiple, different paths from the denatured state to a unique folded structure (K.A. Dill and H.S. Chan, ÒFrom Levinthal to
Pathways to Funnels,Ó Nature Structural Biology 4:10-19, 1997). Within cells, the shape of the mitotic spindle is due partly toselective stabilization of randomly generated microtubules whose ends happen to be close to a chromosome (R. Heald, R.Tournebize, T. Blank, R. Sandaltzopoulos, P. Becker, A. Hyman, and E. Karsenti, ÒSelf-organization of Microtubules into Bipolar
Spindles Around Artificial Chromosomes in Xenopus Egg Extracts,Ó Nature 382(6590):420-425, 1996). Within the brain, the pat-terning of the nervous system is refined by the death of nerve cells and the decay of synapses that fail to connect to an appropri-ate target.12This point can be formalized in the language of theoretical computer science. See J. Hartmanis, ÒComputational Complexityand Mathematical Proofs,Ó pp. 251-256 in Informatics: 10 Years Back, 10 Years Ahead, 2000, Lecture Notes in Computer Science,Springer-Verlag, Berlin, Heidelberg, 2001.13For example, random processes are at the heart of stochastic optimization methods that can be used for protein structureprediction and receptor ligand docking, including simulated annealing, basin hopping, and parallel tempering. (An interestingintroduction to stochastic optimization methods can be found at W. Wenzel, ÒStochastic Optimization Methods,Ó available at
http://iwrwww1.fzk.de/biostruct/Opti/opti.htm.) Also, the systematic exploration of ecological models discussed in Section5.4.8 is also based on the use of random processes.14The quote is taken from L.H. Hartwell, J.J. Hopfield, S. Leibler, and A.W. Murray, ÒFrom Molecular to Modular Cell Biol-ogy,Ó Nature 402(6761 Suppl.):C47-C52, 1999. Hartwell et al. credit Sejnowski and Rosenberg with the neural network example(T.J. Sejnowski and C.R. Rosenberg, ÒParallel Networks That Learn to Pronounce English Text,Ó Complex Systems 1:145-168, 1987).15A classic paper on this subject is F. Jacob, ÒEvolution and Tinkering,Ó Science 196(4295):1161-1166, 1977.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY211behavior, and manifest an evolutionary history.16 Engineered artifacts serve the purposes of theirhuman designers, and biological organisms serve the purposes of natureÑthat is, to survive and repro-
duce.17 Thus, the concepts needed to understand biological function may have some resemblance tosome of the concepts already developed for ÒsyntheticÓ disciplines, of which engineering and computer
science are prime examples.A third rationale is that the engineering disciplines have already had a long history of systems-levelthinking and, indeed, have produced artifacts that are arguably approaching biological levels of com-
plexity. For example, a Boeing 777 jetliner contains about 150,000 subsystem modules, including 1,000
computers, a number of the same order of magnitude as the estimated 300,000 different proteins in a
typical human cell. Just as in the cell, moreover, these aeronautical subsystems are linked into an
immensely complex Ònetwork of networksÓÑa control system that just happens to fly.18A related point, and a key lesson from engineering, is that large systems are built out of smallersystems that are stable. Decomposition of a complex structure into an assembly of simpler structures
whose operation is coordinated tends to be a much more successful strategy that building the complex
structure from scratch, and this approach can be seen in the structure of the cell. Consider that a human
cell has many physical structures within itÑnucleus, mitochondria, and so on; each of these can be
regarded as a device, many of which compose the cell. Further, many and perhaps even most cellular
functions (e.g., genetic regulatory networks, metabolic pathways, signaling cascades) are implemented
in a manner that is highly robust against single-point failure (i.e., the function will continue to operate
properly even when one element is missing). Section 6.2.3 addresses this point in more detail.A second view of biological organisms as engineered entitiesÑas novel entities to be constructed byhuman beings rather than as existing organisms to be understood by human beingsÑis discussed in
Section 8.4.2 on synthetic biology.6.2.2  Biology as Reverse Engineering
Biological organisms are generally presented to scientists as completed entities, so the challenge ofachieving an engineering understanding of them is in fact a challenge of reverse engineering. One defini-tion of reverse engineering is Òthe process of analyzing a subject system with two goals in mind: (1) to16While it is generally recognized that biology and evolution are intimately linked, the analogous connection between engi-neering and evolution is less well understood. Nevertheless, most human-engineered objects have a lot of historicity in them as
well. Most human objects are designs based as improvements on previous designs, not de novo, and this can complicate theunderstanding of the relationship between functionality and design of a human artifact. One reason is a desire for backwardcompatibilityÑconsider the fact that two-prong electric plugs and sockets are much more hazardous than some alternative
designs and yet they are ubiquitous in appliances today. The same is true for operating systemsÑlater versions of an operatingsystem often incorporate large amounts of code from previous versions to facilitate backward compatibility. A second reason isthat previous designs may have solved a design problem in a particularly effective way, and these solutions from the past are
ignored today at the designerÕs peril. For example, consider the evolution of the rotary phone into todayÕs push-button phones.Donald Norman observes that the cradle of the phone handset and the button-switch in it had two distinct functions: the cradleprovided a place for the user to put the phone and the button-switch turned the phone on and off. Norman notes that whether
deliberately or by accident, the particular design of the rotary phone that placed the on-off switch in a protected spot in thecradle also protected the on-off switch from the user accidentally hanging up the phone. However, the designers of newer push-button phones did not pick up on that feature; many push-button phones are designed so that the on-off switch and the hang-up
cradle are separateÑthus making the on-off switch much easier to bump and thereby to accidentally disconnect a phone call. SeeD. Norman, The Design of Everyday Things, Basic Books, New York, 1998.17See for example L.H. Hartwell, J.J. Hopfield, S. Leibler, and A.W. Muray, ÒFrom Molecular to Modular Cell Biology,Ó Nature402(6761 Suppl):C47-52, 1999, available at http://cgr.harvard.edu/publications/modular.pdf. Hartwell et al. further argue thatit is notions of function and purpose that differentiate biology from other natural sciences such as chemistry or physics, andhence that reductionist biologyÑinquiry that seeks to explain biological phenomena only in chemical or physical termsÑis
inherently incomplete.18M.E. Csete and J.C. Doyle, ÒReverse Engineering of Biological Complexity,Ó Science 295(5560):1664-1669, 2002, available athttp://www.sciencemag.org/cgi/content/abstract/295/5560/1664.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.212CATALYZING INQUIRYidentify the systemÕs components and their interrelationships and (2) to create representations of thesystem in another form or at a higher level of abstraction.Ó19A better description could not be developed for the goal of systems biology, even without having tochange any words in this definition. And yet reverse engineering, despite being a fairly standard
engineering topic, is not taught to biologists.20 One drawback is that the metaphor itself is foreign tobiologists; if they wanted to do engineering of any kind, they would have been engineers. Second,
reverse engineering is generally a more difficult task than forward engineering (i.e., the fabrication of a
device to implement some specific functionality), and reverse engineering of a biological organism is a
particularly difficult endeavor.One important reason is that reverse engineering is often underdetermined, in the sense that mul-tiple solutions can be developed to account for a given behavior. In such cases, choosing among them
thus requires either more data or a priori assumptions about the true nature of the system being reverse-
engineered. For example, in dealing with the reverse-engineering task of building detailed kinetic
models of intracellular processes from time-series data, Rice and Stolovitzky note that assumptions
such as linearity or sparseness or the use of predetermined model structures (e.g., reactions limited in
the number of possible reactants and substrates) can help to reduce the non-uniqueness.21A second and even more important reason for the difficulty of reverse engineering is that because oftheir evolutionary history, the organisms of interest are constructed in a highly nonoptimal manner.
When engineers seek to understand how an artifact has been constructed, the basic question they ask is,
Why? Why is this structure here? Why was that material used? By asking such questions of a human-
engineered artifact, the engineer can often divine a reason that answers them. The reason is that engi-
neers can be expected to design artifacts using principles such as modularity and separation of function
(i.e., to minimize unnecessary links between subsystems with different purposes). These principles
guard human designs against unforeseen side effects that would arise if components were not deliber-
ately assembled in such a way as to minimize undesired or unanticipated interactions.However, the same is not true of biological organisms. In many cases, the only answer for biologicalsystems is, ÒThatÕs the way it was built.Ó Nature builds from accidents that happen to work and creates
new mechanisms on top of old ones. While some evolved systems are quite elegant (e.g., the sensory and
the motor components of the Escherichia coli chemotaxis mechanism), many if not most such systems atleast appear to a human as inelegant, redundant, Òkludgy,Ó and inefficientÑsome of them extremely so.
Systems engineered by humans, even very poorly engineered ones and even though they too often show
their historical origins, are seldom if ever as arcane and kludgy as evolved biological organisms.Finally, it is helpful to distinguish between two different approaches to reverse engineering. Oneapproach to reverse engineering of biological systemsÑa Òtop-downÓ approachÑbegins with its ob-
servable behavior and characteristics, and seeks to decompose the system into components or sub-
systems that collectively exhibit the macroscopic behavior in question. That is, the top-down approach
is based on a successive decomposition down to the systemÕs most elemental components.A second approach is based on a Òbottom-upÓ approach, which begins with an understanding of theconstituent parts at the lowest level, e.g., the macromolecules and the genetic regulatory networks of the19E.J. Chikofsky and J.H. Cross, ÒReverse Engineering and Design Recovery: A Taxonomy,Ó IEEE Software 13-17, 1990.20Indeed, the BIO2010 report on undergraduate education in biology (National Research Council, Bio 2010: UndergraduateEducation to Prepare Biomedical Research Scientists, National Academies Press, Washington, DC, 2003) noted that Òone approach tothe study of biology is as a problem in reverse engineering. Manufactured systems are easier to understand than biologicalsystems, because they have no unknown components, and their design principles can be explicitly stated. It is easiest to learnhow to analyze systems through investigating how manufactured systems achieve their designed purpose, how their function
depends on properties of their components, and how function can be reliable even with imperfect components.Ó Also, under-scoring the point that engineering is not a part of biology education today, the report emphasized the importance of exposingbiology students to engineering principles and analysis in the course of their undergraduate educations. Chapter 10 has more
discussion of this point.21J.J. Rice and G. Stolovitzky, ÒMaking the Most of It: Pathway Reconstruction and Integrative Simulation Using the Data atHand,Ó Biosilico 2(2):70-77, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY213cells that make up the system. The philosophical notion embedded in the bottom-up approach is that acomponent is likely to be easier to understand than the system in which it is embedded. By successive
assembly of component parts, one is able to create ever-larger assemblies whose operation is understood.Both approaches seek as their underlying ultimate goal an understanding of how a biologicalsystem works in all of its complexity. But they require different strategies for acquiring data at different
levels of scale (top-down entails data acquisition at ever-smaller scales, while bottom-up entails data
acquisition at ever-larger scales). And also, it should be expected that they will generate different
intermediate outputs and products along the way to this ultimate goal.6.2.3  Modularity in Biological Entities
22A functional perspective on biology is centrally based on the notion that biological function isseparable, into what might be called modules. The essence of a moduleÑwell known in engineering
disciplines as well as computer scienceÑis that of an entity whose function is separable from other
modules. In the computer science context, a module might be a subroutine upon which various pro-
grams can build. These various programs would interact with the subroutine only through the pro-
gramming interfaceÑthe set of arguments to the subroutine that parameterize its behavior. Box 6.3
describes how the search for functional modules plays into systems biology.Box 6.3Functional Modules in BiologyAn important theme in systems biology has been to look for functional modules that have been conserved andreused. The idea of breaking biological systems into small functional blocks has obvious appeal; the parts can be
divided and conquered so that the most complex of machines become readily understood in terms of block diagramsor sets of subroutines. Clearly, some conserved modules exist such as the ribosome and the tricarboxylic acid cycle.One method to search for modules involves looking for higher-order structures or recurring sub-networks (often
termed ÒmotifsÓ) in metabolic or gene regulatory networks. Another approach mentioned earlier is clustering expres-sion profiles to produce groups of genes that appear to be co-regulated that should ideally reveal the functionalmodules. However, this assumption does not appear to generalize to all functional groups under all conditions, as
some functional groups show well-correlated expression profiles whereas others do not. The low correlation of genesobserved within some functional groups has been attributed to the fact that some of these genes belong to multiplefunctional classes. In another analysis in E. coli, 99 cases were found where one reaction existed in multiple path-ways in EcoCyc. These observations suggest potential pitfalls with anticipating too much functional modularity interms of biology being neatly partitioned into non-overlapping modules. Moreover, the tissue- or species-specificdifferences mentioned earlier may prevent simplistic transfer of modules from one biological system to another. It
remains to be seen if biology is as modular as the system biologist might like it to be.Biological modules may turn out be more interconnected and overlapping than independent in many systems. Inaddition, the experiences with pathway reconstruction suggest that the combinations of data source produce a moreaccurate if not more complete characterization of the system under study. These observations point to an eventualneed to develop large-scale, predictive models based on a multitude of data sources. For example, metabolic models
may combine data from many sources into a quantitative set of equations that can make predictions amenable toexperimental verification. In another system, cardiac models can bridge data at multiple levels (i.e. molecular,cellular, organ, etc.) and their corresponding characteristic timescales. In this system, modeling efforts at the single-
cell level in the heart suggested a mechanism of increased contraction force that was later confirmed in experimentalstudies of whole heart.SOURCE: Reprinted by permission from J.J. Rice and G. Stolovitzky, ÒMaking the Most of It: Pathway Reconstruction and IntegrativeSimulation Using the Data at Hand,Ó Biosilico 2(2):70-77. Copyright 2004 Elsevier.22Section 6.2.3 is based largely on L.H. Hartwell, J.J. Hopfield, S. Leibler, and A.W. Murray, ÒFrom Molecular to Modular CellBiology,Ó Nature 402(6761 Suppl.):C47-C52, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.214CATALYZING INQUIRYImportant insights into biological organisms can be gained by seeking to identify general principles thatgovern the structure and function of modules (Box 6.4). In a biological context, a module might be an entity
that performs some biochemical function apart from other modules, isolated from those other modules by
spatial localization (i.e., it is physically separated from those other modules) or by chemical specificity (i.e., its
biochemical processes are sensitive only to the specific chemical signals of that module and not to others that
may be present). Furthermore, modules must be able to interact with each other selectively. Specific connec-
tivity enables module A to influence the functional behavior of module B, but not to affect the operation of
modules C through Z. Also, the particular pattern of connectivity can account for some emergent properties
of these modules, such as an ability to integrate information from multiple sources.As noted by Hartwell et al., ÒHigher-level functions can be built by connecting modules together.For example, the super-module whose function is the accurate distribution of chromosomes to daughter
cells at mitosis contains modules that assemble the mitotic spindle, a module that monitors chromo-
some alignment on the spindle, and a cell-cycle oscillator that regulates transitions between interphase
and mitosis.Ó When a function of a protein is restricted to one module, and the connections of that
module to other modules are through such proteins, it becomes much easier to alter connections to
other modules without global consequences for the entire organism.Modular structures have many advantages. For example, the imposition of modular design on anentity allows a module to be used repeatedly by different parts of the entity. Furthermore, changes
internal to the module do not have global impact if those changes do not affect its functional behavior.
Modules can be combined and recombined in ways that alter the functionality of the complete systemÑBox 6.4Some Mechanisms Underlying the Structure and Function of Modules1.Positive feedback loops can drive rapid transitions between two different stable states of a system. For example,
positive feedback drives cells rapidly into mitosis, and another makes the exit from mitosis a rapid and irreversibleevent.12.Negative feedback loops can maintain an output parameter within a narrow range, despite widely fluctuating
input. For example, negative feedback in bacterial chemotaxis2 allows the sensory system to detect subtle variationsin an input signal whose absolute size can vary by several orders of magnitude.3 (This topicÑrobustness againstnoiseÑis described in more detail in Section 6.2.5.)3.Coincidence detection systems require two or more events to occur simultaneously in order to activate an output.
For example, coincidence detection is central in eukaryotic gene transcription, in which several different transcrip-
tion factors must be present simultaneously at a promoter site before transcription can occur. (Note the similarity toa multi-input AND gate.)4.Parallel circuits allow devices to survive failures in one of the circuits. For example, DNA replication involves

proofreading by the DNA polymerase backed up by a mismatch repair process that removes incorrect bases after thepolymerase has moved on. Both of these must fail before a cell cannot produce viable progeny, and these twomechanisms, combined with a system for killing potentially cancerous cells, reduce the frequency at which individ-
ual cells give rise to cancer to about 1 in 1015.5.Quality control systems monitor the output of many biological processes to ensure that the processes have
executed correctly. Such systems can be seen in cell-cycle checkpoints, DNA replication and repair, choices be-
tween cell survival and death after insults to cells, or quality control in protein folding and/or sorting events.1D.O. Morgan, ÒCyclin-dependent Kinases: Engines, Clocks, and Microprocessors,Ó Annual Review of Cell and Developmental Biology13:261-291, 1997.2Chemotaxis is the propensity of certain bacteria, such as E. coli, to swim toward higher concentrations of nutrients.3H.C. Berg, ÒA Physicist Looks at Bacterial Chemotaxis,Ó Cold Spring Harbor Symposium on Quantitative Biology 53(1):1-9, 1988.SOURCE: Items 1-4 adapted from L. Hartwell, J.J. Hopfield, S. Leibler, and A.W. Murray, ÒFrom Molecular to Modular Cell Biology,ÓNature 402(Suppl.):C47-C52, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY215the building blocks remain more or less stable, while the connectivity among them determines thecharacter of the system.If biological modules really do exist, one might expect to find them reused in different cellularcontexts, performing the same function but to different ends. Understanding the function and behavior
of a cellular pathway would entail the discovery and characterization of such modular building blocks,
tasks that should be simpler than trying to understand biological networks of different organisms as an
irreducible whole.Several independent pieces of evidence have emerged supporting the modularity hypothesis. Forexample, evidence is accruing that certain regions of DNA are ÒconservedÓ from one species to another.
These regions may be associated with genes coding for proteins or with regulatory and structural
functionality. Caenepeel et al. found that the human and mouse kinomes (i.e., the collection of protein
kinases in an organism) are 99 percent identical, although the percentage of identity between orthologues
(i.e., genes or proteins from different organisms that have the same function) ranges from 70 percent to
99 percent (with single nucleotide insertions or deletions in many cases).23 Dermitzakis et al. found thatperhaps a third of the highly conserved DNA regions between mouse and human code for proteins,
while much of the rest probably codes for regulatory and structural functionality.24Genetic expression networks may also display regular patterns of interconnections (motifs) recur-ring in many different parts of a network at frequencies much higher than those found in randomized
networks.25 Such motifs might be regarded as building blocks that can be used to assemble entities ofmore complex functionality.26 For example, Shen-Orr et al. discovered a series of simple, recurringnetwork motifs in the gene interaction map of the bacterium E. coli.27 Shortly afterwards, RichardYoung and colleagues found the same motifs to recur at statistically surprising frequencies in yeast.28Milo et al. found that these motifs were also overrepresented in a neuronal connectivity network of
Caenorhabditis elegans as well as the connectivity networks in the ISCAS89 benchmark set of sequentiallogic electronic circuits, but not in ecosystem food webs.29 Milo et al. speculate that these motifs reflectthe underlying processes that generated each type of network, in this case one set of motifs for those that
process information (the genetic regulation, neuronal connectivity, and electronic logic networks) and
another set of motifs for those that process and carry energy.Finally, a collaborative project led by Eric Davidson and his group at the California Institute ofTechnology, and involving Bolouri and Hood at the Institute for Systems Biology, also suggests simple
design principles and building blocks in genetic networks. Figure 6.1 is a map of the interactions among23S. Caenepeel, G. Charydezak, S. Sudarsanam, T. Hunter, and G. Manning, ÒThe Mouse Kinome: Discovery and ComparativeGenomics of All Mouse Protein Kinases,Ó Proceedings of the National Academy of Sciences 101(32):11707-11712, 2004.24E.T. Dermitzakis, A. Reymond, R. Lyle, N. Scamuffa, C. Ucla, S. Deutsch, B.J. Stevenson, et al., ÒNumerous PotentiallyFunctional But Non-genic Conserved Sequences on Human Chromosome 21,Ó Nature 420(6915):578-582, 2002.25R. Milo, S. Shen-Or, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon, ÒNetwork Motifs: Simple Building Blocks ofComplex Networks,Ó Science 298(5594):824-827, 2002.26Alon refines the notion of module as building block to suggest that modules and motifs are related but separate concepts. InAlonÕs view, a module in a network is a set of nodes that have strong interactions and a common function. Some nodes areinternal and do not interact significantly with nodes outside the module. Other nodes accept inputs and produce outputs that
control the moduleÕs interactions with the rest of the network. Alon argues that one reason modules evolve in biology is that newdevices or entities can be constructed out of existing, well-tested modules; thus, adaptation to new conditions (and new forces ofnatural selection) is more easily accomplished. If modules are to be swapped in and out, they must possess the property that
their input-output response is approximately independent of what is connected to themÑthat is, that the module is functionallyencapsulated. By contrast, a motif is an overrepresented patterns of interconnections in a network that is likely to perform someuseful behavior. However, it may not be functionally encapsulated, in which case it is not a module. For more discussion, see U.Alon, ÒBiological Networks: The Tinkerer as an Engineer,Ó Science 301(5641):1866-1867, 2003.27S.S. Shen-Orr, R. Milo, S. Mangan, and U. Alon, ÒNetwork Motifs in the Transcriptional Regulation Network of Escherichiacoli,Ó Nature Genetics 31(1):64-68, 2002.28T.I. Lee, H.J. Yang, S.Y. Lin, M.T. Lee, H.D. Lin, L.E. Braverman, and K.T. Tang, ÒTranscriptional Regulatory Networks inSaccharomyces cerevisiae,Ó Science 298(5594):799-804, 2002.29R. Milo et al., ÒNetwork Motifs,Ó 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.216CATALYZING INQUIRYFIGURE 6.1The endomesoderm specification network in the sea urchin species 
Strongylocentrotus purpuratus.The period of activity represented spans embryonic growth from single cell to gastrulation (approximately 600cells). The different background colors denote different cell types, as indicated on the cartoon of an early blastula-stage embryo on the top right. The short, thick horizontal lines represent regulatory DNA of a particular gene inthe network, to which transcription factors bind to activate or repress transcription. The bent arrow emanating
from each regulatory domain represents the basal transcription apparatus of the gene, and the line(s) emergingfrom it represent the interactions of the product of the gene with other proteins (via the white and black interactionboxes) or cis-regulatory DNA.The architecture of the network is based on perturbation and expression data, on data from cis-regulatoryanalyses for several genes, and on other experiments discussed in the references below. For quantitative results ofperturbation experiments and temporal details and the latest view of the network, see http://sugp.caltech.edu/
endomes/.The repression cascade motif referred to in the text is indicated by the thick black (upstream gene) and gray(downstream genes) arrows. This work is described in the following:1.E.H. Davidson, J.P. Rast, P. Oliveri, A. Ransick, C. Calestani, C.H. Yuh, T. Minokawa, et al., ÒA Genomic
Regulatory Network for Development,Ó Science 295(5560):1669-1678, 2002.2.H. Bolouri and E.H. Davidson, ÒModeling DNA Sequence-based 
cis-Regulatory Gene Networks,Ó Develop-mental Biology 246(1):2-13, 2002.3.C.T. Brown, A.G. Rust, P.J.C. Clarke, Z. Pan, M.J. Schilstra, T. De Buysscher, G. Griffin, et al., ÒNew Compu-
tational Approaches for Analysis of cis-Regulatory Networks,Ó Developmental Biology 246(1):86-102, 2002.4.A. Ransick, J.P. Rast, T. Minokawa, C. Calestani, and E.H. Davidson, ÒNew Early Zygotic Regulators of
Endomesoderm Specification in Sea Urchin Embryos Discovered by Differential Array Hybridization,Ó Develop-mental Biology 246(1):132-147, 2002.5.C.H. Yuh, C.T. Brown, C.B. Livi, L. Rowen, P.J.C. Clarke, and E.H. Davidson, ÒPatchy Interspecific Sequence
Similarities Efficiently Identify Positive cis-Regulatory Elements in the Sea Urchin,Ó Developmental Biology246(1):148-161, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY217approximately 50 genes underlying an early cell-type specification event in sea urchin embryos thatincludes several recurring interaction motifs. For example, there are several cases in which a gene (thick
black arrow), instead of activating another gene directly, represses a repressor of the target gene (thick
gray arrows). Such an arrangement can provide a number of possible advantages, including a sharper
activation profile for the target gene, important in defining spatial boundaries between cell types.Modularity and conservation suggest a potential for comparative studies across species (e.g.,pufferfish, mice, humans) to contribute to an understanding of biological function. That is, understand-
ing the role of a certain protein in mice, for example, may suggest a similar role for that same protein if
it is found in humans.These comments should not be taken to mean that functional modules in biological entities arenecessarily simple or static. Biological systems are often made up of elements with multiple functions
interacting in ways that are complex and difficult to separate, and nature exploits multiple linkages that
a human engineer would not tolerate in the design of an artifact.30 For example, a component of onemodule may (or may not) play a role in a different module at a different time. A moduleÕs functional
behavior may be quantitatively regulated or switched between qualitatively different functions by
chemical signals from other modules. Despite these important differences between biological modules
and the modules that constitute humanly engineered artifacts, the notion of a collection of parts that can
be counted on to perform a given functionÑthat is, a moduleÑis meaningful from an analytical per-
spective and our understanding of that function.6.2.4  Robustness in Biological Entities
Robustness is one of the characteristics of biological systems that is most admired and most desiredfor engineered systems. Especially as compared to software and information systems, which are notori-
ously brittle, biological systems maintain functionality in the face of a range of perturbations. More
traditional hardware engineering, however, has studied the questions of robustness (under various
names including fault-tolerance and control systems). Applying the analytical techniques developed in
engineering to studying the mechanics of robustness in biology, the logic goes, might reveal new
insights not only about biology, but about robust system design.In biology, the term robustness is used in many different ways in different subfields, including thepreservation of species diversity, a measure of healing, comprehensibility in the face of incomplete
information, continuity of evolutionary lineages, phenotypic stability in development, cell metabolic
stability in the face of stochastic events, or resistance to point mutations.31 Its most general usage,6.E.H. Davidson, J.P. Rast, P. Oliveri, A. Ransick, C. Calestani, C.H. Yuh, T. Minokawa, et al., ÒA Provisional
Regulatory Gene Network for Specification of Endomesoderm in the Sea Urchin Embryo,Ó Developmental Biology246(1):162-190, 2002.7.J.P. Rast, R.A. Cameron, A.J. Poustka, and E.H. Davidson, ÒBrachyury Target Genes in the Early Sea Urchin
Embryo Isolated by Differential Macroarray Screening,Ó Developmental Biology 246(1):191-208, 2002.8.P. Oliveri, D.M. Carrick, and E.H. Davidson, ÒA Regulatory Gene Network That Directs Micromere Specifi-
cation in the Sea Urchin Embryo,Ó Developmental Biology 246(1):209-228, 2002.SOURCE: Figure from M. Levine and E.H. Davidson, ÒGene Regulatory Networks for Development,Ó Proceedingsof the National Academy of Sciences 102(14):4936-4942, 2005, available at http://www.pnas.org/cgi/content/full/102/14/4936. Copyright 2005 National Academy of Sciences.30This is not to say that human-engineered artifacts are not affected by their origins. ÒCapture by historyÓ characterizes many
human artifacts as well, but likely not as strongly. For more discussion of these points, see D. Norman, 1998, cited in Footnote 16.31D.C. Krakauer, ÒRobustness in Biological SystemsÑA Provisional Taxonomy,Ó Complex Systems Science in Biomedicine, T.S.Deisboeck, J.Y. Kresh, and T.B. Kepler, eds., Kluwer, New York, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.218CATALYZING INQUIRYhowever, refers to the ability of a structure or process to persist in the face of perturbations of internalcomponents or the environment. Those perturbations might include outright component failure, unex-
pected behavior from components or other cooperating systems, stochastic changes in chemical concen-
trations or reaction rates, mutations, or the motion of external biochemical parameters. These sorts of
perturbations, such as stochastic changes of molecular concentrations, are intrinsic to the nature of
biology, from the molecular scale to the ecological.A robust response to these perturbations generally consists of one of three types: (1) parameterinsensitivity, meaning that a robust process does not depend on a single ideal value of an input; (2)
graceful degradation, in which the level of functionality of the system is indeed lessened by component
failures, but it continues to function; and (3) adaptation, in which internal components reconfigure to
react to a change to maintain the same level of functionality.32Kitano notes that robustness is attained in biological systems by using mechanisms well known tohuman engineers. He describes four mechanisms or approaches to biological robustness:331.System control mechanisms such as negative-feedback and feed-forward control;

2.Redundancy, whereby multiple components with equivalent functions are introduced for backup;

3.Structural stability, where intrinsic mechanisms are built to promote stability; and

4.Modularity, where subsystems are physically or functionally insulated so that failure in one
module does not spread to other parts and lead to system-wide catastrophe.Kitano then notes that these approaches used in engineering systems are also found in biologicalsystems, pointing out that Òredundancy is seen at the gene level, where it functions in control of the cell
cycle and circadian rhythms, and at the circuit level, where it operates in alternative metabolic path-
ways in E. coli.Ó Furthermore, engineering approaches have proven to be a useful lens when investigat-ing biological robustness.For example, Barkai and Leibler34 established a model (later confirmed experimentally) to explainperfect robust adaptation in bacterial chemotaxis, or the ability of bacteria to move toward increased
concentrations of certain ligands. It had long been known that the mechanism responsible for this ability
had several key attributes, among them a high sensitivity to changes in chemical concentration, together
with an ability to adapt to the absolute level of that concentration. Working with the known molecular
makeup of these cells (e.g., the receptors, kinases, and diffusible messenger proteins), Barkai and Leibler
showed that when varied separately, many of the rate constants (such as molecular concentrations of
elements of the signaling network or reaction rates) could be varied by orders of magnitude without
affecting the magnitude of the response.35Later work by Yi et al. used the mathematics of control systems to show how the Barkai-Leiblermodel was a special case of integral feedback control, a well-studied approach of control theory.36 Inaddition to control theory (including feedback and feed-forward control), many other engineering
approaches are found in biological systems, including redundancy, modularity, purging (quickly elimi-
nating failing components), and spatial compartmentalization.3732H. Kitano, ÒSystems Biology: A Brief Overview,Ó Science 295(5560):1662-1664, 2002. Available at http://www.sciencemag.org/cgi/content/abstract/295/5560/1662.33H. Kitano, ÒSystems Biology,Ó 2002.34N. Barkai and S. Leibler, ÒRobustness in Simple Biochemical Networks,Ó Nature 387(6636):913-917, 1997.35 However, the mechanism does not account for the full dynamic range of the sensor patches at a molecular level. (It may bethat some sort of emergent property of the sensor patch as a whole, as opposed to some property of the individual sensor
complexes, is necessary to obtain the full dynamic range. See, for example, T.S. Shimizu, S.V. Aksenov, and D. Bray, ÒA SpatiallyExtended Stochastic Model of the Bacterial Chemotaxis Signaling Pathway,Ó Journal of Molecular Biology 329(2):291-309, 2003.)36T.M. Yi, Y. Huang, M.I. Simon, and J. Doyle, ÒRobust Perfect Adaptation in Bacterial Chemotaxis Through Integral FeedbackControl,Ó Proceedings of the National Academy of Sciences 97(9):4649-4653, 2000.37D.C. Krakauer, ÒRobustness in Biological Systems,Ó 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY219Kitano makes the point that robustness is a property of an entire system;38 it may be that noindividual component or process within a system would be robust, but the system-wide architecture
still provides robust behavior. This presents a challenge for analysis, since elucidating such behaviors
can be counterintuitive and computationally demanding.39 In one such example, von Dassow and
colleagues investigated the development of striped patterns in Drosophila.40 They computationally mod-eled a network of interactions between genes and regulatory proteins active during embryogenesis and
explored the parameter space to see which sets of parameters produced stable striping. In their first
attempt, they were unable to reproduce such behavior computationally. However, once they added two
more molecular events and their interactions to the network, a surprisingly high proportion of the
randomly chosen parameters produced the desired results. This strongly implies that such a network,
taken as a whole, is a robust developmental module, able to produce a particular effect despite wide
variation in reaction parameters.In a refinement to that work, Ingolia investigated the architecture of that network to attempt todetermine the structural sources of such robust behavior.41 He determined that the source of the robust-ness at the network level was a pair of positive feedback loops of gene expression, which led to cells
being forced to one of two stable states (bistability). That is, small perturbations or changes in certain
parameters would necessarily result in individual cells reaching one of two states. Ingolia showed that
such bistability, at both an individual cell level and a network level, is an important architectural
property leading to robust behavior and that the latter is in fact a consequence of the former. Moreover,
it is this bistability that is responsible for the ability of the network to maintain a fixed pattern of gene
expression even in the face of cell division and growth.42Robustness comes at a cost of increased complexity. The simplest bacteria can survive only withinnarrow ranges of environmental parameters, while more complex bacteria, such as E. coli (with agenome an order of magnitude larger than mycoplasma), can withstand more severe environmental
fluctuations.43 This increased complexity can in turn be the root of cascading failures, if the elements ofthe network responsible for the adaptive response fail. This implies that increased robustness of a
certain aspect or element of a system with respect to a certain perturbation may come at the cost of
increased vulnerability in a different aspect or element or to a different attack.Robustness can also serve as a signpost for discovering the details of biological function. Althoughthere may be a prohibitively large number of ways that a genetic network could produce a given result,
for example, only a few of those ways are likely to do so robustly. Knowledge of the robust qualities of
a biological system, coupled with theoretical or simulated analysis of networks, could aid in reverse
engineering the system to determine its actual configuration.44An open and intriguing question is the relationship between robustness and evolution. Becauserobustness is the quality of maintaining stability, in some sense it stands as a potential inhibitor to
evolution, for example, by masking the effects of point mutations. And yet robust modules or organ-
isms are more likely to survive, and thus pass on into succeeding generations. How does robustness
evolve? How do robust systems evolve? One engineering approach to this problem is to consider
biological systems as sets of components interacting through protocols,45 with one critical measure of a38H. Kitano, ÒSystems Biology,Ó 2002. Available at http://www.sciencemag.org/cgi/content/abstract/295/5560/1662.39A.D. Lander, ÒA Calculus of Purpose,Ó PLoS Biology2(6):e164, 2004.40G. von Dassow, E. Meir, E.M. Munro, and G.M. Odell, ÒThe Segment Polarity Network Is a Robust Developmental Module,ÓNature 406(6792):188-192, 2000.41N.T. Ingolia, ÒTopology and Robustness in the Drosophila Segment Polarity Network,Ó PLoS Biology2(6):e123, 2004.
42A.D. Lander, ÒA Calculus of Purpose,Ó 2004.43J.M. Carlson and J. Doyle, ÒComplexity and Robustness,Ó Proceedings of the National Academy of Sciences 99(Suppl. 1):2538-2545, 2002.44U. Alon, ÒBiological Networks: The Tinkerer as an Engineer,Ó Science 301:1866-1867, 2003.45M.E. Csete and J.C. Doyle, ÒReverse Engineering of Biological Complexity,Ó Science 295:1664-1669, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.220CATALYZING INQUIRYgood protocol being its ability to support both robustness and evolvability, a key consideration intechnical protocols of human engineering such as TCP/IP.6.2.5  Noise in Biological Phenomena
46As one illustration of how engineering disciplines might shed light on biological mechanism, con-sider the opposition of robustness and noise in biological phenomena. Biological organisms exhibit high
degrees of robustness in the face of changing environments. Engineered artifacts designed by human
beings have used mechanisms such as negative feedback to provide stability, redundancy to provide
backup, and modularity for the isolation of failures to enhance robustness. As the discussion below
indicates, these mechanisms are used for these purposes in biological organisms, as well.47In a biological context, noise can take the form of fluctuations in quantities such as reaction rates,concentrations, spatial distributions, and fluxes. In addition, fluctuations may also occur at the molecu-
lar level. However, despite the noise inherent in the internal environment of a cell, cells operateÑoften
robustly and quite stablyÑwithin strict parameters, and robustness has been hypothesized as an intrin-
sic property of intracellular networks. (For instance, the chemotaxis pathway in E. coli functions over awide range of enzymatic activities and protein concentrations.48 Robustness is also illustrated in somedevelopmental processes49 and phage lambda regulation.50) This robustness suggests that cells use andreject noise in a systematic manner.For the analysis of biological noise, much of the analysis originally derived from signal processingand control theory is applicable.51 Indeed, pathways can be regarded as analog filters and classified interms of frequency response, where the differences between filtering electronic noise and filtering
biological noise are reflected only in the details of the underlying mechanisms rather than in high-level
abstractions of filtering theory.Cascades and relays such as two-component systems and the mitogen-activated protein kinasepathway function as low-pass filters (i.e., filters that attenuate high-frequency noise).52 As a generalrule, longer cascades are more effective at reducing noise. However, because noise arises in the pathway
itself, the amount of internally generated noise increases with cascade lengthÑsuggesting that there is
an optimal cascade length for attenuating noise.53It is not surprising that low-pass filters are components of biological systems. As noted above,biological systems operate homeostatically,54 and the essential principle underlying homeostasis is thatof negative feedback. From the standpoint of signal processing, a negative feedback loop functions as a
low-pass filter.46Section 6.2.5 is based on and incorporates several excerpts from C.V. Rao, D.M. Wolf, and A.P. Arkin, ÒControl, Exploitationand Tolerance of Intracellular Noise,Ó Nature 420(6912):231-237, 2002.47H. Kitano, ÒSystems Biology: A Brief Overview,Ó Science 295(5560):1662-1664, 2002. Available at http://www.sciencemag.org/cgi/content/abstract/295/5560/1662.48N. Barkai and S. Leibler, ÒRobustness in Simple Biochemical Networks,Ó Nature 387:913-917, 1997; U. Alon, M.G. Surette, N.Barkai and S. Leibler, ÒRobustness in Bacterial Chemotaxis,Ó Nature 397:168-171, 1999. (Cited in Rao et al., 2002.)49G. von Dassow, E. Meir, E.M. Munro, and G.M. Odell, ÒThe Segment Polarity Network Is a Robust Developmental Module,ÓNature 406:188-192, 2000; E. Meir, G. von Dassow, E. Munro, and G.M. Odell, ÒRobustness, Flexibility, and the Role of LateralInhibition in the Neurogenic Network,Ó Current Biology 12:778-786, 2002. (Cited in Rao et al., 2002.)50J.W. Little, D.P. Shepley, and D.W. Wert, ÒRobustness of a Gene Regulatory Circuit,Ó EMBO Journal 18:4299-4307, 1999.51A.P. Arkin, ÒSignal Processing by Biochemical Reaction Networks,Ó pp. 112-144, Self-organized Biological Dynamics and Non-linear Control, J. Walleczek, ed., Cambridge University Press, London, 2000; M. Samoilov, A. Arkin, and J. Ross, ÒSignal Process-ing by Simple Chemical Systems,Ó Journal of Physical Chemistry 106:10205-10221, 2002. (Cited in Rao et al., 2002.)52P.B. Detwiler, S.A. Ramanathan, A. Sengupta, and B.I. Shraiman, ÒEngineering Aspects of Enzymatic Signal Transduction:Photoreceptors in the Retina,Ó Biophysical Journal 79(6):2801-2817, 2000. (Cited in Rao et al., 2002.)53M. Thattai and A.Van Oudenaarden, ÒAttenuation of Noise in Ultrasensitive Signaling Cascades,Ó Biophysical Journal82(6):2943-2950, 2002. (Cited in Rao et al., 2002.)54Homeostasis is the property of a system that enables it to respond to changes in its environment in such a way that it tends tomaintain its original state.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY221A second useful construct from signal processing is the bandpass filter, which is based on thecontrol theory notion of integral feedback. Integral feedback is a kind of negative feedback that ampli-
fies intermediate frequencies and attenuates low and high frequencies. A biological instantiation of
integral feedback is contained in bacterial chemotaxis.55In addition to the filters described above, other mechanisms attenuate noise in systems. Theseinclude the following:¥Redundancy. Noise in a single channel might be misinterpreted as a genuine signal. However,redundancyÑin the form of multiple channels serving the same functionÑcan help to minimize the
likelihood of such an occurrence. In a biological context, redundancy has been demonstrated in mecha-
nisms such as gene dosage and parallel cascades,56 which attenuate the effects of noise by increasing thelikelihood of gene expression or establishing a consensus from multiple signals.¥Checkpointing. Noise can interfere with the successful completion of various biological operationsthat are essential in a pathway. However, a checkpoint can ensure that each step in a pathway is
completed successfully before proceeding with the next step. Such checkpoints have been characterized
in the cell cycle and flagellar biosynthesis.57¥Proofreading. Noise can introduce errors into a process. But error-correcting mechanisms canreduce this effect of noise, as is the case of kinetic proofreading in protein translation.58A final, and surprising, mechanism is that complexity itself in some cases can be implicated in therobustness of an organism against noise. In 1942, Waddington noted the stability of phenotypes (from
the same species) against a backdrop of considerable genetic variation, a phenomenon known as canali-
zation.59 In principle, such stability could result from explicit genetic control of phenotype features,such as the number of fingers on a hand or the placement of wings on an insectÕs body. However, Siegal
and Bergman modeled the developmental process responsible for the emergence of such features as a
network of interacting transcriptional regulators and found that the network constrains the genetic
system to produce canalization.60 Furthermore, the extent of canalization, measured as the insensitivityof a phenotype to changes in the genotype (i.e., to mutations), depends on the complexity of the
network, such that more highly connected (i.e., more complex) networks evolve to be more canalized.
(Box 6.5 provides more details.)Consider that noise can also make positive contributions to biological systems. For example, it iswell known from the agricultural context that monocultures are less robust than ecosystems that
involve multiple speciesÑthe first can be wiped out by a disease that targets the specific crop in
question, whereas the second cannot. Thus, some degree of variation in a populating species is
desirable, and noise is one mechanism for introducing variation that results in population heteroge-55The size of a single bacterium is so small that the bacterium is unable to sense a spatial gradient across the length of its body.Thus, to sense a spatial gradient, the bacterium moves around and senses chemical concentrations in different locations at
different times; the result is a motion bias toward attractants. See T.M. Yi, Y. Huang, M.I. Simon, and J. Doyle, ÒRobust PerfectAdaptation in Bacterial Chemotaxis Through Integral Feedback Control,Ó Proceedings of the National Academy of Sciences 97(9):4649-4653, 2000. (Cited in Rao et al., 2002.)56H.H. McAdams and A. Arkin, ÒItÕs a Noisy Business! Genetic Regulation at the Nanomolar Scale,Ó Trends in Genetics 15(2):65-69, 1999; D.L. Cook, A.N. Gerber, and S.J. Tapscott, ÒModeling Stochastic Gene Expression: Implications for Haploinsufficiency,ÓProceedings of the National Academy of Sciences 95(26):15641-15646, 1998. (Cited in Rao et al., 2002.)57L.H. Hartwell and T.A. Weinert, ÒCheckpoints: Controls That Ensure the Order of Cell Cycle Events,Ó Science 246(4930):629-634, 1989. (Cited in Rao et al., 2002.)58M.V. Rodnina and W. Wintermeyer, ÒRibosome Fidelity: tRNA Discrimination, Proofreading and Enduced Fit,Ó Trends inBiochemical Science 26(2):124-130, 2001. (Cited in Rao et al., 2002.)59C.H. Waddington, ÒCanalization of Development and the Inheritance of Acquired Characters,Ó Nature 150:563-565, 1942.60M.L. Siegal and A. Bergman, ÒWaddingtonÕs Canalization Revisited: Developmental Stability and Evolution,Ó Proceedings ofthe National Academy of Sciences 99(16):10528-10532, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.222CATALYZING INQUIRYneity and diversity. For example, noise (in the form of molecular fluctuations) introduced into thegenetic circuit governing development in phage lambda can cause an initially homogeneous popula-
tion to separate into lytic and lysogenic populations.61 (In this case, the basic mechanism involves
Box 6.5Canalization and the Connectivity of Transcriptional Regulatory NetworksTo explore the possibility that genetic canalization may be a by-product of other selective forces, . . . [we start with]the model of A. Wagner, who treats development as the interaction of a network of transcriptional regulatory genes,phenotype as the equilibrium state of this network, and fitness as a function of the distance between an individualÕsequilibrium state and the optimum state. . . . Evolution in the model [a generalized version of WagnerÕs] consists of
three phases: mating, development, and selection. Mating and selection are modeled in accord with traditionalpopulation-genetic approaches. . . . [To handle development] one can represent a network of transcriptional regula-tors by a state vector containing the concentration of each gene product and a matrix, the entries of which represent
the effects of each gene product on the expression of each gene. Entries may be either positive (activating) ornegative (repressing) and may differ in magnitude. Zero elements in the matrix represent the absence of interactionbetween the given gene product and gene. The developmental process is then fully described by a set of nonlinear
coupled difference equations. . . . Wagner draws an analogy between the rows of the interaction matrix and theenhancer regions of the genes in the network and further justifies the biological realism of this type of model byreference to data from actual genetic networks. An important assumption in the model, also justified by A. Wagner,
is that functional genetic networks will reach a stable equilibrium gene-expression state, and that unstable networksreflect, in a sense, the failure of development. Thus, in his model and ours, development itself enforces a kind ofselection, because we require that the network of regulatory interactions produce a stable equilibrium gene-expres-
sion state (its ÒphenotypeÓ), whose distance to an optimum state can then be measured during the selection phase.. . . We report here the results of numerical simulations of our model of an evolving developmental-genetic system.We demonstrate an important, perhaps primary, role for the developmental process itself in creating canalization, inthat insensitivity to mutation evolves even when stabilizing selection is absent. We go on to demonstrate that thecomplexity of the network is a key factor in this evolutionary process, in that networks with a greater proportion of
connections evolve greater insensitivity to mutation.. . . One is led to wonder whether the evolution of canalization under no stabilizing selection on the gene-expressionpattern is an artifact of the modeling framework or whether it represents a finding of real biological significance. Weargue that the latter is true on a number of counts. To begin, we acknowledge that it is difficult to envision a scenarioin nature in which the stability of a developmental module is required, but the phenotype produced by that module
is not subject to selection. One situation in which this condition may hold is when a species colonizes a new territorywith virtually unlimited resources, so selection is only for those that develop to reproduce. Furthermore, even if sucha scenario does not pertain, the conceptual decomposition of stabilizing selection into selection for an optimum and
selection for developmental stability is important. Thus, even in scenarios in which members of a population aresubject to selection for an optimum, the evolution of canalization may proceed because of the underlying selectionfor stability of the developmental outcome. Our results suggest that this underlying selection can occur very fast.
Because others have argued that the evolution of canalization under stabilizing selection may be slow, developmen-tal stability may therefore be the dominant force in the evolution of canalization.SOURCE: Reprinted by permission from M.L. Siegal and A. Bergman, ÒWaddingtonÕs Canalization Revisited: Developmental Stability andEvolution,Ó Proceedings of the National Academy of Sciences 99(16):10528-10532, 2002. Copyright 2002 National Academy of Sciences.(References and figures are omitted above and can be found in the original article.)61A. Arkin, J. Ross, and H.H. McAdams, ÒStochastic Kinetic Analysis of Developmental Pathway Bifurcation in Phage Lambda-infected Escherichia coli Cells,Ó Genetics 149(4):1633-1648, 1998. (Cited in Rao et al., 2002.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY223two antagonistic feedback loops that create a switch and molecular fluctuations that partition theinitial population stochastically.)Noise can be used to enhance a signal when certain nonlinear effects are present, as demonstratedby the phenomenon of stochastic resonance.62 Stochastic resonance is found in many biological sys-
tems, including the electroreceptors of paddlefish,63 mechanoreceptors in the tail fins of crayfish,64 andhair cells in crickets.65 A similar phenomenon can potentially increase sensitivity in certain signalingcascades.66Finally, noise can be useful for introducing stability. The network that controls circadian rhythmsconsists of multiple, complex, interlocking feedback loops. Both deterministic and stochastic mecha-
nisms for noise resistance in circadian rhythms have been explored,67 and it turns out that stochasticmodels are able to produce regular oscillations when the deterministic models do not,68 suggesting thatthe regulatory networks may utilize molecular fluctuations to their advantage.The discussion above suggests that biological robustness is in some ways a problem of controllingthe effects of noise and in other ways one of exploiting those effects. Considerations of noise and
robustness thus offer insight into the design and function of intracellular networks.69 That is, thefunction of an intracellular network may require specific regulatory and information structures, and
certain design features are necessary for a stable network phenotype.Finally, note that mechanisms of the sorts described above do not generally function in isolation,but rather interact in complex networks involving multiple feedback loops, and the resulting networks
can produce diverse phenomena, including switches, memory, and oscillators.70 Such coupling also hasan important analytical consequenceÑnamely, that the composite behavior of multiple coupled mecha-
nisms is much more difficult to predict than the behavior of individual components. To analyze mul-
tiple coupled systems, computational models are highly useful.6.3  A COMPUTATIONAL METAPHOR FOR BIOLOGY
In addition to the abstractions described above, computing and computer science can also providelife scientists with a rich source of language, metaphors, and analogies with which to describe biological
phenomena and insights from a computational perspective. These linguistic and cognitive aspects may
well make it easier for insights originating in computing to be made relevant to biology, and thus62L. Gammaitoni, P. Hanggi, P. Jung, and F. Marchesoni, ÒStochastic Resonance,Ó Reviews of Modern Physics 70:223-287, 1998.(Cited in Rao et al., 2002.)63D.F. Russell, L.A. Wilkens, and F. Moss, ÒUse of Behavioural Stochastic Resonance by Paddle Fish for Feeding,Ó Nature402(6759):291-294, 1999. (Cited in Rao et al., 2002.)64J.K. Douglass, L. Wilkens, E. Pantazelou, and F. Moss, ÒNoise Enhancement of Information Transfer in Crayfish Mechanore-ceptors by Stochastic Resonance,Ó Nature 365(6444):337-340, 1993. (Cited in Rao et al., 2002.)65J.E. Levin and J.P. Miller, ÒBroadband Neural Encoding in the Cricket Cercal Sensory System Enhanced by StochasticRresonance,Ó Nature 380(6570):165-168, 1996. (Cited in Rao et al., 2002.)66J. Paulsson, O.G. Berg, and M. Ehrenberg, ÒStochastic Focusing: Fluctuation-enhanced Sensitivity of Intracellular Regula-tion,Ó Proceedings of the National Academy of Sciences 97(13):7148-7153, 2000. (Cited in Rao et al., 2002.)67N. Barkai and S. Leibler, ÒCircadian Clocks Limited by Noise,Ó Nature 403(6767):267-268, 2000; D. Gonze, J. Halloy, and A.Goldbeter, ÒRobustness of Circadian Rhythms with Respect to Molecular Noise,Ó Proceedings of the National Academy of Sciences99(2):673-678, 2002; P. Smolen, D.A. Baxter, and J.H. Byrne, ÒModeling Circadian Oscillations with Interlocking Positive andNegative Feedback Loops,Ó Journal of Neuroscience 21(17):6644-6656, 2001. (Cited in Rao et al., 2002.)68J.M. Vilar, H.Y. Kueh, N. Barkai, and S. Leibler, ÒMechanisms of Noise Resistance in Genetic Oscillators,Ó Proceedings of theNational Academy of Sciences 99(9):5988-5992, 2002. (Cited in Rao et al., 2002.)69M.E. Csete and J.C. Doyle, ÒReverse Engineering of Biological Complexity,Ó Science 295(5560):1664-1669, 2002; M. Morohashi,et al., ÒRobustness as a Measure of Plausibility in Models of Biochemical Networks,Ó Journal of Theoretical Biology 216(1):19-30,2002; L.H. Hartwell, J.J. Hopfield, S. Leibler, and A.W. Murray, ÒFrom Molecular to Modular Cell Biology,Ó Nature 402(6761Suppl):C47-C52, 1999. (Cited in Rao et al., 2002.)70M.B. Elowitz and S. Leibler, ÒA Synthetic Oscillatory Network of Transcriptional Regulators,Ó Nature 403(6767):335-338,2000; T.S. Gardner, C.R. Cantor, and J.J. Collins, ÒConstruction of a Genetic Toggle Switch in Escherichia coli,Ó Nature 403(6767):339-342, 2000. (Cited in Rao et al., 2002.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.224CATALYZING INQUIRYinformation abstractions can be used to communicate about or to explain biological processes andconcepts. Consider, for example, the Jacob and Monod description of the genome as a Ògenetic pro-
gram,Ó capable of controlling its own execution.71 (Conversely, biological metaphors and languagemight offer analogous benefits to computing, which is the subject of Chapter 8.) At the same time,
poorly chosen metaphors can limit understanding by carrying over misleading or irrelevant details. For
example, the Ògenetic programÓ metaphor described above might lead one to think of protein synthesis
as being executed one instruction at a time (as most computer programs would be), obscuring the
parallel and interconnected nature of the genetic protein synthesis network.72The use of a metaphor (to look at a problem in field A through the lens of field B) invites one toapply insights from field B to the problem in field A. Metaphors are often (indeed, almost always)
imprecise and somewhat vague, because they are not specific about which insights from field B are
relevant to field A. They can nevertheless be useful, because they constitute an additional source of
insight and new ways of thinking to be brought to bear on field A that might not otherwise be available
in the absence of those metaphors. Moreover, field BÑas a disciplineÑconstitutes an existence proof
that the insights in question can in fact be part of an intellectually coherent whole.Consider, for example, extending the notion of the Ògenetic program.Ó In some sense, the DNA sequencecan be analogized to the binary code of a program. However, in many real computer programs, a program
structure or architecture or individual components may be apparent from representing the program in its
source code form, where things such as variable declarations and subroutines make manifestly obvious what
is obscured in the binary representation. Calling sequences between program and subprogram define pro-
gram interfaces and protocols for how different components of a program may communicateÑdata defini-
tions, formats, and semantics, for instance. Thus, it may be meaningful to inquire about the analogous things
in biology, and indeed, a gene contained in DNA might well be one analogue of a subprogram or the action
potential in neuroscience one analogue of a communications protocol.Another analogy can be drawn between the evolution of computing and the biological transitionfrom single-cell organisms to multicell organisms. Multicellular life exploits four broad strategies: col-
laboration between highly specialized cells; communication by polymorphic messages; self, defined by
a stigmergic structure; and self, protected by programmed cell death. These strategies are rare in single-
cell organisms but nearly universal in multicellular organisms, and evolved before or coincident with
the emergence of multicellular life. As described in Table 6.1, each of these strategies may be analogous
to trends seen in computing today.To illustrate how the use of a computational metaphor can provide insight and lead to deeper explora-tion, note that cellular processes are concurrent (i.e., changes in the surrounding environment can trigger the
execution of many parallel processes); operate at many levels including the submolecular, molecular, subcel-
lular, and cellular; and involve relationships among many subcellular and molecular objects. Computer
scientists have devised a number of formalisms that are capable of representing such processes, and Kam et
al.73 modeled aspects of T-cell activation using the formalism of Statecharts,74 as they have been adapted tothe framework of object-oriented modeling.75 Because the object-oriented Statechart approach supports71F. Jacob and J. Monod, ÒGenetic Regulatory Mechanisms in the Synthesis of Proteins,Ó Journal of Molecular Biology 3:318-356,1961.72E.F. Keller, Making Sense of LifeÑExplaining Biological Developments with Models, Metaphors, and Machines, Harvard UniversityPress, Cambridge, MA, 2003.73N. Kam, I.R. Cohen, and D. Harel, ÒThe Immune System as a Reactive System: Modeling T Cell Activation with Statecharts,ÓProceedings of a Symposium on Visual Languages and Formal Methods (VLFMÕ01), part of IEEE Symposium on Human-centricComputing (HCCÕ01), 2001, pp. 15-22.74D. Harel, ÒStatecharts: A Visual Formalism for Complex Systems,Ó Science of Computer Programming 8:231-274, 1987. (Cited inKam et al., Ò The Immune System as a Reactive System,Ó 2001.)75G. Booch, Object-Oriented Analysis and Design, with Applications, Addison-Wesley, Menlo Park, CA, 1994; D. Harel and E.Gery, ÒExecutable Object Modeling with Statecharts,Ó Computer, 31-42, 1997; J. Rumbaugh, M. Blaha, W. Premerlani, F. Eddy, andW. Lorensen, Object-Oriented Modeling and Design, Prentice Hall, Englewood Cliffs, NJ, 1991. (Cited in Kam et al., 2001.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.A COMPUTATIONAL AND ENGINEERING VIEW OF BIOLOGY225concurrency, multilevel description, and object orientation, Kam et al. constructed a T-cell simulation thatpresents its results by displaying animated versions of the modelÕs Statecharts.A second example is provided by the work of Searls. It is a common, if not inescapable, metaphorthat DNA represents the language of life. In the late 1980s and early 1990s, David B. Searls and collabo-
rators made the metaphor much more concrete, applying formal language theory to the analysis of
nucleic acid sequences.76 Linguistics theory considers four levels of interpretation of text: lexical (theTABLE 6.1Principles of Operation for Multicellular Organisms and Networked Computing
PrincipleMulticellular OrganismsNetworked Computing
CollaborationCells in biofilms specializeToday most computers retain a
between highlytemporarily according to ÒquorumÓlarge repertoire of unused general
specialized cellscues from neighbors. Cells inbehavior susceptible to viral or
ÒtrueÓ multicellular organismsworm attack. Biology suggests
permanently specializethat more specialization and less
(differentiate) during development.monoculture would be

Loss of differentiation is an earlyadvantageous (although market
sign of cancer.forces may oppose this).
CommunicationCells in multicelled organismsExecutable code is the analogue of
by polymorphiccommunicate with each other viaDNA. Most PCs permit easy, and
messagesmessenger 
molecules, never DNA.hidden, download of executable
The ÒmeaningÓ of cell-to-cellcode (Active-X or even exe).
messages is determined by theHowever, importing executable
receiving cell, not the sender.code is well known to create
security risks, and secure systemsminimize or eliminate thiscapability.ÒSelfÓ defined byMulticelled organisms and biofilmsDetermination of self is largely ad
a stigmergicbuild extracellular stigmergichoc in todayÕs systems. However,

structurestructures (bone, 
shell, or justan organizationÕs intranet is a
slime) that define the persistentstigmergic structure, as are its
self. ÒSelfnessÓ resides as much inpersistent databases.

the extracellular matrix as in the
cells.ÒSelfÓ protectedEvery healthy cell in a multicelledA familiar example in computing
by programmedorganism is prepared to commitis the Blue Screen of Death, which
cell death (PCD)suicide. PCD evolved to deal withis a programmed response
 to anDNA replication errors, viralunrecoverable error. An analogous

infection, and rogue undifferentiatedcomputer should sense its own
cells. PCD reflects a multicellularrogue behavior (e.g., download of
perspectiveÑsacrificing theuncertified code) and disconnect

individual cell for the good of theitself from the network or reboot
multicellular organism.itself periodically to give itself a
clean initial state.SOURCE: Steve Burbeck, IBM, personal communication, October 11, 2004.76D.B. Searls, ÒThe Linguistics of DNA,Ó American Scientist 80:579-591, 1992. Formal language theory is a major subfield ofcomputer science theory; it is based on Noam ChomskyÕs work on linguistics in the 1950s and 1960s, especially the Chomskyhierarchy, a categorization of languages by their inherent complexity. Formal languages are at the heart of parsers and compilers,and there exists a wide range of both theoretic analysis and practical software tools for the production, transformation, andanalysis of text. The main algorithmic tool of language theory is the generative grammar, a series of rules that transforms higher-level abstract units of meaning (such as ÒsentenceÓ or Ònoun phraseÓ) into more concrete potential statements in a given lan-
guage. Grammars can be categorized into regular, context-free, context-sensitive, and recursively enumerable, each of whichrequires more algorithmic complexity to recognize than the level before it.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.226CATALYZING INQUIRYidentification of specific words), syntactic (the grouping of words into grammatically correct phrases),semantic (the assignment of meaning to words and phrases), and pragmatic (the role of a piece of text in
the larger context). These match entirely well to genomic analysis: grouping bases into codons, genes,
the function of the resulting protein, and the role of that protein in the larger molecular system.77Linguistic analyses can reveal or explain relationships between bases that are far apart in a se-quence. For example, an RNA structure called a stem-loop has a palindrome-like sequence, with Watson-
Crick pairs at equal distances away from the center. Traditional probabilistic or pattern-searching
approaches would have some difficulty recognizing this structure, but it is quite simple with a grammar
that produces palindromes. Some sequences of nucleic acids result in ambiguous linguistic interpreta-
tions; while this is a difficulty for computer languages, it represents a strength of biological linguistic
analysis, because these ambiguities correctly represent alternative secondary structures.78This approach has been fruitful for analyzing genetic sequences and characterizing the complexityand structure of genes. GenLang, a software system that employs linguistic approaches, has success-
fully identified tRNA genes, group I introns, protein-encoding genes, and the specification of gene
regulatory elements.79 Other important findings include placing RNA in the Chomsky hierarchy as atleast beyond context-free languages. Finally, the approach provides a powerful tool for understanding
the evolution of nucleic acid sequences; since the first sequences were most likely random (and thus
regular languages), there must be a mechanism that somehow promoted sequence language into more
powerful linguistic categories. This can be seen as an algebraic problem of operational closure, and the
question is, For which string operations are regular languages and context-free languages not closed?8077D.B. Searls, ÒReading the Book of Life,Ó Bioinformatics 17(7):579-580, 2001.78D.B. Searls, ÒThe Language of Genes,Ó Nature 420(6912):211-217, 2002.79D.B. Searls, and S. Dong, ÒA Syntactic Pattern Recognition System for DNA SequencesÓ in Proceedings of the Second Interna-tional Conference on Bioinformatics, Supercomputing, and Complex Genome Analysis, H.A. Lim, J. Fickett, C.R. Cantor, and R.J. Robbins,eds., World Scientific Publishing Co., pp. 89-101, 1993.80D.B. Searls, ÒFormal Language Theory and Biological Macromolecules,Ó Series in Discrete Mathematics and Theoretical Com-puter Science 47:117-140, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION2272277Cyberinfrastructure and Data Acquisition
7.1  CYBERINFRASTRUCTURE FOR 21
ST CENTURY BIOLOGYTwenty-first century biology seeks to integrate scientific understanding at multiple levels of bio-logical abstraction, and it is holistic in the sense that it seeks an integrated understanding of biological
systems through studying the set of interactions between components. Because such an enormous,
data-intensive effort is necessarily and inherently distributed over multiple laboratories and investiga-
tors, an infrastructure is necessary that facilitates the integration of experimental data, enables collabo-
ration, and promotes communication among the various actors involved.7.1.1  What Is Cyberinfrastructure?
Cyberinfrastructure for science and engineering is a term coined by the National Science Founda-tion (NSF) to refer to distributed computer, information, and communication technologies and the
associated organizational facilities to support modern scientific and engineering research conducted on
a global scale. As articulated by the Atkins panel,1 the technology substrate of cyberinfrastructureinvolves the following:¥High-end general-purpose computing centers that provide supercomputing capabilities to the com-munity at large. In the biological context, such capabilities might be used to undertake, for example,
calculations to determine the three-dimensional structure of proteins given their genetic sequence. In
some cases, these computing capabilities could be provided by local clusters of computers; in other
cases, special-purpose hardware; and in still others, computing capabilities on demand from a comput-
ing grid environment.¥Data repositories that are well curated and that store and make available to all researchers largevolumes and many types of biological data, both in raw form and as associated derived products. Such
repositories must store data, of course, but they must also organize, manage, and document these1ÓRevolutionizing Science and Engineering Through Cyberinfrastructure: Report of the NSF Blue-Ribbon Advisory Panel onCyberinfrastructure,Ó 2003, available at http://www.communitytechnology.org/nsf_ci_report/report.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.228CATALYZING INQUIRYdatasets dynamically. They must provide robust search capabilities so that researchers can find thedatasets they need easily. Also, they are likely to have a major role in ensuring the data interoperability
necessary when data collected in one context are made available for use in another.¥Digital libraries that contain the intellectual legacy of biological researchers and provide mecha-nisms for sharing, annotating, reviewing, and disseminating knowledge in a collaborative context. Where
print journals were once the standard mechanism through which scientific knowledge was validated,
modern information technologies allow the circumvention of many of the weaknesses of print. Knowl-
edge can be shared much more broadly, with much shorter lag time between publication and availability.
Different forms of information can be conveyed more easily (e.g., multimedia presentations rich in bio-
logical imagery). One researcherÕs annotations to an article can be disseminated to a broader audience.¥High-speed networks that connect large-scale, geographically distributed computing resources, datarepositories, and digital libraries. Because of the large volumes of data involved in biological datasets,
todayÕs commodity Internet is inadequate for high-end scientific applications, especially where there is a
real-time element (e.g., remote instrumentation and collaboration). Network connections ten to a hundred
times faster than those generally available today are a lower bound on what will be necessary.In addition to these components, cyberinfrastructure must provide software and services to thebiological community. For example, cyberinfrastructure will involve many software tools, system soft-
ware components (e.g., for grid computing, compilers and runtime systems, visualization, program
development environments, distributed scalable and parallel file systems, human computer interfaces,
highly scalable operating systems, system management software, parallelizing compilers for a variety
of machine architectures, sophisticated schedulers), and other software building blocks that researchers
can use to build their own cyberinfrastructure-enabled applications. Services, such as those needed to
maintain software on multiple platforms and provide for authentication and access control, must be
supported through the equivalent of help-desk facilities.From the committeeÕs perspective, the primary value of cyberinfrastructure resides in what it en-ables with respect to data management and analysis. Thus, in a biological context, machine-readable
terminologies, vocabularies, ontologies, and structured grammars for constructing biological sentences
are all necessary higher-level components of cyberinfrastructure as tools to help manage and analyze
data (discussed in Section 4.2). High-end computing is useful in specialized applications but, by com-
parison to tools for data management and analysis, lacks broad applicability across multiple fields of
biology.7.1.2  Why Is Cyberinfrastructure Relevant?
The Atkins panel noted that the lack of a ubiquitous cyberinfrastructure for science and engineeringresearch carries with it some major risks and costs. For example, when coordination is difficult, re-
searchers in different fields and at different sites tend to adopt different formats and representations of
key information. As a result, their reconciliation or combination becomes difficult to achieveÑand
hence disciplinary (or subdisciplinary) boundaries become more difficult to break down. Without sys-
tematic archiving and curation of intermediate research results (as well as the polished and reduced
publications), useful data and information are often lost. Without common building blocks, research
groups build their own application and middleware software, leading to wasted effort and time.As a field, biology faces all of these costs and risks. Indeed, for much of its history, the organizationof biological research could reasonably be regarded as a group of more or less autonomous fiefdoms.
Unifying biological research into larger units of aggregation is not a plausible strategy today, and so the
federation and loose coordination enabled by cyberinfrastructure seem well suited to provide the major
advantages of integration while maintaining a reasonably stable large-scale organizational structure.Furthermore, well-organized, integrated, synthesized information is increasingly valuable to bio-logical research (Box 7.1). In an era characterized by data-intensive research observations, collecting,Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION229Box 7.1A Cyberinfrastructure View: Envisioning and Empowering Successes for21st Century Biological SciencesCreating and sustaining a comprehensive cyberinfrastructure (CI; the pervasive applications of all domains ofscientific computing and information technology) are as relevant and as required for biology as for any sci-
ence or intellectual endeavor; in the advances that led to todayÕs opportunity, the National Science Founda-tionÕs Directorate for Biological Sciences (NSF BIO) made numerous, ad hoc contributions, and now canintegrate its efforts to build the complete platforms needed for 21st century biology. Doing so will accelerate
progress in extraordinary ways.The time has arrived for creating a CI for all of the sciences, for research and education, and NSF will lead theway. NSF BIO must co-define the extent and fine details of the NSF structure for CI, which will involve majorinternal NSF partnerships and external partnerships with other agencies, and will be fully international inscope.Only the biological sciences have seen advances as remarkable, sustained, and revolutionary as those incomputer and information sciences. Only in the past few years has the world of computing and information
technology reached the level of being fully applicable to the wide range of cutting-edge themes characteristicof biological research. Multiplying the exponentials (of continuing advances in computing and bioscience)through deep partnerships will inevitably be exciting beyond any anticipation.The stretch goals for the biological sciences community include both community-level involvement andrealization of the complete spectrum of CI, namely, people and training, instrumentation, collaborations,
advanced computing and networking, databases and knowledge management; and analytical methods (mod-eling and simulation).NSF BIO must:¥Invest in people;¥Ensure science pull, technology push;¥Stay the course;¥Prepare for the data deluge;¥Enable science targets of opportunity;¥Select and direct the technology contributions; and¥Establish national and international partnerships.The biology community must decide how it can best interact with the quantitative science community, whereand when to intersect with computational sciences and technologies, how to cooperate on and contribute to
infrastructure projects, and how NSF BIO should partner administratively. An implementation meeting, aswell as briefings to the community through professional societies, will be essential.For NSF BIO to underestimate the importance of cyberinfrastructure for biology, or fail to provide fuel over theentire journey, would severely retard progress and be very damaging for the entire national and internationalbiological sciences community.SOURCE: Adapted from Subcommittee on 21st Century Biology, NSF Directorate for Biological Sciences Advisory Committee, Building aCyberinfrastructure for the Biological Sciences 2005 and Beyond: A Roadmap for Consolidation and Exponentiation, a workshop report, July14-15, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.230CATALYZING INQUIRYmanaging, and connecting data from various modalities and on multiple scales of biological systems,from molecules to ecosystems, are essential to turn that data into information. Each biological subdisci-
pline also now requires the tools of information technology to probe that information, to interconnect
experimental observations and modeling, and to contribute to an enriched understanding or knowl-
edge. The expansion of biology into discovery and synthetic analysis, that is, genome-enabled biology
and systems biology as well as the hardening of many biological research tools into high-throughput
pipelines, serves also to drive the need for cyberinfrastructure in biology.Box 7.2 illustrates existing efforts in the development of cyberinfrastructure for biology that arerelevant. Note that the examples span a wide range of subfields within biology, including proteomics
(PDB), ecology (NEON and LTER), neuroscience (BIRN), and biomedicine (NBCR).Data repositories and digital libraries are discussed in Chapter 3. The discussion below focusesprimarily on computing and networking.Box 7.2Examples of Possible Elements of a Cyberinfrastructure for BiologyPacific Rim Application and Grid Middleware AssemblyThe Pacific Rim Application and Grid Middleware Assembly (PRAGMA) is a collaborative effort of 15 institu-tions around the Pacific Rim. PRAGMAÕs mission is to establish sustained collaborations and advance the useof grid technologies among a community of investigators working with leading institutions around the PacificRim. To fulfill this mission, PRAGMA hosts a series of workshop for members to focus on developing applica-
tions and on developing a testbed for these applications. Current applications include workflows in biology(protein annotation); linking via Web services climate data (working with some Long-Term Ecological Re-search [LTER] Network sites in the United States and East Asia Pacific region [ILTER]); running solvation
models; and extending telescience application to more institutions.The Protein Data BankThe Protein Data Bank (PDB) was established in 1971 as a computer-based archival resource for macromolec-ular structures. The purpose of the PDB was to collect, standardize, and distribute atomic coordinates and
other data from crystallographic studies. In 1977 the PDB listed atomic coordinates for 47 macromolecules. In1987, the number began to increase rapidly at a rate of about 10 percent per year due to the development ofarea detectors and widespread use of synchrotron radiation; by April 1990, atomic coordinate entries existed
for 535 macromolecules. Commenting on the state of the art in 1990, Holbrook and colleagues [citationomitted] noted that crystal determination could require one or more man-years. As of 1999, the BiologicalMacromolecule Crystallization Database (BMCD) of the PDB contain[ed] entries for 2,526 biological macro-
molecules for which diffraction quality crystals had been obtained. These include proteins, protein-proteincomplexes, nucleic acids, nucleic acid-nucleic acid complexes, protein-nucleic acid complexes, and viruses.In July 2004, the PDB held information on 26,144 structures (23,676 proteins, peptides, and viruses; 1,338
nucleic acids; 1,112 protein/nucleic acid complexes; and 18 carbohydrates).The National Center for Biotechnology InformationThe National Center for Biotechnology Information (NCBI), part of NIHÕs National Library of Medicine, has beencharged with creating automated systems for storing, analyzing, and facilitating the use of knowledge about
molecular biology, biochemistry, and genetics. In addition to GenBank, NCBI curates the Online MendelianInheritance in Man (OMIM), the Molecular Modeling Database (MMDB) of three-dimensional protein structures,the Unique Human Gene Sequence Collection (UniGene), the Taxonomy Browser, and the Cancer Genome
Anatomy Project (CGAP), in collaboration with the National Cancer Institute. NCBIÕs retrieval system, Entrez,permits linked searches of the databases, while a variety of tools have been developed for data mining, sequenceanalysis, and three-dimensional structure display and similarity searching. NCBIÕs senior investigators and ex-
tended staff collaborate with the external research community to develop novel algorithms and research ap-proaches that have transformed computational biology and will enable further genomic discoveries.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION2317.1.3  The Role of High-performance Computing
Loosely speaking, processing capability refers to the speed with which a computational solution toa problem can be delivered. High processing capability is generally delivered by computing units
operating in parallel and is generally dependent on two factorsÑthe speed with which individual units
compute (usually measured in operations per second) and the communications bandwidth between
individual units. If a problem can be partitioned so that each subcomponent can be processed indepen-
dently, then no communication at all is needed between individual computing units. On the other hand,
as the dependence of one subcomponent on others increases, so does the amount of communications
required between computing units.EUROGRIDÕs Bio GRIDFunded by the European Commission, Bio GRID is intended to help biologists and chemists who are notfamiliar with high-performance computing (HPC) execution systems by developing intuitive user interfaces forselected biomolecular modeling packages and creating compatibility interfaces between the packages and
their databases through Bio GRIDÕs UNICORE platform. The UNICORE system will allow investigators tostreamline their work processes, connect to Internet-accessible databases, and run a number of quantumchemistry and molecular dynamics software programs developed as plug-ins by Bio GRIDÕs staff.The NSF National Ecological Observatory Network (NEON)NEON is a continental-scale research instrument consisting of geographically distributed networked infra-structure, with lab and field instrumentation; site-based experimental infrastructure; natural history archivefacilities; and computational, analytical, and modeling capabilities. NEON is intended to transform ecological
research by enabling studies on major environmental challenges at regional to continental scales. Scientistsand engineers use NEON to conduct real-time ecological studies spanning all levels of biological organizationand many temporal and geographical scales. NEONÕs synthesis, computation, and visualization infrastructure
constitutes a virtual laboratory that enables the development of a predictive understanding of the direct effectsand feedbacks between environmental change and biological processes.The NSF Long-Term Ecological Research Network (LTER)Since 1980, NSF has supported the Long-Term Ecological Research (LTER) Network. The LTER program ischaracterized by long temporal and broad spatial scales of research and fosters ecological comparisons among26 U.S. sites that illustrate the importance of comprehensive analyses of ecosystems and of distinguishingsystem features across multiple scales of time and space. Data collected at each site are accessible to other
scientists and the general public, and the LTER network works with other research institutions to standardizeinformation management practices to achieve network- and community-wide data integration, facilitatingdata exchange and advancing data analysis and synthesis. LTER-supported work has included efforts in cli-
mate variability and ecosystem response, standardization of protocols for measuring soil properties for long-term ecological research, synthesis of global data on winter ice duration on lakes and rivers, and comparisonsof ecosystem productivity, among others.SOURCES: PRAGMA: material adapted from http://www.pragma-grid.net.PDB: material pre-2004 excerpted from T. Lenoir, ÒShaping Biomedicine as an Information Science,Ó pp. 27-45 in Proceedings of the1998 Conference on the History and Heritage of Science Information Systems, M. Bowden, T. Hahn, and R. Williams, eds., ASIS MonographSeries, Information Today, Inc., Medford, NJ, 1999, available at http://www.stanford.edu/dept/HPST/TimLenoir/Publications/
Lenoir_BioAsInfoScience.pdf. Information for 2004 taken from Protein Data Bank Annual Report 2004, available at http://www.rcsb.org/pdb/annual_report04.pdf.NCBI: material adapted from http://www.ncbi.nlm.nih.gov.Bio GRID: material adapted from http://www.eurogrid.org.NEON: material adapted from http://www.nsf.gov/bio/neon/.LTER: material adapted from the LTER brochure, available at http://intranet.lternet.edu/archives/documents/Publications/brochures/
lter_brochure.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.232CATALYZING INQUIRYMany biological applications must access large amounts of data. Furthermore, because of thecombinatorial nature of the exploration required in these applications (i.e., the relationships between
different pieces of data is not known in advance and thus all possible combinations are a priori
possible), assumptions of locality that can be used to partition problems with relative ease (e.g., in
computational fluid dynamics problems) do not apply, and thus the amount of data exchange in-
creases. One estimate of the magnitude of the data-intensive nature of a biological problem is that a
comparison of two of the smallest human chromosomes using the best available dynamic program-
ming algorithm allowing for substitutions and gaps would require hundreds of petabytes of memory
and hundred-petaflop processors.2Thus, in supercomputers intended for biological applications, speed in computation and in commu-nication are both necessaryÑand many of todayÕs supercomputing architectures are thus inadequate
for these applications.3 Note that communications issues deal both with interprocessor communica-tions (e.g., comparing sequences between processors, dividing long sequences among multiple proces-
sors) and traditional input-output (e.g., searching large sequence libraries on disk, receiving many
requests at a time from the outside world). When problems involve large amounts of data exchange,
communications become increasingly important.Greater processing capability would enable the attack of many biologically significant problems.Today, processing capability is adequate to sequence and assemble data from a known organism. To
some extent, it is possible to find genes computationally (as discussed in Chapter 4), but the accuracy of
todayÕs computationally limited techniques is modest. Simulations of interesting biomolecular systems
can be carried out routinely for about hundreds of thousands of atoms for tens of nanoseconds. Order-
of-magnitude increases (perhaps even two or three orders of magnitude) in processing capability would
enable great progress in problem domains such as protein folding (ab initio prediction of three-dimen-
sional structure from one-dimensional sequence information), simulation methods based on quantum
mechanics that can provide more accurate predictions of the detailed behavior of interesting
biomolecules in solution,4 simulations of large numbers of interacting macromolecules for times ofbiological interest (i.e., for microseconds and involving millions of atoms), comparative genomics (i.e.,
finding similar genetic sequences across the genomes of different organismsÑthe multiple sequence
alignment problem), proteomics (i.e., understanding the combinatorially large number of interactions
between gene products), predictive and realistic simulations of biological systems ranging from cells to
ecosystems), and phylogenetics (the reconstruction of historical relationships between species or indi-
viduals). Box 7.3 provides some illustrative applications of high-performance computing in life sciences
research.Any such estimate of the computing power needed to solve a given problem depends on assump-tions about how a solution to that problem might be structured. Different ways of structuring a problem2Shankar Subramanian, University of California, San Diego, personal communication, September 24, 2003.3This discussion of communications issues is based on G.S. Heffelfinger, ÒSupercomputing and the New Biology,Ó PowerPointpresentation at the AAAS Annual Meeting, Denver, CO, February 13-18, 2003.4A typical problem might be the question of enzymes that exhibit high selectivity and high catalytic efficiency, and a detailedsimulation might well provide insight into the related problem of designing an enzyme with novel catalytic activity. Simulationsbased on classical mechanics treat molecules essentially as charged masses on springs. These simulations (so-called molecular
dynamics simulations) have had some degree of success, but lead to seriously inaccurate results where ions must interact inwater or when the breaking or forming of bonds must be taken into account. Simulations based on quantum mechanics modelmolecules as collections of nuclei and electrons and entail solving of quantum mechanical equations governing the motion of
such particles; these simulations offer the promise of much more accurate simulations of these processes, although at a muchhigher computational cost. These comments are based on excerpts from a white paper by M. Colvin, ÒQuantum MechanicalSimulations of Biochemical Processes,Ó presented at the National Research CouncilÕs Workshop on the Future of Super-comput-
ing, Lawrence Livermore National Laboratory, Santa Fe, NM, September 26-28, 2003. See also ÒBiophysical Simulations Enabledby the Ultrasimulation Facility,Ó available at http://www.ultrasim.info/doe_docs/Biophysics_Ultrasimulation_White_Paper_4-1-03.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION233Box 7.3Grand Challenges in Computational Structural and Systems BiologyThe Onset of CancerIt is well known that cancer develops when cells receive inappropriate signals to multiply, but the details ofcell signaling are not well understood. For example, activation of the epidermal growth factor signaling path-way is under the control of growth factors that bind to a receptor site on the exterior of a cell. Binding of the
receptor initiates a cascade of protein conformational changes through the cell membrane, involving a com-plex rearrangement of many different proteins, including the Ras enzyme. The Ras enzyme is a molecularswitch that can initiate a cascade of protein kinases that in turn transfer the external signal to the cell nucleus
where it controls cell proliferation and differentiation. Disruption of this signaling pathway can have direconsequences as illustrated by the finding that mutations of the Ras enzyme have been found in 30 percent ofhuman tumors. Because computer simulations can provide atomic-level detail that is difficult or impossible to
obtain from experimental studies, computational studies are essential. However, this requires the modeling ofan extremely large complex of biomolecules, including bilayer lipid membranes, transmembrane proteins,and a complex of many intercellular kinases, and thousands of molecules of waters of solvation.Environmental RemediationMicrobes may be able to contribute to the cleanup of polluted sites by concentrating waste materials ordegrading them into nontoxic form. Understanding the role of gram-negative bacteria in moderating subsur-face reduction-oxidation chemistry and the role of such systems in bioremediation technologies requires the
study of how cell walls, including many transmembrane protein substituents, interact with extracellular min-eral surfaces and solvated atomic and molecular species in the environment. Simulations of these processesrequires that many millions of atoms be included.Degradation of Toxic Chemical WeaponsComputational approaches can be used for the rational redesign of enzymes to degrade chemical agents. Anexample is the enzyme phosphotriesterase (PTE), which could be used to degrade nerve gases. Combinedexperimental and computational efforts can be used to develop a series of highly specific PTE analogues,redesigned for optimum activity at specific temperatures, or for optimum stability and activity in nonaqueous,
low-humidity environments or in foams, for improved degradation of warfare neurotoxins. Advanced compu-tations can also facilitate the design of better reactivators of the enzyme acetylcholinesterase (AChE) that canbe used as more efficient therapeutic agents against highly toxic phosphoester compounds such as the nerve
warfare agents DFP (diisopropyl fluorophosphate), sarin, and soman and insecticides such as paraoxon. AChEis a key protein in the hydrolysis of acetylcholine, and inhibition of AChE through a phosphorylation reactionwith such phosphoesters can rapidly lead to severe intoxication and death.Multiscale Physiological Modeling of the HeartThe heart has a characteristic volume of around 60 cm3. At a resolution of 0.1 mm, a grid of some 6 × 107 cellsis required. If 100 variables are associated with each cell, 10 floating point operations are needed for eachtime step in a simulation, and the time resolution is around 1 ms (a single heartbeat has a duration around 1
second), a computing throughput of 6 × 1013 floating point operations per second (60 teraflops) is necessary.In addition, a flexible and composable simulation infrastructure is required. For example, for a spatiallydistributed system, only a representative and relatively small subset of substructures can be represented in the
model explicitly, because it is not feasible to model all of them. Contributions of the substructures missingfrom the model are inferred by an interpolative process. For practical purposes, it will not be known inadvance how much and what kinds of detail will be necessary for a useful simulation; the same a priori
ignorance also characterizes the nature and extent of the communications required between different levels ofthe simulation. Thus, the infrastructure must support easy experimentation in which different amounts ofdetail and different degrees of communication can be explored.SOURCE: The first three examples are adapted with minimal change from D.A. Dixon, T.P. Straatsma, and T. Head-Gordon, ÒGrandChallenges in Computational Structural and Systems Biology,Ó available at http://www.ultrasim.info/doe_docs/ESC-response.bio.dad.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.234CATALYZING INQUIRYsolution often result in different estimates for the required computing power, and for any complexproblem, the ÒbestÓ structuring may well not be known. (Different ways of structuring a problem may
involve different algorithms for its solution, or different assumptions about the nature of the biologi-
cally relevant information.) Furthermore, the advantage gained through algorithm advances, concep-
tual reformulations of the problem, or different notions about the answers being sought is often compa-
rable to advantages from hardware advances, and sometimes greater. On the other hand, for decades
computational scientists have been able to count on regular advances in computing power that accrued
Òfor free,Ó and whether or not scientists are able to develop new ways of looking at a given problem,
hardware-based advances in computing are likely to continue.Three types of computational problem in biology must be distinguished.5 Problems such as proteinfolding and the simulation of biological systems are similar to other simulation problems that involve
substantial amounts of Ònumber crunching.Ó A second type of problem entails large-scale comparisons
or searches in which a very large corpus of dataÑfor example, a genomic sequence or a protein data-
baseÑis compared against another corpus, such as another genome or a large set of unclassified protein
sequences. In this kind of problem, the difficult technical issues involve the lack of good software for
broadcast and parallel access disk storage subsystems. The third type of problem involves single in-
stances of large combinatorial problems, for example, finding a particular path in a very large graph. In
these problems, computing time is often not an issue if the object can be modeled in the memory of the
machine. When memory is too small, the user must write code that allows for efficient random access to
a very large objectÑa task that significantly increases development time and even under the best of
circumstances can degrade performance by an order of magnitude.The latter two types of problem often entail the consideration of large numbers of biological objects(cells, organs, organisms) characterized by high degrees of individuality, contingency, and historicity.
Such problems are typically found in investigations involving comparative and functional genomics
and proteomics, which generally involve issues such as discrete combinatorial optimization (e.g., the
multiple sequence alignment problem) or pattern inference (e.g., finding clusters or other patterns in
high-dimensional datasets). Algorithms for discrete optimization and pattern inference are often NP-
hard, meaning that the time to find an optimal solution is far too long (e.g., longer than the age of the
universe) for a problem of meaningful size, regardless of the computer that might be used or that can be
foreseen. Since optimal solutions are not in general possible, heuristic approaches are needed that can
come reasonably close to being optimalÑand a considerable degree of creativity is involved in develop-
ing these approaches.Historically, another important point has been that the character of biological data is different fromthat of data in fields such as climate modeling. Many simulations of nonbiological systems can be
composed of multiple repeating volume elements (i.e., a mesh that is well suited for finding floating
point solutions of partial differential equations that govern the temporal and spatial evolution of vari-
ous field quantities). By contrast, some important biological data (e.g., genomic sequence data) are
characterized by quantities that are better suited to integer representations, and biological simulations
are generally composed of heterogeneous objects. However, today, the difference in speed between
integer operations and floating point operations is relatively small, and thus the difference between
floating point and integer representations is not particularly significant from the standpoint of super-
computer design.Finally, it is important to realize that many problems in computational biology will never be solvedby increased computational capability alone. For example, some problems in systems biology are com-
binatorial in nature, in the sense that they seek to compare Òeverything against everythingÓ in a search
for previously unknown correlations. Search spaces that are combinatorially large are so large that even5The description of problem types in this paragraph draws heavily from G. Myers, ÒSupercomputing and ComputationalMolecular Biology,Ó presented at the NRC Workshop on the Future of Supercomputing, Santa Fe, NM, September 26-28, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION235with exponential improvements in computational speed, methods other than exhaustive search must beemployed as well to yield useful results in reasonable times.6The preceding discussion for the life sciences focuses on the large-scale computing needs of thefield. Yet these are hardly the only important applications of computing, and rapid innovation is likely
to require information technology on many scales. For example, researchers need to be able to explore
ideas on local computers, albeit for scaled-down problems. Only after smaller-scale explorations are
conducted do researchers have the savvy, the motivation, and the insight needed for meaningful use of
high-end cyberinfrastructure. Researchers also need tools that can facilitate quick and dirty tasks, and
working knowledge of spreadsheets or Perl programming can be quite helpful. For this reason, biolo-
gists working at all scales of problem size will be able to benefit from advances in and knowledge of
information technology.7.1.4  The Role of Networking
 As noted in Chapter 3, biological data come in large quantities. High-speed networking (e.g., one ortwo orders of magnitude faster than that available today) would greatly facilitate the exchange of
certain types of biological data such as high-resolution imaging as well as enable real-time remote
operation of expensive instrumentation. High-speed networking is critical for life science applications
in which large volumes of data change or are created rapidly, such as those involving imaging or remote
operation of instrumentation.7The Internet2 effort also includes the Middleware Initiative (I2-MI), intended to facilitate the cre-ation of interoperable middleware infrastructures among the membership of Internet2 and related
communities.8 Middleware generally consists of sets of tools and data that help applications use net-worked resources and services. The availability of middleware contributes greatly to the interoperability
of applications and reduces the expense involved in developing those applications. I2-MI develops
middleware to provide services such as identifiers (labels that connect a real-world subject to a set of
computerized data); authentication of identity; directories that index elements that applications must
access; authorization of services for users; secure multicasting; bandwidth brokering and quality of
service; and coscheduling of resources, coupling data, networking, and computing together.7.1.5  An Example of Using Cyberinfrastructure for Neuroscience Research
The Biomedical Informatics Research Network (BIRN) project is a nationwide effort by NationalInstitutes of Health (NIH)-supported research sites to merge data grid and computer grid
cyberinfrastructure into the workflows of biomedical research. The Brain Morphometry BIRN, one of
the testbeds driving the development of BIRN, has undertaken a project that uses the new technology
by integrating data and analysis methodology drawn from the participating sites. The Multi-site Imag-
ing Research in the Analysis of Depression (MIRIAD) project (Figure 7.1) applies sophisticated image
processing of a dataset of magnetic resonance imaging (MRI) scans of a longitudinal study of elderly
subjects. The subjects include patients who enroll in the study with symptoms of clinical depressions6Consider the following example. The human genome is estimated to have around 30,000 genes. If the exploration of interest isassumed to be 5 genes operating together, there are approximately 3 × 1020 possible combinations of 30,000 genes in sets of 5. Ifthe assumption is that 6 genes may operate together, there are on the order of 1026 possible combinations (the number of possiblecombinations of n genes in groups of k is given by n!/(k!(n Ð k)!), which for large n and small k reduces to nk/k!).7In the opposite extreme case, in which enormous volumes of data never change, it is convenient rather than essential to useelectronic or fiber links to transmit the informationÑfor a small fraction of the cost of high-speed networks, media (or even entireservers!) can be sent by Federal Express more quickly than a high-speed network could transmit the comparable volume ofinformation. See, for example, Jim Gray et al., TeraScale SneakerNet: Using Inexpensive Disks for Backup, Archiving, and Data Ex-change, Microsoft Technical Report, MS-TR-02-54, May 2002, available at ftp://ftp.research.microsoft.com/pub/tr/tr-2002-54.pdf.8See http://middleware.internet2.edu/overview/.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.236CATALYZING INQUIRYand age-matched controls. Some of the depression patients go on to develop AlzheimerÕs disease (AD)and the goal of the MIRIAD project is to measure the changes in brain images, specifically volume
changes in cortical and subcortical gray matter, that correlate with clinical outcome.Of particular significance from the standpoint of cyberinfrastructure, the MIRIAD project is distrib-uted among four separate sites: Duke University Neuropsychiatric Imaging Research Laboratory,
Brigham and WomenÕs Hospital Surgical Planning Laboratory, University of California, Los Angeles
Laboratory of Neuro Imaging, and University of California, San Diego BIRN. Each of these sites has
responsibility for some substantive part of the work, and the work would not be possible without the
BIRN infrastructure to coordinate it.DukeArchivesUCLAAIR Registrationand Lobar AnalysisBWHIntensity Normalization
and EM SegmentationDukeClinical Analysis1234BWH Probabilistic Atlas
(one-time transfer)UCSDSupercomputingMIRIAD Data Flow
1. Uploading of 
    retrospective date from 

    Duke study

2. Lobar analysis and 

    registration of atlas 

    to subjects
3. Anatomical segmentation

4. Comparison to clinical 

    history
FIGURE 7.1Steps in data processing in the BIRN MIRIAD project.
1. T2-weighted and proton density (PD) MRI scans from the Duke University longitudinal study are loaded intothe BIRN data archive (data grid), accessible by members of the MIRIAD group for analysis using the computer
resources at the University of California, San Diego (UCSD) and the San Diego Supercomputer Center (computegrid).2. The Laboratory of Neuro Imaging at the University of California, Los Angeles (UCLA) performs a nonlinearregistration to define the three-dimensional geometric mapping between each subject and a standard brain atlasthat encodes the probabilities of each tissue class at each location in the brain.3. The Surgical Planning Laboratory at Brigham and WomenÕs Hospital (BWH) then applies an intensity nor-malization and expectation-maximization algorithm to combine the original image pixel intensities (T2 and PD)and the tissue probabilities to label each point in the images and to calculate the overall volumes of tissue classes.4. Duke performs statistical tests on the image-processing results to assess the predictive value of the brainmorphometry measurements with respect to clinical outcome.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION2377.2  DATA ACQUISITION AND LABORATORY AUTOMATIONAs noted in Chapter 3, the biology of the 21st century will be data-intensive across a wide range ofspatial and temporal scales. TodayÕs high-throughput data acquisition technologies depend onparallelization rather than on reducing the time needed to take individual data points. These technolo-
gies are capable of carrying out global (or nearly global) analyses, and as such they are well suited for
the rapid and comprehensive assessment of biological system properties and dynamics. Indeed, in 21st
century biology, many questions are asked because relevant data can be obtained to answer them.
Whereas earlier researchers automated existing manual techniques, todayÕs approach is more oriented
toward techniques that match existing automation.7.2.1  TodayÕs Technologies for Data Acquisition9Some of todayÕs data acquisition technologies include the following:10¥DNA microarrays. Microarray technology enables the simultaneous interrogations of a humangenomic sample for complete human transcriptomes, provided that the arrays do not contain only
putative protein coding regions. The oligonucleotide microarray can identify single-nucleotide differ-
ences and distinguish mRNAs from individual members of multigene families, characterize alterna-
tively spliced genes, and identify and type alternative forms of single-nucleotide polymorphisms.
Microarrays are also used to observe in vitro protein-DNA binding events and to do comparative
genome hybridization (CGH) studies. Box 7.4 provides a close-up of microarrays.¥Automated DNA sequencers. Prior to automated sequencing, the sequencing of DNA was per-formed manually, at many tens (up to a few hundred) of bases per day.11 In the 1970s, the developmentof restriction enzymes, recombinant DNA techniques, gene cloning techniques, and polymerase chain
reaction (PCR) contributed to increasing amounts of data on DNA, RNA, and protein sequences. More
than 140,000 genes were cloned and sequenced in the 20 years from 1974 to 1994, many of which were
human genes. In 1986, an automated DNA sequencer was first demonstrated that sequenced 250 bases
per day.12 By the late 1980s, the NIH GenBank database (release 70) contained more than 74,000 se-quences, while the Swiss Protein database (Swiss-Prot) included nearly 23,000 sequences. In addition,
protein databases were doubling in size every 12 months. Since 1999, more advanced models of auto-
mated DNA sequencer have come into widespread use.13 Today, a state-of-the-art automated sequencercan produce on the order of a million base pairs of raw DNA sequence data per day. (In addition,
technologies are available that allow the parallel processing of 16 to 20 residues at a time.14 These enablethe determination of complete transcriptomes in individual cell types from organisms whose genome is
known.)¥Mass spectroscopy. Mass spectroscopy (MS) enables the in-quantity identification and quantifica-tion of large numbers of proteins.15 Used in conjunction with genomic information, MS information canbe used to identify and type single-nucleotide polymorphisms. Some implementations of mass spec-9Section 7.2.1 is adapted from T. Ideker, T. Galitski, and L. Hood, ÒA New Approach to Decoding Life: Systems Biology,ÓAnnual Review of Genomics and Human Genetics 2:343, 2001.10Adapted from T. Ideker et al., ÒA New Approach to Decoding Life,Ó 2001.11L. Hood and D.J. Galas, ÒThe Digital Code of DNA,Ó Nature 421(6921):444-448, 2003.12L.M. Smith, J.Z. Sanders, R.J. Kaiser, P. Hughes, C. Dodd, C.R. Connell, C. Heiner, et al., ÒFluorescence Detection in Auto-mated DNA Sequence Analysis,Ó Nature 321(6071):674-679, 1986. (Cited in Ideker et al., 2001.)13L. Rowen, S. Lasky, and L. Hood, ÒDeciphering Genomes Through Automated Large Scale Sequencing,Ó Methods in Microbi-ology, A.G. Craig and J.D. Hoheisel, eds., Academic Press, San Diego, CA, 1999, pp. 155-191. (Cited in Ideker et al., 2001.)14S. Brenner, M. Johnson, J. Bridgham, G. Golda, D.H. Lloyd, D. Johnson, S. Luo, et al., ÒGene Expression Analysis by Mas-sively Parallel Signature Sequencing (MPSS) on Microbead Arrays,Ó Nature Biotechnology 18(6):630-634, 2000. (Cited in Ideker etal., 2001.)15J.K. Eng, A.L. McCormack, and J.R.I. Yates, ÒAn Approach to Correlate Tandem Mass Spectral Data of Peptides with Amino AcidSequences in a Protein Database,Ó Journal of the American Society for Mass Spectrometry 5:976-989, 1994. (Cited in Ideker et al., 2001.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.238CATALYZING INQUIRYtroscopy today allow 1,000 proteins per day to be analyzed in an automated fashion, and there is hopethat a next-generation facility will be able to analyze up to 1 million proteins per day.16¥Cell sorters. Cell sorters separate different cell types at high speed on the basis of multiple param-eters. While microarray experiments provide information on average levels of mRNA or protein within
a cell population, the reality is that these levels vary from cell to cell. Knowing the distribution of
expression levels across cell types provides important information about the underlying control mecha-
nisms and regulatory network structure. A state-of-the-art cell sorter can separate 30,000 elements per
second according to 32 different parameters.17Box 7.4Microarrays: A Close-upA ÒclassicalÓ microarray typically consists of single-stranded pieces of DNA from virtually an entire genomeplaced physically in tiny dots on a flat surface and labeled with a fluorescent dye. (Lithographic techniquesused to develop semiconductor chips are now used to deposit the DNA on a silicon chip that can later be readoptically.) In a microarray experiment, messenger RNA (mRNA) from a cell of interest is extracted and placed
in contact with the prepared surface. If the sample contains mRNA corresponding to the DNA on one or moreof the dots on the surface, the molecules will bind and the dye will fluoresce. Because the mRNA representsthe fraction of genes from the sample that have been transcribed from DNA into mRNA, the resulting fluores-
cent dots on the surface are a visual indicator of gene expression (or transcription) in the cellÕs genome.Different intensities of the dots reflect greater or lesser levels of transcription of particular genes.Obtaining the maximum value from a microarray experiment depends on the ability to correlate the data froma microarray experiment per se with extensive data that identify or classify the genes by other characteristics.In the absence of such data, any given microarray experiment merely points out the fact that some genes are
expressed to a greater extent than others in a particular experimental situation.Protein microarrays can identify protein-protein (and protein-drug) interactions among some 10,000 proteinsat once.1 As described by Templin,2[protein] microarray technology allows the simultaneous analysis of thousands of parameters within a single exper-iment. Microspots of capture molecules are immobilized in rows and columns onto a solid support and exposed tosamples containing the corresponding binding molecules. Readout systems based on fluorescence, chemilumines-cence, mass spectrometry, radioactivity or electrochemistry can be used to detect complex formation within each
microspot. Such miniaturized and parallelized binding assays can be highly sensitive, and the extraordinary powerof the method is exemplified by array-based gene expression analysis. In these systems, arrays containing immobi-lized DNA probes are exposed to complementary targets and the degree of hybridization is measured. Recent
developments in the field of protein microarrays show applications for enzyme-substrate, DNA-protein and differenttypes of protein-protein interactions. Here, we discuss theoretical advantages and limitations of any miniaturizedcapture-molecule-ligand assay system and discuss how the use of protein microarrays will change diagnostic meth-
ods and genome and proteome research.1See G. MacBeath and S.L. Schreiber, ÒPrinting Proteins as Microarrays for High-Throughput Function Determination,Ó Science 289(5485):1760-1763, 2000.2Reprinted by permission from M.F. Templin, D. Stoll, M. Schrenk, P.C. Traub, C.F. Vohringer, and T.O. Joos, ÒProtein MicroarrayTechnology,Ó Trends in Biotechnology 20(4):160-166, 2002. Copyright 2002 Elsevier.NOTE: An overview of microarray technology is available on a private Web site created by Leming Shi: http://www.gene-chips.com/. Seealso http://www.genome.gov/10000533 and P. Gwynne and G. Page, ÒMicroarray Analysis: The Next Revolution in Molecular Biology,Ó
special advertising supplement, Science 285, August 6, 1999, available at http://www.sciencemag.org/feature/e-market/benchtop/micro.shl.16S.P. Gygi, B. Rist, S.A. Gerber, F. Turecek, M.H. Gelb, and R. Aebersold, ÒQuantitative Analysis of Complex Protein MixturesUsing Isotope-coded Affinity Tags,Ó Nature Biotechnology 17(10):994-999, 1999. (Cited in Ideker et al., 2001.)17See, for example, http://www.systemsbiology.org/Default.aspx?pagename=cellsorting.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION239¥Microfluidic systems. Microfluidic systems, also known as micro-TAS (total analysis system), al-low the rapid and precise measurement of sample volumes of picoliter size. These systems put onto a
single integrated circuit all stages of chemical analysis, including sample preparation, analyte purifica-
tion, microliquid handling, analyte detection, and data analysis.18 These Òlab-on-a-chipÓ systems pro-vide portability, higher-quality and higher-quantity data, faster kinetics, automation, and reduction of
sample and reagent volumes.¥Embedded networked sensor (ENS) systems. ENS systems are large-scale, distributed systems, com-posed of smart sensors embedded in the physical world, that can provide data about the physical world
at unprecedented granularity. These systems can monitor and collect large volumes of information at
low cost on such diverse subjects as plankton colonies, endangered species, and soil and air contami-
nants. Across a wide range of large-scale biological applications broadly cast, these systems promise to
reveal previously unobservable phenomena. Box 7.5 describes some applications of ENS systems.Finally, a specialized type of data acquisition technology is the hybrid measurement device thatinteracts directly with a biological sample to record data from it or to interact with it. As one illustra-
tion, contemporary tools for studying neuronal signaling and information processing include implant-
able probe arrays that record extracellularly or intracellularly from multiple neurons simultaneously.18See, for example, http://www.eurobiochips.com/euro2002/html/agenda.asp. To illustrate the difficulty, consider the han-dling of liquids. Dilution ratios required for a process may vary by three or four orders of magnitude, and so an early challenge(now largely resolved successfully) is the difficulty of engineering an automated system that can dispense both 0.1-microliter and1-milliliter volumes with high accuracy and in reasonable time periods.Box 7.5Applications of Embedded Network Sensor SystemsMarine Microorganisms1Marine microorganisms such as viruses, bacteria, microalgae, and protozoa have a major impact on theecology of the coastal ocean; present public health issues for coastal human populations as a consequence of
the introduction of pathogenic microorganisms into these waters from land runoff, storm drains, and sewageoutflow; and have the potential to contaminate drinking water supplies with harmful, pathogenic, or nuisancemicrobial species.Today, the environmental factors that stimulate the growth of such microorganisms are still poorly under-stood. To understand these factors, scientists need to correlate environmental conditions with microorganis-
mal abundances at the small spatial and temporal scales that are relevant to these organisms. For a variety oftechnological and methodological reasons, sampling the environment at the necessary high resolution andidentifying microorganisms in situ in near-real time has not been possible in the past.Habitat Sensing2Understanding in detail the environmental, organismal, and cultural conditions, and the interactions betweenthem, in natural and managed habitats is a problem of considerable biological complexity. Data must becaptured and integrated across a wide range of spatial and temporal scales for chemical, physiological, eco-
logical, and environmental purposes. For example, data of interest might include microclimate data; a videoof avian behavioral activities related to climate, nesting, and reproduction; and data on soil moisture, nitrate,CO2, temperature, and root-fungi activities in response to weather.1Adapted from http://www.cens.ucla.edu/portal/aquatic_microbial_observing_syst.html.2Adapted from http://deerhound.ats.ucla.edu:7777/portal/page?_pageid=54,42365,54_42372&_dad=portal&_schema=PORTAL.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.240CATALYZING INQUIRYSuch arrays have been used in moths (Manduca sexta) and sea slugs (Tritonia diomeda) and, when linkeddirectly to the electronic signals of a computer, essentially record and simulate the neural signaling
activity occurring in the organism. Box 7.6 describes the dynamic clamp, a hybrid measurement device
that has been invaluable in probing the behavior of neurons. Research on this interface will serve both
to reveal more about the biological system and to represent that system in a format that can be
computed.Box 7.6The Dynamic ClampThe dynamic clamp is a device that mimics the presence of a membrane or synapse proximate to a neuron.That is, the clamp essentially simulates the electrical conductances in the network to which a neuron isostensibly connected. During clamp operation, the membrane potential of a neuron is continuously measured
and fed into a computer. The dynamic clamp program contains a mathematical model of the conductance tobe simulated and computes the current that would flow through the conductance as a function of time. Thiscurrent is injected into the neuron, and the cycle of membrane potential measurement, current computation,
and current injection continues. This cycle enables researchers to study the effects of a membrane current orsynaptic input in a biological cell (the neuron) by generating a hybrid system in which the artificial conduc-tance interacts with the natural dynamic properties of the neuron.The dynamic clamp can be used to mimic any voltage-dependent conductance that can be expressed in amathematical model. Depending on the type of conductance, most applications can be grouped in one of the
following categories:1.Generating artificial membrane conductances. These may be voltage dependent or independent.2.Simulating natural stimuli. The dynamic clamp can mimic natural conditions such as barrages of synapticinputs to neurons in silent brain slices. Here, an artificial synaptic conductance trace is used to compute anartificial synaptic current from the momentary membrane potential of the postsynaptic neuron. That current is
continuously injected into the neuron, and the effect of the artificial input on the activity of the neuron isassessed.3.Generating artificial synapses. In a configuration where the dynamic clamp computer monitors the mem-brane potential of several neurons and computes and injects current through several output channels, thedynamic clamp can be used to create artificial chemical or electrotonic synaptic connections between neu-rons that are not connected in vivo or to modify the strength or dynamics of existing synaptic connections.
4.Coupling of biological and model neurons. The dynamic clamp can also be used to create hybrid circuitsby coupling model and biological neurons through artificial synapses. In this application, the dynamic clampcomputer continuously solves the differential equations that describe the model neuron and the synapses that
connect it to the biological neuron.The first application of the dynamic clamp involved the stimulation of a gamma-aminobutyric acid (GABA)response in a cultured stomatogastric ganglion neuron. This application illustrated that the dynamic clampeffectively introduces a conductance into the target neuron. Demonstration of an artificial voltage-dependentconductance resulted from simulation of the action of a voltage-dependent proctolin response on a neuron in
the intact stomatogastric ganglion, which showed that shifts in the activation curve and the maximal conduc-tance of the response produced different effects on the target neuron. Lastly, the dynamic clamp was used toconstruct reciprocal inhibitory synapses between two stomatogastric ganglion neurons that were not coupled
naturally, illustrating that the dynamic clamp could be used to simulate new networks at will.SOURCE: The description of a dynamic clamp is based heavily on A.A. Prinz, ÒThe Dynamic Clamp a Decade After Its Invention,Ó AxonInstruments Newsletter 40, February 2004, available at http://www.axon.com/axobits/AxoBits40.pdf. The description of the first applicationof the dynamic clamp is nearly verbatim from A.A. Sharp, M.B. OÕNeil, L.F. Abbott, and E. Marder, ÒDynamic Clamp: Computer-generatedConductances in Real Neurons,Ó Journal of Neurophysiology 69(3):992-995, 1993.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION2417.2.2  Examples of Future Technologies
As powerful as these technologies are, new instrumentation and methodology will be needed in thefuture. These technical advances will have to reduce the cost of data acquisition by several orders of
magnitude.Consider, for example, the promise of genomically individualized medical care, which is based onthe notion that treatment and/or prevention strategies for disease can be customized to groups of
individuals smaller than the entire population, and perhaps ultimately groups as small as one. Because
these groups will be identified in part by particular sets of genomic characteristics, it will be necessary
to undertake the genomic sequencing of these individuals. The first complete sequencing of the human
genome took 13 years and $2.7 billion. For broad use in the population at large, the cost of assembling
and sequencing a human genome must drop to hundreds or thousands of dollarsÑa reduction in cost
of 105 or 106 that would enable the completion of a human genome at such cost in a matter of days.19Computation per se is expected to continue to drop in cost in accordance with MooreÕs law at leastover the next decade. But automation of data acquisition will also play an enormous role in facilitating
such cost reductions. For example, the laboratory of Richard Mathies at the University of California,
Berkeley, has developed a 96-lane microfabricated DNA sequencer capable of sequencing at a rate of
1,700 bases per minute.20 Using this technology, the complete sequencing of an individual 3-billion basegenome would take 1,000 sequencer-days. Future versions will incorporate higher degrees of parallel-
ism.Similar advances in technology will help to reduce the cost of other kinds of biological research aswell. A number of biological signatures useful for functional genomics have been susceptible to signifi-
cantly greater degrees of automation, miniaturization, and multiplexing; these signatures are associated
with electrophoresis, molecular microarrays, mass spectrometry, and microscopy.21 Electrophoresis,molecular microarrays, and mass spectrometry provide more opportunities for multiplexed measure-
ment (i.e., the simultaneous measurement of signatures from many molecules from the same source).
Such multiplexing can reduce errors due to misalignment of unmultiplexed measures in space and/or
time.In general, the biggest payoffs in laboratory automation are those efforts that can address processesthat involve physical material. Much work in biology involves multiple laboratory procedures that each
call for multiple fluid transfers, heating and cooling cycles, and mechanical operations such as centri-
fuging, waiting, and imaging. When these procedures can be undertaken Òon-chip,Ó they reduce the
amount of human interaction involved and thus the associated time and cost.In addition, the feasibility of lab automation is closely tied to the extent to which human craft can betaken out of lab work. That is, because so much lab work must be performed by humans, the skills of the
particular individuals involved matter a great deal to the outcomes of the work. A particular individual
may be the only one in a laboratory with a ÒknackÓ for performing some essential laboratory procedure
(e.g., interpretation of certain types of image, preparation or certain types of sample) with high reliabil-
ity, accuracy, and repeatability.19L.M. Smith, J.Z. Sanders, R.J. Kaiser, P. Hughes, C. Dodd, C.R. Connell, C. Heiner, et al., ÒFluorescence Detection in Auto-mated DNA Sequence Analysis,Ó Nature 321(6071):674-679, 1986; L. Hood and D. Galas, ÒThe Digital Code of DNA,Ó Nature421(6921):444-448, 2003. Note that done properly, the second complete sequencing of a human being would be considerably less
difficult. The reason is that every member of a biological species has a DNA that is almost identical to that of every othermember. In humans, the difference between DNA sequences of different individuals is about one base pair per thousand. (Seespecial issues on the human genome: Science 291(5507) February 16, 2001; Nature 409(6822), February 15, 2001.) So, assuming it isknown where to check for every difference, a reduction in effort of at least a factor of 103 is obtainable in principle.20B.M. Paegel, R.G. Blazej, and R.A. Mathies, ÒMicrofluidic Devices for DNA Sequencing: Sample Preparation and Electro-phoretic Analysis,Ó Current Opinion in Biotechnology 14(1):42-50, 2003, available at http://www.wtec.org/robotics/us_workshop/June22/paper_mathies_microfluidics_sample_prep_2003.pdf.21G. Church, ÒHunger for New Technologies, Metrics, and Spatiotemporal Models in Functional Genomics,Ó available athttp://recomb2001.gmd.de/ABSTRACTS/Church.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.242CATALYZING INQUIRYWhile reliance on individuals with specialized technical skills is often a workable strategy for anacademic laboratory, it makes much less sense for any organization interested in large-scale production.
For large-scale, cost-effective production, process automation is a sine qua non. When a process can be
automated, it is generally faster to perform, more free from errors, more accurate, and less expensive.22Some of the clearest success stories involve genomic technologies. For example, DNA sequencingwas a craft at the start of the 1990sÑtoday, automated DNA sequencing is common, with instruments
to undertake such sequencing in high volume (a million or more base pairs per day) and even a
commercial infrastructure to which sequencing tasks can be outsourced. Nevertheless, a variety of
advanced sequencing technologies are being developed, primarily with the intent of lowering the cost
of sequencing by another several orders of magnitude.23An example of such a technology is pyrosequencing, which has also been called Òsequencing bysynthesis.Ó24 With pyrosequencing, the DNA to be sequenced is denatured to form a single strand andthen placed in solution with a set of selected enzymes. In a cycle of individual steps, the DNA-enzyme
solution is mixed with deoxynucleotide triphosphate molecules containing each of the four bases. When
the base that is the complement to the next base on the target strand is added, the added base joins a
forming complement strand and releases a pyrophosphate molecule. That molecule starts a reaction
that ends with luciferin emitting a detectable amount of light. Thus, by monitoring the light output of
the reaction (for example, with a CCD camera), it is possible to observe in real time which of the four
bases has successfully matched.454 Life Sciences has applied pyrosequencing to whole-genome analyses by taking advantage of itshigh parallelizability. Using a PicoTiter plate, a microfluidic system performs pyrosequencing on hun-dreds of thousands of DNA fragments simultaneously. Custom software analyzes the light emitted and
stitches together the complete sequence. This approach has been used successfully to sequence the
genome of an adenovirus,25 and the company is expected to produce commercial hardware to performwhole-genome analysis in 2005.A second success story is microarray technology, which historically has relied heavily on electro-phoretic techniques.26 More recently, techniques have been developed that do away entirely withelectrophoresis. One approach relies instead on microbeads with different messenger RNAs on their
surfaces (serving as probes to which targets bind selectively) and a novel sequencing procedure to22The same can be said for many other aspects of lab work. In 1991, Walter Gilbert noted, ÒThe march of science devises evernewer and more powerful techniques. Widely used techniques begin as breakthroughs in a single laboratory, move to being usedby many researchers, then by technicians, then to being taught in undergraduate courses and then to being supplied as pur-
chased servicesÑor, in their turn, superseded. . . . Fifteen years ago, nobody could work out DNA sequences, today everymolecular scientists does so and, five years from now, it will all be purchased from an outside supplier. Just this happened withrestriction enzymes. In 1970, each of my graduate students had to make restriction enzymes in order to work with DNA
molecules; by 1976 the enzymes were all purchased and today no graduate student knows how to make them. Once one had tosynthesize triphosphates to do experiments; still earlier, of course, one blew oneÕs own glassware.Ó See W. Gilbert, ÒTowards aParadigm Shift in Biology,Ó Nature 349(6305):99, 1991.23A review by Shendure et al. classifies emerging ultralow-cost sequencing technologies into one of five groups: microelectro-phoretic methods (which extend and incrementally improve todayÕs mainstream sequencing technologies first developed byFrederick Sanger); sequencing by hybridization; cyclic array sequencing on amplified molecules; cyclic array sequencing on
single molecules; and noncyclical, single-molecule, real-time methods. The article notes that most of these technologies are still inthe relatively early stages of development, but that they each have great potential. See J. Shendure, R.D. Mitra, C. Varma, andG.M. Church, ÒAdvanced Sequencing Technologies: Methods and Goals,Ó Nature Reviews: Genetics 5(5):335-344, 2004, available athttp://arep.med.harvard.edu/pdf/Shendure04.pdf. Pyrosequencing, provided as an example of one new sequencing technol-ogy, is an example of cyclic array sequencing on amplified molecules.24M. Ronaghi, ÒPyrosequencing Sheds Light on DNA Sequencing,Ó Genome Research 11(1):3-11, 2001.25A. Strattner, ÒFrom Sanger to ÔSequenatorÕ,Ó Bio-IT World, October 10, 2003.26Genes are expressed as proteins, and these proteins have different weights. Electrophoresis is a technique that can be used todetermine the extent to which proteins of different weights are present in a sample.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION243identify specific microbeads.27 Each bead can be interrogated in parallel, and the abundance of a givenmessenger RNA is determined by counting the number of beads with that mRNA on their surfaces. Inaddition to greatly simplifying the sample-handling procedure, this technique has two other important
advantages: a direct digital readout of relative abundances (i.e., the bead counts) and throughput
increases by more than a factor of 10 compared to other techniques.A second approach to the elimination of electrophoresis is known as optical mapping or sequencing(Box 7.7). Optical mapping eliminates dependence on ensemble-based methods, focusing on the statis-
tics of individual DNA molecules. Although this technique is fragile and, to date, not replicable in
multiple laboratories,28 it may eventually be capable of sequencing entire genomes much more rapidlythan is possible today.A different approach based on magnetic detection of DNA hybridization seeks to lower the cost ofperforming microarray analysis. Chen et al. have suggested that instead of tagging targets with fluores-
cent molecules, targets are tagged with microscopic magnetic beads.29 Probes are implanted on amagnetically sensitive surface, such as a floppy disk, after removing the magnetic coating at the probe27S. Brenner, ÒGene Expression Analysis by Massively Parallel Signature Sequencing (MPSS) on Microbead Arrays,Ó NatureBiotechnology 18(6):630-634, 2002. The elimination of electrophoresis (a common laboratory technique for separating biologicalsamples by molecular weight) has many practical benefits. Conceptually, electrophoresis is a straightforward process. A taggedbiological sample is inserted into a viscous gel and then subjected to an external electric field for some period of time. The sampledifferentiates in the electric field because the lighter components move farther under the influence of the electric field than theheavier ones. The tag on the biological sample is, for example, a compound that fluoresces when exposed to ultraviolet light.
Measuring the intensity of the fluorescence provides an indication of the relative abundances of components of different molecu-lar weight. However, in practice there are difficulties. The gel must undergo appropriate preparationÑno small task. For ex-ample, the gel must be homogeneous, with no bubbles to interfere with the natural movement of the sample components. Thetemperature of the gel-sample combination may be important, because the viscosity of the gel may be temperature-sensitive.
While the gel is drying (a process that takes a few hours), it must not be physically disturbed in a way that introduces defects intothe gel preparation.28Bud Mishra, New York University, personal communication, December 2003.29C.H.W. Chen, V. Golovlev, and S. Allman, ÒInnovative DNA Microarray Hybridization Detection Technology,Ó poster ab-stract presented at Human Genome Meeting 2002, April 14-17, 2004, Shanghai, China; also, ÒDetection of Polynucleotides on
Surface of Magnetic Media, available at http://www.scien-tec.com/news1.htm.Box 7.7On Optical MappingOptical mapping is a single molecule based physical mapping technology, which creates an ordered restriction mapby enumerating the locations of occurrences of a specific Òrestriction patternÓ along a genome. Thus, by locating thesame patterns in the sequence reads or contigs, optical maps can detect errors in sequence assembly, and determine
the phases (i.e., chromosomal location and orientation) of any set of sequence contigs. Since the input genomic datathat can be collected from a single DNA molecule by the best chemical and optical methods (such as those used inOptical Mapping) are badly corrupted by many poorly understood noise processes, this type of technology derives
its utility through powerful probabilistic modeling used in experiment design and Bayesian algorithms that canrecover from errors by using redundant data. In this way, optical mapping with Gentig, a powerful statistical map-assembly algorithm invented and implemented by the authors, has proven instrumental in completing many micro-
bial genomic maps (Escherichia coli, Yersinia pestis, Plasmodium falciparum, Deinococcus radiodurans, Rhodo-bacter sphaeroides, etc.) as well as clone maps (DAZ locus of Y chromosome).SOURCE: T. Anantharaman and B. Mishra, Genomics via Optical Mapping (I): 0-1 Laws for Single Molecules, S. Yancopoulos, ed., OxfordUniversity Press, Oxford, England, 2005, in press.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.244CATALYZING INQUIRYlocation, and different probes are attached to different locations. Readout of hybridized probe-targetpairs is accomplished through the detection of a magnetic signal at given locations; locations without
such pairs provide no signal because the magnetic coating of the floppy disk has been removed from
those locations. Also, the location of any given probe-target pair is treated simply as a physical address
on the floppy disk. Preliminary data suggest that with the spatial resolution currently achieved, a single
floppy diskette can carry up to 45,000 probes, a figure that compares favorably to that of most glass
microarrays (of order 10,000 probes or less). Chen et al. argue that this approach has two advantages:
greater sensitivity and significantly lower cost. The increased sensitivity is due to the fact that signal
strength is controlled by the strength of the beads rather than the amount of hybridizing DNA per se;
and so, in principle, this approach could detect even a single hybridization event. Lower costs arguably
result from the fact that the most of the components for magnetic detection are mass-produced in
quantity for the personal computer industry today.Laboratory robotics is another area that offers promise of reduced labor costs. For example, theminimization of human intervention is illustrated by the introduction of compact, user-programmable
robot arms in the early 1980s.30 One version, patented by the Zymark Corporation, equipped a robotarm with interchangeable hands. This arm was the foundation of robotic laboratory workstations that
could be programmed to carry out multistep sample manipulations, thus allowing them to be adapted
for different assays and sample-handling approaches.Building on the promise offered by such robot arms, a testbed laboratory formed in the 1980s by Dr.Masahide Sasaki at the Kochi Medical School in Japan demonstrated the feasibility of a high degree of
laboratory automation: robots carried test tube racks, and conveyor belts transported patient samples to
various analytical workstations. Automated pipettors drew serum from samples for the required as-
says. One-armed stationary robots performed pipetting and dispensing steps to accomplish preanalytical
processing of higher complexity. The laboratory was able to perform all clinical laboratory testing for a
600-bed hospital with a staff of 19 employees. By comparison, hospitals in the United States of similar
size required up to 10 times as many skilled clinical laboratory technologists.Adoption of the Òtotal laboratory automationÓ approach was mixed. Many clinical laboratories inparticular found that it provided excess capacity whose costs could not be recovered easily. Midsized
hospital laboratories had a hard time justifying the purchase of multimillion-dollar systems. By con-
trast, pharmaceutical firms invested heavily in robotic laboratory automation, and automated facilities
to synthesize candidate drugs and to screen their biological effects provided three- to fivefold increases
in the number of new compounds screened per unit time.In recent years, manufacturers have marketed ÒmodularÓ laboratory automation products, includ-ing modules for specimen centrifugation and aliquoting, specimen analysis, and postanalytical storage
and retrieval. While such modules can be assembled like building blocks into a system that provides
very high degrees of automation, they also enable a laboratory to select the module or modules that best
address its needs.Even mundane but human-intensive tasks are susceptible to some degree of automation. Considerthat much of biological experimentation depends on the availability of mice as test subjects. Mice need
to be housed and fed, and thus require considerable human labor. The Stowers Institute for Medical
Research in Kansas City has approached this problem with the installation of an automated mouse care
facility involving two robots, one of which dumps used mouse bedding and feeds it to a conveyor
washing machine and the other of which fills the clean cages with bedding and places them on a rack.31These robots can process 350 cages per hour and reduce the labor needs of cleaning cages by a factor of
three (from six technicians to two). At a cost of $860,000, the institute expects to recoup its investment in30J. Boyd, ÒRobotic Laboratory Automation,Ó Science 295(5554):517-518, 2002. Much of the discussion of laboratory automationis based on this article.31C. Holden, ed., ÒHigh-tech Mousekeeping,Ó Science 300(5618):421, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CYBERINFRASTRUCTURE AND DATA ACQUISITION2456 years, with much of the savings coming from reduced repetitive motion injuries and fewer healthproblems caused by allergen exposure.In the future, modularization is likely to continue. In addition, fewer stand-alone robot arms arebeing used because the robotics necessary for sampling from conveyor belts are often integrated di-
rectly into clinical analyzers. Attention is turning from the development of hardware to the design of
process control software to control and integrate the various automation components; to manage the
transport, storage, and retrieval of specimens; and to support automatic repeat and follow-up testing
strategies.7.2.3  Future Challenges
From a conceptual standpoint, automation for speed depends on two thingsÑspeeding up anindividual process and processing many samples in parallel. Individual processes can be speeded up to
some extent, but because they are limited by physical time constants (e.g., the time needed to mix a
solution uniformly, the time needed to dry, the time needed to incubate), the speedups possible are
limitedÑperhaps factors of a few or even ten can be possible. By contrast, parallel processing is a much
bigger winner, and it is easy to imagine processing hundreds or even thousands of samples simulta-
neously.In addition to quantitative speedups, qualitatively new data acquisition techniques are needed aswell. The difficulty of collecting meaningful data from biological systems has often constrained the level
of complexity at which to collect data. Biologists often must use indirect or surrogate measures that
imply activity. For example, oxygen consumption can be used as a surrogate for breathing.There is a need to develop new mechanisms to collect data, particularly mechanisms that can forma bridge from the living system to a computer system, in other words, tools that detect and monitor
biological events and directly collect and store information about those events for later analysis. Chal-
lenges in this area include the connection of cellular material, cells, tissues, and humans to computers
for rapid diagnostics and data download, bio-aided computation, laboratory study, or human-com-
puter interactivity, and how to perform ÒsmartÓ experiments that use models of the biological systems
to probe the biology dynamically so that measurements of the spatiotemporal dynamics of living cells at
many scales become possible.A good example of future data acquisition challenges is provided by single-cell assays and single-molecule detection. Traditional assays can involve thousands or tens of thousands of cells and produce
datasets that reflect the aggregate behavior of the entire sample. While for many types of experiments
this is an appropriate approach, there are current and future biological research issues for which this
does not provide sufficient resolution. For example, cells within a population may be in different stages
of their life cycle, may be experiencing local variations of environmental conditions, or may be of
entirely different types. Alternatively, a probe might not touch the cell type of interest, due to inad-
equate purification of a sample drawn from a subject that contains many cell types.32 For some biologi-cal questions, there is simply not a sufficient supply of cells of interest; for example, certain human
nervous system tissue is highly specialized, and a biological inquiry may concern only a few cells.
Similarly, in attempts to isolate some diseases, there may be only a few, or even only one, affected cellÑ
for example, in attempts to detect cancerous cells before they develop into a tumor.Many technologies offer approaches to analyzing and characterizing the behavior of single cells,including the use of mass spectrometry, microdissection, laser-induced fluorescence, and electrophore-
sis. Ideally, it would be possible to monitor the behavior of a living cell over time with sufficient
resolution to determine the functioning of subcellular components at different stages of the life cycle
and in response to differing environmental stimuli.32Today, this issue is addressed by the very labor-intensive process of ÒpluckingÓ individual cells from a sample and aggregat-ing themÑa process that typically requires 104 to 105 cells when todayÕs assays are used.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.246CATALYZING INQUIRYA further challenge in ultrasensitive data acquisition in living cells is that the substances of interest,particularly proteins, occur at a wide range of concentrations (varying by many orders of magnitude).
For many important proteins, this may be as few as hundreds of individual molecules. Detection and
analysis at such low levels must work even in the face of wide statistical fluctuation, transient modifica-
tions, and a wide range of physical and chemical properties.33At the finest grain, detection and analysis of single molecules could provide further understandingof cellular mechanisms. Again, although there are current techniques to analyze molecular structure
(such as nuclear magnetic resonance and X-ray crystallography), these work on large, static samples. To
achieve more precise understanding of cellular mechanisms, it is necessary to detect the presence and
activity of very small concentrations, even single molecules, dynamically within living cells. Making
progress in this field will require advances in chemistry, instrumentation, sensors, and image analysis
algorithms.34Embedded networked sensor (ENS) systems will ride the cost reduction curve that characterizesmuch of modern electronic systems. Based on microsensors, on-board processing, and wireless commu-
nications, ENS systems can monitor phenomena Òup close.Ó Nevertheless, taken as a whole, ENS sys-
tems present challenges with respect to longevity, autonomy, scalability, performance, and resilience.
For example, off-the-shelf sensors embedded in heterogeneous soil for monitoring soil moisture and
nitrate levels raise issues related to calibration when embedded in a previously unknown environment.
In addition, the uncertainty in the data they provide must be characterized. Interesting theoretical
issues arise with respect to the statistical and information-theoretic foundations for adaptive sampling
and data fusion. Also, of course, programming abstractions, common services, and tools for program-
ming the network must be developed.To illustrate a specific application, consider some of the computing challenges in deploying ENSsystems for marine microorganisms. The ultimate goal is to deploy large groups of autonomous, mobile
microrobots capable of identifying and tracking microorganisms in real time in the marine environ-
ment, while measuring the relevant environmental conditions at the required temporal and spatial
scales. Sensors must be mobile to track microorganisms and assess their abundance with a reasonable
number of sensors. They must be small, so that they are able to gather information at a spatial scale
comparable to the size of the microorganisms and to avoid disturbing them. They must operate in a
liquid environmentÑcombined with small sensor size, operation in such an environment raises many
difficult issues of mobility, communications, and power, which in turn strongly impact network algo-
rithms and strategies. Also, sensors must be capable of in situ, real-time identification of microorgan-
isms, which requires the development of new sensors with considerable on-board processing capability.
Progress in this applicationÑmonitoring marine environments and single-cell identificationÑis ex-
pected to be applicable to other liquid environments, such as the circulatory system of higher organ-
isms, including humans.33R.D. Smith et al., ÒApplication of New Technologies for Comprehensive, Quantitative and High Throughput MicrobialProteomics,Ó abstracts of the Department of EnergyÕs (DOE) Genomes to Life Systems-Biology Projects on Microbes Sequencedby the U.S. DOEÕs Microbial Genome Program, available at http://doegenomestolife.org/pubs/2004abstracts/html/Tech_
Dev.shtml#_VPID_289.34See, for example, the text of the NIH Program Announcement PA-01-049, ÒSingle Molecule Detection and Manipulation,Óreleased February 12, 2001, available at http://grants.nih.gov/grants/guide/pa-files/PA-01-049.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING2472478Biological Inspiration for ComputingChapters 4-7 address ways in which computer science and engineering can assist in the pursuit of abroadly defined research agenda in biology. This chapter suggests how insights from the biological
sciences may have a positive impact on certain research areas in computing, although the impact of this
reversed direction is at present much more speculative.18.1THE IMPACT OF BIOLOGY ON COMPUTING
8.1.1Biology and Computing: Promise and Skepticism
TodayÕs computer systems are highly complex and often fragile. Although they provide high de-grees of functionality to their users, many of todayÕs systems are also subject to catastrophic failure,
difficult to maintain, and full of vulnerabilities to outside attack. An important goal of computing is to
be able to build systems that can function with high degrees of autonomy, robustly handle data with
large amounts of noise, configure themselves automatically into networks (and reconfigure themselves
when parts are damaged or destroyed), rapidly process large amounts of data in a massively parallel
fashion, learn from their environment with minimal human intervention, and ÒevolveÓ to become better
adapted to what they are supposed to do.There is little doubt that such computer systems with these properties would be highly desirable.Although the development of such systems is an active area of computer science research today (in-
deed, the Internet itself is an example of a system that is capable of operating without centralized
authority and reconfiguring itself when parts are damaged), computer science researchers are working
to develop new such systems, and the prospect of looking outside the existing computer science toolbox
for new types of hardware, software, algorithms, or something entirely different (and unknown) is
increasingly attractive.One possible area of research focuses on a set of techniques inspired by the biological sciences,because biological organisms often exhibit properties that would be desirable in computer systems.1A popularized account of biological inspiration for computing is N. Forbes, Imitation of Life: How Biology Is Inspiring Comput-ing, MIT Press, Cambridge, MA, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.248CATALYZING INQUIRYThey function with high degrees of autonomy. Some biological entitiesÑsuch as neurons in a brainÑcan configure themselves automatically into networks (and reconfigure themselves to some degree
when parts are damaged or destroyed). Sensory systems rapidly pick out salient features buried in large
amounts of data. Many animals learn from their environment and become better adapted to what they
are supposed to do. All biological organisms have mechanisms for self-repair, and all multicellular
organisms grow from an initial state that is much less phenotypically complex than their final states.Carver Mead once noted that Òengineers would be foolish to ignore the lessons of a billion years ofevolution.Ó The solutions that nature has evolved to difficult engineering problems are, in many cases,
far beyond present-day engineering capability. For example, the human brain is not fast enough to
process all of the raw sensory data detected by the optic or auditory nerves into meaningful informa-
tion. To reduce processing load, the brain uses a strategy we know as ÒattentionÓ that focuses on certain
parts of the available information and discards other parts. Such a strategy might well be useful for an
artificial machine processing a large visual field. Studies of the way in which humans limit their atten-
tion has led to computational models of the strategy of shifting attention. Such models of biological
systems are worth studying even if they appear intuitively less capable than computation, if only for the
fact that no machine systems exist that can function as autonomously as a housefly or an ant.On the other hand, biological organisms operate within a set of constraints that may limit theirsuitability as sources of inspiration for computing. Perhaps the most important constraint is the fact that
biological organisms emerge from natural selection and the evolutionary process. Because selection
pressures are multidimensional, biological systems must be multifunctional. For example, a biological
system may be able to move, but it has also evolved to be able to feed itself, to reproduce, and to defend
itself. The list of desirable functions in a biological system is long, and successfully mimicking biology
for one particular function requires the ability to separate nonrelevant parts of the system that do not
contribute to the desired function. Furthermore, because biological systems are multifunctional, they
cannot be optimized for any one function. That is, their design always represents a compromise be-
tween competing goals. Organisms must be adequately (rather than optimally) adapted to their envi-
ronments. (The notion of Òoptimal designÓ is also somewhat problematic in the context of stochastic
real-world environments.) Also, optimal adaptation to any one environment is likely to disadvantage
an organism in a significantly different environment, and so adequately adapted organisms tend to be
more robust across a range of environments.The evolutionary process constrains biological solutions as well. For example, biological systemsinevitably include vestiges of genetic products and organs that are irrelevant to the organism in its
current existence. Thus, biological adaptation to a given environment depends not only on the circum-
stances of the environment but also on its entire evolutionary historyÑa fact that may well obscure the
fundamental mechanisms and principles in play that are relevant to the specific environment of interest.
(This point is a specific instantiation of a more general phenomenon, which is that our understanding of
biological phenomena will often be inadequate to provide detailed guidance in engineering a computa-
tional device or artifact.)A corollary notion is that nature may evolve different biological mechanisms to solve a givenproblem. All of these mechanisms may enable the organism to survive and even to prosper in its
environment, but it is far from clear how well these mechanisms work relative to one another.2 Thus,which one of many biological instantiations is the most appropriate model to mimic remains an impor-
tant question.Finally, there are only a few examples of successful biologically inspired computing innovations.Thus, the jury is still out on the ultimate value of biology for computing. Rather than biology beinghelpful across the board to all of computing, the committee believes that biologyÕs primary relevance
(at least in the short term) is likely to be to specific problem areas within computing that are poorly2For example, fish and squid use different mechanisms to propel themselves through the water. Which mechanism is betterunder what circumstances and for what engineered artifacts is a question for research to answer.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING249understood, or for which the relevant underlying technologies are too complex or unwieldy, and inproviding approaches that will address parts of a solution (as described in Section 8.1.2). Neverthe-
less, the potential benefits that biology might offer to certain problem areas in computing are large,
and it is worth exploring different approaches to exploit these benefits; this is the focus of Sections 8.2
to 8.4.8.1.2The Meaning of Biological Inspiration
What does it mean for something to be biologically inspired?  It is helpful to consider several
possible interpretations. One interpretation is that significant progress in computing can occur onlythrough the application of principles derived from the study of biology. This interpretation, offered
largely as a strawman, is absurdÑthere are many ways in which computing can progress without the
application of biologically derived principles.A second, somewhat less grandiose and more reasonable interpretation is that significant progressin computing can occur through the application of principles derived from the study of biology. That is,a biological system may operate according to principles that have applicability to nonbiological com-
puting problems. By studying the biological system, one may be able to derive or understand the
relevant principles and use them to help solve a nonbiological problem. It is this interpretationÑthat
biology is relevant to computing only when principles emerge directly from a study of biological
phenomenaÑthat underlies many claims of biological relevance or irrelevance to computing.A third interpretation is that certain aspects of biology are analogous to aspects of computing,which means that insights from biology are relevant to aspects of computing. This is the case, for
instance, when a set of principles or paradigms turns out to have strong applicability both to a biological
system or systems and to interesting problems in computing. These principles or paradigms may have
had their intellectual origin in the study of a biological or a nonbiological system.When their origin is in a biological system, this interpretation reduces to the second interpretationabove. What makes the case of an origin in a nonbiological system interesting is that the principles in
question may be more manifestly obvious in a biological context than in a nonbiological context. That is,
the principles and their application may most easily be seen and appreciated in a biological context,
even if they did not initially originate in a biological context. Moreover, the biological context may also
provide a source of language, concepts, and metaphors that are useful in talking about a nonbiological
problem or phenomenon.For this report, the term ÒinspirationÓ will be used in its broadest sense, that is, the third interpreta-tion above, but there are three other points to keep in mind:¥Biological inspiration does not mean that the weaknesses of biology must be adopted along withthe strengths. In some cases, it may be possible to overcome problems found in the actual biological
system when the principles underlying them are implemented in engineered artifacts.¥As noted in Chapter 1, even when biology cannot provide insight into potential computingsolutions, the drive to solve biological problems can still inspire interesting, relevant, and intellectually
challenging research in computingÑso biology can serve as a useful and challenging problem domain
for computing.33For example, IBM used the problem of protein folding to motivate the development of the BlueGene/L supercomputer.Specifically, the problem was formulated in terms of obtaining a microscopic view of the thermodynamics and kinetics of thedynamic protein-folding process over longer time scales than have previously been possible. Because this project involved bothcomputer architecture and the exploration of algorithmic alternatives, the applications architecture was structured in such a waythat subject experts in molecular simulation could work on their applications without having to deal with the complexity of theparallel communications environment required by the underlying machine architecture (see BlueGene/L Team, ÒAn OverviewCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.250CATALYZING INQUIRY¥Incomplete (and sometimes even incorrect) biological understandings help to inspire differentand useful approaches to computing problems. Important and valuable insights into possible ways to
solve a current problem have been derived from biological models that were incomplete (as in the case
of evolutionary programming) or even inaccurate (as in the case of immunologically based computer
security).On the other hand, it must be understood that the use of a biological metaphor to inspire newapproaches to computing does not necessarily imply that the biological side is well understood, whether
or not the metaphor leads to progress in computing. That is, even if a biological metaphor is applicable
and relevant to a computing problem, this does not mean that the corresponding biological phenomena
can necessarily be understood in computational terms.For example, although researchers use the term Ògenetic algorithmsÓ to describe a class of algo-rithms using operators that have a similar flavor to evolutionary genetic operators such as mutation or
recombination to search a solution space stochastically, the definition and implementation of these
genetic operators does not imply a fundamental understanding of biological evolutionary processes.
Similarly, although the field of Òartificial neural networksÓ is an information-processing paradigm
inspired by the parallel processing capabilities and structure of nerve tissue, and it attempts to mimic
learning in biology by learning to adjust ÒsynapticÓ connections between artificial processing elements,
the extent to which an artificial neural network reflects real neural systems may be tenuous.8.1.3  Multiple Roles: Biology for Computing Insight
Biological inspiration can play many different roles in computing, and confusion about this multi-plicity of meanings accounts for a wide spectrum of belief about the value of biology for developing
better computer systems and improved performance of computational tasks. One point of view is that
only a detailed Òground-upÓ understanding of a biological system can result in such advances, and
because such understanding is available for only a very small number of biological systems (and Òvery
smallÓ is arguably zero), the potential relevance of biology for computing is small, at least in the near
term.A more expansive view of biologyÕs value for computing acknowledges that detailed understand-ing is the key for a maximal application of biology to computing, but also holds that biological meta-
phors, analogies, examples, and phenomenological insights may suggest new and interesting ways of
thinking about computational problems that might not have been imagined without the involvement of
biology.4 From this perspective, what matters is performance of a task rather than simulation of what abiological system actually does, though one would not necessarily expect initial performance modelsof the BlueGene/L Supercomputer,Ó presented at Supercomputing Conference, November 2002, available at http://sc-2002.org/paperpdfs/pap.pap207.pdf). Other obvious problems inspired by biology include computer vision and artificial intelligence. It isalso interesting to note this historical precedent of biological problems being the domain in which major suites of statistical toolswere developed. For instance, Galton invented regression analysis (correlation tests) to study the relation of phenotypes betweenparents and progeny (see F. Galton, Natural Inheritance, 5th Edition, Macmillan and Company, New York, 1894). Pearson in-vented the chi-square and other discrete tests to study the distribution of different morphs in natural populations (see K.Pearson, ÒMathematical Contributions to the Theory of Evolution, VIII. On the Inheritance of Characters Not Capable of Exact
Quantitative Measurement,Ó Philosophical Transactions of the Royal Society of London, Series A 195:79-150, 1900). R.A. Fisher in-vented analysis of variance to study the partitioning of different effects in inheritance (see R. Fisher, ÒThe Correlation BetweenRelatives on the Supposition of Mendelian Inheritance,Ó Transactions of the Royal Society of Edinburgh 52:399-433, 1918).4An analogy might be drawn to the history of superconducting materials. A mix of quantum principles, phenomenology, andtrained experience has led to superconducting materials with ever-higher transition temperatures. (Indeed, the discovery ofsuperconducting materials preceded quantum mechanics by more than a decade.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING251based on biological systems to function more effectively than models constructed using more tradi-tional techniques.One of biologyÕs most important roles is that it can serve as an existence proof of performanceÑthatsome desirable behavior is possible. The reasoning is that if a biological system can do something
interesting, why canÕt an artificial system to the same thing? Birds fly, so why shouldnÕt people or
constructed artifacts be able to fly? Many biological behaviors and functions would be desirable in a
computing context, and biological systems that exhibit such behavior demonstrate that this behavior is
possible.5Existence proofs are important in engineering. For example, in the view of many nuclear scientistsassociated with the Manhattan Project, the information that was most critical to the Soviet develop-
ment effort was not a secret gained through espionage, but rather the fact that a nuclear explosion was
possible at allÑand that fact was reported in every major newspaper in the world.6 In other words, itis one thing to work toward a goal that may well be impossible to achieve and an entirely different
psychological matter to work toward a goal whose achievement is knownÑwith certaintyÑto be
possible.An example of using a biological metaphor for understanding some dimension of computing re-lates to computer security. From many centuries of observation, it is well known that an ecology based
on a monoculture is highly vulnerable to threats that are introduced from the outside. With this insight
in mind, many expert observers have used the term ÒmonocultureÓ to describe the present-day security
environment for desktop computers in which one vendor dominates the operating system market. This
report does not take a position on whether such a characterization is necessarily accurate,7 but the pointis that the metaphor, used in this manner, can determine the terms of discussion and thus provide a
useful way of looking at the issue.Despite its conceptual value, an existence proof does not speak directly to how to build the artifactso that it does the same thing. That is, existence proofs do not necessarily provide insight about con-
struction or creation. Diversity as a strategy for survival does not necessarily indicate how much or
what kinds of diversity would be helpful in any given instance. Similarly, aerodynamics is a body of
theory that explains the flight of birds, and also enables human beings to design airplanes, but a study
of birds did not lead to the airplane. For construction or creations, a deeper understanding of biology is
required. Knowing what kind of deeper understanding is possible potentially leads to at least three
additional roles for biology:¥Biology as source of principles. Nature builds systems out of the same atoms that are available tohuman engineers. If a biological system can demonstrate a particular functionality, it is because that
system is built according to principles that enable such functionality. The hope is that upon closeexamination, the physical, mathematical, and information-processing principles underlying the inter-esting biological functionality can be applied through human engineering to realize a better artificial
system. Note also that in some cases, the actual principles underlying some biological functionality may
be difficult to discern. However, plausibility counts for a great deal here, and biology may well provide
inspiration for engineered artifacts if human beings propose a set of plausible principles that govern the
behavior of interest in an actual organism, even if those principles, as articulated, turn out not to have a
biological instantiation in that organism. (Note that in this domain the division between Òapplying5An accessible and more extended discussion of these ideas can be found in J. Benyus, Biomimicry: Innovation Inspired by Nature,William Morrow, New York, 1997.6D. Holloway, Stalin and the Bomb: The Soviet Union and Atomic Energy, 1939-1956, Yale University Press, New Haven, 1994.7For example, it may be that even though the number of operating system platforms is small compared to the number ofdesktop computers in use, different computer configurations and different operational practices might introduce sufficientdiversity to mitigate any system-wide instabilities. Furthermore, replication has many other advantages in the computer context,such as easier interoperability.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.252CATALYZING INQUIRYbiological principles to information processingÓ and Òunderstanding biological information process-ingÓ is least meaningful.)¥Biology as implementer of mechanism. Nature also implements mechanisms to effect certain func-tions. For example, a biological organism may implement an algorithm that could be the basis of a
solution to a computing problem of interest to people. Or, it may implement an architecture or a way to
organize and design the structural and dynamic relationships between elements in a complex system,
knowledge of which might greatly improve the design of an engineered artifact. In this category are the
neural network architecture as inspired by the activation model of dendrites and axons in the brain,
evolutionary computation as driven by genomic changes and selection pressures, and the use of
electroactive polymers as actuator mechanisms for robots, inspired by the operation of animal muscles(rather than, for example, gears). (Note that implementations of biological mechanisms tend to be easier
to identify and extract for later use when they involve physical observablesÑand so mechanisms
underlying sensors and locomotion have had some nontrivial successes in their application to engi-
neered artifacts.)¥Biology as physical substrate for computing. Computation can be regarded as an abstract or a physi-cally instantiated form. In the abstract, it is divorced from anything tangible. But all real-world compu-
tation requires hardwareÑa device of some kind, whether artificial or biologicalÑand given that bio-
logical organisms are functional physical devices, it makes sense to consider how engineered artifacts
might have biological components. For example, biology may provide parts that can be integrated into
engineered devices. Thus, a sensitive chemical detection system might use a silk moth as the sensor for
chemicals in the air and thus instrument the moth to appropriate readouts. Or a small animal might be
used as the locomotive platform for carrying a useful payload (e.g., a camera), and its movements might
be teleoperated through electrodes implanted in the animal by a human being viewing the images sent
back by a camera.These three different roles are closely connected to the level(s) of abstraction appropriate for think-ing about biological systems. For some systems and phenomena of interest, a very Òbottom-upÓ per-
spective is warranted. In the same way that one needs to know how to use transistors to build a logic
gate for a silicon-based computer, one needs to know how neurons in the brain encode information in
order to understand how a neural implant or prosthetic device might be constructed. For other systems
and phenomena, architecture provides the appropriate level of abstraction. In this case, understanding
how parts of a system are interconnected, the nature of the information that is passed between them,
and the responses of those parts to such information flows may be sufficient.Another way of viewing these three roles is to focus on the differences between computationalcontent, computational representation, and computational hardware. Consider, for example, a catenary
curveÑthe shape that a cable suspended at both ends takes when subjected to gravity.¥The computational content is specified by a differential equation and the appropriate boundaryconditions. Although the solution is not directly apparent from the differential equation, the differential
equation implies a specific curve that represents the answer.¥The computational representation refers to how the computation is actually representedÑindigital form (as bits in a computer), in analog form (as voltages in an analog computer), in neural form
(as how a calculus student would solve the problem), or in physical form (as the string or cable being
represented).¥The computational hardware refers to the physical device used to solve the equationÑthe digitalcomputer, the analog computer, the human being, or the cable itself.These three categories correspond roughly and loosely to the three categories described above: contentas source of principles, representation as implementer of mechanism, and hardware as physical substrate.
The remaining sections of this chapter describe some biological inspirations for work in computing.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING2538.2  EXAMPLES OF BIOLOGY AS A SOURCE OF PRINCIPLES FOR COMPUTING
8.2.1  Swarm Intelligence and Particle Swarm Optimization
Swarm intelligence is a property of systems of nonintelligent, independently acting robots thatexhibit collectively intelligent behavior in an environment that the robots do sense and can alter.8  Oneform of swarm intelligence is particle swarm optimization, based on the flocking of birds.9The canonical example of flocking behavior is a flight of birds wheeling through the sky, or a schoolof fish darting through a coral reef. Somehow, myriad not-very-bright individuals manage to move,
turn, and respond to their surroundings as if they were as a single, fluid organism. Moreover, they seem
to do so collectively, without a leader: biologists armed with high-speed video cameras have shown that
the natural assumptionÑthat each flock or school has a single, dominant individual that always ini-
tiates each turn just a fraction of a second before the others followÑis simply not true.The first known explanation of the leaderless, collective quality of flocking or schooling behavioremerged in 1986. This explanation used swarms of simulated creaturesÑÒboidsÓÑthat could form
surprisingly realistic flocks if each one simply sought to maintain an optimum distance from its neigh-
bors. The steering rules of the so-called Reynolds simulation were simple:10¥Separation: steer to avoid crowding local flock mates.¥Alignment: steer toward the average heading of local flock mates.¥Cohesion: steer toward the average position of local flock mates.These rules were entirely local, referring only to what an individual boid could see and do in itsimmediate vicinity;11 none of them said, ÒForm a flock.Ó Yet the flocks formed every time, regardless ofthe starting positions of the boids. These flocks were able to fly around obstacles in a very fluid and
natural manner. Sometimes the flock would even break into subflocks that flowed around both sides of
an obstacle, rejoining on the other side as if the boids had planned it all along. In one run, a boid
accidentally hit a pole, fluttered around for a moment, and then darted forward to rejoin the flock as it
moved on.Today, the Reynolds simulation is regarded as one of the best and most evocative demonstrations ofemergent behavior, in which complex global behavior arises from the interaction of simple local rules. Theapproach embodied in the simple-rule/complex-behavior approach has become a widely used tech-
nique in computer animationÑwhich was ReynoldsÕ primary interest in the first place.128T. White, ÒSwarm Intelligence: A Gentle Introduction with Applications,Ó PowerPoint presentation, available at http://www.sce.carleton.ca/netmanage/tony/swarm-presentation/tsld001.htm.9Bird flocks are an example of complex, adaptive systems. Among the many other examples that scientists have studied are theworld economy, brains, rain forests, traffic jams, corporations, and the prehistoric Anasazi civilization of the Four Corners area.Complex adaptive systems are similar in structure and behavior even if they differ in their superficial manifestations. Forexample, complex adaptive systems are massively parallel and involve many quasi-independent ÒagentsÓ interacting at once.(An agent might be a single firm in an economy, a single driver on a crowded freeway, and so on.) Each of them is adaptive,
meaning that the agents that constitute them are constantly responding and adapting to each other. And each of them isdecentralized, meaning that no one agent is in charge. Instead, a complex systemÕs overall behavior tends to emerge spontane-ously from myriad low-level interactions.10C.W. Reynolds, ÒFlocks, Herds, and Schools: A Distributed Behavioral Model,Ó Computer Graphics 21(4):25-34, 1987, availableat http://www.cs.toronto.edu/~dt/siggraph97-course/cwr87/ and http://www.red3d.com/cwr/papers/1987/SIGGRAPH87.pdf. An updated discussion, with many pictures and references to modern applications, can be found in C.W. Reynolds, ÒBoids:
Background and Update,Ó 2001, available at http://www.red3d.com/cwr/boids/.11More precisely, each boid had global information about the physical layout of its environment, including any obstacles, but ithad no information about its flock mates, except for those that happened to come within a certain distance that defined its localneighborhood.12The first Hollywood film to use a version of ReynoldsÕ boids software was Tim BurtonÕs Batman Returns (1992), whichfeatured swarms of animated bats and flocks of animated penguins. Since then it has been used in films such as The Lion King(1994) and many others (see http://www.red3d.com/cwr/boids/).Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.254CATALYZING INQUIRYA second simulation of flocking behavior, developed in 1990, employed the ReynoldsÕ rules (thoughthey were independently developed) and also incorporated the influence of Òdynamic forcesÓ on the
behavior of the simulated creatures.13 These dynamic forces would allow the creatures to be attractedtoward a convenient roosting point, say, or a particularly rich cornfield. As a result, the flock would
turn and head in the direction of a cornfield as soon as it was placed into view, with various subgroups
swinging out and in again until finally the whole group had landed right on target.These two models are direct ancestors of the particle swarm optimization (PSO) algorithm, firstpublished in 1995.14 The algorithm substitutes a mathematical function for the original roosts andcornfields, and employs a conceptual swarm of bird-like particles that swoop down on the functionÕs
maximum value, even when the function has many local maxima that might confound more standard
optimization algorithms.The essential innovation of the PSO algorithm is to scatter particles at random locations throughouta multidimensional phase space that represents all the arguments to the function to be maximized. Then
the algorithm sets the particles in motion. Each particle evaluates the function as it flies through phase
space and keeps trying to turn back toward the best value that it has found so far. However, it is
attracted even more toward the best value that any of its neighboring particles have found. So it
inexorably begins to move in that directionÑalbeit with a little built-in randomness that allows it to
explore other values of the function along the way. The upshot is that the particles quickly form a flock
that flows toward a point that is one of the highest function values available, if not the highest.The PSO algorithm is appealing for both its simplicityÑthe key steps can be written in just a fewlines of computer codeÑand its effectiveness. In the original publication of the PSO algorithm, the
algorithm was applied to a variety of neural network problems, and it was found to be a very efficient
way to choose the optimum set of connection weights for the network.15 Since then, the basic techniquehas been refined and extended to systems that have discrete variables, say, or that change with time. It
also has been applied to a wide variety of engineering problems,16 such as the automatic adjustment ofpower systems.17The PSO algorithm is biologically inspired in the sense that it is a plausible account of bird flockingbehavior. However, it is not known whether birds, in fact, use the PSO algorithm to fly in formation.Swarm algorithms have the virtues of simplicity and robustness, not to mention an ability to func-tion without the need for centralized control. For this reason, they may find their most important
applications in, say, self-healing and self-organizing communications networks or in electrical power
networks that could protect themselves from line faults and reroute current around a broken link Òon
the fly.Ó18On the other hand, simple rules are not automatically good. Witness army ants, which are suchobsessive self-organizers that the members of an isolated group will often form a Òcircular mill,Ó follow-13F.H. Heppner and U. Grenander, ÒA Stochastic Nonlinear Model for Coordinated Bird Flocks,Ó The Ubiquity of Chaos, S.Krasner, ed., AAAS Publications, Washington, DC, 1990.14J. Kennedy and R.C. Eberhart, ÒParticle Swarm Optimization,Ó pp. 1942-1948 in Proceedings of the IEEE International Conferenceon Neural Networks, IEEE Service Center, Piscataway, NJ, 1995; R. Eberhart, Y. Shi, and J. Kennedy, Swarm Intelligence, MorganKaufman, San Francisco, CA, 2001.15See Section 8.3.3.2 for further discussion.16A good sense of current activity in the field can be gleaned from the programs and talks at the 2003 IEEE Swarm IntelligenceSymposium, April 24-26, 2003, available at http://www.computelligence.org/sis/index.html. Extensive references to PSO canbe found at ÒWelcome to Particle Swarm Central,Ó 2003, available at http://www.particleswarm.info. This site also contains a
number of links to online tutorials and downloadable PSO code.17K.Y. Lee and M.A. El-Sharkawi, eds., Modern Heuristic Optimization Techniques with Applications to Power Systems, John Wileyand IEEE Press, New York, March 2003.18E. Bonabeau, ÒSwarm Intelligence,Ó presented at the OÕReilly Emerging Technology Conference, April 22-25, 2005, SantaClara, CA. Powerpoint presentation available at http://conferences.oreillynet.com/presentations/et2003/Bonabeau_eric.ppt.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING255ing one another around and around and around until they die from starvation.19 Such blind-leading-the-blind behaviors are an ever-present possibility in swarm intelligence; the trick is to find simple rules
that minimize the chances of that happening.A closely related challenge is to find ways of designing emergent behavior, so that the swarm willproduce predictable and desirable results. Today, swarm algorithms are based on the loose and impre-
cise specification of a relatively small number of parametersÑbut it is almost certainly true that engi-
neered artifacts that exhibit complex designed behavior will require the tight specification of many
parameters.This point is perhaps most obvious in the cooperative construction problem, where the rule sets thatproduce interesting, complex structures are actually very rare; most self-organized structures look more
like random blobs.20 The same problem is common to all collective behaviors; finding the right rules isstill largely a matter of trial and errorÑnot least because it is in the very nature of emergence for a
simple-seeming change in the rules to produce a huge change in the outcome. Thus, in their efforts to
find the right rules, researchers may well seek to develop procedures that will find in the right rules
rather than trying to find them directly themselves. This point is discussed further in Section 8.3.1.8.2.2  Robotics 1: The Subsumption Architecture
One approach to robotic design is based on the notion that complex and highly capable systems areinherently expensive, and hence fewer can be built. Instead, this approach asserts the superiority of
using large numbers of individually smaller, less capable, and inexpensive systems.21 In 1989, Brooksand Flynn suggested that Ògnat robotsÓ might be fabricated using silicon micromachining to fabricate
freely movable structures onto silicon wafers. Such an approach potentially allows sensors, actuators,
and electronics to be embedded on the same silicon substrate. This arrangement is the basis for BrooksÕ
subsumption architecture, in which low-level functionality can be used as building blocks for higher-
level functionality.Robots fabricated in this manner could be produced by the thousands, just as integrated circuits areproduced todayÑand thus become an inexpensive, disposable system that does its work and need not
be retrieved. For applications such as exploration in hostile environments, the elimination of a retrieval
requirement is a significant cost savings.To the best of the committeeÕs knowledge, no self-propelled robots or other operational systemshave been built using this approach. Indeed, experience suggests that the actual result of applying the
swarm principle is that one highly capable robot is not replaced by many robots of lesser capability, but
rather one such robot. This suggests that real-world applications are likely to depend on the ability tofabricate many small robots inexpensively.A key challenge is thus to develop ways of assembling microrobots that are analogous to chipfabrication production lines. One step toward meeting this challenge has been instantiated in a concept
known as Òsmart dust,Ó for which actual prototypes have been developed. Smart dust is a concept for a19B. Hılldobler and E.O. Wilson, The Ants, Belknap Press of Harvard University Press, Cambridge, MA, 1990, pp. 585-586. In afamous account published in 1921, the entomologist William Beebe described a mill he saw in the Amazonian rain forest thatmeasured some 360 meters across, with each ant taking about 21/2 hours to complete a circuit. They kept at it for at least 2 days,stumbling along through an ever-accumulating litter of dead bodies, until a few workers finally straggled far enough from the
trail to break the cycle. And from there, recalled Beebe, the group resolutely marched off into the forest. See W. Beebe, Edge of theForest, Henry Holt and Company, New York, 1921.20But then, so do most insect nests. Honeycombs, waspsÕ nests, and other famous examples are the exception rather than therule.21R.A. Brooks and A.M. Flynn, ÒFast, Cheap and Out of Control: A Robot Invasion of the Solar System,Ó Journal of the BritishInterplanetary Society 42:478-485, 1989.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.256CATALYZING INQUIRYhighly distributed sensor system.22 Each dust mote has sensors, processors, and wireless communica-tions capabilities and is light enough to be carried by air currents. Sensors could monitor the immediate
environment for light, sound, temperature, magnetic or electric fields, acceleration, pressure, humidity,
selected chemicals, and other kinds of information, and the motes, when interrogated, would send the
data over kilometer-scale ranges to a central base station, as well as communicate with local neighbors.This architecture was the basis of an experiment that sought to track vehicles with an unmanned aerialvehicle (UAV)-delivered sensor network.23 The prototype sensors were approximately a cubic inch involume and contained magnetic sensors for detecting vehicles (at ranges of about 10 meters), a micropro-
cessor, radio-frequency communications, and a battery or solar cell for power. With six to eight air-
delivered sensor motes landed diagonally across a road at about 5-meter intervals, the sensor network was
able to detect and track vehicles passing through the network, store the information, and then transfer
vehicle track information from the ground network to the interrogating UAV and then to the base camp.The subsumption architecture also asserts that this robust behavior can emerge from the bottomup.24 For example, in considering the problem of an autonomously functioning vehicle (i.e., one thatdrives itself), a series of layers can be defined that¥Avoid contact with objects (whether the objects move or are stationary),¥Wander aimlessly around without hitting things, and¥Explore the world by seeing places in the distance that look reachable and heading for them.Any given level contains as a subset (subsumes) the lower levels of competence, and each level canbe built as a completely separate component and added to existing layers to achieve higher levels of
competence. In particular, a level 0 machine would be built that simply avoided contact with objects. A
level 1 machine could be built by adding another control layer that monitors data paths in the level 0
layer and inserts data onto the level 0 data paths, thereby subsuming the normal data flow of level 0.
More complex behavior is thus built on top of simpler behaviors.Brooks claims that the subsumption architecture is capable of accounting for the behavior of insects,such as a house fly, using a combination of simple machines with no central control, no shared representa-
tion, slow switching rates, and low-bandwidth communication. This results in robust and reliable behavior
despite its limited sensing capability and an unpredictable environment, because individual behaviors can
compensate for each othersÕ failures, resulting in coherent and emergent behavior despite the limitations of
the component behaviors. A number of robots have been built using subsumption architectures. Of particu-
lar note is Hannibal,25 a hexapod with more than 100 physical sensors and 1,500 augmented finite-state
machines grouped into several dozen behaviors split over eight on-board computers.268.2.3  Robotics 2: Bacterium-inspired Chemotaxis in Robots
27The problem of locating gradient sources and tracking them over time is an important problem inmany real-world contexts. For example, fires cause temperature gradients in their immediate vicinity;22See, for example, http://robotics.eecs.berkeley.edu/~pister/SmartDust/.23See http://robotics.eecs.berkeley.edu/~pister/29Palms0103/.24R.A. Brooks and A.M. Flynn, ÒFast, Cheap and Out of Control,Ó 1989.25C. Ferrell, ÒRobust Agent Control of an Autonomous Robot with Many Sensors and Actuators,Ó Ph.D. thesis, Department ofElectrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, 1993.26A finite-state machine is a machine with a finite number of internal states that transitions from one state to another on thebasis of a specified function. That is, the argument of the function is the machineÕs previous state, and the functionÕs output is itsnew state. An augmented finite-state machine is a finite-state machine augmented with a timer that forces a transition after a
certain time.27Material in Section 8.2.3 is based on excerpts from A. Dhariwal, G.S. Sukhatme, and A.A.G. Requicha, ÒBacterium-inspiredRobots for Environmental Monitoring,Ó International Conference on Robotics and Automation, New Orleans, LA, April 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING257chemical spills lead to chemical concentration gradients in the soil and/or water; ecosystems hostgradients of light, salinity, and pH. In many cases, the source intensity of these gradients varies with
time (e.g., because of movement of the source), and there may be multiple sources for any given
characteristic (e.g., two fires causing a complex temperature gradient).Autonomous detection, location, and tracking of gradient sources would be very helpful for thosetrying to study or respond to the environment. Using robots, an environmental scientist might need to
find the source(s) of a given toxic chemical, whereas a firefighter might need to locate the source(s) of a
fire in order to extinguish it.Noting that other approaches for locating and tracking gradient sources were primarily useful instatic or quasi-static environments, and inspired by biological studies of how bacteria are attracted to
gradient sources of nutrition, Dhariwal et al.28 sought to develop a strategy for finding gradient sourcesthat worked well with sources that are small, weak, mobile, or time-varying in intensity. Specifically,
their algorithm is based on the repetition of a straight-line run for a certain time, followed by a random
change in direction that sets up the direction for a new run. If the bacterium senses a higher concentra-
tion in its immediate environment, the run length is longer. Thus, although the bacterium still under-
goes a random walk, it is a random walk biased in the direction of the gradient source.This algorithm is also well suited for implementation in a simple robot. That is, only the last sensorreading must be stored, and so memory requirements are lower. Because only one computation has to
be done (a comparison between the present and the previous sensor reading), processing requirements
are minimal.Dhariwal et al. compared the performance of this algorithm with a simple gradient descent algo-rithm. They found that for single, weak sources, the simple gradient algorithm displayed better perfor-
mance. However, the bacterium-inspired algorithm displayed better performance in locating and track-
ing multiple and/or dissipative sources and in covering the entire area in which the gradient can be
found.8.2.4  Self-healing Systems
In the past few years, the term Òself-healingÓ has become a fashionable object of study and interestin the academic and research computer science communities29 and in the marketing materials of infor-mation technology (IT) companies such as IBM,30 Microsoft,31 Sun,32 and HP.33 Despite (or becauseof?) this level of interest, there is no commonly accepted definition of Òself-healingÓ or agreement of
what functionality it encompasses or requires.28A. Dhariwal, G.S. Sukhatme, and A.A.G. Requicha, ÒBacterium-inspired Robots for Environmental Monitoring,Ó IEEE Inter-national Conference on Robotics and Automation, New Orleans, LA, April 25-30, 2004, available at http://www-lmr.usc.edu/~lmr/publications/Icra04bact.pdf.29Workshop on Self-healing, Adaptive and Self-managed Systems (SHAMAN), June 23, 2002, available at http://www.cse.psu.edu/~yyzhang/shaman/proc.html; ICSE 2003 Workshop on Software Architectures for Dependable Systems, May 2003 (formore information, see http://www.cs.kent.ac.uk/events/conf/2003/wads/); David Garlan, Self-healing Systems Course, #17-
811, Carnegie Mellon University seminar, Spring 2003 (for more information see http://www-2.cs.cmu.edu/~garlan/17811/);D. Garlan, J. Kramer, and A. Wolf, eds., Proceedings of the First Workshop on Self-healing Systems, ACM Press, New York, 2002.30M. Hamblen, ÒIBM to Boost Self-healing Capabilities in Tivoli Line,Ó Computerworld, April 4, 2003, available at http://www.computerworld.com/softwaretopics/software/story/0,10801,80050,00.html.31"Windows 2000 Professional: Most Reliable Windows Ever,Ó December 5, 2000, available at http://www.microsoft.com/windows2000/professional/evaluation/business/overview/reliable/default.asp.32"Sun and Raytheon Create Open, Adaptive, Self-healing Architecture for DD 21,Ó available at http://wwws.sun.com/software/jini/news/Jini-Raytheon.pdf.33"HP Delivers Self-healing and Virtual Server Software to Advance the Adaptive Enterprise,Ó press release, May 6, 2003,available at http://www.hp.com/hpinfo/newsroom/press/2003/030506c.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.258CATALYZING INQUIRYIn fact, many of the techniques described as self-healing are familiar to the decades-old hardwarefield of reliable systems, also known as fault tolerance or high availability. These techniques, such as
fault detection, fault masking, and fault tolerance, are in common use when designing hardware to
improve the reliability and availability of large systems. This is most likely because hardware designers,
unlike software programmers, long ago accepted the unavoidable reality that components of their
designs will fail at some point. (It also helps immeasurably that hardware failures are often easier to
characterize than software failures.) In areas with extremely high demands for reliability, such as
aerospace or power plants, these fault-tolerance techniques have become quite sophisticated, as have
mechanisms for testing system operation. The oldest and most accepted use of the term self-healing is
found in networking;34 networks from the original ARPANET (and even the public switched telecom-munications network) to modern peer-to-peer embedded networks are self-healing in the sense that
traffic is routed around unresponsive nodes.In contrast, until quite recently, software quality has focused on producing bug-free products, by anintensive effort of careful design, code review, and extensive prerelease testing. However, when bugs
do occur, software typically has no ability to detect or react to them, or to continue to operate. This was
a workable strategy for much of the history of modern software, but the continuing rise of the complex-
ity of software applications has made formal review or correctness proofs inadequate to provide mini-
mum levels of reliability.35This rise in complexity and the resulting rise in human cost of configuration and maintenance ofsoftware applications has spurred interest in self-healing, hoping to shift much of the burden of this
configuration and maintenance back to the software. The idea is that, like its biologically analogous
namesake, a self-healing system would detect the presence of nonfunctioning (or, more challengingly,
malfunctioning) components and initiate some response to continue proper overall functionality, pref-
erably without any centralized or external force (such as a system administrator) required. The most
common implementation today seems to be one of reconfiguration: if a fault is detected, a spare hard-
ware component is brought into play. This is ÒhealingÓ only in the loosest sense, although it certainly is
a valid fault tolerance technique. However, it doesnÕt translate well to software-only failures.None of the systems that describe themselves as self-healing (such as Microsoft Windows 2000, IBMDB/2, or SunÕs Jini) seem to actually employ biological principles, other than in the grossest sense of
having redundancy. However, one research project that is inspired very explicitly by biology is Swarm
at the University of Virginia.36 The Swarm programming model defines units as individual cells, whichcan both reproduce through cellular division and die. Additionally, they can emit signals at various
strengths and respond to the aggregate strength of signals in the environment. For example, a system
set to grow to a certain size would start with a single cell that emitted a small amount of signal and with
a program set to reproduce if the aggregate signal was at a certain threshold. Until the total amount of
signal exceeded that threshold, the cells would continue to divide, but they would stop once the
threshold was exceeded. If cells were to fail or otherwise be deleted, other cells would respond by
dividing again to bring the signal back to the threshold. This is indeed a primitive form of self-healing.
However, this programming model is unlikely to catch on for complex tasks without significant higher-
level abstractions available.34W.D. Grover, ÒThe Self-healing Network: A Fast Distributed Restoration Technique for Networks Using Digital Cross-connect Machines,Ó Proceedings of the IEEE Global Telecommunications Conference, Tokyo, 1987, pp. 1090-1095.35In his lecture on receiving the ACM Turing Award in 1980, C.A.R. Hoare said, ÒThere are two ways of constructing asoftware design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it socomplicated that there are no obvious deficiencies.Ó Lecture available at http://www.braithwaite-lee.com/opinions/p75-hoare.pdf.36G. Selvin, D. Evans, and L. Davidson, ÒA Biologically Inspired Programming Model for Self-healing Systems,Ó Proceedings ofthe First Workshop on Self-Healing Systems, November 2002, available at http://www-2.cs.cmu.edu/~garlan/woss02/.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING2598.2.5  Immunology and Computer Security
37The mammalian immune system is an information processorÑthis is clear from its ability to distin-guish between self and nonself. (Section 5.4.4.3 provides a brief introduction to the immune system.)
Some have thus been drawn to the architecture of the immune system as a paradigm of information
processing that might be useful in solving a variety of different computational problems. Immunologi-
cal approaches have been proposed for solving problems in computer security, semantic classification
and query, document and e-mail classification, collaborative filtering problem, and optimization.38 Thissection concentrates on computer security applications.8.2.5.1  Why Immunology Might Be Relevant
Computer and network security is intended to keep external threats at bay, and this remains anintellectually challenging problem of the highest order. It is useful to describe two general approaches
to such security problems. The first, widely in use today, is based on the notion of what might be called
environmental controlÑthe idea that by adequately controlling the environment in which a computer
or network functions, better security can be obtained. The computer or network environment is defined
broadly, to include security policy (who should have what rights and privileges), resources (e.g., pro-
grams that provide users with computing or communications capability), and system configuration. In
support of this approach, a number of reports39 cite security problems that arise from flaws in securitypolicy, bugs in programs, and configuration errors and argue that correcting these flaws, bugs, and
errors will result in greater security.A complementary approach is to take as a given the inability to control the computing or networkenvironment.40 This approach is based on the idea that computer security can result from the use ofsystem design principles that are more appropriate for the imperfect, uncontrolled, and open environ-
ments in which most computers and networks currently exist. Note that there is nothing mutually
exclusive about the two approachesÑboth could be used in the design of an effective overall approach
to system or network security.For inspiration in addressing problems in computer security, some researchers have considered theimmune system and the unpredictable and largely hostile environment in which it functions.41 That is,the unpredictable pathogens to which the immune system must respond are analogous to some of the
threats that computer systems face, and the principles underlying the operation of the immune system
may provide new approaches to computer security.8.2.5.2  Some Possible Applications of Immunology-based Computer Security
A variety of loose analogies between computer security and immunology are intuitively obvious,and there is clearly at least a superficial conceptual connection between the protection afforded to37The discussion in Section 8.2.5 owes much to Stephanie Forrest of the University of New Mexico.38For a view of the immune system as information processor, see S. Forrest and S. Hofmeyr, ÒImmunology as InformationProcessing,Ó Design Principles for Immune Systems and Other Distributed Autonomous Systems, L.A. Segal and I.R. Cohen, eds.,Oxford University Press, 2000. For an overview of various applications of an immunological computing paradigm, seewww.hpl.hp.com/personal/ Steve_Cayzer/downloads/030213ais.ppt and references therein.39National Research Council, Cybersecurity Today and Tomorrow: Pay Now or Pay Later, National Academy Press, Washington,DC, 2002.40This discussion is based on A. Somayaji, S. Hofmeyr, and S. Forrest, ÒPrinciples of a Computer Immune System,Ó Proceedingsof the 1997 Workshop on New Security Paradigms, ACM Press, Langdale, UK, 1998, pp. 75-82.41One of the first papers to suggest that self-nonself discrimination, as used by the immune system might be useful in computersecurity was by S. Forrest, A.S. Perelson, L. Allen, and R. Cherukuri, ÒSelf-nonself Discrimination in a Computer,Ó Proceedings of the1994 IEEE Symposium on Research in Security and Privacy, IEEE Computer Society Press, Los Alamitos, CA, 1994, pp. 202-212. Thispaper focused mainly on the issue of protection against computer viruses but set the stage for a great deal of subsequent work.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.260CATALYZING INQUIRYhuman beings by the immune system and computer security. The following examples are adapted fromSomayaji et al.:42¥Protecting active processes on a single host. For this application, a computer running multiple pro-cesses might be conceptualized as a multicellular organism (in which each process is analogous to a
cell). An adaptive immune system could be a detector process that queried other processes to see
whether they were functioning normally. If not (i.e., if the detector process found ÒnonselfÓ in its
probes), the adaptive system could slow, suspend, kill, or restart the misbehaving process. One ap-
proach to detection (positive detection) is based on the establishment of a profile of observed normal
behaviors and using that profile to notice when a program behaves abnormally.43¥Protecting a network of computers. For this application, each computer in a network might beconceptualized as a cell in an individual. Each process would still be considered as a cell, but now an
individual is a network of computers. (Another possible analogy for the network of computers is that
each computer represents a single organism and population-level protections are achieved by the col-
lective group through independence, diversity, and sharing of information.) An adaptive detector pro-
cess could be implemented as described above, with the added feature that these detectors could
migrate between computers, thereby enabling all computers on the network to benefit from the detec-
tion of a problem on one of them.¥Protecting a network of disposable computers. This application is similar to that described above,with the addition that when an anomaly is detected, the problematic machine can be isolated, rebooted,
or shut down. If the true source of the anomaly were outside the network, a detector process or system
could stand in for the victimized machine, doing battle with the malicious host and potentially sacrific-
ing itself for the good of the network. Note that this application requires that hosts be more or less
interchangeableÑotherwise the network could not afford the loss of a single host.8.2.5.3  Immunological Design Principles for Computer Security
The immune system exhibits a number of characteristicsÑone might call them design principlesÑthat could reasonably describe how effective computer security mechanisms might operate in a com-
puter system or network. (As in Section 5.4.4.3, Òimmune systemÓ is understood to mean the adaptiveimmune system.) For example, the immune system is:44¥Distributed, in the sense that it has no central point of control. Instead, the components of theimmune system interact locally to mount responses to foreign pathogens (e.g., pathogen detectors
[lymphocytes] operate locally to flag the presence of pathogens). By contrast, a computer system based
on centralized control is vulnerable to ÒdecapitationÓÑa successful attack on the point(s) of centralized
control renders the system entirely useless.45¥Diverse, in the sense that because of the ways in which pathogen detectors are produced, eachindividual human being can detect a somewhat different set of pathogensÑa diversity that protects42A. Somayaji, S. Hofmeyr, and S. Forrest, ÒPrinciples of a Computer Immune System,Ó Proceedings of the 1997 Workshop on NewSecurity Paradigms, ACM Press, Langdale, UK, 1998, pp. 75-82.43An alternative approach is to use a randomly generated detector or set of detectors, living for a limited amount of time, afterwhich it would be replaced by another detector. Detectors that proved particularly useful during their lifetimes (e.g., by detect-ing new anomalies) could be given a longer life span or allowed to spawn related processes. This approach has been used byForrest et al. in the development of a network intrusion detection system known as LISYS.44This discussion of the immune system is based on S. Forrest and S. Hofmeyr, ÒImmunology as Information Processing,ÓDesign Principles for Immune Systems and Other Distributed Autonomous Systems, L.A. Segal and I.R. Cohen, eds., Oxford UniversityPress, New York, 2001.45A distributed, mobile agent architecture for security was also proposed in M. Crosbie and G. Spafford, ÒActive Defense of aComputer System Using Autonomous Agents,Ó Technical Report 95-008, Department of Computer Science, Purdue University,1995.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING261the species as a whole. By contrast, computer system monoculture (i.e., lack of diversity) implies thatsystems share vulnerabilities, and a successful attack on one system is likely to succeed on other
systems as well.46¥Autonomous, in the sense that it classifies and eliminates pathogens and repairs itself by replacingdamaged cells without the benefit of any centralized control mechanism. Given the growing security
burden placed on todayÕs computer systems and networks, it will be increasingly desirable for these
system and networks to manage security problems with minimal human intervention.¥Tolerant of error, in the sense that some mistakes in identification of pathogens (false positives orfalse negatives) are not generally fatal and do not cause immune system collapse, although they can
cause lingering autoimmune disease. Such tolerance is in part the result of a multilayered design of the
immune system, in which multiple, independently architected layers of defense (Òdefense in depthÓ)
operate to provide levels of protection that are not achievable by any single mechanism.47 Computersystems are often not so tolerant, and small errors or problems in some part of a system can lead to
significant malfunctions.¥Dynamic, in the sense that pathogen detectors are continually being produced to replace thosethat are (routinely) destroyed. These detectors, circulated through the body, provide whole-body pro-
tection and may be somewhat different in each new generation (in that they respond to different
pathogens). Because these detectors turn over, the immune system has a greater potential coverage. By
contrast, protection against computer viruses, for example, is based on the notion that all threat viruses
are knownÑand most antiviral systems are unable to cope with a new virus for which no signature is
known.¥Capable of remembering (adaptable), in the sense that the immune system can learn about newpathogens and ÒrememberÓ how it coped with one pathogen in order to respond more effectively to a
future encounter with the same or a similar pathogen. It can also ÒforgetÓ about nonself entities that are
incorporated into the body (e.g., food gets turned into body parts). Computer systems must also adapt
to new environments, as for example, when new software is added legitimately, as well as identify new
threats.¥Imperfect, in the sense that individual pathogen detectors do not identify pathogens perfectly, butrather respond to a variety of pathogens. Greater specificity is obtained through redundant detection of
a pathogen using different detector types. By contrast, computer security systems that look for precise
signatures of intruders (e.g., viruses) are easily circumvented.¥Redundant, in the sense that multiple and different immune system detectors can recognize apathogen. Pathogens generally contain many parts, called epitopes, that are recognized by immune
system detectors; thus, failure to recognize one epitope is not fatal because many others are available for
recognition.¥Homeostatic, in the sense that the immune system can be regarded as one mechanism throughwhich the human body seeks to maintain a stable internal state despite a changing environment. A
computer system can be designed to autonomously monitor its own activities, routinely making small
corrections to maintain itself in a ÒnormalÓ state, even in the face of wide variations in inputs, such as
those caused by intruders.48At a deeper level, it is instructive to ask whether the particular methods by which the immunesystem achieves these characteristics (implements these design principles) have potential relevance to
computer security. To address this issue, deeper and more detailed immunological knowledge is neces-
sary, but some work has been done in this area and is described below.46For more discussion of this point, see Computer Science and Telecommunications Board, National Research Council, Com-puters at Risk: Safe Computing in the Information Age, National Academy Press, Washington, DC, 1991.47This point suggests that detection mechanisms are biased to be more tolerant of false negatives than false positives, becausethreats that are unaffected by one layer (i.e., false negatives) might well be intercepted by another.48A. Somayaji and S. Forrest, ÒAutomated Response Using System Call Delays,Ó Journal of Computer Security 6:151-180, 1998.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.262CATALYZING INQUIRY8.2.5.4  An Example: Immunology and Intruder Detection
To detect pathogens, the immune system generates detectors that can bind to pathogens, and onlyto pathogens (i.e., do not bind to self). (A detector binding to a pathogen is the marker of a detection
event.) To vastly simplify a complex process, the immune system first generates detectors at random.
Through a process known as tolerization, detectors that bind to self are destroyed, leaving only detec-
tors that bind to nonself at the end; these detectors are called mature. Mature detectors are released
throughout the body; if they do not bind to a nonself entity in some period of time (several days?), they
are destroyed (self-destruct?). Those that do bind to nonself entities are regarded as activated detectors.
However, an activated detector must receive a second, independent signal (created by the binding of
another type of detector to the same pathogen costimulation) to become capable of surviving for a long
period of time. These long-term survivors are memory detectors that enable subsequent immune re-
sponses to be generated much more rapidly and are the basis for long-term immunity. (Memory detec-
tors have lifetimes that range from days to the lifetime of an organism, and the underlying mechanisms
governing their lifetimes are not well understood.)In the context of computer security, Forrest and Hofmeyr have described models for networkintrusion detection and virus detection.49 In the network intruder detection example, self is definedthrough a set of ÒnormalÓ connections in a local area network. Each connection is defined by a triplet
consisting of the addresses of the two parties in communication with each other and the port over which
they communicate (a total of 49 bits), and the set of all triplets (normal triplets) generated during a
training period represents, by definition, normal operation of the network.When the network operates outside the training period, the intrusion detection system generatesrandom detector strings that are 49 bits in length. Matches are declared according to an Òr-contiguous-bitÓ ruleÑa match is deemed to exist if a random detector string matches some normal triplet in at least
r contiguous bit positions. In this phase (the maturation phase), detector strings that match some normaltriplet are eliminated, leaving only mature detectors that have not matched any normal triplet.Mature detectorsÑwhich might match an abnormal triplet that arises as the result of a networkintrusionÑare then exposed to the nontraining network operation. If a mature detector matches some
triplet found in the nontraining network operation, such a match is potentially a sign of network
intrusion (which would be indicated by an unusual pair of systems communicating over an unusual
port). If a mature detector does not match any such triplet in a given period of time, it too is elimi-
nated.50 The remaining detectorsÑactivated detectorsÑare now fully capable of signaling the presenceof abnormal triplets.However, as a further guard against false positives, the system invoked a mechanism inspired byimmunological costimulation. Costimulation reduces the likelihood that a pathogen will be indicatedwhen there is no pathogen. After negative selection of lymphocytes occurs, the remaining now-mature
lymphocytes are likely to bind to nonself entities encountered. However, before the lymphocytes are
ÒpromotedÓ to memory cells, they must be activated by a costimulatory signal indicating that the
substances to which they bind are in fact pathogens. This costimulatory signal is generated indepen-
dently and reduces the incidence of pathogen detectors that are overly sensitive (and hence the likeli-
hood of autoimmune reactions).The intrusion detection system implements a costimulatory mechanism as the requirement of ahuman confirmation of behavior flagged as potentially anomalousÑthat is, it presents matches sig-
naled by an activated detector to a human operator for confirmation. If the system receives human
confirmation within a fixed amount of time, the activated detector responsible for the warning is made49S. Forrest and S. Hofmeyr, ÒImmunology as Information Processing,Ó Design Principles for Immune Systems and Other Distrib-uted Autonomous Systems, L.A. Segal and I.R. Cohen, eds., Oxford University Press, New York, 2001.50In fact, the mature detector is eliminated if it does not exceed some parametrically set threshold (the activation threshold) forthe number of matches to abnormal triplets.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING263into a memory detector (with an indefinite lifetime and a subsequent activation threshold of 1). How-ever, if human confirmation is not forthcoming, the detector responsible is eliminated.An intrusion detection product based on this approach was introduced in early 2003.51 The real-world success of this product remains to be seen.8.2.5.5  Interesting Questions and Challenges
8.2.5.5.1  Definition of Self
Any paradigm for computer security that is based on the differentiation ofself from nonself must imply some operational definition of self that represents normal and benign
operation. It is clear that a good definition is matched to the signature of the threat being defended
against, and hence the designer must be able to answer the question, ÒHow would I know my system
were under attack?Ó Thus, self might be definable in terms of memory access patterns on a single host,
TCP/IP packets entering and leaving a single host, the collective behavior of a local network of comput-
ers, network traffic through a router, instruction sequences in an executing or stored program, se-
quences of system calls, user behavior patterns, or keyboard typing patterns.52At the same time, computer security must account for the fact that ÒselfÓ on a computer system,even one that has not been subject to threat or intrusion, changes over time. New users are added, new
software is added, and files are created, deleted, and modified in the course of normal activity, even
though all such activities may also occur in the course of an attack. That is, the notion of self must be
dynamically modifiable.These points suggest that better insights into characterizing threat signatures dynamically would behelpful if immunological approaches are to be used to enhance computer security.8.2.5.5.2  More Immunological Mechanisms
Another intellectual challenge is to incorporate more ofwhat is known about immunology into computer security. Thus, it is interesting to consider how a
number of immunological mechanisms known today might be useful in making the analogy closer,
using the functions and design principles of these specific mechanisms within the general context of an
immunologically based approach to computer security. One such mechanism is antigen processing and
the major histocompatibility complex (MHC). Some pathogens have the ability to ÒhideÓ within cells
generally recognized as self. Because lymphocytes can detect antigens only by binding to them, they are
unable to detect pathogens inside friendly cells. Molecules from the MHC have the ability to bring key
parts of such pathogens to the surface of those cells, thereby enabling the lymphocytes to detect them.
Moreover, each individual has a different set of MHC molecules; hence the kinds of hidden pathogens
that can be brought to a cellÕs surface are different for different individuals, providing an important
immunological diversity in the population as a whole.An analogous mechanism was implemented in the intrusion detection system described above. Justas certain pathogens are able to hide within cell interiors to avoid detection, the use of detectors that can
match a number of subsets of nonself patterns (so that fewer detectors are needed) implies that there
exist some nonself patterns for which no detectors can be generated. In other words, a detector capable
of matching such nonself patterns would also match some patterns found in self. Furthermore, as the
number of nonself patterns that can be recognized by a single detector increases, the number of prob-
lematic nonself patterns also increases. Because they result from the structure of the set of self patterns,
dynamic change in the detectors cannot find them.A solution that proved to be effective at reducing the overall number of holes (i.e., gaps in coverage)is multirepresentationÑdifferent representations are used for different detectors. One way of achieving51See http://www.sanasecurity.com.52S. Forrest, S.A. Hofmeyr, and A. Somayaji, ÒComputer Immunology,Ó Communications of the ACM 40(10):88-96, 1997.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.264CATALYZING INQUIRYthis is for each detector to have a randomly generated permutation rule, according to which all datapath triples are permuted before being matched against the detector. This effectively changes the struc-
ture of the self set for each detector, with the result that different detectors will be subject to different
holes. Consequently, where one detector fails to detect a nonself triple, another may succeed.
Multirepresentation was particularly effective at reducing the number of holes when the nonself pat-
terns were similar to self patterns. To deal with this problem, the bits in a given triplet of connection
triplets were randomly permuted before presentation to detectors, just as the specific MHC molecules
that are operating to bring pathogens to the surface are probabilistically determined (with respect to an
averaging over the population).8.2.5.6  Some Possible Difficulties with an Immunological Approach
Although these analogies have appeal, it remains to be seen how far they can be pushed. Given thatthe immune system is a very complex entity whose operation is not fully understood, a bottom-up
development of a computer security system based on the immune system is not possible today. The
human immune system has evolved to its present state due to many evolutionary accidents as well as the
constraints imposed by biology and chemistryÑmuch of which is likely to be artifactual and mostly
irrelevant to the underlying principles that the system embodies and also to the design of a computer
security system. Further, the immune system is oriented toward problems of survival. By contrast, com-
puter security is traditionally concerned with confidentiality, accountability, and trustworthinessÑand
the relevance of immunological processes to confidentiality and accountability is entirely unclear today.8.2.6  Amorphous Computing
An area of research known as amorphous computing seeks to understand how to obtain Òcoherentbehavior from the cooperation of large numbers of unreliable parts that are interconnected in unknown,
irregular, and time-varying ways.Ó53 This work, inspired by observations of cooperative and self-organizing biological phenomena, seeks to identify the engineering principles that can be used to
observe, control, organize, and exploit the behavior of cooperating multitudes for human purposes such
as the design of engineered artifacts.An individual entity in a collection of cooperating multitudes has the following characteristics:¥It is inexpensive, in the sense that it is easy to create large numbers of them. For all practicalpurposes, each entity is identical to every other one.¥It is locally guided or programmed. That is, the guidance or programming is carried by the entityÒon-boardÓ rather than being resident elsewhere in the overall system. As a consequence of fabrication,
the guidance or programming aboard any given entity is identical to that aboard every other entity.¥It communicates with nearby entities, but in a stochastic manner without the need for preciseinterconnections and testing. Note also that the ability to function in a stochastically connective environ-
ment implies that the overall macrosystem is robust in the face of damaged or nonoperational compo-
nents. Furthermore, by eliminating the need for precision interconnections, these entities can reduce the
enormous costs usually associated with interconnection in traditional forms of assembly, costs that are
generally higher than those associated with individual elements.¥It interacts with its environment locally, so that the entity is directly knowledgeable about someaspect of its immediate environment but not about anything more global. To the extent that an indi-
vidual entity gains global knowledge about the environment, it is as the result of a self-organizing
process that develops such information and transmits it to all entities in the system. Similarly, any on-
board effectors affect only the immediate environment.53See http://www.swiss.ai.mit.edu/projects/amorphous/.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING265These characteristics are easily obtained by biology and are increasingly true for certain artifactsthat result from todayÕs chip fabrication technologies. A metaphor with some resonance is that of
ÒpaintableÓ computersÑa paint that can be applied to a surface, in which are suspended millions of
computing and MEMS-like entities that communicate with each other and interact with the surface on
which they are painted. (MEMS is an acronym for microelectromechanical systems.)The vision presented by Abelson et al.54 is that smart materials may reduce the need for strengthand precision in mechanical and electrical apparatus, through the application of computation. For
example, coating a building or a bridge with Òsmart paintÕÕ may enable it to report on traffic loads and
wind loads, to monitor the integrity of the structure, to resist buckling, or to heal small cracks by shifting
material around. A different kind of smart paint may make it possible to build clean rooms with Òactive
skins,Ó lined with cilia that can push dirt and dust into a corner for removal. Still other paints may
enable walls that sense vibration or actively cancel noise. ÒSmart dust,Ó with light sensors in each
particle, could be spread over a wide area to recognize shadows or other traffic passing overhead.In short, the hope is to create systems with unprecedented responsiveness to their environment.Abelson et al. further argue that the study of amorphous computing has implications for software
design in a more general sense. Specifically, a software problem has long been recognizedÑthe depen-
dence of greater functionality of software on increasingly complex software packages and systems.
Today, software is mostly developed Òby hand,Ó and each line is individually coded. One obtains a high
degree of detailed control in this manner, but reliably abstracting the higher-level behavior of a software
system so developed is highly problematic. Principles of amorphous computing may enable a more top-
down specification of systems that more closely tracks how humans define the functionality they wish
to obtain from software.Amorphous computing may be applicable to fabrication as well. For example, consider amorphouscomputing entities that are capable of some mechanical interaction with the substrate on which they are
painted (e.g., they might expand or contract in certain directions). Nagpal has demonstrated the feasi-
bility of an amorphous computing substrate that is capable of pattern formation (Box 8.1); if the entities
making up this formation have the mechanical property described, it is conceivable that they might be
able to warp a sheet onto which they were painted into a three-dimensional structure.It is also conceivable that the vision described in amorphous computing and other approaches tothat area could be extended so that appropriately configured microentities could be programmed to
self-assemble into useful physical structures on the nanoscale. These structures might be useful to end
users in and of themselves, or might serve as nanofabrication machinery that could construct other
structures useful to the end user. In particular, the large macromolecules involved in the biochemistry
of lifeÑspecifically protein moleculesÑdemonstrate the ability to configure themselves into structures,
and some research seeks to co-opt biochemical machinery to assemble structures designed for entirely
human purposes (as described in Section 8.4.3).8.3  BIOLOGY AS IMPLEMENTER OF MECHANISMS FOR COMPUTING
8.3.1  Evolutionary Computation
558.3.1.1  What Is Evolutionary Computation?
Evolutionary computation is inspired by genetics and evolutionary events.56 Given a particularproblem for which a solution is desired, evolutionary computation requires three components:54H. Abelson, T.F. Knight, G.J. Sussman, et al., ÒAmorphous Computing,Ó available at http://www.swiss.ai.mit.edu/projects/amorphous/white-paper/amorph-new/amorph-new.html.55The discussion in Section 8.3.1 owes much to Melanie Mitchell, now at Portland State University in Oregon.56Evolutionary computation is a generic name for techniques that are based loosely on evolutionary principles. There are anumber of variants, including evolutionary programming, evolution strategies, genetic programming, and genetic algorithms,which have somewhat different emphases but share the generic approach.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.266CATALYZING INQUIRY¥A population of candidate solutions to the problem. For example, these candidate solutions maybe a sequence of amino acids that can fold into some protein, a computer program, some encoding of the
design for something, or some set of rules in a production system.¥A ÒfitnessÓ metric by which the ÒgoodnessÓ of a candidate solution can be evaluated. For ex-ample, if the program was intended to model the output of some designer circuit, the fitness metric
might be based on the performance of a candidate program acting on a test case. That is, given the test
case, the fitness metric would be the deviation of the output of the program from a known, appropriate
answer. Programs that minimized this deviation would be more fit.Box 8.1Pattern Formation Using Identical Autonomous AgentsIn a 2001 Ph.D. thesis, Nagpal describes a language for instructing a sheet of identically programmed, flexi-ble, and randomly but densely distributed autonomous agents (ÒcellsÕÕ) to assemble themselves into a prede-
termined global shape, using only local interactions. A wide variety of global shapes and patterns can besynthesized (patterns including flat layered shapes, all plane Euclidean constructions, and a variety of tessel-lation patterns) using only local interactions between identically programmed deformable cells. That is, the
global shape results from a coordinated set of local shape changes in individual cells. Despite being pro-grammed identically, each cell deforms in its own individualized manner, depending on the behavior andstate of its neighbors. (The governing structural metaphor is that of epithelial cells, which generate a wide
variety of structures: skin, capillaries, and many embryonic structures (gut, neural tube) through the coordinat-ed effect of many cells changing their individual shape.)The global shape is specified as a folding construction on a continuous sheet, using a small set of axioms, simpleinitial conditions (edges and corners of the sheet), and two types of folds. From an engineering standpoint, thesignificance of global shape description is that a process that is inherently local can be harnessed to produce a
shape of known configuration. This differs significantly from approaches based on cellular automata, in whichthe local-to-global relationship is not well understood and there is no framework for constructing local rules toobtain any desired pattern (and patterns ÒemergeÓ in a non-obvious way from the local interactions).In this formalism, the specific global shape desired uniquely determines the program executed by all cells. Thecellular program is based on several (biologically inspired) primitives for interacting with the cellÕs local
environment. A cell can change the local environment in ways that create the equivalent of chemical gradi-ents, query its local neighborhood and collect information about the state of local companions (e.g., collectneighboring values of a gradient), broadcast messages to all the cells in its local neighborhood, invert its
polarity, connect with neighbors in physical contact to establish communication (thus allowing multiplelayers of the sheet to act as a single fused layer), and fold itself along a particular orientation by calling thelocal fold within its program with two arguments: a pair of neighbors and a cell surface.Each cell has limited resources and reliability. All cells execute the same program and differ only in a smallamount of local dynamic state. The cell program does not rely on regular cell placement, global coordinates,
or synchronous operation. Robustness against a small amount of random cell death is achieved by dependingon large and dense cell populations, using average behavior rather than individual behavior, trading offprecision for reliability, and avoiding any centralized control. Further, global coordinates are not required,
because cells are able to ÒdiscoverÓ positional information. An average cell neighborhood of 15 is sufficientto reliably self-assemble complex shapes and geometric patterns on randomly distributed cells.SOURCE: R. Nagpal, ÒProgrammable Self-assembly: Constructing Global Shape Using Biologically-inspired Local Interactions andOrigami Mathematics,Ó Ph.D. thesis, MIT Department of Electrical Engineering and Computer Science, June 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING267¥A mechanism (or mechanisms) by which changes to the candidate solutions can be introducedÑportions of different candidate solutions are exchanged, for example, or modified in some small random way.57With these components in place, an evolutionary process takes place. The set of new solutions isevaluated for fitnessÑthose with lower fitness scores are thrown out and those with higher scores are
retained. This mutation process is iterated many times, and the result at the end is (supposed to be) a
solution that is much better than anything in the starting set.Initially demonstrated on the solving of what might be called ÒtoyÓ problems, evolutionary tech-niques have been used in a variety of business applications, including scheduling and production
optimization, image processing, engine design, and drug design. Evolutionary computation has also
achieved results that are in some sense competitive with human-developed solutions to quite substan-
tive problems. Competitiveness has a number of possible measures, among them results that are com-
parable to those produced by a succession of human researchers working on a well-defined problem
over a period of years, a result that is equivalent to a previously patented or patentable invention, a
result that is publishable in its own right (i.e., independent of its origins), or a result that wins or ranks
highly in a judged competition involving human contestants.58Evolutionary computation has demonstrated successes according to all of these measures. Forexample, there are at least 21 instances in which evolutionary techniques have led to artifacts related to
previously patented inventions.59 Eleven of these infringe on previously issued patents, and ten dupli-cate the functionality of previously patented inventions in a non-infringing way. Also, while some of
the relevant patents were issued many years ago (as early as 1917), others were issued as recently as
2001. Some of the inventions created by evolutionary processes include the ladder filter, the crossover
filter, a second-derivative controller, a NAND circuit, a PID (proportional, integrative, and derivative)
controller, a mixed analog-digital variable capacitor circuit, a voltage-current conversion circuit, and a
cubic function generator. They have also created a soccer-playing program that won its first two games
in the Robo Cup 1997 competition and another that ranked in the middle of the field of 34 human-
written programs in the Robo Cup 1998 competition, four different algorithms for the transmembrane
segment identification problem for proteins, and a variety of quantum computing algorithms, and have
rediscovered the Campbell ladder topology for low-pass and high-pass filters.Evolutionary computation also poses intellectual challenges, as described in the next severalsections.8.3.1.2  Suitability of Problems for Evolutionary Computation
60Whether or not an evolutionary approach will be successful in solving a given problem is not yetfully understood. Although many components of a full theory of evolutionary algorithms have been
worked out, there are critical gaps that remain open questions.It is known that the relationship between the representation of a problem, genetic operators, and theobjective function is the primary determinant of the performance of an evolutionary algorithm. For any
optimization problem, there is always a representation or a genetic operator that makes the optima easy
to find with an evolutionary algorithm.61  In addition, evolutionary algorithms are no better or worse57In biology, ÒcrossoverÓ refers to the process in which chromosomal material is exchanged between chromosomes during cellduplication. The exchanged chromosomal material is analogous to portions of the different candidate solutions. ÒMutationsÓ aregenetic changes induced as the result of random environmental events.58See http://www.genetic-programming.org.59See http://www.genetic-programming.com/humancompetitive.html. More information on these accomplishments can befound in J.R. Koza, M.A. Keane, M.J. Streeter, W. Mydlowec, J. Yu, and G. Lanza, Genetic Programming IV: Routine Human-Competitive Machine Intelligence, Series in Genetic Programming, Volume 5, Springer, New York, 2005.60Lee Altenberg of the University of Hawaii was a major contributor to Section 8.3.1.2.61G.E. Liepins and M.D. Vose, ÒRepresentational Issues in Genetic Optimization,Ó Journal of Experimental and Theoretical Artifi-cial Intelligence 2(2):101-115, 1990.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.268CATALYZING INQUIRYthan any other search algorithm over the space of all problems.62 Therefore, problem-specific knowl-edge must be incorporated either implicitly or explicitly in an evolutionary algorithm for it to perform
well. Finally, evolutionary algorithms are dynamical systems, and the systems properties necessary to
make them good search algorithms are well characterized.63The primary question that remains to tie together the above is the following: HowÑand whenÑcanknowledge about the problem be translated into representations and genetic operators that produce an
evolutionary algorithm with good performance?In the absence of this critical link in the theory of evolutionary algorithms, the approach taken bydesigners resorts to the empirical: try it out and see if it works. Evolutionary approaches provide the

greatest advantage over other methods in cases where it is not understood how to construct answers
from Òfirst principlesÓ (i.e., logico-deductive procedures), but where approximate solutions can be
refined by variation and testing. Such problems can be characterized as Òdifficult inverse problems,Ó
where the inverse refers to finding inputs that produce desired outputs of the system in question.Moreover, evolutionary techniques tend to work best on problems involving relatively large searchspaces and large numbers of variables that are not well understood. Evolutionary algorithms have been
able to construct and adapt complex neural networks that are intractable analytically or for which
derivative-based back-propagation is inapplicable. Genetic programming has produced complex cir-
cuits that infringe on patented inventions. By contrast, problems involving small search spaces can
usually be searched systematically, and search spaces being well understood generally means that
special-purpose heuristics are available. (For example, the Traveling Salesman Problem is reasonably

well understood, and there are very good special heuristics for solving that problem.)For problems in which evolutionary techniques are unable to find global optima, they may never-theless find very good approximations that are robust to wide-ranging initial conditions. Thus, the
solutions generated may be adequate to the task at hand. For this reason, evolutionary techniques may
also be better when data are very noisy or in the presence of a varying fitness function: the algorithm
may rapidly produce approximate solutions that track the changing environment, just as evolving
species can track environmental changes. (An example of a problem calling for a varying fitness func-
tion might be a robot that must learn, online, in a dynamic environment, where the task facing the robot
changes over time.)8.3.1.3  Correctness of a Solution
One of the most challenging aspects of evolutionary computation is evaluating the correctness of asolution derived through evolutionary means. Because evolutionary solutions are cumulative, in the
sense that they build on previous solutions, the design process does not have an opportunity to develop
solutions that are clean and elegantly designed from first principles. Human inspection of a solution so
derived is unlikely to yield much insight. Thus, essentially the only way known today to assess the
correctness of such a solution is to subject it to extensive testing. Rather than a human being under-
standing how the solution achieves its goals, the proposed solution convinces a human being that it will
do so.Note, however, that ascertaining the correctness of any large computational artifact (e.g., a complexsoftware system or a VLSI chip) depends to a large degree on testing. Of course, because the thought
and decision-making processes of human beings are not available to public inspection, it is only by
observing a human being in action that one develops confidence in the designerÕs ability to perform62D.H. Wolpert and W.G. Macready, ÒNo Free Lunch Theorems for Optimization,Ó IEEE Transactions on Evolutionary Computa-tion 1(1):67-82, 1997, available at http://citeseer.ist.psu.edu/wolpert96no.html.63L. Altenberg, ÒOpen Problems in the Spectral Analysis of Evolutionary Dynamics,Ó pp. 73-102 in Frontiers of EvolutionaryComputation, A. Menon, ed., Genetic Algorithms and Evolutionary Computation Series, Volume 11, Kluwer Academic Publish-ers, Boston, MA, 2004.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING269appropriately under certain circumstances. Thus, in the limit of increasing complexity, testing an evolu-tionary solution may resemble the Turing test. (In the Turing test, an outside observer is asked to
distinguish between a human beingÕs answers to a set of questions and a computerÕs answers. The
computer is said to have passed the Turing test if the outside observer is unable to distinguish between
the two.) 8.3.1.4  Solution Representation
In biological organisms, the genetic code of DNA is subject to changes (e.g., mutation), and theimpact of these changes becomes manifest as the new mutated code is involved in the reproductive
process. That is, the particular DNA sequence of an organism can be said to be biologyÕs representation
of a ÒsolutionÓ to the problem of adapting the organism to a particular set of evolutionary selective
pressures.From the standpoint of someone solving a problem with techniques from evolutionary computa-tion, the question arises as to the analogue of DNA. More formally, how is a solution to a computational
problem to be represented?In general, the solution to a computational problem is an algorithm. However, an algorithm can berepresented in many different ways. Just as data can be represented as lists of numbers or in graphical
form, computer programs (which embed algorithms) can be represented as Òsource codeÓ that is read-
able by human beings or as Òobject codeÓÑthe raw ones and zeros of binary computation.If candidate solutions are to be computer programs, one might imagine that their machine languagerepresentation is an obvious possible representation. However, changing a machine language program
one bit at a time, at random, is highly likely to prevent the (modified) program from running at all
(because previously valid op-codes will be turned into invalid ones), and a nonrunning program is
useless. The same comments apply to the source code of a program. By randomly changing characters
in the source code file, the most likely result is a program that will not compile and therefore cannot be
evaluated in any meaningful way. Thus, attempting to evolve a binary program or the source code of a
program would likely result in an extraordinarily slow rate of evolution.A more robust way to conduct this process is to impose the constraint that the program must beexecutable. Thus, one might insist that the source code of a program be syntactically correct but not placeany limits whatsoever on its semantics (on what it does). For example, statements in a program can be
represented as combinations of functions with various numbers of arguments, and the only require-
ment for syntactic correctness is that a function have the right number of arguments.64 Changes to theprogram can be effected by changing the functions and the specific arguments to the functions. The
result, by definition, is a program that is still syntactically correct, still runs, but does not necessarily do
what is desirable. A typical initial program is then created by randomly generating a parse tree. A
population of such parse trees is then subject to crossovers that exchange different parts of the various
parse trees, or mutations that replace one argument or function with a new argument or function.8.3.1.5  Selection of Primitives
Closely related to the issue of representations is the question of the appropriate semantic primitives(i.e., the smallest meaningful unit that can be changed). For example, in the representation of programs
as parse trees, the relevant primitives are functions with arguments, and the efficacy of a genetic
algorithm is strongly dependent on the particular set of functions that the evolutionary process can
manipulate.64This approach is based on parse trees, a way of representing statements in computer programs. See J.R. Koza, GeneticProgramming: On the Programming of Computers by the Means of Natural Selection, MIT Press, Cambridge, MA, 1992.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.270CATALYZING INQUIRYTo illustrate, any computable function can in principle be built from the appropriate combination ofBoolean operators (AND, OR, and NOT). But these functions operate at too low a level to build the kind
of hierarchical structures needed to do anything complicated. It is for this reason that high-level pro-
gramming languages have emerged that are not based on these operators directly. Such languages
allow the creation of many other kinds of structure. For example, a program intended to undertake
financial analysis might benefit from an operator or function that would allow finding the average stock
price for the previous month. If its task were to evolve a program for financial analysis, such functions
might be included in the set of primitives from which an evolutionary process might draw.One important aspect of the evolutionary approach is the ability to evolve new operators or newfunctions that can be used subsequently. In some instances, new structures can emerge spontaneously
that are more or less stable; more frequently, it is possible to insert rules that will prevent such struc-
tures from changing. Alternatively, functions can be defined automaticallyÑthe environment provides
slots for function and the ability to call on those function (even if they are no-ops), and the subsequent
evolutionary process fills in those spaces with functions.658.3.1.6  More Evolutionary Mechanisms
The model described above is a very crude model of evolution, incorporating only a few bareessential features. However, biologists have characterized other features of evolution. Two of the most
important with possible application to computing are coevolution and development; these are dis-
cussed below. Other aspects of evolution, such as diploid behavior and sexual selection, do not at this
stage provide obvious new approaches to computing.8.3.1.6.1  Coevolution
Coevolution refers to the biological phenomenon in which two or more speciesinteract as they evolve. For example, a host may be susceptible to infection by a parasite. The host
evolves some defenses against the parasite, which in turn stimulates the parasite to evolve ways in
which to penetrate or circumvent those defenses. In coevolution, other speciesÑwhich are also evolv-
ingÑconstitute part of the environment in which a given species is embedded.One application of coevolution to evolutionary programming is to allow the evolution of testingdata simultaneously with the solution. Doing so enables the program to account for a wider range of
input. In this case, one fitness function is required for the program to evaluate how well it performs
against a given set of test data, while a different fitness function is needed for the test data to evaluate
how well it breaks the program.668.3.1.6.2  Development
Development refers to the phenomenon in which biological complexity is shapedby growth within the organism (what might be called maturation) and the action of environmental
forces on the organism. It is very difficult to create significant complexity using genetic mechanisms
alone. Thus, one intellectual thrust in evolutionary computation focuses on the creation of developmen-
tal mechanisms that can be evolved to better create their own complexity. For example, evolutionary
techniques can be used to evolve neural networks (see Section 8.3.3.2). In designing neural networks,
the problems involve various issues related to the topology and configuration of the network. However,
a grammar can be used to generate structures of interest. (A grammar is a formal system of rules that
can be used to generate far larger structures.) Grammars can evolve as well, with the fitness function
being the complexity of the structures it can generate.65J.R. Koza, Genetic Programming, 11: Automatic Discovery of Reusable Programs, MIT Press, Cambridge, MA, 1994.66D. Hillis, ÒCo-evolving Parasites Improve Simulated Evolution as an Optimization Procedure,Ó Physica D 42(1-3):228-234,1990.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING271In this case, the goal is to evolve a neural network that has the potential to learn things, rather thanevolving the things themselves that are the object of learning. In the case of a robotic brain, it is too
difficult to anticipate all of the possibilities that might face the robot, and thus it is impossible to develop
a fitness function that fully reflects this diversity. By giving the brain the ability to learn and reason, one
can circumvent this difficulty, and as long as one can develop a fitness function for how well the brain
has learned over some period, evolutionary techniques can be used to evolve a robotic brain. (Note that
the indirect nature of this approach makes it doubly difficult to understand what is going on.)An example of such work is that of Sims (Box 8.2).8.3.1.7  Behavior of Evolutionary Processes
Today, those working in evolutionary computation are not able to predict, in general, how long itwill take to evolve some desired solution or determine a priori how large an initial population size
should be, how rapidly mutations should occur, or how often genetic crossovers should take place.
Obviously, all of these parameters have some potential impact on the rate of evolution and how effec-
tive a solution might be. Yet how they should be set and their possible relationship to the nature of a
given problem are, in general, not known, although some intuitions exist in this area.Box 8.2Genetic Programming in AnimationIn the world of computer graphics and animation, it can be difficult to build virtual creatures that behave in arealistic manner and simultaneously remain under the userÕs direct control. For example, directly controllingthe positions and angles of moving objects such as limbs can result in detailed behavioral control, but likelyat the expense of achieving physically plausible motions. On the other hand, providing a realistic, physics-
based environment in which the relevant dynamics are simulated can result in a higher degree of realism, butwill likely make it difficult to achieve the desired behavior, especially as the entities involved become morecomplex.One way to manage the complexity of control is to optimize the behavior of the creature against some fitnessfunction. Using evolutionary techniques, it is possible to fabricate creatures that behave realistically without
understanding the procedures or parameters used to generate them. Different fitness functions can representdifferent modes of movement (e.g., swimming, walking, jumping, following a source). This approach forcesthe user to sacrifice some detailed control, but there is also considerable gain in automating the creation of
complexityÑand the user still influences the outcome by specifying the fitness function.For purposes of animation, a creature is determined by its physical morphology (e.g., size, shape, number oflegs) and the neural system for controlling the relevant muscle forces (the neural system involves sensors thattell the creature about its immediate environment, effectors that cause motion [analogous to muscles], andneurons that retain some memory of its previous states). Both morphology and neural system can be evolved,
resulting in a succession of increasingly ÒfitÓ creatures that move realistically in a given mode.In SimsÕ work, a developmental process was used to generate the creatures and their control systems. The useof such a process allowed similar components, including their local neural circuitry, to be defined once andthen replicated, instead of requiring each to be separately specified. Thus, a coded representationÑa geno-typeÑof a creature was established that uniquely defined the phenotype of that creatureÑits morphology and
neural system. By evolving the genotype, different phenotypes emerged.SOURCE: Adapted from K. Sims, ÒEvolving Virtual Creatures,Ó Computer Graphics, Annual Conference Series (SIGGRAPH Ô94 Proceed-ings), July 1994, pp. 15-22.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.272CATALYZING INQUIRYFor example, variation in a species results from mutations (involving random changes to a genome)and crossovers (involving exchanges of different parts of existing genomes). One hypothesis is that
crossovers result in changes that are much more rapid than those driven by mutation. The argument in
favor of this is that genomic exchange is in some sense enabling an organism to build on stable substruc-
tures. On the other hand, it may be that evolutionary solutions cannot make good use of existing
substructures or that crossover is incapable of integrating existing substructures.If it is true that evolutionary change is more rapid with crossovers than with mutations, thissuggests that programs designed to evolve genetic programs may wish to emphasize crossover in their
processes for introducing variation.8.3.2  Robotics 3: Energy and Compliance Management
Biological systems provide an existence proof that self-effected motion is possible. Furthermore,compared to the locomotion made possible by human engineering, biological mechanisms capable of
locomotion appear to be energetically efficient, possible in a wide variety of physical environments, and
often small in size.Given these characteristics, it is not unreasonable to ask what lessons biology might hold for thedesign of engineered systems for locomotion. For example, one reason that biological systems are
energetically efficient is that they are not rigid, but rather compliant, and often have mechanisms for
energy recovery. That is, these mechanisms store kinetic energy that might otherwise be dissipated,
much as a braking electric car can store in batteries the kinetic energy associated with slowing down. A
kangaroo employs such a mechanism in its tail, which acts as a spring that compresses as the kangaroo
lands from one jump and then assists the kangaroo in pushing off for the next jump. Full has argued that
leg locomotion can be described as a point mass attached to a spring and finds that the ratio of relative
leg stiffness67 to body mass is more or less constant across legged animals spanning a wide range ofsize.68 In this context, leg musculature functions not just as a source of power but also as an actuator, aspringy ÒstrutÓ that participates in energy absorption, storage, and return.A second example is that many-legged animals demonstrate an inherent dynamic stability. Con-trary to expectations that locomotion would require complex neural control feedback mechanisms, the
structure of the leg itself and its inherent multifunctionality provide a key aspect of the control of the
system and the combination of stability and forward momentum needed for locomotion. Indeed, analy-
sis of many-legged animals reveals that this inherent stability arises from the production of large lateral
and opposing leg forces when the legs are moving. Modeling these forces as a spring between opposing
legs reveals that the system is highly stable against perturbationsÑand the leg assembly is capable of
stabilizing itself without any equivalent of neural reflexes at all. Thus, the animal does not need to
devote expensive neurological processing to the supervision of locomotive tasks.Raibert was one of the pioneers of robotics engineering based on physics-inspired control lawsÑone for height, one for pitch, and one for speed. A fundamental insight was that running animals make
use of dynamic stabilityÑa running animal moving forward is out of balance, but legs move forward in
rhythm to break its fall. To model this phenomenon, a one-legged ÒanimalÓ (the ÒPlanar One-legged
HopperÓ) was created. It consisted of a mechanized pogo stick with a three-part control systemÑone
controlling forward running speed, one controlling body attitude, and one controlling hopping height.
Stepping motion was not programmed explicitly, but rather emerged under the constraints of balance67Relative leg stiffness is the weight-normalized, size-normalized spring constant of the leg.68R. Blickhan and R.J. Full, ÒSimilarity in Multilegged Locomotion: Bouncing Like a Monopode,Ó Journal of Comparative Physi-ology 173:509-517, 1993; T.M. Kubow and R.J. Full, ÒThe Role of the Mechanical System in Control: A Hypothesis of Self-stabiliza-tion in Hexapedal Runners,Ó Philosophical Transactions of the Royal Society of London B 354:849-862, 1999; A. Altendorfer et al.,ÒRHex: A Biologically Inspired Hexapod Runner,Ó Journal of Autonomous Robots 11:207-213, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING273and controlled travel.69 With this basic unit, a two-legged running animal (the Planar Biped) could bemodeled as a body with two pogo sticks working 180° out of phase.70 A four-legged animal couldconsist of two two-legged pairs working in opposition (left front and right rear, for example).71Since RaibertÕs pioneering work, these insights have been applicable to the design of other artificiallegged locomotion devices. For example, an autonomous hexapod named ÒRHexÓ has a motor associ-
ated with each leg, each of which is springy and is able to turn on its central axis. This design enables
RHex to have self-correcting reflexes that enable it to respond to obstacles without computational
control. Another family of six-legged robots, called the SPRAWL family, is cockroaches. Each leg,
driven by a piston, acts as a spring that enables SPRAWL robots to bounce over objects in their path
without feedback from the environment. Analysis of the force pattern exerted by the legs closely matches
that exerted by a running cockroach.Other robots are intended to manipulate objects into precise orientations. The traditional way tobuild such robots is to build them rigidly, with limb motion effected through motors and gear assem-
blies to increase torque. However, gear assemblies are inherently imprecise, because their very motion
requires some degree of play where the gears meet (i.e., some nonzero compliance). In practice, the
effect of compliance in the gears introduces a noise function that greatly complicates the prediction of
how a limb will move given a certain motor input, and puts limits on the precision with which the final
orientation can be known.One solution to this problem is to use Òdirect-driveÓ motors placed at every joint, thus eliminatingthe gears entirely.72 Another solution is based on the deliberate introduction of compliance into a gearassembly. This solution is based on the observation that humans can effect precise positioning without
precision in their joints. In particular, natural joints are often based on ball-and-socket mechanisms even
when they are intended to exhibit 1 degree of freedom. Soft tissue around and in the ball joint intro-
duces compressive compliance in the joint, allowing it to absorb impact and automatically maintain a
degree of tightness in the joint.In the robot context, Pratt et al. inserted a spring mechanism into a limb joint so that the responselags the input.73 This spring adds a large but known compliance in series into the joint (so-called serieselasticity) that is much larger than the unknown compliance of the gears; thus, the gear compliance can
safely be ignored in the prediction of final position. Entirely apart from the increased ease of prediction,
the introduction of series elasticity enables a local response to any sudden changes in loadingÑduring
which time the motors involved can build up torque to handle that load. Other benefits include shock
tolerance, lower reflected inertia, more accurate and stable force control, less damage during inadvert-
ent contact, and energy storage.8.3.3  Neuroscience and Computing
Natural brains demonstrate an alternative to the traditional von Neumann computing architecture(i.e., a fully serial information processor); thus, it is natural to consider possible lessons of neuroscience
for computer design. These lessons occur at varying levels of detail.69See http://www.ai.mit.edu/projects/leglab/robots/2D_hopper/2D_hopper.html; see also M.H. Raibert and H.B. Brown,Jr., ÒExperiments in Balance with a 2D One-legged Hopping Machine,Ó ASME Journal of Dynamic Systems, Measurement, andControl 106:75-81, 1984.70See http://www.ai.mit.edu/projects/leglab/robots/2D_biped/2D_biped.html; see also J. Hodgins and M.H. Raibert, ÒPla-nar Biped Goes Head Over Heels,Ó Proceedings ASME Winter Annual Meeting, Boston, December 1987.71See http://www.ai.mit.edu/projects/leglab/robots/quadruped/quadruped.html; see also M.H. Raibert, ÒFour-legged Run-ning with One-legged Algorithms,Ó pp. 311-315 in Second International Symposium on Robotics Research, H. Hanafusa and H. Inoue,eds., MIT Press, Cambridge, MA, 1985.72H. Asada and T. Kanade, ÒDesign of a Direct-Drive Mechanical Arm,Ó ASME Journal of Vibration, Stress, and Reliability inDesign 105(3):312-316, 1983.73G.A. Pratt, M.M. Williamson, P. Dillworth, J. Pratt, K. Ulland, and A. Wright, ÒStiffness IsnÕt Everything,Ó preprints of theFourth International Symposium on Experimental Robotics, ISER Õ95, Stanford, CA, June 30-July 2, 1995.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.274CATALYZING INQUIRY8.3.3.1  Neuroscience and Architecture in Broad Strokes
The most general lesson is that much of human cognition depends on the ability to ignore most ofthe information made available by the senses.74 That is, a very high fraction of the raw information thatis accessible through sight, sound, and so on does not participate directly in the humanÕs cognitive
processes. Human and mammalian cognition is based on an architecture that involves a flexible, but
low-capacity, working memory and attentional selection mechanisms that place events and objects into
working memory where they become available for cognitive processing.75This approach of selective attention stands in sharp contrast to traditional algorithms that aredesigned with the goal of seeking optimal solutions and based on the use of as much information about
the problem domain as possible. The architecture of biological computation has generally evolved with
a different purposeÑthe adequate management of a complex, changing, and potentially dangerous
environment in real time (where ÒadequateÓ means Òprovides for survivalÓ).This architecture is based on a two-track processing arrangementÑa very flexible, albeit slowsystem that implements consciousness, awareness, and cognition but attends to only few things, and a
large number of online, fast-acting, sensory-motor systems that bypass attention and awareness (e.g.,
eye movements, head and hand movements, posture adjustments, and other reflex and reflex-like
responses).Koch et al. have investigated the utility of such a strategy in multiple contexts: (1) a saliency-basedvisual attention mechanism that selects highly ÒsalientÓ location in natural images for further process-
ing;76 (2) a competitive, two-person video game in which an algorithm that focuses on a restrictedportion of the playing field outperforms an ÒoptimalÓ player when a temporal limitation is imposed on
the duration of each move;77 and (3) an algorithm that rapidly solves the NP-complete bin-packingproblem under most conditions.788.3.3.2  Neural Networks
Biology affords an alternative computing model that (1) appears well suited for many ill-posedproblems constrained by uncertainty, which is the problem set for which digital machines to date have
been reasonably ineffective; and (2) provides an existence proof that slow and noisy circuits can under-
take very rapid computations of a certain class. Furthermore, it provides huge numbers of working
examples. Although the mechanisms underlying nerve tissue computation are not well understood
despite many decades of study, the fact remains that biology has found incredibly good solutions to
many engineering problems, and these approaches may well serve to inform practical solutions for
engineering problems posed by human beings. Indeed, although biological tissue is not naturally suited
for information processing as understood in traditional terms, the fact that biological tissue can do
information processing suggests that the underlying architectural principles must be powerful indeed.Neural networks are among the most successful of biology-inspired computational systems and aremodeled on the massively parallel architecture of the brainÑand on the brainÕs inherent ability to learn74C. Koch, ÒWhat Can Neurobiology Teach Computer Engineers?Ó January 31, 2001, unpublished paper, available at http://www7.nationalacademies.org/compbio_wrkshps/Christof_Koch_Position_Paper.doc.75F. Crick and C. Koch, ÒConsciousness and Neuroscience,Ó Cerebral Cortex 8(2):97-107, 1998.76F. Crick and C. Koch, ÒConsciousness and Neuroscience,Ó Cerebral Cortex 8(2):97-107, 1998; L. Itti and C. Koch, ÒA Saliency-based Search Mechanism for Overt and Covert Shifts of Visual Attention,Ó Vision Research 40(10-12):1489-1506, 2000; L. Itti and C.Koch, ÒTarget Detection Using Saliency-based Attention,Ó Search and Target Acquisition, RTO Meeting Proceedings 45, NATO,RTO-MP-45, 2000; L. Itti, C. Koch, and E. Niebur, ÒA Model of Saliency-based Visual Attention for Rapid Scene Analysis,Ó IEEETransactions on Pattern Analysis and Machine Intelligence (PAMI) 20:1254-1259, 1998.77J.G. Billock, ÒAttentional Control of Complex Systems,Ó Ph.D. Thesis, 2001, available at http://sunoptics.caltech.edu/~billgr/thesis/thesiscolor.pdf.78J.G. Billock, D. Psaltis, and C. Koch, ÒThe Match Fit Algorithm: A Testbed for the Computational Motivation of Attention,ÓInternational Conference on Computational Science 2: 208-216, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING275from experience.79 A neural network is a network of nodes and links.80 The nodes, or units, are verysimple processors that correspond to neuronsÑthe brainÕs electrically active cellsÑand are usually
organized in layers, while the links, or connections, are node-to-node data channels that correspond to
synapsesÑthe junctions that convey nerve impulses from one neuron to the next. Each node has an
activation level that corresponds to a neuronÕs rate of firing off nerve impulses, while each link has a
numeric weight that corresponds to the strength or efficiency of a synapse.Digital Òactivation energyÓ patterns are presented to the network via the Òinput layer.Ó81 From theinput layer, the activation surges through the various intermediate layers automatically, with the flow
being shaped and channeled by the connection strengths in much the same way that the flow of nerve
impulses in the brain is shaped by synapses. Once everything has settled down, the answer can be read
out from the pattern of activation on a set of designated output nodes in the final layer.This computation-by-network architecture is where parallelism is relevant:82 all of the nodes areactive at once, and the activation can travel on any number of paths simultaneously. It is also the basis
of the systemÕs ability to learn: since the flow of activation (and, thus, the computation) is shaped by the
connection weights, it can be reshaped by changing the weights according to some form of learning rule.How the connection weights are modified in response to the input patterns is the content of the learning
rule. This seems similar in some ways to what happens in the cerebral cortex, where knowledge and
experience are encoded as subtle changes in the synaptic strengths. Likewise in a neural network: with
very few exceptions, it will always contain some sort of built-in mechanism that can adjust the weights
to improve its performance.These brain-like characteristics give neural networks some decided advantages over traditionalalgorithms in certain contexts and problem types. Because they can learn, for example, the networks canbe trained to recognize patterns and compute functions for which no rigorous algorithms are known,
simply by being shown examples. (ÒThis is a letter B: B. So is this: B.Ó) Often, in fact, they can generalizefrom the training examples well enough to recognize patterns theyÕve never seen before. And their
parallel architecture helps them keep on doing so even in the face of noisy or incomplete data or, for that
matter, faulty components. The multiple data streams can do a lot to compensate for whatever is
missing.Training a neural network generally involves the use of a large number of individual runs todetermine the best solution (i.e., a specific set of connection weights that enables the network to do its
job).83 Most learning rules have a parameter that controls the rate of convergence between the currentsolution and the global minimum and another that controls the degree to which the network will ignore
local minima. Once the network is trained to demonstrate satisfactory performance, it can be presented
with other data.84 With new data, the network no longer invokes the learning rule, and the connectionweights remain constant.79Note that neural networks are only one approach to the general problem of machine learning. A second general approachinvolves what is called statistical learning techniques, so called because they are techniques for the estimation of unknownprobabilistic distributions based on data. These techniques have not, as a rule, been derived from the consideration of biologicalsystems.80Useful online tutorials can be found at http://neuralnetworks.ai-depot.com/3-Minutes/ and http://www.colinfahey.com/2003apr20_neuron/2003apr20_neuron.htm.81Some of this discussion is adapted from http://www.cs.wisc.edu/~bolo/shipyard/neural/neural.html.82Note, however, that this does not represent parallelism on the scale of the brain, where the neurons are numbered in thehundreds of billions, if not trillions. The number of units in a neural network is more likely to be measured in the dozens. In
practice, moreover, these networks are usually simulated on ordinary, serial computersÑalthough for specific applications theycan also be implemented as specialized microchips. (See the online tutorial at http://www.particle.kth.se/~lindsey/HardwareNNWCourse/home.html.) Still, the parallelism is there in principle.83Some of this is adapted from http://www.cs.wisc.edu/~bolo/shipyard/neural/neural.html.84Note that it is possible to ÒovertrainÓ a neural network, which means that the network cannot respond properly to anythingbut the training data. (This might correspond to rote memorization.) Obviously, such a network is not particularly useful.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.276CATALYZING INQUIRYNeural networks are most useful for problems that are not amenable to computational approachesand are constrained by strict assumptions of normality, linearity, variable independence, and so on.85That is, they work well in classifying objects, capturing associations, and discovering regularities within
a set of patterns where the volume, number of variables, or diversity of the data is very great; when the
relationships between variables are vaguely understood or the relationships are difficult to describe
adequately with conventional approaches; or when the problems in question are ill-posed and involve
high degrees of uncertainty.86 In addition, they are well suited for problems that are subject to distor-tions in the input data.Neural networks have been applied to a large number of real-world problems of high complexity,including the following.87¥Optical character recognition. Commercial OCR (optical character recognition) software packageshave incorporated neural network technology since the mid-1980s, when it significantly increased their
ability to recognize unfamiliar fonts and noisy, degraded documents such as faxes.88 Today, OCRsystems typically use a mix of neural network and rule-based approaches.¥Finance and marketing. Neural networksÕ ability to detect unanticipated patterns has made them afavored tool for analyzing market trends, predicting risky loans, detecting credit card fraud, managing
risk, and many other such tasks in the financial sector.89¥Security and law enforcement. Neural networksÕ pattern-detection ability has likewise made thema useful tool for fingerprint matching, face identification, and surveillance applications.90¥Robot navigation. Neural networksÕ ability to extract relevant features from noisy sensor data canhelp autonomous robots do a better job of avoiding obstacles.91¥Detection of medical phenomena. A variety of health-related indices (e.g., a combination of heartrate, levels of various substances in the blood, respiration rate) can be monitored. The onset of a
particular medical condition could be associated with a very complex (e.g., nonlinear and interactive)
combination of changes on a subset of the variables being monitored. Neural networks have been used
to recognize this predictive pattern so that the appropriate treatment can be prescribed.¥Stock market prediction. Fluctuation of stock prices and stock indices is another example of acomplex, multidimensional, but in some circumstances at least partially deterministic phenomenon.
Neural networks are being used by many technical analysts to make predictions about stock prices
based on a large number of factors such as past performance of other stocks and various economic
indicators.¥Credit assignment. A variety of pieces of information are usually known about an applicant for aloan. For instance, the applicantÕs age, education, occupation, and many other facts may be available.
After training a neural network on historical data, neural network analysis can identify the most rel-
evant characteristics and use them to classify applicants as good or bad credit risks.85This material adapted from http://cfei.geomatics.ucalgary.ca/matlab/ann.html.86See http://www.cs.wisc.edu/~bolo/shipyard/neural/neural.html.87See http://www.emsl.pnl.gov:2080/proj/neuron/neural/what.html; see also http://neuralnetworks.ai-depot.com/Applications.html. Examples in the list below for the topics Òdetection of medical phenomenaÓ through Òengine manage-mentÓ are taken from http://www.statsoftinc.com/textbook/stneunet.html#apps.88See http://www.scansoft.com/omnipage/ocr/. At the time, the state of the art in commercial OCR software was the rule-based approach, in which a system broke each character image into simple features and then identified the letters by reasoningabout curves, lines, and such. This approach worked wellÑbut only if the fonts were known and the text was very clean.89See http://neuralnetworks.ai-depot.com/Applications.html; see also http://www.nd.com/ and http://www.walkrich.com/value_investing/howdo.htm.90See http://www.neurodynamics.com/.91See http://ai-depot.com/BotNavigation/Obstacle-Introduction.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING277¥Monitoring the condition of machinery. Neural networks can be instrumental in cutting costs bybringing additional expertise to scheduling the preventive maintenance of machines. A neural network
can be trained to distinguish between the sounds a machine makes when it is running normally (Òfalse
alarmsÓ) versus those it makes when it is on the verge of a problem. After this training period, the
expertise of the network can be used to warn a technician of an upcoming breakdown, before it occurs
and causes costly unforeseen Òdowntime.Ó¥Engine management. Neural networks have been used to analyze the input of sensors from anengine. The neural network controls the various parameters within which the engine functions, in order
to achieve a particular goal, such as minimizing fuel consumption.8.3.3.3  Neurally Inspired Sensors
One of the first attempts to draw on the principles underlying biological sensors occurred in themid-1980s, when researchers such as Carver Mead and his coworkers at Caltech made their first at-
tempts to create artificial retinas using VLSI technology,92 with hoped-for applications that ranged fromartificial eyes for the blind to better sensors for robots. A second, more recent example of a neurally
inspired sensor is the computational sensor of Brajovic and Kanade.93 Many approaches toward im-proving machine vision have been based on better cameras with higher resolution and sensitivity, new
sensors such as uncooled infrared cameras, and new recognition algorithms. But standard vision sys-
tems typically have high latency (a long time between registration of the image on the vision systemÕs
sensors and image recognition), induced by the requirements of transferring large amounts of data from
the sensor to the processor and processing those large amounts of data quickly. In addition, latency
increases more or less linearly with image size. Standard vision systems can also be very sensitive to
small details in the appearance of an object in sensor images. A number of processor-based algorithms
have been developed that adjust for such variations, but they are often complex and ad hoc, and hence
unreliable.The computational sensor approach borrows biological architectural principles to use low-latencyprocessing and top-down sensory adaptation as techniques for speeding up vision processes. Computa-
tional sensors are (usually) VLSI circuits that include on-chip processing elements tightly coupled with
on-chip sensors, exploit unique optical design or geometrical arrangement of elements, and use the
physics of the underlying material for computation. The integration of sensor and processor elements
on a VLSI chip enables latency to be reduced by a considerable factor and provides opportunities for
fast processor-sensor feedback in service of top-down adaptationÑand computational sensors have
produced an order-of-magnitude improvement in sensing and information processing itself, such as
range sensing, sorting, high-dynamic range imaging, and display.8.3.4  Ant Algorithms
Ant colonies depend on workers that can collectively build nests, find food, and carry out a multi-tude of other complex tasks while having little or no intelligence of their own. Further, they must do so
without the benefit of a leader to organize their efforts. They also continue to do so even in the face of
outside disruptions, or the failure and death of individual members, thereby exhibiting a high degree of
flexibility and robustness.92M.A. Sivilotti, M.A. Mahowald, and C. Mead, ÒReal-time Visual Computations Using Analog CMOS Processing Arrays,Ó pp.295-312 in Advanced Research in VLSI: Proceedings of the 1987 Stanford Conference, P. Losleben, ed., MIT Press, Cambridge, MA,1987.93V. Brajovic, ÒComputational Sensor for Global Operations in Vision,Ó Ph.D. Thesis, Carnegie Mellon University, Pittsburgh,PA, 1996.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.278CATALYZING INQUIRY8.3.4.1  Ant Colony Optimization
Entomologists have devoted a great deal of research to figuring out how the social insects achievethese feats.94 Their answers, in turn, have led computer scientists to devise a variety of Òant algo-rithms,Ó all of which attempt to capture some of those same qualities of bottom-up self-organization,
flexibility, and robustness.95  Ant algorithms are an example of agent-based modelsÑa broad class ofsimulations that began to emerge in the early 1990s as researchers tried to model complex adaptive
systems on a computer. The idea was to represent different agents with variables that werenÕt just
numbers, as they would be in conventional econometric models, but complex data structures that could
respond and adapt to one anotherÑrather like agents in the real world. (In practice, each agent could be
modeled as an expert system, a neural network, or any number of other ways.)The first ant-based optimizationÑthe Ant Colony Optimization algorithmÑwas created in theearly 1990s.96  The algorithm is based on observations of ant foraging, something that ants do with highefficiency. Imagine that worker ants wandering far from the nest come across a rich food source. Each
ant carrying food back to the nest marks her trail by laying pheromone on the ground. When another
randomly moving ant encounters this previously marked trail, it will follow it with high probability and
reinforce the trail with its own pheromone. This behavior is thus characterized by a positive feedback
loop in which the probability with which an ant chooses a given trail increases with the number of ants
that previously chose the same trail.Because the first ant to reach the nest will be the one whose path just happens to be the shortest,there will be a period of time during which the shortest path is the only path to the nest. This fact
provides a ÒseedÓ around which further pheromone depositions can occur and collectively converge on
a path that is one of the shortest possible.The paradigmatic application of this algorithm is the Traveling Salesman Problem. A salesman isassigned to visit a specified list of cities, going through each of them once and only once before return-
ing to his starting point. In what sequence should he visit them so as to minimize his total distance?What makes the Traveling Salesman Problem difficult is that there seems to be no guaranteed wayof finding the absolute shortest path other than to check every possible sequence, and the number of
such sequences grows explosively as the number of cities increases, quickly outstripping the computa-
tional ability of any computer imaginable.97 As a result, practical programmers have had to give up on94See, for example, E.O. Wilson and B. Hılldobler, The Ants, Belknap Press of Harvard University Press, Cambridge, MA, 1990.95Overviews can be found in E. Bonabeau, M. Dorigo, and G. Theraulaz, Swarm Intelligence: From Natural to Artificial Systems,Oxford University Press, New York, 1999; E. Bonabeau, ÒSwarm Intelligence,Ó presented at the OÕReilly Emerging Technology
Conference, available at http://conferences.oreillynet.com/presentations/et2003/Bonabeau_eric.ppt; and E. Bonabeau and G.Theraulez, ÒSwarm Smarts,Ó Scientific American 282(3):72-79, 2000.96M. Dorigo, ÒOptimization, Learning, and Natural Algorithms,Ó Ph.D. Dissertation, Politecnico di Milano, Italy, 1992; M.Dorigo, V. Maniezzo, and A. Colorni, ÒThe Ant System: An Autocatalytic Optimizing Process,Ó Technical Report No. 91-016Revised, Politecnico di Milano, Italy, 1991; M. Dorigo, V. Maniezzo, and A. Colorni, ÒPositive Feedback as a Search Strategy,ÓTechnical Report No. 91-016, Politecnico di Milano, Italy, 1991 (later published as M. Dorigo et al., ÒThe Ant System: Optimiza-tion by a Colony of Cooperating Agents,Ó IEEE Transactions on Systems, Man, and Cybernetics-Part B 26(1):29-41, 1996, available atFuture GenerationComputer Systems (Special Issues on Ant Algorithms) 16(8), 2000. Dorigo maintains a Web page on ant colony optimization,including an extensive bibliography (with many papers downloadable), plus links to tutorials and software, available at http://iridia.ulb.ac.be/~mdorigo/ACO/about.html.97If there are N cities in the list, then the number of possible routes is on the order of N!Ñthat is, N × (N Ð 1) × (N Ð 2). . . . × 2 ×1. (There are N choices of a place to start, N Ð 1 choices of a city to visit next, N Ð 2 choices to visit after that, and so on.) This isnothing much to worry about for small numbers: 10 cities yield only 10! = 3.628 million paths, which a personal computer couldexamine fairly quickly, but 20 cities would yield about 2.4 × 1018 pathsÑa (very fast) computer that examined one path pernanosecond would take more than 77 years to get through all of them; and 30 cities (30! = 2.65 × 1032) would keep that samecomputer busy for 8 quadrillion years. In computer science, this is a classic example of an NP-complete problem. An NP-complete problem is both NP (i.e., verifiable in nondeterministic polynomial time) and NP-hard (any other NP problem can be
translated into this problem). In an NP-complete problem, the number of computations required to solve it grows faster than anypower of its size. (ÒVerifiable in nondeterministic polynomial timeÓ means that a proposed solution to this problem can beverified in polynomial time on a computer that can execute different instructions depending on its input. Polynomial time means
a time that is proportional to some power of the problemÕs size.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING279finding the best solution to the Traveling Salesman Problem and its relatives, and instead look foralgorithms that find an acceptable solution in an acceptable amount of time. Many such algorithms havebeen developed over the years, and the Ant Colony Optimization algorithm has proved to rank among
the bestÑespecially after Dorigo and his colleagues introduced several refinements during the 1990s to
improve its scaling behavior.98Variations of the algorithm have also been developed for practical applications such as vehiclerouting, scheduling, routing of traffic through a data network, or the design of connections between
components on a microchip, and the scheduling of special orders in a factory.99 The technique isparticularly useful in such cases because it allows for very rapid rerouting in the face of unexpecteddisruptions in the network. Among the successful commercial applications are plant scheduling for the
consumer products giant Unilever; truck routing for the Italian oil company Pina Petroli; supply chain
optimization and control for the French industrial gas supplier Air Liquide; and network routing for
British Telecom, France Telecom, and MCI.1008.3.4.2  Other Ant Algorithms
Ant algorithms are based on two essential principles: (1) self-organization, in which global behaviorarises from a myriad of low-level interactions, and (2) stigmergy, in which the individuals interact with
one another indirectly using the environment as an intermediary.101 That is, one individual changes itssurroundings (e.g., by laying a pheromone trail), and other individuals then react to those changes at a
later time. As researchers have looked to other ant colony behaviors for inspiration, moreover, those
same two principles turn up again and again.102 For example:¥Sorting behavior. Certain species of ants apparently have an instinct to keep their surroundingsclean; if dead ants are scattered through the nest at random, the workers will immediately begin moving
all the corpses into neat little piles (albeit piles in random locations). These ants likewise seem to have an
instinct for keeping the brood chambers well organized; if workers are presented with a random jumble
of ants-to-be, they will quickly see to it that the eggs and micropupae are in the center, while the larger
and more developed pupae and larvae are toward the outside where they have more room. Simulated
ants can produce much the same results by following a simple local rule: pick up any item that is
isolatedÑthat is, any item that has no others like it in the neighborhoodÑand drop it whenever many
of those items are encountered. Picking things up and then dropping them modifies the environment,
while the constant shifting causes the piles and/or broods to self-organize fairly rapidly.98The algorithm and its refinements are discussed at length in Chapter 2 of E. Bonabeau, M. Dorigo, and G. Theraulaz, SwarmIntelligence: From Natural to Artificial Systems, Oxford University Press, New York, 1999.99Many of their key papers are available for downloading at M. Dorigo, ÒAnt Colony Optimization,Ó 2003, available at http://iridia.ulb.ac.be/~mdorigo/ACO/about.html.100E. Bonabeau, ÒSwarm Intelligence,Ó presented at the OÕReilly Emerging Technology Conference, 2003, April 22-25, 2003,Santa Clara, CA, available at http://conferences.oreillynet.com/presentations/et2003/Bonabeau_eric.ppt.101E. Bonabeau, M. Dorigo, and G. Theraulaz, Swarm Intelligence: From Natural to Artificial Systems, Oxford University Press,New York, 1999.102Among the most notable of these investigators have been entomologist Guy Theraulaz of the French National Center forScientific Research (CNRS) and telecommunications engineer Eric Bonabeau, formally of France Telecom. Bonabeau, in particu-lar, has been among the most active in the promotion and commercialization of ant algorithms, first as head of European
operations for the Santa Fe-based BiosGroup and since 2000 as head of his own company, Icosystem, Inc., of Cambridge,Massachusetts. Details of the various ant behaviors under study, and the algorithms drawn from them, can be found in E.Bonabeau, M. Dorigo, and G. Theraulaz, Swarm Intelligence: From Natural to Artificial Systems, Oxford University Press, New York,1999; E. Bonabeau and G. Theraulez, ÒSwarm Smarts,Ó Scientific American 282(3):72-79, 2000; and E. Bonabeau, ÒSwarm Intelli-gence,Ó presented at the OÕReilly Emerging Technology Conference, 2003, available at http://conferences.oreillynet.com/presentations/et2003/Bonabeau_eric.ppt.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.280CATALYZING INQUIRY¥Division of labor. In order to gather food, maintain the nest, defend against predators, and so on,a colony has to allocate many different tasks among many different ants simultaneouslyÑagain, with-
out the benefit of central planning or individual intelligence. In many cases this is done by a physical
caste system, so that workers do certain jobs, soldiers do others, and so on. Yet ants will often allocate
tasks even within a single caste. A simple mechanism that reproduces this behavior is to give each
individual a response threshold for each task: once the stimuli associated with that task pass the
thresholdÑimagine the smell of accumulating garbageÑthe individual gets to work. The result is that
individuals with higher and higher thresholds keep pitching in until the stimuli are under control,
leaving everyone else free to engage in tasks for which they have low thresholds.¥Cooperative transport. If a single ant encounters a food item thatÕs too big for her to carry alone(e.g., a dead cockroach), she will recruit nest mates via pheromones to help. Now, however, without a
leader or brains, they somehow have to start pulling in the same direction. A simple, two-part rule that
reproduces the observed behavior is (1) if the object is already moving in the direction youÕre pulling,
keep pulling, and (2) itÕs not moving at all, or is moving in a different direction, reorient yourself at
random and start pulling that way. The result is a sequence in which the ants start out pulling theirburden from every direction at once, to no effectÑuntil suddenly, when enough ants just happen to line
up by accident, a kind of phase transition sets in and the load begins to move.¥Cooperative construction. Many species of social insects can build structures of astonishing com-plexity: witness the vast, hexagonal combs of the honeybee or the multilayered, intricately swirling
nests of the paper wasp. And yet again, they manage to do so without the benefit of central planning or
individual intelligence. One way to account for such behavior in simulated insects is to equip each
individual with a collection of local rules: in situation 1, take action A; in situation 2, take action B; and
so on. For a wasp carrying a load of wood pulp, say, such a rule might be, ÒIf youÕre surrounded by
three walls, then deposit the pulp.Ó In general, each insect will modify the environment encountered by
the others, and the structure will organize itself in much the same way that the proteins comprising a
virus particle assemble themselves inside an infected cell.Ant algorithms are conceptually similar to the particle swarm optimization algorithm described inSection 8.2.1. However, at least in the case of the Ant Colony Optimization algorithm, it is known that
ants really use the algorithm described. For this reason, this algorithm was placed in the category of
biologically inspired mechanisms (rather than principles).8.4  BIOLOGY AS PHYSICAL SUBSTRATE FOR COMPUTING
8.4.1  Biomolecular Computing
The idea of constructing computer components from single molecules or atoms is the logical, ifdistant, end point of the seemingly inexorable miniaturization of chips and has been foreseen at least
since Richard FeynmanÕs lecture ÒThereÕs Plenty of Room at the BottomÓ in 1959.103 Molecular comput-ing would have significant advantages, most obviously minuscule size of the resulting component, but
also a potentially low marginal cost per component and extreme energy efficiency. However, the tech-
nology for the precision placing of single atoms or molecules on a large scale is still in its infancy.However, there is a significant shortcut available: to use biological molecules, including DNA,RNA, and various enzymes, as instruments to perform computational tasks. The sophisticated func-
tions of DNA and related molecules, coupled with the existing technological infrastructure for synthe-
sizing, manipulating, and analyzing them found in molecular biology laboratories, make it feasible to
employ them as a universal set of computing components. Also, because the code of DNA is essentially103R.P. Feynman, ÒThereÕs Plenty of Room at the Bottom,Ó American Physical Society, December 29, 1959; available at http://www.zyvex.com/nanotech/feynman.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING281a digital code, particular strands of DNA can be used to code information, and in particular, joiningsand other recombinations of these strands can be used to represent putative solutions to certain compu-
tational problems.This idea is known variously as DNA computation, molecular computation, and biomolecularcomputation (BMC). The use of DNA as a computational system had been discussed theoretically by T.
Head in 1987,104 but the idea leapt into prominence with Len AdlemanÕs publication in 1994 of a
working experiment (Box 8.3) that solved a seven-node instance of the Hamiltonian path problem, an
NP-complete problem that is a special case of the Traveling Salesman Problem.1058.4.1.1  Description
Early attention has focused on DNA because its properties are extremely attractive as a basis for acomputational system. First, it offers a digital abstraction: the value of a piece of DNA can be precisely
and only A, G, T, or C. This abstraction is of course quite familiar to the digital abstractions of 0 and 1.Second, the Watson-Crick complementarity of the bases (A with T, G with C) allows matching opera-
tions, conceptually similar to ÒifÓ clauses in programming. Third, DNAÕs construction as a string allows
a number of useful operations such as insertion, concatenation, deletion, and appending. Next, billions
of years of evolution have provided a large set of enzymes and other molecules that perform those
operations, some in very specific circumstances. Finally, the last few decades of progress in molecular
biology have created a laboratory and instrument infrastructure for the manipulation and analysis of
DNA, such as the custom synthesis of sequences of DNA, chips that can detect the presence of indi-
vidual sequences, and techniques such as polymerase chain reaction (PCR) that can amplify existing
sequences. Without such an infrastructure (importantly including the existence of a body of trained
laboratory technicians), the use of DNA for computation would be entirely theoretical.Biomolecular computing provides a number of advantages that make it quite attractive as a poten-tial base for computation. Most obvious are its information density, about 1021 bits per gram (billions oftimes more dense than magnetic tape), and its massive parallelism, 1015 or 1016 operations per sec-ond.106 Less immediately apparent, but of equal potential importance, is its energy efficiency: it usesapproximately 10Ð19 joules per operation, close to the information theoretic limit (compared to 10Ð9joules per operation for silicon).One class of biomolecular computing generates witness molecules for all possible solutions to aproblem and then uses molecular selection to sift out molecules that represent solutions to the problem
at hand. This was the basic architecture developed by Adleman (described in Box 8.3), and with an
exponential amount of witness material, this approach can theoretically solve NP-complete problems.
Short sequences of DNA (or RNA) are used to represent data, and these are combined to form longer
strands, each of which represents a potential solution. Obtaining the particular DNA strand that repre-
sents the solution is thus based on laboratory processes that extract the proper DNA strand, and these
laboratory processes are based on the existence of an algorithm that can distinguish between correct and
incorrect solutions.A further important step was taken in 2001 by Benenson et al., who developed a programmablefinite automaton comprising DNA and DNA-manipulating enzymes that solves certain computational
problems autonomously.107 In particular, the automatonÕs ÒhardwareÓ consisted of a restriction nu-104T. Head, ÒFormal Language Theory and DNA: An Analysis of the Generative Capacity of Specific Recombinant Behaviors,ÓBulletin of Mathematical Biology 49(6):737-759, 1987.105L.M. Adleman, ÒMolecular Computation of Solutions to Combinatorial Problems,Ó Science 266(5187):1021-1024, 1994.106It is only the fact of massive parallelism that makes biological computing at all feasible, because biological switching speedsare diffusion-limited and quite slow.107Y. Benenson, T. Paz-Elizur, R. Adar, E. Keinan, Z. Livneh, and E. Shapiro, ÒProgrammable and Autonomous ComputingMachine Made of Biomolecules,Ó Nature 414(6862):430-434, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.282CATALYZING INQUIRYBox 8.3Adleman and DNA ComputingAdleman used the tools of molecular biology to solve an instance of the directed Hamiltonian path problem.A small graph was encoded in molecules of DNA, and the ÒoperationsÓ of the computation were performedwith standard protocols and enzymes. This experiment demonstrates the feasibility of carrying out computa-tions at the molecular level.The Hamiltonian path problem is based on finding a special path through an arbitrarily connected set of nodes(i.e., an arbitrary directed graph). (The adjective ÒdirectedÓ means that the connections between nodes are
unidirectional, so that a path from A to B does not mean necessarily that another connection from B to Aexists.) This path (the Hamiltonian path) is special in the sense that beginning with a specified entering nodeand ending with a specified exiting node, a continuous path exists that enters and exits every other node once
and only once. Hamiltonian paths do not necessarily exist for a given directed graph, and their existence maydepend on an appropriate specific choice of entering and exiting nodes.All known algorithms for determining whether an arbitrary directed graph with designated vertices has a Hamil-tonian path exhibit worst-case exponential complexity, which means that there are some directed graphs with asmall number of nodes for which this determination would take an impractical amount of computing time.One method for determining if a Hamiltonian path exists is illustrated in the first column of the table below.StepAlgorithmic StepBiological Equivalent
0Establish directed graph notationEncode each node and directed node-to-node path as
as problem representation.a specific DNA sequence.
1Generate all possible pathsCombine large amounts of these DNA sequences,
through the graph.and with a sufficiently large quantity, the probability
that all possible paths will be generated is essentiallyunity. (In general, these various combinations will
be in length several multiples of a single sequence.)2Keep only those paths that beginUse polymerase chain reaction (PCR) that amplifies
with a specified starting andonly those molecules encoding paths that begin and
ending node.end with the specified nodes.
3If the graph has n nodes,Separate only those sequences from step 2 that have
then keep only those paths that enterthe correct length (corresponding to the number of
exactly n nodes.nodes in the graph).
4Keep only those paths that enterSeparate the sequences from step 3 that have a
all of the nodes of the graph atsubsequence corresponding to each and every node.

least once.5If any paths remain, say, ÒYes, aUse PCR amplification on the output of step 4, what
Hamiltonian path existsÓ;remains after step 5 represents the solution to the
otherwise, say ÒNo.Óproblem.
SOURCE: Adapted from L.M. Adleman, ÒMolecular Computation of Solutions to Combinatorial Problems,Ó Science 266(5187):1021-1024, 1994.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING283clease and ligase, while the software and input were encoded by double-stranded DNA. Programmingwas implemented by choosing appropriate software molecules. The automaton processed the input
molecule through a cascade of restriction, hybridization, and ligation cycles, producing a detectable
output molecule encoding the automatonÕs computational result. However, a finite-state automaton is
not Turing-complete, and the actual demonstration of a Turing-complete biomolecular machine with a
set of primitives sufficient for universal computation has yet to be shown experimentally.108Since AdlemanÕs initial publication, researchers have explored many variants of the basic biologicalapproach. One such variant is the use of RNA, which simplifies the process of removing invalid se-
quences. In this variant, RNA is used for the solution sequences and DNA is used to represent an
element of an invalid solution. Thus, any potential solution that was invalid would be represented by a
DNA-RNA hybridized double strand. A single enzyme, ribonuclease H, destroys all DNA-RNA hy-
bridized pairs, leaving only valid solutions. This is significantly simpler than the use of many, poten-
tially noncompatible enzymes necessary to mark and destroy the appropriate DNA-DNA hybrids in the
traditional method. (In developing an algorithm based on RNA computing for solving a certain chess
problem, Cukras et al.109 found that although the algorithm was able to recover many more correctsolutions than would be expected at random, the persistence of errors continued to present the most
significant challenge.)Other variants of the process seek to automate or simplify the management of stages of the reac-tions. In the original experiments, the DNA reactions took place in solution in test tubes or other
containers, with stages of the process controlled by humansÑfor example, by introducing new en-
zymes, changing the temperature (perhaps to break chemical bonds), or mixing DNA solutions. Some of
these steps can be automated through the use of laboratory robotics. In some variants, DNA strands are
chemically anchored to various types of beads; these beads can be designed with different properties,
such as being magnetic or electrically charged, allowing the manipulation of the DNA strands through
the application of electromagnetic fields. Another solution is to use microfluidic technologies, which
consist of MEMS devices that operate as valves and pumps; a properly designed system of pipettes and
microfluidic devices offers significant advantages by automating tasks and reducing the total volume of
materials required.110Still another variant is to restrict the chemical operations to a surface, rather than to a three-dimensional volume.111 In this approach, DNA sequences, perhaps representing all of the solution
space of an NP problem, would be chemically attached to a surface. Challenges in this approach include
the attachment chemistry, addressing particular strands on the surface, and determining whether chemi-
cal attachment interferes with DNA hybridization and enzymatic reactions.A second class of biomolecular computing begins with an input and a program represented in amolecular form and evolves the program in a number of steps to process the input to produce an output.
In this approach, the complexity of the problem does not manifest itself in the number of starting
molecules, but rather in the form of the rules provided and the amount of time or number of steps
needed to fully evaluate a particular problem and input. For example, in the programmed mutagenesis
method, DNA molecules that represent rewrite rules are combined with DNA molecules that encode
input data and program. When the combined mixture of these DNA molecules is thermally cycled in the108However, Rothemund has provided a highly detailed description of a Turing-complete DNA computer. See P.W.K.Rothemund, ÒA DNA and Restriction Enzyme Implementation of Turing Machines,Ó pp. 75-119 in DNA Based Computers: Pro-ceedings of a DIMACS Workshop, Vol. 27, R.J. Lipton and E.B. Baum eds., DIMACS Series in Discrete Mathematics and TheoreticalComputer Science, American Mathematical Society,  Princeton, NJ, 1996.
109A.R. Cukras, D. Faulhammer, R.J. Lipton, and L.F. Landweber, ÒChess Games: A Model for RNA Based Computation,ÓBiosystems 52(1-3):35-45, 1999.110A. Gehani and J.H. Reif, ÒMicro-Flow Bio-Molecular Computation,Ó BioSystems 52(1-3):197-216, October 1999.111L.M. Smith, R.M. Corn, A.E. Condon, M.G. Lagally, A.G. Frutos, Q. Liu, and A.J. Thiel, ÒA Surface-based Approach to DNAComputation,Ó Journal of Computational Biology 5(2):255-267, 1998.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.284CATALYZING INQUIRYpresence of DNA polymerase and DNA ligase, the rewrite rules cause new DNA molecules to beproduced that represent intermediate states in a computation. These new DNA molecules can be a very
general function of the beginning mixture of DNA molecules, and a DNA encoding has been discovered
that permits such a system to theoretically implement arbitrary computation.8.4.1.2  Potential Application Domains
The field of biomolecular computing is still composed of theory and tentative laboratory steps; weare years away from commercial activity. The results of laboratory experiments are proofs of concept; as
yet, no biomolecular computer has outperformed an electronic computer.Biomolecular computing is, in principle, well suited for problems that involve Òbrute forceÓ solu-tions, in which candidate solutions can be tested individually to see if they are correct. As noted above,
the main application pursued for the first decade of biomolecular computing work is the exhaustive
solution of NP-complete problems. While this has been successful for small numbers of nodes (up to 20),
the fact that it requires exponential volumes of DNA most likely limits the further development of NP-
solving systems (see below for further discussion).Biomolecular computation also has potential value in the field of cryptography. For example, DNA,with its incredible information density, could serve as an ideal one-time pad, as a tiny sample could
provide petabytes of data suitable for use for encryption (as long as it was suitably random). More
generally, biomolecules could serve as components of a larger computational system, possibly serving
alongside traditional silicon-based semiconductors. For this, and indeed any biomolecular computing
system, a challenge is the transformation of information from digital representation into biomolecules
and back again. Traditional molecular biological engineering has provided a number of tools for synthe-
sizing DNA sequences and reading them out; however, these tend to be fairly lengthy processes. Recent
advances in DNA chips show the potential for more efficient biodigital interfaces. For example, photo-
sensitive chips will synthesize given sequences of DNA based on optical inputs and, similarly, will
produce optical signals in the presence of certain sequences. These optical signals are two-dimensional
arrays of intensities that can be read by digital image-processing hardware and software. Other ap-
proaches for output include the inclusion of fluorescent materials in the DNA molecules or other
additives that can be detected with the use of microscopy.A potential component role for biomolecules is as memory. Whereas biomolecular computationmust compete against rapidly improving and increasingly parallel optoelectronic technologies for com-
putation, biomolecular memory is many orders of magnitude superior to conventional magnetic imple-
mentations in terms of density. Although DNA memory is unlikely to be used as the rapid-access read-
write memory of modern computers, its density makes it useful for Òblack-boxÓ applications that write
a great deal of data, but read only on rare occasions (a fact that would usually tend to increase the
acceptable retrieval time).One such implementation would use DNA as the storage medium of an associative database. ADNA strand would encode the information of a specific record, with sequences on that strand repre-
senting attributes of the record and a unique index. Query strings would be composed of the comple-
ment of the desired attribute. Although individual lookups would be slow (limited by the speed of
DNA chemistry), the total amount of information stored would be enormous and the queries would
execute in parallel over the entire database. In contrast, conventional electronic computer implementa-
tions of associative memory require linear time with the size of the database.Such a DNA database might be most useful as a set of tools to manipulate, retrieve, or analyzeexisting biological or chemical substances. For example, special-purpose DNA computers might search
through databases of genetic material. In this model, a large library of genetic material (perhaps repre-
senting DNA sequences of various biological lineages, or of criminals) would be stored in its original
DNA form, rather than as an electronic digital representation. Biomolecular computers would generate
appropriate strands representing a query (matching a sequence found in a new organism, or at a crimeCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING285scene) and, in massively parallel fashion, identify potential matches. This idea could even be extendedto queries of proteins or chemicals, if the appropriate query strand of DNA can be generated.A separate approach to biomolecular memory uses changes in the sequence of individual strands torepresent bits. Certain enzymes known as site-specific recombinases (SSRs) can (among a set of other
potential modifications) reverse the sequence of the bases between two marker sequences; repeated
application of such an enzyme would flip the sequence back and forth, representing 0 and 1. In this
implementation, a single bit requires a long series of bases; research aims at attaining the far more dense
use of single bases as bits (in fact, as two bits, since each base can have four values).8.4.1.3  Challenges
Biomolecular computing faces some significant challenges to adoption beyond the laboratory. Themost cited barrier is the exponential doubling of the volume of DNA required to perform exhaustive
search of NP-complete problems, such as done by Adleman (Section 8.4.1.1). That is, while the number
of different DNA sequences required grows linearly with the number of directed paths in a graph, the
volume of those DNA sequences needed to solve a given problem is exponential in the problemÕs size
(in this case, the number of nodes in the graph). Put differently, for the problems to which DNA
computing is applicable, a problem that can be solved in exponential time on silicon-based von
Neumann computers is replaced by one that can be solved with exponential increases of mass. It is thus
an open question today about what kinds of problems can be solved practically using DNA computing.
For example, Hartmanis reports that the amount of DNA necessary to replicate AdlemanÕs experiment
for a 200-node problem would exceed the weight of the Earth.112While this is a valid concern, standard computers have been widely accepted despite their inabilityto solve NP-complete problems in a timely fashion. To the best understanding of computer science
today, NP-complete problems are fundamentally challenging, and so it ought to be no surprise that
even new models of computation struggle with them. Nevertheless some breakthrough may provide
subexponential scaling for biomolecular-based exhaustive search.A second concern involves the time-consuming and expensive laboratory techniques necessary toset up and read out the answer from an experimentÑin essence, the input-output problem for bio-
molecular computing. While DNA reactions themselves offer staggering parallelism (although in fact
they take about an hour), the bottleneck may be the time it takes for trained humans to undertake the
experiment. AdlemanÕs experiment required about 7 days of laboratory work. And although DNA

synthesis itself is cheap, some of the enzymes used in AdlemanÕs experiments cost 10,000 times as much
as gold,113 suggesting that scaling up significantly may not be feasible on economic grounds.Related to this is the fact that DNA computation is not error-free. Synthesis of sequences canintroduce errors; strands of DNA that are close to being complementsÑbut not quiteÑmay still hybrid-
ize; point mutations may occur; sheer chance may allow strands of DNA to escape enzymatic destruc-
tion; and so forth. Although comparatively high error rates can be acceptable in laboratory environ-
ments, this is far more problematic for computation. The problem can be ameliorated partly by the use
of techniques familiar to communications protocols, including error-correcting codes and careful design
of the code words used in computation, so as to maximize the information distance between any pair.
This last example is a good case of computer science and biological cooperation: the distance between a
pair of code words composed of a series of bases is a product of both its information content and its
biochemical properties. Word design is currently an active area of DNA computation research.112J. Hartmanis, ÒOn the Weight of Computations,Ó Bulletin of the European Association for Theoretical Computer Science 55:136-138, 1995.113A.L. Delcher, L. Hood, and R.M. Karp, ÒReport on the DNA/Biomolecular Computing Workshop (June 6-7, 1996),Ó Na-tional Science Foundation, NSF 97-168, 1998, available at http://www.nsf.gov/pubs/1998/nsf97168/nsf97168.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.286CATALYZING INQUIRYA related problem is the lack of programmability of current models. Even if experimental verifica-tion of Turing-complete biomolecular computing can be achieved, individual runs must still be care-
fully tuned to a specific instance of a specific problem, much like the hardwiring of the first generation
of electronic computers. Worse yet, the sequences of biomolecules synthesized for a particular
biomolecular computation are usually consumed or destroyed during the computation. For a replica-
tion of the experiment, even with the same dataset, much of the entire process of setup must be
repeated. If a different dataset or a different ÒprogramÓ is run, then other steps must be included, such
as designing the set of sequences to be used as ÒwordsÓ of the computation and determining the set of
enzymes and concentration levels necessary to correctly identify, mark, destroy, and read out the
appropriate strands of nucleic acids. The ability to formulate a problem of any generality in terms that
map onto a set of chemical processing lab procedures is likely an essential aspect of DNA computing,
but it is not at all clear today how such formulations can occur in general.Finally, the most significant challenge is the high bar that DNA computation will have to surpass togain wide acceptance. MooreÕs law is expected to continue unabated for at least a decade, resulting in
petaflop machines by 2015. Additionally, biomolecular computation is not the only radical technique in
town; quantum computation, various other applications of nanotechnology, analog computing, and
other contenders may turn out to offer more favorable performance, programmability, or convenience.These challenges are quite significant and possibly decisive. Len Adleman himself was pessimisticabout the prospect of general computation in a 2002 paper: ÒDespite our successes, and those of others,
in the absence of technical breakthroughs, optimism regarding the creation of a molecular computer
capable of competing with electronic computers on classical computational problems in not war-
ranted.Ó114 Of course, such breakthroughs may yet occur, and this possibility warrants some level ofcontinued research.8.4.1.4  Future Directions
While it was DNAÕs resemblance to the tape of a Turing machine that inspired Adleman to investi-gate the possibility, this model has not yet been pursued experimentally. Nor is it likely that it would
have practical computing utilityÑa Turing machine is extraordinarily slow even executing simple
algorithms.A very different approach would involve single molecules of DNA (or RNA or another biomolecule)acting as the memory of a single process, while enzymes performed the computation by splicing and
copying sequences of bases. Although this has been discussed theoretically, it has not yet been shown in
an experiment. This model would be best used for massively parallel applications, since the individual
operations on DNA are still quite slow compared to electronic components, but it would offer massive
improvements of density and energy efficiency over traditional computers.In a slightly different approach, enzymes that operate on DNA sequences are used as logic gates,such as XOR, AND, or NOT. DNA strands are data, and the enzymes, by reacting to the presence of
certain sequences, modify the DNA or generate new strands. Thus, using fairly traditional digital logic
design techniques, assemblies of logic gates can be constructed. The resulting circuits will operate in
exactly the same manner as traditional silicon electronic-based circuits, but at the energy efficiency and
size of molecules.115Even if it turns out that biomolecular computation is a dead end, the research that went into it willnot be for naught: the laboratory techniques, enabling technologies, and deeper understanding of
114R.S. Braich, C. Johnson, P.W.K. Rothemund, D. Hwang, N. Chelyapov, and L.M. Adleman, ÒSolution of a 20-Variable 3-SATProblem on a DNA Computer,Ó Science 296(5567):499-502, 2002.115M.N. Stojanovic, T.E. Mitchell, and D. Stefanovic, ÒDeoxyribozyme-based Logic Gates,Ó Journal of the American ChemicalSociety 124(14):3555-3561, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING287biomolecular processes will be valuable. Already, commercial spinoff technologies are available: basedon AdlemanÕs research, a company in Japan developed a way to synthesize 10,000 DNA sequences to
rapidly search for the presence of genes related to cancer.116 Also, biologist Laura LandweberÕs researchinto biomolecular computation at Princeton has provided insights for her research on DNA and RNA
mechanisms in living organisms. For example, her and Lila KariÕs analysis of the DNA manipulations
that occur in some protozoa is based on techniques of formal languages from computer science, show-
ing that the cellular operations performed by these protozoa are actually Turing-complete. The use of
formal computer science theory, in other words, has proven a useful tool for the analysis of natural
genetic processes.8.4.2  Synthetic Biology
As a field of inquiry, the goal of biologyÑreductionist or otherwiseÑhas been to catalog the diver-sity of life and to understand how it came about and how it works. These goals emphasize the impor-
tance of observation and understanding. Synthetic biology, in contrast, is a new subfield of biology with
different intent: based on biological understanding, synthetic biology seeks to modify living systems
and create new ones.Synthetic biology encompasses a wide variety of projects, definitions, and goals and thus is difficultto define precisely. It usually involves the creation of novel biological functions, such as custom meta-
bolic or genetic networks, novel amino acids and proteins, and even entire cells. For example, a syn-
thetic biology project may seek to modify Escherichia coli to fluoresce in the presence of TNT, creating ineffect a new organism that can be used for human purposes.117 In one sense, this is a mirror image ofnatural selection: adding new features to lineages not through mutation and blind adaptation to an
environment, but through planned design and forethought. Synthetic biology shares some similarities
with recombinant genetic engineering, a common approach that involves transplanting a gene from one
organism into the genome of another. However, synthetic biology does not restrict itself to using actual
genes found in organisms; it considers the set of all possible genes. In effect, synthetic biology involves
writing DNA, not merely reading it.One basic motivation of this field is that creating artificial cells, or introducing novel biologicalfunctions, challenges our understanding of biology and requires significant new insight. In this view,
only by reproducing life can we demonstrate that we fully understand it; this is the ultimate acid test for
our theories of biology. It is precisely analogous to early synthetic chemistry: only by the successful
synthesis of a substance would a theory of its composition be verified.118More broadly, some synthetic biology researchers see created life as an opportunity to explorewider conceptions of life beyond the examples provided by nature. For example, what are the physical
limitations of biological systems?119 Are other self-replicating molecular information systems possible?Are there general principles of biochemical organization? These inquires may help researchers to un-
derstand how life began on Earth, as well as the possibility of life in extraterrestrial environments.120Finally, synthetic biology has the potential to contribute significantly to technology, offering inmany ways a new industrial revolution. In this view, chemical synthesis, detection, and modification
could all be done by creating a microbe with the desired characteristics. This holds the promise of new
methods for energy production, environmental cleanup, pharmaceutical synthesis, pathogen detection116Business Week, ÒLen Adleman: Tapping DNA Power for Computers,Ó January 4, 2002.117L.L. Looger, M.W. Dwyer, J.J. Smith, and H.W. Hellinga, ÒComputational Design of Receptor and Sensor Proteins withNovel Functions,Ó Nature 423(6936):185-190, 2003.118S.A. Benner, ÒAct Natural,Ó Nature 421:118, 2003.119D. Endy, quoted in L. Clark, ÒWriting DNA: First Synthetic Biology Conference Held at MIT,Ó available at http://web.mit.edu/be/news/synth_bio.htm.120J.W. Szostak, D.P. Bartel, and P.L. Luisi, ÒSynthesizing Life,Ó Nature 409(6818):387-390, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.288CATALYZING INQUIRYand neutralization, biomaterials synthesis, or any task that can be done by biochemistry. This is essen-tially a form of nanotechnology, in which the already existing mechanisms of biology are employed to
operate on structures at the molecular scale.However, all of these goals will require a different set of approaches and techniques than traditionalbiology or any natural science provides. While synthetic biology employs many of the same techniques
and tools as systems biologyÑsimulation, computer models of genetic networks, gene sequencing and
identification, massively parallel experimentsÑit is more of an engineering discipline than a purely
natural science.8.4.2.1  An Engineering Approach to Building Living Systems
Although as a viewpoint it is not shared by all synthetic biology researchers, a common desire is toinvent an engineering discipline wherein biological systems are both the raw materials and the desired
end products. EngineeringÑparticularly, electronics designÑis an appropriate discipline to draw on,
because no other design field has experience with constructing systems composed of millions or even
billions of components. The engineering design approaches of abstraction, modularity, protocols, and
standards are necessary to manage the complexity of the biomolecular reality.One important piece of establishing an engineering discipline of building living systems is to createa library of well-defined, well-understood parts that can serve as components in larger designs. A team
led by Tom Knight and Drew Endy at the Massachusetts Institute of Technology (MIT) have created the
MIT Registry of Standard Biological Parts, also known as BioBricks, to meet this need.121 An entry in theregistry is a sequence of DNA that will code for a piece of genetic or metabolic mechanism. Each entry
has a set of inputs (given concentrations or transcription rates of certain molecules) and a similar set of
outputs.The goal of such a library is to provide a set of components for would-be synthetic biology design-ers, where the parts are interchangeable, components can be composed into larger assemblies and easily
be shared between separate researchers, and work can build on previous success by incorporating
existing components. Taken together, these attributes allow the designers to design in ignorance of the
underlying biological complexity.These BioBricks contain DNA sequences at either end that are recognized by specific restrictionenzymes (i.e., enzymes that will cut DNA at a target sequence); thus, by adding the appropriate en-
zymes, a selected DNA section can be spliced. When two or more BioBricks sequences are ligated
together, the same restriction sequences will flank the ends of the DNA sequence, allowing the re-
searcher to treat the composite as a single component. BioBricks are in the early stages of research still,
and the final product will likely be substantially different in construction.8.4.2.2  Cellular Logic Gates
Of particular interest to synthetic biologists are modifications to cellular machinery that simulatethe operations of classical electronic logic gates, such as AND, NOT, XOR, and so forth. These are
valuable for many reasons, including the fact that that their availability in biological systems would
mean that researchers could draw on a wide range of existing design experience from electronic circuits.
Such logic gates are especially powerful because they increase the ability of designers to build more
sophisticated control and reactivity into engineered biological systems. Finally, it is the hope of some
researchers that, just as modern electronic computers are composed of many millions of logical gates, a
new generation of biological computers could be composed of logic gates embedded in cells.121T. Knight, ÒIdempotent Vector Design for StandardAssembly of Biobricks,Ó available at http://docs.syntheticbiology.org/
biobricks.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING289Researchers have begun to construct cellular logic gates in which signals are represented by proteinconcentrations rather than electrical voltages, with the intent of developing primitives for digital com-
puting on a biological substrate and control of biological metabolic and genetic networks. In other
words, the logic gate is an abstraction of an underlying technology (based on silicon or on cellular
biology): once the abstraction is available, the designer can more or less forget about the underlying
technology.A biological logic gate uses intracellular chemical mechanisms, such as the genetic regulatorynetwork, metabolic networks, or signaling systems to organize and control biological processes, just as
electronic mechanisms are used to control electronic processes.Any logic gate is fundamentally nonlinear, in the sense that it must be able to produce two levels ofoutput (zero and one), depending on the input(s), in a manner that is highly insensitive to noise (hence,
subsequent computations based on the output of that gate are not sensitive to noise at the input). That
is, variations in the input levels that are smaller than the difference between 1 and 0 must not be
significant to the output of the gate.Once a logic gate is created, all of the digital logic design principles and tools developed for use inthe electronic domain are in principle applicable to the construction of systems involving cellular logic.A basic construct in digital logic is the inverting gate. Knight et al.122 describe a cellular inverterconsisting of an ÒoutputÓ protein Z and an ÒinputÓ protein A that serves as a repressor for Z. Thus,
when A is present, the cellular inverter does not produce Z, and when A is not present, the inverter does
produce Z. One implementation of this inverter is a genetic unit with a binding site for A (an operator),
a site on the DNA at which RNA polymerase binds to start transcription of Z (a promoter), and a
structural gene that codes for the production of Z.Protein Z is produced when RNA polymerase binds to the promoter site. However, if A binds to theoperator site, it prevents (represses) the binding of RNA polymerase to the promoter site. Thus, if
proteins have a finite lifetime, the concentration of Z varies inversely with the concentration of A. To
turn this behavior into digital form, it is necessary for the cellular inverter to provide low gain for
concentrations of A that are very high and very low, and high gain for intermediate concentrations of A.Overall gain can be increased by providing multiple copies of the structural gene to be controlled bya single operator binding site. Where high and low concentrations call for low gain, a combination of
multiple steps or associations into a single pathway (e.g., the mitogen-activated protein [MAP]-kinase
pathway, which consists of many switches that turn on successively) can be used to generate a much
sharper nonlinear response for the system as a whole than can be obtained from a single step.Once this inverter is available, any logic gate can be constructed from combinations of inverters.123For example, a NAND gate can be constructed from two inverters that have different input repressors
(e.g., A1 and A2) but the same output protein Z, which will be produced unless both A1 and A2 are
present. On the other hand, cellular logic and electronic logic differ in that cellular logic circuits are
more inherently asynchronous because signal propagation in cellular logic circuits is based on diffusion
of proteins, which makes both synchronization and high speed very hard to achieve. In addition,
because these diffusion processes are, by definition, not channeled in the same way that electrical
signals are confined to wires, a different protein must be used for each unique signal. Therefore, the
number of proteins required to implement a circuit is proportional to the complexity of the circuit.
Using different proteins means that their physical and chemical properties are different, thus complicat-
ing the design and requiring that explicit steps be taken to ensure that the signal ranges for coupled
gates are appropriately matched.122T.F. Knight and G.J. Sussman, ÒCellular Gate Technology,Ó Unconventional Models of Computation, C. Calude, J. Casti, andM.J. Dinneen, eds., Springer, Auckland, New Zealand, 1998.123In general, the availability of an inverter is not sufficient to compute all Boolean functionsÑan AND or an OR function isalso needed. In this particular case, however, the implementing technology permits inverters to be placed side by side to formNOT-AND (NAND) gates.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.290CATALYZING INQUIRYCellular circuits capable of logic operations have been demonstrated. For example, Elowitz andLeibler designed and implemented a three-gene network that produced oscillations in protein concen-
tration.124 The implemented network worked in only a fraction of the cells but did, in fact, oscillate.Gardner et al. built a genetic latch that acted as a toggle between two different stable states of gene
expression.125 They demonstrated that different implementations of the general designs yielded moreor less stable switches with differing variances of concentration in the stable states. While both of these
applications demonstrate the ability to design a simple behavior into a cell, they also demonstrate the
difficulty in implementing these circuits experimentally and meeting design specifications.In a step toward clinical application of this type of work,126 Benenson et al. developed a molecularcomputer that could sense its immediate environment for the presence of several mRNA species of
disease-related genes associated with models of lung and prostate cancer and, upon detecting all of
these mRNA species, release a short DNA molecule modeled on an anticancer drug.127 Benenson et al.suggest that this approach might be applied in vivo to biochemical sensing, genetic engineering, and
medical diagnosis and treatment.8.4.2.3  Broader Views of Synthetic Biology
While cellular logic emphasizes the biological network as a substrate for digital computing, syn-thetic biology can also use analog computing. To support analog computing, the biomolecular networks
involved would be sensitive to small changes in concentrations of substances of interest. For example, a
microbe altered by synthetic biology research might fluoresce with an intensity proportional to the
concentration of a pollutant. Such analog computing is in one sense closer to the actual functionality of
existing biomolecular networks (although of course there are many digital elements in such networks as
well), but is more alien to the existing engineering approaches borrowed from electronic systems.For purposes of understanding existing biology, one approach inspired by synthetic biology is tostrip down and clean up genomes for maximal clarity and comprehensibility. For example, Drew
EndyÕs group at MIT is cleaning the genome of the T7 bacteriophage, removing all unnecessary se-
quences, editing it so that genes are contiguous, and so on.128 Such an organism would be easier tounderstand than the wild genotype, although such editing would obscure the evolutionary history of
the genome.While synthetic biology stresses the power of hand-designing biological functions, evolution andselection may have their place. Ron WeissÕs group at Princeton University has experimented with using
artificial selection as a way to achieve desired behavior.129 This approach can be combined with engi-neering approaches, using evolution as a final stage to eliminate unstable or faulty designs.The most extreme goal of synthetic biology is to generate entirely synthetic living cells. In principle,these cells need have no chemical or structural similarity to natural cells. Indeed, achieving an under-
standing of the range of potential structures that can be considered living cells will represent a profound
step forward in biology. This goal is discussed further in Section 9.3.124M.B. Elowitz and S. Leibler, ÒA Synthetic Oscillatory Network of Transcriptional Regulators,Ó Nature 403(6767):335-338,2000.125T.S. Gardner, C.R. Cantor, and J.J. Collins, ÒConstruction of a Genetic Toggle Switch in Escherichia coli,Ó Nature 403(6767):339-342, 2000.126Y. Benenson, B. Gil, U. Ben-Dor, R. Adar, and E. Shapiro, ÒAn Autonomous Molecular Computer for Logical Control ofGene Expression,Ó Nature 429(6990):423-429, 2004.127In fact, the molecular computerÑanalogous to a process control computerÑis designed to release a suppressor moleculethat inhibits action of the drug-like molecule.128W.W. Gibbs, ÒSynthetic Life,Ó Scientific American 290(5):74-81, 2004.129Y. Yokobayashi, C.H. Collins, J.R. Leadbetter, R. Weiss, and F.H. Arnold, ÒEvolutionary Design of Genetic Circuits and Cell-Cell Communications,Ó Advances in Complex Systems, World Scientific, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING2918.4.2.4  Applications
While significant from a research view, synthetic biology also has practical applications. A strongdriver of this is the rapidly falling cost of custom DNA synthesis. For a few dollars per base pair in 2004,
laboratories can synthesize an arbitrary sequence of DNA;130 these prices are expected to fall by ordersof magnitude over the next decade. This not only has enabled research into constructing new genes, but
also offers the promise of cost-effective use of synthetic biology for commercial or industrial applica-
tions. Once a new lineage is created, of course, organisms can self-replicate in the appropriate environ-
ment, implying extremely low marginal cost.Cells can be abstracted as chemical factories controlled by a host of process control computers. If theprogramming of these process control computers can be manipulated, or new processes introduced, it
isÑin principleÑpossible to co-opt the functional behavior of cells to perform tasks of engineering or
industrial interest. Natural biology creates cells that are capable of sensing and actuating functions: cells
can generate motion and light, for example, and respond to light or to the presence of chemicals in the
environment. Natural cells also produce a variety of enzymes and proteins with a variety of catalytic
and structural functions. If logic functions can be realized through cellular engineering, cellular com-
puting offers the promise of a seamlessly integrated approach to process control computing.Synthetic or modified cells could lead to more rational biosynthesis of a variety of useful organiccompounds, including proteins, small molecules, or any substance that is too costly or difficult to
synthesize by ordinary bench chemistry. Some of this is already being done by cloning and gene
transfection (e.g., in yeast, plants, and many organisms), but synthetic biology would allow finer con-
trol, increased accuracy, and the ability to customize such processes in terms of quantity, precise mo-
lecular characteristics, and chemical pathways, even when the desired characteristics are not available
in nature.8.4.2.5  Challenges
Synthetic biology brings the techniques and metaphor of electronic design to modify biomolecularnetworks. However, in many ways, these networks do not behave like electronic networks, and the
nature of biological systems provides a number of challenges for synthetic biology researchers in at-
tempting to build reliable and predictable systems.A key challenge is the stochastic and noisy nature of biological systems, especially at the molecularscale. This noise can lead to random variation in the concentration of molecular species; systems that
require a precise concentration will likely work only intermittently. Additionally, as the mechanisms of
synthetic biology are embedded in the genome of living creatures, mutation or imperfect replication can
alter the inserted gene sequences, possibly disabling them or causing them to operate in unforeseen
ways.Unlike actual electronic systems, the components of biomolecular networks are not connected byphysical wires that direct a signal to a precise location; the many molecules that are the inputs and
outputs of these processes share a physical space and can commingle throughout the cell. It is therefore
difficult to isolate signals and prevent cross-talk, in which signals intended for one recipient are re-
ceived by another. This physical location sharing also means that it is more difficult to control the timing
of the propagation of signals; again, unlike electronics, which typically rely on a clock to precisely
synchronize signals, these biomolecular signals are asynchronous and may arrive at varying speeds.
Finally, the signals may not arrive, or may arrive in an attenuated fashion.131130One firm claims to be able to provide DNA sequences as long as 40,000 base pairs. See http://www.blueheronbio.com/genemaker/synthesis.html. Others suggest that sequences in the 100 base pair range are the longest that can be synthesized
today without significant error in most of the resulting strands.131R. Weiss, S. Basu, S. Hooshangi, A. Kalmbach, D. Karig, R. Mehreja, and I. Netravali, ÒGenetic Circuit Building Blocks forCellular Computation, Communications, and Signal Processing,Ó Natural Computing 2:47-84, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.292CATALYZING INQUIRYAside from the technical challenges of achieving the desired results of synthetic biology projects,there are significant concerns about the misuse or unintended consequences of even successful work. Of
major concern is the potential negative effect on the environment or the human population if modified
or created organisms became unmanaged, through escape from a laboratory, mutation, or any other
vector. This is especially a concern for organisms, such as those intended to detect or treat pollutants,
that are designed to work in the open environment. Such a release could occur as a result of an accident,
in which case the organism would have been intended to be safe but may enter an environment in
which it could pose a threat. More worrisome, an organism could be engineered using the techniques of
synthetic biology, but with malicious intent, and then released into the environment. The answer to
such concerns must include elements of government regulation, public health policy, public safety, and
security. Some researchers have suggested that synthetic biology needs an ÒAsilomarÓ conference, by
analogy to the conference in 1975 that established the ground rules for genetic engineering.132Some technical approaches to answer these concerns are possible, however. These include Òbar-codingÓ engineered organisms, that is, including a defined marker sequence of DNA in their genome
(or in every inserted sequence) that uniquely identifies the modification or organism. More ambitiously,
modified organisms could be designed to use molecules incompatible with natural metabolic pathways,
such as right-handed amino acids or left-handed sugars.1338.4.3  Nanofabrication and DNA Self-Assembly
134Nanofabrication draws from many fields, including computer science, biology, materials science,mathematics, chemistry, bioengineering, biochemistry, and biophysics. Nanofabrication seeks to apply
modern biotechnological methodologies to produce new materials, analytic devices, self-assembling
structures, and computational components from both naturally occurring and artificially synthesized
biological molecules such as DNA, RNA, peptide nucleic acids (PNAs), proteins, and enzymes. Ex-
amples include the creation of sensors from DNA-binding proteins for the detection of trace amounts of
arsenic and lead in ground waters, the development of nonsocial DNA cascade switches that can be
used to identify single molecular events, and the fabrication of novel materials with unique optical,
electronic, rheological, and selective transport properties.8.4.3.1  Rationale
Scientists and engineers wish to be able to controllably generate complex two- and three-dimen-sional structures at scales from 10Ð6 to 10Ð9 meters; the resulting structures could have applications inextremely high-density electronic circuit components, information storage, biomedical devices, or
nanoscale machines. Although some techniques exist today for constructing structures at such tiny
scales, such as optical lithography or individual atomic placement, in general they have drawbacks of
cost, time, or limited feature size.Biotechnology offers many advantages over such techniques; in particular, the molecular precisionand specificity of the enzymatic biochemical pathways employed in biotechnology can often surpass
what can be accomplished by other chemical or physical methods. This is especially true in the area of
nanoscale self-assembly. Consider the following quote from M.J. Frechet, a chemistry professor at the132D. Ferber, ÒSynthetic Biology: Microbes Made to Order,Ó Science 303(5655):158-161, 2004.133O. Morton, ÒLife, Reinvented,Ó Wired 13.01, 2005.134Section 8.4.3 draws heavily from T.H. LaBean, ÒIntroduction to Self-Assembling DNA Nanostructures for Computation andNanofabrication,Ó World Scientific, CBGI, 2001; E. Winfree, ÒAlgorithmic Self-Assembly of DNA: Theoretical Motivations and 2DAssembly Experiments,Ó Journal of Biomolecular Structure and Dynamics 11(2):263-270, 2000; J.H. Reif, T.H. LaBean, and N.C.Seeman, ÒChallenges and Applications for Self-Assembled DNA Nanostructures,Ó pp. 173-198 in Proceedings of the Sixth Interna-tional Workshop on DNA-Based Computers, A. Condon and G. Rozenberg, eds., DIMACS Series in Discrete Mathematics andTheoretical Computer Science, Springer-Verlag, Berlin, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING293University of California, Berkeley, who is a leader in the area of the synthesis and control of moleculararchitectures on the nanometer scale:135While most common organic moleculesÑÒsmall moleculesÓÑhave sizes well below one nanometer, mac-romolecules such as proteins or synthetic polymers have sizes in the nanometer range. Within this size
range, it is generally very difficult to control the 3-D structure of the molecules. Nature has learned howto achieve this with proteins and DNA, but most other large synthetic macromolecules have little shapepersistence and precise functional group placement is difficult.It is this fine control of nanoscale architecture exhibited in proteins, membranes, and nucleic acidsthat researchers hope to harness with these applied biotechnologies, and the goal of research into Òself-
assemblyÓ is to develop techniques that can create structures at a molecular scale with a minimum of
manual intervention.Self-assembly, also known as bottom-up construction, is a method of fabrication that relies onchemicals forming larger structures without centralized or external control.136 Because of its ability torun in parallel and at molecular scales, self-assembly is considered to be a potentially important tech-
nique for constructing submicron devices such as future electronic circuit components.Since the role of DNA and related molecules in biology is to generate complicated three-dimen-sional macromolecules such as proteins, DNA is a natural candidate for a system of self-assembly.
Researchers have investigated the potential of using DNA as a medium for self-assembling structures at
the nanometer scale. DNA has many characteristics that make it an excellent candidate for creating
arbitrary components: its three-dimensional shape is well understood (in contrast to most proteins,
which have poorly understood folding behavior); it is a digital, information-encoding molecule, allow-
ing for arbitrary customization of sequence; and it, with a set of easily accessible enzymes, is designed
for self-replication. Box 8.4 describes some key enabling technologies for DNA self-assembly.One important focus of DNA self-assembly research draws on the theory of Wang tiles, a math-ematical theory of tiling first laid out in 1961.137 Wang tiles are polygons with colored edges, and theymust be laid out in a pattern such that the edges of any two neighbors are the same color. Later, Berger
established three important properties of tiling: the question of whether a given set of tiles could cover
an area was undecidable; aperiodic sets of tiles could cover an area; and tiling could simulate a univer-
sal Turing machine,138 and thus was a full computational system.139The core of DNA self-assembly is based on constructing special forms of DNA in which strandscross over between multiple double helices, creating strong two-dimensional structures known as DNA
tiles. These tiles can be composed of a variety of combinations of spacing and interconnecting patterns;
the most common, called DX and TX tiles, contain two or three double helices (i.e., four or six strands),
although other structures are being investigated as well. Ends of the single strands, sequences of
unhybridized bases, stick out from the edges of the tile, and are known as Òsticky endsÓ (or ÒpadsÓ)
because of their ability to hybridizeÑstick toÑother pads. Pads can be designed to attach to the sticky
ends of other tiles. By careful design of the base sequence of these pads, tiles can be designed to connect
only with specific other tiles that complement their base sequence.The congruence between Wang tiles and DNA tiles with sticky ends is straightforward: the stickyends are designed so that they will bond only to complementary sticky ends on other tiles, just as Wang
tiles must be aligned by color of edge. The exciting result of combining Wang tiles with DNA tiles is that
DNA tiles have also been shown to be Turing-complete and thus a potential mechanism for computing.135See http://www.cchem.berkeley.edu.136See, for example, G.M. Whitesides et al., ÒMolecular Self-Assembly and NanochemistryÑA Chemical Strategy for theSynthesis of Nanostructures,Ó Science 254(5036):1312-1319, 1991.137H. Wang, ÒProving Theorems by Pattern Recognition,Ó Bell System Technical Journal 40:1-41, 1961.138A universal Turing machine is an abstract model of computer execution and storage with the ability to perform anycomputation that any computer can perform.139R. Berger, ÒThe Undecidability of the Domino Problem,Ó Memoirs of the American Mathematical Society 66:1-72, 1966.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.294CATALYZING INQUIRYBox 8.4Enabling Technologies for DNA Self-replicationDNA Surface ArraysCurrent DNA array technologies based on spotting techniques or photolithography extend down to pixel sizeson the order of 1 micron.1 Examples of these arrays are those produced by Affymetrix and Nanogen.2  Thecreation of DNA arrays on the nanometer scale require new types of non-photolithographic fabrication tech-nologies, and a number of methods utilizing scanning probe microscopic techniques and self-assembledsystems have been reported.DNA MicrochannelsThe separation and analysis of DNA by electrophoresis is one of the driving technologies of the entire genom-ics area. The miniaturization of these analysis technologies with micron-sized fluidic channels has beenvigorously pursued with the end goal of creating Òlab on a chipÓ devices. Examples are the products of Caliper
Technologies and Aclara Biosciences.3 The next generation of these devices will target the manipulation ofsingle DNA molecules through nanometer-sized channels. Attempts to make such channels both lithograph-ically and with carbon nanotubes have been reported.DNA Attachment and Enzyme ChemistryRobust attachment of DNA, RNA, and PNA onto surfaces and nanostructures is an absolute necessity for theconstruction of nanoscale objectsÑboth to planar surfaces and to nanoparticles. The primary strategy is to usemodified oligonucleotides (e.g., thiol, amine-containing derivatives) that can be reacted either chemically or
enzymatically. The manipulation of DNA sequences by enzymatic activity has the potential to be a verysequence-specific methodology for the fabrication of DNA nanostructures.4DNA-modified NanoparticlesNanoscale objects that incorporate DNA molecules have been used successfully to create biosensor materi-als. In one example, the DNA is attached to a nanometer-sized gold particle, and then the nucleic acid is usedto provide biological functionality,while the optical properties of the gold nanoparticles are used to reportparticle-particle interactions.5 Semiconductor particles can also be used, and recently the attachment of DNAto dendrimers or polypeptide nanoscale particles has been exploited for both sensing and drug delivery.6DNA Code DesignTo successfully self-assemble nucleic acid nanostructures by hybridization, the DNA sequences (often re-ferred to as DNA words) must be Òwell behavedÓ (i.e., they must not interact with incorrect sequences). The
creation of large sets of well behaved DNA molecules is important not only for DNA materials research byalso for large-scale DNA array analysis. An example of the work in this area is the DNA word design byProfessor Anne Condon at the University of British Columbia.7DNA and RNA Secondary StructureThe secondary structure of nucleic acid objects beyond simple DNA Watson-Crick duplex formation, whetherthey are simple single strands of RNA or the complex multiple junctions of Ned Seeman, have to be under-stood by a combination of experimental methods and computer modeling. The incorporation of nucleic acid
structures that include mismatches (e.g., bulges, hairpins) will most likely be an important piece of the self-assembly process of DNA nanoscale objects.8Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING295Given a set of tiles with the appropriate pads, any arbitrary pattern of tiles can be created. Simple,periodic patterns have been successfully fabricated and formed from a variety of different DNA tiles,140and large superstructures involving these systems and containing tens of thousands of tiles have been
observed. However, nonperiodic structures are more generally useful (e.g., for circuit layouts), and
larger tile sets with more complicated association rules are currently being developed for the assembly
of such patterns.The design of the pads is a critical element of DNA self-assembly. Since the sticky ends are com-posed of a sequence of bases, the set of different possible sticky ends is very large. However, there are
physical constraints that restrict the sequences chosen; pads and their complements should be suffi-
ciently different from other matched pairs, as to avoid unintended hybridization; they should avoid
palindromes, and so on.141 Most importantly, the entire set of pads must be designed so as to producethe desired overall assembly.The process of DNA self-assembly requires two steps: the first is the creation of the tiles, by mixinginput strands of DNA together; then, the tiles are placed in solution and the temperature is lowered
slowly until the tilesÕ pads connect and the overall structure takes form. This process of annealing can
take from several seconds to hours.Multistrand DNA Nanostructures and ArraysThe creation of three-dimensional objects with multistrand DNA structures has been pursued for many yearsby researchers such as Ned Seeman at New York University. Computer scientists such as Erik Winfree at theCalifornia Institute of Technology and John Reif at Duke University have been using the assembly of thesenanostructures to create mosaics and tile arrays on surfaces. The application of computer science concepts to
ÒprogramÓ the self-assembly of materials is the eventual goal. Since single-stranded RNA forms many biolog-ically functional structures, researchers are also pursuing the use of RNA as well as DNA for these self-assembling systems.91A.C. Pease, D. Solas, E.J. Sullivan, M.T. Cronin, C.P. Holmes, and S.P.A. Fodor, ÒLight-generated Oligonucleotide Arrays for Rapid DNASequence Analysis,Ó Proceedings of the National Academy of Sciences 91(11):5022-5026, 1994.2See http://www.affymetrix.com and http://www.nanogen.com.3See http://www.caliper.com; and http://www.alcara.com.4A.G. Frutos, A.E. Condon, L.M. Smith, and R.M. Corn, ÒEnzymatic Ligation Reactions of DNA ÔWordsÕ on Surfaces for DNA Computing,ÓJournal of the American Chemical Society 120 (40):10277-10282, 1998. Also, Q. Liu, L. Wang. A.G. Frutos, A.E. Condon, R.M. Corn, andL.M. Smith, ÒDNA Computing on Surfaces,Ó Nature 403:175-179, 2000.5C.A. Mirkin, R.L. Letsinger, R.C. Mucic, and J.J. Storhoff, ÒA DNA-based Method for Rationally Assembling Nanoparticles into Macro-scopic Materials,Ó Nature 382(6592):607-609, 1996; T.A. Taton, C.A. Mirkin, and R.L. Letsinger, ÒScanometric DNA Array Detection withNanoparticle Probes,Ó Science 289(5485):1757-1760, 2000.6F. Zeng and S.C. Zimmerman, ÒDendrimers in Supramolecular Chemistry: From Molecular Recognition to Self-Assembly,Ó ChemicalReview 97(5):1681-1713, 1997; M.S. Shchepinov, K.U. Mir, J.K. Elder, M.D. Frank-Kamenetskii, and E.M. Southern, ÒOligonucleotideDendrimers: Stable Nano-structures,Ó Nucleic Acids Research 27(15):3035-3041, 1999.7A. Maranthe, A.E. Condon, and R.M. Corn, ÒOn Combinatorial Word Design,Ó DIMACS Series in Discrete Mathematics and TheoreticalComputer Science 54:75-90, 2000.8C. Mao, T. LaBean, J.H. Reif, and N.C. Seeman, ÒLogical Computation Using Algorithmic Self-Assembly of DNA Triple CrossoverMolecules,Ó Nature 407(6803):493-496, 2000.9E. Winfree, F. Liu, L.A. Wenzler, and N.C. Seeman, ÒDesign and Self-Assembly of Two-Dimensional DNA Crystals,Ó Nature394(6693):539-544, 1998.140C. Mao, ÒThe Emergence of Complexity: Lessons from DNA,Ó PLoS Biology 2(12):e431, 2004, available at http://www.plosbiology.org/archive/1545-7885/2/12/pdf/10.1371_journal.pbio.0020431-S.pdf.141T.H. LaBean, ÒIntroduction to Self-Assembling DNA Nanostructures for Computation and Nanofabrication,Ó Computa-tional Biology and Genome Informatics, J.T.L. Wang et al., eds., World Scientific, Singapore, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.296CATALYZING INQUIRYOnce the structure is completed, a number of methods can be used to obtain the output if necessary.The first is to image the resulting structure, for example, with an atomic force microscope or transmis-
sion electron microscope. In some cases, the structure by itself is visible; in others, tiles can be made
distinguishable by reflectivity or the presence of extra atoms such as gold or fluorescents possibly
added to a turn of the strand that extends out of the plane. Second, with the use of certain tiles, a
ÒreporterÓ strand of DNA can be included in such a way that when all the tiles are connected, the single
reporter strand winds through all of them. Once the tiling structure completes assembly, that strand can
then be isolated and sequenced by PCR or another technique to determine the ordering of the tiles.8.4.3.2  Applications
DNA self-assembly has a wide range of potential applications, drawing on its ability to createarbitrary, programmable structures. Self-assembled structures can encode data (especially array data
such as images); act as a layout foundation for nanoscale structures such as circuits; work as part of a
molecular machine; and perform computations.Since a tiled assembly can be programmed to form in an arbitrary pattern, it is potentially a usefulway to store data or designs. In one dimension, this can be accomplished by synthesizing a sequence of
DNA bases that encode the data; then, in the self-assembly step, tiles join to the input strand, extending
the encoding into the second dimension. This two-dimensional striped assembly can be inspected
visually using microscopy, enabling a useful way to read out data. To store two-dimensional data, the
input strand is designed with a number of hairpin turns so that the strand weaves across every other
line of the assembly; the tiles then attach between adjacent turns of the input strand. The resulting
assembly can encode any two-dimensional pattern, and in principle this approach could be extended to
three dimensions.This approach can also be used to create a foundation for nanometer-scale electronic circuits. Forthis application, the DNA tiles would contain some extra materials, such as tiny gold beads, possibly in
a strand fragment that extended above the plain of the tile. After the tiles have formed the desired
configuration, chemical deposition would be used to coat the gold beads, increasing their size, until
they merge and form a wire. Box 8.5 describes a fantasy regarding a potential application to circuit
fabrication.DNA has been used as a scaffold for the fabrication of nanoscale devices.142 In crystalline form,DNA has enabled the precise and closely spaced placement of gold nanoparticles (at distances of 10-20
angstroms). Gold nanoparticles might function as a single-electron storage device for one bit, and other
nanoparticles might be able to hold information as well (e.g., in the form of electric charge or spin). At
one bit per nanoparticle, the information density would be on the order of 1013 to 1014 bits per squarecentimeter.Computation through self-assembly is an attractive alternative to traditional exhaustive search DNAcomputation. Although traditional DNA computation, such as performed by Adleman, required a linear
number of steps with the input size, in algorithmic self-assembly, the computation occurs in a single step.
In current experiments with self-assembly, a series of tiles are provided as input, and computation tiles
and output tiles form into position around the input. For example, in an experiment that used DNA tiles
to calculate cumulative XOR, input tiles represented the Boolean values of four inputs, while output tiles,
designed such that a tile representing the value 0 would connect to two identical inputs, and a tile
representing the value of 1 would connect to two dissimilar inputs, formed alongside the input tiles. Then,
the reporter strand is ligated, extracted, and amplified to read out the answer.143142S. Xiao, F. Liu, A.E. Rosen, J.F. Hainfeld, N.C. Seeman, K. Musier-Forsyth, and R.A. Kiehl, ÒAssembly of NanoparticleArrays by DNA Scaffolding,Ó Journal of Nanoparticle Research 4:313-317, 2002.143C. Mao, T.H. LaBean, J.H. Reif, and N.C. Seeman, ÒLogical Computation Using Algorithmic Self-assembly of DNA Triple-crossover Molecules,Ó Nature 407:493-496, 2000.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.BIOLOGICAL INSPIRATION FOR COMPUTING297This approach has two main drawbacks: the speed of individual assemblies, and the error rate.First, the DNA reactions can take minutes or hours, and so any individual computation by self-assem-
bly will likely be substantially slower than using a traditional computer. The potential for self-assembly
is that, like exhaustive DNA computation, it can occur in parallel, with a parallelism factor as high as
1018. In the XOR experiment, researchers observed an error rate of 2 to 5 percent. Certainly, this rate maybe lowered as experience is gained in designing laboratory procedures and assembly methods; how-
ever, the error rate is likely to remain higher than that for electronic computers. For certain classes of
problems, an ultraparallel though unreliable approach may be an effective way to compute a solution.8.4.3.3  Prospects
So far, DNA self-assembly has been demonstrated successfully in the laboratory, constructing rela-tively simple patterns (e.g., alternating bands, or the encoding of a binary string) that are visible through

microscopy. It has also been used successfully for simple computations such as counting, XOR, and
addition.Moving forward, laboratory techniques must improve in sophistication to handle the more complexassemblies and reactions that will accompany large-scale computations or designs. Along with progress
in the lab, further theoretical developments are possible in developing algorithms for constructing
arbitrary aperiodic patterns.Although so far DNA self-assembly has used only naturally occurring variants of DNA, a possibleimprovement is to employ alternative chemistries, such as peptide nucleic acid, an artificial form of
DNA in which the backbone has peptide links in place of the phosphate that occurs in natural DNA.Box 8.5A Fantasy of Circuit FabricationConsider:. . . a fantasy of nanoscale circuit fabrication in a future technology. Imagine a family of primitive molecular-electronic components, such as conductors, diodes, and switches, is available from generic parts suppliers.
Perhaps we have bottles of these common components in the freezer. . . . Suppose we have a circuit to imple-ment. The first stage of the construction begins with the circuit and builds a layout incorporating the sizes of thecomponents and the ways they might interact. Next, the layout is analyzed to determine how to construct a
scaffold. Each branch is compiled into a collagen strut that links only to its selected targets. The struts are labeledso that they bind only to the appropriate electrical component molecules. For each strut, the DNA sequence tomake that kind of strut is assembled, and a protocol is produced to insert the DNA into an appropriate cell. These
various custom parts are then synthesized by the transformed cells.Finally, we create an appropriate mixture of these custom scaffold parts and generic electrical parts. Speciallyprogrammed worker cells are added to the mixture to implement the circuit edifice we want. The worker cellshave complex programs, developed through amorphous computing technology. The programs control how theworkers perform their particular task of assembling the appropriate components in the appropriate patterns. With
a bit of sugar (to pay for their labor), the workers construct copies of our circuit we then collect, test, and packagefor use.SOURCE: H. Abelson, R. Weiss, D. Allen, D. Coore, C. Hanson, G. Homsy, T.F. Knight, Jr., et al., ÒAmorphous Computing,Ó Commu-nications of the ACM 43(5):74-82, 2000.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.298CATALYZING INQUIRYAlso, a wide variety of potential geometries exists for crossover tiles. There have been experiments witha so-called 4 × 4 tile, where the sticky ends extend at right angles.DNA also has the property that its length scale can bridge the gap between molecular systems andmicroelectronics components. If the issues of surface attachment chemistry, secondary structure, and
self-assembly can be worked out, hybrid DNA-silicon nanostructures may be feasible, and a DNA-
controlled field effect transistor is one possible choice for a first structure to fabricate. Some other
specific near-term objectives for research in DNA self-assembly include the creation of highly regular
DNA nanoparticles and the creation of programmable DNA self-assembling systems. For the cell regu-
latory systems and enzymatic pathways, some specific near-term objectives include the creation of sets
of coupled protein-DNA interactions or genes, the simulation and emulation of kinase phosphor-relay
systems, and the creation of networks of interconnecting nanostructures with unique enzyme commu-
nication paths.To be adopted successfully as an industrial technology, however, DNA self-assembly faces chal-lenges similar to solution-based exhaustive search DNA computing: a high error rate, the need to run
new laboratory procedures for each computation, and the increasing capability of non-DNA technolo-
gies to operate at nanoscales. For example, while it is likely true that current lithography technology has
limits, various improvements already demonstrated in laboratories such as extreme ultraviolet lithogra-
phy, halo implants, and laser-assisted direct imprint techniques can achieve feature sizes of 10 nm,
comparable to a single DNA tile. Some other targets might be the ability to fabricate biopolymers such
as oligonucleotides and polypeptides as long as 10,000 bases for the creation of molecular control
systems and the creation of biochemical and hybrid biomolecular-inorganic systems that can be self-
assembled into larger nanoscale objects in a programmable fashion.8.4.3.4  Hybrid Systems
A hybrid system is one that is assembled from both biological and nonbiological parts. Hybridsystems have many applications, including biosensors, measurement devices, mechanisms, and pros-
thetic devices.Biological sensors, or biosensors, probe the environment for specific molecules or targets throughchemical, biochemical, or biological assays. Such devices consist of a biological detection element at-
tuned to the target and a transduction mechanism to translate a detection event into a quantifiable
electronic or optical signal for analysis. For example, antennae from a living silkworm moth have been
used as an olfactory sensor connected to a robot.144 Such antennae are much more sensitive thanartificial gas sensors, in this case to moth pheromones. A mobile robot, so equipped, has been shown to
be able to follow a pheromone plume much as a male silkworm moth does. When a silkworm mothÕs
antennae are stimulated by the presence of pheromones, the mothÕs nervous system activities alternate
between active and inactive states in a pattern consistent with the activity pattern of neck motor neu-
rons that guide the mothÕs direction of motion. In the robot, the silkworm mothÕs antennae are con-
nected to an electrical interface, and a signal generated by the right (left) antenna results in a Òturn
rightÓ (Òturn leftÓ) command. This suggests that such signals may play an important role in controlling
the pheromone-oriented zigzag walking of a silkworm moth.144Y. Kuwana et al., ÒSynthesis of the Pheromone-oriented Behaviour of Silkworm Moths by a Mobile Robot with MothAntennae as Pheromone Sensors,Ó Biosensors and Bioelectronics 14:195-202, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY2992999Illustrative Problem Domains at the
Interface of Computing and Biology9.1  WHY PROBLEM-FOCUSED RESEARCH?
Problems offered by nature do not respect disciplinary boundaries. That is, nature does not packagea problem as a ÒbiologyÓ problem, a ÒcomputingÓ problem, or a ÒphysicsÓ problem. Many disciplines
may have helpful insights to offer or useful techniques to apply to a given problem, and to the extent
that problem-focused research can bring together practitioners of different disciplines to work on shared
problems, this can only be a good thing.This chapter describes problem domains in which the expenditure of serious intellectual effort canreasonably be expected to generate (or to require!) significant new knowledge in biology and/or com-
puting. Biological insight could take different formsÑthe ability to make new predictions, the under-
standing of some biological mechanism, the construction of a new biological organism. The same is true
for computingÑinsight might take the form of a new biologically inspired approach to some computing
problem, different hardware, or novel architecture. It is important to note that these domains contain
very difficult problemsÑand it is unrealistic to expect major progress in a short time.Challenge problems can often be found in interesting problem domains. A Òchallenge problemÓ isa scientific challenge focused on a particular intellectual goal or application (Box 9.1). Such problems
have a long history of stimulating important research efforts, and a list of Ògrand challengesÓ in compu-
tational biology originating with David Searls, senior vice president of Worldwide Bioinformatics for
GlaxoSmithKline, includes protein structure prediction, homology search, multiple alignment and phy-
logeny construction, genomic sequence analysis, and gene finding.1 Appendix B provides a sampling ofgrand challenge problems found in other reports and from other life scientists.The remainder of this chapter illustrates problem domains that display the intertwined themes ofunderstanding biological complexity and enabling a novel generation of computing and information
science. It incorporates many of the dimensions of the basic knowledge sought by each field and
discusses some of the technical and biological hurdles that must be overcome to make progress. How-
ever, no claim whatsoever is made that these problems exhaust the possible interesting or legitimate
domains at the BioComp interface.1D.B. Searls, ÒGrand Challenges in Computational Biology,Ó Computational Methods in Molecular Biology, S.L. Salzberg, D.Searls, and S. Kasif, eds., Elsevier Science, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.300CATALYZING INQUIRY9.2  CELLULAR AND ORGANISMAL MODELING
2A living cell is a remarkable package of biological molecules engaged in an elaborate and robustchoreography of biological functions. Currently, however, we have very incomplete knowledge about
all of the components that make up cells and how these components interact to perform those functions.
Understanding how cells work is one of biologyÕs grand challenges. If it were possible to understand
more completely how at least some of the machinery of cells works, it might be possible to anticipate the
onset and effects of disease and create therapies to ameliorate those effects. If it were possible to
influence precisely the metabolic operations of cells, they might be usable as highly controllable facto-
ries for the production of a variety of useful organic compounds.However, cell biology is awash in data on cellular components and their interactions. Althoughsuch data are necessary starting points for an understanding of cellular behavior that is sufficient for
prediction, control, and redesign, making sense out of the data is difficult. For example, diagrams
tracing all of the interactions, activities, locations, and expression times of the proteins, metabolites, and
nucleic acids involved have become so dense with lines and annotations that reasoning about their
functions has become almost impossible.Box 9.1On Challenge ProblemsChallenge problems have a history of stimulating scientific progress. For example:¥The U.S. High Performance Computing and Communications Program focused on problems in appliedfluid dynamics, meso- to macroscale environmental modeling, ecosystem simulations, biomedical imaging
and biomechanics, molecular biology, molecular design and process optimization, and cognition.1 Theseproblem domains were selected because they drove applications needs for very high-performance computing.¥A second example is the Text REtrieval Conference (TREC), sponsored by the National Institute of Standardsand Technology, in cooperation with the National Security Agency and the Defense Advanced Research ProjectsAgency. The purpose of this conference is to Òsupport research within the information retrieval community byproviding the infrastructure necessary for large-scale evaluation of text retrieval methodologies. . . . The TREC
workshop series has the following goals: to encourage research in information retrieval based on large testcollections; to increase communication among industry, academia, and government by creating an open forumfor the exchange of research ideas; to speed the transfer of technology from research labs into commercial
products by demonstrating substantial improvements in retrieval methodologies on real-world problems; and toincrease the availability of appropriate evaluation techniques for use by industry and academia, including devel-opment of new evaluation techniques more applicable to current systems.Ó2 TREC operates by presenting aproblem in text retrieval clearly and opening it up to all takers. It makes available to the community at large allbasic tools, and its structure and organization have attracted a large number of research sites.¥Still another approach to challenge problems is to offer prizes for the accomplishment of certain well-specified tasks. For example, in aeronautics, the Kremer Prize was established in 1959 for the first human-powered flight over a specific course; this prize was awarded to Paul MacReady for the flight of the GossamerCondor in 1977. The Kremer Prize is widely regarded as having stimulated a good deal of innovative researchin human-powered flight. A similar approach was taken in cryptanalysis, in which nominal prizes were of-fered for the first parties to successfully decrypt certain coded messages. These prizes served to motivate thecryptanalytic community by providing considerable notoriety for the winners. On the other hand, pressures to
be the first to achieve a certain result often strongly inhibit cooperation, because sharing oneÕs own work mayeliminate the competitive advantage that one has over others.1See http://www.ccic.gov/pubs/blue96/index.html.2See http://trec.nist.gov/overview.html.2Section 9.2 is based largely on A.P. Arkin, ÒSynthetic Cell Biology,Ó Current Opinion in Biotechnology 12(6):638-644, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY301As noted in Section 5.4.2, cellular simulation efforts have for the most part addressed selectedaspects of cellular functionality. The grand challenge of cellular modeling and simulation is a high-
fidelity model of a cell that captures the interactions between the many different aspects of functional-
ity, where Òhigh fidelityÓ means the ability to make reasonably accurate and detailed predictions about
all interesting cellular behavior under the various environmental circumstances encountered in its life
cycle. Of course, a model perforce is an abstraction that omits certain aspects of the phenomenon it is
representing. But the key term in this description is ÒinterestingÓ behaviorÑbehavior that is interesting
to researchers. In this context, the model is intended to integrateÑas a real cell wouldÑdifferent aspects
of its functionality. Although the grand challenge may well be unachievable, almost by definition, the
goal of increasing degrees of integration of what is known and understood about various aspects of
cellular function remains something for which researchers strive.The development of a high-fidelity simulation of a cellÑeven the simplest cellÑis an enormousintellectual challenge. Indeed, even computational models that are very well developed, such as models
of neural and cardiac electrophysiology, often fail miserably when they are exercised beyond the data
that have been used to construct them. Yet if a truly high-fidelity simulation could be developed, the
ability to predict cellular response across a wide range of environmental conditions using a single modelwould imply an understanding of cellular function far beyond what is available today, or even in the
immediate future, and would be a tangible and crowning achievement in science. And, of course, the
scientific journey to such an achievement would have many intermediate payoffs, in terms of tools and
insights relevant to various aspects of cellular function. From a practical standpoint, such a simulation
would be an invaluable aid to medicine and would provide a testbed for biological scientists and
engineers to explore techniques of cellular control that might be exploited for human purposes.An intermediate step toward the high-fidelity simulation of a real cell would be a model of a simplehypothetical cell endowed with specific properties of real cells. This model would necessarily include
representations of several key elements (Box 9.2). The hundreds of molecules and hundreds of thou-
sands of interactions required do not appear computationally daunting, until it is realized that the time
scale of molecular interaction is on the order of femtoseconds, and interesting time scales of cellular
response may well be hours or days.The challenges fall into three general categories:¥Mechanistic understanding.  High-fidelity simulations will require a much more profound physi-
cal understanding of basic biological entities at multiple levels of detail than is available today. (For
example, it is not known how RNA polymerase actually moves along a DNA strand or what rates of
binding or unbinding occur in vivo.) An understanding of how these entities interact inside the cell is
equally critical. Mechanistic understanding would be greatly facilitated by the development of new
mathematical formalisms that would enable the logical parsing of large networks into small modulesBox 9.2Elements of a Hypothetical Cell¥An outside and inside separated by some coat or membrane (e.g., lipid)¥One or more internal compartments inside the cell¥Genes and an internal code for regulation of function¥An energy supply to keep the cell ÒaliveÓ or ÒworkingÓ¥Reproductive capability¥At least hundreds of biologically significant molecules, with potentially hundreds of thousands of interac-tions between them
¥Responsiveness to environmental conditions that affect the internal operation and behavior of the cell (e.g.,changes in temperature, acidity, salinity)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.302CATALYZING INQUIRYwhose behavior can be analyzed. Such modules would be building blocks that researchers could use tobuild functionality, understand controllable aspects, and identify points of failure.¥Data acquisition.  Simulation models are data-intensive, and today there are relatively few sys-
tems with enough quality data to create highly detailed models of cellular function. It will be important
to develop ways of measuring many more aspects of internal cellular state, and in particular, new
techniques for measuring rates of processes and biochemical reactions in situ in living cells will be
necessary. Besides additional reporter molecules, selective fluorescent dyes, and so on, a particular need
is to develop good ways of tracking cellular state at different points in time, so that cellular dynamics
can be better understood. Large volumes of data on reaction rates will also be necessary to model many
cellular processes.¥Integrative tools.  Because cellular function is so complex, researchers have used a variety of data
collection techniques. Data from multiple sourcesÑmicroarrays, protein mass spectroscopy, capillary
and high-pressure chromatographies, high-end fluorescence microscopy, and so onÑwill have to be
integratedÑand are indeed requiredÑif validated, high-fidelity cellular models are to be built. More-
over, because existing models and simulations relevant to a given cell span multiple levels of organiza-
tional hierarchy (temporal, spatial, etc.), tools are necessary to facilitate their integration. With such
tools at the researcherÕs disposal, it will be possible to develop complex models rapidly, assembling
molecular components into modules, linking modules, computing dynamic interactions, and compar-
ing predictions to data.Finally, despite the power of cellular modeling and simulation to advance understanding, modelsshould not be regarded as an end product in and of themselves. Because all models are unfaithful to the
phenomena they represent in some way, models should be regarded as tools to gain insight and to be
used in continual refinement of our understanding, rather than as accurate representations of real
systems, and model predictions should be taken as promising hypotheses that will require experimental
validation if they are to be accepted as reliable.The discussion above suggests that many researchers will have to collaborate in the search for anintegrated understanding. Such coordinated marshaling of researchers and resources toward a shared
goal is a common model for industry, but this multi-investigator approach is new for the academic
environment. Large government-funded projects such as the Alliance for Cellular Signaling (discussed
in Chapter 4) or private organizations like the Institute for Systems Biology3 are the new great experi-ments in bringing a cooperative approach to academic biology.Still more ambitiousÑprobably by an order of magnitude or moreÑis the notion of simulating thebehavior of a multicelled organism. For example, Harel proposes to develop a model of the Caenorhabditiselegans nematode, an organism that is well characterized with respect to its anatomy and genetics.4Harel describes the challenge as one of constructing Òa full, true-to-all-known-facts, 4-dimensional,fully animated model of the development and behavior of this worm. . . , which is easily extendable as
new biological facts are discovered.ÓIn HarelÕs view, the feasibility of such a model is based on the notion that the complexity ofbiological systems stems from their high reactivity (i.e., they are highly concurrent and time-intensive,
exhibit hybrid behavior that is predominantly discrete in nature but with continuous aspects as well,
and consist of many interacting, often distributed, components). The structure of a reactive system may
itself be dynamic, with its components being repeatedly created and destroyed during the systemÕs life
span. Harel notes:3See http://www.systemsbiology.org/home.html.4D. Harel, ÒA Grand Challenge for Computing: Towards Full Reactive Modeling of a Multi-Cellular Animal,Ó European Asso-ciation for Theoretical Computer Science (EATCS) Bulletin, 2003, available at http://www.wisdom.weizmann.ac.il/~dharel/papers/GrandChallenge.doc.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY303[B]iological systems exhibit the characteristics of reactive systems remarkably, and on many levels; fromthe molecular, via the cellular, and all the way up to organs, full organisms, and even entire populations.
It doesnÕt take much to observe within such systems the heavy concurrency, the event-driven discretenature of the behavior, the chain-reactions and cause-effect phenomena, the time-dependent patterns, etc.Harel concludes that biological systems can be modeled as reactive systems, using languages andtools developed by computer science for the construction of man-made reactive systems (briefly dis-
cussed in Section 5.3.4 and at greater length in the reference in footnote 4 of this chapter).If the Harel effort is successful, a model of C. elegans would result that is fully executable, flexible,interactive, comprehensive, and comprehensible. By realistically simulating the wormÕs development
and behavior, it would help researchers to uncover gaps, correct errors, suggest new experiments,
predict unobserved phenomena, and answer questions that cannot be addressed by standard laboratory
techniques alone. In addition, it would enable users to switch rapidly between levels of detail (from the
entire macroscopic behavior of the worm to the cellular and perhaps molecular levels). Most impor-
tantly, the model would be extensible, allowing biologists to enter new data themselves as they are
discovered and to test various hypotheses about aspects of behavior that are not yet known.9.3  A SYNTHETIC CELL WITH PHYSICAL FORM
The most ambitious goal of synthetic biology (Section 8.4.2) is the biochemical instantiation of arealÑif syntheticÑcell with the capability to grow and reproduce. Such an achievement would
necessarily be accompanied by new insights into the molecular dynamics of cells, the origins of life
on Earth, and the limits of biological life. In practical terms, such cells could be engineered to
perform specific functions, and thus could serve as a platform for innovative industrial and bio-
medical applications.Cellular modification has a long history ranging from the development of plasmids carrying bio-synthetic genes, or serving as Òengineering blanksÓ for production of new materials, to the creation of
small genetic circuits for the control of gene expression. However, the synthetic cells being imagined
today would differ from the original cell much more substantially than those that have resulted from
modifications to date. In principle, these cells need have no chemical or structural similarity to natural
cells. Since they will be designed, not evolved, they may contain functions or structures unachievable
through natural selection.Synthetic cells are a potentially powerful therapeutic tool that may be able to deliver drugs todamaged tissue to seek and destroy foreign cells (in infections), destroy malignant cells (in cancer),
remove obstructions (in cardiovascular disease), rebuild or correct defects (e.g., reattach severed nerves),
or replace parts of tissue that was injuredÑand doing so without affecting nonproblematic tissues,
while reducing the side effects of current conventional treatments.The applications of synthetic cells undertaking cell-level process control computing are not limitedto those of medicine and chemical sensing. There are also potential applications to the nanofabrication
of new and useful materials and structures. Indeed, natural biology exhibits propulsive rotors and
limbs at the microscale, and synthetic cells may be an enabling technology for nanofabricationÑthe
building of structures at the microscopic level. There may be other techniques to accomplish this, but
synthetic cells offer a promise of high efficiency through massively parallel reproduction. The gene
regulatory networks incorporated into synthetic cells allow for the simultaneous creation of multiple
oligonucleotide sequences in a programmable fashion. Conversely, self-assembled DNA nanostructures
can potentially be used as control structures that interact with intracellular components and molecules.
Such control could enable the engineering construction of complex extracellular structures and precise
control of fabrication at the subnanometer level, which might in turn lead to the construction of complex
molecular-scale electronic structures (Section 8.4.3.2) and the creation of new biological materials, much
as natural biological materials result from natural biological processes.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.304CATALYZING INQUIRYConstructing these structures will require the ability to fabricate individual devices and theability to assemble these devices into a working system, since it is likely to be very difficult to
assemble a system directly from scratch. One approach to an assembly facility is to use a mostly
passive scaffold, consisting of selectively self-assembling molecules that can be used to support the

fabrication of molecular devices that are appropriately interconnected. Indeed, DNA molecules and
their attendant enzymes are capable of self-assembly. By exploiting that capability, it has been pos-
sible to create a number of designed nanostructures, such as tiles and latticed sheets. Although the
characteristics of these biomaterials need further exploration, postulated uses of them include as
scaffolds (for example, for the crystallization of macromolecules); as photonic materials with novel

properties; as designable zeolite-like materials for use as catalysts or as molecular sieves; and as
platforms for the assembly of molecular electronic components or biochips.5 Uses of DNA as amolecular ÒLegoÓ kit with which to design nanomachines, such as molecular tweezers and motors on
runways, are also under investigation.The relevance of synthetic cell engineering to nanofabrication is driven by the convergence ofdevelopments in several areas, including the miniaturization of biosensors and biochips into the na-
nometer-scale regime, the fabrication of nanoscale objects that can be placed in intracellular locations
for monitoring and modifying cell function, the replacement of silicon devices with nanoscale, molecu-
lar-based computational systems, and the application of biopolymers in the formation of novel
nanostructured materials with unique optical and selective transport properties. The highly predictable
hybridization chemistry of DNA, the ability to completely control the length and content of oligonucle-
otides, and the wealth of enzymes available for modification of DNA make nucleic acids an attractive
candidate for all of these applications.Furthermore, by designing and implementing synthetic cells, a much better understanding will begained of how real cells work, how they are regulated, and what limitations are inherent in their
machinery. Here, the discovery process is iterative, in that our understanding and observations of living
cells serve as ÒtruthingÓ mechanisms to inform and validate or refute the experimental constructs of
synthetic cells. In turn, the mechanisms underlying synthetic cells are likely to be more easily under-
stood than comparable ones in natural cells. Using this combined information, the behavior of biologi-
cal processes in living cells can slowly be unraveled. For such reasons, the process of creating synthetic
cells will spin off benefits to biology and science, just as the Human Genome Project led to dramatic
improvements in the technology of molecular biology.To proceed with the creation of synthetic cells, three separate but interrelated problems must beaddressed:¥The theoretical and quantitative problem of formulating, understanding, and perhaps even opti-mizing the design of a synthetic cell;¥The biological problem of applying lessons learned from real cells to such designs and usingsynthetic cells to inform our understanding of more complicated natural cells; and¥The engineering and chemistry problem of assembling the parts into a physical system (or todesign self-assembling pieces).One approach to building such a cell de novo is to start with a set of parts and assemble them intoa functional biomolecular machine. Conceiving a cell de novo means that cellular components and their
assembly are predetermined, and that the cell engineer has a quantifiable understanding of events and
outcomes that can be used to predict the behavior of the components and their interactions at least
probabilistically. A key aspect of de novo construction is that a de novo cellular design is not con-strained by evolutionary history and hence is much more transparent than cells found in nature. Be-5E. Winfree, F. Liu, L.A. Wenzler, and N.C. Seeman, ÒDesign and Self-Assembly of Two-Dimensional DNA Crystals,Ó Nature394(6693):539-544, 1998.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY305cause an engineered cell would be designed by human beings, the functions of its various elementswould be much better known. This fact implies that it would be easier to identify critical control points
in the system and to understand the rules by which the system operates.A second approach is to modify an existing living cell to give it new behaviors or to removeunwanted behaviors; classical metabolic engineering and natural product synthesis would be relevant
to this approach. One starting point would be to use the membrane of an existing cell, but modification
of these lipid bilayers to incorporate chemically inducible channels, integrated inorganic structures for
sensing and catalysis, and other biopolymer structures for the identification and modification of bio-
logical substrates will provide a greater degree of freedom in the manipulation of the chemical state of
the synthetic cell.A third approach is to abandon DNA-based cells. Szostak et al.6 argue that the Òstripping-downÓ ofa present-day bacterium to its minimum essential components still leaves hundreds of genes and
thousands of different proteins and other molecules. They suggest that synthetic cells could use RNA as
the repository of ÒgeneticÓ information and as enzymes that catalyze metabolism. In their view, the
most important requirements of a synthetic cell from a scientific standpoint are that it replicates autono-
mously and that it is subject to evolutionary forces. In this context, autonomous replication means
continued growth and division that depends only on the input of small molecules and energy, not on
the products of preexisting living systems such as protein enzymes. Evolution in this context means that
the structure is capable of producing different phenotypes that are subject to forces of natural selection,
although being subject to evolutionary forces has definite disadvantages from an engineering perspec-
tive seeking practical application of synthetic cells.The elements of a synthetic cell are likely to mirror those of simulations (see Box 9.2), except ofcourse that they will take physical representation. Inputs to the synthetic cell would take the form of
environmental sensitivities of various kinds that direct cellular function. (Another perspective on Òarti-
ficial cellsÓ similar to this reportÕs notion of synthetic cells is offered by Pohorille.7 In general, synthetic
cells share much with artificial cells, and the dividing line between them is both blurry and somewhat
arbitrary. The modal use of the term Òartificial cellÓ appears to refer to an entity with a liposome
membrane, whose physical dimensions are comparable to those of natural cells, that serves a function
such as enzyme delivery, drug delivery for cell therapy, and red blood cell substitutes.8) However, ifsynthetic cells are to be useful or controllable, it will be necessary to insert control points that can supply
external instructions or ÒreprogramÓ the cell for specialized tasks (e.g., a virus that injects DNA into the
cell to insert new pieces of code or instructions).Researchers are interested in expanding the size and complexity of pathways for synthetic cells thatwill do more interesting things. But there is little low-hanging fruit in this area, and todayÕs computa-
tional and mathematical ability to predict cellular behavior quantitatively is inadequate to do so, let
alone to select for the desired behavior. To bring about the development of synthetic cells from concept
to practical reality, numerous difficulties and obstacles must be overcome. Following is a list of major
challenges that have to be addressed:¥A framework for cellular simulation that can specify and model cellular function at different levels ofabstraction (as described in Section 9.2). Simulations will enable researchers to test their proposed de-signs, minimizing (though not eliminating) the need for in vivo construction and experimentation. Note
that the availability of such a framework implies that the data used to support it are also available to
assist in the engineering development of synthetic cells.6J.W. Szostak, D.P. Bartel, and P.L. Luisi, ÒSynthesizing Life,Ó Nature 409(6818):387-390, 2001.7A.Pohorille, ÒArtificial Cells: Prospects for Biotechnology,Ó 
Trends in Biotechnology20(3):123-128, 2002.8See, for example, T.M.S. Chang, ÒArtificial Cell Biotechnology for Medical Applications,Ó Blood Purification 18:91-96, 2000,available at http://www.medicine.mcgill.ca/artcell/isbp.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.306CATALYZING INQUIRY¥Stability and robustness in the face of varying environmental conditions and noise.  For example, it iswell known that nature provides a variety of redundant pathways for biological function, so that (for
example) the incapacitation of one gene is often not unduly disruptive to the cell.¥Improvement in the libraries of DNA-binding proteins and their matching repressor patterns.  These areat present inadequate, and good data about their kinetic constants are unavailable (hence signal transfer
characteristics cannot be predicted). Any specific combination of proteins might well interact outside
the genetic regulatory mechanisms involved, thus creating potentially undesirable side effects.¥Control point design and insertion.¥Data measurement and acquisition.  To facilitate the monitoring of a synthetic cellÕs behavior, it isdesirable to incorporate into the structure of the cell itself methods for measuring internal state param-
eters. Such measurements would be used to parameterize the functionality of cellular elements and
compare performance to specifications.¥Deeper understanding of biomolecular design rules.  Engineering of proteins for the modification ofbiointeractions will be required in all aspects of cell design, because it is relevant to membrane-based
receptors, protein effectors, and transcriptional cofactors. Today, metabolic engineers are frequently
frustrated in attempts to reengineer metabolic pathways for new functions because, at this point, the
Òdesign principlesÓ of natural cells are largely unknown. To design, fabricate, and prototype cellular
modules, it must be possible to engineer proteins that will bind to DNA and regulate gene expression.
Current examples of DNA binding proteins are zinc fingers, response regulators, and homeodomains.
The goal is to create flexible protein systems that can be modified to vary binding location and strength
and, ultimately, to insert these modules into living cells to change their function.¥A Òdevice-packingÓ design framework that allows the rapid design and synthesis of new networks insidecells.  This framework would facilitate designs that allow the reuse of parts and the rapid modificationof said parts for creating various ÒmodulesÓ (switches, ramps, filters, oscillators, etc.). The understand-
ing available today regarding how cells reproduce and metabolize is not sufficient to enable the inser-
tion of new mechanisms that interact with these functions in predictable and reliable ways.¥Tool suites to support the design, analysis, and construction of biologic circuits.  Such suites are as yetunavailable (but see Box 9.3).9.4  NEURAL INFORMATION PROCESSING AND NEURAL PROSTHETICS
Brain research is a grand challenge area for the coming decades. In essence, the goal of neuroscienceresearch is to understand how the interplay of structural dynamics, biochemical processes, and electri-Box 9.3Tool SuitesOne tool suite is a simulator and verifier for genetic digital circuits, called BioSPICE. The input to BioSPICE isthe specification of a network of gene expression systems (including the relevant protein products) and a smalllayout of cells on some medium. The simulator computes the time-domain behavior of concentration ofintracellular proteins and intercellular message-passing chemicals. (For more information, see http://
www.biospice.org.)A second tool would be a Òplasmid compilerÓ that takes a logic diagram and constructs plasmids to implementthe required logic in a way compatible with the metabolism of the target organism. Both the simulator and thecompiler must incorporate a database of biochemical mechanisms, their reaction kinetics, their diffusionrates, and their interactions with other biological mechanisms.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY307cal signals in nervous tissue gives rise to higher-order functions such as normal or abnormal thoughts,actions, memories, and behaviors. Experimental advances of the past decades have given the brain
researcher an increasingly powerful arsenal of tools to obtain dataÑfrom the level of molecules to
nervous systemsÑand to compare differences between individuals.Today, neuroscientists have begun the arduous process of adapting and assembling neurosciencedata at all scales of resolution and across disciplines into electronically accessible, distributed databases.
These information repositories will complement the vast structural and sequence databases created to
catalog, organize, and analyze gene sequences and protein products. Such databases have proven
enormously useful in bioinformatics research; whether equal rewards will accrue from similar efforts
for tissue-level data, whole-brain imaging, physiological data, and so forth remains to be seen, but
based on the successes of the molecular informatics activities and the challenge questions of the neuro-
scientist, big payoffs can be anticipated.At the very least, multiscale informatics efforts for brain research will provide organizing frame-works and computational tools to manage neuroscience data, from the lab notebook to published data.
An ideal and expected outcome will be the provisioning for new opportunities to integrate large amounts
of biological data into unified theories of function and aid in the discovery process.To provide some perspective on the problem, consider that animal brains are the information-processing systems of nature. A honeybeeÕs brain contains roughly 100 million synapses; a contempo-
rary computer contains roughly 100 million transistors. Given a history of inputs, both systems choose
from among a set of possible outputs. Yet although it is understood how a digital computer adds and
subtracts numbers and stores error-free data, it is not understood how a honeybee learns to find nectar-
rich flowers or to communicate with other honeybees.We do not expect a honeybee to perform numerical computations; likewise, we do not expect adigital computer to learn autonomously, at least not today. However, an interesting question is the
extent to which the structure of an information-processing system and the information representations
that it uses predispose the system to certain types of computation. Put another way, in what ways and
under what circumstances, if any, are neuronal circuits and neural information-processing systems
inherently superior to von Neumann architectures and Shannon information representations for adap-
tation and learning? Given the desirability of computers that can learn and adapt, an ability to answer
this question might provide some guidance in the engineering of such systems.Some things are known about neural information processing:¥Animal brains find good solutions to real-time problems in image and speech processing, motorcontrol, and learning. To perform these tasks, nervous systems must represent, store, and process
information. However, it is highly unlikely that neural information is represented in digital form.¥It is likely that neurons are the nervous systemÕs primary computing elements. A typical neuronis markedly unlike a typical logic gate; it possesses on average 10,000 synaptic inputs and a similar
number of outputs.¥The stored memory of a neural information-processing system is contained in the pattern andstrength of the analog synapses that connect it to other neurons. Nervous systems use vast numbers of
synapses to effect their computations: in neocortical tissue, the synapse density is roughly 3 × 108synapses per cubic millimeter.9 Specific memories are also known not to be localized to particularneurons or sets of neurons in the brain.109R. Douglas, ÒRules of Thumb for Neuronal Circuits in the Neocortex,Ó Notes for the Neuromorphic VLSI Workshop, Telluride,CO, 1994.10The essential reason is that specific memories are generally richly and densely connected to other memories, and hence canbe reconstructed through that web of connections.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.308CATALYZING INQUIRY¥The disparity between the information processing that can be done by digital computers and thatdone by nervous systems is likely to be a consequence of the different way in which nerve tissue
represents and processes information, although this representation is not understood.¥At the device level, nervous tissue operates on physical principles that are similar to those thatunderlie semiconductor electronics.11 Thus, differences between neural and silicon computation mustbe the result of differences in computational architecture and representation. It is thus the higher-level
organization underlying neural computation that is of interest and relevance. Note also that for the
purposes of understanding neural signaling or computation, a neuron-by-neuron simulation of nervous
tissue per se cannot be expected to reveal very much about the principles of organization, though it may
be necessary for the development of useful artifacts (e.g., neural prostheses).Some of the principles underlying neural computation are understood. For example, neurobiologyuses continuous adaptation rather than absolute precision in responding to analog inputs. The dynamic
range of the human visual system is roughly 10 decades in input light intensityÑabout 32 bits. But
biology doesnÕt process visual signals with 32-bit precision; rather, it uses a 7- or 8-bit instantaneous
dynamic range and adapts the visual pathwayÕs operating point based on the background light inten-
sity. Although this approach is similar to the automatic gain control used in electronic amplifiers,
biology takes the paradigm much farther: adaptation pervades every level of the visual system, rather
than being concentrated just at the front end.12There are essentially two complementary approaches toward gaining a greater understanding ofneural information processing. One approach is to reproduce physiological phenomena to increase our
understanding of the nervous system.13 A second approach is based on using a manageable subset ofneural properties to investigate emergent behavior in networks of neuron-like elements.14 Those favor-ing the first approach believe that these details are crucial to understanding the collective behavior of
the network and are developing probes that are increasingly able to include the relevant physiology.
Those favoring the second approach make the implicit assumption that reproducing many neurophysi-
ological details is secondary to understanding the collective behavior of nervous tissue, even while
acknowledging that only a detailed physiological investigation can reveal definitively whether the
details are in fact relevant.What can be accomplished by building silicon circuits modeled after biology? First, once the neu-ronal primitives are known, it will be possible to map them onto silicon. Once it is understood how
biological systems compute with these primitives, biologically based silicon computing will be possible.
Second, we can investigate how physical and technological limits, such as wire density and signal
delays and noise, constrain neuronal computation. Third, we can learn about alternative models of
computation. Biology demonstrates nondigital computing machines that are incredibly space- and
energy-efficient and that find adequate solutions to ill-posed problems naturally.11In both integrated circuits and nervous tissue, information is manipulated principally on the basis of charge conservation. Inthe former, electrons are in thermal equilibrium with their surroundings and their energies are Boltzmann distributed. In the
latter, ions are in thermal equilibrium with their surroundings and their energies also are Boltzmann distributed. In semiconduc-tor electronics, energy barriers are used to contain the electronic charge, by using the work function difference between siliconand silicon dioxide or the energy barrier in a pn junction. In nervous tissue, energy barriers are also erected to contain the ioniccharge, by using lipid membranes in an aqueous solution. In both systems, when the height of the energy barrier is modulated,the resulting current flow is an exponential function of the applied voltage, thus allowing devices that exhibit signal gain.Transistors use populations of electrons to change their channel conductance, in much the same way that neurons use popula-
tions of ionic channels to change their membrane conductance.12Adaptation helps to explain why some biological neural systems never settle downÑthey can be built so that when facedwith unchanging inputs, the inputs are adapted away. This phenomenon helps to explain many visual aftereffects. A stabilized
image on the retina disappears after a minute or so, and the whole visual field appears gray.13M.A. Mahowald and R.J. Douglas, ÒA Silicon Neuron,Ó Nature 354(6354):515-518, 1991.14J. Hertz, A. Krogh, and R.G. Palmer, Introduction to the Theory of Neural Computation, Addison-Wesley, Reading, MA, 1991.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY309The challenges of neural information processing fall into two primary categories: the semantics ofneural signaling and the development of neural prostheses. Signaling is the first challenge. It is known
that the spike trains of neurons carry information in some wayÑneurons that cannot ÒfireÓ are essen-
tially dead.15 Also, the physical phenomena that constitute ÒfiringÓ are knownÑelectrical spikes ofvarying amplitude and timing. However, the connections among these patterns of signaling in multiple
neurons to memories of specific events, motor control of muscles, sensory perception, or mental compu-
tation are entirely unknown. How do neurons integrate data from large numbers of multimodal sen-
sors? How do they deal with data overload? How do they decide a behavioral response from multiple
alternatives under severe and ill-posed constraints?TodayÕs neural instrumentation (e.g., positron emission tomography [PET] scans, functional magneticresonance imaging [fMRI]) can identify areas of the brain that are active under various circumstances, but
since the spatial resolution of these probes is wholly inadequate to resolve individual neuronal activity,16such instrumentation can provide only the roughest guidance about where researchers need to look for
more information about neuronal signaling, rather than anything specific about that information itself.
The primary challenge in this domain is the development of a formalism for neuronal signaling (most
likely a time-dependent one that takes kinetics into account), much like the Boolean algebra that provides
a computational formalism based on binary logic levels in the digital domain.A step toward a complete molecular model of neurotransmission for an entire cell is provided byMCell, briefly mentioned in Chapter 5. MCell is a simulation program that can model single synapses
and groups of synapses. To date, it been used to understand one aspect of biological signal transduc-
tion, namely the microphysiology of synaptic transmission. MCell simulations provide insights into the
behavior and variability of real systems comprising finite numbers of molecules interacting in spatially
complex environments. MCell incorporates high-resolution physical structure into models of ligand
diffusion and signaling, and thus can take into account the large complexity and diversity of neural
tissue at the subcellular level. It models the diffusion of individual ligand molecules used in neural
signaling using a Brownian dynamics random walk algorithm, and bulk solution rate constants are
converted into Monte Carlo probabilities so that the diffusing ligands can undergo stochastic chemical
interactions with individual binding sites, such as receptor proteins, enzymes, and transporters.17The second challenge is that of neural prosthetics. A neural prosthesis is a device that interfacesdirectly with neurons, receiving and transmitting signals that affect the function and activity of those
neurons, and that behaves in predictable and useful ways. Perhaps the ÒsimplestÓ neural prosthesis is
an artificial implant that can seamlessly replace nonfunctioning nerve tissue.Today, some measure of cognitive control of artificial limbs can be achieved through bionic brain-machine or peripheral-machine interfaces. William Craelius et al.18 have designed a prosthetic handthat offers amputees control of finger flexion using natural motor pathways, enabling them to under-
take slow typing and piano playing. The prosthetic hand is based on the use of natural tendon move-
ments in the forearm to actuate virtual finger movement. A volitional tendon movement within the
residual limb causes a slight displacement of air in foam sensors attached to the skin in that location,
and the resulting pressure differential is used to control a multifinger hand.15It is also known that not all neural signaling is carried by spikes. A phenomenon known as graded synaptic transmission alsocarries neural information and is based on a release of neurotransmitter at synaptic junctions whose volume is voltage depen-dent and continuous. Graded synaptic transmission appears to be much more common in invertebrates and sometimes exists
alongside spike-mediated signaling (as in the case of lobsters). The bandwidth of this analog channel is as much as five times thehighest rates measured in spiking neurons (see, for example, R.R. de Ruyter van Steveninck and S.B. Laughlin, ÒThe Rate ofInformation Transfer at Graded-Potential Synapses,Ó Nature 379:642-645, 1996), but the analog channel is likely to suffer a muchhigher susceptibility to noise than do spike-mediated communications.16The spatial resolution of neural instrumentation is on the order of 1 to 10 mm. See D. Purves et al., Neuroscience, SinauerAssociates Inc., Sunderland, MA, 1997. Given about 3 × 108 synapses per cubic millimeter, not much localization is possible.17See http://www.mcell.cnl.salk.edu/.18W. Craelius, R.L. Abboudi, and N.A. Newby, ÒControl of a Multi-finger Prosthetic Hand,Ó ICORR Õ99: International Conferenceon Rehabilitation Robotics, Stanford, CA, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.310CATALYZING INQUIRYA second example of a neural prosthesis is a retinal prosthesis intended to provide functionality whenthe retina of the eye is nonfunctional. In one variant, a light-sensitive microchip is implanted into the back
of the eye. Light striking the microchip (which has thousands of individual sensors) generates electrical
signals that travel through the optic nerve to the brain and are interpreted as an image.19 In anothervariant, the retina is bypassed entirely through the use of a camera mounted on a pair of eyeglasses to
capture and transmit a light image via a radio signal to a chip implanted near the ganglion cells, which
send nerve impulses to the brain.20 In a third variant, an implanted microfluidic chip that controls the flowof neurotransmitters translates digital images into neurochemical signals that provide meaningful visual
information to the brain. The microfluidic chip has a two-dimensional array of small controllable pores,
corresponding to pixels in an image. An image is created by the selective drip of neurotransmitters onto
specific bipolar cells, which are the cells that carry retinal information to the brain.21A third example of work in this area is that of Musallam et al., who have demonstrated the feasibil-ity of a neural interface that enables a monkey to control the movement of a cursor on a computer screen
by thinking about a goal the monkey would like to achieve and assigning a value to that goal.22 Theinteresting twist to this work is the reliance of signals from parts of the brain related to higher-order
(ÒcognitiveÓ) brain functions for movement planning for the control of a prosthetic device. (Previous
studies have relied on lower-level signals from the motor cortex.23)The advantage of using higher-level cognitive signals is that they capture information about themonkeyÕs goal (moving the cursor) and preferences (the destination on the screen the monkey wants).
Musallam et al. point out that once the signals associated with the subjectÕs goals are decoded, a smart
external device can perform the lower-level computations necessary to achieve the goals. For example,
a smart robotic arm would be able to understand what the intended goal of an arm movement is and
then computeÑon its ownÑthe trajectory needed to move the arm to that position. Furthermore, the
abstract nature of a cognitive command would allow it to be used for the control and operation of a
number of different devices. If higher-level signals associated with speech or emotion could be decoded,
it would become possible to record thoughts from speech areas (reducing the need for the use of
cumbersome letter boards and time-consuming spelling programs) or to provide online indications of a
patientÕs emotional state.A fourth example is provided by Theodore Berger of the University of Southern California, who isattempting to develop an artificial hippocampusÑa silicon implant that will behave neuronally in a
manner identical to the brain tissue that it replaces.24 The hippocampus is the part of the brain respon-sible for encoding experiences so that they can be stored as long-term memories elsewhere in the brain;
without the hippocampus, a person is unable to store new memories but can recall ones stored prior to
its loss. Because the manner in which the hippocampus stores information is unknown, BergerÕs ap-
proach is based on designing a chip that can provide the identical input-output response. The input-19N.S. Peachey and A.Y. Chow, ÒSubretinal Implantation of Semiconductor-based Photodiodes: Progress and Challenges,ÓJournal of Rehabilitation Research and Development 36(4):371-376, 1999.20W. Liu, E. McGucken, M. Clements, S.C. DeMarco, K. Vichienchom, C. Hughes, et al., ÒMultiple-Unit Artificial RetinaChipset System to Benefit the Visually Impaired,Ó to be published in IEEE Transactions on Rehabilitation Engineering. Available athttp://www.icat.ncsu.edu/projects/retina/files/MARC_system_paper.pdf.21B. Vastag, ÒFuture Eye Implants Focus on Neurotransmitters,Ó Journal of the American Medical Association 288(15):1833-1834,2002.22S. Musallam, B.D. Corneil, B. Greger, H. Scherberger, and R.A. Andersen, ÒCognitive Control Signals for Neural Prosthetics,ÓScience 305(5681):258-262, 2004. A Caltech press release of July 8, 2004, available at http://pr.caltech.edu/media/Press_Releases/PR12553.html, describes this work in more popular terms.23J. Wessberg, C.R. Stambaugh, J.D. Kralik, P.D. Beck, M. Laubach, J.K. Chapin, J. Kim, S.J. Biggs, M.A. Srinivasan, and M.A.L.Nicolelis, ÒReal-Time Prediction of Hand Trajectory by Ensembles of Cortical Neurons in Primates,Ó Nature 408(6810):361-365,2000. Similar work on rats is described in J.K. Chapin, K.A. Moxon, R.S. Markowitz, and M.A.L. Nicolelis, ÒReal-Time Control ofa Robot Arm Using Simultaneously Recorded Neurons in the Motor Cortex,Ó Nature Neuroscience 2(7):664-670, 1999.24R. Merritt, ÒNerves of Silicon: Neural Chips Eyed for Brain Repair,Ó EE Times, March 17, 2003 (10:37 a.m. EST), available athttp://www.eetimes.com/story/OEG20030317S0013.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY311output response of a hippocampal slice was determined by stimulating it with a random-signal genera-tor, and a mathematical model was developed to account for its response to these different stimuli. This
model is then the basis for the chip circuitry.By December 2003, Berger and his colleagues had completed the first test of using a microchipmodel to replace a portion of the hippocampal circuitry contained in a specific hippocampal brain slice.
In that slice is the major intrinsic circuitry of the hippocampus that consists of three major cell fields,
designated A, B, and C. Field A projects to and excites field B, which projects to and excites field C.
Berger et al. developed a predictive mathematical model of the signal transformations that field B
performs on the input signals that come from field A, and that field B then projects onto field C, and
implemented the model in a field-programmable gate array (FPGA) for field B. When field B was
surgically removed and the FPGA model of B was substituted, the result was that the output from area
C of the hippocampal slice remained unchanged in all meaningful respects. Next steps beyond this
work (e.g., developing circuitry that is less sensitive to the details of slice preparation, understanding
the hardware in terms of meaningful abstractions) remain to be realized.One result of such work may be the creation of building blocks that can be used to calculateuniversal mathematical functions and ultimately be the basis of families of devices for neural pattern
matching. Such building blocks may also serve as a point of departure for understanding neural func-
tions at a higher level of abstraction than is possible today.An analogy might be drawn to finding a mathematical representation of a particular dataset. Theapproach of mapping an exhaustive input-output response is similar to a curve-fitting process that
generates a function capable of reproducing the dataset perfectly. Knowledge of such a function does
not necessarily entail any understanding of the casual mechanisms underlying that dataset; thus, a
function resulting from a curve-fitting process is highly unlikely to be able to account for new data. Still,
developing such a function may be the first step toward such understanding.As suggested above, building a successful neural prosthetic implies some understanding of thesemantics of neural information processing: how the relevant nerve tissue stores and replicates and
processes information. However, it also requires a well-understood interface between a biological or-
ganism (e.g., a person) and the engineered device.One of the primary challenges in the area of neural interface design is the physical connection ofneurons to a chipÑthe right neurons must make connection with the right electrodes. The bodyÕs
natural response to an electrode implanted in living tissue is to wall it off with glial cells that prevent
neuron and electrode from making contact. One approach to solving this problem is to coat the elec-
trode with a substance that does not trigger the glial reaction. Another is to rely on the neural tissue to
reconfigure itself. Based on the knowledge that auditory nerves can reconfigure themselves to accom-
modate the signals emitted by cochlear implants, it may be possible to send out a signal that attracts the
right nerves to the right contacts.Prosthetic devices that restore or augment human physical abilities are increasingly sophisticated,and follow-on work will focus on enabling control of more complex actions by robotic arms and other
devices. On the other hand, although some early work on prostheses that help to replace cognitive
abilities has been successful, prostheses that improve cognitive abilities, by enhancing perception (su-
perhuman sense) and decision-making (superhuman computation or knowledge) capabilities, must at
present be regarded as being on the distant horizon.9.5  EVOLUTIONARY BIOLOGY
25Although the basic principles of evolution (natural selection and mutation) are understood in thelarge, both population genetics and phylogenetics have been radically transformed by the recent avail-25Section 9.5 is adapted largely from the Web page of John Huelsenbeck, University of California, San Diego, http://biology.ucsd.edu/faculty/huelsenbeck.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.312CATALYZING INQUIRYability of large quantities of molecular data. For example, in population genetics (the study of mutationsin populations), more molecular variability was found in the 1960s than had been expected, and this
finding stimulated KimuraÕs neutral theory of molecular evolution.26 Phylogenetics (the study of theevolutionary history of life) makes use of a variety of different kinds of data, of which DNA sequences
are the most important, as well as whole-genome, metabolic, morphological, geographical, and geologi-
cal data.27Evolutionary biology is founded on the concept that organisms share a common origin and havediverged through time. The details and timing of these divergencesÑthat is, the estimation or recon-
struction of an evolutionary historyÑare important for both intellectual and practical reasons, and
phylogenies are central to virtually all comparisons among species. From a practical standpoint,
phylogenetics has helped to trace routes of infectious disease transmission (e.g., dental transmission of
AIDS/HIV) and to identify new pathogens such as the New Mexico hantavirus. Moret (footnote 27)
notes that phylogenetic analysis is useful in elucidating functional relationships within living cells,
making functional predictions from sequence data banks of gene families, predicting ligands, develop-
ing vaccines, antimicrobials, and herbicides, and inferring secondary structure of RNAs. A clear picture
of how life evolved from its humble origins to its present diversity would answer the age-old question,
Where do we come from?There are many interesting phylogenetic problems. For example, consider the problem of estimat-ing large phylogenies, which is a central challenge in evolutionary biology. Given three species, there
are only three possible trees that could represent their phylogenetic history: (A,(B,C)); (B,(A,C)); and
(C,(A,B)). (The notation (A,(B,C)) means that B and C share a common ancestor, who itself shares a
different common ancestor with A. Thus, even if one picks a tree at random, there is a one in three
chance that the tree chosen will be correct. But the number of possible trees grows very rapidly with the
number of species involved. For a ÒsmallÓ phylogenetic problem involving 10 species, there are
34,459,425 possible trees. For a problem involving 22 species, the number of trees exceeds 1023. Today,most phylogenetic problems involve more than 80 species and some data sets contain more than 500
species. (For 500 species, there are approximately 1.0085 × 101280 possible trees, only one of which can becorrect.)  Of course, the grandest of all challenges in this area is the construction of the entire phylogeny

of all organisms on the planetÑthe complete ÒTree of LifeÓ involving some 107 to 108 species.Given the existence of such large state spaces, it is clear that exhaustive search for the single correctphylogenetic tree is not a feasible strategy, regardless of how fast computers become in the foreseeable
future. Researchers have developed a number of methods for coping with the size of these problems,
but many of these methods have serious deficiencies. For example, the optimality criteria used by these
methods often have dubious statistical justifications. In addition, many of these methods are simply
stepwise addition algorithms and make no effort to explore the space of trees. Methods with the best
statistical justification, such as maximum likelihood and Bayesian inference, are also the most difficult
to implement for large problems.Thus, the algorithmics of evolutionary biology are a fertile area for research. Moret (footnote 27)notes that reconstruction of the Tree of Life will require either the scaling-up of existing reconstruction
methods or the development of entirely new ones. He notes that sequence-based reconstruction meth-
odologies are available that are likely to scale effectively from 15,000 to 100,000 taxa, but that these
methodologies are not likely to scale to millions of taxa. Moret also points out that the use of gene-order
data (i.e., lists of genes in the order in which they occur along one or more chromosomes) can circum-
vent many of the difficulties associated with using sequence data. On the other hand, there are relatively26M. Kimura, ÒEvolutionary Rate at the Molecular Level,Ó Nature 217(129):624-626, 1968; Motoo Kimura, The Neutral Theory ofMolecular Evolution, Cambridge University Press, Cambridge, MA, 1983.27B.M.E. Moret, ÒComputational Challenges from the Tree of Life,Ó Proceedings of the 7th Workshop on Algorithm Engineering andExperiments, ALENEX Õ05, Vancouver, SIAM Press, Philadelphia, PA, 2005. This paper presents a number of computationalchallenges in evolutionary biology, of which only a few are mentioned in the subsequent discussion in this section.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY313few whole-genome data today, few models for the evolution of gene content and gene order, and a fargreater complexity of the mathematics for gene orders compared to that for DNA sequences.A related problem is that of comparing one or more features across species. The comparativemethod has provided much of the evidence for natural selection and is probably the most widely used
statistical method in evolutionary biology. But comparative analyses must account for phylogenetic
history, since the similarity in features common to multiple species that originate in a common evolu-
tionary history can inappropriately and seriously bias the analyses. A number of methods have been
developed to accommodate phylogenies in comparative analyses, but most of these methods assume
that the phylogeny is known without error. However, this is patently unrealistic, because almost all
phylogenies have a large degree of uncertainty. An important question is therefore to understand how
comparative analyses can be performed that accommodate phylogenetic history without depending on
any single phylogeny being correct.Still another interesting problem concerns the genetics of adaptationÑthe genomic changes thatoccur when an organism adapts to a new set of selection pressures in a new environment. Because the
process of adaptive change is difficult to study directly, there are many important and unanswered
questions regarding the genetics of adaptation. For example, how many mutations are involved in a
given adaptive change? Does this figure change when different organisms or different environments are
involved?  What is the distribution of fitness effects implied by these genetic changes during a bout of

adaptation?  How and to what extent are adaptations constrained by phylogenetic history? To what

extent are specific genetic changes inevitable given a change of selection pressures?9.6  COMPUTATIONAL ECOLOGY
28The long-term scientific goal of computational ecology is the development of methods to predict theresponse of ecosystems to changes in their physical, biological, and chemical components. Computa-
tional ecology seeks to combine realistic models of ecological systems with the often large datasets
available to aid in analyzing these systems, utilizing techniques of modern computational science to
manage the data, visualize model behavior, and statistically examine the complex dynamics that arise.29Questions raised immediately by computational ecology have a direct bearing on issues of important
policy significance todayÑpotential losses of biodiversity, achievement of sustainable futures, and
impact of global change on local communities.30The scientific questions to be addressed by computational ecology have both theoretical and ap-plied significance. These questions include the following:31¥How are communities organized in space and time?¥What factors maintain or reduce biodiversity?¥What are the implications for ecosystem function?¥How should biodiversity be measured?¥How is ecological robustness maintained?Consider, for example, ecological robustness. In ecological communities, many of the salient fea-tures remain unchanged, despite the fact that the identities of the relevant actors are continually in flux.28Much of the discussion in this section is based on J. Helly, T. Case, F. Davis, S. Levin, and W. Michener, eds., The State ofComputational Ecology, National Center for Ecological Analysis and Synthesis, Santa Barbara, CA, 1995, available at http://www.sdsc.edu/compeco_workshop/report/report.html.29J. Helly et al., eds., The State of Computational Ecology, National Center for Ecological Analysis and Synthesis, Santa Barbara,CA, 1995, available at http://www.sdsc.edu/compeco_workshop/report/report.html.30J. Lubchenco et al., ÒThe Sustainable Biosphere Initiative: An Ecological Research Agenda,Ó Ecology 72(2):371-412, 1991.31Much of this list is taken from Helly et al., The State of Computational Ecology, 1995.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.314CATALYZING INQUIRYSpecies richness, species abundance relations, and biogeochemical cycles exhibit remarkable regularity,despite changes at lower levels of organization. In marine systems, the Redfield ratios,32 which charac-terize the mean stoichiometry of plankton and of the water column, summarize the great constancy seen
in the concentration ratios of carbon, nitrogen, and phosphorus relative to each other, although absolute
levels vary considerably across the oceans. Similarly, Sheldon et al.33 observed that the size spectrum,from the smallest particles to large fish, follows a power law with a characteristic exponent, valid across
a range of trophic levels.Ecosystems and the biosphere are complex adaptive systems,34 in which macroscopic patternsemerge from interactions at lower levels of organization and feed back to influence dynamics on those
scales. Although macroscopic investigations, such as those of Carlson and Doyle,35 can shed consider-able light on designed or managed systems, or on organ systems that have been the direct products of
evolution, they provide at best a benchmark for comparisons for complex adaptive systems in which
selection acts well below the level of the whole system.The robustness of complex adaptive systems is dependent upon the same suite of characteristicsthat govern the robustness of any systemÑheterogeneity and diversity, redundancy and degeneracy,
modularity, and the tightness of feedback loops. Heterogeneity, for example, provides the adaptive
capacity that allows a system to persist in a changing environment; indeed, the robustness of the
macroscopic features of such systems may arise despite, in fact even because of, the lack of robustness
of their components. Yet these systems are neither designed nor selected for their macroscopic features.
How different then are such systems from those in which the level of selection is the whole system?
Should robustness be expected to emerge from the bottom up, and how does this self-organized robust-
ness differ from what would be optimal for the robustness of systems as a whole?Given that selection is most effective at much lower levels of organization, it is unclear whatsustains ecological robustness at the macroscopic level. A key problem is to understand the properties
of such self-organized, complex adaptive systemsÑto develop theories that facilitate scaling from indi-
viduals to whole systems and relating structure to function in order to identify signals warning of
collapse. What are the consequences of the erosion of biodiversity, the homogenization of systems, and
the breakdown of ecological barriers? How, indeed, will such changes affect the spread of disturbances,
from forest fires to novel infectious diseases? Addressing these questions will require iterative integra-
tion of computational approaches with explorations into large-scale stochastic and distributed dynami-
cal systems, with the goal of developing more parsimonious descriptors of essential aspects.General theory concerning the robustness of complex systems focuses on a few key features: hetero-geneity and diversity, redundancy and degeneracy, modularity, and the tightness of feedback loops.36Robustness is a design objective for most engineering applications, and investigations such as those of
Carlson and Doyle have demonstrated how one might select on complex systems as a whole to achieve
tolerance to particular classes of perturbations. One general principle that emerges from such studies is
that there are trade-offs between robustness on diverse scales. Systems in general may be characterized
as Òrobust, yet fragile.Ó That is, their robustness to one class of perturbations, or on one scale, may32A.C. Redfield, ÒOn the Proportions of Organic Derivatives in Sea Water and Their Relation to the Composition of Plankton,Ópp. 176-192 in James Johnstone Memorial Volume, R.J. Daniel, ed., University Press of Liverpool, Liverpool, UK, 1934.33R.W. Sheldon and T.R. Parsons, ÒA Continuous Size Spectrum for Particulate Matter in the Sea,Ó Journal of the FisheriesResearch Board of Canada 24:909-915, 1967; R.W. Sheldon, A. Prakash, and W.H. Sutcliffe, Jr., ÒThe Size Distribution of Particles inthe Ocean,Ó Limnological Oceanography 17:327-340, 1972.34S.A. Levin, Fragile Dominion: Complexity and the Commons, Perseus Books, Reading, MA, 1999; S.A. Levin, ÒComplex AdaptiveSystem: Exploring the Known, the Unknown and the Unknowable,Ó Bulletin of the American Mathematical Society 40:3-19, 2003.35J.M. Carlson and J. Doyle, ÒHighly Optimized Tolerance: Robustness and Design in Complex Systems,Ó Physical ReviewLetters 84(11):2529-2532, 2000.36S.A. Levin, Fragile Dominion: Complexity and the Commons, Perseus Books, Reading, MA, 1999; S.A. Levin, ÒComplex AdaptiveSystems; Exploring the Known, the Unknown and the Unknowable,Ó Bulletin of the American Mathematical Society 40:3-19, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY315necessarily lead to fragility to other classes of perturbations, or on other scales. Understanding suchtrade-offs is one dimension of considerable intellectual challenge and problem richness.These general points are instantiated in many different problem areas. Two illustrative areasÑeachimportant in its own rightÑinclude the dynamics of infectious diseases and the dynamics of marine
microbial systems. In the first case, increased computational resources have fostered the development of
models that relate individual behaviors to the spread of novel diseases, including smallpox and new
strains and subtypes of influenza. Such models have been given added stimulus by concerns about the
introduction and spread of infectious agents as weapons of bioterror, but the potential for new pandemics
of influenza and other infectious diseases is probably a greater motivation for their development.Marine microbial systems represent a vast and important storehouse of biodiversity, about whichmuch too little is known. Recent efforts, stimulated by the success of genomics, have directed attention
to characterizing the massive genetic diversity found in these systems. The computational challenges
are substantial, even to catalog the vast array of data being collected. Yet just as sequencing efforts in
genomics have highlighted the importance of knowing what the catalog of genetic detail reveals about
how systems function in their ecological environments, the mass of accumulating information about
marine microbial diversity spurs efforts at understanding how those marine ecosystems are organized
and what maintains the robustness of features such as microbial diversity.To address the scientific questions described above, researchers need techniques for dealing withsystems across scales of space, time, and organizational complexity. Ultimately, an essential enabling
tool will be a statistical mechanics of heterogeneous and nonindependent entities, in which the compo-
nents of a system of interest are continually changing through processes of mutation and other forms of
change.37  Such a system differs dramatically from systems that have traditionally been analyzedthrough the machinery of traditional statistical mechanics (e.g., systems composed of identical, inde-
pendently moving particles), and analytical methods for dealing with heterogeneous, nonindependent
entities are generally very sophisticated. In general, such methods rely on the ability to capture the
heterogeneity of the distribution (e.g., of traits) in terms of a small number of moments or other descrip-
tors or rely on Òequation-freeÓ approaches38 that finesse the need for explicit closures. In the absence ofsuch an analytical characterization, computation is generally the only alternative to gaining insights
about ensemble behavior, although computation may often provide analytical insights (and vice versa).Today, computational ecology makes use of continuum and individual descriptions. Continuummodeling focuses on the impact on local ecological communities of large-scale (global) influences such
as climate and fluxes of key elements such as carbon and nitrogen. These models are typically character-
ized by parameterized partial differential equations that represent appropriately averaged continuum
quantities of ecological significance (e.g., density of a species). A central intellectual challenge of the top-
down approach is reconciling the hundred-kilometer resolution of models that predict global climate
change and elemental fluxes with the meter and centimeter scales of interest in natural and managed
ecosystems.The ab initio formulation of realistic continuum models is difficult, because the details of theunderlying populations and entities matter a great deal. For example, naŁve assumptions of indepen-
dence, random motion, zero mixing time, or infinite propagation speed, which are often used in the ab
initio formulation of continuum models, simply do not hold at the underlying individual level.39Accordingly, great care must be taken to derive a continuum description from knowledge of the indi-
vidual elements in play.37S. Levin, Mathematics and Biology: The Interface, Lawrence Berkeley Laboratory Pub-701, Berkeley, CA, 1992, available athttp://www.bio.vu.nl/nvtb/Interface.html.38C. Theodoropolous, Y. Quan, and I.G. Kevrekidis, ÒCoarse Stability and Bifurcation Analysis Using Time-Steppers: A Reac-tion-Diffusion Example,Ó Proceedings of the National Academy of Sciences 97(18):9840-9843, 2000.39S.A. Levin, ÒComplex Adaptive Systems: Exploring the Known, the Unknown and the Unknowable,Ó Bulletin (New Series) ofthe American Mathematical Society 40(1):3-19, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.316CATALYZING INQUIRYIndividual-based modeling seeks to extrapolate from the level of effects on individual plants andanimals to changes in community-level patterns, which are necessarily characterized by longer time
scales and broader space scales than those of individuals. Individual-based models, an ecological form
of agent-based models, are rule-based approaches that can track the growth, movement, and reproduc-
tion of many thousands of individuals across the landscape40 and, in looking at the global consequencesof local interactions of individuals, are particularly well suited to address questions that relate to spatial
heterogeneities (e.g., ecological sanctuaries).In individual-based models, the inherent parallelism of ecological systemsÑthat organisms interactconcurrently across spaceÑis manifest.41 (By contrast, the parallelism in many computational modelsof other biological systems such as genomes and proteins is primarily a speedup mechanism for compu-
tation-intensive problems.) Individual-based models have been used to represent populations of preda-
tors, trees, and endangered species, and they are very useful in understanding the detailed response of
the population of interest to alternative environmental circumstances.In general, individual-based models are powerful tools for investigating systems that are analyti-cally intractable, and they provide opportunities for the consideration of various scenarios and for
exploring ecosystem management protocols that would not otherwise be possible. Nevertheless, such
simulations often contain too many degrees of freedom to allow robust prediction. Thus, efforts to
develop macroscopic representations that reduce dimensionality and that suppress irrelevant detail are
essentialÑa point that reinforces the desirability of developing an appropriate statistical mechanics as
described above.Individual-based modeling is generally computation-intensive, for two reasons. The first is that amultitude of individuals must be represented, the behavior of each must be computed, and the entire
ecosystem being modeled must be time-stepped at appropriately fine intervals. The second is that realism
demands a certain amount of stochasticity; thus, an ensemble of simulations must be run in order to
understand how changes in environmental and other parameters affect predicted outcomes. Grid imple-
mentations, taking advantage of the inherent parallelism of ecosystems, are one recent effort to advance
individual-based modeling. The development of algorithms implementing parallelization for individual-
based ecological models has enabled a number of simulations, including simulations for fish populations
in the Everglades42 and for more general models aimed ultimately at resource management.43Data issues in computational ecology are also critical. Information technology has been a key enablerfor a great deal of ecological data. For example, high-resolution multispectral images captured by satellites
provide a wealth of information about ecosystems, resulting in maps that can depict how ecologically
significant quantities can vary across large areas. While such images cannot yield significant information
on the behavior of individuals, modern telemetry can be used to follow the movements of many indi-
vidual organisms, a method applied routinely for certain endangered and threatened species.At the same time, much remains to be done. Ground-based sensors take data only in their immedi-ate locality. Thus, the spatial resolution provided by such sensors is a direct function of their areal
density. Therefore, the advent of inexpensive networked sensors, described in Chapter 7, is potentially
the harbinger of a new explosion of ecological data. For example, a survey of thirty papers chosen
randomly from the journal Ecology illustrates that most ecological sampling is conducted with measure-ments being taken in small areas or at low frequency (often including one-time sampling).44 Wireless40See D.L. DeAngelis and L.J. Gross, eds., Individual-Based Models and Approaches in Ecology, Routledge, Chapman and Hall,New York, 1992.41J. Haefner, ÒParallel Computers and Individual-Based Models: An Overview,Ó pp. 126-164 in D. DeAngelis and L. Gross,eds., Individual-Based Models and Approaches in Ecology, Chapman and Hall, New York, 1992.42D. Wang, M.W. Berry, E.A. Carr, and L.J. Gross, ÒA Parallel Landscape Model for Fish as Part of a Multi-Scale EcologicalSystem,Ó available at http://www.tiem.utk.edu/gem/papers/dalipaper.pdf.43D. Wang, E.A. Carr, M.R. Palmer, M.W. Berry, and L.J. Gross, ÒA Grid Service Module for Natural-Resource Managers,ÓIEEE Internet Computing 9(1):35-41, 2005, available at http://www.tiem.utk.edu/gem/papers/gridservice.pdf.44J. Porter et al., ÒWireless Sensor Networks for Ecology,Ó Biosciences, 2005, in press.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY317sensor networks can fill a gap in our current capabilities by enabling researchers to sample at finerspatial scales or faster rates not currently possible. It is this range of space-time (widely distributed
spatial sensing with high temporal frequency) that will be critical to address the grand challenges of the
environmental sciences (biogeochemical cycles, biological diversity and ecosystem functioning, climate
variability, hydrologic forecasting, infectious disease and the environment, institutions and resource
use, land-use dynamics, reinventing the use of materials) proposed by the National Research Council.45Similarly, an explosion of data and of information will arise from sensors carried by individual animals.
The extent of information potentially provided by continuous monitoring of position and physiological
data, compared to tags and radio collars, is obvious.Note also an important synergy between modeling and the use of sensor networks. The effectiveuse of sensor networks relies on modeling and analytical work to guide the placement of sensors. In
turn, sensor data provide data to models that allow for prediction and interpretation of models, to
understand the underlying processes. In this sense, models are the basis for an adaptive sampling
scheme for sensor use.Another data issue is progress in capturing specimen data in electronic form. Over the years,hundreds of millions of specimens have been recorded in museum records. While the information in
extant collections could provide numerous opportunities for modeling and increased understanding,
very few records are in electronic form and even fewer have been geocoded.  Museum records carry a

wealth of image and text data, and digitizing these records in a meaningful and useful way remains a
serious challenge, in terms of both appropriate technical methods and the practical effort and resources
required.9.7  GENOME-ENABLED INDIVIDUALIZED MEDICINE
By many accounts, knowledge of the sequence of the human genome has enormous potential forchanging the practice of medicine and the delivery of health care services. As more is understood about
human biology, it is increasingly feasible for medicine to be predictiveÑto have advance knowledge of
how a personÕs health status will respond (positively or negatively) to various exposures to different
foods and environmental events, and to prevent disease and sustain lifelong health and well-being.
Both these goals depend on a personalized medicine that begins with deep knowledge of the implica-
tions of the genetic makeup of any given individual, as well as his or her health and medical life history.
Indeed, one of the most important implications of knowledge of the genome is the possibility that
medical treatment and interventions might be more customized to the genetic profile of individuals or
groups in ways that maximize the likelihood of successful outcomes.46One necessary precondition for genome-based individualized medicine is technology for the inex-pensive acquisition of sequence informationÑperhaps a few hundred dollars for an individualÕs com-
plete genome, for example.47 On the other hand, from a cost-effectiveness standpoint, it is better to
stratify individuals into subcategories that are relevant to various treatment or intervention regimes by
looking at a limited number of genetic markers, rather than to acquire the complete genetic sequence of
all individuals involved. This vision has led major pharmaceutical companies to proclaim that genomic45National Research Council, Grand Challenges in Environmental Sciences, National Academy Press, Washington, DC, 2001.46One of the most ambitious efforts to exploit the potential of genome-enabled individualized medicine is being undertaken byMexico, whose population is composed of more than 65 native Indian groups and Spaniards. Because the overall genetic makeupof this population is associated with a characteristic set of disease susceptibilities, Mexico has undertaken this initiative to reducethe social and financial burden of health problems, since new strategies for prevention, early diagnosis, and more effectivetreatment are essential to meet the mid- and long-term health care goals in Mexico. See Gerardo Jimenez-Sanchez, ÒDeveloping aPlatform for Genomic Medicine in Mexico,Ó Science 300:295-296, 2003.47Note that this is 105 times less expensive than the sequencing of the first genome. Whether the least expensive approachturns out to be sequencing individual genomes from scratch, or sequencing only those portions specific to individuals andintegrating those portions into the genome of the generic human, remains to be seen.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.318CATALYZING INQUIRYmedicine and related technologies will allow physicians to provide the right drug to the right patient atthe right time. Thus, the term Òindividualized medicineÓ should be regarded as one that ranges from
single individuals (likely in the farther-term future) to genetically differentiated subpopulations (more
likely to happen in the near term).The fundamental challenge is to correlate genetic variation to susceptibility for specific diseases,specific drug reactions, and specific responses to environmental insult. But even with these correlations
in hand, it is a very long way from examination of individual drug-gene interactions to individualized
medicineÑwhat might be called translational medicineÑthat affects the well-being of the citizenry at
large. Traversing this distance will require considerable advances on multiple fronts: in the laboratory,
on the computer, and in how scientists conceptualize the relationships between all of the individual
components involved.9.7.1  Disease Susceptibility
48It has been known for many years that many medical conditions have a genetic basis. Indeed, formany illnesses, the strongest predictor of risk is an individualÕs family history. The association of
specific genomic differences with the likelihood of disease will provide physicians and patients with
more specific and more certain information. Such knowledge will allow individuals to takes steps that
reduce the likelihood and/or severity of such disease in the future. These steps might include greater
medical surveillance or screening, environmental changes, diet, exercise, or preventive drug therapy
(e.g., more frequent colonoscopies starting earlier in life for individuals with genetic profiles that imply
a high degree of risk for colon cancer).It is useful to distinguish between genetic signatures that are highly penetrant and those that arehighly prevalent. A highly penetrant genetic signature associated with a disease is one whose presence
implies a high likelihood that the disease will develop in an individual with that signature: examples
provided by Guttmacher and Collins (Footnote 48) include mutations in the BRCA1 and BRCA2 genes
that increase the risk of breast and ovarian cancer, in the HNPCC gene set that increases the risk of
hereditary nonpolyposis colorectal cancer, and in the gene for synuclein that causes ParkinsonÕs dis-
ease. A highly prevalent genetic signature is one that occurs frequently in the population, but its
presence may or may not be associated with a large increase in the likelihood that a disease will develop
in an individual with that signature: as examples, Guttmacher and Collins (Footnote 48) include a
mutation in the factor V Leiden gene that increases the risk of thrombosis, in the APC (adenomatosis
polyposis coli) gene that increases the risk of colorectal cancer, and in the apolipoprotein gene that
increases the risk of AlzheimerÕs disease.From the standpoint of the individual, identification of a highly penetrant genetic signature associatedwith disease will have important clinical ramifications. However, from a public health standpoint, it is the
identification of highly prevalent genetic signatures associated with disease that is most significant.The best-understood genetic disorders leading to disease are those associated with the inheritanceof a single gene. Such disease conditions have been cataloged in the Online Mendelian Inheritance in
Man (OMIM) catalog.49 Examples of single-gene conditions cited by Guttmacher and Collins includehereditary hemochromatosis, cystic fibrosis, alpha1-antitrypsin deficiency, and neurofibromatosis. These48The discussion in this section on monogenic and highly penetrant signatures is based on excerpts from A.E. Guttmacher andF.S. Collins, ÒGenomic MedicineÑA Primer,Ó New England Journal of Medicine 347(19):1512-1520, 2002. The discussion in thissection on polygenic and highly prevalent signatures is based on excerpts from P.D. Pharoah, A. Antoniou, M. Bobrow, R.L.
Zimmern, D.F. Easton, and B.A. Ponder, ÒPolygenic Susceptibility to Breast Cancer and Implications for Prevention,Ó NatureGenetics 31(1):33-36, 2002. A highly positive and optimistic view of the impact of the genome on medicine can be found in F.S.Collins and V.A. McKusick, ÒImplications of the Human Genome Project for Medical Science,Ó Journal of the American MedicalAssociation 285(5):540-544, 2001. A somewhat contrary view can be found in N.A. Holtzman and T.M. Marteau, ÒWill GeneticsRevolutionize Medicine?Ó New England Journal of Medicine 343(2):141-144, 2000.49See http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=OMIM.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY319disorders are highly penetrant, but occur relatively rarely in the population (with approximate inci-dences of one in several hundred or less).On the other hand, multifactor genetic causality for disease is almost certainly much more com-mon than monogenic causality. In principle, knowledge of the range of genetic variations across many
loci in the population will allow researchers to estimate risks arising from the combined effect of such
variations.Using breast cancer as a case study, Pharoah et al. (footnote 48) compared the potential for predic-tion of risk based on common genetic variations with the predictions that could be made using known
and established risk factors. They concluded that a typical polygenic approach for analysis would
suggest that the half of the population at highest risk would account for 88 percent of all affected
individuals, if all of the susceptibility genes could be identified. However, using currently known
factors for breast cancer to stratify the population, they estimated that the half of the population at
highest risk would account for only 62 percent of all cases. Pharoah et al. thus suggest that genetic
profiles may provide significant improvement in the ability to differentiate at-risk individuals from
individuals not at risk.Nevertheless, for a variety of reasons, identifying the relevant genetic signatures over multiplegenes that account for disease susceptibility will pose significant intellectual challenges. Probably the
most important point is that the contribution of any given gene involved is likely to be weak; hence
detecting its clinical significance may be problematic. Nongenomic effects, such as posttranslational
modifications, may also be relevant. Zimmern50 notes that even monogenic conditions can result invariable expressivity and incomplete penetrance, and that similar disease phenotypes may result from
genetic heterogeneity, whether in the form of allelic heterogeneity (different mutations at the same
locus) or locus heterogeneity (where mutations occur at different loci). Different mutations of the same
gene may also give rise to separate clinical effects. Environmental factors may be difficult to disentangle
from genetic ones. As a consequence of such issues, definitive conclusions about the relationship of a
given polygenic genotype to a specific disease condition may well be difficult to draw.An extension of the genomic approach to disease susceptibility applies to understanding the impactof an individualÕs genomic composition on that individualÕs response to various environmental insults
to the body, such as those caused by exposure to chemicals (e.g., from drinking water or air pollution)
or electromagnetic fields (e.g., from cell phones or ambient radiation). Furthermore, in dealing with
certain environmental insults, stochasticity is likely to play an important role. For example, in consider-
ing the effects of radiation on the genome, macroscopic parameters that characterize radiation such as
duration and intensity are insufficient to determine its effect, simply because what part of a genome is
affected is mostly a matter of chance. Thus, a given dose of a certain kind of radiation will not affect
individuals in equal measure and, more to the point, could not be expected to affect even an ensemble
of identical twins similarly.Overall, there is wide variability in individual responses to environmental influences. While exist-ing diseases, differences in gender, or differences in nutritional status affect such variability, genetic
influences are also important. Genes that affect the human response to environmental exposure (called
environmentally responsive genes by the Environmental Genome Project [EGP] of the National Insti-
tute of Environmental Health Sciences [NIEHS] tend to fall into several categories.51 That is, they affectthe cell cycle, DNA repair, cell division, cell signaling, cell structure, gene expression, apoptosis, and
metabolism. The initial phases of the EGP are focused on identifying single nucleotide polymorphisms
(SNPs) associated with 554 genes identified by the scientific community as environmentally responsive.
Identification of the SNPs associated with environmentally responsive genes would make it possible to
conduct epidemiological studies that classify subjects by SNPs, thus increasing the utility of these50R.L. Zimmern, ÒThe Human Genome Project: A False Dawn?Ó British Medical Journal 319(7220):1282, 1999.51See http://www.niehs.nih.gov/envgenom/egp.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.320CATALYZING INQUIRYstudies in detecting genetic contributions to the likelihood of various diseases with at least partialenvironmental causation.The challenges of polygenic data analysis are formidable. An example of methodological researchin this area is that of Nelson et al.,52 who developed the combinatorial partitioning method (CPM) forexamining multiple genes, each containing multiple variable loci, to identify partitions of multilocus
genotypes that predict interindividual variation in quantitative trait levels. The CPM offers a strategy
for exploring the high-dimensional genotype state space so as to predict the quantitative trait variation
in the population at large that does not require the conditioning of the analysis on a prespecified genetic
model, such as a model that assumes that interacting loci can each be identified through their indepen-
dent, marginal contribution to trait variability. On the other hand, a brute-force approach to this corre-
lation problem explodes combinatorially. Therefore, it is likely that finding significant correlations will
depend on the ability to prune the search space before specific combinations are testedÑand the ability
to prune will depend on the availability of insight into biological mechanisms.9.7.2  Drug Response and Pharmacogenomics
53As with disease susceptibility, it has been known for many years that different individuals responddifferently to the same drug at the same dosages and that the relevant differences in individuals are at
least partly genetic in origin. However, characterization of the first human gene containing DNA se-
quence variations that influence drug metabolism did not take place until the late 1980s.54 Today,pharmacogenomicsÑthe impact of an individualÕs genomic composition on his or her response to
various drugsÑis an active area of investigation that many believe holds significant promise for chang-
ing the practice of medicine by enabling individual-based prescriptions for compound and dosage.55An individualÕs genetic profile may well suggest which of several drugs is most appropriate for a given
disease condition. Because genetics influence drug metabolism, an individualÕs weight will no longer be
the determining factor in setting the optimal dosage for that individual.Similarly, many drugs are known to be effective in treating specific disease conditions. However,because of their side effects in certain subpopulations, they are not available for general use. Detailed
ÒomicÓ knowledge about individuals may help to identify the set of people who might benefit from
certain drugs without incurring undesirable side effects, although some degree of empirical testing will
be needed if such individuals can be identified.56 In addition, some individuals may be more sensitivethan others to specific drugs, requiring differential dosages for optimal effect.As in the case of disease susceptibility, the best-understood genetic polymorphisms that affect drugresponses in individuals are those that involve single genes. As an example, Evans and Relling note that52M.R. Nelson, S.L.R. Kardia, R.E. Ferrell, and C.F. Sing, ÒA Combinatorial Partitioning Method to Identify Multilocus Geno-typic Partitions That Predict Quantitative Trait Variation,Ó Genome Research 11(3):458-470, 2001.53Much of the discussion in Section 9.7.2 is based on excerpts from W.E. Evans and M.V. Relling, ÒMoving Towards Individu-alized Medicine with Pharmacogenomics,Ó Nature 429(6990):464-468, 2004.54F.J. Gonzalez, R.C. Dkoda, S. Kimura, M. Umeno, U.M. Zanger, D.W. Nebert, H.V. Gelboin, et al., ÒCharacterization of theCommon Genetic Defect in Humans Deficient in Debrisoquine Metabolism,Ó Nature 331(6155):442-446, 1988. Cited in Evans andRelling, 2004.55If the promise of pharmacogenomics is realized, a number of important collateral benefits follow as well. Drug compoundsthat have previously been rejected by regulatory authorities because of their side effects on some part of the general populationat large may become available to those individuals genomically identified as not being subject to those side effects. Thus, theseindividuals would have options for treatment that would not otherwise exist. Furthermore, clinical trials for drug testing couldbe much more targeted to appropriate subpopulations with a higher likelihood of ultimate success, thus reducing expenses
associated with failed trials. Also, in the longer term, pharmacogenomics may enable the customized creation of more powerfulmedicines based on the specific proteins and enzymes associated with genes and diseases.56Another application often discussed in this context is the notion of drugs customized to specific individuals based on ÒomicÓdata. However, the business model of pharmaceutical companies today is based on large markets for their products. Until itbecomes possible to synthesize and manufacture different drug compounds economically in small quantity, custom-synthesizeddrugs for small groups of individuals will not be feasible.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY321individuals that are deficient in thiopurine S-methyltransferase (TPMT) can be treated with much lowerdoses of the thiopurine drugs mercaptopurine and azathiopurine used as immunosuppressants and to
treat neoplasias. There is a clinical diagnostic test available for the genomic detection of the TPMT
deficiency, but routine use of TPMT genotyping to make treatment decisions is limited. A second
example also discussed by Evans and Relling is that polymorphisms in a gene known as CYP2D6 have
a strong effect on individualsÕ responses to the antihypertensive drug debrisoquine and in the metabo-
lism of the oxytocic drug sparteine.A second example is found in the area of certain drugs for the treatment of cardiovascular disease.Numerous examples of differences among individuals have been seen as potential candidate pharmaco-
dynamic loci (e.g., those for angiotensinogen, angiotensin-converting enzyme, and the angiotensin II
receptor). Polymorphisms at these loci predict responses to specific treatments such as the inhibition of
angiotensin-converting enzyme. Here, researchers hope to establish and utilize antihypertensive drugs
that are matched to the genetic variations among individuals, and thus to optimize blood pressure
control and reduce side effects.57A number of monogenic polymorphisms have been found, encoding drug-metabolizing enzymes,drug transporters, and drug targets, as well as disease-modifying genes, that have been linked to drug
effects in humans. However, these are the Òlow-hanging fruitÓ of pharmacogenetics, and for most drug
effects and treatment outcomes, monogenic polymorphisms with clearly recognizable drug-response
phenotypes do not characterize the situation. For example, as in the case of disease susceptibility,
nongenomic effects (e.g., posttranslational modifications) on protein function may be relevant. Or,
multiple genes may act together in networks to create a single drug-response phenotype.As Evans and Relling note, genome-wide approaches, such as gene expression arrays, genome-wide scans, or proteomic assays, can contribute to the identification of as-yet-unrecognized candidate
genes that may have an influence on a drug response phenotype. For example, it may be possible to
detect genes whose expression differentiates drug responders from nonresponders (or those for whom
certain drugs are toxic from those for whom they are not), genomic regions with a paucity of heterozy-
gosity in responders compared with nonresponders, or proteins whose abundance differentiates drug
responders from nonresponders.In expression-array and proteomic approaches, the level of the signal may directly reflect functionalvariationÑa distinct advantage from an experimental point of view. Yet there can be many other
reasons for differences in signal level, such as the choice of tissue from which the samples are drawn
(which may not be the tissue of interest where toxicity or response is concerned) or changes in function
not reflected by levels of mRNA or protein. Thus, when such studies suggest that a given gene or gene
product is relevant to drug response, Evans and Redding point out that large-scale molecular epidemio-
logical association studies (in vivo or in vitro with human tissues), biochemical functional studies, and
studies on preclinical animal models of candidate gene polymorphisms become necessary to further
establish the link between genetic polymorphism and drug response.A second challenge in pharmacogenomics relates to integrating pharmacogenomics with the every-day practice of medicine. Although there are cultural and historical sources of resistance to such inte-
gration, it is also true that definitive clinical pharmacogenomic studies have not been conducted that
demonstrate unambiguously the benefits of integration on clinical outcomes. Indeed, there are many
difficulties in conducting such studies, including the multigenic nature of most drug effects and the
difficulty in controlling for nongenetic confounders such as diet or exercise. Until such difficulties are
overcome, it is unlikely that a significant change will occur in clinical practice.One of the most important databases for the study of pharmacogenomics is a database known as theStanford PharmGKB, described in Box 3.4. Supported by the National Institute of General Medical57P. Cadman and D. OÕConnor, ÒPharmacogenomics of Hypertension,Ó Current Opinion in Nephrology and Hypertension 12(1):61-70, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.322CATALYZING INQUIRYScience (NIGMS), PharmGKB is a publicly available, Internet-accessible database for pharmacogeneticsand pharmacogenomics. Its overall aim is to aid researchers in understanding how genetic variation
among individuals contributes to differences in reactions to drugs.58 The database integrates pharma-codynamics (drug actions), pharmacokinetics (drug metabolism), toxicity, sequence and other molecu-
lar data, pathway information, and patient data.9.7.3  Nutritional Genomics
Traditional nutrition research has had among its goals the establishment of overarching dietaryrecommendations for everyoneÑin principle, for the worldÕs entire population. Today, and more so in
the future, the implications of individual genetic makeup for optimal diet have changed that perspec-
tive. To understand and exploit the interplay of diet and genetics, nutritional genomics is a relatively
new specialization within the life sciences with two separate but related foci. One focus relates an
individualÕs genetic makeup to dietary regimes that are more or less healthy for him or her. For ex-
ample, it is well known that some individuals are more likely to suffer from high blood pressure if they
consume salt in relatively large quantities, while others are not. Poch et al.59 found a possible geneticbasis on which to differentiate salt-sensitive individuals and salt-insensitive ones. If it is possible to
develop genetic tests for salt sensitivity, salt-sensitive individuals could be advised specifically to limit
their salt intake, and salt-insensitive individuals could continue to indulge at will their taste for salty
snacks.The traditional focus of nutrition research is not in any way rendered irrelevant by nutritionalgenomics. Still, beyond general good advice and informed common sense, in the most ambitious sce-
narios, recommended dietary profiles could be customized for individuals based on their specific ge-
nomic composition. Ordovas and Corella write:60Nutritional genomics has tremendous potential to change the future of dietary guidelines and personalrecommendations. Nutrigenetics will provide the basis for personalized dietary recommendations based
on the individualÕs genetic makeup. This approach has been used for decades for certain monogenicdiseases; however, the challenge is to implement a similar concept for common multifactorial disordersand to develop tools to detect genetic predisposition and to prevent common disorders decades before
their manifestation. . . . [P]reliminary evidence strongly suggests that the concept should work and thatwe will be able to harness the information contained in our genomes to achieve successful aging usingbehavioral changes; nutrition will be the cornerstone of this endeavor.A second focus of nutritional genomics is on exploiting the potential for modifying foodstuffs to bemore healthy, and so dietary advice and discipline might be supplanted in part by such modifications.For example, it may be possible to redesign the lipid composition of oil seed crops using genetic
modification techniques (through either selective breeding or genetic engineering). However, whether
this is desirable depends on how consumption of a different mix of lipids affects human health. Watkins
et al.61 argue for an understanding of the overall metabolomic expression of lipid metabolism to ensurethat a particular metabolite composition truly improves overall health, so that a change in lipid compo-
sition that is deemed healthy when viewed as lowering the risk of one disease does not simultaneously
increase the risk of developing another.58T.E. Klein and R.B. Altman, ÒPharmGKB: The Pharmacogenetics and Pharmacogenomics Knowledge Base,Ó PharmacogenomicsJournal 4(1):1, February 2004.59E. Poch, D. Gonzalez, V. Giner, E. Bragulat, A. Coca, and A. de La Sierra, ÒMolecular Basis of Salt Sensitivity in HumanHypertension: Evaluation of Renin-Angiotensin-Aldosterone System Gene Polymorphisms,Ó Hypertension 38(5):1204-1209, 2001.60J.M. Ordovas and D. Corella, ÒNutritional Genomics,Ó Annual Review of Genomics and Human Genetics 5:71-118, 2004.61S.M. Watkins, B.D. Hammock, J.W. Newman, and J.B. German, ÒIndividual Metabolism Should Guide Agriculture TowardFoods for Improved Health and Nutrition,Ó American Journal of Clinical Nutrition 74(3):283-286, 2001.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY323More generally, Watkins et al. point out that the goal of nutritional improvement of agricultureÑtoproduce changes in crops and foods that provide health benefits to allÑis difficult to achieve because
modifications of existing foodstuffs are likely to advantage some people while disadvantaging others.
Watkins et al. cite the example of recent attempts to increase the carotenoid content of the food supplyÑ
a move that was thought to have protective value against certain cancers, especially lung cancer. In the
midst of this effort, it was found that high intakes of §-carotene as a supplement actually increased theincidence of lung cancer in smokersÑand the move was abandoned.The intellectual underpinning of this effort is thus metabolomics, the quantitative characterizationof the set of metabolitesÑgenerally small, nonprotein moleculesÑinvolved in the metabolism or a cell,
tissue, or organism over its lifetime. In the context of nutritional genomics, metabolomic studies attempt
to characterize the levels, activities, regulation, and interactions of all metabolites in an individual and
determine how this characterization changes in response to various foods that are consumed. Genomics
is important because genetic makeup is an important influence on the specific nature of the metabolomic
changes that result as a function of food consumption.9.8  A DIGITAL HUMAN ON WHICH A SURGEON CAN OPERATE VIRTUALLY
A surgical act on a human being is by definition an invasive process, one that inflicts many insultson the body. Prior to the advent of medical imaging techniques, surgeons relied on their general
knowledge of anatomy to know where and what to cut. TodayÕs imaging technologies provide the
surgeon with some idea of what to expect when he or she opens the patient.At the same time, a surgeon in the operating room has no opportunity to practice the operation onthis particular patient. Experience with other patients with similar conditions helps immeasurably, of
course, but it is still not uncommon even in routine surgical operations to find some unexpected
problem or complication that the surgeon must manage. Fortunately, most such problems are minor
and handled easily. Surgeons-in-training operate first on cadavers and move to live patients only after
much practice and under close supervision.Consider then the advantages that a surgeon might have if he or she were to be able to practice adifficult operation before doing it on a live patient. That is, a surgeon (or surgeon-in-training) would
practice or train on a digital model of a human patient that incorporates static and dynamic physical
properties of the body in an operating room environment (e.g., under anesthesia, in real gravity) when
it is subject to surgical instruments.In this environment, the surgeon would likely wear glasses that projected an appropriate image tohis or her retina and use implements that represented real instruments (e.g., a scalpel). Kinetic param-
eters of the instrument (e.g., speed, velocity, orientation) would be monitored and registered onto the
image that the surgeon sees. When ÒtouchedÓ by the instrument, the image would respond appropri-
ately with a change in shape and connectivity (e.g., when a scalpel touches a piece of skin, it might
separate into two parts and a cut would appear). Blood would emerge at realistic rates, and tissue under
the skin would appear.Even in this very simple example, many challenges can be seen. To name just a few:¥Realistic modeling of body subsystems. From the perspective of a surgeonÕs scalpel, the body issimply a heterogeneous and spatially organized mass of tissue. Of course, this mass of tissue is func-
tionally a collection of subsystems (e.g., organs, muscle tissue, bone) that have different properties.
These subsystems must be separated so that the physiological responses of surgery are appropriately
propagated through them when surgery occurs.¥Integration of person-specific information with a generic model of a human being. Because of the laborinvolved in constructing a digital model of a human being, it makes sense to consider an approach in
which a model of a generic human being is developed and then adjusted according to person-specific
information of any given patient.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.324CATALYZING INQUIRY¥Spatial registration and alignment of instruments, the surgeonÕs hands, and the digital body being oper-ated on. The surgeon must see an instrument move to the position in the body to which the surgeon hasmoved it. When a cutting motion is made, the appropriate tissue should split in the appropriate place
and amount.¥The different feel and texture of tissue depending on whether the instrument is a scalpel or a finger. Adigital human for surgical use must provide appropriate force feedback (Òhaptic capabilityÓ) to the
surgeon so that, for example, cutting into soft tissue feels different than cutting into bone.¥Incorporation of gravity in the model. Many organs consist of soft tissue that is deformed easilyunder pressure from instruments and touch. As importantly, tissues are subject to gravitational forces
that will change their shape as their orientation is changed (the breast of a woman lying on her back has
an entirely different shape than when she is lying on her side).Some first steps have been taken in many of these areas. For example, a project at the OhioSupercomputer Center (OSC) in 1996 sought to develop a virtual reality-based simulation of regional
anesthesia that employed haptic techniques to simulate the resistance felt when an injection is given in
a certain area (Box 9.4).A second example is work in computational anatomy, one application of which has sought tocharacterize the structure of human brains in a formal manner. Structure is interesting to neuroscientists
because of a presumed link between physical brain structure and neurological function. Through math-
ematical transformations that can deform one structure into another, it is possible to develop metrics
that can characterize how structurally different two brains are. These metrics can then be correlated
with understanding of the neurological functions of which each brain is capable (Box 9.5). Such metrics
can also be used to identify normal versus diseased states that are reflected anatomically.Box 9.4A Virtual Reality Simulation of Regional AnesthesiaA collaborative effort between researchers at the Ohio State University Hospitals, Immersion Corporation, andthe Ohio Supercomputer Center has led to the creation of a virtual reality simulator that enables anesthesiol-ogists-in-training to practice in a realistic environment the injection of a local anesthetic into the epiduralspace of the spinal column. The system includes a workstation capable of stereo display, a real-time spatial
volume renderer, a voice-activated interface, and most importantly, a one-dimensional haptic probe capableof simulating the resistive forces of penetrated tissues.Although this procedure appears simple, it is in fact a delicate manual operation that requires the placementof a catheter into a small epidural space using only haptic cues (i.e., cues based on tactile sensations ofpressure) to guide the needle. By feeling the resistive forces of the needle passing through various tissues, the
anesthesiologist must maneuver the tip of the needle into the correct space without perforating or damagingthe spinal cord in the process.The system is designed to enable the trainee to practice the procedure on a variety of datasets representativeof what he or she might experience with real patients. That is, the pressure profile as a function of needlepenetration would vary from patient to patient. By training in this environment, the trainee can gain proficien-
cy in the use of this technique in a non-harmful manner.SOURCE: L. Hiemenz, J.S. McDonald, D. Stredney, and D. Sessanna, ÒA Physiologically Valid Simulator for Training Residents to Performan Epidural Block,Ó Proceedings of the 15th Southern Biomedical Engineering Conference, March 29-31, 1996, Dayton, OH. See alsohttp://www.osc.edu/research/Biomed/past_projects/anesthesia/index.shtml.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY3259.9  COMPUTATIONAL THEORIES OF SELF-ASSEMBLY AND SELF-MODIFICATION
62Self-assembly is any process in which a set of components joins together to form a larger, morecomplex structure without centralized or manual control. For example, it includes biologically signifi-
cant processes ranging from the joining of amino acids to form a protein and embryonic development to
nonbiological chemical processes such as crystallization. More recently, the term has become widely
used as researchers attempt to create artificial self-assembling systems as a way to fabricate structures
efficiently at nanometer scale.One kind of structureÑthat can be described as a simple repeating pattern in which molecules forminto a regular structure or latticeÑis the basis for creating artifacts such as crystals or batteries that can
be extended to potentially macroscopic scale; this process is known as periodic self-assembly. However,
for applications such as electronic circuits, which cannot be described as a simple repeating pattern, aBox 9.5Computational AnatomyComputational anatomy seeks to make more precise the commonsense notion that samples of a given organ froma particular species are both all the same and all different. They are the same in the sense that all human brains, for
example, exhibit similar anatomical characteristics and can be associated with the canonical brain of Homosapiens, rather than the canonical brain of a dog. They are all different in the sense that each individual has a slightlydifferent brain, whose precise anatomical characteristics differ somewhat from those of other individuals.Computational anatomy is based on a mathematical formalism that allows one structure (e.g., a brain) to bedeformed reversibly into another. (Reversibility is important because irreversible processes destroy informa-
tion about the original structure.) In particular, the starting structure is considered to be a deformable template.The template anatomy is morphed into the target structure via transformations applied to subvolumes, con-tours, and surfaces. These computationally intensive transformations are governed by generalizations of the
Euler equations of fluid mechanics and are required only to preserve topological relationships (i.e., to trans-form smoothly from one to the other).Key to computational anatomy is the ability to calculate a measure of difference between similar structures.That is, a distance parameter should represent in a formalized manner the extent to which two structuresdifferÑand a distance of zero should indicate that they are identical. In the approach to computational
anatomy pioneered by Grenander and Miller,1 the distance parameter is the square root of the energy re-quired to transform the first structure onto the metric of the second with the assumption that normal transfor-mations follow the least-energy path.One instance in which computational anatomy has been used is in understanding the growth of brains asjuveniles mature into adults. Thompson et al.2 have applied these deformation techniques to the youngestbrains, with results that accord well with what was seen in older subjects. In particular, they are able to predictthe most rapid growth in the isthmus, which carries fibers to areas of the cerebral cortex that support languagefunction. A second application has sought to compare monkey brains to human brains.1U. Grenander and M.I. Miller, ÒComputational Anatomy: An Emerging Discipline,Ó Quarterly Journal of Applied Mathematics 56:617-694, 1998.2P.M. Thompson, J.N. Giedd, R.P. Woods, D. Macdonald, A.C. Evans, and A.W. Toga, ÒGrowth Patterns in the Developing BrainDetected by Using Continuum Mechanical Tensor Maps,Ó Nature 404:190-193, March 9, 2000; doi:10.1038/35004593.SOURCE: Much of this material is adapted from ÒComputational Anatomy: An Emerging Discipline,Ó EnVision 18(3), 2002, available athttp://www.npaci.edu/envision/v18.3/anatomy.html#establishing.62Section 9.9 is based largely on material from L. Adleman, Q. Cheng, A. Goel, M.-D. Huang, D. Kempe, P. Moisset de Espan”s,P. Wilhelm, and K. Rothemund, ÒCombinatorial Optimization Problems in Self-Assembly,Ó STOC Õ02, available at http://www.usc.edu/dept/molecular-science/optimize_self_assembly.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.326CATALYZING INQUIRYmore expressive form of self-assembly is required. Ideally, a designer could select a set of componentsand a set of rules by which they connect, and the system would form itself into the desired final shape.This kind of self-assembly, called nonperiodic or programmable self-assembly, would allow thecreation of arbitrary arrangements of components. Nonperiodic self-assembly would be useful for the
efficient execution of tasks such as electronic circuit design, material synthesis, micro- and nanomachine
construction, and many other technological feats. For the purposes of artificial self-assembly technol-
ogy, the pinnacle result of a theory would be to be able to select or design an appropriate set of
components and assembling rules to produce an arbitrary desired result.Self-assembly, both as a biological process and as a potential technology, is poorly understood. Arange of significant (and possibly insuperable) engineering and technological challenges stands in the
way of effectively programming matter to form itself into arbitrary arrangements. A less prominent but
no less important challenge is the lack of a theoretical foundation for self-assembly.A theory of self-assembly would serve to guide researchers to determine which structures are achiev-able, select appropriate sets of components and assembling rules to produce desired results, and estimate the
likely time and environmental conditions necessary to do so. Such a theory will almost certainly be based
heavily on the theory of computation and will more likely be a large collection of theoretical results and
proofs about the behavior of self-assembling systems, rather than a single unified theory such as gravity.The grandest form of such a theory would encompass and perhaps unify a number of disparateconcepts from biology, computer science, mathematics, and chemistryÑsuch as thermodynamics, ca-
talysis and replication, computational complexity, and tiling theory63and would require increases inour understanding of molecular shape, the interplay between enthalpy and entropy, and the nature of
noncovalent binding forces.64 A central caveat is that self-assembly occurs with a huge variety of mecha-nisms, and there is no a priori reason to believe that one theory can encompass all or most of self-
assembly and also have enough detail to be helpful to researchers. In more limited contexts, however,
useful theories may be easier to achieve, and more limited theories could serve in guiding researchers to
determine which structures are achievable or stable, to identify and classify failure modes and malfor-
mation, or to understand the time and environmental conditions in which various self-assemblies can
occur. Furthermore, theories in these limited contexts may or may not have anything to do with how
real biological systems are designed.For example, progress so far on a theory of self-assembly has drawn heavily from the theory oftilings and patterns,65 a broad field of mathematics that ties together geometry, topology, combinato-rics, and elements of group theory such as transitivity. A tiling is a way for a set of shapes to cover a
plane, such as M.C. EscherÕs famous tesselation patterns. Self-assembly researchers have focused on
nonperiodic tilings, those in which no regular pattern of tiles can occur. Most important among aperi-
odic patterns are Wang tiles, a set of tiles for which the act of tiling a plane was shown to be equivalent
to the operation of a universal Turing machine.66 (Because of the grounding in the theory of Wang tilesin particular, the components of self-assembled systems are often referred to as ÒtilesÓ and collections of
tiles and rules for attaching them as Òtiling systems.Ó)With a fundamental link between nonperiodic tilings and computation being established, it becomespossible to consider the possibility of programming matter to form desired shapes, just as Turing machines can
be programmed to perform certain computations. Additionally, based on this relationship, computationally
inspired descriptions might be sufficiently powerful to describe biological self-assembly processes.Today, one of the most important approaches to a theory of self-assembly focuses on this abstractmodel of tiles, which are considered to behave in an idealized, stochastic way. Tiles of different types
are present in the environment in various concentrations, and the probability of a tile of a given type63L.M. Adleman, ÒToward a Mathematical Theory of Self-Assembly,Ó USC Tech Report, 2000, available at http://www.usc.edu/dept/molecular-science/papers/fp-000125-sa-tech-report-note.pdf.64G. Whitesides, ÒSelf-Assembly and Nanotechnology,Ó Fourth Foresight Conference on Molecular Nanotechnology, 1995.65B. Grunbaum and G.C. Shephard, Tilings and Patterns, W. H. Freeman and Co., New York, 1987.66H. Wang, ÒNotes on a Class of Tiling Problems,Ó Fundamenta Mathematicae 82:295-305, 1975.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY327attempting to connect to an established shape is proportional to its share of the total concentration oftiles. Then, the ÒgluesÓ of touching sides of the adjacent tiles have a possibility of attaching. Simulating
such self-assembly is actually relatively simple. Given a set of tiles and glues, a simulation can predict
with arbitrary accuracy the end result. However, this is complicated by the fact that a given tiling
system might not have a unique end result; situations could arise in which two different tiles join the
assembly in the same location. While this may seem an undesirable situation, such ambiguous systems
may be necessary to perform universal computation.67The more challenging question is the converse of simulation: Given a desired result, how do we getthere? Research into the theory of self-assembly has focused on two more specific framings of this
question. First, what is the minimum number of tile types necessary to create a desired shape (the
ÒMinimum Tile Set ProblemÓ) and, given a specific tiling system, what concentrations produce the end
result the fastest (the ÒTile Concentrations ProblemÓ)? The former has been shown to be NP-complete,
but has polynomial solutions given certain restrictions on shape and temperature.The current state of the art in the theory of self-assembly abstracts away much of the details ofchemistry. First, the theory considers only the assembly of two-dimensional patterns. For artificial DNA
tiles, designed to be flat, rigid, and square, this may be a reasonable approximation. For a more general
theory that includes the self-assembly in three dimensions of proteins or other nonrigid and highly
irregularly shaped macromolecules, it is less clear that such a theory is sufficient. Extending the current
theory to irregular shapes in three dimensions is a key element of this challenge problem.The history of the motivation of research into the theory of self-assembly provides a lesson forresearch at the BioComp interface. Originally, researchers pursued the link between self-assembly and
computation because they envisioned self-assembled systems constructed from DNA as potential com-
petitors to electronic digital computing hardware, that is, using biochemistry in the service of computa-
tion. However, as it became less obvious that this research would produce a competitive technology,
interest has shifted to using the computational theory of self-assembly to increase the sophistication of
the types of molecular constructs being created. In other words, todayÕs goal is to use computational
theory in the service of chemistry. This ebb and flow of both source theory and application between
computation and biochemistry is a hallmark of a successful model of research at the interface.Another area related to theories of self-assembly is what might be called adaptive programming.Today, most programs are static; although variables change their values, the structure of the code does
not. Because computer hardware does not fundamentally differentiate between ÒcodeÓ and ÒdataÓ (at
the machine level, both are represented by 1Õs and 0Õs), there is no reason in principle that code cannot
modify itself in the course of execution. Self-modifying code can be very useful in certain contexts, but
its actual execution path can be difficult to predict and, thus, the results that might be obtained from
program execution are uncertain.However, biological organisms are known to learn and adapt to their environmentsÑthat is, theyself-modify under certain circumstances. Such self-modification occurs at the genomic level, where the
DNA responsible for the creation of cellular proteins contains both genetic coding and regions that
regulate the extent to which, and the circumstances under which, genes are activated. It also occurs at
the neural level, where cognitive changes (e.g., a memory or a physical skill) are reflected in reorganized
neural patterns. Thus, a deep understanding of how biology organizes self-modification in using DNA
or in a neural brain may lead to insights about how one might approach human problems that call for
self-modifying computer programs.9.10  A THEORY OF BIOLOGICAL INFORMATION AND COMPLEXITY
Much of this report is premised on the notion of biology as an information science and has arguedthat information technology is essential for acquiring, managing, and analyzing the many types of67P.W. Rothemund, ÒUsing Lateral Capillary Forces to Compute by Self-Assembly,Ó Proceedings of the National Academy ofSciences 97(3):984-989, 2000.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.328CATALYZING INQUIRYbiological data. This factÑthat an understanding of biological systems depends on so many differentkinds of biological data, operating at so many different scales, and in such volumeÑsuggests the
possibility that biological information and/or biological complexity might be notions with some formal
quantitative meaning.How much information does a given biological system have? How should biological complexity beconceptualized? Can we quantify or measure the amount of information or the degree of complexity
resident in, say, a cell, or perhaps even more challengingly, in an organelle, an ecosystem, or a species?
In what sense is an organism more complex than a cell or an ecosystem more complex than an indi-
vidual organism? Establishing an intellectually rigorous methodology through which such information
could be measured, capturing not only the raw scale of information needed to describe the constituent
elements of a system but also its complexity, could be a powerful tool for answering questions about the
nature of evolution, for quantifying the effects of aging and disease, and for evaluating the health of
ecologies or other complex systems.Developing such a theory of biological information and complexity will be extraordinarily challeng-ing, however. First, complexity and information exist at a vast range of orders of magnitude in size and
time, as well as in the vast range of organisms on Earth, and it is not at all clear that a single measure or
approach could be appropriate for all scales or creatures. Second, progress toward such a theory has
been made in fields traditionally separate from biology, including physics and computer science. Trans-
ferring knowledge and collaboration between biology and these fields is difficult at the best of times,
and doubly challenging when the research is at an early stage. Finally, such a theory may prove to be the
basis of a new organizing principle for biology, which may require a significant reorientation for
practicing biologists and biological theory.Some building blocks for such a theory may already be available. These include information theory,formulated by Claude Shannon in the mid-20th century for analyzing the performance of noisy commu-
nication channels; an extension of information theory, developed over the last few decades by theoreti-
cal physicists, that defines information in thermodynamic terms of energy and entropy; the body of
computational complexity theory, starting from TuringÕs model of computation and extending it to
include classes of complexity based on the relative difficulty of families of algorithms; and complexity
theory (once called Òchaos theoryÓ), an interdisciplinary effort by physicists, mathematicians, and biolo-
gists to describe how apparently complex behavior can arise from the interaction of large numbers of
very simple components.Measuring or even defining the complexity of a biological systemÑindeed, of any complex, dy-namic systemÑhas proven to be a difficult problem. Traditional measures of complexity that have been
developed to analyze and describe the products of human technological engineering are difficult to
apply or inappropriate for describing biological systems. For example, although both biological systems
and engineered systems often have degrees of redundancy (i.e., multiple instances of the same Òcompo-
nentÓ that serve the same function for purposes of reliability), biological systems also show many other
systems-level design behaviors that are rarely if ever found in engineered systems. Indeed, many such
behaviors would be considered poor design. For example, ÒdegeneracyÓ in biological systems refers to
the property of having different systems produce the same activity. Similarly, in most biological sys-
tems, many different components contribute to global properties, a design that if included in a human-
engineered system would make it very difficult to understand.Other attempts at measuring biological complexity include enumerating various macroscopic prop-erties of an organism, such as the number of distinct parts, number of distinct cell types, number of
biological functions performed, and so forth. In practice this can be difficult (what is considered a
ÒdistinctÓ part?) or inconclusive (is an organism with more cell types necessarily more complex?).More conveniently, the entire DNA sequence of an organismÕs genome can be analyzed. Since DNAplays a major role in determining the structure and functions of an organism, one approach is to
consider the information content of the DNA string. Of course, biological knowledge is nowhere close
to actually being able to infer the totality of an organism merely from a DNA sequence, but the argu-Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.ILLUSTRATIVE PROBLEM DOMAINS AT THE INTERFACE OF COMPUTING AND BIOLOGY329ment is that sequence complexity will be highly correlated with organismal complexity. (Some advan-tages of dealing with strings of letters as an abstraction are discussed in Section 4.4.1.)Because information theory treats all bits as alike and of equal significance, a purely information-theoretic view would suggest that a gene of a thousand base pairs that encode a crucial protein required
for the development of a human characteristic has the same information content (about 2,000 bits) as a
random sequence of the same length with no biological function. This view strains plausibility or,
rather, would have limited applicability to biology. Thus, the example suggests that something more is
needed.Generally, the types of complexity measures applied to DNA sequences are defined by their rela-tionship to the process of computation. For example, a string might be considered to be a program, an
input to a program, or the output of a program, and the resulting complexity measure might include the
size of the Turing machine that produced it, its running time, or the number of states. Each measure
captures a different sense of complexity of the DNA string and will consider different strings to be
relatively more or less complex.One such approach is the notion of Kolmogorov (or more formally, Kolmogorov-Chaitin-Solomonoff) complexity. Kolmogorov complexity is a measure of the extent to which it is possible to
eliminate redundancies from a bit string without loss of information. Specifically, a program is written
to generate the bit string in question. For a truly random string, the program is at least as long as the
string itself. But if there are information redundancies in the string, the string can be compressed, with
the compressed representation being the program needed to reproduce it. A string with high
Kolmogorov complexity is one in which the difference in length between the string and its program is
small; a string with low Kolmogorov complexity is one that contains many redundancies and thus for
which the generating program is shorter than the string.However, for the purpose of analyzing overall complexity, a purely random string will have amaximal Kolmogorov score, which is not what seems appropriate intuitively for estimating biological
complexity. In general, a desired attribute of measures of biological complexity is the so-called one-
hump criterion. A measure that incorporated this criterion would indicate a very low complexity for
both very ordered sequences (e.g., a purely repeating sequence) and very random sequences and the
highest complexity for sequences in the middle of a notional continuum, neither periodic nor random.68Feldman and Crutchfield further suggest that biological complexity must also be defined in a setting
that gives a clear interpretation to what structures are quantified.69Other measures that have been proposed include thermodynamic depth, which relates a systemÕsentropy to the number of possible histories that produced its current state; logical depth, which consid-
ers the minimal running time of a program that produced a given sequence; statistical complexity
measures, which indicate the correlation among different elements of an entityÕs components and the68A related phenomenon, highly investigated but poorly understood, is the ubiquity of so-called 1/f spectra in many interest-ing phenomena, including biological systems. The term Ò1/f spectraÓ refers to a type of signal whose power distribution as afunction of frequency obeys an inverse power law in which the exponent is a small number. A 1/f signal is not random noise(random noise would result in an exponent of zero; i.e., the power spectrum of a random noise source is flat). On the other hand,there is some stochastic component to 1/f spectra as well as some correlation between signals at different nonadjacent times (i.e.,1/f noise exhibits some degree of long-range correlation). Similar statistical analyses have been applied to spatial structures, suchas DNA, although power and frequency are replaced by frequency of base-pair occurrence and spatial interval, respectively (see,
for example, A.M. Selvam, ÒQuantumlike Chaos in the Frequency Distributions of the Bases A, C, G, T in Drosophila DNA,ÓAPEIRON 9(4):103-148, 2002; W. Li, T.G. Marr, and K. Kaneko, ÒUnderstanding Long-range Correlations in DNA Sequences,ÓPhysica D 75(1-3):392-416, 1994 [erratum:82, 217,1995]). 1/f spectra have been found in the temporal fluctuations of many biologi-cal processes, including ion channel kinetics, auditory nerve firings, lung inflation, fetal breathing, human cognition, walking,blood pressure, and heart rate. (See J.M. Hausdorff and C.K. Peng, ÒMultiscaled Randomness: A Possible Source of 1/f Noise inBiology,Ó Physical Review E 54(2):2154-2157, 1996, and references therein. Hausdorff and Peng suggest that if the time scales of theinputs affecting a biological system are ÒstructuredÓ and there are a large number of inputs, it is very likely that the output willexhibit 1/f spectra, even if individual input amplitudes and time scales are loosely correlated.)69D.P. Feldman and J.P. Crutchfield, ÒMeasures of Statistical Complexity: Why?,Ó Physics Letters A 238:244-252, 1997.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.330CATALYZING INQUIRYdegree of structure or pattern in that entity; and physical complexity, which interprets the sharedKolmogorov complexity of an ensemble of sequences as information stored in the genome about the
environment. This last makes the interesting point that one cannot know anything about the meaning of
a DNA sequence without considering the environment in which the corresponding organism is ex-
pected to live.All of these capture some aspect of the way in which complexity might arise over time through anundirected evolutionary process and be stored in the genome of a species. However, in their physics-
inspired search for minimal descriptions, they may be missing the fact that evolution does not produce
optimal or minimal descriptions. That is, because biological organisms are the result of their evolution-
ary histories, they contain many remnants that are likely to be irrelevant to their current environmental
niches, yet contribute to their complexity. Put differently, any given biological organism is almost
certainly not optimized to perform the functions of which it is capable.Another difficulty with many of these measuresÕ application to biology is that, regardless of theirtheoretical soundness, they will almost certainly be hard to determine empirically. More prosaically,
they often involve a fair amount of mathematics or theoretical computational reasoning (e.g., to what
level of the Chomskian hierarchy of formal languages does this sequence belong?) completely outside
the experience of the majority of biologists. Regardless, this is an area of active research, and further
integration with actual biological investigation is likely to produce further progress in identifying
accurate and useful measures of complexity.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE33133110Culture and Research Infrastructure
Earlier chapters of this report have focused on what might be achieved experimentally and on thescientific and technical hurdles that must be overcome at the interface of biology and computing. This
chapter focuses on the infrastructural underpinnings needed to support research at this interface. Note
that because the influence of computing on biology has been much more significant than the influence
of biology on computing, the discussion in this chapter is focused mostly on the former.10.1  SETTING THE CONTEXT
In 1991, Walter Gilbert sketched a vision of 21st century biology (described in Chapter 1) and notedthe changes in intellectual orientation and culture that would be needed to realize that vision. He wrote:To use [the coming] flood of [biological] knowledge [i.e., sequence information], which will pour across thecomputer networks of the world, biologists not only must become computer-literate, but also change theirapproach to the problem of understanding life. . . . The next tenfold increase in the amount of information inthe databases will divide the world into haves and have-nots, unless each of us connects to that information
and learns how to sift through it for the parts we need. This is not more difficult than knowing how toaccess the scientific literature as it is at present, for even that skill involves more than a traditional reading ofthe printed page, but today involves a search by computer. . . . We must hook our individual computers into
the worldwide network that gives us access to daily changes in the database and also makes immediate ourcommunications with each other. The programs that display and analyze the material for us must beimprovedÑand we [italics added] must learn how to use them more effectively.1In short, Gilbert pointed out the need for institutional change (in the sense of individual life scien-tists learning to cooperate with each other) and for biologists to learn how to use the new tools of
information technology.Because the BioComp interface encompasses a variety of intellectual paradigms and disparateinstitutions, Section 10.2 describes the organizational and institutional infrastructure supporting work
at this interface, illustrating a variety of programs and training approaches. Section 10.3 addresses some1W. Gilbert, ÒToward a Paradigm Shift in Biology,Ó Nature 349(6305):99, 1991.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.332CATALYZING INQUIRYof the barriers that affect research at the BioComp interface. Chapter 11 is devoted to proposing possibleways of helping to reduce the negative impact of these barriers.10.2  ORGANIZATIONS AND INSTITUTIONS
Efforts to pursue research at the BioComp interface, as well as the parallel goal of attracting andtraining a sufficient workforce, are supported by a number of institutions and organizations in the
public and private sectors. A prime mover is the U.S. government, both by pursuing research in its own
laboratories, and by providing funding to other, largely academic, organizations. However, the govern-
ment is only a part of a larger web of collaborating (and competing) academic departments, private
research institutions, corporations, and charitable foundations.10.2.1  The Nature of the Community
The members of an established scientific community can usually be identified by a variety ofcommonalitiesÑfields in which their degrees were received, journals in which they publish, and so on.2The fact that important work at the BioComp interface has been undertaken by individuals who do not
necessarily share such commonalities indicates that the field in question has not jelled into a single
community, but in fact is composed of many subcommunities. Members of this community may come
from any of a number of specialized fields, including (but not restricted to) biology, computer science,
engineering, chemistry, mathematics, and physics. (Indeed, as the various epistemological and onto-
logical discussions of previous chapters suggest, even philosophers and historians of science may have
a useful role to play.)Because the intellectual contours of work at the intersection have not been well established, thedefinition of the community must be broad and is necessarily somewhat vague. Any definition must
encompass a multitude of cultures and types, leaving room for approaches that are not yet known.
Furthermore, the field is sufficiently new that people may enter it at many different stages of their
careers.For perspective, it is useful to consider some possible historical parallels with the establishment ofbiochemistry, biophysics, and bioengineering as autonomous disciplines. In each case, the phenomena
associated with life have been sufficiently complex and interesting to warrant the bringing to bear of
specialized expertise and intellectual styles originating in chemistry, physics, and engineering.
Nonbiologists, including chemists, physicists, and engineers, have made progress on some biologically
significant problems precisely because their approaches to problems differed from those of biologists
and thus have advanced biological understanding because they were not limited by what biologists felt
could not be understood. On the other hand, chemists, physicists, and engineers have also pursued
many false or unproductive lines of inquiry because they have not appreciated the complexity that
characterizes many biological phenomena or because they addressed problems that biologists already
regarded as solved. Eventually, biochemistry, biophysics, and bioengineering became established in
their own right as education and cultural inculcation from both parent disciplines came to be required.It is also to be expected that the increasing integration of computing and information into biologywill raise difficult questions about the nature of biological research and science. If an algorithm to
examine the phylogenetic tree of life is too slow to run on existing hardware, clearly a new algorithm
must be developed. Does developing such an algorithm constitute biological research?  Indeed, modern

biology is sufficiently complex that many of the most important biological problems are not easily
tamed by existing mathematical theory, computational models, or computing technologies. Ultimately,
success in understanding biological phenomena will depend on the development and application of
new tools throughout the research process.2T.S. Kuhn, The Structure of Scientific Revolutions, Third Edition, University of Chicago Press, Chicago, IL, 1996.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE33310.2.2  Education and Training
Education, either formal or informal, is essential for practitioners of one discipline to learn aboutanother, and there are many different venues in which training for the BioComp interface may occur.
(Contrast this to a standard program in physics, for example, in which a very typical career path
involves an undergraduate major in physics, graduate education in physics culminating in a doctorate,
and a postdoctoral appointment in physics.)Reflecting this diversity, it is difficult to generalize about approaches toward academic training at theBioComp interface, since different departments and institutions approach it with varied strategies. One
main difference in approaches is whether the initiative for creating an educational program and the
oversight and administration of the program come from the computer science department or the biology
department. Other differences include whether it is a stand-alone program or department, or a concentra-
tion or interdisciplinary program that requires a student or researcher to have a ÒhomeÓ department as
well, and whether the program was established primarily as a research program for postdoctoral fellows
and professors (and is slowly trickling down to undergraduate and graduate education), or as an under-
graduate curriculum that is slowly building its way up to a research program. Those differences in origin
result in varying emphases on what constitutes core subject matter, whether interdisciplinary work is
encouraged and how it is handled, and how research is supported and evaluated.What is clear is that this is an active area of development and investment, and many major collegesand universities have a formal educational program of some sort at the BioComp interface (generally in
bioinformatics or computational biology) or are in the process of developing one. Of course, there is not
yet widespread agreement on what the curriculum for this new course of study should be3 or indeed ifthere should be a single, standard, curriculum.10.2.2.1  General Considerations
As a general rule, serious work at the BioComp interface requires knowledge of both biology andcomputing. For example, many models and simulations of biological phenomena are constrained by
lack of quantitative data. The paucity of measurements of in vivo rates or parameters associated with
dynamics means that it is difficult to understand systems from a dynamic, rather than a static, point of
view. For example, to further the use of biological modeling and simulation, kinetics should be an
important part of early biological courses, including biochemistry and molecular biology, to instill an
appreciation in experimental biologists that kinetics is important. The requisite background in quantita-
tive methods is likely to include some nontrivial exposure to continuous mathematics, nonlinear dy-
namics, linear algebra, probability and statistics, as well as computer programming and algorithm
design.From the engineering side, few nonbiologists get any exposure to biological laboratory research ordevelop an understanding of the collection and analysis of biological data. This also leads to unrealistic
expectations of what can be done practically, how repeatable (or unrepeatable) a set of experiments can
be, and how difficult it can be to understand the system in detail. Computer scientists also require
exposure to probability, statistics, laboratory technique, and experimental design in order to under-
stand the biologistÕs empirical methodology. More fundamentally, nonbiologists working at the
BioComp interface must have an understanding of the basic principles relevant to the biological prob-
lem domains of interest, such as physiology, phylogeny, or proteomics. (A broad perspective on biol-
ogy, including some exposure to evolution, ecosystems, and metabolism, is certainly desirable, but is
likely not absolutely necessary.)Finally, it must be noted that many students choose to study biology because it is a science whosestudy has traditionally not involved mathematics to any significant extent. Similarly, W. Daniel Hillis3R. Altman, ÒA Curriculum for Bioinformatics: The Time Is Ripe,Ó Bioinformatics 14(7):549-550, 1998.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.334CATALYZING INQUIRYhas noted that Òbiologists are biologists because they love living things. A computation is not alive.Ó4Indeed, this has been true for several generations of students, so that many of these same students arenow incumbent instructors of biology. Managing this particular problem will pose many challenges.10.2.2.2  Undergraduate Programs
The primary rationale for undergraduate programs at the BioComp interface is that the under-graduate years of university education in the sciences carry the greatest burden in teaching a student
the professional language of a science and the intellectual paradigms underlying the practice of that
science. The term ÒparadigmÓ is used here in the original sense first expounded by Kuhn, which
includes the following:5¥Symbolic generalizations, which the community uses without question,¥Beliefs in particular models, which help to determine what will be accepted as an explanation ora solution,¥Values concerning prediction (e.g., predictions must be accurate, quantitative) and theories (e.g.,theories must be simple, self-consistent, plausible, compatible with other theories in current use), and¥Exemplars, which are the concrete problem solutions that students encounter from the start oftheir scientific education.The description in Section 10.3.1 suggests that the disciplinary paradigm of biology is significantlydifferent from that of computer science. Because the de novo learning of one paradigm is easier than
subsequently learning a second paradigm that may (apparently) be contradictory or incommensurate
with one that has already been internalized, the argument for undergraduate exposure is based on the
premise that simultaneous exposure to the paradigms of two disciplines will be more effective than
sequential exposure (as would be the case for someone receiving an undergraduate degree in one field
and then pursuing graduate work in another).Undergraduate programs in most scientific courses of study are generally designed to preparestudents for future academic work in the field. Thus, the goal of undergraduate curricula at the BioComp
interface is to expose students to a wide range of biological knowledge and issues and to the intellectual
tools and constructs of computing such as programming, statistics, algorithm design, and databases.
Today, most such programs focus on bioinformatics or computational biology, and in the most typical
cases, the integration of biology and computing occurs later rather than earlier in these programs (e.g.,
as senior-year capstone courses).Individual programs vary enormously in the number of computer science classes required. Forexample, the George Washington University Department of Computer Science offers a concentration in
bioinformatics leading to a B.S. degree; the curriculum includes 17 computer science courses and 4
biology courses, plus a single course on bioinformatics. The University of California, Los Angeles
(UCLA) program in cybernetics offers a concentration in bioinformatics, in contrast, in which the stu-
dents can take as few as seven computer science courses, including four programming classes and two
biology-themed classes. In other cases, a university may have an explicit undergraduate major in
bioinformatics associated with a bioinformatics department. Such programs are traditionally structured
in the sense of having a set of specific courses required for matriculation.In addition to concentrations at the interface, a number of other approaches have been used toprepare undergraduates:¥An explicitly interdisciplinary B.S. science program can expose students to the interrelationshipsof the basic sciences. Sometimes these are co-taught as single units: students in their first year may take4W.D. Hillis, ÒWhy Physicists Like Models, and Biologists Should,Ó Current Biology 3(2):79-81, 1993.5T.S. Kuhn, The Structure of Scientific Revolutions, Third Edition, University of Chicago Press, Chicago, 1996.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE335mathematics, physics, chemistry, and biology as a block, taught by a team of dedicated professors.Modules in which ideas from one discipline are used to solve problems in another are developed and
used as case studies for motivating the connections between the topics. Other coordinated science
programs intersperse traditional courses in the disciplines with co-taught interdisciplinary courses
(Examples: Applications of Physical Ideas in Biological Systems; Dimensional Analysis in the Sciences;
Mathematical Biology).¥A broad and unrestricted science program can allow students to count basic courses in anydepartment toward their degree or to design and propose their personal degree program. Such a system
gives graduates an edge in the ability to transcend boundaries between disciplines. A system of co-
advising to help students balance needs with interests would be vital to ensure that such open programs
function well.¥Courses in quantitative science with explicit ties to biology may be more motivating to biologystudents. Some anecdotal evidence indicates that biology students can do better in math and physics
when the examples are drawn from biology; at the University of Washington, the average biology
studentÕs grade in calculus rose from C to B+ when Òcalculus for biologistsÓ was introduced.6 (
Note thatsuch an approach requires that the instructor have the knowledge to use plausible biological examplesÑ
a point suggesting that simply handing off modules of instruction will not be successful.)¥Summer programs for undergraduates offer undergraduates an opportunity to get involved inactual research projects while being exposed to workshops and tutorials in a range of issues at the
BioComp interface. Many such programs are funded by a National Science Foundation or National
Institutes of Health program.7When none of these options are available, a student can still create a program informally (either onhis or her initiative or with the advice and support of a sympathetic faculty member). Such a program
would necessarily include courses sufficient to impart a thorough quantitative background (mathemat-
ics, physics, computer science) as well as a solid understanding of biology. As a rule, quantitative
training should come first, because it is often difficult to develop expertise in quantitative approaches
later in the undergraduate years. Exposure to intriguing ideas in biology (e.g., in popular lecture series)
would also help to encourage interest in these directions.Finally, an important issue at some universities is the fact that computer science departments andbiology departments are located in different schools (school of engineering versus school of arts and
sciences). As a result, biology majors may well face impediments to enrolling in courses intended for
computer science majors, and vice versa. Such a structural impediment underlines both the need and
the challenges for establishing a biological computing curriculum.10.2.2.3  The BIO2010 Report
In July 2003, the National Research Council (NRC) released Bio 2010: Undergraduate Education toPrepare Biomedical Research Scientists (National Academies Press, Washington, DC). This report con-cluded that undergraduate biology education had not kept pace with computationally driven changes
in life sciences research, among other changes, and recommended that mathematics, physics, chemistry,
computer science, and engineering be incorporated into the biology curriculum to the point that inter-
disciplinary thinking and work become second nature for biology students. In particular, the report
noted Òthe importance of building a strong foundation in mathematics, physical and information sci-
ences to prepare students for research that is increasingly interdisciplinary in character.ÓThe report elaborated on this point in three other recommendationsÑthat undergraduate life sci-6Mary Lidstrom, University of Washington, personal communication, August 1, 2003.7See http://www.nsf.gov/pubs/2002/nsf02109/nsf02109.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.336CATALYZING INQUIRYence majors should be exposed to engineering principles and analysis, should receive quantitativetraining in a manner integrated with biological content, and should develop enough familiarity with
computer science that they can use information technology effectively in all aspects of their research.10.2.2.3.1 EngineeringIn arguing for exposure to engineering, the report noted that the notion of func-tion (of a device or organism) is common to both engineering and biology, but not to mathematics,
physics, or chemistry. Echoing the ideas described in Chapter 6 of this report, BIO2010 concluded:Understanding function at the systems level requires a way of thinking that is common to many engi-neers. An engineer takes building blocks to build a system with desired features (bottom-up). Creating
(or re-creating) function by building a complex system, and getting it to work, is the ultimate proof thatall essential building blocks and how they work in synchrony are truly understood. Getting a system towork typically requires (a) an understanding of the fundamental building blocks, (b) knowledge of the
relation between the building blocks, (c) the systemÕs design, or how its components fit together in aproductive way, (d) system modeling, (e) construction of the system, and (f) testing the system and itsfunction(s). . . . Organisms can be analyzed in terms of subsystems having particular functions. To under-
stand system function in biology in a predictive and quantitative fashion, it is necessary to describe andmodel how the system function results from the properties of its constituent elements.The pedagogical conclusion was clear in the report:Understanding cells, organs, and finally animals and plants at the systems level will require that thebiologist borrow approaches from engineering, and that engineering principles are introduced early in
the education of biologists. . . . Students should be frequently confronted throughout their biology curric-ulum with questions and tasks such as how they would design Ôxxx,Õ and how they would test to seewhether their conceptual design actually works. [For example,] they should be asked to simulate their
system, determine its rate constants, determine regimes of stability and instability, investigate regulatoryfeedback mechanisms, and other challenges.A second dimension in which engineering skills can be useful is in logistical planning. There aremany areas in biology now where it is relatively easy to conceive of an important experiment, but
drawing out the implications of the experiment involves a combinatorial explosion of analytical effort
and thus is not practical to carry out. It is entirely plausible that many important biological discoveries
will depend on both the ability to conceive an experiment and the ability to reconceive and restructure
it logistically so that it is, in fact, doable. Engineers learn to apply their fundamental scientific knowl-
edge in an environment constrained by nonscientific concerns, such as cost or logistics, and this ability
will be critically important for the biologist who must undertake the restructuring described above.
Box 10.1 provides a number of examples of engineering for life science majors.10.2.2.3.2  Quantitative Training
  In its call for greater quantitative training, the BIO2010 report echoedthat of other commentators.8 Recognizing that quantitative analysis, modeling, and prediction playimportant roles in todayÕs biomedical research (and will do so increasingly in the future), the report
noted the importance to biology students of understanding concepts such as rate of change, modeling,
equilibrium, and stability, structure of a system, and interactions among components, and argued that
every student should acquire the ability to analyze issues arising in these contexts in some depth, using
analytical methods (including paper-and-pencil techniques) and appropriate computational tools. As
part of a necessary background, the report suggested that an appropriate course of study would include
aspects of probability, statistics, discrete models, linear algebra, calculus and differential equations,
modeling, and programming (Box 10.2).8See for example, A. Hastings and M.A. Palmer, ÒA Bright Future for Biologists and Mathematicians,Ó Science 299(5615):2003-2004, 2003, available at http://www.biosino.org/bioinformatics/a%20bright%20future.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE33710.2.2.3.3  Computer Science
  Finally, the BIO2010 report noted the importance of information technol-ogy-based tools for biologists. It recommended that all biology majors be able to develop simulations of
physiological, ecological, and evolutionary processes; to modify existing applications as appropriate; to
use computers to acquire and process data; to carry out statistical characterization of the data and
perform statistical tests; to graphically display data in a variety of representation; and to use informa-
tion technology (IT) to carry out literature searches, locate published articles, and access major data-Box 10.1Engineering for Life Science MajorsOne example of an engineering topic suitable for inclusion in a biology curriculum is the subject of long-rangeneuron signals. Introducing such a topic might begin with the electrical conductivity of salt water and of the lipidcell membrane, and the electrical capacitance of the cell membrane. It would next develop the simple equationsfor the attenuation of a voltage applied across the membrane at one end of an axon ÒcylinderÓ with distance
down the axon, and the effect of membrane capacitance on signal dynamics for time-varying signals.After substituting numbers, it becomes clear that amplifiers will be essential. On the other hand, real systemsare always noisy and imperfect; amplifiers have limited dynamical range; and the combination of these factsmakes sending an analog voltage signal through a large number of amplifiers essentially impossible.The pulse coding of information overcomes the limitations of analog communication. How are ÒpulsesÓgenerated by a cell? This would lead to the power supply needed by an amplifierÑion pumps and the Nernstpotential. How are action potentials generated? A first example of the transduction of an analog quantity into
pulses might be stick-slip fraction, in which a block resting on a table, and pulled by a weak spring whose endis steadily moved, moves in ÒjumpsÓ whose distance is always the same. This introduction to nonlineardynamics contains the essence of how an action potential is generated.The Ònegative resistanceÓ of the sodium channels in a neuron membrane provides the same kind of Òbreak-downÓ phenomenon. Stability and instabilities (static and dynamic) of nonlinear dynamical systems can be
analyzed, and finally the Hodgkin-Huxley equations illustrated.The material is an excellent source of imaginative laboratories involving electrical measurements, circuits,dynamical systems, batteries and the Nernst potential, information and noise, and classical mechanics. It hasgreat potential for simulations of systems a little too complicated for complete mathematical analysis, and thusis ideal for teaching simulation as a tool for understanding.Other biological phenomena that can be analyzed using an engineering approach and that are suitable forinclusion in a biology curriculum include the following:¥The blood circulatory system and its control; fluid dynamics; pressure and force balance;¥Swimming, flying, walking, dynamical description, energy requirements, actuators, control; material prop-erties of biological systems and how their structure relates to their function (e.g., wood, hair, cell membranecartilage);¥Shapes of cells: force balance, hydrostatic pressure, elasticity of membrane and effects of the spatial depen-dence of elasticity; effects of cytoskeletal force on shape; and¥Chemical networks for cell signaling; these involve the concepts of negative feedback, gain, signal-to-noise, bandwidth, and cross-talk. These concepts are simple to experience in the context of how an electrical
amplifier can be built from components.SOURCE: Adapted from National Research Council, BIO2010: Transforming Undergraduate Education for Future Research Biologists, TheNational Academies Press, Washington, DC, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.338CATALYZING INQUIRYBox 10.2Essential Concepts of Mathematics and Computer Science for Life ScientistsCalculus¥Complex numbers¥Functions¥Limits¥Continuity¥The integral¥The derivative and linearization¥Elementary functions¥Fourier series¥Multidimensional calculus: linear approximations, integration over multiple variablesLinear Algebra¥Scalars, vectors, matrices¥Linear transformations¥Eigenvalues and eigenvectors¥Invariant subspacesDynamical Systems¥Continuous time dynamicsÑequations of motion and their trajectories¥Test points, limit cycles, and stability around them¥Phase plane analysis¥Cooperativity, positive feedback, and negative feedback¥Multistability¥Discrete time dynamicsÑmappings, stable points, and stable cycles¥Sensitivity to initial conditions and chaosProbability and Statistics¥Probability distributions¥Random numbers and stochastic processes¥Covariation, correlation, and independence¥Error likelihoodInformation and Computation¥Algorithms (with examples)¥Computability¥Optimization in mathematics and computation¥ÒBitsÓ: information and mutual informationData Structures¥Metrics: generalized ÒdistanceÓ and sequence comparisons¥Clustering¥Tree relationships¥Graphics: visualizing and displaying data and models for conceptual understandingSOURCE: Reprinted from National Research Council, BIO2010: Transforming Undergraduate Education for Future Research Biologists,The National Academies Press, Washington, DC, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE339bases. From the perspective of this report, Box 10.3 describes some of the essential intellectual aspects ofcomputer science that biologists must understand.Recognizing that students might require competence at multiple levels depending on their needs,the BIO2010 report identified three levels of competence as described in Box 10.4.Box 10.3Essential Concepts of Computer Science for the Biologist Key for the computer scientist is the notion of a field that focuses on information, on understanding ofcomputing activities through mathematical and engineering models and based on theory and abstraction, onthe ways of representing and processing information, and on the application of scientific principles andmethodologies to the development and maintenance of computer systemsÑwhether they are composed of
hardware, software, or both.There are many views of understanding the essential concepts of computer science. One view, developed in1991 in the NRC report Computing the Future, is that the key intellectual themes in computing are algorithmicthinking, the representation of information, and computer programs.1¥An algorithm is an unambiguous sequence of steps for processing information. Of particular relevance ishow the speed of the algorithm varies as a function of problem sizeÑthe topic of algorithmic complexity.Typically, a result from algorithmic complexity will indicate the scaling relationships between how long it
takes to solve a problem and the size of the problem when the solution of the problem is based on a specificalgorithm. Thus, algorithm A might solve a problem in a time of order N2, which means that a problem that is100 times as large would take 1002 = 10,000 times as long to solve, whereas a faster algorithm B might solvethe same problem in time of order N ln N, which means a problem 100 times as large would take 100 ln 100= 460.5 times as long to solve. Such results are important because all computer programs embed algorithmswithin them. Depending on the functional relationship between run time and problem size, a given program
that works well on a small set of test data mayÑor may notÑwork well (run in a reasonable time) for a largerset of real data. Theoretical computer science thus imposes constraints on real programs that software devel-opers ignore at their own peril.
¥The representation of information or a problem in an appropriate manner is often the first step in designing analgorithm, and the choice of one representation or another can make a problem easy or difficult, and its solutionslow or fast. Two issues arise: (1) how should the abstraction be represented, and (2) how should the represen-
tation be structured properly to allow efficient access for common operations? For example, a circle of radius 2can be represented by an equation of the form x2 + y2 = 4 or as a set of points on the circle ((0.00, 2.00), (0.25,1.98), (0.50, 1.94), (0.75, 1.85), (1.00, 1.73), (1.25, 1.56), (1.50, 1.32), (1.75, 0.97), (2.00, 0.00)), and so on.
Depending on the purpose, one or the other of these representations may be more useful. If the circle of radius2 is just a special case of a problem in which circles of many different radii are involved, representation as anequation may be more appropriate. If many circles of radius 2 have to be drawn on a screen and speed is
important, a listing of the points on the circle may provide a faster basis for drawing such circles.¥A computer program expresses algorithms and structure information using a Òprogramming language.ÓSuch languages provide a way to represent an algorithm precisely enough that a Òhigh-levelÓ description (i.e.,
one that is easily understood by humans) can be translated mechanically (ÒcompiledÓ) into a Òlow-levelÓversion that the computer can carry out (ÒexecuteÓ); the execution of a program by a computer is what allowsthe algorithm to be realized tangibly, instructing the computer to perform the tasks the person has requested.
Computer programs are thus the essential link between intellectual constructs such as algorithms and informa-tion representations and the computers that perform useful tasks.continued1The discussion below is adapted from Computer Science and Telecommunications Board, National Research Council, Computing theFuture: A Broader Agenda for Computer Science and Engineering, National Academy Press, Washington, DC, 1992.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.340CATALYZING INQUIRYBox 10.3 ContinuedThis last point is often misunderstood. For many outsiders, computer science is the same as computer pro-grammingÑa view reinforced by many introductory Òcomputer scienceÓ courses that emphasize the writing
of computer programs. But it is better to understand computer programs as the specialized medium in whichthe ideas and abstractions of computer science are tangibly manifested. Focusing on the writing of the com-puter program without giving careful consideration to the abstractions embodied in the program is not unlike
understanding the writing of a novel as no more than the rules of grammar and spelling.Algorithmic thinking, information representation, and computer programs are themes central to all subfieldsof computer science and engineering research. They also provide material for intellectual study in and ofthemselves, often with important practical results. The study of algorithms is as challenging as any area ofmathematics, and one of practical importance as well, since improperly chosen or designed algorithms may
solve problems in a highly inefficient manner. The study of programs is a broad area, ranging from the highlyformal study of mathematically proving programs correct to very practical considerations regarding tools withwhich to specify, write, debug, maintain, and modify very large software systems (otherwise called software
engineering). Information representation is the central theme underlying the study of data structures (howinformation can best be represented for computer processing) and much of human-computer interaction (howinformation can best be represented to maximize its utility for human beings).Finally, computer science is closely tied to an underlying technological substrate that evolves rapidly. Thissubstrate is the ÒstuffÓ out of which computational hardware is made, and the exponential growth that char-
acterizes its evolution makes it possible to construct ever-larger, ever-more-complex systemsÑsystems thatare not predictable based on an understanding of their individual components. (As one example, the proper-ties of the Internet prove a rich and surprisingly complex area of study even though its componentsÑcomput-
ers, routers, fiber-optic cablesÑare themselves well understood.)A second report of the National Research Council described fluency with information technology as requiringthree kinds of knowledge: skills in using contemporary IT, foundational concepts about IT and computing, andintellectual capabilities needed to think about and use IT for purposeful work.2 The listing below is theperspective of this report on essential concepts of IT for everyone:¥Computers (e.g., programs as a sequence of steps, memory as a repository for program and data, overallorganization, including relationship to peripheral devices).
¥Information systems (e.g., hardware and software components, people and processes, interfaces (bothtechnology interfaces and human-computer interfaces), databases, transactions, consistency, availability, per-sistent storage, archiving, audit trails, security and privacy and their technological underpinnings).
¥Networks: physical structure (messages, packets, switching, routing, addressing, congestion, local areanetworks, wide area networks, bandwidth, latency, point-to-point communication, multicast, broadcast, Eth-ernet, mobility), and logical structure (client/server, interfaces, layered protocols, standards, network services).
¥Digital representation of information: concept of information encoding in binary form; different informa-tion encodings such as ASCII, digital sound, images, and video/movies; precision, conversion and interoper-ability (e.g., of file formats), resolution, fidelity, transformation, compression, and encryption; standardization
of representations to support communication.¥Information organization (including forms, structure, classification and indexing, searching and retrieving,assessing information quality, authoring and presentation, and citation; search engines for text, images, video,
audio).¥Modeling and abstraction: methods and techniques for representing real-world phenomena as computermodels, first in appropriate forms such as systems of equations, graphs, and relationships, and then in appro-
priate programming objects such as arrays or lists or procedures. Topics include continuous and discrete2Computer Science and Telecommunications Board, National Research Council, Being Fluent with Information Technology, NationalAcademy Press, Washington, DC, 1999.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE34110.2.2.4  Graduate Programs
Graduate programs at the BioComp interface are often intended to provide B.S. graduates in onediscipline with the complementary expertise of the other. For example, individuals with bachelorÕs
degrees in biology may acquire computational or analytical skills during early graduate school, with
condensed ÒretrainingÓ programs that expose then to nonlinear dynamics, algorithms, and so on. Alter-
natively, individuals with bachelorÕs degrees in computer science might take a number of courses to
expose them to essential biological concepts and techniques.Graduate education at the interface is much more diverse than at the undergraduate level. Al-though there is general agreement that an undergraduate degree should expose the student to the
component sciences and prepare him or her for future work, the graduate degree involves a far wider
array of goals, focuses, fields, and approaches. Like undergraduate programs, graduate programs can
be stand-alone departments, independent interdisciplinary programs, or certificate programs that re-
quire students to have a ÒhomeÓ department.A bioinformatics program oriented toward genomics is very common. Virginia TechÕs program, forexample, has been renamed the program in ÒGenetics, Bioinformatics, and Computational Biology,Ó
indicating its strong focus on genetic analysis. In contrast, the Keck Graduate Institute at Claremont
stresses the interdisciplinary skill set necessary for the effective management of companies that straddle
the biology-quantitative science boundary. It awards a masterÕs of bioscience, a professional degreemodels, discrete time events, randomization, and convergence, as well as the use of abstraction to hideirrelevant detail.
¥Algorithmic thinking and programming: concepts of algorithmic thinking, including functional decomposi-tion, repetition (iteration and/or recursion), basic data organization (record, array, list), generalization andparameterization, algorithm vs. program, top-down design, and refinement.
¥Universality and computability: ability of any computer to perform any computational task.¥Limitations of information technology: notions of complexity, growth rates, scale, tractability, decidability,and state explosion combine to express some of the limitations of information technology; connections to
applications, such as text search, sorting, scheduling, and debugging.¥Societal impact of information and information technology: technical basis for social concerns about priva-cy, intellectual property, ownership, security, weak/strong encryption, inferences about personal characteris-
tics based on electronic behavior such as monitoring Web sites visited, Ònetiquette,Ó Òspamming,Ó and freespeech in the Internet environment.A third perspective is provided by Steven Salzberg, senior director of bioinformatics at the Institute for Genom-ic Research in Rockville, Maryland. In a tutorial paper for biologists, he lists the following areas as importantfor biologists to understand:3¥Basic computational concepts (algorithms, program execution speed, computing time and space require-ments as a function of input size; really expensive computations),
¥Machine learning concepts (learning from data, memory-based reasoning),¥Where to store learned knowledge (decision trees, neural networks),¥Search (defining a search space, search space size, tree-based search),¥Dynamic programming, and¥Basic statistics and Markov chains.3S.L. Salzberg, ÒA Tutorial Introduction to Computation for Biologists,Ó Computational Methods in Molecular Biology, S.L. Salzberg, D.Searls, and S. Kasif, eds., Elsevier Science Ltd., New York, 1998.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.342CATALYZING INQUIRYsomewhat like an M.B.A. with a science requirement. Some programs, such as StanfordÕs, are adminis-tered by the medical school, leading to a focus on medical informatics as well as bioinformatics. This
would include topics such as clinical trials and image analysis, which would not show up in a more
traditional genomics-focused bioinformatics degree.The Research Training Program of the Keck Center for Computational and Structural Biology isintended to develop one of two different kinds of expertise. Emerging from this program, a trainee
would be a computational expert well versed in computer science and quantitative methods who
would also be knowledgeable in at least one application area of biological significance, or an expert in
some biological area (e.g., molecular biology) who would also be aware of the most advanced concepts
in computing. Students entering from computational backgrounds take at least three courses in biol-
ogy-biochemistry-biophysics areas, while students entering from biological backgrounds at least three
courses in computational areas. In addition, all students take an introductory course in computational
science. Dissertation research is supervised by a committee with faculty members as required by
the studentÕs home department, but with representation from the computational biology faculty
at other Keck Center institutions as well. Research can be undertaken in areas including the visualiza-
tion of biological complexes, the development of DNA and protein sequence analysis, and advanced
simulations.Box 10.4Competence and Expertise in Computer Science for Biology StudentsThe BIO2010 report recommended that all biology students receive instruction in computer science, distin-guishing among three levels of competency. From lowest to highest, these include the following:¥Fluency. Based on the NRC report Being Fluent with Information Technology, fluency refers to the abilityof biology students to use information technology today and to adapt to changes in IT in the future. Forexample, they need a basic understanding of how computers work and of programming, and a higher degree
of fluency in using networks and databases. Students should also be exposed to laboratory experiences usingMEDLINE, GenBank, and other biological databases, as well as physiological and ecological simulations. Forexample, students could be asked to use computer searches to track down all known information about a
given gene and the protein it encodes, including both structure and function. This would involve exploring theinternal structure of the gene (exons, introns, promoter, transcription factor binding sites); the regulatorycontrol of the gene; sequence homologues of the gene and the protein; the structure and function of the
protein; gene interaction networks and metabolic pathways involving the protein; and interactions of theprotein with other proteins and with small molecules.¥Capability in program design for computational biology and genomics applications. Students at this levelacquire the minimal skills required to be effective computer users within a computationally oriented biologyresearch team. For example, they would learn structured software development and selected principles ofcomputer science, with applications in computational biology and allied disciplines, and would use examples
and tutorials drawn from problems in computational biology.¥Capability in developing software tools for use by the biology community. At this sophisticated level,students need a grounding in discrete mathematics, data structures, and algorithms, as well as database man-
agement systems, information systems, software engineering, computer graphics, or computer simulationtechniques. Students at this level would be able to design and specify database and information systems foruse by the entire community. Of special interest will be tools that require background in graph theory, com-
binatorics, and computational geometry as applications in high-throughput genomics research and rationaldrug design become increasingly important.SOURCE: Adapted from National Research Council, BIO2010: Transforming Undergraduate Education for Future Research Biologists, TheNational Academies Press, Washington, DC, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE343A challenge for a field as interdisciplinary as this is that incoming students will arrive with possiblycompletely non-overlapping backgrounds. Most programs accept a B.S. in computer science, biology, or
math as a prerequisite; to produce a well-rounded computational biologist will require very different
training programs. The University of ColoradoÕs certificate program in computational biology requires
incoming students to take preparatory classes in ÒBiology for Computer Scientists,Ó ÒComputer Science
for Bioscientists,Ó or ÒMathematics for Bioscientists,Ó depending on what the student missed earlier in
his or her education.An advantage of graduate programs is that when communication among faculty of different disci-plines is good, graduate projects provide an ideal opportunity for students to work in an interdiscipli-
nary environment. In some cases, work with adjunct professors from industry can lead to exciting
projects. On the other hand, if communication between faculty is poor (which may be possible for
reasons described later in this chapter), a graduate student dependent on completing a project (e.g., a
dissertation) can get caught in the middle of a dispute with no way to graduate.10.2.2.5  Postdoctoral Programs
Postdoctoral programs at the BioComp interface are also varied. Some postgraduate programs areexplicitly aimed at Òconversion,Ó that is, training a fully trained member of one field (usually biology) in
the basic tenets of its complement. For example, the University of PennsylvaniaÕs postdoctoral program
in computational biology is a masterÕs degree in computer and information systems, designed for those
with Ph.D.s in biology who need the training. Other programs focus on involving the participant in
research and laboratory work, in preparation for industry or a faculty position, just as in postdoctoral
programs in other fields.Some programs, such as DukeÕs Center for Bioinformatics and Computational Biology, are similarto graduate programs in that they focus on genome analysis. Others, like the Johns HopkinsÕ program in
computational biology, are firmly grounded in genomics but are pointedly reaching out to larger
questions of integrative biology and experimental biology.In promoting postdoctoral programs at the interface of computing and biology, it will be necessaryto take into account the very different traditions of the two fields. In biology, one or more postdoctoral
fellowships are quite common (indeed, routine) before an individual strikes out on his or her own. By
contrast, the most typical career path for a newly graduated Ph.D. in computer science calls for appoint-
ment to a junior faculty position or a position in industryÑpostdoctoral fellows in computer science are
relatively rare (though not unheard of).Two foundation-supported postdoctoral programs have been influential in stimulating interest atthe BioComp interface: the Sloan-Department of Energy (DOE) program and the Burroughs-Welcome
program.10.2.2.5.1  The Sloan-DOE Postdoctoral Awards for Computational Molecular Biology
9  For 8 years,the Alfred P. Sloan Foundation and the U.S. Department of Energy (Office of Biological and Environ-
mental Research) have jointly sponsored postdoctoral research awards for scientists interested in com-
putational molecular biology. The purpose of these fellowships has been to catalyze career transitions
into computational molecular biology by those holding doctorates in mathematics, physics, computer
science, chemistry, engineering or other relevant fields who would like to bring their computational
sophistication to bear on the complex problems that increasingly face molecular biology.Operationally, the program was designed to offer computationally sophisticated young scientistsan intensive postdoctoral opportunity in an appropriate molecular biology laboratory. In most cases,
awardees had strong educational backgrounds in a computationally intensive field, although in a few9See http://www.sloan.org/programs/scitech_postdoct.shtml.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.344CATALYZING INQUIRYinstances, awardees had backgrounds from more traditional biological orientations without the compu-tational dimension. Of particular interest to the Sloan-DOE program are important problems in struc-
tural biology and genome analysis, including analysis of protein and nucleic acid sequence, protein and
nucleic acid structure, genome structure and maps, cross-species genome analysis, multigenic traits,
and structure-function relationships where the structures are from genomes, genes, or gene products.The Sloan-DOE postdoctoral award supports up to 2 years of research in an appropriate molecularbiology department or laboratory in the United States or Canada selected by the awardee. In magni-
tude, the award provides for a total budget of $120,000 (including indirect and overhead costs), spread
over a grant period of 2 years.10.2.2.5.2  The Burroughs-Wellcome Career Awards at the Scientific Interface
10  The Burroughs-Wellcome Career Awards at the Scientific Interface are intended to foster the early career development
of researchers with backgrounds in the physical and computational sciences whose work addresses
biological questions and who are dedicated to pursuing a career in academic research.11 Prospectiveawardees are expected to have Ph.D.-level training in a scientific field other than biology and are
encouraged to describe potential collaborations with well-established investigators working on inter-
face problems of interest.The program provides $500,000 over 5 years to support up to 2 years of advanced postdoctoraltraining and the first 3 years of a faculty appointment. In general, an awardee is expected to accept a
faculty position at an institution other than the one supporting the postdoc, a requirement that is likely
to spread the philosophy of interface research embodied in the program more effectively than the
publishing of papers or program descriptions.In addition, the Burroughs-Welcome Fund (BWF) requires the faculty-hiring institution to make asignificant commitment to the award recipientÕs career development, where Òsignificant commitmentÓ
is demonstrated by the financial and professional situation offered. Tenure-track faculty appointments
are strongly preferred, accompanied by salary support and/or support for starting up a laboratory.
Awardees are required to devote at least 80 percent of their time to research-related activities. Further-
more, the faculty-hiring institution must offer the awardee to take an adjunct appointment in a second
department and name at least one tenured faculty member in a discipline complementary to the
awardeeÕs primary discipline who is willing to serve as an active collaborator.10.2.2.5.3  Keck Center for Computational and Structural Biology: The Research Training Program
  TheW.M. Keck Center for Computational and Structural Biology is an interdisciplinary and interinstitu-
tional organization, including Baylor College of Medicine, the University of Houston, Rice University,
University of Texas Health Science Center, the M.D. Anderson Cancer Center, and University of Texas
Medical Branch at Galveston. Subareas of focus include computational methods and tools, biomolecular
structure and function, imaging and dynamics, mathematical modeling of biosystems, and medical and
genomic informatics. The faculty include some 130 members, drawn from member institutions, and a10See http://www.bwfund.org/programs/interfaces/career_awards_background.html.11A previous Burroughs-Wellcome Fund (BWF) program, known as Institutional Awards at the Scientific Interface, has beendiscontinued. (Together with the Career Awards program, it constituted the BWF Interfaces in Science effort.) The purpose of theInstitutional Awards program was to support U.S. and Canadian academic institutions in developing interdisciplinary graduateand postdoctoral training programs for individuals with backgrounds in the physical, computational, or mathematical sciences
to pursue biological questions. For example, pre- and postdoctoral fellows at the La Jolla Consortium and the University ofChicagoÕs Institute for Biophysical Dynamics had to propose research projects that required the participation of two mentorsÑone from the quantitative sciences and one from the biological sciencesÑbefore being awarded financial support. For more on
the Institutional Awards program, see N.S. Sung, J.I. Gordon, G.D. Rose, E.D. Getzoff, S.J. Kron, D. Mumford, J.N. Onuchic, et al.,ÒScience Education: Educating Future Scientists,Ó Science 301(5639):1485, 2003, available at http://www.bwfund.org/programs/interfaces/institutional_main.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE345few dozen predoctoral and postdoctoral fellows. The Keck Center was established in 1990 by a$5 million grant from the Keck foundation and currently receives more than $20 million annually in
grants, from agencies such as the National Institutes of Health, the National Science Foundation, the
Department of Defense, and private sources.The Keck CenterÕs training program, supported by the W.M. Keck Foundation and the NationalLibrary of Medicine, seeks to cross-train new scientists in both computational science and a specialized
area of biology so that they can shed new light on the cellular and molecular basis of biological pro-
cesses.12 Fellowships are supported for research in algorithm development, advanced computationalmethods, biomedicine, crystallography, electron cryomicroscopy and computer reconstruction, genome
studies, imaging and visualization, mathematical modeling of biosystems, medical informatics, neuro-
science, protein dynamics and design, robotics applications in molecular biology, and the structure and
function of biomolecules. The fellowship provides trainees with cross-training in computational science
and in biological applications, dual mentorship, and access to cutting-edge facilities.10.2.2.6  Faculty Retraining in Midcareer
Faculty training or retraining can augment the above opportunities. In some cases, this meansparticipation in workshops (given release time to allow for this investment), sabbaticals spent learning
a new subject, or explicitly switching from one field to another. As a rule, funded release time will be
necessary to provide a break from academic constraints and to offer the time and opportunity to see
biological work up close. In some cases, a good way to develop cross-disciplinary expertise is to spend
a sabbatical year in the laboratory of a colleague in another discipline.The committee was unable to find programs specifically oriented toward retraining computer scien-tists to do biological research. However, the National Science Foundation (NSF) does support the Interdis-
ciplinary Grants in the Mathematical Sciences program through its Mathematical and Physical Sciences
Directorate whose objective is Òto enable mathematical scientists to undertake research and study in
another discipline so as to expand their skills and knowledge in areas other than the mathematical
sciences, subsequently apply this knowledge in their research, and enrich the educational experiences and
broaden the career options of their students.Ó13 Recipients spend a year full-time (in a 12-month period) ina nonmathematical academic science department or in an industrial, commercial, or financial institution,
and the outcome is expected to be sufficient familiarity with another discipline on the part of the sup-
ported individual Òto open opportunities for effective collaboration by the mathematical scientist with
researchers in another discipline.Ó Applicants must have a tenured or tenure-track academic appointment,
and the proposal must include a co-principal investigator at the level of dean (or higher-level university
official) at the submitting institution as well as a commitment from the host institution or department that
the hosted individual will be treated as a regular faculty member within the host unit and that at least one
senior person will be provided who will serve as institutional host.In addition, the National Institutes of Health (NIHÕs) National Research Service Awards programfor Senior Fellows (F33) supports scientists from any field with 7 or more years of postdoctoral research
experience who wish to make major changes in the direction of their research careers or who wish to
broaden their scientific background by acquiring new research capabilities. In most cases, these awards
are used to support sabbatical experiences for established independent scientists in which they receive
training to increase their scientific capabilities. Such training must be within the scope of biomedical,
behavioral, or clinical research and must offer an opportunity for individuals to broaden their scientific
background or extend their potential for research in health-related areas. The maximum annual stipend
is considerably lower than senior scientists typically receive, but most awardees find supplements so
that they may obtain their full salaries while pursuing studies in a new field. The guidelines for eligibil-12See http://cohesion.rice.edu/centersandinst/keckcenter/training.cfm?doc_id=2368.13See http://www.nsf.gov/pubs/2001/nsf01115/nsf01115.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.346CATALYZING INQUIRYity specifically do not include previous experience in biomedical research; and thus, computer scientistswould be eligible for such a program.1410.2.3  Academic Organizations
Typically, academic research is conducted in departments or in centers that draw on faculty frommultiple departments. The descriptions of the three departments below are simply illustrative and not
exhaustive (no inference should be drawn from the fact that any given department or center is not
included below):¥Cornell University maintains four distinct programs in computational biology, three hosted by aparent discipline. Biological sciences, computer science, and mathematics all offer concentrations in
computational biology (the math department calls it Òmathematical biologyÓ). The only stand-alone
department is the Department of Biological Statistics and Computational Biology (BSCB), a part of the
College of Agriculture and Life Sciences. BSCB was originally the Department of Biometry and Biosta-
tistics, and in 2005, it has six tenure-track faculty (plus two emeritus professors), one nontenure-track
lecturer, and four ÒadjunctÓ faculty. There are 2 postdoctoral associates, 26 graduate students, and 65-70
undergraduate students. The department focuses mainly on biological statistics, computational biol-
ogy, and statistical genomics. Research interests of the faculty include statistical genomics, Bayesian
statistics, population genetics, epidemiology, modeling, molecular evolution, and experiment design.¥The University of California at Santa Cruz has a Department of Biomolecular Engineering, aninterdisciplinary department that contains research programs in bioinformatics and experimental
systems biology, among others. The bioinformatics program was originally administered by the
computer engineering department. In 2005, the program has nine core tenure-track faculty members,and one affiliated faculty member. The bioinformatics curriculum includes a core of bioethics, Baye-
sian statistics, molecular biology, biochemistry, computational analysis of proteins, and computa-
tional genomics. Electives are drawn from biology, chemistry, computer science, and applied math-
ematics and statistics.¥Carnegie Mellon University (CMU) has offered programs in computational biology (through itscomputer science, biology, mathematics, physics, and chemistry departments) since 1989. In 2005,
CMUÕs Department of Biological Sciences had 5 faculty involved in both computational biology and
bioinformatics and genomics, proteomics, and systems biology. The department offers a B.S. in compu-
tational biology, which consists largely of a traditional biological curriculum augmented with math,
programming, and computer science classes. In addition, students (B.S. or Ph.D.) can participate in the
interdepartmental Merck Computational Biology and Chemistry Program, which requires students to
have a home department in biology, computer science, statistics, math, or chemistry. This program was
established in 1999 with a grant from the Merck Company Foundation.Centers are often created without specific departmental affiliation because the number of depart-ments that might plausibly contribute expertise is large. In these instances, absent a center, it is difficult
to unify and coordinate research and educational activities or to convey to the outside world what the
university is doing in the area. Centers are intended to be focal points for research at the BioComp
interface (most often with a bioinformatics or computational biology flavor), and they usually work
with departments to make new faculty appointments and provide a single point for students to learn
about university programs.Four university-based centers are described below, simply as illustrative:14For more information, see http://grants.nih.gov/grants/guide/pa-files/PA-00-131.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE347¥The University of California-BerkeleyÕs Center for Integrative Genomics was founded in Decem-ber 2002, supported by the Gordon and Betty Moore Foundation.15 Its mission is to bring tools frommany disciplines to bear on problems at the intersection of evolution and developmental biology. The
enabling technology for new progress in this field will be acceleration of the sequencing of species
genomes, and it is hoped to sequence 100 genomes of various species in the next 5 years.16 The facultyincludes 20 researchers drawn from molecular cellular biology, integrative biology, statistics, plant and
microbial biology, mathematics, computer science, bioengineering, physics, paleontology, and the
Lawrence Berkeley National Laboratory. The center also plans to serve an educational role, teaching or
supporting the teaching of genomic science to computer science students and computer topics to biol-
ogy students, as well as providing a center for graduate and postgraduate work.¥The Vanderbilt Institute for Integrative Biosystem Research and Education (VIIBRE) at VanderbiltUniversity (Nashville, Tennessee) was begun with an initial grant from VanderbiltÕs Academic Venture
Capital Fund.17 VIIBRE has also received project-specific funding and other support from NSF, DARPA,NIH, and other institutions, enabling it to create centers of bioengineering education technologies and
to begin research in cellular instrumentation and control, biomedical imaging, technology-guided

therapy, biological applications of nanosystems, cellular and tissue bioengineering and biotechnology,
and bioengineering education technologies. Engineers, scientists, doctors, and mathematicians conduct
research for VIIBRE; more than 20 biological physics and bioengineering faculty in VanderbiltÕs College
of Arts and Science and the Schools of Engineering and Medicine participate in the program. VIIBRE is
also developing a postdoctoral training program for physical scientists and engineers who wish to
direct their careers toward the interface between biology, medicine, engineering, and the physical
sciences.¥The Computational and Systems Biology Initiative (CSBi) at the Massachusetts Institute of Tech-nology (MIT) is a campus-wide education and research program that links biologists, computer scien-
tists, and engineers in a multidisciplinary approach to the systematic analysis of complex biological
phenomena.18 CSBi places equal emphasis on computational and experimental methods and on mo-lecular and systems views of biological function. CSBi includes about 80 faculty members from more
than 10 academic units in science, engineering, and management. Overall, membership in CSBi is self-
determined, based on a self-identified interest in systems biology, and it is offered to faculty and
principal investigators, postdoctoral fellows, graduate students, and research staff.¥The Institute for Biophysical Dynamics at the University of Chicago19 is focused on interdiscipli-nary study of biological entities and is supported by the BWF program of Institutional Awards at the
Scientific Interface. Drawing on the biological and physical science divisions of the university, the
institute focuses on RNA-DNA structure, function, and regulation; protein dynamics, folding, and
engineering; cytoskeleton, membranes, and organelles; hormones and cell signaling; and cell growth,
death, and multicellular function. Physical scientists at the institute have expertise in macromolecular-
scale manipulation via optical tweezer and chemical means; biologically relevant model systems; mea-
surement of dynamics of macromolecules and assemblies on scales from femtoseconds to seconds;
theoretical and simulation methods; soft condensed matter theory of complex and analysis of nonlinear
dynamic phenomena.  Part of the instituteÕs mission is to establish cross-disciplinary training programs

for students. The essential feature of the program is the placement, on a competitive basis, of predoctoral
fellows with backgrounds in the physical sciences into biological science research groups, thereby15See http://www.moore.org/grantees/grant_summaries_content.asp?Grantee=ucb_cig.16G. Shiffrar, ÒNew Center for Integrative Genomics to Study Major Evolutionary Changes,Ó College News; see http://ls.berkeley.edu/new/02/cig.html.17See http://www.vanderbilt.edu/viibre/av-goal.html and http://www.physics.vanderbilt.edu/oldpurplesite/whatshot/newsletterwinter0102.html.18For more information, see http://csbi.mit.edu/whatis.19For more information, see http://ibd.uchicago.edu/.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.348CATALYZING INQUIRYformalizing interdisciplinary connections. Fellows participate in new Òtranslational core courses,Ó es-tablishing a common culture, and select an individualized program of additional coursework tailored to
their research and career goals. They also take a lead role in a weekly seminar-discussion program.Finally, in some cases centers are not associated with a specific university at all. Their purpose canbe to consolidate resources on a larger scale or simply to provide a congenial intellectual home for like-
minded individuals. Three nonuniversity centers are described below, again as illustrations only:¥Cold Spring Harbor Laboratory (CSHL) is a private research institution on Long Island, NewYork, that employs more than 800 people (300 classified as scientists) and has an annual budget of over
$120 million. CSHL was established in 1889 with missions in biological research and education. In 1993,
it began the annual Cold Spring Harbor Symposium on Quantitative Biology. As of 1998, it offers a
Ph.D. program. Its prime research focus is on cancer biology, although it also has strong programs in
plant genetics, genomics and bioinformatics, and neurobiology. In genomics, its researchers are investi-
gating genome structure, sequencing, pattern recognition, gene expression, prediction of protein struc-
ture and function, and other related topics. A large portion of its funding comes from revenue, such as
publications, intellectual property licensing, and events fees.¥The Institute for Systems Biology (ISB) is a private nonprofit institution founded in 2000 inSeattle, Washington, by Leroy Hood, Alan Aderem, and Ruedi Aebersold.20 With a mission of applyingsystems biology to problems of human health such as cancer, diabetes, and diseases of the immune
system, its 11 faculty members and 170 staff have expertise in fields such as immunity, proteomics,
genomics, computer science, biotechnology, and biophysics. Since its founding, ISB has received its
funding predominantly from federal grants, although also including private, corporate, and foundation
support and industrial collaboration.21 ISB has also spun out a number of companies to pursue com-mercialization opportunities around cell sorting and cancer therapies, in addition to cooperating in a
multiventure capital firm-backed incubator for new biotechnology start-ups.22 Of particular signifi-cance is the report that Hood left the University of Washington after he failed to convince it to establish
a systems biology research center; he later said that he thought Òthe university culture and bureaucracy
just could not have sufficient flexibilityÓ to respond to the opportunity that post-Human Genome
Project systems biology presented.23¥The Sloan-Swartz Centers for Theoretical Neurobiology were created in 1994 under the auspicesof the Sloan Foundation.24 Located at Brandeis University, California Institute of Technology, NewYork University, Salk Institute, and University of California, San Francisco, the Swartz Foundation also
made major grants to these centers in 2000. These centers place experimentalists and theoreticians from
physics, mathematics, and computer sciences in experimental brain research laboratories, where they
learn about neuroscience and apply their vantage point and nontraditional skills to cooperative lines of
inquiry. The centers have investigated topics such as gain fields and gain control in nerve circuits,
neural coding and information theory, neural population coding and response, natural field analysis,
and short-term memory.20See http://www.systemsbiology.org.21L. Timmerman, ÒProgress, Not Profit: Nonprofit Biotech Research Groups Grow in Size, Influence,Ó Seattle Times, August 4,2003.22J. Cook, ÒAccelerator Aims to Lure, Nurture Best Ideas in Biotech,Ó Seattle Post-Intelligencer, May 23, 2003.23ÒUnder BiologyÕs Hood,Ó Technology Review, September 2001, available at http://www.techreview.com/articles/01/09/qa0901.asp.24See http://www.swartzneuro.org/research_a.asp and http://www.sloan.org/programs/scitech_supresearch.shtml.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE34910.2.4  Industry
Industrial interest in the BioComp interface is driven by the prospect of potentially very largemarkets in the life sciencesÑespecially medicine. Information-enabled bioscience is further expected to
create large markets for information technologies customized and adapted to the needs of life scien-
tistsÑaccounting in substantial measure for the interest of some large IT companies in this area. Indeed,
according to the International Data Corporation, life science organizations will spend an estimated $30
billion on technology-related purchases in 2006, up from $12 billion in 2001.25Life science companies (e.g., pharmaceuticals) view information technology as a (or perhaps the)key enabler for drug design and treatments that can in principle be customized to groups as small as a
single individual. Consider, for example, the specific problem of finding useful organic compounds,
such as drugs, to treat or reduce the effects of disease. One approach is based on the use of combinato-
rial methods in chemistry, genetic engineering, and high-throughput screening technology. Such an
approach relies on trial-and-error to sift candidate compounds on a large scale to sidestep the complexi-
ties of data in a search for compounds with sufficient potential to be worth the effort of laboratory
testing for useful outcomes; similar techniques can be used for strain improvement and natural product
synthesis.26A second approach is to use computational modeling and simulation. Data mining (Section 4.4.8)can be used in addition to empirical screening to identify compounds that are likely to have a desired
pharmacological effect. Moreover, what the combinatorial and high-throughput empirical approach
gains in expediency, it may lose in insight. For example, causality in combinatorial approaches is often
difficult to attribute; and thus, it is difficult to generalize these results to other systems. Combinatorial
methods are less likely to find solutions when the desired functionality is complex (e.g., when the
biosynthetic route to a product is complicated or when a disease treatment relies on the inhibition,
without side effects, of various pathways). Also, of course, from the standpoint of basic science, predic-
tive understanding is at a premium. Computational simulation is thus used as the screening tool for
promising compoundsÑa cellÕs predicted functional response to a given compound is used as that
compoundÕs measure of promise for further (empirical) testing. Thus, although granting drug approv-
als on the basis of simulations makes little sense, simulations may be able to predict with an adequate
degree of reliability what drugs should not advance to expensive in vivo clinical trials.27 Many believethat information-enabled bioscience and biotechnology have the potential to be as revolutionary as
information technology was a few decades ago.25E. Frauenheim, ÒComputers Replace Petri Dishes in Biological Labs,Ó CNET News.com, June 2, 2003, available at http://news.com.com/2030-6679_3-998622.html?tag=fd_lede2_hed.26See, for example, C. Khosla, and R.J. Zawada, ÒGeneration of Polyketide Libraries via Combinatorial Biosynthesis,Ó Trends inBiotechnology 14(9):335-341, 1996; C.R. Hutchinson, ÒCombinatorial Biosynthesis for New Drug Discovery,Ó Current Opinion inMicrobiology 1(3):319-329, 1998; A.T. Bull, A.C. Ward, and M. Goodfellow, ÒSearch and Discovery Strategies for Biotechnology:The Paradigm Shift,Ó Microbiology in Molecular Biology Review 64(3):573-606, 2000; Y. Xue and D.H. Sherman, ÒBiosynthesis andCombinatorial Biosynthesis of Pikromycin-related Macrolides in Streptomyces venezuelae,Ó Metabolic Engineering 3(1):15-26, 2001;and L. Rohlin, M. Oh, and J.C. Liao, ÒMicrobial Pathway Engineering for Industrial Processes: Evolution, Combinatorial Biosyn-thesis and Rational Design,Ó Current Opinion in Microbiology 4(3):330-335, 2001.27For example, the Tufts Center for the Study of Drug Development estimates the cost of a new prescription drug at $897million, a figure that includes expenses of project failures (e.g., as those drugs tested that fail to prove successful in clinicaltrials). Since clinical trialsÑoccurring later in the drug pipelineÑare the most expensive parts of drug development, the
ability to screen out drug candidates that are likely to fail in clinical trials would have enormous financial impact and wouldalso reduce the many years associated with clinical trials. See Tufts Center for the Study of Drug Development news release,ÒTotal Cost to Develop a New Prescription Drug, Including Cost of Post-Approval Research, Is $897 Million,Ó May 13, 2003,
available at http://csdd.tufts.edu/NewsEvents/RecentNews.asp?newsid=29. Of particular interest is a finding reported byDiMasi that if preclinical screening could increase success rates from the current 21.5 percent to 33 percent, the cost perapproved drug could be reduced by $230 million (J.A. DiMasi, ÒThe Value of Improving the Productivity of the Drug Devel-
opment Process: Faster Times and Better Decisions,Ó PharmacoEconomics 20(S3):1-10, 2002).Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.350CATALYZING INQUIRYAs in the case of academic organizations, specific company names provided below are illustrativeand hardly exhaustive, and no inference should be drawn from the fact that any given company is not
included.10.2.4.1  Major IT Corporations
As a fast-growing, (comparatively) well-funded, and high-profile sector, life sciences research andbusiness represents an irresistible target to large IT vendors. As such, companies such as HP and IBM
have both developed suites of products and services customized for the consumption of research labs as
well as the biotechnology and pharmaceutical sectors. These services are not necessarily substantially
different from those that vendors provide to other sectorsÑa disk drive is a disk driveÑbut are bundled
with useful software or interfaces designed with the life sciences in mind.IBM established its Life Sciences Business Unit in 1998, incorporating hardware, consulting ser-vices, and an aggressive alliance program that includes many major vendors of bioinformatics and
related software. In addition, it provides DiscoveryLink, a customized front end to IBMÕs successful
DB/2 relational database product. Among other features, DiscoveryLink allows single-application views
and queries into multiple back-end databases, providing a convenient answer to a very common situa-
tion in bioinformatics, which often deals with many databases simultaneously.Of higher profile are IBMÕs research activities in computational biology. One of these is Blue Gene,the architectural successor to Deep Blue, the IBM-designed supercomputer that beat chess champion
Gary Kasparov in 1997. Blue Gene, announced in 1999 as a $100 million, 5-year project, is projected to be
1 petaflop (1015 floating point operations per second), a thousand times more powerful than Deep Blue,and 30 times more powerful than the NEC Earth-Simulator/5120. Blue Gene is designed in part to be
able to simulate the molecular forces that occur during protein folding, in order to better understand
how a large protein shape emerges from a peptide sequence.28Blue Gene is only one project, albeit the best known, of IBM ResearchÕs Computational BiologyCenter. This is a group of approximately 35 researchers who are investigating computational techniques
in molecular dynamics, pattern discovery, genome annotation, heterogeneous database techniques, and
so forth.Hewlett-Packard also maintains a life sciences division, and aggressively sells hardware, software,and services to genomics research organizations, pharmaceutical companies, and agribusiness.29 HPhas had good success in winning high-profile clients.10.2.4.2  Major Life Science Corporations
Genomic bioinformatics, and more generally the use of information technology to support researchand development, has become one of the central pillars of the modern biotechnology industry, espe-
cially the pharmaceutical sector. A waveÑsome say a boomÑof investment in bioinformatics in the late
1990s and early 2000s has tapered off, however, due to disappointing returns amid mounting costs.
While few in the industry doubt the eventual impact of computational techniques, the more significant
effects may not be felt for years. Even in 2002, however, corporate spending in bioinformatics was
estimated to be $1 billion.30The first wave of biotechnology firms, established in the 1970s, has grown into multibillion dollaroperations. These firmsÑAmgen, Biogen, Chiron, Genentech, and GenzymeÑwere all founded with28F. Allen, G. Almasi, W. Andreoni, D. Beece, B.J. Berne, A. Bright, J. Bruheroto, et al., ÒBlue Gene: A Vision for Protein ScienceUsing a Petaflop Supercomputer,Ó IBM Systems Journal 40(2):310-327, 2001, available at http://www.research.ibm.com/journal/sj/402/allen.html.29See http://www.hp.com/techservers/life_sciences/overview.html.30See http://www.redherring.com/investor/2002/0419/dealflop.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE351the idea of capitalizing on progress in genetic technologies. Yet because they predate the bioinformaticsboom, they were often late to the game, catching up by heavy investment or by outright purchasing of
other firms that had organically grown the bioinformatics capability. For example, in December of 2001,
Amgen announced that it was buying the bioinformatics-rich biotech company Immunex Corp for $16
billion.31 Genentech highlights its own bioinformatics capabilities as a key part of the research portfo-lio.32 However, while these firms and the pharmaceutical giants are clearly great consumers ofbioinformatics software and human resources, it is less clear to what extent they are performing original
computational biology research.A second wave of companies was founded in the 1990s, in the era of the Human Genome Projectand the increase in availability of information technology. Millennium Pharmaceuticals, for example,
was founded in 1993 with the goal of being a science- and technology-driven pharmaceutical company,
with a capability for target discovery based on the human genome information being published. How-
ever, most of MillenniumÕs drugs on the market have come from acquisitions, and the goal of real
rational drug discovery remains challenging. Millennium does have a high-profile leader in charge of
bioinformatics and uses IT for three main functions: bioinformatic inference making, such as identifying
likely functions of novel proteins or the existence of gene expression patterns that correlate with disease
states; chemoinformatics, searchable databases of chemical structure and biological activity; and com-
putational analysis to predict drug candidatesÕ physiological qualities such as absorption rates, distri-
bution, metabolism, excretion, and toxicity.Of higher profile is Celera, which Craig Venter founded in 1998 to compete with the publiclyfunded Human Genome Project. While genomics experts still argue over his methods, he certainly
found innovative uses for computational and analytic techniques in stitching together the results of his
ÒshotgunÓ sequencing method. Regardless of its scientific success, however, Celera has had little com-
mercial success33 as it turned from sequencing to the potentially more lucrative field of drug discovery.It still makes money by offering access to its proprietary databases to other biotechnology and pharma-
ceutical companies, but its has given up on its efforts to commercialize its software platform, selling the
Celera Discovery System to sister company Applied Biosystems (both Celera and Applied Biosystems
are owned by Applera Corporation). In addition to the Celera Discovery System, a subscription-based
database, Applied Biosystems offers an array of software for gene sequencing, laboratory information
management, and gene analysis (as well as a variety of instrumentation and reagents).10.2.4.3  Start-up and Smaller Companies
The area still receives some attention from venture capital firms such as Flagship Ventures, KleinerPerkins Caufield Byers, Atlas Ventures, and Alloy Ventures. However, the emphasis seems to be shift-
ing from bioinformatics to a stronger emphasis on biology, including medical devices and drug discov-
ery. Even companies that once positioned themselves as bioinformatics companies now describe them-
selves as being in the drug discovery business,34 most notably Celera but also many smaller companies.For companies that concentrate primarily or exclusively on informatics, times are very difficult, in large
part due to the same sort of bubble collapse as mainstream IT faced from 2000 onward.Analysts blame overinvestment in the area, leading to more companies than the space can support;companies founded by IT players with insufficient biological knowledge; and increasing competition
from big players such as IBM and HP.Midsize companies such as Gene Bank and Incyte have a similar business model to Celera, offeringaccess to proprietary databases, which often contain patented gene sequences. One model that seems to31See http://www.informationweek.com/story/IWK20011221S0038.32See http://www.genentech.com/gene/research/biotechnology/bioinformatics.jsp.33See http://www.fool.com/portfolios/rulebreaker/2002/rulebreaker020423.htm.34See http://www.bizjournals.com/washington/stories/2002/07/08/newscolumn5.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.352CATALYZING INQUIRYbe more successful than others is Òin silicoÓ simulation of various biological and biomedical processes,such as offered by AnVil Informatics.35 Beyond Genomics develops proprietary algorithms that look forlarge-scale biological systems such as pathways in gene and protein bioinformatics and experimental
data. A second seemingly successful model is a focus on providing information about pathways and
networks; Ingenuity, Cytoscape, GeneGO, PathArt, are companies that have sought to exploit this niche.Beyond bioinformatics, vendors are attempting to develop or customize for life sciences customersa number of IT solutions, including applications for knowledge management, laboratory information
management, and tracking clinical trials (including sophisticated statistical analysis).A leading example of real computer science research being applied to biology problems is theapplication of distributed or grid computing to extremely computation-intensive tasks such as protein
folding simulation. While many IT vendors are developing and pushing their grid platform, Stanford
has been running Folding@Home, a screen-saver that anyone can download and run on a home com-
puter, which calculates a tiny piece of the protein folding problem.3610.2.5  Funding and Support
Both the federal government and private foundations support research at the BioComp interface. (Thelatter can be regarded as an offshoot of the historically extensive foundation support for biology research.)10.2.5.1  General Considerations
10.2.5.1.1  The Role of Funding Institutions
  Funding institutions obviously exert a great deal of controland influence over the nature and direction of research. That is, researchers tend to gravitate toward
research problems for which funding is available. Funding agencies can also influence the development
of new talent in the field by encouraging faculty development, as illustrated non-exhaustively below:¥Release time to design new curricula and collect successful course material. However, as in peer-reviewed scientific research, the fruits of these efforts should be made public, and their successes or
limitations should be openly available (e.g., as online courses or published material).¥Supervision of undergraduate special projects or research at the BioComp interface. Specialprojects for one or a few undergraduates (e.g., summer student projects, undergraduate theses) can be
undertaken with minimal risk, and facilitating early exposure to a variety of ideas would benefit both
students and faculty.¥Support for individuals who wish to make the transition to research at the BioComp interfaceearly in their careers. Such individuals may lack the publication track record that would enable more
senior researchers to undertake such a transition. Thus, support dedicated to such people may facilitate
early career transitions and all of the accompanying benefits.10.2.5.1.2  The Review Process
  A central dimension of funding institutions is the review process theyemploy to decide what research to support. Different institutions have different styles, but they all face
the same types of issues.¥Excellence. No institution wants to support mediocre research. But as suggested below in Section10.3.1, definitions of excellence are in many ways field-specific. Thus, an effective review process must
find ways of managing this tension when proposals cross disciplinary lines.35See http://www.anvilinformatics.com.36See http://folding.stanford.edu. Perhaps the most famous of such distributed applications is SETI@Home, a program thatsupports the data processing underlying the search for extraterrestrial life.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE353¥Potential impact. All else being equal, institutions would prefer to support research in which thepotential impact of success is large. However, as a rule, claims of large impact are much more specula-
tive than other claims, simply because the long-term ramifications of any given discovery are difficult to
underscore in any convincing manner before the fact.¥Technical risk. A research investigation may or may not be successful. Research that presents thelowest technical risk (i.e., the lowest risk of failure or of being unsuccessful) is most often very closely
tied to some existing and successful research. Thus, as a rule, research that is of low technical risk tends
also to be of lesser potential impact.¥Personnel risk. Research is performed by people, and any given research effort can be executedmore or less effectively depending on the people involved. Established track records of success are an
important dimension of the teams proposed to undertake research but cannot be the only dimension
taken into account if new researchers with good ideas are to be welcomed.¥Budget. Institutions with a fixed level of support to offer investigators can support a largernumber of inexpensive research proposals or a smaller number of more expensive ones. All else being
equal, inexpensive proposals will tend to be favored over expensive ones.Proposals for research must weigh each of these factors and make trade-offs among them. Forexample, a lower budget may mean greater technical or personnel risk; a high-impact project may have
greater technical risk. Funding agencies must assess the plausibility of the trade-offs that a prospective
research team has made.These notions suggest that review panels need a wide range of expertise and experience to judge themerits of new proposals effectively or to carry out peer review of scientific papers. In principle, the
requisite range of expertise can be obtained through the use of a set of individual disciplinary experts
whose collective expertise is adequately broad. An alternative is to use a few individuals who them-
selves have interdisciplinary expertise. The disadvantage of the first model is that for practical purposes
it may reproduce forums in which the difficulties of cross-disciplinary understanding are manifested.
The disadvantage of the second model is that such individuals may be few in number and thus difficult
to enlist.10.2.5.2  Federal Support
A variety of federal agencies support work at the BioComp interface, and this support has grownover time.10.2.5.2.1  The National Institutes of Health
  For computational biology (i.e., the computing-to-biologyside of the BioComp interface), the main actor in the U.S. government is the National Institutes of
Health, part of the Department of Health and Human Services.A notable instance of bioinformatics work at NIH is the National Center for Biotechnology Informa-tion (NCBI), a part of the National Library of Medicine. Established in 1988, it is NCBI that created and
maintains GenBank (see Chapter 3).The NIHÕs National Institute of General Medical Sciences (NIGMS) manages the Biomedical Infor-mation Science and Technology Initiative, or BISTI. BISTI represents an NIH-wide collaboration and
coordination program between its many institutes and centers, as computational biology and
bioinformatics activity is spread throughout the organization. In addition, NIGMS also runs the Center
for Bioinformatics and Computational Biology, which focuses on theoretical and methodological infra-
structure, such as modeling, simulation, theory, and analysis tools in biological networks.37 The NIHÕsCenter for Information Technology, in addition to providing IT services to the rest of NIH, also main-37See http://www.nigms.nih.gov/news/releases/cbcb.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.354CATALYZING INQUIRYtains the Division of Computational Bioscience, which includes activities in high-performance comput-ing and molecular modeling; it is staffed mostly by computer scientists rather than biologists and
appears to focus on the computer science aspects of problems.In addition, the National Center for Research Resources (NCRR) is a center within NIH whosemission is to create new research technologies and provide researchers access to resources such as high-
end instrumentation, animal models, and cell line repositories. In FY 2004, it had a budget of slightly
over a billion dollars, in large part dedicated to funding research centers, as well as individual
predoctoral, postdoctoral, and career awards. NCRRÕs 2004-2008 strategic plan includes a number of
computational biology activities within its funding programs. This includes support for software and
algorithm development, mathematical modeling, and simulation. NCRR, through its Research Infra-
structure Division, also supports the creation of networks to promote cross-institutional collaboration,
including virtual laboratories and shared databases for a variety of specific clinical research programs.
This includes the Biomedical Informatics Research Network (BIRN), an Internet2 project first funded in
2002 and slated to expand in 2004. NCRR also supports cross-discipline training at all levels of a
researcherÕs careerÑfor example, supporting the entry into biology of individuals with backgrounds in
technical fields such as computer science, and retraining established researchers in appropriate fields.The National Institute for Biomedical Imaging and Bioengineering (NIBIB) is the newest institute atNIH, and is unusual for its mission of assessing and developing technological capabilities for health and
medical research. Its research goals and portfolio include support for a number of activities at the
BioComp interface, including bioinformatics, simulation and computational modeling, image process-
ing, brain-computer interfaces, and telemedicine. More broadly, its support for interdisciplinary train-
ing and research that draw on engineering, as well as physical and life sciences, mark it as another
instrument for encouraging the development of researchers and scientists having experience with and
exposure to computational science.In addition to these institutional entities, NIH has created a set of programmatic initiatives to promotequantitative, interdisciplinary approaches to biomedical problems that involve the complex, interactive
behavior of many components.38 One initiative consists of a variety of programs to develop human
capital, including those for predoctoral training for life scientists in bioinformatics and computational
biology,39 support for short courses on mathematical and statistical tools for the study of complex pheno-types and complex systems,40 postdoctoral fellowships in quantitative biology,41 
and support for a periodof supervised study and research for professionals with quantitative scientific and engineering back-
grounds outside of biology or medicine who have the potential to integrate their expertise with biomedi-
cine and develop into productive investigators.42 The National Library of Medicine supported awards forpredoctoral and postdoctoral training programs in informatics research oriented toward the life sciences
(originally medical informatics but moving toward biomedical informatics in its later years).
43A second group of programs is targeted toward specific problems involving complex biomedicalsystems. This group includes an R01 program focused on genetic architecture, biological variation, and
complex phenotypes (including human diseases);44 another on quantitative approaches to the analysisof complex biological systems, with a special focus on research areas in which systems approaches are
likely to result in the determination of the system-organizing principles and/or the system dynamics;45and still another on evolutionary mechanisms in infectious diseases.4638See http://www.nigms.nih.gov/funding/complex_systems.html.39See http://grants.nih.gov/grants/guide/pa-files/PAR-99-146.html.40See http://grants.nih.gov/grants/guide/pa-files/PA-98-083.html.41See http://grants.nih.gov/grants/guide/pa-files/PA-98-082.html.42See http://grants.nih.gov/grants/guide/pa-files/PA-02-127.html.43See http://grants.nih.gov/grants/guide/rfa-files/RFA-LM-01-001.html.44See http://grants.nih.gov/grants/guide/pa-files/PA-02-110.html.45See http://grants.nih.gov/grants/guide/pa-files/PA-98-077.html. This program includes P01 program project awards as well.46See http://grants.nih.gov/grants/guide/pa-files/PA-02-113.html. This program includes P01 program project awards as well.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE355A third group of programs is institutional in nature. One program establishes new academic Cen-ters of Excellence in Complex Biomedical Systems Research47 that promote the analysis of the organiza-tion and dynamic behaviors of complex biological systems through the development of multi-investiga-
tor teams capable of engaging biomedical complexity with a scope of activities not possible with other
funding mechanisms, including research, training, workshops, symposia, and other forms of outreach.
Typical areas of interest include computationally based modeling of processes such as the cell cycle;
pattern formation during embryogenesis; the flux of substrates and intermediates in metabolism; and
the application of network analysis to understanding the integrated systemic host responses to trauma,
burn, or other injury. A second program on Integrative and Collaborative Approaches to Research48encourages collaborative and integrative approaches to research on multifaceted biological problems
for individual investigators with existing support who need to attract and coordinate expertise in
different disciplines and approaches and require access to specialized resources, such as computational
facilities, high-throughput technologies, and equipment. A third program49 supports new quantitativeapproaches to the study of complex, fundamental biological processes by encouraging nontraditional
collaborations across disciplinary lines through supplements to existing R01, R37, or P01 NIGMS grants
to support the salary and expenses of collaborating investigators such as physicists, engineers, math-
ematicians, and other experts with quantitative skills relevant to the analysis of complex systems.Finally, a major contributor to research that includes biology and computation is the NIH Roadmap.The Roadmap is a broad set of funding opportunities and programs dealing with research issues that,
due to their complexity, scope, or interdisciplinary nature, could not be addressed adequately by a
single NIH institute or center. Relevant BioComp programs described by the Roadmap include molecu-
lar libraries, which in part seek to develop large databases of Òsmall molecules,Ó and structural biology,
which includes research to develop algorithmic tools for analyzing and predicting protein structure.The most significant BioComp initiative within the Roadmap, however, is the Bioinformatics andComputational Biology program. This program seeks to create and support a National Program of
Excellence in Biomedical Computing (NPEBC), a national network of software engineering and grid
resources to support cutting-edge biomedical research. The prime components of the NPEBC are the
National Centers for Biomedical Computing (NCBCs), seven 5-year U54 grants that total approximately
$120 million, along with a larger number of R01 and R21 individual grants to support collaboration
opportunities with the NCBCs.The NCBCs are intended as more than merely well-funded research centers; their missions oftraining, tool creation and dissemination, community support, and liberal intellectual property policies
for software and data are designed to create national networks and communities of researchers orga-
nized around BioComputational research. The structure of the grant process required the identification
of three different research thrusts (or ÒcoresÓ): a core of computational research, responsible for per-
forming original work in algorithms and computer science; a core of biomedical research, or Òdriving
biological projects,Ó and a core Biocomputing engineering, responsible for both interfacing between
computation and biomedical research, and creating the concrete tools and software systems to actualize
the research.The recipients of the first round of NCBC funding were announced in September of 2004, coveringfour centers. The second round, expected to fund an additional three centers, will be announced in 2005.
The centers funded in the first round include:¥The Stanford Center for Physics-based Simulation of Biological Structures, an effort that seeks tocreate common software and algorithmic representation for modeling and simulation, addressing prob-47See http://grants.nih.gov/grants/guide/rfa-files/RFA-GM-03-009.html.48See http://grants.nih.gov/grants/guide/pa-files/PA-00-099.html.49See http://grants.nih.gov/grants/guide/pa-files/PA-98-024.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.356CATALYZING INQUIRYlems of how to integrate models that may have widely different physical scales, have discrete orcontinuous approximations, or work at very different levels of abstraction. The driving biological prob-
lems for this center include RNA folding, myosin dynamics, neuromuscular dynamics, and cardiovas-
cular mechanics.¥The National Alliance for Medical Image Computing (NAMIC) is a center based at Brigham andWomenÕs Hospital in Boston that includes partners from universities and research centers around the
country. The goal of NAMIC is to develop computational tools for analysis and visualization of image
data, especially in integrating data from many different imaging technologies (e.g., magnetic resonance
imaging, electroencephalography, positron emission tomography, etc.) with genomic and clinical data.
The initial driving biological projects for NAMIC are various forms of neurological abnormality associ-
ated with schizophrenia.¥The Center for Computational Biology at UCLA is also investigating questions of imaging, con-centrating on the production of Òcomputational atlases,Ó database-like structures that allow sophisti-
cated queries of large-scale data. The computational research includes mathematics of volumes and
geometry, and the driving biological projects are language development, AlzheimerÕs, multiple sclero-
sis, and schizophrenia.¥The Center for Informatics for Integrating Biology and the Bedside (I2B2), organized by a consor-tium of Boston-area universities, hospitals, and medical insurance providers, seeks to develop tech-
niques to integrate and present huge sets of clinical data in ways appropriate for research into the
genetic bases of disease and, thus, helping to identify appropriate targeted therapies for individual
patients. This involves the development of statistical and algorithmic techniques for analyzing protein
structure, as well as population dynamics. The driving biological projects include airways diseases such
as asthma, hypertension, HuntingtonÕs disease, and diabetes.10.2.5.2.2  The National Science Foundation
  The National Science Foundation provides a great deal ofsupport for research at the BioComp interface through its programs of individual and institutional
grants. The NSFÕs Directorate of Biological Sciences (BIO) formerly offered a funding program in com-
putational biology activities. The BIO directorate ended this program in 1999,50 not because the re-search no longer deserved funding, but because computational biology had ÒmainstreamedÓ to become
an important part of many other biological research activities, particularly environmental biology,
integrative biology, and molecular and cellular biosciences. NSF does, in its Biological Infrastructure
Division, maintain a biological databases and informatics program that funds direct research into the
creation of tools and datasets.In its 2003 report Science and Engineering Infrastructure for the 21st Century: The Role of the NationalScience Foundation, NSF concludes that its support for science and engineering infrastructure(cyberinfrastructure), in which it includes next-generation computational tools and data analysis and
interpretation toolkits (along with a great deal of other infrastructure elements), should increase from 22
percent of its total budget to 27 percent; it also recommends strengthening its support for cross-disci-
plinary fields of research. Both of these recommendations are likely to improve the funding climate for
computational biology and bioinformatics, although of course they will still be competing with a num-
ber of other important infrastructure programs.Many existing NSF funding programs emphasize interdisciplinary research and thus are effectivevehicles for supporting BioComp research, although not exclusively. For example, the Integrative Gradu-
ate Education and Research Traineeship (IGERT) Program offers 5-year, $3 million grants to universities
to support interdisciplinary graduate student training.51 Many of the existing programs funded byIGERT work at the BioComp interface, such as bioinformatics, computational neuroscience, computa-50See http://www.nsf.gov/pubs/1999/nsf99162/nsf99162.htm.51See http://www.nsf.gov/pubs/2005/nsf05517/nsf05517.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE357tional phylogenetics, functional genomics, and so forth. Within biology, the Frontiers in IntegrativeBiological Research (FIBR) program is designed to fund research projects using innovative approaches
that draw on many fields, including information sciences, to attack major unanswered questions in
biology.52 It funds projects for five years at $1 million per year, and the 2005 round will fund eight
projects. Also, a funding program for postdoctoral training in bioinformatics is funded at $1 million.53A central and challenging application in BioComp research is an attempt to construct the entirehistoric phylogenetic Tree of Life. NSF is supporting this research through its Assembling the Tree of
Life program, funded at $29 million; databases will contain molecular, morphological, and physiologi-
cal evidence for placing taxa in relationship to other taxa. Current algorithms and data structures do not
scale well at the number of taxa and data points necessary, so both computational and biological
research is necessary to achieve this grand challenge.The NSF participates with other government agencies in coordinating research agendas and pro-grams. Of particular note is the joint initiative between the NSF Directorate for Mathematics and
Physical Sciences and NIGMS to support research in mathematical biology.54 Work supported underthis initiative is expected to impact biology and advance mathematics or statistics, and the competition
is designed to encourage new collaborations between the appropriate mathematical and biological
scientists as well as to support existing ones. The Office of Science and Technology Policy (OSTP)
included research into Òmolecular-level understanding of life processesÓ in a list of the governmentÕs
top priorities for science and engineering research.55 NSF is supporting this goal through its CAREERfunding program, which is aimed at faculty members early in their careers.56Finally, NSF sponsors a Small Grants Exploratory Research Program that supports high-risk re-search on a small scale. According to NSF, proposals eligible for support under this program must be for
Òsmall-scale, exploratory, high-risk research in the fields of science, engineering and education nor-
mally supported by NSF may be submitted to individual programs. Such research is characterized as
preliminary work on untested and novel ideas; ventures into emerging research ideas; application of
new expertise or new approaches to ÔestablishedÕ research topics; efforts having a severe urgency with
regard to availability of, or access to data, facilities, or specialized equipment, including quick-response
research on natural disasters and similar unanticipated events; or efforts of a similar character likely to
catalyze rapid and innovative advances.Ó57 Typically, grants provided under this program are less than$200,000.10.2.5.2.3  Department of Energy
  The Department of Energy played a key role in the initiation of theHuman Genome Project. Its scientific interest was first motivated by a need to understand the biological
effects of ionizing radiation, which it viewed as part of the science mission surrounding its stewardship
of the nationÕs nuclear weapons program. Furthermore, DOE scientists have had considerable experi-
ence with advanced computation in the design and manufacturing process for nuclear weapons, a fact
that DOE leveraged to investigate the genome.Today, the Department of Energy is a major supporter of 21st century biology, because it believesthat biological approaches may help it to meet its missions of energy production, global climate change
mitigation, and environmental cleanup.¥For energy production, renewable energy from plants requires the design of plants with biomassthat can be transformed efficiently to fuels. However, a limiting factor in developing such plants is the52See http://www.nsf.gov/pubs/2004/nsf04596/nsf04596.htm.53See http://www.nsf.gov/pubs/2004/nsf04539/nsf04539.html.54See http://www.nsf.gov/pubs/2002/nsf02125/nsf02125.htm.55See FY 2004 Interagency Research and Development Priorities, http://www.ostp.gov/html/ombguidmemo.pdf.56See http://www.nsf.gov/pubs/2002/nsf02111/nsf02111.htm.57See http://www.nsf.gov/pubs/2004/nsf042/dcletter.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.358CATALYZING INQUIRYlack of understanding about their metabolic pathways, and knowledge of these pathways may lead tomore efficient strategies for converting biomass to fuels.¥For mitigating climate change, reduction in the buildup of greenhouse gases (specifically CO2)would be desirable. One approach to this problem is to alter natural biological cycles to store extra
carbon in the terrestrial biomass, soils, and biomass that sinks to ocean depthsÑa sequestration ap-
proach. Research continues on the best ways to achieve large-scale carbon sequestration, and one
method under investigation is tied to microbial metabolism and activities that may lead to new ways to
store and monitor carbon.¥For environmental cleanup, microbes may provide a means to degrade or immobilize contami-nants and accelerate the development of new, less costly strategies for cleaning up a variety of DOE
waste sites. For example, microbes may be developed that can consume waste materials and degrade
them or concentrate them in a form that is easier to clean up.To address these missions, DOE supports a number of programs. Perhaps the best known is theGenomes-to-Life (GTL) program, a large research grant-providing program with four major scientific
goals: (1) identification of systems of interacting proteins at the microbial level (Òprotein machinesÓ), (2)
characterization of gene regulatory networks, (3) exploration of microbial communities and ecosystems,
and (4) development of the computational capability for modeling biological systems. To pursue these
goals, the GTL program combines large experimental datasets with advanced data management, analy-
sis, and computational simulations to create predictive simulation models of microbial function and of
the protein machines and pathways that embody those behaviors. The program identifies specific
challenges for computer science:58 automated gene annotation; software to support protein expression-proteomics analysis; the ability to meaningfully and automatically extract meaning from biological
technical papers; simulation for cellular networks; and model and system interoperability. These will
require advances in data representation, analysis tools, integration methods, visualization techniques,
models, standards, and databases. The program has funded five major projects (three at DOE labs and
two at academic institutions) for a total of $103 million over the period from 2002 to 2007. In the project
descriptions of the winners, four included Òcomputational modelsÓ as part of their charge.59A second DOE effort is the Microbial Genome program, which spun off from the Human GenomeProject in 1994. The Microbial Genome program exploits modern sequencing technologies to sequence
completely the genomes of microbes, primarily prokaryotes, based on their relevance for energy, the
global carbon cycle, and bioremediation. As of April 2003, the genomes of about 100 microbes had been
sequenced, most of them by the Joint Genome Institute,60 and placed in public databases. Microbialgenomics presents some particularly interesting science in that for newly sequenced microbial ge-
nomes, a large fraction of the genes identified (about 40 percent) have unknown functions and biologi-
cal value. In addition, most of what is known about microbes involves microbes that are easy to culture
and study or that cause serious human and animal diseases. These constitute only a small minority of all
microbes living in natural environments. Most microbes are part of communities that are very difficult
to study but play critical roles in EarthÕs ecology, and a genomic approach to understanding these
microbes may be one of the only paths toward developing an understanding of them.A third component of DOEÕs efforts is in structural biology. The purpose of this program is tounderstand the function of proteins and protein complexes that are key to the recognition and repair of
DNA damage and the bioremediation of environmental contamination by metals and radionuclides.
Research supported in this program focuses on determining the high-resolution three-dimensional58See http://www.doegenomestolife.org/pubs/ComputerScience10exec_summ.pdf.59See http://doegenomestolife.org/research/2002awards.htm.60The Joint Genome Institute, established in 1997, is a consortium of scientists, engineers, and support staff from DOEÕsLawrence Berkeley, Lawrence Livermore, and Los Alamos National Laboratories. See http://www.jgi.doe.gov/whoweare/index.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE359structures of key proteins; understanding the changes in protein structure related to interaction withmolecules such as DNA, metals, and organic ligands; visualization of multiprotein complexes that are
essential to understand DNA repair and bioremediation; prediction of protein structure and function
from sequence information, and modeling of the molecular complexes formed by protein-protein or
protein-nucleic acid interactions.10.2.5.2.4  Defense Advanced Research Projects Agency
  With a reputation for engaging in Òhigh-risk,high-returnÓ research, DARPA has been a key player in the development of applications that utilize
biomolecules as information processing, sensing, or structural components in anticipation of reaching
the limits of MooreÕs law. This research area, largely supported under DARPAÕs biocomputation pro-
gram,61 was described in Section 8.4. Managed out of DARPAÕs Information Processing TechnologyOffice (IPTO), the biocomputation program has also supported the BioSPICE program, a computational
framework with analytical and modeling tools that can be used to predict and control cellular processes
(described in Chapter 5 (Box 5.7)). Finally, the biocomputation program has supported work in syn-
thetic biology (i.e., the design and fabrication of biological components and systems that do not already
exist in the natural world) as well as the redesign and fabrication of existing biological systems (de-
scribed in Section 8.4.2.2).IPTO also supports a number of programs that seek to develop information technology that embod-ies certain biological characteristics.62 These programs have included the following:¥Software for distributed robotics, to develop and demonstrate techniques to safely control, coordi-nate, and manage large systems of autonomous software agents. A key problem is to determine effec-
tive strategies for achieving the benefits of agent-based systems, while ensuring that self-organizing
agent systems will maintain acceptable performance and security protections.¥Mobile autonomous robot software, to develop the software technologies needed for controlling theautonomous operation of singly autonomous, mobile robots in partially known, changing, and unpre-
dictable environments. In this program, ideas from robot learning and control are extended, including
soft computing, robot shaping, and imitation.¥Taskable agent software kit, to codify agent design methodology as a suite of control and decisionmechanisms, to devise metrics that characterize the conditions and domain features that indicate appro-
priate design solutions, and to explain and formalize the notion of emergent behavior.¥Self-regenerative systems, to develop core technologies necessary for making computational sys-tems able to continue operation in the face of attacks, damage, or errors. Specific avenues of investiga-
tion include biological metaphors of diversity, such as mechanisms to automatically generate a large
number of different implementations of a given function that most of them will not share a given flaw;
immune systems; and human cognitive models.¥Biologically inspired cognitive architectures, to codify a set of theories, design principles, and archi-tectures of human cognition that are specifically grounded in psychology and neurobiology. Although
implementation of such models on computers is beyond the scope of the current project, it is a natural
extension once sufficiently complete models can be created.DARPAÕs Defense Sciences Office (DSO) supports a variety of programs that connect biology tocomputing in the broad sense in which this report uses the term. These programs have included the
following:61See http://www.darpa.mil/ipto/programs/biocomp/index.htm.62See http://www.darpa.mil/ipto/Programs/programs.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.360CATALYZING INQUIRY¥Bio:Info:Micro. In collaboration with DSO, IPTO and the Microsystems Technology Office, theBio:Info:Micro program supports research in neuroprocessing and biological regulatory networks. These

research thrusts seek to develop devices for interrogating and manipulating living brains and brain
slices (in the neuroprocessing track) and single cells or components thereof (in the regulatory network
track), and the computational tools needed to analyze and interpret information derived from these
devices. Thus, neural decoding algorithms for neural spikes and local field potentials, and methods for
representing spatial components in distributed systems and using decision theoretic approaches for
decoding brain signals are of interest to the neuroprocessor track, and algorithms that can automatically
detect patterns and networks given appropriate data and models for networks that govern cell growth
and death are of interest to the regulatory track.¥Biological input/output systems. Focused on the design and assembly of molecular components andpathways that can be used to sense and report the presence of chemical or biological analytes, this
program seeks to develop technologies to enable the facile engineering and assembly of functional
biological circuits and pathways in living organisms, thereby enabling such organisms to serve as
remote sentinels for those analytes. The essential notion is that the binding of an analyte to an engi-
neered cytoplasmic or cell surface receptor will lead to regulated and specific changes in an organism,
which might then be observed by imaging, spectroscopy, or DNA analysis.¥Simulation of biomolecular microsystems. Biological or chemical microsystems in which biomolecularsensors are integrated with electronic processing elements offer the potential for significant improve-
ments in the speed, sensitivity, specificity, efficiency, and affordability of such systems. This program
seeks to develop data, models, and algorithms for the analysis of molecular recognition processes;
transduction of molecular recognition signals into measurable optical, electrical, and mechanical sig-
nals; and on-chip fluidic-molecular transport phenomena. The ultimate goal is to produce advanced
computer-aided design (CAD) tools for routine analysis and design of integrated biomolecular
microsystems.¥Engineered biomolecular nanodevices and systems. This program is focused on hybrid (biotic-abiotic)nanoscale interface technologies that enable direct, real-time conversion of biomolecular signals into
electrical signals. Success in this area would enable engineered systems to exploit the high sensory
sensitivity, selectivity, and efficiency that characterize many biological processes. The objective of this
research is to develop hybrid biomolecular devices and systems that use biological units (e.g., protein
ion channels or nanopores, g-protein-coupled receptors) for performing a sensing function but use
silicon circuitry to accomplish the signal processing. Ultimately, this research is intended to lay the
foundation for advanced Òbiology-to-digitalÓ converter systems that enable direct, real-time conversion
of biological signals into digital information.¥Biologically inspired multifunctional dynamic robots. This program seeks to exploit biological ap-proaches to propulsion mechanisms for multifunctional, dynamic, energy-efficient, and autonomous
robotic locomotion (e.g., running over multiple terrains, climbing trees, jumping and leaping, grasping
and digging); recognition and navigation mechanisms that enable biological organisms to perform
terrain following, grazing incidence landings, target location and tracking, plume tracing, and hive and
swarm behavior; and the integration of these capabilities into demonstration robotic platforms.¥Compact hybrid actuators program. This program seeks to develop electromechanical andchemomechanical actuators that perform the same functions for engineered systems that muscle per-
forms for animals. The performance goal is that these new actuators must exceed the specific power and
power density of traditional electromagnetic- and hydraulic-based actuation systems by a factor of 10.¥Active biological warfare sensors. This program seeks to develop technology to place living cellswith similar behavior to human cells onto chips, so that their health and behavior can be monitored for
the presence of harmful chemical or biological agents.¥Protein design processes. This program is using two specific challenge problems to motivate re-search into technologies for designing novel proteins for specific biological purposes. Such design will
require advances in computational models, as well as knowledge of molecular biology. The challengeCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE361problems include tasks of designing specific proteins in less than a day that can catalyze specificchemicals or inactivate an influenza virus.As a vehicle for pursuing its mission, DARPA typically uses Broad Agency Announcements. Someare focused on achieving a specific technical capability (e.g., a program that will develop technology for
synthesizing, within 24 hours, an arbitrary 10,000-oligonucleotide sequence in quantity), whereas oth-
ers are more broadly cast. In many cases, these programs target private industry as well as the more
engineering-oriented academic institutions.10.3  BARRIERS
Because work at the BioComp interface draws on different disciplines, there are barriers to effectivecooperation between practitioners from each field. (In some cases, Òeach fieldÓ is more properly cast as
the contrast between practitioners of systems biology and practitioners of empirical or experimental
biology.) This section describes some of these barriers.6310.3.1  Differences in Intellectual Style
It is almost axiomatic that substantial progress in any area of intellectual inquiry depends on theexcellence of work undertaken in that area. On the other hand, differences in intellectual style will affect
what is regarded as excellence, and computer scientists and biologists often have very different intellec-
tual styles.The existence of shared intellectual styles tends to increase the mutual understanding of colleaguesworking within their home disciplines, a fact that leads to more efficient communication and to shared
epistemological understanding and commitments. However, when working across disciplines, lack of a
shared intellectual style increases the difficulties for both parties in making meaningful progress.What are some of the differences involved? While it is risky (indeed, foolhardy) to assert hard andfast differences between the disciplines, an examination of the intellectual traditions and histories
associated with each discipline suggests that practitioners in each are generally socialized and edu-
cated with different styles.64 Over time, these differences may moderate as biology becomes a morequantitative discipline (indeed, a premise of this report is that such evolution is to be encouraged and
facilitated).10.3.1.1  Historical Origins and Intellectual Traditions
Many differences in intellectual style between the two fields originate in their histories.65 Computerscience results from a marriage between mathematics and electrical engineeringÑalthough it has
evolved far from these beginnings. The mathematical thread of computer science is based on formal
problem statements, formulating hypotheses (conjectures) based on those statements, and generating
formally correct proofs of those hypotheses. Most importantly, a single counterexample to a conjecture
invalidates the conjecture. Note also that formal proofs often entail problems that are far from reality,
because many real problems are simply too complex to be represented as formal problem statements
that are at all comprehensible. Research in mathematics (specifically, applied mathematics) often con-63An early perspective on some of these barriers can be found in K.A. Frenkel, ÒThe Human Genome Project and Informatics:A Monumental Scientific Adventure,Ó Communications of the ACM 34:40-51, 1991.64An interesting ethnographic account of life in an academic biology laboratory is provided in J. Owen-Smith, ÒManagingLaboratory Work Through Skepticism: Processes of Evaluation and Control,Ó American Sociological Review 66(3):427-452, 2001.65Some of this discussion is inspired by G. Wiederhold, ÒScience in Two Domains,Ó Stanford University, March 2002, updatedFebruary 2003. Unpublished manuscript.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.362CATALYZING INQUIRYsists of finding solutions to abstractly formulated problems and then finding real-world problems towhich these solutions are applicable.The engineering thread of computer science is based on finding useful and realizable solutions toreal-world problems. The space of possible solutions is usually vast and involves different architectures
and approaches to solving a given problem. Problems are generally simplified so that only the most
important aspects are addressed. Economic, human, and organizational factors are at least as important
as technological ones, and trade-offs among alternatives to decide on the ÒbestÓ approach to solve a
(simplified) problem often involve art as much as science.BiologyÑthe study of living thingsÑhas an intellectual tradition grounded in observation andexperiment. Because biological insight has often been found in apparently insignificant information,
biologists have come to place great value on data collection and analysis. In contrast to the theoretical
computer scientistÕs idea of formal proof, biologists and other life scientists rely on empirical work to
test hypotheses.Because accommodating a large number of independent variables in an experiment is expensive, acommon experimental approach (e.g., in medicine and pharmaceuticals) is to rely on randomized
observations to eliminate or reduce the effect of variables that have not explicitly been represented in
the model underlying the experiment. Subsequent experimental work then seeks to replicate the results
of such experiments.A biological hypothesis is regarded as ÒprovenÓ or ÒvalidatedÓ when multiple experiments indicatethat the result is highly unlikely to be due to random factors. In this context, the term ÒprovenÓ is
somewhat misleading, as there is always some chance that the effect found is a random event. A
hypothesis ÒvalidatedÓ by experimental or empirical work is one that is regarded as sufficiently reliable
as a foundation for most types of subsequent work. Generalization occurs when researchers seek to
extend the study to other conditions, or when investigation is undertaken in a new environment or with
more realism. Under these circumstances, the researcher is investigating whether the original hypoth-
esis (or some modification thereof) is more broadly applicable.Within the biological community (indeed, for researchers in any science that relies on experiment),repetition of an experiment is usually the only way to validate or generalize a finding, and replication
plays a central role in the conduct of biological science. By contrast, reproducing the proof of a theorem
is done by mathematicians and computer scientists mostly when a prior result is suspicious. Although
there is an honored tradition of seeking alternative proofs of theorems even if the original proof is not at
all suspicious, replication of results is not nearly as central to mathematics as it is to biology.Finally, biology is constrained by nature, which makes rules (even if they are not known a priori tohumans), and models of biological phenomena must be consistent with the constraints that those rules
imply. By contrast, computer science is a science of the artificialÑmore like a game in which one can
make up oneÕs own rulesÑand the only ÒhardÓ constraints are those imposed by mathematical logic
and consistency (hence data for most computer scientists have a very different ontological role than for
biologists).10.3.1.2  Different Approaches to Education and Training
The first introduction to computer science for many individuals involves building a computerprogram. The first introduction to biology for many individuals is to watch an organism grow (remem-
ber growing seeds in Dixie cups in grade school?).  These differences continue in different training

emphases for practitioners in computer science and biology in their undergraduate and graduate work.To characterize these different emphases in broad strokes, formal training in computer sciencetends to emphasize theory, abstractions, problem solving, and formalism over experimental work (in-
deed, computer programmingÑcore to the fieldÑis itself an abstraction). Moreover, as with many
mathematically oriented disciplines, much of the intellectual content of computer science is integrated
and, in that sense, cumulative. By contrast, data and experimental technique play a much more centralCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE363role in a biologistÕs education. Traditionally, mathematics (apart from statistics) is not particularlyimportant to biology education; indeed many biologists have entered the field because they wish to
pursue science that does not involve a great deal of math. Although there is a common core of knowl-
edge among most biologists, there is an enormous amount of highly specialized knowledge that is not
tightly integrated.A second issue, often encountered in conversion programs, is the difficulty of expanding oneÕshorizons to choose intellectual approaches or tools appropriate to the nature of the problem. Disciplin-
ary training in any field entails exposure to the tools and approaches of that field, which may not be the
best techniques for addressing problems in another field. Thus, successful researchers and practitio-
ners at the BioComp interface must be willing to approach problems with a wide array of methodolo-
gies and problem-solving techniques. Computer scientists often may be specialists in some specific
methodology, but biological research often requires the coordination of multiple approaches. Con-
versely, biological labs or groups that address a wide range of questions may be more hospitable to
computational researchers, because they may provide more opportunities in which computational
expertise is relevant.10.3.1.3  The Role of Theory
Theory plays a very different role and has a very different status in the two fields. For computerscientists, theoretical computer science is essentially mathematics, with all of the associated rigor, cer-
tainty, and difficulties. Of particular interest in theoretical computer science is the topic of algorithmic
complexity. The most important practical results from algorithmic complexity indicate the scaling rela-
tionships between how long it takes to solve a problem and the size of the problem when its solution is
based on a specific algorithm. Thus, algorithm A might solve a problem in a time of order N2, whichmeans that a problem that is 3 times as large would take 32 = 9 times as long to solve, whereas a fasteralgorithm B might solve the same problem in time of order N log N (that is, O(N log N)), which meansthat a problem 3 times as large would take 3 log 3 = 3.29 times as long to solve. (A specific example is
that when asked to write a program to sort a list of numbers in ascending order, one of the most
common programs written by novice programmers involves an O(N2) algorithm. It takes a somewhatgreater degree of algorithmic sophistication to write a program that exhibits O(N log N) behaviorÑwhich can be proven to the best that is possible.)Such results are important to algorithm design, and all computer programs embody algorithms.Depending on the functional relationship between run time and problem size, a given program that
works well on a small set of test data mayÑor may notÑwork well (i.e., run in a reasonable time) for a
larger set of real data. Theoretical computer science thus imposes constraints on real programs that
software developers ignore at their own peril.Computer scientists and mathematicians derive satisfaction and pleasure from elegance of reason-ing, logic, and structure. Being able to explain a phenomenon or account for a dynamical behavior with
a simple model is highly valued. The reason for this penchant is clear: the simpler the model, the more
likely it is that the tools of analysis can be used to dissect and understand the model fully.This sometimes means that a tendency to oversimplify overwhelms the need for preserving realisticfeatures, to the dissatisfaction or derision of biologists. Computer scientists, of course, may well per-
ceive a biologistÕs dissatisfaction as a lack of analytical or theoretical sophistication and an unwilling-
ness to be rigorous, and often fail to recognize the complexity inherent in biological systems. In other
cases, the love of elegance leads to fixation with elegant, but irrelevant, models far beyond their value
outside the field, simply because the inherent model is clean and simple. In still other cases, the lack of
training of computer scientists in eliciting from users the precise nature of their problems has led
computer scientists to develop good solutions to problems that are not interesting to most biologists or
relevant to real biological phenomena.By contrast, manyÑperhaps mostÑbiologists today have a deep skepticism about theory andCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.364CATALYZING INQUIRYBox 10.5Some Examples of Oversimplified and/or Misleading Computational andMathematical Models in Biology¥The Turing reaction-diffusion theory for pattern formation in developmental biologyÑfirst suggested by Tur-ing in 1952, and largely dormant until the mid-1970s, this theory, based on an activator-inhibitor system, be-came a focus of partial differential equations research. Initially, attempts were made to show that diffusion and
reaction of the activator-inhibitor type are responsible for the development of real structures in real embryos(stripes or spots, positions of limbs and digits, etc.) However, later work has shown that the biological solution tothe pattern formation problem is inelegant and ÒkludgyÓ, with many ÒredundantÓ or ÒinefficientÓ parts.1¥A senior computer scientist faced the issue of how one might infer the structure of a genetic regulatorynetwork from data on the presence or absence of transcription factors. In a cell, a set of genes interact toproduce a proteinÑand the transcription factors (themselves proteins) influence the rate at which that protein
is produced. His initial model of this network was a Boolean circuit, in which the presence or absence ofcertain factors led to the production of the protein. A typical experimental procedure in a biology lab to probethe nature of this circuit is to observe its behavior by inhibiting the production of some transcription factor and
to observe whether or not the protein is produced. The analogous action in the Boolean circuit would becutting a wire in that circuit. However, this simple analogy failed to model the actual behavior of the biolog-ical system because, in many cases, the inhibition of one transcription factor results in another set of proteins
that do the same job. Thus, the notion of simple perturbation experiments that can be viewed as analogous tojust snipping a wire in a logic circuit is obvious for computer scientistsÑbut turns out to be not particularlyrelevant to this particular phenomenon.
¥The problem of genome sequence assembly involves piecing together a large number of short sequences(fragments) into the correct master sequence. The initial computer scientist formulation of this problem was tofind the shortest sequence that would contain a given set of sequences as a consecutive piece. But this
formulation of the problem was completely wrong for two reasons. First, the available information on thefragments is sometime erroneousÑthat is, the data might indicate that a fragment would have a certain baseat a given location, but in reality it would have a different base at that location. Second, DNA molecules have
a great deal of repeated structure (i.e., the same sequence is typically found multiple times). Thus, the shortestsequence is not biologically plausible because that repeated structure is ignored.¥Amino acids are represented by codons (i.e., triplets of nucleotide bases). Because there are 4 nucleotides,the number of possible codons is 43, or 64. But for a long time, only 20 amino acids were known that occurin nature. It turns out that by assuming that the codons overlapped each other and requiring that the coding beunambiguous, only 20 codons are possible. Because of this match, a natural assumption was that an overlap-
ping code was operative in DNA coding. However, experimental data dispelled this notion, indicating insteadthat multiple codons can represent the same amino acid and further that the codons were not overlapping.1See, for example, G. von Dassow, E. Meir, E.M. Munro, and G.M. Odell, ÒThe Segment Polarity Network Is a Robust DevelopmentalModule,Ó Nature 406(6792):188-192, 2000. At the same time, the reaction-diffusion approach appears to have nontrivial utility in explain-ing other biological phenomena, such as certain aspects of microtubule organization (C. Papaseit, N. Pochon, and J. Tabony, ÒMicrotubuleSelf-organization Is Gravity-dependent,Ó Proceedings of the National Academy of Sciences 97(15):8364-8368, 2000).models, at least as represented by mathematics-based theory and computational models. For example,theoretical biology has a very different status within biology and has often been a poor stepchild to
mainstream biology. Results from theoretical biology are often irrelevant to specific biological systems
such as a particular species, and even the simplest biological organism is so complex as to render
virtually impossible a theoretical analysis based on first principles. Indeed, most biologists have a long-
ingrained suspicion of theoretical models that they regard as vastly oversimplified (i.e., almost all of
them) and are skeptical of any purported insights that emerge from such models. (Box 10.5 provides
some examples of misleading computational and mathematical models of biological phenomena.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE365A related point is that computer scientists tend to assume that universal statements have no excep-tions, whereas biologists have learned that there are almost always exceptions to rules. For example, if
a biologist says that all crows are black, and one asks about albino crows, the answer will be, ÒOh, sure,
albino crows are white, but all normal crows are black.Ó The biologist is describing the average caseÑall
standard crows are blackÑbut keeps in the back of his or her mind the exceptional cases. By contrast,
the computer scientist wants to know if the crow database he or she is building needs to accommodate
anything other than a black crowÑand thus, when a computer scientist makes a biological generaliza-
tion, the biologist will often jump immediately to the exceptional case as a way of dismissing the
generalization.These comments should not be taken to imply that biologists do not use theory at all; in fact,biologists use theory and models in their everyday work. The theory of evolution is among the most
powerful of all scientific theories, in the sense that it underlies the scientific understanding of all natural
biological phenomena. But because the outcomes of evolutionary processes are driven by a myriad of
environmental and chance influences, it is difficult to make measurable or quantitative predictions
about specific biological phenomena. In this context, evolution is more of an organizing principle than
a predictive formalism.Perhaps a fairer statement is that many biologists remain to be persuaded of the value of quantita-tive theory and abstraction on a global basis, although they accept their value in the context of special-
ized hypothesis, individual probes, or inquiries on a biological process. Biological researchers are begin-
ning to see the potential explanatory value of computational and mathematical approachesÑa potential
that is less apparent than might be expected because of the very success of an empirical approach to
biology that has been grounded in experiment and observation for many decades.10.3.1.4  Data and Experimentation
As mentioned above, computer scientists and biologists also view data quite differently. For thecomputer scientist, data usually result from measurements of some computational artifact in use (e.g.,
how long it takes for a program to run, how many errors a program has). Because these data are tied to
artifacts that have been made by human beings, they are as ephemeral and transient as the underlying
artifact, which may indeed change in the next revision or release. Because computer science is a science
of the artificial, the intellectual process of the computer scientist does not begin with data, but rather
with an act of artifact creation, after which measurements can be taken.66Indeed, for the computer scientist, the term Òexperimental computer scienceÓ refers to the engineer-ing and creation of new or improved computational artifactsÑhardware or softwareÑas the central
objective of intellectual efforts.67 Engineering has intellectual biases toward model reduction, extractingkey elements, and understanding subsystems in isolation before assembling larger structures. The
engineering approach also rests on the idea that basic units (e.g., transistors, silicon chips) have repeat-
able, predictable behavior; that ÒmodulesÓ with specific capability (e.g., switches, oscillators, and filters)
can be made from such units; and that larger systems with arbitrary complexity are, in turn, made of
such modules.In contrast, biology today is a data-driven scienceÑand theories and models are created to fit thedata. Data, presuming they are accurate, impose ÒhardÓ constraints on the biologist in much the same
way that results from theoretical computer science impose hard constraints on the computer scientist.
Because of the central role that data play in biology, biologists pay a great deal of attention to experi-66This is not to deny that computer scientists often work with large datasets. For example, computer scientists may work withterabytes of textual or image data. But these data are the subjects of manipulation and processing, rather than being tied directlyto the performance of the hardware and software artifacts of the computer scientist.67National Research Council, Academic Careers for Experimental Computer Scientists and Engineers, National Academy Press,Washington, DC, 1994.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.366CATALYZING INQUIRYmental technique and laboratory procedure and instrumentationÑmuch more so than most computerscientists pay to the comparable areas in computer science. Thus, a computer scientist with insufficient
awareness of experimental design may not be accustomed to or even aware of techniques of formal
model or simulation validation.In addition, biology has not traditionally looked to engineering for insight or inspiration. Forexample, proteins come in an endless variety with many variations and do not necessarily have straight-
forward analogues to engineering parts. Experimental biologists often focus on discovering new pieces
of cellular machinery and on how defective behavior stems from broken or missing pieces (e.g., muta-
tions). Experimental work is aimed at proving or disproving specific hypotheses, such as whether or not
a particular biochemical pathway is relevant to some cellular phenomena.The training that computer scientists receive also emphasizes general solutions that give guaranteesabout events in terms of their worst-case performance. Biologists are interested in specific solutions that
relate to very particular (although voluminous) datasets. (A further complication is that biological data
are often erroneous and/or inconsistent, especially when collected in large volume.) By recognizing and
exploiting special characteristics of biologically significant datasets, special-purpose solutions can be
crafted that function much more effectively than general-purpose solutions. For example, in the prob-
lem of genomic sequence assembly, it turns out that by exploiting the information available concerning
the size of fragments, the number of choices for where a fragment might fit is sharply restricted.The central role that experimental data plays in biology is responsible for the fact that, to date,computer scientists have been able to make their most important contributions in areas in which the
details of some biological phenomena can be neglected to some important extent. Thus, the abstraction
of DNA as merely a string of characters derived from a four-letter alphabet is a very powerful notion,
and considerable headway in genomics can be made knowing little else. To be sure, there are experi-
mental errors to take into account, and a model of the noisiness of the data must be developed, but the
underlying problem is pretty clear to a computer scientist.On the other hand, as the discussion in Section 4.4.1 makes clear, there are limits to this abstractionthat arise from just such Òdetails.Ó Also, proteomicsÑin which the three-dimensional structure of a pro-
tein, rather than the linear sequence, determines its functionÑpresents even greater challenges. To under-
stand the geometry of a three-dimensional structure, discrete mathematicsÑthe stock in trade of the
computer scientistÑis far less useful than continuous mathematics.68 Furthermore, the properties andcharacteristics of the specific amino acids in a protein matter a great deal to its structure and function,
whereas the various nucleotide bases are more or less equivalent from an informational standpoint. In
short, proteomics involves a much more substantial body of domain knowledge than does genomics.One illustration related by a senior computer scientist working in biology is his original dream that,with enough data,69 it would be computationally straightforward to understand the mechanisms ofgene regulation. That is, with sufficient data on regulatory pathways, cascades, gene knockouts, expres-
sion levels, and their dependencies on environmental factors, how genetic regulatory networks work
would become reasonable clear. With the hindsight of several years, he now believes that this dream
was hopelessly naŁve in that it did not account for the myriad exceptions and apparent special cases
inherent in biological data that make the biologistÕs intellectual life very complicated indeed.Finally, consider that many biologists are suspiciousÑor at least not yet persuadedÑof the value andimportance of high-throughput measurement of biological systems (Section 7.2). Because many biologists
were educated and worked in an era in which data were scarce, experiments in biology have historically
been oriented toward hypothesis testing. High-throughput data collection drives in the opposite direction,68The reason is that geometric descriptions naturally involve continuous variables such as lengths and angles, and functions ofthose variables.69Richard Karp, University of California, Berkeley, personal communication, July 29, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE367and relies on the ability to sift through and recognize patterns in large volumes of data whose meaning canthen be inferred. Of course, predictions that emerge from the analysis of large volumes of data must still
be verified one at a time, and science is today far from the point at which such analysis would, by itself,
provide reliable biological conclusions. Nevertheless, such analysis can play an important role in suggest-
ing interesting hypotheses and thus expand the options available for biological exploration.10.3.1.5  A Caricature of Intellectual Differences
A number of one-liners that can be used to encapsulate the differences described above, though aswith all one-liners, there is considerable oversimplification. Here are four:¥The goal of computer science (CS) is to develop solutions that can be useful in solving manyproblems, while the goal of biology is to look for solutions to individual and specific problems.¥Computer science is driven by the development of method and technique, while biology isdriven by experiment and data.¥Computer scientists are trained to search for boundary conditions and constraints, whereasbiologists are trained to seek signal in the noise of their experimental data.¥Computer scientists are trained to take categorical statements literally, whereas biologists usethem informally.10.3.2  Differences in Culture
Another barrier at the BioComp interface is cultural. Each field has its own cultural style, and whatseems obvious to practitioners in one field may not be obvious to those in the other. Consider, for
example, differences between computer science and biology. Before PowerPoint became ubiquitous to
both fields, computer scientists tended to use overhead transparencies in visiting lectures, while biolo-
gists tended to use 35 mm slides. Computer science, as a discipline, can often be pursued while working
at home, whereas biological lab work requires being Òin the officeÓ to a far greater extentÑa computer
scientist who is away from the lab may well be seen by biologists as Ònot being around enoughÓ or Ònot
being a team player.Ó Computer scientists are accustomed to having their own office space, while
biologists (especially postdoctoral associates) work out of their labs and rarely have their own offices
until they achieve an appropriate seniority.Such differences are in some sense trivial, but they do suggest the reality of different cultures, and it ishelpful to explore some other differences that are not so trivial. One of the most important differences is that
of intellectual style: the discussion in Section 10.3.1 would suggest that biologists (especially those untrained
in quantitative sciences) may well distrust the facile approaches and oversimplified models of computer
scientists or mathematicians unfamiliar with the complexities of living things, and the computer scientist
may well regard the biologist as obsessed with details and molecular parts lists rather than the qualitative or
quantitative whole. This section explores some issues that lie outside the domain of intellectual style.10.3.2.1  The Nature of the Research Enterprise
When practitioners from two fields collaborate, each brings to the table the values that characterizeeach field. Given the importance that biologists place on the understanding of specific biological phe-
nomena of interest, they place the highest value on answers that are specific to those phenomena.
Biologists want Òthe answer,Ó and they are interested in details of a computational model only insofar
as they have an effect on the answer; for the most part, they care far less about a hypothetical biological
phenomenon than about explaining the data obtained from experiment. Computer scientists and math-
ematicians, in contrast, are interested in the parameters of a model or a solution and in ways to improveCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.368CATALYZING INQUIRYit, characterize it, understand it better, or make it more generally applicable to other problems. Thus, thebiologist will likely be interested in the results of a model run on the single dataset of interest, while the
computer scientist will want to run hundreds or thousands of datasets to better analyze the behavior of
the model, and mathematicians will want to explore the limits of a modelÕs applicability.70An example of this cultural difference is illustrated in the history of the Gene Ontology (GO) discussedin Chapter 4. Begun in 1998 as a collaboration between researchers responsible for three model organism
databases (FlyBase [Drosophila], the Saccharomyces Genome Database, and the Mouse Genome Database),
GO collaborators sought to develop structured, controlled vocabularies that describe gene products in terms
of their associated biological processes, cellular components, and molecular functions in a species-indepen-
dent manner. In their work, these researchers have apparently not made extensive use of the (mostly
domain-independent) theoretical contributions of computer science from the last 20 years, but rather have
reinvented much of that work on their own. The reason for this reinvention, offered by one knowledgeable
observer, is that they were unable to find computer scientists with appropriately specialized experience who
were willing to sacrifice their quest for general applicability to develop a functional, usable system.71A related point is that in academia, research computer scientists have very little motivation to take asoftware implementation beyond the prototype stage. That is, they may have developed a powerful
algorithm that is likely to be useful in many biological contexts, implemented a prototype software system
based on this algorithm, and convincingly demonstrated its utility in a few cases. But because most of the
intellectual credit inheres in the prototype (e.g., papers for publication and promotions), research com-
puter scientists have little motivation to move from the prototype system, which can generally be used
only by those familiar with the quirks of its operation, to a more robust system that can be used by the
broader community at large. Because going from prototype to broadly usable system is generally a time-
intensive process, many powerful methods are not available to the biology community.Similar considerations apply in the biology community with respect to data curation. Intellectualcredit for academic biologists inheres in the publication of primary data, rather than in any long-term
follow-up to ensure that the data are useful to the broader community. (Indeed, if the data are not made
useful to the broader community, the researcher originally responsible for the data gains the competi-
tive advantage of being the only one, or one of a few, able to use them.) This suggests that cultural
incentives for data curation (or the lack thereof) have to be altered if data curation is to become a more
significant activity in the research community.7270These differences in perspective are also found at the interface of medical informatics and bioinformatics. For example,Altman notes that Òthe pursuit of bioinformatics and clinical informatics together is not without some difficulties. Practitioners inclinical medicine and basic science do not instantly understand the distinction between the scientific goals of their domains andthe transferability of methodologies across the two domains. They sometimes question whether informatics investigators are reallydevoted to the solution of scientific problems or are simply enamored of computational methodologies of unclear significance [emphasisadded].Ó To reduce these tensions, Altman arguesÑsimilarly to the argument presented in this reportÑthat Òinformatics inves-tigators (and their students) be able to work collaboratively with physicians and scientists in a manner that makes it clear that thecreation of excellent, well-validated methods for solving problems in these domains is the paramount goal.Ó See R.B. Altman,
ÒThe Interactions Between Clinical Informatics and Bioinformatics: A Case Study,Ó Journal of the American Medical InformaticsAssociation 7(5):439-443, 2000.71Russ B. Altman, Stanford University, personal communication, December 16, 2003.72One approach that has been used to support data annotation and curation activities is the data jamboree. In November 1999,the Celera Corporation hosted an invitation-only event (Òthe jamboreeÓ) in which participants worked for two weeks at annotat-ing and correcting data from the Drosophila melanogaster genome. By all accounts a successful event that resulted in the publica-tion of the complete sequence as well as appropriate annotations (see M.D. Adams, S.E. Celniker, R.A. Holt, C.A. Evans, J.D.Gocayne, P.G. Amanatides, S.E. Scherer, et al., ÒThe Genome Sequence of Drosophila melanogaster,Ó Science 287(5461):2185-2195,2000) the event featured a very informal atmosphere that promoted social connection and interaction as well as a work environ-
ment conducive to the task. The emergence of some level of community curation on Amazon and eBay may also provide someuseful hints on how to proceed. In these efforts, community assessment is allowed, but thereÕs no overall review of the quality ofthe assessment. Nonetheless, users have access to a diverse collection of assessments of and can do their own meta-quality
control by deciding which of the reviewers to believe. This model does scale with increasing database size, although consistentcuration is hardly guaranteed. It is an open question worth some investigation as to whether community commentary (perhapssupported with an appropriate technological infrastructure) could result in meaningful data curation.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE36910.3.2.2  Publication Venue
Although both biologists and computer scientists in academia seek to make their work known totheir respective peer communities, they do so in different ways. For many areas within computer
science, refereed conferences are the most prestigious places to publish leading research.73 In biology,by contrast, conference publications are often considered less prestigious, and biologists tend to prefer
journal publications. Computer scientists often write abstracts in such a way as to entice the reader to
read the full paper, whereas biologists often write abstracts in such a way that the reader need not read
the full paper. In publishing work that refers to a new tool, a computer scientist may be more likely to
reference a Web site where the tool can be found, while a biologist may be more likely to reference a
paper describing the tool.This difference in publication venues strongly affects attempts at collaboration. Academic biologistsoften do not understand refereed conferences, and computer scientists often think of journals as mere
repositories of papers, rather than a means of communicating results. Given that publication is the
primary output of academic research, this disagreement can be very disturbing and can have important
inhibitory effects on collaboration.10.3.2.3  Organization of Human Resources
While it is clear to all parties from the start that that the professional expertise of the biologist isneeded to do good work in computational biology, a view that equates computer science with program-
ming can lead biologists to underestimate the intellectual capabilities on the computer science side
necessary for computation-intensive biology problems. Thus, many biologists who do see rewards in
bridging the interdisciplinary gap (especially in academia) tend to prefer doing so in their own labs, by
hiring postdoctoral fellows from physics or computer science to work on their problems, keeping these
ventures Òin the family,Ó rather than by establishing partnerships with more established computer
scientists.Such an approach has advantages and disadvantages. An advantage is that postdoctoral fellowswith good quantitative and computational background can be exposed to the art of biological experi-
mentation and interpretation as a part of their postdoctoral training, the result of which can be the
nurturing of young interdisciplinary scientists. One disadvantage is that by engaging individuals at the
beginning of their careers, the biologist is deprived of the intellectual maturity and insight that gener-
ally accompanies more seasoned computer scientistsÑand such maturity and insight may be most
necessary for making headway on complex problems.The integration of computational expertise into a biological research enterprise can be undertakenin different ways. For example, in some instances, a group of computer scientists can work with a group
of biologists, each bringing its own computational approach to the biological problem. In other cases, a
single individual with computational expertise (e.g., a postdoc) can work in an otherwise ÒpureÓ bio-
logical group, offering expertise in math, modeling, and programming. A second dimension of integra-
tion is that the bioinformaticist can play a role as a team member who engages equally with everyone
else on the research team or as a ÒbridgeÓ that serves as intermediate between practitioners of various
disciplines. These two roles are not mutually exclusive, although the first seems to be more common.10.3.2.4  Devaluing the Contributions of the Other
A sine qua non of interdisciplinary work is that intellectual credit be apportioned appropriately.In some cases known to the committee, the expertise of scientists from a nonbiological discipline has73Such a practice arose because computer science is a fast-moving field, with a tradition of sharing discovery by onlinedemonstration and discussion. Conferences were originally formed as a way to talk together, in person, and with relatively fastpublication of results for the requirements of academia. In this context, journal publication would have been much too slow.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.370CATALYZING INQUIRYbeen used, and yet joint authorship or due credit withheld. In one story, a respected biologist heldregular discussions with an excellent mathematician colleague. The biologist assigned a theoretical
project on a topic already worked out by the mathematician to an in-house physics postdoctoral
fellow instead of pursuing joint work with the mathematician. The results of this in-house work fell
short of what was possible or desirable but displaced other serious attempts at theoretical analysis of
an interesting problem.This example suggests a view of mathematics and computer science that is ancillary and peripheralto the ÒrealÓ substance of biology. The fact that computing and mathematics have developed powerful
tools for the analysis of biological data makes it easy for biologists to see the computer scientist as the
data equivalent of a lab technician. However, although programming is an essential dimension of most
computer scientistsÕ backgrounds, it does not follow that the primary utility of the computer scientist is
to do programming. Algorithm design, to take one example, is not programming, but because algo-
rithms must be implemented as a computer program, it is easy to confuse the two.In other cases known to the committee, the expertise of biological scientists has been denigrated bythose in computing. For example, computer scientists sometimes view a successful biological experi-
ment as one that ÒmerelyÓ produces more data and do not appreciate the fundamental creative act
required to devise the appropriate experiment. This attitude suggests a view of biology in which the
ÒrealÓ science resides in the creation of a theory or a computational model, and data are merely what is
needed to populate the model.What accounts for such attitudes? The committee believes one contributing factor is not muchdifferent than loyalty to oneÕs discipline. Professionals in one discipline quite naturally come to believe
that the ways in which they have learned to see their discipline have inherent advantages (if they did
not, they would not be part of the discipline), and challenges to the intellectual paradigms they bring to
the subject may well be met with a certain skepticism.A second point to consider is that interdisciplinary work is not necessarily symmetric. This isespecially true in the mix of academic research activity vis a vis applied or technical support activity.
That is, it is often possible to identify one field as being the side where research advances are occurring
and the other as applying some kind of support. In some cases, Ph.D.-level research in computer science
can be enriched by what is routinely taught in undergraduate classes in biology, and vice versa.For example, individuals pursuing cutting-edge research in database design may be interested infinding data models to exercise their design. They are interested in finding domain experts to help them
better understand the complexities of a certain interesting problem domain, such as biology, but these
database researchers see the data and the insights coming from the biologist as helping to define the
problem, but as having little to do with finding the solution. Similarly, biologists may be investigating
a new topic in biology and need quantitative or logistical or algorithmic help to accomplish the research,
but they feel the real intellectual contributionÑto biologyÑcomes from their insights on the biological
side.The primary exception to these scenarios is where a research group in computer science gets teamedup with a research group in biological science. In such instances, the relationship can be truly symmetri-
cal. Both parties benefit from a symbiotic relationship. Both yield practical value to the other, while
gaining theoretical value for themselves. Both operate at an equivalent level of intellectual contribution.
Both gain an equivalent level of real research coming out of the activity.10.3.2.5  Attitudinal Issues
Biology laboratories are increasingly dependent on various forms of information technology. High-throughput instrumentation generates large volumes of data very quickly. Computer-based databases
are the only way to keep track of a biological literature that is growing at exponential rates. Computer
programs are increasingly needed to assemble and understand biological data derived from experi-
ments or resident in databases.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE371With such dependence on IT, it would not be surprising if individuals who are especially knowl-edgeable about information technology were necessary to keep these laboratories running at high
efficiency. However, computer scientists are very wary of being put into the role of technician or
programmer. Computer science researchers, facing this prospect from various research disciplines, can
be sensitive about wanting respect for the fundamental research advances they bring to the table.74The roles of intellectual collaborator and co-equal partner are largely incompatible with the role oftechnician, and it is understandable that a computer scientist would want to be treated as a coequal. At
the same time, a certain amount of humility and respect is also necessary. That is, the computer scientist
must refrain from jumping to conclusions, must be willing to learn the facts and contemplate biological
data seriously, and must not work solely on the refined abstraction problem. It may well be necessary
for the computer scientist to do some mundane things to earn the confidence of the biologist partner
before being able to do more interesting things.The biologist has a role to play in facilitating partnership as well. For example, the biologist mustunderstand that the computer scientist (especially one at the beginning of his or her career) wants to do
work of publishable quality as wellÑwork that will earn the respect of colleagues in computer science.
As suggested above, programming generally does not meet this test. A second important point is to
recognize without condescension the fact that many (most?) computer scientists have very little experi-
ence or familiarity with either biological concepts or data. Still a third point is the recognition that while
primary data generation and experiment remain important to the life sciences, analytical work on
existing data can be every bit as valuableÑbioinformatics is not simply Òtaking someone elseÕs data.Ó
This last point suggests a more subtle risk in partnershipsÑthat a person with specialized skills may be
regarded as a technician or a stand-alone consultant rather than as a true collaborator.10.3.3  Barriers in Academia
One important venue for research at the BioComp interface is academia. Universities can provideinfrastructure for work in this area, but institutional difficulties often arise in academic settings for work
that is not traditional or easily identified with existing departments. These differences derive from the
structure and culture of departments and disciplines, and lead to scientists in different disciplines
having different intellectual and professional goals and experiencing different conditions for their
career success. Collaborators from different disciplines must find and maintain common ground, such
as agreeing on goals for a joint project, but also respect one anotherÕs separate priorities, such as having
to publish in primary journals to present at particular conferences or to obtain tenure in their respective
departments according to those departmental criteria. Such cross-pressures and expectations from the
home departments and disciplinary colleagues remain even if the participants develop similar goals for
a project.10.3.3.1  Academic Disciplines and Departmental Structure
Universities are structured around disciplinary departments and often have considerable difficulty insupporting and sustaining interdisciplinary work. Neither fish nor fowl, the interdisciplinary researcher is
often faced with the formidable task of finding an intellectual home within the university that will take the
responsibility for providing tenure, research space, start-up funding, and the like. The essential problem is
that a researcher working at the interface between fields X and Y is often doing work that does not fall
clearly within the purview of either Department X or Department Y. When budgets are expanding and74It is useful to note that research laboratories in both biology and computer science employ technicians and programmers,and such individuals serve very useful functions in each kind of laboratory. But the role of a lab technician in a biology labora-tory or programmer in a computer laboratory is quite different from the role of the senior scientist who directs the biology orcomputer laboratory.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.372CATALYZING INQUIRYresources flush, it is easy for Department X or Department Y to take a risk on an interdisciplinary scholar.But as is more often the case today, when resources are scarce, each department is much more likely to
want someone who fits squarely within its traditional departmental definitions, and any appointment that
goes to an interdisciplinary researcher is seen as a lost opportunity.For example, tenure letters may be requested from traditional researchers in the field for an inter-disciplinary worker; despite great success, the tenure letters may well indicate that they were unfamiliar
with the candidateÕs work. Graduate students seeking interdisciplinary training but nominally housed
in a given department may have difficulty taking that departmentÕs qualifying exam, because their
training is significantly different from mainstream students.Another dimension of this problem is that publication venues often mirror departmental structures.Thus, it may be difficult to find appropriate venues for interdisciplinary work. That is, the forms of
output and forums of publication for the interdisciplinary researcher may be different than for either
Department X or Department Y. For example, even within computer science itself, experimental com-
puter scientists that focus on system building often lack a track record of published papers in refereed
journals, and tenure and promotion committees (often university-wide) that focus on such records for
most other disciplines in the university have a hard time evaluating the worthiness of someone whose
contributions have taken the form of software that the community has used extensively or presentations
at refereed conferences. Even if biologists are aware in principle of such ÒpublicationÓ venues, they may
not be aware that such conferences are heavily refereed or are sometimes regarded as the most presti-
gious of publication venues. Also, prestigious journals known for publishing biology research are often
reluctant to devote space to papers devoted to computational technique or methodology if it does not
include specific application to an important biological problem (in which case the computational di-
mensions are usually given a peripheral rather than primary status).Further, the academic tenure and promotion system is biased toward individual work (i.e., work ona scale that a single individual can publish and receive credit for). However, large software systemsÑ
common in computer science and bioinformaticsÑare constructed by teams. Although small subsystems
can be developed by single individuals, it is the whole system that provides primary value, and univer-
sity-based research that is usually driven by a single-authored Ph.D. thesis or single faculty members is
not very well suited to such a challenge.75Finally, in most departments, it is the senior faculty that are likely to be the most influential withregard to the allocation of resourcesÑspace, tenure, personnel and research assistant support, and so
on. If these faculty are relatively uninformed or disconnected from ongoing research at the BioComp
interface, the needs and intellectual perspectives of interface researchers will not be fully taken into
account.10.3.3.2  Structure of Educational Programs
Stovepiping is also reflected in the structure of educational programs. Stovepiping refers to thetendency of individual disciplines to have different points of view on what to teach and how to teach it,
without regard for what goes on in other disciplines. In some cases, the methods of the future are still
undeveloped, or are undergoing revolution, so that suitable texts or syllabi are not yet available. Fur-
ther, like individual researchers, departments tend to be territorial, protective of their realms, and
insistent on ever-growing specialized course load requirements for their own students. This discour-
ages or precludes cross-discipline shopping. Novel training creates a need for reeducation of faculty to
change the design of old curricula and modernize the teaching. These changes take time and energy,
and require release time from other academic burdens, whether administrative or teaching.75C. Koch, ÒWhat CanNeurobiology Teach Computer Engineers?,Ó Division of Biology and Division of Engineering and
Applied Science, California Institute of Technology, January 31, 2001,position paper to National Research Council workshop,
available at http://www7.nationalacademies.org/compbio_wrkshps/Christof_Koch_Position_Paper.doc.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE373Related to this point is the tension between breadth and depth. Should an individual trained in Xwho wishes to work at the intersection of X and Y undertake to learn about Y on his or her own, or seek
to collaborate with an individual trained in Y? Leading-edge research in any field requires deep knowl-
edge. But work at the interface of two disciplines draws on both of them, and it is difficult to be deep in
both fields; thus, Ph.D.-level expertise in both computer science and biology may be unrealistic to
expect. As a result, collaboration is likely to be necessary in all but extraordinary cases.Thus, what is the right balance to be struck between collaboration and multiskilling of individuals?There is no hard-and-fast answer to this question, but the answer necessarily involves some of both.
Even if ÒcollaborationÓ with an expert in Y is the answer, the individual trained in X must be familiar
enough with Y to be able to conduct a constructive dialogue with the expert in Y, asking meaningful
questions and understanding answers received. At the same time, it is unlikely that an expert in X could
develop in a reasonable time expertise in Y comparable to that of a specialist in Y, so some degree of
collaboration will inevitably be necessary.This generic answer has implications for education and research. In education, it suggests thatstudents are likely to benefit from presentations by both types of expert (in X and in Y), and the
knowledge that each expert has of the otherÕs field should help to provide an integrated framework for
the joint presentations. In research, it suggests that research endeavors involving multiple principal
investigators (PIs) are likely to be more successful on average than single-PI endeavors.Stovepiping can also cause problems for graduate students who are interested in dissertation work,although for graduate students these problems may be less severe than for faculty. Some universities
make it easier for graduate students to do interdisciplinary work by allowing a studentÕs doctoral work
to be supervised by a committee composed of faculty from the relevant disciplines. However, in the
absence of a thesis supervisor whose primary interests overlap with the graduate studentÕs work, it is
the graduate student himself or herself who must be the intellectual integrator. Such integration re-
quires a level of intellectual maturity and perspective that is often uncommon in graduate students.The course of graduate-level education in computing and in biology is different in some ways. Inbiology, students tend to propose thesis topics earlier in their graduate careers, and then spend the
remainder of their time doing the proposed research. In computer science (especially more theoretical
aspects), in contrast, proposals tend to come later, after much of the work has been done. Computer
science graduates do not usually obtain postdoctoral positions, more commonly moving directly to
industry or to a tenure-track faculty position. Receiving a postdoctoral appointment is often seen as a
sign of a weak graduate experience in computer science, making postdoctoral opportunities in biology
seem less attractive.10.3.3.3  Coordination Costs
In general, the cost of coordinating research and training increases with interdisciplinary work.When computer scientists collaborate with biologists, they also are likely to belong to different depart-
ments or universities. The lack of physical proximity makes it harder for collaborators to meet, coordi-
nate student training, and share physical resources, and studies indicate that distance has especially
strong effects on interdisciplinary research.76Recognizing the importance of reducing distances between collaborators, Stanford UniversityÕsBio-X program is designed specifically to foster communication campus-wide among the various disci-
plines in biosciences, biomedicine, and bioengineering. The Clark Center houses meeting rooms, a
shared visualization chamber, low-vibration workspace, a motion laboratory, two supercomputers, the76J. Cummings and S. Kiesler, KDI Initiative: Multidisciplinary Scientific Collaborations, report to National Science Foundation,2003, available at http://netvis.mit.edu/papers/NSF_KDI_report.pdf; R.E. Kraut, S.R. Fussell, S.E. Brennan, and J. Seigel, ÒUn-derstanding Effects of Proximity on Collaboration: Implications for Technologies to Support Remote Collaborative Work,Ó pp.137-162 in Distributed Work, P.J. Hinds and S. Kiesler, eds., MIT Press, Cambridge, MA, 2002.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.374CATALYZING INQUIRYsmall-animal imaging facility, and the Biofilms center. Other core shared facilities available to theStanford research community include a bioinformatics facility, a magnetic resonance facility, a
microarray facility, a transgenic animal facility, a cell sciences imaging facility, a product realization lab,
the Stanford Center for innovation in in vivo imaging, a tissue bank, and facilities for cognitive neuro-
science, mass spectrometry, electron microscopy, and fluorescence-activated cell sorting.77Interdisciplinary projects are often bigger than unidisciplinary projects, and bigger projects increasecoordination costs. Coordination costs are reflected in delays in project schedules, poor monitoring of
progress, and an uneven distribution of information and awareness of what others in the project are
doing. Coordination costs also reduce peopleÕs willingness to tolerate logistical problems that might be
more tolerable in their home contexts. Furthermore, they increase the difficulty of developing mutual
regard and common ground, and they lead to more misunderstandings.78Coordination costs can be addressed in part through changes in technology, management, funding,and physical resources. But they can never be reduced to zero, and learning to live with greater over-
head in conducting interdisciplinary work is a sine qua non for participants.10.3.3.4  Risks of Retraining and Conversion
Retraining or conversion efforts almost always entail reduced productivity for some period of time.This fact is often viewed with dread by individuals who have developed good reputations in their
original fields, and who may worry about sacrificing a promising career in their home field while
entering at a disadvantage in the new one. These concerns are especially pronounced when they involve
individuals in midcareer rather than recently out of graduate school.Such fears often underlie the failure of individuals seeking to retool themselves to commit them-selves fully to their new work. That is, they seek to maintain some degree of ties to their original fieldsÑ
some research, some keeping up with the literature, some publishing in familiar journals, some going to
familiar conferences, and so on. These efforts drain time and energy from the retraining process, but
more importantly they may inhibit the necessary mind-set of success and commitment in the new
domain of work. (On the other hand, keeping a foot in their old fields could also be viewed as a rational
hedge against the possibility that conversion may not be successful in leading to a new field of special-
ization. Moreover, maintaining the discipline of continual output is a task that requires constant prac-
tice, and oneÕs old field is likely to be the best source of such output.)10.3.3.5  Rapid But Uneven Changes in Biology
Biology is an enormously broad field that contains dozens of subfields. Over the past few decades,these subfields have not all advanced or prospered equally. For example, molecular and cell biology
have received the lionÕs share of biological funding and prestige, while subfields such as animal behav-
ior or ecology have faired much less well. Molecular and cell biology (and more recently genomics,
proteomics, and neuroscience) have swept through as departments modernize, in a kind of Òband-
wagonÓ effect, leaving some of the more traditional subfields to lie fallow because promising young
scholars in those subfields are unable to find permanent jobs or establish their careers due to these
shifts.Moreover, prospering subfields are highly correlated with the use of information technology. Sucha close association of IT with prospering fields is likely to exacerbate lingering resentments from non-
prospering subfields toward the use of information technology.77For more information see http://biox.stanford.edu/.78J. Cummings and S. Kiesler, ÒCollaborative Research Across Disciplinary and Institutional Boundaries,Ó Social Studies ofScience, in press, available at http://hciresearch.hcii.cs.cmu.edu/complexcollab/pubs/paperPDFs/cummings_collaborative.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE37510.3.3.6  Funding Risk
Tight funding environments often engender in researchers a tendency to behave conservatively andto avoid risk. That is, unless special care is taken to encourage them in other directions (e.g., through
special programs in the desired areas), researchers seeking funding are likely to pursue avenues of
intellectual inquiry that are likely to succeed. Such researchers are therefore strongly motivated to
pursue work that differs only marginally from previous successful work, where paths to success can
largely be seen even before the actual research is undertaken. These pressures are likely to be exacer-
bated for senior researchers with successful and well-respected groups and hence many mouths to feed.
This point is addressed further in Section 10.3.5.3.10.3.3.7  Local Cyberinfrastructure
Section 7.1 addressed the importance of cyberinfrastructure to the biological research enterprisetaken as a whole. But individual research laboratories need to be able to count on the local counterpart
of community-wide cyberinfrastructure. Institutions generally provide electricity, water, and library
services as part of the infrastructure that serves individual resident laboratories. But information and
information technology services are increasingly as important to biological research as these more
traditional services, and thus it makes sense to consider that they might be provided as a part of the
local infrastructure.On the other hand, regarding computing and information services as part of local infrastructure hasinstitutional implications. For example, one important issue is providing centralized support for decen-
tralized computing. Useful scientific computing must be connected to a network, and networks must
interact and must be run centrally, but nonetheless, scientific computing must be accomplished in the
way scientific instruments are used, that is, very much under the control of the researcher. How can
institutions develop a computing infrastructure that delivers the cost effectiveness and the robustness
and the reliability of well-run centralized systems while at the same time delivering the flexibility
necessary to support innovative scientific use? In many research institutions, managers of centralized
computing regard researchers as cowboys uninterested in exercising any discipline for the larger good,
while researchers regard the managers of centralized computing as bureaucrats who are disinterested
in the practice of science. Though neither of these caricatures is correct, these divergent views of how
computing should effectively be deployed in a research organization will continue to exist unless the
institution takes steps to reconcile them.10.3.4  Barriers in Commerce and Business
10.3.4.1  Importance Assigned to Short-term Payoffs
In a time frame roughly coincident with the dot-com boom, commercial interest in bioinformaticswas very highÑperhaps euphoric in retrospect. Large, established, biotech-pharmaceutical companies,
genomics-era drug discovery companies, and tiny start-ups all believed in the potential for
bioinformatics to revolutionize drug design and even health care, and these beliefs were mirrored in
very high stock prices.More recently, market valuations of biotech firms have dropped along with the rest of the technol-ogy sector, and these more recent negative trends have affected the prevailing sentiment about the
value of bioinformatics for drug design, at least for the short term. Although the human genome
sequencing is complete, only a handful of drugs now in the pipeline stemmed from bioinformatic
analysis of the genome. Bioinformatics does not automatically lead to marketable ÒblockbusterÓ drugs,
and drug companies have realized that the primary bottlenecks involve biological knowledge: not
enough is known of the overall biological context of gene expression and gene pathways. In the wordsCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.376CATALYZING INQUIRYof one person at a 2003 seminar, ÒThis is work for [biological] scientists, not bioinformaticists.Ó For thisreason, further large-scale business investment in bioinformaticsÑand indeed for any research with a
long time horizonÑis difficult to justify on the basis of relatively short-term returns and thus is unlikely
to occur.These comments should not be taken to imply that bioinformatics and information technology havenot been useful to the pharmaceutical industry. Indeed, bioinformatics has been integrated into the
entire drug development process from gene discovery to physical drug discovery, even to computer-
based support for clinical trials. Also, there is a continuing belief that bioinformatics (e.g., simulations of
biological systems in silico and predictive technologies) will be important to drug discovery in the long
term.10.3.4.2  Reduced Workforces
The cultural differences between life scientists and computer scientists described in Section 10.3.2have ramifications in industry as well. For example, a sense that bioinformatics is in essence technical
work or programming in a biological environment leads easily to the conclusion that the use of formally
trained computer scientists is just an expensive way of gaining a year or two on the bioinformatics
learning curve. After all, if all of the scientists in the company use computers and software as a matter
of course and can write SQL (Structured Query Language) queries themselves, why should the com-
pany have on its payroll a dedicated bioinformaticist to serve as an interface between scientists and
software? In a time of expansion and easy money, perhaps such expenditures are reasonable, but when
cash must be conserved, such a person on staff seems like an expensive luxury.10.3.4.3  Proprietary Systems
In all environments, there is often a tension between systems built in a proprietary manner andthose built in an open manner, and the bioinformatics domain is no exception. Proprietary systems are
often not compatible or interoperable with each other, and yet vendors often think that they can maxi-
mize revenues through the use of such systems. This tendency is particularly vexing in bioinformatics
where integration and interoperability have so much value for the research enterprise. Standards and
open application programming interfaces are one approach to addressing the interoperability problem.
But as is often the case, many vendors support standards only to the extent that they are already
incorporated into existing product lines.10.3.4.4  Cultural Differences Between Industry and Academia
As a general rule, private industry has done better than academia in fostering and supportinginterdisciplinary work. The essential reason is that disciplinary barriers tend to be lower and teamwork
is emphasized when all are focused on the common goals of making profits and developing new and
useful products. By contrast, the coin of the realm in academic science is individual recognition for a
principal investigator as measured by his or her publication record.This difference appears to have consequences in a variety of areas. For example, expertise related tolaboratory technique is important to many areas of life sciences research. In an industrial setting, this
expertise is highly valued, because individuals with such expertise are essential to the implementation
of processes that lead to marketable products. These individuals receive considerable reward and
recognition in an industrial setting. Although such expertise is also necessary for success in academic
research, lab technicians rarelyÑif everÑreceive rewards that are comparable to the rewards accrued
by the principal investigator.Related to this is the matter of staffing a laboratory. In todayÕs job environment, it is common for anewly minted Ph.D. to take several postdoctoral positions. If in those positions an individual does notCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE377develop a sufficient publication record to warrant a faculty position, he or she is for all intents andpurposes out of the academic research gameÑa teaching position may be available, but taking a posi-
tion that primarily involves teaching is not regarded as a mark of success. However, it is exactly
individuals with such experience that are in many instances the backbone of industrial laboratories and
provide the continuity that is needed for a productÕs life cycle.The academic drive for individual recognition also tends to inhibit collaboration. Academic re-search laboratories can and do work together, but it is most often the case that such arrangements have
to be negotiated very carefully. The same is true for large companies that collaborate with each other,
but such companies are generally much larger than a single laboratory and intracompany collaboration
tends to be much easier to establish. Thus, the largest projects involving the most collaborators are
found in industry rather than academia.Even ÒsmallÓ matters are affected by the desire for individual recognition. For example, academiclaboratories often prepare reagents according to a lab-specific protocol, rather than buying standard-
ized kits. The kit approach has the advantage of being much less expensive and faster to put into use,
but often does not provide exactly the functionality that custom preparation offers. That is, the aca-
demic laboratory has arranged its processes to require such functionality, whereas an industrial labora-
tory has tweaked its processes to permit the use of standardized kits.The generalization of this point is that because academic laboratories seek to differentiate them-selves from each other, the default position of such laboratories is to eschew standardization of re-
agents, or of database structure for that matter. Standardization does occur, but it takes a special effort
to do so. This default position does not facilitate interlaboratory collaboration.10.3.5  Issues Related to Funding Policies and Review Mechanisms
As noted in Section 10.2.5.2, a variety of federal agencies support work at the BioComp interface.But the nature and scale of this support vary by agency, in terms of the procedures for making decisions
about what proposals are worthy of support.10.3.5.1  Scope of Supported Work
For example, although the NIH does support a nontrivial amount of work at the BioComp interface,its approach to most of its research portfolio, across all of its institutes and centers, focuses on hypoth-
esis-testing researchÑresearch that investigates well-isolated biological phenomena that can be con-
trolled or manipulated and hypotheses that can be tested in straightforward ways with existing meth-
ods. This focus is at the center of reductionist biology and has undeniably been central to much of
biologyÕs success in the past several decades.On the other hand, the nearly exclusive focus on hypothesis testing has some important negativeconsequences. For example, experiments that require breakthrough approaches are unlikely to be di-
rectly supported. Just as importantly, advancing technology that could facilitate research is almost
always done as a sideline. This has had a considerable chilling effect in general on what could have
been, but the impact is particularly severe for implementation of computational technologies in biologi-
cal sciences. That is, in effect as a cultural aspect of modern biological research, technology development
to facilitate research is not considered real research and is not considered a legitimate focus of a stan-
dard grant. Thus, even computing research that would have a major impact on the advancement of
biological science is rarely done (Box 10.6 provides one example of this reluctance).It is worth noting two ironies. First, it was the Department of Energy, rather than the NIH, thatsupported the Human Genome Project. Second, the development of technology to conduct polymerase
chain reaction (PCR)Ña technology that is fundamental to a great deal of biological research today and
was worthy of a Nobel Prize in 1993Ñwould have been ineligible for funding under traditional NIH
funding policy.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.378CATALYZING INQUIRYTo illustrate the consequences in more concrete but future-oriented terms, the list below suggestssome of the activities that would be excluded under a funding model that focuses only on hypothesis-
testing research:¥Developing technologies that enable data collection from a myriad of instruments and sensors,including real-time information about biological processes and systems, that permit us to refine and
annotate this information and incorporate it into accessible repositories to facilitate scientific study or
biomedical procedures;¥Flexible database systems that allow incorporation of multiscale, multimodal information aboutbiological systems by enabling the inclusion (by data federation techniques such as mediation) of informa-
tion distributed in an unlimited number of other databases, data collections, Web sites and so on;¥Acquisition of Òdiscovery-drivenÓ data (discovery science, as described in Chapter 2) to populatedatasets useful for computational analytical methods, or improvements in data acquisition technology
and methodology that serve this end;¥Development of new computational approaches to meet challenges of complex biological sys-tems (e.g., improved algorithmic efficiency, development of appropriate signal processing or signal
detection statistical approaches to biological data); and¥Data curation efforts to correct and annotate already-acquired data to facilitate greaterinteroperability.These considerations suggest that expanding the notion of hypothesis may be useful. That is, thediscussion above regarding hypothesis testing refers to biological hypotheses. But to the extent that thekinds of research described in the immediately preceding list are in fact part of 21st century biology,
nonbiological hypotheses may still lead to important biological discoveries. In particular, a plausible
and well-supported computational hypothesis may be as important as a biological one and may beinstrumental in advancing biological science.Today, a biological research proposal with excellent computational hypotheses may still be rejectedbecause reviewers fail to see a clearly articulated biological hypothesis. To guard against such situa-Box 10.6Agencies and High-risk, High-payoff Technology DevelopmentAn example of agency reluctance to support technology development of the high-risk, high-payoff variety isoffered by Robert Mullan Cook-Deegan:1In 1981, Leroy Hood and his colleagues at Caltech applied for NIH (and NSF) funding to support their efforts toautomate DNA sequencing. They were turned down. Fortunately, the Weingart Institute supported the initial workthat became the foundation for what is now the dominant DNA sequencing instrument on the market. By 1984,
progress was sufficient to garner NSF funds that led to a prototype instrument two years later. In 1989, the newlycreated National Center for Human Genome Research (NCHGR) at NIH held a peer-reviewed competition for large-scale DNA sequencing. It took roughly a year to frame and announce this effort and another year to review the
proposals and make final funding decisions, which is a long time in a fast-moving field. NCHGR wound up fundinga proposal to use decade-old technology and an army of graduate students but rejected proposals by J. Craig Venterand Leroy Hood to do automated sequencing. Venter went on to found the privately funded Institute for Genomic
Research, which has successfully sequenced the entire genomes of three microorganisms and has conducted manyother successful sequencing efforts; HoodÕs groups, first at Caltech and then at the University of Washington, wenton to sequence the T cell receptor region, which is among the largest contiguously sequenced expanses of human
DNA. Meanwhile, the army of graduate students has yet [in 1996, eds.] to complete its sequencing of the bacteriumEscherichia coli.1R. Mullan Cook-Deegan, ÒDoes NIH Need a DARPA?,Ó Issues in Science and Technology XIII:25-28, Winter 1996.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE379tions, funding agencies and organizations would be well served by including in the review processreviewers with the expertise to identify plausible and well-supported computational hypotheses that
may aid their biological colleagues in reaching a sound and unbiased conclusion about research propos-
als at the BioComp interface. More generally, these considerations involve changing the value proposition for what researchdollars should support. At an early point in a research fieldÕs development, it certainly makes sense to
emphasize very strongly the creation of basic knowledge. But as a field develops and evolves, it is not
surprising that a need to consolidate knowledge and make it more usable begins to emerge. In the
future, a new balance will have to be struck between the creation of new knowledge and making that
knowledge more valuable to the scientific community.10.3.5.2  Scale of Supported Work
In times of limited resources (and times of limited resources are always with us), unconventionalproposals are suspect. Unconventional proposals are even more suspect when they require large
amounts of money. No better example can be found than the reactions in many parts of the life sciences
research community to the Human Genome Project when it was first proposedÑwith a projected price
tag in the billions of dollars, the fear was palpable that the project would drain away a significant
fraction of the resources available for biological research.79Work at the BioComp interface, especially in the direction of integrating state-of-the-art computingand information technology into biological research, may well call for support at levels above those
required for more traditional biology research. For example, a research project with senior expertise in
both biology and computing may well call for support for co-principal investigators. Just as biological
laboratories generally require support for lab technicians, a BioComp project could reasonably call for
programmers and/or system administrators. (A related point is that for a number of years in the recent
past [i.e., during the dot-com boom years] computer scientists commanded relatively high salaries.)In addition, some areas of modern life sciences research, such as molecular biology, rely on largegrants for the purchase of experimental instruments. The financial needs for instrumentation and labo-
ratory equipment to collect the data necessary for undertake the data-intensive studies of 21st century
biology are significant, and are often at a scale that is unaffordable to all but a small number of academic
institutions. Although large grants are not unheard of in computer science, the across-the-board depen-
dence of important subfields of biology on experiment means that a larger fraction of biological research
is supported through such mechanisms than is true in computer science.To the extent that proposals for work at the BioComp interface are more costly than traditionalproposals and supported by the same agencies that fund those traditional proposals, it will not be
surprising to find resistance when they are first proposed.What is the scale of increased cost that might be associated with greater integration of informationtechnology into the biological research enterprise? If one believes, as does the committee, that informa-
tion technology will be as transformative to biology as it has been to many modern businesses, IT will
affect the way that biological research is undertaken and the discoveries that are made, the infrastruc-
ture necessary to allow the work to be done, and the social structures and organizations necessary to
support the work appropriately.Similar transformations have occurred in fields such as high finance, transportation, publishing,manufacturing, and discount retailing. Businesses in these fields tend to invest 5-10 percent of their
gross revenues in information technology,80 and this is with data that is well structured and under-stood. It is thus not unreasonable to suggest that a full integration of information technology into the
biological research enterprise might have a comparable cost. Today, there is federal support for only a
very small fraction of that amount.79See, for example, L. Roberts, ÒControversial from the Start,Ó Science 291(5507):1182-1188, 2001.80See, for example, http://www.bain.com/bainweb/publications/printer_ready.asp?id=17269.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.380CATALYZING INQUIRY10.3.5.3  The Review Process
Within the U.S. government, there are two styles of review. In the approach relying mainly on peerreview (used primarily by NIH and NSF), a proposal is evaluated by a review panel that judges its
merits, and the consensus of the review panel is the primary factor that influencing a decision that a
proposal does or does not merit funding. When program budgets are limited, as they usually are, the
program officer decides on actual awards from the pool of proposals designated as merit-worthy. In the
approach relying on program officer judgment (used primarily by DARPA), a proposal is generally
reviewed by a group of experts, but decisions about funding are made primarily by the program officer.The dominant style of review mechanism in agencies that support life sciences research is peerreview. Peer review is intended as a method of ensuring the soundness of the science underlying a
proposal, and yet it has disadvantages. To quote an NRC report,81The current peer-review mechanism for extramural investigator-initiated projects has served biomedicalscience well for many decades and will continue to serve the interests of science and health in the decades
to come. NIH is justifiably proud of the peer review mechanism it has put in place and improved over theyears, which allows detailed independent consideration of proposal quality and provides accountabilityfor the use of funds. However, any system that focuses on accountability and high success rates in
research outcomes may also be open to criticism for discriminating against novel, high-risk proposals thatare not backed up with extensive preliminary data and whose outcomes are highly uncertain. The prob-lem is that high-risk proposals, which may have the potential to produce quantum leaps in discovery, do
not fare well in a review system that is driven toward conservatism by a desire to maximize results in theface of limited funding resources, large numbers of competing investigators, and considerations of ac-countability and equity. In addition, conservatism inevitably places a premium on investing in scientists
who are known; thus there can be a bias against young investigators.Almost by definition, peer review panels are also not particularly well suited to considering areas ofresearch outside their foci. That is, peer review panels include the individuals that they do precisely
because those individuals are highly regarded as experts within their specialties. Thus, an interdiscipli-
nary proposal that draws on two or more fields is likely to contain components that a review panel in a
single field is not able to evaluate as well as those components that do fall into the panelÕs field.A number of proposals have been advanced to support a track of scientific review outside thestandard peer review panels. For example, the NRC report recommended that NIH establish a special
projects program located in the office of the NIH director, funded at a level of $100 million initially to
increase over a period of 10 years to $1 billion a year, whose goal would be to foster the conduct of
innovative, high-risk research. Most importantly, the proposal calls for a set of program managers to
select and manage the projects supported under this program. These program managers would be
characterized primarily by an outstanding ability to develop or recognize unusual concepts and ap-
proaches to scientific problems. Review panels constituted outside the standard peer review mecha-
nisms and specifically charged with the selection of high-risk, high-payoff projects would provide
advice and input to program managers, but decisions would remain with the program managers.
Research initially funded through the special projects program that generated useful results would be
handed off after 3-5 years for further development and funding through standard NIH peer review
mechanisms. Whether this proposal, or a similar one, will be adopted remains to be seen. Different agencies also have different approaches to the proposals they seek. For example, agenciesdiffer in the amount of detail that they insist potential grantees provide in these proposals. Depending
on the nature of the grant or contract sought, one agency might require only a short proposal of a few
pages and minimal documentation, whereas another agency might require many more pages, insisting
on substantial preliminary results and extensive documentation. An individual familiar with one kind81National Research Council, Enhancing the Vitality of the National Institutes of Health: Organizational Change to Meet New Chal-lenges, The National Academies Press, Washington, DC, 2003, p. 93.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CULTURE AND RESEARCH INFRASTRUCTURE381of approach may not be able to cope easily with the other, and the overhead involved in coping with anunfamiliar approach can be considerable.As one illustration, the committee heard from a professor of computer science, accustomed to theNSF approach to proposal writing, who reported that while many biology departments have grant
administrators who provide significant assistance in the preparation of proposals to NIH (e.g., telling
the PI what is required, drafting budgets, filling out forms, submitting the proposal), his department (of
computer science) was unable to provide any such assistanceÑand indeed lacked anyone at all with
expertise in the NIH proposal process. As a result, he found the process of applying for NIH support
much more onerous than he had expected.10.3.6  Issues Related to Intellectual Property and Publication Credit
Issues related to intellectual property (IP) are largely outside the scope of this report. However, it ishelpful to flag certain IP issues that are particularly likely to be relevant in advancing the frontiers at the
intersection of computer science and biology. Specifically, because information technology enables the
sensible use of enormous volumes of biological data, biological findings or results that emerge from
such large volumes are likely to involve the data collection work of many parties (e.g., different labs).
Indeed, biology as a field recognizes as significant, and even primary, the generation of good experi-
mental data about biological phenomena. By contrast, multiparty collaborations on a comparable scale
are unusual in the world of computer science, and datasets themselves are less significant. Thus, com-
puter scientists may well be taken aback by the difficulties in negotiating permissions and credit.A second issue arises that is related to tensions between open academic research and proprietarycommercialization of intellectual advantages. Because of the potential that advances in bioinformatics
will have great commercial value, there are incentives to keep some research in bioinformatics propri-
etary (hence, not easily accessible to the peer community, less amenable to peer review, and less
relevant to professional development and advancement). In principle, this is not particularly different at
the BioComp interface than in any other research area of commercial value. Nevertheless, the fact that
traditions and practices from two different disciplines (disciplines that are at the forefront of economic
growth today) are involved rather than just one may exacerbate these tensions.A third point is the potential tension between making data publicly available and the intellectualproperty rights of journal publishers. For example, some years ago a part of the neuroscience commu-
nity sought to build a functional positron emission tomography database. In the course of their efforts,
they found that they needed to add substantial prose commentary to the image database to make it
useful. Some of the relevant neuroscience journals were reluctant to give permission to use large ex-
tracts from publications in the database. To the extent that this example can be generalized, it suggests
that efforts to build a far-reaching cyberinfrastructure for biology will have to identify and deal with
intellectual property issues as they arise.8282In responding to this report in draft, a reviewer argued that by taking collective action, the major research institutions couldexert strong leverage on publishers to relax their copyright requirements. Today, many top-rated journals require as a conditionof publication the transfer of all copyright rights from the author to the publisher. Given the status of these journals, thisreviewer argued that it is a rare researcher who will take his or her paper from a top-rated journal to a secondary journal withless stringent requirements in order to retain copyright. However, the researcherÕs home institution could adopt a policy inwhich the institution retained the basic copyright (e.g., under the work-for-hire provisions of current copyright law) but allowedresearchers to license their work to publishers but not to transfer the copyright on their own accord. Under such circumstances,goes the argument, journal publishers would be faced with a situation of rejecting work not just from one researcher but from allresearchers at institutions with such a policyÑa situation that would place far more pressure on journal publishers to relax theirrequirements and would improve the ability of researchers to share their information through digital resources and databases.
The committee makes no judgment about the wisdom of this approach, but believes that the idea is worth mention.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONCLUSIONS AND RECOMMENDATIONS38338311Conclusions and Recommendations11.1  DISCIPLINARY PERSPECTIVES
11.1.1  The Biology-Computing Interface
The committee began this study with two key notions. First, it hoped to identify a field of intellec-tual inquiry associated with the biology-computing interface that drew equally and bilaterally on com-
puting and biology. Second, it hoped to explicate a symmetry between computing and biology in which
the impact of computing on biology was increasingly deep and profound and in which biology would
have a comparable effect on computing.Both of these notions proved unfounded in certain important ways. From the standpoint of applica-tions, technology, and practical utility, the committee saw substantial asymmetry. Computing has had
a huge transformational impact on biology and will span virtually all areas of life sciences research, but
the impact of biology on computing is likely to be much more targeted (i.e., affecting specific problem
domains within computing), and large-scale, biology-based technology changes for computing are in
the relatively distant future if they occur at all. At the same time, the committee did find that the
epistemological and conceptual frameworks of each field may have in the future some substantial
influence on the other. The committee believes that an engineering and computational view (as dis-
cussed in Chapter 6) will increasingly be recognized as an important way of looking at biological
systems. In a parallel though somewhat more speculative vein, the committee also believes that insight
into biological mechanisms may have important impact on how certain problems in computing can be
approached (as discussed in Chapter 8).The reason for the deep and transformational impact of computing on biology is that insight intothe vast and heterogeneous datasets of 21st century biology will be possible only through the applica-
tion of computing to analyze and manage those data. (This is not to deny that many quantitative
sciences will contribute to biology, although this report has focused primarily on the computing dimen-
sions.) Views among biologists about where best to deploy computing resources will surely differ, but
the main contributions of computing to biology will come from new ideas for solving complex biologi-
cal problems and new models for testing hypotheses; from delivering cyberinfrastructure for biology
research, providing ever more computing power, distributed computing and storage, complex soft-
ware, fault-tolerant computing, and so forth; and from training fearless scientists who can find the rightCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.384CATALYZING INQUIRYcollaborators for whatever difficulties arise at the frontier. That is, specific computing-to-biology ÒtechtransferÓ of intellectual ideas will have some impact, but the greatest impact of computing on biology
will come from an overall acceleration of the pace of progress.To fulfill the promise of 21st century biology, research scientists from both computer and biologicalscience need to work together more extensively, more often, and more closely than ever before. As
quantitative methods are increasingly adopted within the biological sciences, it will be possible to
answer a new range of scientific questions, not just to accelerate research progress. Uncovering the
meaning implicit in the complete sequence of the human genome to deliver on the promises of the
project for society is an obvious case.A revitalized enterprise driven by this newly trained cadre of interdisciplinary scientists andmaintained through a balance of individual investigator-initiated and group projects along with
continued technology and computational advances, will be able (1) to address fundamental questions
in biology such as the relationship of structure to function and the basis for homeostasis; (2) to
integrate biological knowledge across the vast scales of time, space, and organizational complexity
that characterize biology; (3) to translate basic biology to preventive, predictive, and personalized
medicine and to extend biological knowledge to engineering soft materials and other industrial
nanobiotechnology contributions; and (4) to uncover how biology can contribute to energy produc-
tion and environmental restoration. The Committee on the Frontiers at the Interface of Computing

and Biology believes that such a vision for 21st century biology is realistic, and that the implementa-
tion of its recommendations would ensure decades of exponential progress and a major transforma-
tion of our understanding of life.On the other side of the interface, biological inspiration for new approaches to computing continuesto be important, in the sense that biology provides existence proofs that information-processing tech-
nology based on biochemistry rather than on silicon electronics is possible. For areas of computing that
are generally complex and unwieldy in the associated technologies available so far to address them, or
areas lacking in empirical and/or theoretical knowledge, inspiration from whatever source is wel-
comeÑand biological inspiration is most likely to be valuable in these areas. (For other areas of comput-
ing, whose intellectual terrain is well explored and for which a solid base of empirical and theoretical
knowledge is available, biological inspiration is both unnecessary and less interesting, because good
and useful solutions are available without any kind of biological connection at all.)Furthermore, computer scientists tend to be most interested in the general applicability of theirwork and are often less interested in work that is relevant to only one problem domain. Individuals
from this perspective should thus understand the key difference between applications-driven research
and applications-specific research. That is, problems in the life sciences can be important drivers of
computer science research, and in many cases the knowledge developed in seeking solutions to these
problems will be applicable in other domains.Finally, it is worth noting one possible domain of symmetry between the two fields, although it is asymmetry of ignorance rather than one of knowledge. Both computing and biology provide objects of
enormous complexity whose behavior is not well understoodÑconsider the Internet and a cell. It may
well turn out that studying each of these objects as systems can yield insights useful in understanding
the otherÑand the same kinds of (yet-to-be-developed) formalism may apply to bothÑbut the jury is
still out on this possibility.11.1.2  Other Emerging Fields at the BioComp Interface
Apart from computing-enabled biology and biologically inspired computing, a number of othernew areas of inquiry are also emerging at the BioComp interface, although in addition to biology and
computing they draw from chemistry, materials science, bioengineering, and biochemistry. Some of
these efforts can be characterized loosely as different flavors of biotechnology, and three of the most
important are analytical biotechnology, materials biotechnology, and computational biotechnology.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONCLUSIONS AND RECOMMENDATIONS3851.Analytical biotechnology describes the application of biotechnological tools for the creation of
chemical measurement systems. Examples include the creation of sensors from DNA-binding proteins
for the detection of trace amounts of arsenic and lead in ground waters, and the development of
nanoscale DNA cascade switches that can be used to identify single molecular events. Significant
challenges for analytical biotechnology arise in proteomics, glycomics, and lipidomics.2.Materials biotechnology entails the use of biotechnological methods for the fabrication of novel
materials with unique optical, electronic, rheological, and selective transport properties. Examples in-
clude novel polymers created from genetically engineered polypeptide sequences and the formation of
nanowires and circuits from metal nanoparticles attached to a DNA backbone.3.Computational biotechnology focuses on the potential replacement of silicon devices withnanoscale biomolecular-based computational systems. Examples include the creation of DNA switches
from hairpin structures and the programmable self-assembly of DNA tiles for the creation of memory
circuits.A common feature of many of the three new biotechnology application areas is that they all requirethe production of well-characterized, functional biopolymer nanostructures. The molecular precision
and specificity of the enzymatic biochemical pathways employed in biotechnology can often surpass
what can be accomplished by other chemical or physical methodsÑa point that is especially relevant to
the problem of nanoscale self-assembly. It is this fine control of nanoscale architecture exhibited in
proteins, membranes, and nucleic acids that researchers hope to harness with these applied biotech-
nologies.An important enabler of the production of such nanostructures, especially on a large scale, is theavailability of increasingly standardized and increasingly automatable fabrication techniques. In some
ways, the status of fabrication technologies for these nanostructures is similar to the status of integrated
circuit fabrication technology several decades ago, which evolved from a laboratory activity with trial-
and-error doping of individual devices to a large-scale automated enterprise driven by design automa-
tion software over a period of 20 years beginning in the early 1960s.Although they draw on biology and computing (along with other disciplines), the tools of theseparent disciplines are being applied by researchers in these new biotechnological areas to a different
and unrelated set of scientific interests and goals, and these areas often attract scientists with no inter-
ests in or ties to traditional biology or computing research. Indeed, these researchers are likely to find
intellectual homes in areas such as neuroscience, robotics, and space exploration.These new areas also have obvious relevance to computing. For example, computational biotech-nology is relevant to computing in the same way that lithographic silicon fabrication technologies are
todayÑunderpinning these latter technologies are understandings of fundamental physics and well-
developed electrical engineering techniques and approaches. Similarly, computational biotechnology
will draw on materials science and biochemistry as well as biology as it seeks to create highly regular
DNA nanoparticles, mate DNA with submicron electronic structures fabricated in silicon, and create
networks of interconnecting nanostructures with unique enzyme communication paths. Analytical and
materials biotechnologies are also relevant for enabling MEMSÑmicroelectromechanical systems that
interact with the physical world (taking in data through various sensors and affecting the world through
various actuators).11.2  MOVING FORWARD
The committee believes that the most important barriers today impeding the broader integration ofcomputing and information technology into life sciences research are cultural barriers. Twenty-first
century biology will not entail a diminution of the central role that traditional empirical or experimental
research plays, but it will call for the whole-hearted embrace of a style of biology that integrates
reductionist biology with systems biology research. At the same time, computing and physical scienceCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.386CATALYZING INQUIRYpractitioners must be wary of underestimating the true complexity of biological systems and, in particu-lar, of inappropriately applying their traditional intellectual paradigms for simplicity to biology.Over the long run, a change in the culture of academic life sciences research is required to sustainthe approaches needed for 21st century biology, due to the increased need for disparate skill sets and
collaborative approaches, a change that emphasizes interdisciplinary teams that integrate biology and
computing expertise. For this reason, the main focus of this chapterÕs conclusions and recommendations
concern actions that can accelerate the required cultural shift. By contrast, reflecting the committeeÕs
view that the impact of biological research is likely to be more modest in scope and scale, the conclu-
sions and recommendations place less emphasis on biologyÕs impact on computing. (In this light,
Chapters 4-8 of this report should not be seen as laying out a research agenda for computing-enabled
biology or for biology-inspired computing, but rather as suggesting some of the areas in which the
frontiers of the interface have been pushedÑand that still hold considerable intellectual interest.)11.2.1  Building a New Community
The most important target of promoting cultural change is people. Thus, it should be a key objectiveof science policy makers to create a large, multitalented population of individuals who can act as the
intellectual translators and mediators along the frontier, a group that will directly foster interdiscipli-
nary research and technology development. True for any discipline or research area involving disparate
skill sets, such an approach is especially critical at the interface between the fields of biology and
computing because these areas are enjoying the most rapid growth and intellectual progress. Both
junior and senior talent must be cultivated, the former to be the basis of a next generation ready to
develop and exploit the technology and conduct the science, and the latter to serve in mentorship and
leadership roles.This message is not a new oneÑindeed, private programs such the Burroughs-Wellcome Founda-tion Interfaces in Sciences have avidly sought the development of community. Nevertheless, it remains
true that despite many studies, reports, and proclamations, universities and federal funding agencies
have fallen short of the goal of fully facilitating a range of interdisciplinary science and minimizing the
birth pains associated with new hypotheses and directions.1An essential aspect of this community is the ability to build on each otherÕs work. Indeed, the mostadvanced and sophisticated cyberinfrastructure imaginable will be ineffective if different laboratories
and researchers are not motivated or are unwilling to work together or to share data and other informa-
tion. Formal collaborations between individual laboratories or researchers do exist, of course, but these
exist entirely on the basis of individually negotiated arrangements between consenting parties. A differ-
ent, and complementary, model of working together is one in which individuals researchers contribute
to and draw from an entire research community. In spirit, this model is the familiar one of publishing
research articles and supporting information (data, software) for others to cite and use as appropriate in
their own researchÑand the dominant ethos of the new community should be one of sharing rather
than withholding.This section provides some core principles on how individuals and institutions might help tosupport and nurture such work. The core principles described here may come across as Òmotherhood
and apple pie,Ó but it is often the case that such motherhood is not honored as fully as one might think
appropriate. The committee does recognize the centrality of providing appropriate incentives for hon-1For example, a report was prepared by the National Institutes of Health (NIH) and the National Science Foundation (NSF) inAugust 2001 addressing many of the cultural issues described in Chapter 10. This report on training in bioengineering andbioinformatics, Assessing Bioengineering and Bioinformatics Research Training, Education, and Career Development, recommended thatmeasures be taken to (1) increase the number of fellowships and institutional training grants at all career levels that include
quantitative, computational biology and integrative systems modeling; (2) include funds to support faculty with complementaryexpertise (e.g., computer scientists to teach biologists); and (3) support the development of curricula. In the intervening 2 years,the importance of continued efforts in these areas has not diminished.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONCLUSIONS AND RECOMMENDATIONS387oring these principles. Both institutions and funding agencies have important roles to play in providingincentives for change when these principles are not honored and for continuity when they are. Thus, the
core principles for institutions (Section 11.2.3) and for funding agencies (Section 11.4.1) should be seen
partly in this light.11.2.2  Core Principles for Practitioners
The following items are offered as advice to current and prospective researchers at the BioCompinterface. These workers include those seeking to retrain themselves to work at the BioComp interface
(e.g., a postdoctoral fellow with a computer science background working in a biology laboratory), those
facilitating such retraining (e.g., the director of a biology laboratory employing such a postdoc), and
those who collaborate as peers with others (e.g., a tenured professor of computer science working with
a tenured professor of biology on some interesting problem). Practitioners should:¥Respect their partners. Neither the biologist who sees the computer scientist only as a craftsmanwriting computer programs for data analysis nor the computer scientist who sees the biologist as a
provider of dirty and unreliable data shows respect for the other. Scientists with quantitative back-
grounds and scientists with biomedical backgrounds must work as peers if their collaborations are to be
successful.¥Have reasonable expectations. OneÕs intellectual partners in an interdisciplinary endeavor will havediffering and often unfamiliar intellectual paradigms. Both vocabulary and epistemology will be differ-
ent, and a respect for other ways of looking at the world reflects an understanding that paradigms can
be different for very sound reasons.¥Avoid hype. In the quest for funding and attention, practitioners need to maintain a high degree ofquestioning to avoid hype, unrealistic expectations, or empty promises.¥DonÕt complain. Complaining to close colleagues about the apparently poor science practiced byother disciplines further reinforces xenophobic arrogance and chauvinism. When the other parties sense
such arrogance, the trust needed to achieve scientific collaboration is no longer available.2¥Seek new techniques and intellectual inspiration everywhere. Both biology and computer science havetraditions of applying other disciplines to their problems. For example, LeeuwenhoekÕs optical micro-
scope led to the discovery of cells, electrical recording devices revealed the voltage-gated channels in
neuronal signaling, and knowledge of crystallography uncovered the helical structure and code of
DNA. Computer science, originating from a marriage between electrical engineering and mathematics,
continues to maintain close intellectual connections to these disciplines.¥Nurture young talent. The key to long-term growth of a new field is the ability to sustainand nurture young scientists working in that field. To the extent that attention can be focused on
young scientists (e.g., targeting this generation with well-placed, exciting, and novel funding
opportunities), problems of competing for funds with senior groups working on classical topics
can be reduced.The committee understands that these principles will have different meaning to researchers atdifferent stages of their careers. For those early in their careers, these recommendations should be
taken as a checklist of things to keep in mind as they engage with colleagues and seek support.
However, these items are also relevant to senior researchers who serve as role models for their
younger colleagues.2G. Wiederhold, ÒScience in Two Domains,Ó unpublished working paper, Department of Computer Science, Stanford Univer-sity, March 2002, updated February 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.388CATALYZING INQUIRY11.2.3  Core Principles for Research Institutions
The following items are offered as advice to institutions that are supporting work at the BioCompinterface. These institutions include academic laboratories, research centers, and departments, as well
as business or commercial operations with a research component. Collectively, these items are based on
the barriers to collaboration and community discussed in Chapter 10. However, no attempt has been
made to specifically align recommendations with barriers because in most cases, the correspondence is
many-to-many rather than many-to-one or one-to-many.Relevant institutions should:¥Attract and retain professionals with quantitative, computational, and engineering skills to work inbiological fields. As a rule, recruitment and retention will require reasonable career tracks that hold thepromise of long-term stability and upward mobility. If good individuals are to be attracted to and
retained in any enduring interdisciplinary area, they must have career opportunities that offer the
potential for growth. For example, these individuals must be assured that their intellectual work at the
interface will be fairly evaluated. Such issues are matters of academic survival for many young faculty,
and if processes are not put into place explicitly that ensure an appropriately rigorous but still fair
evaluation process, promising faculty may well have strong disincentives to pursue research at the
interface. A corollary is that traditional departments often see considerable opportunity cost in support-
ing (and granting tenure to) individuals who do not fit squarely in their centers of gravity (Section
10.3.3); thus, independent support for researchers with interdisciplinary interests, or support that can-
not be converted to individuals with traditional interests, helps to remove the threat that departments
may see.¥Support retraining efforts. Because much of the computing talent required at the BioComp inter-face will have to come from individuals with substantial prior experience in computing, retraining will
be an essential part of efforts to build the talent base. Individuals considering retraining will be more
motivated to do so if funding agencies and tenure and promotion committees wishing to support these
faculty members recognize that retooling takes some time to be successful and do not penalize them for
lowered productivity during such periods.¥Develop curricula for interdisciplinary teaching of quantitative, computational, and engineering sciencesmade relevant to the BioComp interface. Note the desirability of such curricula being made available inmultiple formatsÑonline versus in class, 2-week courses versus semester-length courses, and so onÑas
well as on multiple topics. Over the long run, it is likely that immersion in these curricula will become
a natural part of the educational process for all budding biologists, but today, obtaining this back-
ground requires some special effort.¥Facilitate networking. Especially for newcomers to a line of work, intellectual connection to othersplays an important role in their integration into the new community. An institution can promote
informal knowledge exchange and the establishment of social relationships on campus through on-site
seminars for like-minded individuals. It can also facilitate off-campus connections by providing sup-
port for travel to tutorials, workshops, and seminars.¥Nurture partnerships. It is desirable for senior scientists from different intellectual backgrounds towork at the interface and for peer relationships between biologists and computer scientists to develop.
Partnerships are best undertaken in close proximity with intense interaction, and even small issues such
as office arrangements (e.g., whether or not a computer scientist has an office or a desk in the laboratory
of a collaborator or partner) can seriously inhibit the development of close partnerships.3 Many sce-narios could promote partnerships, such as sabbatical visits and the establishment of positions at cen-3For example, a computer scientist developing software to aid in the analysis of biological data would be well advised to spendenough time in the laboratory to understand the actual needs of his or her biologist colleagues. Software delivered Òover thetransomÓ is unlikely to be used easily, a point suggesting that there is more to software design than the development of anappropriate algorithm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONCLUSIONS AND RECOMMENDATIONS389ters of excellence. Partnerships with industry could ensure sabbaticals in complementary work environ-ments and stimulate knowledge dissemination to commercial applications.¥Recognize collaborative work. A corollary of partnerships is that experts from disparate disciplineswill collaborate in publication. Institutions thus have a responsibility to provide fair and appropriate
evaluation measures for tenure and promotion cases in which the individuals involved have under-
taken large amounts of collaborative work. For example, departments may have to be induced to
expand their definitions of tenurable work, or universities may have to establish extradepartmental
mechanisms for granting and holding tenure outside of traditional departments.¥Maintain excellence. Research at the BioComp interface is inherently interdisciplinary, and evalu-ation of such research faces all of the problems described above. Nevertheless, problem domains that
are at the interface of two disciplines can attract not only highly talented individuals who see interesting
and important problems but also individuals of lesser talent who are unable to meet the exacting
standards of one discipline and are seeking a home where the standards of acceptance are lower.
Individuals in the first category are to be sought and cherishedÑindividuals in the second category
ought to be shunned.¥Provide mentors. Mentors play a strong role in the success of any retraining effort. However,mentoring individuals who have an established track record of success in another field is different. For
example, such individuals may be less able to work autonomously and more likely to flail or drift
without an activist mentor than someone with a background in the same field. Shared mentorships may
make particular sense in these circumstances, as illustrated by the Burroughs-Wellcome requirement
that fellowship awardees have a mentor from outside the department of primary appointment.¥Reward good behavior. It has been observed that behavior that is rewarded institutionally is behav-ior that tends to take hold and to be internalized. The institutions with which individual researchers are
associated can play important roles in providing such rewards, especially with respect to the principles
described in Section 11.2.2.11.3THE SPECIAL SIGNIFICANCE OF EDUCATIONAL INNOVATION
AT THE BIOCOMP INTERFACEThe pursuit of 21st century biology will require a generation of biologists who can appreciatefundamental statistical approaches, evaluate computational tools and use them appropriately, and
know how to choose the best collaborators from the quantitative sciences as a whole. To support the
education of this generation, an integrative education, whether formal or informal, will be needed.Many reports have acknowledged a need for broader training.4 Increasingly, bioinformatics pro-grams at both the undergraduate and the graduate level do entail study in mathematics, computer
science, and the natural sciences.11.3.1  Content
The committee fully supports these trends and encourages them further, with the strong caveat thatan appropriate curriculum to deal with the interface of computing and biology should not simply be the
union of course requirements from multiple departments. Courses and other work that deal explicitly
with the integrative issues are necessary, and one of the most important skills that such interdiscipli-
nary courses can teach is the ability to communicate among the relevant disciplines. This does not entail
simply learning the jargon of each one (though this is, of course, essential), but also interleaving the
training in such a way that the student continually sees and explores various parallels between the4See, for example, National Research Council, Bio2010: Undergraduate Education to Prepare Biomedical Research Scientists, TheNational Academies Press, Washington, DC, 2003.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.390CATALYZING INQUIRYdifferent fields of study. Later, in a collaboration, the ability to identify, explain, and exploit theseparallels will be valuable.Cultural barriers should be discussed and addressed specifically. Where it seems easy to dismisssome math or physics as irrelevant to biology, case studies can be assembled to show successes and
contrast these with failures or counterproductive avenues. Where it seems easy to dismiss biology as too
detail oriented and reductionistic, similar case studies showing the need to understand minute details
of the living machinery are also necessary.It is broadly agreed that an essential element of 21st century biology is the (re)introduction ofquantitative science to the biological science curriculum. The committee recognizes, however, that such
reintroduction should not be equated with an abstract, theoretical approach devoid of experimentation
or phenomenology, and educational programs for 21st century biology must provide sound footing in
quantitative science alongside a clear understanding of the intricacies of biology.In light of the discussions in Chapter 6 regarding the view of biological organisms as engineeredentities, the committee believes that students of 21st century biology would benefit greatly from some
study of engineering as well. In this view, the committee emphasizes most strongly its support for the
recommendations of the BIO2010 report for exposure to engineering principles (discussed in Chapter
10), at the earliest possible time in the training of life scientists. Just as engineers must construct physical
systems to operate in the real world, nature also must operate under these same constraintsÑphysical
lawsÑto ÒdesignÓ successful organisms. Despite this fundamental similarity, biology students rarely
learn the important analysis, modeling, and design skills common to engineering curricula nor a suite of
topics such as engineering thermodynamics, solid and fluid dynamics, control theory, and so forth, that
are key to the engineerÕs (and natureÕs) ability to design physical systems.The particular area of engineering (electrical, mechanical, computer, and so forth) is probably muchless relevant than exposure to essential principles of engineering design: the notion of trade-offs in
managing competing objectives, control systems theory, feedback, redundancy, signal processing, in-
terface design, abstraction, and the like (Box 11.1). Ready intellectual access to such notions is likely to
enable researchers in this area to search for higher-level order in the data forest. Indeed, as biology
continues to examine the system-wide functioning of a large number of interacting components, engi-
neering skills may become necessary for successful biological research.11.3.2  Mechanisms
The committee believes that the availability of individuals with significant computing expertise isan important limiting factor for the rate at which the biological sciences can absorb such expertise.5 Thefield, to include both basic and applied life sciences research, is extraordinarily large and dwarfs most
other fields outside of engineering itself; thus, influx from other fields is not likely to result in large-scale
infusion of computing expertise. Only integrated education of new researchers, along with some re-
training of existing researchers, can bring benefits of the computing to a large segment of that world,
and previous calls from groups such as Biomedical Information Science and Technology Initiative
(BISTI) that a new generation of 21st century researchers must be trained remain compelling, true, and
overdue.Given this perspective, it is appropriate to offer educational opportunities across a broad front.Educational opportunities should span a range in several dimensions, including the following:¥Time and format. Monthly lectures or seminars, short-duration workshops (of several weeks),survey courses, undergraduate minors, undergraduate majors, graduate degrees, and postdoctoral5This belief is not based on the existence of a ÒshortageÓ or ÒscarcityÓ in the sense that economists generally recognize. Rather,it is rooted in the premise that most of biology could benefit intellectually from the integration of significant computing exper-tise, and the observation that such integration is more the exception than the rule when taken across biology writ large.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONCLUSIONS AND RECOMMENDATIONS391(re)training focusing on the BioComp interface can serve to motivate interest (when they require littleinvestment or time commitment) or to serve a strong professional interest (when the time commitment
required is substantial).6 Short-term opportunities for cross-disciplinary ÒpollinationÓ workshops thatbring together fields from both sides of the interface and provide a vehicle for tutorials and other
educational exchanges are particularly useful in that they have a low cost of entry for participants; thus,
those who are dabbling can be enticed more easily.¥Content. Although genome informatics is perhaps the most obvious topic, computational tech-niques and approaches will become increasingly relevant to all aspects of biological researchÑand
educational opportunities should target a wide range of subfields in biology.¥Target audience. Given the need for more computing expertise in biology, it is appropriate toprovide instruction at multiple levels of sophistication in different fields. Some research biologists have
substantial informal computing experience but would benefit greatly from more formal exposure; suchBox 11.1Some Engineering Ideas and Concepts That Biologists May Find Useful¥Control theory (feedback, optimization, game theory)¥Model design¥Signal processing (gain, signal-to-noise, cross-talk)¥Engineering thermodynamics and energy¥Optimal design theory¥Modularity (and protocols)¥Robustness¥Multiscale and large-scale stochastic simulation¥Network theory or graph theory¥Fluid and solid dynamics or mechanics¥ÒCollective behaviorÓ from physics¥Reverse engineering¥Computational complexity (decidability, P-NP)¥Information theory, source and channel coding¥Dynamical systems: dynamics, bifurcation, chaos¥Statistical physics: phase transitions, critical phenomena6In this regard, the model of statistics as a discipline may offer a good example for the way in which bioinformatics mightbecome a discipline of its own. Many universities offer three types of programs in statistics. The first and most formal program isdesigned for those aspiring to become professional, academic statisticiansÑthat is, those aspiring to become researchers in the
field of statistics. This program usually culminates in the Ph.D. degree and establishes an absolutely sound theoretical under-standing of the foundations of statistics. The second program is intended for individuals who intend to become professionalapplied statisticiansÑthat is, those who will work in industry, perhaps in research, and whose primary responsibilities will
involve carrying out statistical analyses using established statistical methods. Often, individuals pursuing this degree track willstop with a masterÕs degree and in some cases even with a bachelorÕs degree. The third program involves a set of coursesintended for individuals who will be getting a degree in another field, but who have a need for significant understanding of
statistical methods so that those methods might be applied in the individualÕs home field. In very large universities, sometimesthese third-track courses are specialized even further so that we might see courses in business statistics, biological statistics, oreven medical statistics. Similarly, it seems reasonably clear that in the field of bioinformatics there will always be a need forresearchers, whose primary interest will be in devising new algorithms, new models, and new methods in bioinformatics. Therewill also be a need for applied bioinformaticiansÑthose whose primary responsibility will be in applying establishedbioinformatics methods to current projects in biology and biotechnology. It also seems reasonable to suppose that there will be
those whose careers in other disciplines will be enhanced by some knowledge of bioinformatics. (These latter two program typesmay have to provide the biological knowledge to those trained primarily in computation or the computational overview to thosetrained primarily in biology.)Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.392CATALYZING INQUIRYindividuals are in an obviously different situation than those whose only exposure to computing isspreadsheets and word processors.The development of such educational opportunities generally requires resources, such as releasetime; assistance in compiling lecture notes, assembling readings, or grading; funding for developing
online courses, travel to workshops, and so on. Furthermore, it is desirable to share the outcomes of
such development with the academic community (e.g., in the form of online courses, published books,
and open commentary about successes and failures). Funding agencies can also provide incentives for
such cooperative efforts by giving higher funding priority to research proposals that are put forward in
partnerships between or among universities.11.4  RECOMMENDATIONS FOR RESEARCH FUNDING AGENCIES
The committee believes that it is possibleÑand feasibleÑfor agencies to support work at theBioComp interface that serves to develop simultaneously (1) fundamental knowledge that enables
broad advances in biology; (2) technical innovations that help to improve the quality of life and enhance
industrial competitiveness; and (3) the creation and sustenance of a critical mass of talentedscientists

and engineers intellectually capable and professionally positioned to work creatively at the BioComp
interface and to train new generations effectively.Funding agencies and nongovernmental supporters of research have traditionally been able toinfluence the course of research through the allocation of resources to particular research fields, and the
committee believes that funding at the biology-computing interface is no exception. This support has
made important contributions in the past, and the committee urges that such support be continued and
expanded.11.4.1  Core Principles for Funding Agencies
Recognition of the importance in focusing on the BioComp interface amplifies earlier agency-cen-tered studies and reflects its unprecedented richness. Responding to the opportunities, the scientific
community, private foundations, and the federal government have taken the first steps in recognizing
this enormous intellectual opportunity.However, no single agencyÑlet alone any individual program, directorate, institute, center, orofficeÑowns the science or the excitement and promise at the interface between computing and biol-
ogy. Neither can a single agency by itself establish and sustain a process to realize the grand opportuni-
ties. In their growing commitment to this frontier science effort, the Defense Advanced Research Projects
Agency (DARPA), the National Science Foundation (NSF), the National Institutes of Health (NIH), and
the Department of Energy (DOE) each have unique objectives and existing expertise. To exploit the
potential fully, the agencies, more than ever before, will have to collaborate and also seek (formal or
informal) partnerships with private foundations and industry. Extensive interactions including fully
open, joint planning exercises and shared support for technical workshops will be central to true
coordination at the agency level.As is the case for individuals and institutions, a number of core principles provide good desideratafor the funding policies and practices of agencies. Again, these core principles are not particularly
newÑbut remain essential to realizing goals at the BioComp interface. Of course, how these principles
are instantiated is key.To obtain maximum impact, funding agencies and foundations should pay appropriate attention tothe following items. Agencies and foundations should:¥Support awards that can be used for retraining purposes. While a number of agencies have supportedsuch awards for individuals at early stages of their careers, these programs are fewer in number than inCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONCLUSIONS AND RECOMMENDATIONS393the past. Also, to the best of the committeeÕs knowledge, there are no programs that explicitly targetsenior faculty for retraining at the BioComp interface, although, as noted in Section 10.2.2.6, NIH does
support a retraining program open to scientists of many backgrounds to undertake biomedical re-
search. To the extent that such programs continue to exist, agencies should seek to publicize them
beyond their usual core constituencies.¥Balance quality and excellence against openness to new ideas in the review process. Intellectual excel-lence is central. Yet especially in interdisciplinary work, it is also important to invest in work that
challenges existing assumptions about how research in the field ÒshouldÓ be conductedÑand the prob-
lem is that traditional review mechanisms often have a hard time distinguishing between proposals for
such work and proposals for work that simply does not meet any reasonable standard of excellence.
This point suggests that agencies wishing to support work at the BioComp interface would be wise to
find review mechanisms that can draw on individuals who collectively have the relevant interdiscipli-
nary expertise and, as importantly, an appropriate forward-looking view of the field.¥Encourage team formation. It is important not to discriminate against team-researched articles inindividual performance evaluations and to provide incentives for universities to reward multiple mem-
bers of cross-disciplinary teams of investigators. Under todayÕs arrangements, work performed by an
individual as part of a team often receives substantially less credit than work performed by an indi-
vidual working alone or with graduate students.¥Provide research opportunities for investigators at the interface who are not established enough to obtainfunding on the strength of their track record alone. In these instances, balance must be struck betweentaking a chance on an unproven track record and shutting down nonfruitful lines of inquiry. One
approach is to set time limits (a few years) on grants made to such individuals, requiring them to
compete on their own against more established investigators after the initial period. (As in other
fields, the duration of Òa few yearsÓ is established by the fact that it is unreasonable to expect
significant results in less time, and norms of regular funding set an upper limit for this encourage-
ment of work outside the boundaries.)¥Use funding leverage to promote institutional change. That is, agencies can give priority or differentialadvantages to proposals that are structured in certain ways or that come from institutions that demon-
strate commitments to change. For example, priority or preference could be given to proposals thatÑInvolve co-principal investigators from different disciplines;
ÑOriginate in institutions that offer grant awardees tenure-track faculty appointments with
minimal teaching responsibilities (as illustrated by the Burroughs-Welcome Career Awards (Sec-
tion 10.2.2.5.2));
ÑHave significant and active educational efforts or programs at the BioComp interface; and
ÑMake data available to the larger biological community in standard forms that facilitate reuse
and common interpretation.7 (This action is predicated on the existence of such standards, andagencies should continue to support efforts to develop these common data standards.)¥Use publication venues to promote institutional change. Funding agencies could require as a condi-tion of publication that authors deposit the data associated with a given publication into appropriate
community databases in accordance with relevant curation standards. They could also insist that pub-
lished work describing computational models be accompanied by assurances that detailed code inspec-
tion of models is possible under an appropriate nondisclosure agreement.7The committee notes without comment that the desire on the part of science agencies to promote wider data sharing andinteroperability may conflict with requirements emanating from other parts of the federal government with regard to informa-
tion management in biomedical research. While science agencies are urging data sharing, other parts of the government canimpose restrictions on sharing biomedical data associated with individual human beings in the name of privacy, and theserestrictions can have significant impact on the architecture of biomedical information systems. In some cases, these regulatory
compliance issues have such impact that biomedical scientists have strong incentives to introduce a paper step into their datamanagement processes in order to escape some of the more onerous consequences of these regulations for their informationsystems.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.394CATALYZING INQUIRY¥Support cyberinfrastructure for biological research. Though the National Science Foundation hastaken a lead in this area, the issue of supporting cyberinfrastructure for biological research transcends
any single agency. Chapter 7 discussed the importance of data repositories and digital libraries in
cyberinfrastructure, and it is in these areas that other agencies have important roles to play. Across the
board, agencies engaged in supporting biological research will need to support mechanisms for long-
term data storage and for continuous curation and annotation of the information resources gathered in
publicly supported research for 21st century biology to reach its full potential as a global distributed
intellectual enterprise.¥Recognize quality publicly. Given the role of peer recognition in the value sets of most scientists(especially in their earlier years), public recognition of innovative work can be a strong motivator.
Public recognition can take many formsÑthough by definition the number of people that can
be recognized is necessarily limited. For example, outstanding researchers can be invited to give
keynote addresses at important conferences or profiled in reports to Congress or other important
public documents.¥Recognize the costs of providing access to computing and information resources. Especially at theBioComp interface, collaboration between peers as compared to an investigation conducted by an
individual researcher almost always requires larger grants. Researchers need more support for comput-
ing and information technology as well as the expertise needed to exploit those capabilities and, in
instances that push the computing state of the art, support for high-level expertise as well.¥Define specific challenge problems that stretch the existing state of the art but are nevertheless amenable toprogress in a reasonable time frame. An agency could pose challenge problems drawn from the problemdomains described in Chapter 9. Any number of such challenge problems would be arbitrary, but a
selected few goals of broad impact would influence more complete participation by the community and
make further funding opportunities by other agencies more likely. Note that when common test sets or
other common criteria can be provided or used, clearer metrics for success can be established. A corol-
lary is that agencies should obtain community buy-in with respect to the specifics of such problems. (As
one example, the DOE Office of Biological and Environmental Research specified what microbes to
tackle for complete genome sequencing through a series of Òwhich bugÓ workshops to obtain commu-
nity input on the projects that would be best.)¥Work with other agencies. Different agencies bring to the table different types of expertise, and forwork at the interface, multiple kinds of expertise are always necessary. Thus, agency partnerships (such
as the current collaboration between NIHÕs National Institute of General Medical Sciences (NIGMS) and
NSFÕs Mathematical and Physical Sciences Directorate) may allow proposals at the interface to be
evaluated more fairly and ongoing projects to be overseen more effectively.8¥Provide the funding necessary to capitalize on the intellectual potential of 21st century biology. Chapters2-7 of this report have sought to demonstrate the broad impact of computing and information technol-
ogy on biology. However, a necessary condition to realize this impact is a funding stream that is
adequate in magnitude and sustained over long enough periods. As noted in Section 10.3.5.2, a bench-
mark for comparison is that spending in information-intensive fields such as finance is on the order of
5 to 10 percent of overall budgets. A second necessary condition is the use of a peer review process that
is broadly sensitive to the perspectives of researchers in the new field and is willing to take chances on
new ideas and approaches. As always, the public sector should focus on growing the seed corn for both
people and ideas on which the future depends. Finally, although the committee would gladly endorse
an increased flow of funding to the furtherance of a truly integrated 21st century biology, it does
understand the realities of a budget-constrained environment.8This partnership between the NIGMS and the NSF seeks to award 20 grants in mathematical biology and anticipates morethan $24 million in awards over 5 years. NIGMS supports research and training in the basic biomedical sciences. NSF fundsmathematical and other quantitative sciences such as physics, computer science, and engineering. See http://www.nigms.nih.gov/news/releases/biomath.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONCLUSIONS AND RECOMMENDATIONS395The following sections are addressed to specific funding agencies.11.4.2  National Institutes of Health
As the largest funder of life sciences research, the NIH has a special responsibility to support andfacilitate the building of bridges between biology and other disciplines, especially including computing.
The NIH has already taken a number of commendable steps in seeking to collaborate with other
agencies, including the formal NIGMS partnership with NSF for mathematical biology mentioned in the
previous section and other less formal partnerships with NSF and DOE for structural biology and the
National Center for Research Resources (NCRR)-NSF collaborations in instrumentation. As noted in
Chapter 10, a National Research Council report in 2003 called for NIH to increase investment in high-
risk, high-potential-payoff life sciences research that would be supported outside the usual NIH peer
review system.Such steps, and others like them, are to be encouraged. At the same time, NIH must addressobstacles in a number of other areas that impede the building of bridges between biology and comput-
ing. One important issue is that cooperation across organizational boundaries within NIH leaves much
to be desired. Translational medicine will not arise from funding mechanisms that isolate narrow slices
of human biology, and yet the NIH structure is oriented toward specific diseases and body functions.9No component of a human works separately, in isolation. Most diseases are not single-gene defects,
most proteins act in macromolecular assemblies, organ systems interact by chemical messengers, the
immune system and the circulatory system not only work together but impact all organs of the body,
and so on. The NIH structure has been successful for many years, but the fact remains that its organiza-
tional structure tends to place similar restraints on cross-institute support for collaborative research
(Box 11.2).A consequence of organization of research fields in biology by subfield (e.g., by disease, or by bodyfunction) is that efforts that can benefit the entire community may suffer, even though specialization is
necessary to achieve depth of knowledge. The true value of the large-scale deployment of cyberinfra-
structureÑand especially its data componentsÑis that cyberinfrastructure spans disciplines to inte-
grate findings in one subfield with findings in another subfieldÑto connect information from one
subfield to another subfield, perhaps even via a third subfield. In the absence of explicit direction and
coordination, cyberinfrastructure in one subfield is likely to be incompatible in important ways with
cyberinfrastructure designed and deployed in another. Achieving coordination is likely to require a
level of cooperation across agencies that is substantially greater than has historically been true. It will
also require a level of planning and agency involvement in the actual design of the cyberinfrastructure
that does not typically happen in the funding of research, in which the role of program officers is
primarily to ensure a fair assessment of the science by peer reviewers. In supporting cyberinfrastructure,
program officers must act as procurement officers on behalf of the overall scientific community, and not
just as impartial brokers of an independent discipline-focused review process.9There are 20 institutes within the National Institutes of Health, including the National Cancer Institute (NCI); the NationalEye Institute (NEI); the National Heart, Lung, and Blood Institute (NHLBI); the National Human Genome Research Institute(NHGRI); the National Institute on Aging (NIA); the National Institute on Alcohol Abuse and Alcoholism (NIAAA); the National
Institute of Allergy and Infectious Diseases (NIAID); the National Institute of Arthritis and Musculoskeletal and Skin Diseases(NIAMS); the National Institute of Biomedical Imaging and Bioengineering (NIBIB); the National Institute of Child Health andHuman Development (NICHD); the National Institute on Deafness and Other Communication Disorders (NIDCD); the National
Institute of Dental and Craniofacial Research (NIDCR); the National Institute of Diabetes and Digestive and Kidney Diseases(NIDDK); the National Institute on Drug Abuse (NIDA); the National Institute of Environmental Health Sciences (NIEHS); theNational Institute of General Medical Sciences (NIGMS); the National Institute of Mental Health (NIMH); the National Institute
of Neurological Disorders and Stroke (NINDS); the National Institute of Nursing Research (NINR); and the National Library ofMedicine (NLM).Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.396CATALYZING INQUIRYNIH also supports some of the most scientifically sophisticated research environments in the world.As noted in the Botstein-Smarr report,10 it is in these environments that it makes the most sense to trainthe leaders of the new generation of biologists with computing expertise. These environments are
generally mature enough to support the conduct of interdisciplinary research at the interface, and a
widespread geographical diffusion of young scientists with such expertise will help to generate the
broad impact sought by NIH.Perhaps the most important barrier of all is the philosophy that governs much of the current studygroup approach to proposal review. For historical reasons, the most important and prominent support-
ers of life sciences researchÑsuch as NIHÑhave focused almost exclusively on hypothesis-testing
researchÑresearch that investigates well-isolated biological phenomena that can be controlled or ma-
nipulated and hypotheses that can be tested in straightforward ways with existing methods. This focus
is at the center of reductionist biology and has undeniably been central to much of biologyÕs success in
the past several decades.At the same time, the nearly exclusive focus on hypothesis testing has some important negativeconsequences. For example, experiments that require breakthrough approaches are unlikely to be di-
rectly supported. Just as importantly, advancing technology that could facilitate research is almost
always done as a sideline. Thus, investigators must often disguise an attempt to undertake the develop-
ment of tools or models of great generality by applying them to some (any!) biological system. Subse-
quent citations of such papers are almost always for the part that explains the new tool or model rather
than the phenomenon to which the tool or model was applied.Box 11.2The Shapiro Report on the Structure and Organization of theNational Institutes of HealthA 2003 National Research Council report on the structure and organization of NIH came to conclusions andmade recommendations that are consistent with the view of NIH described in this report. Specifically, theearlier report noted:[T]here is a high payoff potential for carefully selected large- and small-scale strategic projects that require theparticipation of numerous organizations working in partnership. . . . Well-planned, broad-based, trans-NIH pro-
grams will be necessary to meet most effectively scientific or public health needs. . . . Furthermore, there is no formalmandate for NIH to identify, plan, and implement such crosscutting strategic initiatives. [Such crosscutting initiativesare necessary because] scientific mechanisms, risk factors, and social and behavioral influences on health and
disease cut across traditional disease categories. Many patients have multiple chronic conditions, so a patient-centered approach to health care and health promotion will sometimes require integration and synergy across[Institutes and Centers]. [Such issues] lend themselves to a strategic coordinated trans-NIH response in which mul-
tiple institutes could collaborate on a research plan that cuts across administrative structures in terms of planning,funding, and sharing and disseminating results. . . . Proteomics . . . is [an] example [of such an issue]. . . . [C]oncertedtrans-NIH work on the assessment of existing and emerging technology platforms and database formats utilizingreference specimens, could help to advance the whole field and guide NIH-supported studies.The report went on to recommend that initially 5 percent of the NIH budget and eventually 10 percent shouldbe allocated to the support of such trans-NIH initiatives.SOURCE: National Research Council, Enhancing the Vitality of the National Institutes of Health: Organizational Change to Meet NewChallenges, The National Academies Press, Washington, DC, 2003, pp. 84-86.10NIH Working Group on Biomedical Computing, The Biomedical Information Science and Technology Initiative, June 1999. Avail-able at http://www.nih.gov/about/director/060399.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONCLUSIONS AND RECOMMENDATIONS397This has had a considerable chilling effect in general on what could have been, but the impact isparticularly severe for implementation of computational technologies within the biological sciences.
That is, in effect as a cultural aspect of modern biological research, technology development to facilitate
research is not considered real research and is not considered a legitimate focus of a standard grant.
Thus, even computing research that would have a major impact on the advancement of biological
science is simply not done.The committee believes that 21st century biology will be based on a synergistic mix of reductionistand systems biologies. For systems biology researchers, the committee emphasizes that hypothesis-
testing research will continue to be central in providing experimental verification of putative discover-
iesÑand indeed, relevant as much to studies of how components interact as to studies of components
themselves. Thus, disparaging rhetoric about the inadequacies and failures of reductionist biology and
overheated zeal in promoting systems biology should be avoided. For researchers more oriented to-
ward experimental or empirical work, the committee emphasizes that systems biology will be central in
formulating novel, interesting, and in some cases, counterintuitive hypotheses to test. The point sug-
gests that agencies that have traditionally supported hypothesis-testing research would do well to cast
a wide ÒdiscoveryÓ net that supports the development of alternative hypotheses as well as research that
supports traditional hypothesis testing.11.4.3  National Science Foundation
The primary large-scale initiative of NSF relevant to 21st century biology is its cyberinfrastructureeffort. Efforts in this area, including major community databases, collaborative research networks, and
interdisciplinary modeling efforts, will require grants that are larger than the Directorate for Biological
Sciences (BIO) of NSF has traditionally made, as well as greater continuity and stability. In particular,
cyberinfrastructure entails personnel costs (e.g., for programmers, systems administrators, and staff
scientists with the necessary computing expertise) that are not associated with the usual BIO-supported
grant. As for continuity, windows for support must be consistent with the practical considerations to
achieve success. Five-year awards and initial review at that point against specific milestones and
deliverables to the community are essential, and only at longer intervals should there be open calls for
proposals and competitive processes, save in the case of a resource failing to live up to community
expectations.11The professional biological community at large has at least two important roles to play with respectto cyberinfrastructure. First, it must articulate its needs and explicate how it can best exploit the re-
sources that cyberinfrastructure will make available. Second, it must develop a consensus on the expec-
tations that cyberinfrastructure facilities must meet if they are to be continued. Society events (e.g.,
annual meetings) provide a forum for such discussions to take place.11.4.4  Department of Energy
The DOEÕs Office of Science supports a number of programs in genomic studies and structuralbiology (as described in Chapter 10). This office has the capacity to provide sufficient funds and a stable
environment, but doing so has been a challenge in its overall institutional setting. The committee
believes that the payoffs for DOE missions will be extraordinary from the biology supported by the
Office of Science, but success requires that priority be given to stable, long-term programs.11In principle, review provisions could be analogous to the sunset considerations for NSF-supported Science and TechnologyCenters and Engineering Research Centers.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.398CATALYZING INQUIRY11.4.5  Defense Advanced Research Projects Agency
Of all the federal agencies, DARPA appears to be the most heavily involved in exploring thepotential of biology for computing. Chapter 8 describes a variety of potential influences of biology on
computing (the term Òapplications of biology for computingÓ would be promising too much), but in
truth, the ultimate value of biology for changing computing paradigms in deep and fundamental ways
is as yet unproven. Nevertheless, various biological attributesÑrobustness, adaptation, damage recov-
ery, and so onÑare so desirable from a computing point of view that any intellectual inquiry is valuable
if it can contribute to artificial humanly purposive systems with these attributes.In other words, investigations that consider the impact of biology on computing areÑin the ver-nacularÑhigh-risk, high-payoff studies. They are high risk because biology is not prescriptive in its
contributions and success is far from ensured. They are high payoff because computers that possess
attributes associated with biological systems would be enormously valuable. It is for this reason that
they do logically fall into programs supported by DARPA, which has a long tradition of supporting
high-risk, high-payoff work as part of its research portfolio. (As noted in Chapter 10, NSF also sponsors
a Small Grants Exploratory Research Program that supports high-risk research on a small scale.)From the committeeÕs perspective, the high-level goals articulated by DARPA and other agenciesthat support work related to biologyÕs potential contribution to computing seem generally sensible.
This is not to say that every proposal supported under the auspices of these agenciesÕ programs would
necessarily have garnered the support of the committeeÑbut that would be true of any research portfo-
lio associated with any program.One important consequence of supporting high-risk research is that it is unlikely to be successful inthe short term. ResearchÑparticularly of the high-risk varietyÑis often more ÒmessyÓ and takes longer
to succeed than managers would like. Managers understandably wish to terminate unproductive lines
of inquiry, especially when budgets are constrained. However, short-term success cannot be the only
metric of the value of research, and when it is, funding managers invite hyperbole and exaggeration on
the part of proposal submitters, and unrealistic expectations begin to characterize the field. Those
believing the hyperbole (and those contributing to it as well) thus overstate the importance and central-
ity of the research to the broader goal of improving computing. When unrealistic expectations are not
met (and they will not be met, almost by definition), disillusionment sets in, and the field becomes
disfavored from both a funding and an intellectual standpoint.From this perspective, it is easy to see why support for fields can rise rapidly only to drop precipi-tously a few years later. Wild budget fluctuations and an unpredictable funding environment that changes
goals rapidly can damage the long-term prospects of a field to produce useful and substantive knowledge.
Funding levels do matter, but programs that provide steady funding in the context of broadly stated but
consistent intellectual goals are more likely to yield useful results than those that do not.Thus, the committee believes that in the area of biologically inspired computing, funding agenciesshould have realistic expectations, and these expectations should be relatively modest in the near term.
Intellectually, their programs should continue to take a broad view of what Òbiological inspirationÓ
means. Funding levels in these areas ought to be established on a Òlevel-of-effortÓ basis (i.e., what
DARPA believes is a reasonable level of effort to be expended in this area), taking into account the
number of researchers doing and likely to do good work in this area and the potential availability of
other avenues to improved computing. Also, programmatic continuity should be the rule, with playing
rules and priorities remaining more or less constant in the absence of profound scientific discovery or
technology advances in the area.11.5  CONCLUSIONS REGARDING INDUSTRY
 Over the past decade, the commercial sector has provided important validation for the propositionthat information technology (IT) can have a profound impact on the life sciences. As noted in ChapterCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.CONCLUSIONS AND RECOMMENDATIONS39910, there are a host of firms, ranging in size from small start-ups to established multibillion-dollarcompanies that have significant investments in research efforts and products, that make substantial use
of IT in support of medical and pharmaceutical business.Nevertheless, the committee is aware that some large life science companies (e.g., large pharmaceu-tical companies) have not found their investments in information technology living up to their expecta-
tions. Some such companies have reported investing a great deal of money and time in bioinformatics
software and are now looking for and failing to find economic justification for further investment.The hype of the genome era was as intoxicating to many drug companies, it seems, as the Internetwas to mainstream investors, with just as much a comedown. There is a growing realization that the
availability of genomic information is not, by itself, sufficient to lead directly to immediately profitable
drug breakthroughs, regardless of the IT available to help manage and analyze that information. In-
deed, many bottlenecks in drug discovery remain that result from the lack of fundamental biological
knowledge about specific expression and pathways. Whereas the initial expectation was that the ge-
nome could be mined for likely drug targets, todayÕs approach involves a greater tendency to start with
the biology that is known to select likely targets, and then to look to the genome to find genes that
interact with those targets.The committee believes that bioinformaticsÑand broader uses of information technologyÑarelikely to have a positive effect on drug discovery in the long run, but that those enterprises looking to
investments in IT for short-term gain are likely to continue to be disappointed. Commercial advantages
to the use of IT will accrue from its integration into the entire process, from gene discovery to clinical
trials, benefiting both the entire process and the local situation to which information technology is
applied. Also, because of rapidly increasing biological knowledge, the promise of discovering appropri-
ate drug targets in the genome remains, although it is likely to be realized primarily in the long term.
Bioinformatics will also enable a more precise genome-based identification of individuals susceptible to
a given drugÕs side effects, possibly providing a basis for excluding them from clinical trials and
pharmaceutical applications involving that drug.11.6  CLOSING THOUGHTS
The impact of computing on biology could fairly be considered a paradigm change as biologyenters the 21st century. Twenty-five years ago, biology saw the integration of multiple disciplines from
the physical and biological sciences and the application of new approaches to understand the mecha-
nisms by which simple bacteria and viruses function. The impact of the early efforts was so significant
that a new discipline, molecular biology, emerged, and many biologists, including those working at the
level of tissues or systems and whole organisms, came to adopt the approaches and often even the
techniques. Molecular biology has had such success that it is no longer a discipline but simply part of
bioscience research itself.Today, the revolution lies in the application of a new set of interdisciplinary tools: computationalapproaches will provide the underpinning for the integration of broad disciplines in developing a
quantitative systems approach, an integrative or synthetic approach to understanding the interplay of
biological complexes as biological research moves up in scale. Bioinformatics provides the glue for
systems biology, and computational biology provides new insights into key experimental approaches
and how to tackle the challenges of nature. In short, computing and information technology applied to
biological problems is likely to play a role for 21st century biology that is in many ways analogous to the
role that molecular biology has played in biological research across all fields for the last quarter cen-
turyÑand computing and information technology will likely become embedded with biological re-
search itself.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.AppendixesCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A403403AThe Secrets of Life:
A MathematicianÕs Introduction to Molecular Biology
NOTE:  This appendix is a reprint of Chapter 1 of the National Research Council report 
Calculating the Secrets of Life:  Contribu-
tions of the Mathematical Sciences to Molecular Biology (National Academy Press, Washington, DC, 1995), copyright 1995 by theNational Academy of Sciences.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A405Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.406CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A407Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.408CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A409Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.410CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A411Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.412CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A413Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.414CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A415Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.416CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A417Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.418CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A419Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.420CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A421Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.422CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A423Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.424Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.425Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.426CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX A427Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.428CATALYZING INQUIRYCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX B429429BChallenge Problems in Bioinformatics and
Computational Biology from Other Reports
B.1 GRAND CHALLENGES IN COMPUTATIONAL BIOLOGY (David Searls)11.Protein structure prediction
2.Homology searches

3.Multiple alignment and phylogeny construction

4.Genomic sequence analysis and gene-finding
B.2 OPPORTUNITIES IN MOLECULAR BIOMEDICINE IN THE ERA OFTERAFLOP COMPUTING (Klaus Schulten et al.)
21.Study protein-protein and protein-nucleic acid recognition and assembly

2.Investigate integral functional units (dynamic form and function of large macromolecular and
supramolecular complexes)3.Bridge the gap between computationally feasible and functionally relevant time scales

4.Improve multiresolution structure prediction

5.Combine classical molecular dynamics simulations with quantum chemical forces

6.Sample larger sets of dynamical events and chemical species

7.Realize interactive modeling

8.Foster the development of biomolecular modeling and bioinformatics

9.Train computational biologists in teraflop technologies, numerical algorithms, and physical con-cepts10.Bring experimental and computational groups in molecular biomedicine closer together.
1D. Searls, ÒGrand Challenges in Computational Biology,Ó Computational Methods in Molecular Biology, S. Salzberg, D. Searls,and Simon Kasif, eds., Elsevier Science, 1998.2K. Schulten, G. Budescu, F. Molnar, Opportunities in Molecular Biomedicine in the Era of Teraflop Computing, NIH Resource forMacromolecular Modeling and Bioinformatics, March 3-4, 1999, Rockville, MD; see http://whitepapers.zdnet.co.uk/
0,39025945,60014729p-39000617q,00.htm.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.430CATALYZING INQUIRYB.3 WORKSHOP ON MODELING OF BIOLOGICAL SYSTEMS(Peter Kollman and Simon Levin)3Challenging Issues That Span All Areas of Modeling SystemsA. Integrating data and developing models of complex systems across multiple spatial and tempo-ral scales¥Scale relations and coupling¥Temporal complexity and coding¥Parameter estimation and treatment of uncertainty¥Statistical analysis and data mining¥Simulation modeling and predictionB. Structure-function relationships¥Large and small nucleic acids¥Proteins¥Membrane systems¥General macromolecular assemblies¥CeIlular, tissue, organismal systems¥Ecological and evolutionary systemsC. Image analysis and visualization¥Image interpretation and data fusion¥Inverse problems¥Two-, three- and higher-dimensional visualization and virtual realityD. Basic mathematical issues¥Formalisms for spatial and temporal encoding¥Complex geometry¥Relationships between network architecture and dynamics¥Combinatorial complexity¥Theory for systems that combine stochastic and nonlinear effects often in partially distributed systemsE. Data management¥Data modeling and data structure design¥Query algorithms, especially across heterogeneous data types¥Data server communication, especially peer-to-peer replication¥Distributed memory management and process managementB.4 WORKSHOP ON NEXT-GENERATION BIOLOGY: THE ROLE OF NEXT-GENERATIONCOMPUTING (Shankar Subramaniam and John Wooley)
4Exemplar Challenges for Bioinformatics and Computational Biology1.Full genome-genome comparisons
2.Rapid assessment of polymorphic genetic variations
3ÒModeling of Biological Systems,Ó P. Kollman and S. Levin (chairs), a workshop at the National Science Foundation, March 14and 15, 1996, available at http://www.resnet.wm.edu/~jxshix/math490/Modeling%20of%20Biological%20Systems.htm.4S. Subramaniam and J. Wooley, DOE-NSF-NIH 1998 Workshop on Next-Generation Biology: The Role of Next GenerationComputing, available at http://cbcg.lbl.gov/ssi-csb/nextGenBioWS.html.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX B4313.Complete construction of orthologous and paralogous groups of genes
4.Structure determination of large macromolecular assemblies/complexes

5.Dynamical simulation of realistic oligomeric systems

6.Rapid structural/topological clustering of proteins

7.Prediction of unknown molecular structures; protein folding

8.Computer simulation of membrane structure and dynamic function

9.Simulation of genetic networks and the sensitivity of these pathways to component stoichiom-
etry and kinetics10.Integration of observations across scales of vastly different dimensions and organization to
yield realistic environmental models for basic biology and societal needsB.5 TECHNOLOGIES FOR BIOLOGICAL COMPUTER-AIDED DESIGN (Masaru Tomita)
51.Enzyme engineering: to refine enzymes and to analyze kinetic parameters in vitro
2.Metabolic engineering: to analyze flux rates in vivo

3.Analytical chemistry: to determine and analyze the quantity of metabolites efficiently

4.Genetic engineering: to cut and paste genes on demand, for modifying metabolic pathways

5.Simulation science: to efficiently and accurately simulate a large number of reactions

6.Knowledge engineering: to construct, edit and maintain large metabolic knowledge bases

7.Mathematical engineering: to estimate and tune unknown parameters
B.6 TOP BIOINFORMATICS CHALLENGES (Chris Burge et al.)61.Precise, predictive model of transcription initiation and termination: ability to predict where
and when transcription will occur in a genome2.Precise, predictive model of RNA splicing/alternative splicing: ability to predict the splicing
pattern of any primary transcript3.Precise, quantitative models of signal transduction pathways:ability to predict cellular response
to external stimuli4.Determining effective protein-DNA, protein-RNA and protein-protein recognition codes

5.Accurate ab initio structure prediction

6.Rational design of small molecule inhibitors of proteins

7.Mechanistic understanding of protein evolution: understanding exactly how new protein func-
tions evolve8.Mechanistic understanding of speciation: molecular details of how speciation occurs

9.Continued development of effective gene ontologies-systematic ways to describe the functions
of any gene or protein10.(Infrastructure and education challenge)

11.Education: development of appropriate bioinformatics curricula for secondary, undergraduate,
and graduate educationB.7 EMERGING FIELDS IN BIOINFORMATICS (Patricia Babbitt)
71.Data storage and retrieval, database structures, annotation
2.Analysis of genomic/proteomic/other high-throughput information
5M. Tomita, ÒTowards Computer Aided Design (CAD) of Useful Microorganisms,Ó Bioinformatics 17(12):1091-1092, 2001.6C. Burge, ÒBioinformaticists Will Be Busy Bees,Ó Genome Technology, No. 17, January, 2002. Available (by free subscription) athttp://www.genome-technology.com/articles/view-article.asp?Article=20021023161457.7P. Babbitt et al., ÒA Very Very Very Short Introduction to Protein Bioinformatics,Ó August 22-23, 2002, University of Califor-nia, San Francisco, available at http://baygenomics.ucsf.edu/education/workshop1/lectures/w1.print2.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.432CATALYZING INQUIRY3.Evolutionary model building and phylogenic analysis
4.Architecture and content of genomes

5.Complex systems analysis/genetic circuits

6.Information content in DNA, RNA, protein sequences and structure

7.Metabolic computing

8.Data mining using machine learning tools, neural nets, artificial intelligence

9.Nucleic acid and protein sequence analyses
B.8 TEN GRAND CHALLENGES (Sylvia Spengler)81.The origin, structure, and fate of the universe
2.The fundamental structure of matter

3.EarthÕs physical systems

4.The diversity of life on Earth

5.The tree of life

6.The language of life

7.The web of life

8.Human ecology

9.The brain and artificial thinking machines
10.Integrating Earth and human systems

11.A knowledge server for planetary management
Research Across Domains: Data¥Information managementÑhuman evolution continued¥Exponential increase in data and information across domains¥Access to information across domainsÑas or more important than the information itself¥Integration of data across knowledge domains¥Apply analytical tools across knowledge domains¥Modeling of complex systems¥Simulation of phenomenaÑdescriptive science becomes predictive scienceResearch Across Domains: People¥Share data across disciplines¥Build and use analytical and modeling tools across disciplines¥Work in collaborative, cross-domain groupsResearch Across Domains: Time¥Real-time data access, integration, and analysis¥Real-time modeling and effects prediction¥Real-time dissemination of research results¥Real-time testing by research community¥Real-time policy discussions¥Real-time policy decisions8S. Spengler, Lawrence Berkeley National Laboratory, personal communication to John Wooley, January 3, 2005.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX B433B.9 GRAND CHALLENGES IN BIOMEDICAL COMPUTING (John A. Board, Jr.)
9Biomedical Applications from Coupling Imaging and Modeling¥Real-time noninvasive three-dimensional imaging of many body systems¥Real-time generation of three-dimensional patient-specific models¥Multiple-technology (multimodal) imaging and modeling¥Whole-organ modeling¥Multiple-organ system modeling¥Patient-specific modeling of organ anomalies¥Model support for (partial) restoration of hearing, coarse vision, and locomotion (via both para-lyzed and artificial limbs)All of these applications make use of:
¥Three-dimensional models¥Increasingly refined grids and increasing levels of tissue discrimination¥Anatomically realistic models¥Special-purpose hardware for visualization¥Distributed computing techniques.B.10 ACCELERATING MATHEMATICAL-BIOLOGICAL LINKAGES:REPORT OF A JOINT NSF-NIH WORKSHOP (Margaret Palmer et al.)
10List of Top Ten Problems at the Mathematical Biology Interface1.Model multilevel systems: from the cells in people, to human communities in physical, chemi-
cal, and biotic ecologies.2.Model networks of complex metabolic pathways, cell signaling, and species interactions.

3.Integrate probabilistic theories: understand uncertainty and risk.

4.Understand computation: gaining insight and proving theorems from numerical computation
and agent-based models.5.Provide tools for data mining and inference.

6.Address linguistic and graph theoretical approaches.

7.Model brain function.

8.Build computational tools for problems with multiple temporal and spatial scales.

9.Provide ecological forecasts.
10.Understand effects of erroneous data on biological understanding.
B.11 GRAND CHALLENGES OF MULTIMODAL BIOMEDICAL SYSTEMS (J. Chen et al.)11Science Challenges1.Allow early detection of where and when an infectious disease outbreak occurs, whether it is
naturally occurring or man-made, in real time.9J.A. Board, Jr., ÒGrand Challenges in Biomedical Computing, High-Performance Computing in Biomedical Research, T.C.Pilkington, B. Loftis, J.F. Thompson, S.L.Y. Woo, T.C. Palmer, and T.F. Budinger, eds., CRC Press, Boca Raton, FL, 1993.10M. Palmer et al., ÒAccelerating Mathematical-Biological Linkages: Report of a Joint NSF-NIH Workshop,Ó February 2003,available at www.maa.org/mtc/NIH-feb03-report.pdf.11J. Chen et al., ÒGrand Challenges of Multimodal Bio-Medical Systems,Ó IEEE Circuits and Systems Magazine, pp. 46-52, 2ndQuarter 2005, available at http://gsp.tamu.edu/Publications/PDFpapers/pap_CASmag_MBM.pdf.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.434CATALYZING INQUIRY2.Develop multidimensional drug profiling databases to facilitate drug discovery and to identify
biomarkers for diagnosis and monitoring the progress of individual disease treatments.3.Connect activities and events derived from cellular processes to high-level cognitions.

4.Support personalized medical care and clinical decision for patients
Technology Challenges and Enabling Technologies1.Formalization of biological knowledge into predictive models for systems biology and system-
based analysis2.Interdisciplinary training

3.Development of open source, multiscale modality informatics toolkits
B.12 THE DEPARTMENT OF ENERGYÕS GENOMES TO LIFE PROGRAM1221st Century Biology Requiring ÒBiocompÓ Tools1.Population models, symbiosis, and stability
2.Discrete growth models

3.Reaction kinetics

4.Biological oscillators and switches

5.Coupled oscillators

6.Reaction-diffusion, chemotaxis, and nonlocality

7.Oscillator-generated wave phenomena and patterns

8.Spatial pattern formation with population interactions

9.Mechanical models for generating pattern and form in development
10.Evolution and morphogenesis
 A Mathematica for Molecular, Cellular, and Systems Biology1.Core data models and structures [database management]
2.Optimized functions [core libraries]

3.Scripting environment [e.g., Python, PERL, ruby, etc.]

4.Database accessors and built-in schemas

5.Simulation interfaces

6.Parallel and accelerated kernels

7.Visualization interfaces (for information visualization and scientific visualization)

8.Collaborative workflow and group use interfaces
Hierarchical Biological Modeling Environment1.Genetic sequences
2.Molecular machines

3.Molecular complexes and modules

4.Networks + pathways [metabolic, signaling, regulation]

5.Structural components [ultrastructures]

6.Cell structure and morphology

7.Extracellular environment

8.Populations and consortia
12R. Stevens, ÒGTL Software Infrastructure: A Computer Science Perspective,Ó undated presentation, Argonne National Labo-ratory, available at www.doegenomics.org/compbio/mtg_1_22_02/RickStevens.ppt.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX B435Modeling and Simulation Challenges for 21st Century Biology1.Modeling activity of single genes
2.Probabilistic models of prokaryotic genes and regulation

3.Logical models of regulatory control in eukaryotic systems

4.Gene regulation networks and genetic network inference in computational models and applica-
tions to large-scale gene expression data5.Atomistic-level simulation of biomolecules

6.Diffusion phenomena in cytoplasm and extracellular environment

7.Kinetic models of excitable membranes and synaptic interactions

8.Stochastic simulation of cell signaling pathways

9.Complex dynamics of cell cycle regulation
10.Model simplification
B.13 HIGH-PERFORMANCE COMPUTING, COMMUNICATION, AND INFORMATIONTECHNOLOGY GRAND CHALLENGES (LATE 1980s, EARLY 1990s)
13Computing Applications to Map and Sequence Human Genome1.Understanding protein folding
2.Predicting structure of native protein

3.Exhaustive discovery and analysis of cancer genes

4.Molecular recognition and dynamics

5.Drug discovery
13Committee on Physical, Mathematical, and Engineering Sciences of the Federal Coordinating Council for Science, Engineer-ing, and Technology, U.S. Office of Science and Technology Policy, FY1992 Blue Book: Grand Challenges: High Performance Com-puting and CommunicationsÑThe FY 1992 U.S. Research and Development Program.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX C437437CBiographies of Committee Members and Staff
C.1COMMITTEE MEMBERS
JOHN C. WOOLEY (Chair) is the associate vice chancellor for research, University of California at SanDiego (UCSD), an adjunct professor in pharmacology, and in chemistry and biochemistry, and a strategic
advisor and senior fellow of the San Diego Supercomputer Center. He received his Ph.D. degree in 1975 at
the University of Chicago, working with Al Crewe and Robert Uretz in biological physics. Dr. Wooley
created the first programs within the U.S. federal government for funding research in bioinformatics and
in computational biology and has been involved in strengthening the interface between computing and
biology for more than a decade. For the new UCSD California Institute for Telecommunication and
Information Technology (Cal-(IT)2), Dr. Wooley directs the biology and biomedical layer or applications
component, termed Digitally-enabled Genomic Medicine (DeGeM), a step in delivering personalized
medicine in a wireless clinical setting. His current research involves bioinformatics and structural
genomics, while his principal objectives at UCSD are to stimulate new research initiatives for large-scale,

multidisciplinary challenges. He also collaborates in developing scientific applications of information
technology and high-performance computing; creating industry-university collaborations; expanding
applied life science opportunities, notably around drug discovery; and establishing a biotechnology and
pharmacology science park on UCSDÕs health sciences campus zone.ADAM P. ARKIN is a faculty scientist in computational and theoretical biology at Lawrence BerkeleyNational Laboratory, an assistant professor of bioengineering and chemistry at the University of Cali-
fornia, Berkeley, and an investigator of the Howard Hughes Medical Institute. His focus is on detailed
modeling of genetic and biochemical networks with emphasis on developmental systems. The Arkin
laboratory applies theoretical and computational analyses from dynamical systems, stochastic pro-
cesses, chemical kinetics, and statistical mechanics and methods from molecular biology to determine
the principles of cellular signal processing and to aid in design of custom cellular circuitry that may, for
example, act as sensitive biosensors.ERIC BRILL is a researcher in the Machine Learning and Applied Statistics Group at Microsoft Re-search. His research interests include natural language processing (primarily empirical natural lan-
guage processing), speech recognition and spoken language systems, machine learning, and artificialCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.438CATALYZING INQUIRYintelligence. Some specific research topics include lexical disambiguation, parsing, classifier combina-tion, spelling correction, and language modeling. Before joining Microsoft, he was an assistant professor
of computer science at Johns Hopkins University. He has served on the editorial board of ComputationalLinguistics and the Journal for Artificial Intelligence Research. Dr. Brill received his Ph.D. in computerscience from the University of Pennsylvania in 1993.ROBERT M. CORN is a professor in the Department of Chemistry at the University of California,Irvine. Dr. Corn is a leader in the development and application of surface-sensitive spectroscopic tech-
niques such as surface plasmon resonance (SPR) imaging, optical second harmonic generation (SHG),
and polarization modulation Fourier transform infrared (PM-FTIR) spectroscopy. His primary research
interests include the study of biopolymer (e.g., DNA, protein) adsorption onto surfaces and the chemi-
cal modification of surfaces for the creation of ultrathin films and adsorption-based biosensors. Profes-
sor Corn also has ongoing research projects in the implementation of DNA computing algorithms at
surfaces and the study of ion transfer processes at liquid-liquid interfaces. He received a B.A. in chem-
istry summa cum laude in 1978 from the University of California, San Diego, and earned a Ph.D. in 1983
from the University of California, Berkeley, under the direction of Professor Herbert L. Strauss in the
application of FTIR to the study of motion in molecular solids. From 1983 to1984, Professor Corn was a
visiting scientist at the IBM Research Laboratory in San Jose, California, where he applied the tech-
niques of surface plasmon-enhanced Raman scattering and optical SHE to electrochemical surfaces. In
1985, Professor Corn moved to Wisconsin where he was a member of the Analytical Sciences Division of
the Department of Chemistry and the Water Chemistry Program until 2004. In July of 2004, he moved to
the University of California, Irvine, where he joined the Department of Chemistry. Professor Corn is a
co-founder of two companies: GWC Technologies, Inc., maker of SPR instrumentation and other surface
spectroscopic equipment, and GenTel BioSurfaces, Inc.CHRIS DIORIO is an associate professor of computer science and engineering at the University ofWashington. His research focuses on building electronic systems that employ the computational and
organizational principles used in the nervous systems of living organisms. This work on neurally
inspired computing includes studies of computing with action potentials, silicon learning systems, and
implantable computers. He also works on high-speed circuit design. Dr. Diorio teaches courses in both
digital electronics and integrated-circuit (IC) design, and is developing new course material in two
areas: (1) alternative computing paradigms, including neural, quantum, and DNA computers, and (2)
digital IC design at microwave clock frequencies. He received a National Science Foundation (NSF)
Presidential Early Career Award in 1999. Dr. Diorio was awarded a 5 year Packard Foundation Fellow-
ship in science and engineering in 1998 and also an NSF Career Award that same year. In 1996, he was
awarded the Electron Devices SocietyÕs (EDSÕs) Paul Rappaport Award for the best paper in an Institute
of Electrical and Electronics Engineers EDS publication. He completed his doctoral research in electrical
engineering at the Physics of Computation Laboratory, California Institute of Technology, in 1997. Dr.
Diorio has also served as a senior staff engineer for TRW, Inc., and as a senior staff scientist for Ameri-
can Systems Corporation. He received his B.A. in physics from Occidental College in 1983 and his M.S.
in electrical engineering in 1984 from The California Institute of Technology.LEAH EDELSTEIN-KESHET is a professor of mathematics at the University of British Columbia. Shereceived her Ph.D. in 1982 from the Weizmann Institute of Science in Rehovot, Israel, specializing in
applied mathematics and working with Professor Lee A. Segel. She is a member of the Mathematics
Department and the Institute of Applied Mathematics at the University of British Colombia. She is also
a former president of the Society for Mathematical Biology. Although her main area of interest is
mathematical biology, Dr. Edelstein-Keshet works in several areas, including the molecular biology of
the cytoskeleton, the dynamics of swarming and social organisms and, more recently, models for
neuroinflammation in AlzheimerÕs disease and pathogenesis of type 1 (autoimmune) diabetes.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX C439MARK H. ELLISMAN is professor in the Department of Neurosciences at the School of Medicine andthe Department of Bioengineering, director of the National Center for Microscopy and Imaging Re-
search at UCSD, and chair of the San Diego Supercomputer Center (SDSC) Executive Committee. Dr.
EllismanÕs research focuses on cellular neurobiology and the dynamic interplay between structure and
function in the nervous system, with a focus on excitable membrane properties and enabling remote
access to large-scale scientific instrumentation. At UCSD, Dr. Ellisman is director of the Center for
Research in Biological Structure and director of the Neurosciences Laboratory for Neurocytology. Since
1997, he has been the neuroscience thrust leader and cross-disciplinary coordinator for the National
Partnership for Advanced Computational Infrastructure. Dr. Ellisman is a member of the American
Association for the Advancement of Science, Society for Neurosciences, and American Institute for
Medical and Biological Engineering. He has served on numerous editorial boards and has been associ-
ate editor of the Journal of Neurocytology since 1980. Dr. Ellisman is a also grant reviewer for organiza-tions such as the National Institutes of Health and the National Science Foundation, and a consultant for
associations such as the Association for Advanced Technology in the Biomedical Sciences and Pfizer.
He has published numerous journal and conference articles and technical reports. He holds a Ph.D.
degree in biology and an M.A. degree in neurophysiology both from the University of Colorado,
Boulder, and an A.B. degree with honors from the University of California, Berkeley.MARCUS W. FELDMAN is a professor of biological sciences at Stanford University. He uses appliedmathematics and computer modeling to simulate and analyze the process of evolution. Specific areas of
research include the evolution of complex genetic systems that can undergo both natural selection and
recombination and the evolution of learning as one interface between modern methods in artificial
intelligence and models of biological processes, including communication. He also studies the evolution
of modern humans using models for the dynamics of molecular polymorphisms, especially DNA vari-
ants. He is managing editor of Theoretical Population Biology and associate editor of Genetics and ofComplexity. Dr. Feldman is a member of the American Society of Naturalists, and the American Societyof Human Genetics, and a fellow of the American Academy of Arts and Sciences. He received his B.Sc.
in 1964 from the University of Western Australia, his M.Sc. in 1966 from Monash University, Australia,
and his Ph.D. in biomathematics from Stanford in 1969.DAVID K. GIFFORD is a professor of electrical engineering and computer science at the MassachusettsInstitute of Technology. He is working on the analysis of RNA expression data using graphical models.
Professor Gifford has also developed programmed mutagenesis, a technique for programmatically
rewriting DNA sequences by incorporating sequence-specific oligonucleotides into newly manufac-
tured strands of DNA. Dr. Gifford serves as group leader for the Programming Systems Research
Group at the MIT Laboratory for Computer Science. This group is dedicated to finding new ways of
programming existing systems and developing new programmable systems. The groupÕs efforts con-
centrate on combining existing technologies and inventing new ones to deliver new ways of computing
in selected areas: programming language development; information discovery, retrieval, and distribu-
tion; algebraic and computational video; and most recently, computation using biological substrates.
Dr. Gifford earned his S.B. in 1976 from MIT and his M.S. and Ph.D. in electrical engineering from
Stanford University in 1978 and 1981, respectively. He is a tenured member of the MIT faculty, which he
joined in 1982. He was appointed to the Karl Van Tassel Career Development Chair at MIT in 1990.TAKEO KANADE received his Ph.D. in electrical engineering from Kyoto University, Japan, in 1974.After being on the faculty in the Department of Information Science, Kyoto University, he joined the
Computer Science Department and Robotics Institute in 1980. He became associate professor in 1982, a
full professor in 1985, the U.A. and Helen Whitaker Professor in 1993, and a University Professor in
1998. He has been the Director of the Robotics Institute since 1992. He served as the founding chairman
(1989-1993) of the robotics Ph.D. program at Carnegie Mellon University, probably the first of its kind inCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.440CATALYZING INQUIRYthe world.  Dr. Kanade has worked in multiple areas of robotics, ranging from manipulator, sensor,
computer vision, and multimedia applications to autonomous robots, with more than 200 papers on
these topics. He is the founding editor of the International Journal of Computer Vision. Dr. KanadeÕsprofessional honors include election to the National Academy of Engineering, a fellow of the IEEE, a
fellow of the ACM, and a fellow of the American Association of Artificial Intelligence, and several
awards including the Joseph Engelberger Award, Yokogawa Prize, JARA Award, Otto Franc Award,
and Marr Prize Award.STEPHEN S. LADERMAN is the manager of the Molecular Diagnostics Department, dedicated tomolecular biology, biochemistry, computational biology, and engineering for the development of ge-
netic, genomic, and proteomic analysis systems for biomedical research and molecular diagnostics. He
earned his B.A. in physics, magna cum laude, from Wesleyan University in 1976 and his Ph.D. in
materials science and engineering from Stanford University in 1983. Dr. Laderman was a postdoctoral
Scholar from 1982 to 1984 at Stanford University and Exxon Research Corporation. Before joining
Agilent Labs, he worked in a variety of positions at Hewlett-Packard Laboratories. Dr. Laderman was a
member of the Basic Energy Sciences Advisory Committee Panel on Novel, Coherent Light Sources and
chair of the selection committee for the George E. Pake Prize of the American Physical Society. He is
currently a member of the International Society for Computational Biology, American Society of Hu-
man Genetics, American Physical Society, American Chemical Society, American Association for the
Advancement of Science, and a senior member of the IEEE.JAMES S. SCHWABER is associate professor of pathology, anatomy and cell biology at ThomasJefferson University Medical College (TJU) and is Director of the Daniel Baugh Institute for Functional
Genomics and Computational Biology at TJU. Prior to joining TJU in 2000, he was technical leader and
research fellow of the Computational Biology Program in the Core Genomics Group at DuPont. His
interest is in neuron and neuronal network modeling (e.g., of cardiorespiratory control functions) and,
in particular, how alterations in neuron properties will be dependent on input activity over time, by
linking the molecular processes activated by synaptic inputs to cell physiology. His research group
focuses on computational analysis of genomic datasets from functionally identified neurons as a corner-
stone to support modeling of the adaptive intracellular response to synaptic inputs. Currently the work
is related to systems analysis of gene regulatory circuits, the modeling of neuronal inputs into these
circuits as modular patterns of transcription factor activation, and the central issue of discovering
principles that relate gene output to functional phenotype (electrophysiology; models of ion fluxes) at
the systems level.C.2STAFF MEMBERS
Herbert S. Lin is senior scientist and senior staff officer at the Computer Science and Telecommunica-tions Board (CSTB), National Research Council (NRC) of the National Academies, where he has been
the study director for major projects on public policy and information technology. These studies include
a 1996 study on national cryptography policy (CryptographyÕs Role in Securing the Information Society), a1991 study on the future of computer science (Computing the Future), a 1999 study of Defense Depart-ment systems for command, control, communications, computing, and intelligence (Realizing the Poten-tial of C4I: Fundamental Challenges), and a 2000 study on workforce issues in high technology (Building aWorkforce for the Information Economy). Prior to his NRC service, he was a professional staff member andstaff scientist for the House Armed Services Committee (1986 to 1990), where his portfolio included
defense policy and arms control issues. He also has significant expertise in math and science education.
He received his Ph.D. in physics from MIT in 1979. Avocationally, he is a long-time folk and swing
dancer, and a poor magician. In addition to his CSTB work, he is published in cognitive science, science
education, biophysics, and arms control and defense policy.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX C441Robin Schoen is the director of the Board on Agriculture and Natural Resources (BANR) of the Na-tional Academies. Prior to joining BANR in March 2005, she was a senior program officer for the
AcademiesÕ Board on Life Sciences, where she directed several studies, including Discovery of AntiviralsAgainst Smallpox; Stem Cells and the Promise of Regenerative Medicine; The National Plant Genome Initiative:Objectives for 2003-2005; Sharing Publication-Related Data and Materials: Responsibilities of Authorship in theLife Sciences; and a BANR study titled Predicting Invasions of Nonindigenous Plants and Plant Pests. Robinreceived a B.S. in biology and chemistry from Frostburg State College, Maryland, and an M.A. in science
and technology policy from George Washington University.C.3REPORT COORDINATOR
Russ Biagio Altman is a professor of genetics, bioengineering and medicine (and of computer scienceby courtesy) at Stanford University. His primary research interests are in the application of computing
technology to basic molecular biological problems of relevance to medicine. He is currently developing
techniques for collaborative scientific computation over the Internet, including novel user interfaces to
biological data, particularly for pharmacogenomics. Other work focuses on the analysis of functional
microenvironments within macromolecules and the application of nonlinear optimization algorithms
for determining the structure and function of biological macromolecules, particularly the bacterial
ribosome. Dr. Altman holds an M.D. from Stanford Medical School, a Ph.D. in medical information
sciences from Stanford, and an A.B. from Harvard College. He has been the recipient of the U.S.
Presidential Early Career Award for Scientists and Engineers, an NSF Career Award, and the Western
Society of Clinical Investigation Annual Young Investigator Award. He is a fellow of the American
College of Physicians and the American College of Medical Informatics. He is a past-president and
founding board member of the International Society for Computational Biology, an organizer of the
annual Pacific Symposium on Biocomputing, and an associate editor of the journal Bioinformatics. Hecurrently directs the Stanford Center for Biomedical Computation and the training program in Biomedi-
cal Informatics, and he won the Stanford Medical School graduate teaching award in 2000.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX D443443DWorkshop ParticipantsTo assist in its information-gathering efforts, the Committee on Frontiers at the Interface of Com-puting and Biology held three workshops on various topics at the interface of computing and biology.
The participants in these workshops are listed below.WORKSHOP ON BIO-INSPIRED COMPUTING ANDENABLING TECHNOLOGIES (JANUARY 2001)Rick Adrion, National Science FoundationRoger Brent, Molecular Sciences Institute
Anne Condon, University of British Columbia
Mita Desai, National Science Foundation
Stephanie Forrest, University of New Mexico
Bob Full, University of California, Berkeley
James J. Hickman, National Science Foundation
Ken Johnson, Gen-Tel, Inc.
Tom Knight, Massachusetts Institute of Technology
Christof Koch, California Institute of Technology
Patricia Mead, National Academy of Engineering
Allen Northrup, Cepheid
Shankar Sastry, Defense Advanced Research Projects Agency
Shihab Shamma, University of Maryland
Sylvia Spengler, National Science Foundation
Gary Strong, National Science Foundation
Erik Winfree, California Institute of TechnologyCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.444CATALYZING INQUIRYWORKSHOP ON CHALLENGES AND OPPORTUNITIESIN DATA MANAGEMENT (MARCH 2001)Helen Berman, Rutgers UniversityPat Brown, Stanford University
Barb Bryant, Millennium Predictive Medicine
Mike Colvin, Lawrence Livermore National Laboratory
Stephen Dahms, California State University Program for Education and Research in Biotechnology
Dan Davison, Bristol Myers Squibb
Joe Deken, Southern Illinois University
Skip Garner, Southwestern Medical Center
Jim Gray, Microsoft
David Haussler, University of California, Santa Cruz
Dick Karp, University of California, Berkeley
David Kingsbury, Discovery Biosciences Corporation
Michael Marron, National Center for Research Resources
Dan Masys, University of California, San Diego
Richard Morris, National Institute of Allergy and Infectious Diseases
Bernhard Palsson, University of California, San Diego
Larry Smarr, University of California, San Diego
Paul Spellman, Stanford University
Sylvia Spengler, National Science Foundation
Gary Strong, National Science Foundation
Art Toga, University of California, Los Angeles
Chris Wood, Los Alamos National LaboratoryWORKSHOP ON MODELING OF BIOLOGICAL SYSTEMS (MAY 2001)Rick Adrion, National Science FoundationRuzena Bajcsy, National Science Foundation
Eugene Bruce, National Science Foundation
Marvin Cassman, National Institutes of Health
Su Chung, geneticXchange
Jim Collins, Boston University
Joe Decken, University of California, San Diego
Mita Desai, National Science Foundation
Drew Endy, Molecular Sciences Institute
Warren Ewens, University of Pennsylvania
Joe Felsenstein, University of Washington
Teresa Head-Gordon, Lawrence Berkeley National Laboratory
James Hickman, National Science Foundation
Sri Kumar, Defense Advanced Research Projects Agency
Simon Levin, Princeton University
Michael Marron, National Center for Research Resources
Andrew McCulloch, University of California, San Diego
Garrett Odell, University of Washington
Dave Polidori, Entelos, Inc.
Terry Sejnowski, Salk Institute
Sylvia J. Spengler, National Science Foundation
Gary Strong, National Science Foundation
John J. Tyson, Virginia Polytechnic Institute and State UniversityCatalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.APPENDIX B445445What Is CSTB?As a part of the National Research Council, the Computer Science and Telecommunications Board(CSTB) was established in 1986 to provide independent advice to the federal government on technicaland public policy issues relating to computing and communications. Composed of leaders from indus-
try and academia, CSTB conducts studies of critical national issues and makes recommendations to
government, industry, and academic researchers. CSTB also provides a neutral meeting ground for
consideration of complex issues where resolution and action may be premature. It convenes invitational
discussions that bring together principals from the public and private sectors, ensuring consideration of
all perspectives. The majority of CSTBÕs work is requested by federal agencies and Congress, consistent
with its National Academies context.A pioneer in framing and analyzing Internet policy issues, CSTB is unique in its comprehensivescope and effective, interdisciplinary appraisal of technical, economic, social, and policy issues. From its
early work in computer and communications security, cyber-assurance and information systems trust-
worthiness have been cross-cutting themes in CSTBÕs work. CSTB has produced several reports re-
garded as classics in the field, and it continues to address these topics as they grow in importance.To do its work, CSTB draws on some of the best minds in the country, inviting experts to participatein its projects as a public service. Studies are conducted by balanced committees without direct financial
interests in the topics they are addressing. Those committees meet, confer electronically, and build
analyses through their deliberations. Additional expertise from around the country is tapped in a
rigorous process of review and critique, further enhancing the quality of CSTB reports. By engaging
groups of principals, CSTB obtains the facts and insights critical to assessing key issues.The mission of CSTB is toRespond to requests from the government, nonprofit organizations, and private industry for adviceon computer and telecommunications issues and from the government for advice on computerand telecommunications systems planning, utilization, and modernization;Monitor and promote the health of the fields of computer science and telecommunications, with atten-tion to issues of human resources, information infrastructure, and societal impacts;Initiate and conduct studies involving computer science, computer technology, and telecommunica-tions as critical resources; andFoster interaction among the disciplines underlying computing and telecommunications technolo-gies and other fields, at large and within the National Academies.More information about CSTB can be obtained online at http://www.cstb.org.Catalyzing Inquiry at the Interface of Computing and BiologyCopyright National Academy of Sciences. All rights reserved.
DETAILSDistribution, posting, or copying of this PDF is strictly prohibited without written permission of the National Academies Press.  (Request Permission) Unless otherwise indicated, all materials in this PDF are copyrighted by the National Academy of Sciences.Copyright © National Academy of Sciences. All rights reserved.THE NATIONAL ACADEMIES PRESSVisit the National Academies Press at NAP.edu and login or register to get:Œ  
Œ  10% off the price of print titles
Œ  Special offers and discountsGET THIS BOOKFIND RELATED TITLESThis PDF is available at SHARECONTRIBUTORS
http://nap.edu/10324Technical, Business, and Legal Dimensions of ProtectingChildren from Pornography on the Internet: Proceedings of aWorkshop144 pages | 6 x 9 | PAPERBACKISBN 978-0-309-08326-3 | DOI 10.17226/10324Committee to Study Tools and Strategies for Protecting Kids from Pornography andTheir Applicability to Other Inappropriate Internet Content, Board on Children,Youth, and Families, National Research CouncilTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.
Committee to Study Tools and Strategies forProtecting Kids from Pornography and Their Applicability to
Other Inappropriate Internet ContentComputer Science and Telecommunications BoardDivision on Engineering and Physical Sciences
National Research CouncilBoard on Children, Youth, and FamiliesDivision of Behavioral and Social Sciences and Education
National Research Council and Institute of MedicineNATIONAL ACADEMY PRESSWashington, D.C.
AND
OF
FROMON THETechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.NATIONAL ACADEMY PRESS ¥ 2101 Constitution Avenue, N.W. ¥ Washington, DC 20418
NOTICE: The project that is the subject of this report was approved by the Gov-erning Board of the National Research Council, whose members are drawn fromthe councils of the National Academy of Sciences, the National Academy of Engi-neering, and the Institute of Medicine.  The members of the committee responsible

for the report were chosen for their special competences and with regard for ap-propriate balance.The study of which this workshop report was a part was supported by Grant No.1999-JN-FX-0071 between the National Academy of Sciences and the U.S. Depart-ments of Justice and Education; Grant No. P0073380 between the National Acad-
emy of Sciences and the W.K. Kellogg Foundation; awards (unnumbered) fromthe Microsoft Corporation and IBM; and National Research Council funds.  Any
opinions, findings, conclusions, or recommendations expressed in this publica-
tion are those of the author(s) and do not necessarily reflect the views of the orga-nizations or agencies that provided support for this project.  Any opinions, find-
ings, conclusions, or recommendations expressed in this material are those of thesymposium presenters and do not necessarily reflect the views of the sponsors.International Standard Book Number 0-309-08326-5Additional copies of this report are available from:National Academy Press2101 Constitution Avenue, N.W.Box 285Washington, DC  20055

800/624-6242202/334-3313 (in the Washington metropolitan area)Copyright 2002 by the National Academy of Sciences.  All rights reserved.
Printed in the United States of AmericaTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.The National Academy of Sciences is a private, nonprofit, self-perpetuating soci-
ety of distinguished scholars engaged in scientific and engineering research, dedi-cated to the furtherance of science and technology and to their use for the general
welfare.  Upon the authority of the charter granted to it by the Congress in 1863,
the Academy has a mandate that requires it to advise the federal government onscientific and technical matters.  Dr. Bruce M. Alberts is president of the National
Academy of Sciences.The National Academy of Engineering was established in 1964, under the charter
of the National Academy of Sciences, as a parallel organization of outstanding
engineers.  It is autonomous in its administration and in the selection of its mem-
bers, sharing with the National Academy of Sciences the responsibility for advis-ing the federal government.  The National Academy of Engineering also sponsors

engineering programs aimed at meeting national needs, encourages education andresearch, and recognizes the superior achievements of engineers.  Dr. Wm. A.
Wulf is president of the National Academy of Engineering.The Institute of Medicine was established in 1970 by the National Academy of
Sciences to secure the services of eminent members of appropriate professions inthe examination of policy matters pertaining to the health of the public.  The Insti-

tute acts under the responsibility given to the National Academy of Sciences by itscongressional charter to be an adviser to the federal government and, upon itsown initiative, to identify issues of medical care, research, and education.
Dr. Kenneth I. Shine is president of the Institute of Medicine.The National Research Council was organized by the National Academy of Sci-
ences in 1916 to associate the broad community of science and technology with
the AcademyÕs purposes of furthering knowledge and advising the federal gov-ernment.  Functioning in accordance with general policies determined by the
Academy, the Council has become the principal operating agency of both the Na-
tional Academy of Sciences and the National Academy of Engineering in provid-ing services to the government, the public, and the scientific and engineering com-munities.  The Council is administered jointly by both Academies and the Institute

of Medicine.  Dr. Bruce M. Alberts and Dr. Wm. A. Wulf are chairman and vice
chairman, respectively, of the National Research Council.National Academy of SciencesNational Academy of Engineering
Institute of Medicine
National Research Council
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.COMMITTEE TO STUDY TOOLS AND STRATEGIES FORPROTECTING KIDS FROM PORNOGRAPHY ANDTHEIR APPLICABILITY TO OTHER INAPPROPRIATEINTERNET CONTENTRICHARD THORNBURGH, Kirkpatrick & Lockhart LLP, ChairNICHOLAS J. BELKIN, Rutgers University
WILLIAM J. BYRON, Holy Trinity Parish
SANDRA L. CALVERT, Georgetown University
DAVID FORSYTH, University of California at Berkeley
DANIEL GEER, @Stake
LINDA HODGE, Parent Teacher Association
MARILYN GELL MASON, Independent Consultant
MILO MEDIN, Excite@Home
JOHN B. RABUN, National Center for Missing and Exploited Children
ROBIN RASKIN, FamilyPC Magazine
ROBERT SCHLOSS, IBM T.J. Watson Research Center
JANET WARD SCHOFIELD, University of Pittsburgh
GEOFFREY R. STONE, University of Chicago
WINIFRED B. WECHSLER, Independent ConsultantStaffHERBERT S. LIN, Senior Scientist and Study DirectorGAIL PRITCHARD, Program Officer (through June 2001)
LAURA OST, Consultant
JOAH G. IANOTTA, Research Assistant
JANICE SABUDA, Senior Project Assistant
DANIEL D. LLATA, Senior Project Assistant (through May 2001)ivTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.COMPUTER SCIENCE AND TELECOMMUNICATIONS BOARDDAVID D. CLARK, Massachusetts Institute of Technology, ChairDAVID BORTH, Motorola Labs
JAMES CHIDDIX, AOL Time Warner
JOHN M. CIOFFI, Stanford University
ELAINE COHEN, University of Utah
W. BRUCE CROFT, University of Massachusetts at Amherst
THOMAS E. DARCIE, AT&T Labs Research
JOSEPH FARRELL, University of California at Berkeley
JEFFREY M. JAFFE, Bell Laboratories, Lucent Technologies
ANNA KARLIN, University of Washington
BUTLER W. LAMPSON, Microsoft Corporation
EDWARD D. LAZOWSKA, University of Washington
DAVID LIDDLE, U.S. Venture Partners
TOM M. MITCHELL, Carnegie Mellon University
DONALD NORMAN, Nielsen Norman Group
DAVID A. PATTERSON, University of California at Berkeley
HENRY (HANK) PERRITT, Illinois Institute of Technology
BURTON SMITH, Cray Inc.
TERRY SMITH, University of California at Santa Barbara
LEE SPROULL, New York University
JEANNETTE M. WING, Carnegie Mellon UniversityStaffMARJORY S. BLUMENTHAL, DirectorHERBERT S. LIN, Senior Scientist
ALAN S. INOUYE, Senior Program Officer
JON EISENBERG, Senior Program Officer
LYNETTE I. MILLETT, Program Officer
CYNTHIA PATTERSON, Program Officer
STEVEN WOO, Program Officer
JANET BRISCOE, Administrative Officer
DAVID PADGHAM, Research Associate
MARGARET HUYNH, Senior Project Assistant
DAVID DRAKE, Senior Project Assistant
JANICE SABUDA, Senior Project Assistant
JENNIFER BISHOP, Senior Project Assistant
BRANDYE WILLIAMS, Staff AssistantvTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BOARD ON CHILDREN, YOUTH, AND FAMILIESEVAN CHARNEY, University of Massachusetts Medical School, ChairJAMES A. BANKS, University of Washington
DONALD COHEN, Yale University
THOMAS DEWITT, ChildrenÕs Hospital Medical Center of CincinnatiMARY JANE ENGLAND, Washington Business Group on Health
MINDY FULLILOVE, Columbia University
PATRICIA GREENFIELD, University of California at Los Angeles
RUTH T. GROSS, Stanford University
KEVIN GRUMBACH, University of California at San Francisco, SanFrancisco General HospitalNEAL HALFON, University of California at Los Angeles School ofPublic HealthMAXINE HAYES, Washington State Department of Health
MARGARET HEAGARTY, Columbia University
REN…E R. JENKINS, Howard UniversityHARRIET KITZMAN, University of Rochester
SANDERS KORENMAN, Baruch College, City University of New York
HON. CINDY LEDERMAN, Juvenile Justice Center, Dade County,FloridaVONNIE McLOYD, University of Michigan
GARY SANDEFUR, University of Wisconsin-Madison
ELIZABETH SPELKE, Massachusetts Institute of Technology
RUTH STEIN, Montefiore Medical CenterLiaisonsELEANOR E. MACCOBY (Liaison, Division of Behavioral and SocialSciences and Education), Department of Psychology (emeritus),
Stanford UniversityWILLIAM ROPER (Liaison, IOM Council), Institute of Medicine,University of North Carolina, Chapel HillStaffMICHELE D. KIPKE, Director (through September 2001)MARY GRAHAM, Associate Director, Dissemination andCommunicationsSONJA WOLFE, Administrative Associate
ELENA NIGHTINGALE, Scholar-in-Residence
JOAH G. IANNOTTA, Research AssistantviTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.Preface
In response to a mandate from Congress in conjunction with the Pro-tection of Children from Sexual Predators Act of 1998, the Computer Sci-
ence and Telecommunications Board (CSTB) and the Board on Children,
Youth, and Families of the National Research Council (NRC) and the In-
stitute of Medicine established the Committee to Study Tools and Strate-
gies for Protecting Kids from Pornography and Their Applicability to
Other Inappropriate Internet Content.To collect input and to disseminate useful information to the nationon this question, the committee held two public workshops.  On Decem-

ber 13, 2000, in Washington, D.C., the committee convened a workshop to
focus on nontechnical strategies that could be effective in a broad range of
settings (e.g., home, school, libraries) in which young people might be
online. This workshop brought together researchers, educators, policy
makers, and other key stakeholders to consider and discuss these ap-
proaches and to identify some of the benefits and limitations of various
nontechnical strategies.  The December workshop is summarized in Non-technical Strategies to Reduce ChildrenÕs Exposure to Inappropriate Material on
the Internet: Summary of a Workshop.11National Research Council and Institute of Medicine, Nontechnical Strategies to ReduceChildrenÕs Exposure to Inappropriate Material on the Internet: Summary of a Workshop, ComputerScience and Telecommunications Board and Board on Children, Youth, and Families, JoahG. Iannotta, ed., Washington, D.C.:  National Academy Press, 2001.
viiTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.viiiPREFACEThe second workshop was held on March 7, 2001, in Redwood City,California.  This second workshop focused on some of the technical, busi-

ness, and legal factors that affect how one might choose to protect kids
from pornography on the Internet.  The present report provides, in the

form of edited transcripts, the presentations at that workshop.  Obviously,

because the report reflects the presentations on that day, it is not intended
to be a comprehensive review of all of the technical, business, and legal
issues that might be relevant to this subject.  All views expressed in this

report are those of the speaker (who sometimes is a member of the study
committee speaking for himself or herself).  Most importantly, this report

should not be construed as representing the views of the Committee to
Study Tools and Strategies for Protecting Kids from Pornography and
Their Applicability to Other Inappropriate Internet Content; the Com-
puter Science and Telecommunications Board; the Board on Children,
Youth, and Families; the National Research Council; or the Institute of
Medicine.The report contains 17 chapters, each of which is essentially an editedtranscript of the various briefings to the committee during the workshop.
Questions and comments from the audience and committee members are
included as footnotes.  The first four chapters are devoted to the basics of
information retrieval and searching.  The next three (Chapters 5-7) ad-

dress some of the technology and business dimensions of filtering, the
process through which certain types of putatively objectionable content
are blocked from display on a userÕs screen.  Two chapters (Chapters 8-9)
then address technical and infrastructural dimensions of authenticationÑthe process of proving that one is who one asserts to be.  The next three

chapters (Chapters 10-12) address automated approaches to negotiating
individualized policy preferences and dealing with issues of intellectual
property (and preventing unauthorized parties from viewing protected
material).  Chapter 13 addresses the problems associated with a dot-xxx

domain for Òcordoning offÓ sexually explicit material on the Internet.
Chapters 14-16 cover various issues associated with business models for
the Internet, and the final chapter, Chapter 17, discusses one legal scholarÕsperspective on regulating sexually explicit material on the Internet.Gail Pritchard was largely responsible for assembling the speakers atthis workshop, and Laura Ost generated the first draft of the report.This report was reviewed in draft form by individuals chosen for their
diverse perspectives and technical expertise, in accordance with proce-
dures approved by the NRCÕs Report Review Committee. The purpose ofthis independent review is to provide candid and critical comments that
will assist the institution in making the published report as sound as pos-
sible and to ensure that the report meets institutional standards for objec-
tivity, evidence, and responsiveness to the study charge. The review com-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.PREFACEixments and draft manuscript remain confidential to protect the integrity ofthe deliberative process.We thank the following individuals for their participation in the re-view of these workshop proceedings:William Aspray, Computing Research Association,Hinrich Schtze, Novation Biosciences, andFrederick Weingarten, American Library Association.Although these individuals reviewed the report, they were not askedto endorse it, nor did they see the final draft of the report before its re-
lease. The review of this report was overseen by Peter Blair of the Division
on Engineering and Physical Sciences.  Appointed by the National Re-

search Council, he was responsible for making certain that an indepen-
dent examination of this report was carried out in accordance with insti-
tutional procedures and that all review comments were carefully
considered. Responsibility for the final content of this report rests entirely
with the authoring committee and the institution.Herbert S.  Lin, Senior Scientist and Study Director
Computer Science and Telecommunications BoardTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.Contents1BASIC CONCEPTS IN INFORMATION RETRIEVAL1
Nicholas Belkin1.1Definitions and System Design, 1

1.2Problems, 2
2TEXT CATEGORIZATION AND ANALYSIS5
David Lewis and Hinrich Schtze2.1Text Categorization, 5

2.2Advanced Text Technology, 7
3CATEGORIZATION OF IMAGES11
David Forsyth3.1Challenges in Object Recognition, 11

3.2Screening of Pornographic Images, 12

3.3The Future, 14
4THE TECHNOLOGY OF SEARCH ENGINES16
Ray Larson4.1Overview, 16

4.2Boolean Search Logic, 17

4.3The Vector Space Model, 18

4.4Searching the World Wide Web, 19
xiTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.xiiCONTENTS5CYBER PATROL: A MAJOR FILTERING PRODUCT23
Susan Getgood5.1Introduction, 23

5.2Why Filter?, 24

5.3SuperScout and Cyber Patrol, 25

5.4The Review Process, 29

5.5The Future, 31
6ADVANCED TECHNIQUES FOR AUTOMATIC
WEB FILTERING33
Michel Bilello6.1Background, 33

6.2The WIPE System, 34
7A CRITIQUE OF FILTERING36
Bennett Haselton7.1Introduction, 36

7.2Deficiencies in Filtering Programs, 37

7.3Experiments by Peacefire.org, 38

7.4Circumvention of Blocking Software, 45
8AUTHENTICATION TECHNOLOGIES48
Eddie Zeitler8.1The Process of Identification, 48

8.2Challenges and Solutions, 50
9INFRASTRUCTURE FOR AGE VERIFICATION53
Fred Cotton9.1The Real World Versus the Internet, 53

9.2Solutions, 56

9.3The Extent of the Problem, 59
10AUTOMATED POLICY PREFERENCE NEGOTIATION62
Deirdre Mulligan11DIGITAL RIGHTS MANAGEMENT TECHNOLOGY65
John Blumenthal11.1Technology and Policy Constraints, 65

11.2Designing a Solution to Fit the Constraints, 67

11.3Protecting Children, 73

11.4Summary, 75
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.CONTENTSxiii12A TRUSTED THIRD PARTY IN DIGITAL RIGHTS
MANAGEMENT76
David Maher12.1InterTrust Technologies, 77

12.2Countermeasures and Hackers, 80

12.3Summary, 84
13PROBLEMS WITH A DOT-XXX DOMAIN85
Donald Eastlake14BUSINESS DIMENSIONS: THE EDUCATION MARKET90
Irv Shapiro14.1The Role of Teachers, 90

14.2Historical Perspective, 91

14.3The School Marketplace, 92
15BUSINESS MODELS: KID-FRIENDLY INTERNET
BUSINESSES96
Brian Pass15.1Building an Internet Business, 96

15.2Comparing Business Models, 98

15.3The Role of Parents, 103
16BUSINESS MODELS BASED ON ADVERTISING104
Chris Kelly16.1Comparison of Advertising Models, 104

16.2Portals, Advertising Networks, and Targeting, 105

16.3Choice of Models, 106

16.4Advertising, Regulation, and Kids, 107
17CONSTITUTIONAL LAW AND THE LAW OF
CYBERSPACE110
Larry Lessig17.1Introduction, 110

17.2Regulation in Cyberspace, 111

17.3Possible Solutions, 112

17.4Practical Considerations, 118
APPENDIX:  BIOGRAPHIES OF PRESENTERS124
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.11Basic Concepts in Information RetrievalNicholas Belkin1.1DEFINITIONS AND SYSTEM DESIGN
Information retrieval and information filtering are different functions.Information retrieval is intended to support people who are actively seek-
ing or searching for information, as in Internet searching.  Information

retrieval typically assumes a static or relatively static database against
which people search.  Search engine companies construct these databases

by sending out ÒspidersÓ and then indexing the Web pages they find.  By

contrast, information filtering supports people in the passive monitoring
for desired information.  It is typically understood to be concerned with

an active incoming stream of information objects.The problem in information retrieval and information filtering is thatdecisions must be made for every document or information object regard-
ing whether or not to show it to the person who is retrieving the informa-
tion.  Initially, a profile describing the userÕs information needs is set up

to facilitate such decision making; this profile may be modified over the
long term through the use of user models.  These models are based on a

personÕs behaviorÑdecisions, reading behaviors, and so on, which may
change the original profile.  Both information retrieval and information

filtering attempt to maximize the good material that a person sees (that
which is likely to be appropriate to the information problem at hand) and
minimize the bad material.When people refer to filtering, they often really mean information re-trieval.  That is, they are not concerned with dynamic streams of docu-

ments but rather with databases that are already constructed and in whichTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.2BASIC CONCEPTS IN INFORMATION RETRIEVALthere is some way to represent the information objects and relate them toone another. Thus, filtering corresponds to the Boolean filter in informa-
tion retrieval: a yes/no decision.Most search engines designed for the World Wide Web use the prin-ciple of Òbest match,Ó that is, not making yes/no decisions but, rather,
ranking information objects with respect to some representation of the
information problem.  Thus, the basic processes in information retrieval

or information filtering are the representations of information objects and
of information needs, or more generally, the problem or goal that the per-
son has in mind.  The retrieval techniques themselves then compare needs

with objects.The interaction of the user with other components of the system isimportant.  In fact, the prevailing view in information retrieval research is

that the most effective approach for helping a user obtain the appropriate
information is relevance feedback, in which the system takes into account
whether a person likes or dislikes a document as it automatically re-repre-
sents the userÕs query.  This leads to performance improvements of as
much as 150 percentÑmuch better than any other technique.  Thus, the
personÕs judgment of the information objects is an important part of theprocess.  The user is an actor in the information retrieval system, because
many of the processes depend on his or her expression and interpretation
of the need.  The relevance of a document cannot be determined unless

the person is considered a part of the system.The second important part of the system is the information resource,a collection of information objects that has been selected, organized, and
represented according to some schema.  The third component is the inter-

mediaryÑa device or person that mediates between the information re-source and the user and that has knowledge of the user, the userÕs prob-lem, and the types of users that exist, as well as the information resource,
the way the resource is organized, what it contains, and so on.  The inter-
mediary supports the interaction between people and the information
objects and knowledge resource, through prediction and other means.1.2PROBLEMS
The representation of information problems is inherently uncertain,because people look for that which they do not know, and it is probably
inappropriate to ask them to specify what they do not know.  The repre-

sentation of information objects requires interpretations by a human in-
dexer, machine algorithm, or other entity.  The problem is that anyone
Õsinterpretation of a particular text is likely to be different from anyone
elseÕs, and even different for the same person at different times.  As our
state of knowledge or problems change, our understanding of a textTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.NICHOLAS BELKIN3changes.  Everyone has experienced the situation of finding a document
not relevant at some point but highly relevant later on, perhaps for a dif-
ferent problem or perhaps because we, ourselves, are different.  The easi-
est and most effective way to deal with this problem is to support usersÕinteractions with information objects and let them take control.Because of these uncertainties, the comparison of needs and informa-tion objects, or retrieval process, is also inherently uncertain and probabi-
listic.  The understanding of information objects is subjective, and, there-

fore, representation is necessarily inconsistent.  We do not know how well

we are representing either the personÕs need or the information object.An extensive literature on interindexer consistency shows that when
people are asked to represent an information object, even if they are highly
trained in using the same meta-language (indexing language), they might
achieve as much as only 60 to 70 percent consistency in tasks such as as-
signing descriptors.  We will never achieve 
ÒidealÓ information retrieval
Ñthat is, all the relevant documents and only the relevant documents, or
precisely that one thing that a person wants.The implication is that we must think of probabilistic ways of repre-senting information problems.  Even if computers were as smart as people,

they probably could not do the job.  A standard information retrieval re-

sult is that automatic indexingÑin which algorithms do statistical wordcounting and indexingÑleads to performance that is no worse, and oftenbetter, than systems in which people do manual indexing.There is no reason to suppose that people will do a better job thanmachines, and neither one will do a perfect job, ever.  Making absolute

predictions in an inherently probabilistic environment is not a good idea.Algorithms for representing information objects, or information prob-lems, do give consistent representations.  But they give one interpretation

of the text, out of a great variety of possible representations, depending
on the interpreter.  Language is ambiguous in many ways:  polysemy,

synonymity, and so on.  For example, a bank can be either a financial

institution or something on the side of a river (polysemy).  The context

matters a lot in the interpretation.The meta-language used to describe information objects, or linguisticobjects, often is construed to be exactly the same as the textual language
itself.  But they are not the same. The similarity of the two languages has

led to some confusion.  In information retrieval, it has led to the idea that
the words in the text represent the important concepts and, therefore, can
be used to represent what the text is about.  The confusion extends to

image retrieval, because images can be ambiguous in at least as many
ways as can language.  Furthermore, there is no universal meta-language

for describing images.  People who are interested in images for advertis-
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.4BASIC CONCEPTS IN INFORMATION RETRIEVALing purposes have different ways to talk and think about them than do arthistorians, even though they may be searching for the same images.  The

lack of a common meta-language for images means that we need to think
of special terms for images in special circumstances.In attempting to prevent children from getting harmful material, it ispossible to make approximations and give helpful direction.  But in the

end, that is the most that we can hope for.  It is not a question of prevent-
ing someone from getting inappropriate material but, rather, of support-
ing the person in not getting it.  At least part of the public policy concern

is kids who are actively trying to get pornography, and it is unreasonable
to suppose that information retrieval techniques will be useful in achiev-
ing the goal of preventing them from doing so.There are a variety of users.  The user might be a concerned parent or
manager who suspects that something bad is going on.  But mistakes are

inevitable, and we need to figure out some way to deal with that.  It is

difficult to tell what anything means, and usually we get it wrong.  Gener-

ally we want to design the tools so that getting it wrong is not as much of
a nuisance as it otherwise might be.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.52Text Categorization and Analysis
2.1TEXT CATEGORIZATION
Automatic text categorization is the primary language retrieval tech-nology in content filtering for children.  Text categorization is the sorting
of text into groups, such as pornography, hate speech, violence, and un-
objectionable content.  A text categorizer looks at a Web page and decides

into which of these groups a piece of text should fall.  Applications of text

categorization include filtering of e-mail, chat, or Web access; text index-
ing; and data mining.Why is content filtering a categorization task?  One way to frame the
problem is to say that the categories are actions, such as Òallow,Ó Òallow
but warn,Ó or Òblock.Ó  We either want to allow access to a Web page,

allow access but also give a warning, or block access.  Another way to

frame the problem is to say that the categories are different types of con-
tent, such as news, sex education, pornography, or home pages.  Depend-

ing on which category we put the page in, we will take different actions.
For example, we want to block pornography and give access to news.The automation of text categorization requires some input frompeople.  The idea is to mimic what people do.  Two parts of the task need
to be automated.  One is the categorization decision itself.  The categoriza-

tion decision says, for example, what we should do with a Web page.  The

second part to be automated is rule creation.  We want to determine auto-

matically the rules to apply.Automation of the categorization decision requires a piece of soft-ware that applies rules to text.  This is the best architecture because then
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.6TEXT CATEGORIZATION AND ANALYSISwe can change the behavior by changing the rules rather than rewritingthe software every time.  This automatic categorizer applies two types of

rules.  One type is extensional rules that explicitly list all sites that cannot

be accessed (i.e., ÒblacklistedÓ sites) or, alternatively, all sites that can be
accessed (e.g., kid-safe zones or ÒwhitelistedÓ sites).  The second type,
which is technically more complicated, is intentional rules or keyword
blocking.  We look at the content of the page, and, if certain words occur,
then we take certain actions, such as blocking access to that page.  It can

be more complicated than just a single word.  For example, it can be logic

based, where we use AND and OR operators, or it can be a weighted
combination of different types of words.Automated rule writing is called supervised learning.  One or more
persons are needed to provide samples of the types of decisions we wish
to make.  For example, we could ask a librarian to identify which of 500

texts or Web pages are pornography and which ones are not.  This pro-

vides a training set of 500 sample decisions to be mimicked.  The rule-

writing software attempts to produce rules that mimic those categoriza-
tion decisions.  The goal is to mimic the categorization decisions made by

people.  The selection of the persons who provide the samples is funda-

mental, because whatever they do becomes the gold standard, which the
machine tries to mimic.  Everything depends on the particular persons

and their judgments.Research shows that supervised learning is at least as good as experthuman rule writing.  (Supervised learning is also very flexible.  For ex-

ample, foreign content is not a problem, as long as the content involves
text rather than images.)  The effectiveness of these methods is far from

perfectÑthere is always some error rateÑbut sometimes it is near agree-ment with human performance levels.  Still, the results differ from cat-

egory to category, and it is not clear how directly it applies to, for ex-
ample, pornography.  As discussed in the next presentation, there is an

inevitable trade-off between false positives and false negatives, and cat-
egories vary widely in difficulty.  Substantially improved methods are

not expected in the next 10 to 20 years.It is not clear which text categorization techniques are most effective.Some recently developed techniques are not yet used commercially, so
there may be incremental improvements.  Nor is it clear how effective

semiautomated categorization is, or whether the categories that are diffi-
cult for automated methods are the same as those that perplex people.
With regard to spam e-mail, it is possible to circumvent it, but there is no
foolproof way to filter it.  The question is whether the error rate is accept-

able.This all comes back to community standards.  We can train the classi-
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DAVID LEWIS AND HINRICH SCHƒTZE7fier to predict the probability that a person would find an item inappro-priate, and training can give equal weight to any number of community
volunteers.  In other words, we can build a machine that mimics a com-

munity standard.  We take some people out of the community, get their

judgments about what they find objectionable or not, and then build a
machine that creates rules that mimic that behavior.  But this does not

solve the political question of how to define the community, who to select
as representatives of that community, and where in that community to
apply the filter.  The technological capability does not solve the applica-

tion issues in practice.2.2ADVANCED TEXT TECHNOLOGY
True text understanding will not happen for at least 20 or 30 years,and maybe never.  Therein lies the problem, because to filter content with

absolute accuracy we would need text understanding.  As a result, there

will always be an error rate; the question is how high it is.The text categorization methods discussed above use the Òbag-of-wordsÓ model.  This is a simplistic machine representation of text.  It takes
all the words on a page and treats them as an unstructured list.  If the text

is ÒDick Armey chooses Bob Shaffer to lead committee,Ó then a represen-tative list would be: Armey, Bob, chooses, committee, Dick, lead, Shaffer.
The structure and context of the text is completely lost.  This impover-

ished representation is the basis of text classification methods in existing
content filters.There are problems with this type of representation.  It fails, in manycases, because of ambiguous words.  The context is important.  Ambigu-

ous words such as ÒbeaverÓ have both a hunter
Õs meaning and a graphicmeaning.  Using the bag-of-words model alone, you cannot tell which

meaning is relevant.  The bag-of-words model is inherently problematic

for these types of ambiguous words.  Other words, such as 
ÒbreastÓ and
Òblow,Ó are not ambiguous but can be used pornographically.  Again, if
we use a bag-of-words model, then we lose context and cannot deal with
these words properly.  When context counts, the bag-of-words model fails.
The problem cannot be resolved fully by looking for adjacent words,as search engines do when they give higher weight to information objects
that match the query and have certain words in the same sentence.  There

is a distinction between search engines and classification.  Search engines

compute a ranking of pages.  The end users look at the top 10 or maybe

the top 100 ranked pages.  Because they are looking only at pages in which

the signal is strongest and because they are making a relative judgment,
this type of methodology works very well; the highest-rated pages areTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.8TEXT CATEGORIZATION AND ANALYSISprobably very relevant to the query.1 But in classification, we have to
make a decision about one page by itself.  This is a much more difficult

problem.  By looking at the words that lie nearby, we cannot always make

a decent statistical guess as to whether a situation is innocuous or not.When context is important, when the bag-of-words model fails, por-nography filters and content filters make errors.  However
Ñsurpris-inglyÑthe bag-of-words model is effective in many applications, so it isnot a hopeless basis for pornography filters despite its error rate.  It al-

ways comes down to what error rate is acceptable.2  To go beyond the
bag-of-words model, a number of technologies are currently available:
morphological analysis, part-of-speech tagging, translation, disambigua-
tion, genre analysis, information extraction, syntactic analysis, and pars-
ing.  Even using these technologies, thorough text understanding will re-
main in the distant future; a 100-percent-accurate categorization decision
cannot be made today.  But these advanced text technologies can increase

the accuracy of content filters, and this increased accuracy may be signifi-
cant in some areas.The first area relates to over-broad filters that block material thatshould not be blocked, raising free speech issues.  It is relatively easy to

build an over-broad filter, which blocks pornography very well but also
blocks a lot of good content, like Dick ArmeyÕs home page.  These over-
broad filters may suffice in many circumstances.  For example, there may

be parents who would say, ÒAs long as not a single pornographic pagecomes through, or it almost never happens, it is OK if my child cannot see
a lot of good content.Ó  But these over-broad filters are problematic in
many other settings, such as in libraries, where there is an issue of free
speech.  If a lot of good content is blocked, then that is problematic.  Ad-

vanced technology can really make a difference, because by increasing
the accuracy of the filter, less good content would be blocked.1Milo Medin said that various search engine companies have come with a number of tech-niques to filter adult content, so that you have to turn on the capability to see certain typesof references.  Most of it is ranking based, but there are some other obvious things as well.

Part of the challenge is that many adult sites are trying to get people to visit, so they fill theirheaders with all kinds of information that make it obvious what is going on.  The question is,
how practical is that?2Milo Medin said that the people who run search engines have an economic interest inmaking their results as accurate as possible, to satisfy their subscribers.  Normal large search
engines want the adult-content filter to be as accurate as possible.  If the filter is turned on,

we basically want to eliminate adult content.  The Google folks, as an example, have devoted
a lot of energy to these issues, but it is not aimed directly at pornography.  They focus on a
broader set of issues to which pornography is a business input.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DAVID LEWIS AND HINRICH SCHƒTZE9The second area is pornography versus other objectionable content,such as violence and hate speech.  The bag-of-words model is most suc-

cessful under two conditions: (1) when there are unambiguous words in-
dicating relevant content and (2) when there are a few of these indicators.
Pornography has these properties; probably about 40 or 50 words, most of
them unambiguous, indicate pornography.  Thus, the bag-of-words model

is actually not so bad for this application, especially if you like over-broad
filters.  However, in many other areas, such as violence and hate speech,

the bag-of-words model is less effective.  Often you must read four or five

sentences of a text before identifying it as hate speech.  Accuracy becomes

important in such applications, and advanced technology can be helpful
here.The third area is automated blacklisting.  Remember the distinction
between extensional and intentional rules; extensional rules are lists of
sites that you want to block.  This is an effective content-filtering tech-

nique, mostly driven by human editors now.  This is a promising area for

automation.  Accuracy is important because blocking one site can block

thousands of pages; you want to be sure of doing the right thing.  Ad-

vanced text technology also can play a role here.A potential problem with these text technologies is their lack of ro-bustness.  They can be circumvented through changes in meaning.  If a

pornographer wants to get through a filter that he knows and can test,
then he or she will be able to get through itÑit is simply a question ofeffort.  But pornographers are not economically motivated to expend a lot

of effort to get through these filters.  I may be wrong, but my sense is that,

because children do not pay for pornography, this is probably not a prob-
lem.In summary, true machine-aided text understanding will not be avail-able in the near term, and that means there always will be a significant
error rate with any automated method.  The advanced text technologies

improve accuracy, which may be important in contexts such as free speech
in libraries, identification of violence and hate speech, and automated
blacklisting.The extent of the improvement from these technologies depends onmany parameters, and tests must be run.3  The latest numbers I know of
are from Consumer Reports,4 but they are aggregated and not broken down
3Milo Medin said that it is difficult to do good experiments and that sloppy experimenta-tion is rewarded in a strange way.  First, you run a very large collection of text through your
filter and determine how much of the material identified as pornographic was, in fact, not.
Second, you find out how much of the material identified as not pornographic was, in fact,a problem.  If you do that analysis badly or carelessly, your filter looks better.
4Consumer Reports, March 2001.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.10TEXT CATEGORIZATION AND ANALYSISby area.  There is probably a big difference in accuracy between pornogra-
phy and the other objectionable areas.  There is also a trade-off between

false positives and false negatives.  The extent to which advanced tech-

niques make a difference depends on where in the trade-off you start out.
If I had to give a number, I would expect a 20 to 30 percent improvement
in accuracy over the bag-of-words modelÑif you want to let all good con-tent through (if you do not want over-blocking).Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.113Categorization of ImagesDavid Forsyth3.1CHALLENGES IN OBJECT RECOGNITION
The process of determining whether a picture is pornographic in-
volves object recognition, which is difficult for a lot of reasons.  First, it is
difficult to know what an object is; things look different from different
angles and in different lights.  When color and texture change, things look

different.  People can change their appearance by moving their heads

around.  We do not look different to one another when we do this, but we

certainly look different in pictures.The state of the art in object recognition is finding buildings in pic-tures taken from satellites.  Computer programs sometimes can find

people.  We are good at finding faces.  We can tellÑsort ofÑwhether a

picture has nearly naked people in it.  But there is no program that reli-

ably determines whether there are people wearing clothing in a picture.
The main way to look for people with clothes is to look for the ones with-
out clothes.  It is a remarkable fact of nature that virtually everyoneÕs skin

looks about the same in a picture (even across different racial groups), as
long as we are careful about intensity issues.  Skin is easy to detect reli-

ably in pictures, so the first thing we look for is skin.  But we need to

realize that photographs of the California desert, apple pies, and all sorts
of other things also have a blank color.  Therefore, we need a pattern for

how skin is arranged.Long, thin bits of skin might be an arm, leg, or torso.  Because the
kinematics of the body is limited, certain things cannot be done with arms
and legs.  If I find an arm, for example, then I know where to look for a
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.12CATEGORIZATION OF IMAGESleg.  If I put enough of them together, then there is a person in the picture.
If there is a person and there is skin, then they have no clothes on, and
there is a problem.  We could reason about the arrangement of skin, or we

could simply say that any big blob of skin must be a naked person.  We

did a classification based on kinematics.Performance assessment is complicated.  There are two things to con-
sider: first, the probability that the program will say a picture is rude when

it is not (i.e., false positive) and, second, the probability that the program
will say a picture is not rude when it is (i.e., false negative).  Although it is

desirable to try to make both numbers as small as possible, the appropri-
ate trade-off between false positives and false negatives depends on the
application, as described below.  Moreover, false positive and false nega-
tive rates can be measured in different ways.  Doing the experiments can

be embarrassing because a lot of pictures need to be handled and viewed,
and all sorts of other things make it tricky as well.  The experiments are

difficult to assess because they all use different sets of data.  People usu-

ally report the experiments that display their work in a good light.  In

view of these phenomena, it is not easy to say what would happen if we
dropped one of these programs on the Web.3.2SCREENING OF PORNOGRAPHIC IMAGES
One way to reduce viewing of pornographic images is intimidation.A manager or parent might say to employees or children that Internet
traffic will be monitored.  They might explain that the image categoriza-

tion program will store every image it is worried about in a folder and,
once a week, the folder will be opened and the contents displayed.  If the
images are problematic, the manager or parent will have a conversation
with the employee or child.  This approach might work, because when

people are warned about monitoring, they may not behave in a silly way.But it will work only if there is a low probability of false positives.  No
one will pay attention to monitoring if each week 1,500 ÒpornographicÓpictures are discovered in the folder, all being pictures of apple pies that
the program has misinterpreted.  The security industry usually says that

people faced with many false positives get bored and do not want to deal
with the problem.1  On the other hand, a high rate of false negatives is not
a concern in this context.  Typically, in a monitoring application, letting
1Milo Medin noted that the Internal Revenue Service (IRS) uses the intimidation approach.In the tax context, many false positives may not be a problem.  Certain behaviors cause the

IRS to expend a lot of energy to respond.  If the consequences of an investigation are high
enough, then the IRS needs to do it only a few times to generate certain behaviors.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DAVID FORSYTH13one or two pictures sneak in is not a problem.  If there is a high false-
negative rate, then we will get a warning.  We might not see every one,

but we will know there is an issue.Another approach is to render every picture coming through a net-work. We could fill a building with banks of people looking at all the
pictures and saying, ÒI donÕt like this one.Ó  This is not practical.  We
could take a Òno porn shall passÓ attitude, but then we really care whether
the possibility of a false negative is small, and there is a risk that we might
not know what is being left out.  Large chunks of information might be

ruled as objectionable by the program without, in fact, being objection-
able, and we would not know about it.Yet another approach is site classification.  We could look at a series
of pictures from one site, and if our program thinks that enough of them
are rude, then we could say that the whole site is rude.  We need to be

careful about such rules, however, because of a conditional probability
issue, as discussed below.A program that I wrote with Ida Fleck marks about 40 percent of por-nographic pictures, where a pornographic picture is an image that can be
downloaded from an adult-oriented site. This program thinks pictures
are pornographic if they contain lots of stuff that looks like skin that is in
long bits and in a certain arrangement.  A picture that appears to have lots

of skin but in the wrong arrangement is not judged to be pornographic.
Pictures with little skin showing are not identified as pornographic.  But

pictures of things like deserts, cabins, the Colorado plateau, cuisine, bar-
becue, salads, fruit, and the colors of autumn are sometimes identified as
pornographic.  Spatial analysis is difficult and is done poorly.  The pro-

gram often identifies pies as torsos.  But the program is not completely

worthlessÑit does find some naughty pictures.  Sometimes the colors are
not adjusted correctly, so that the skin does not look like skin, but the
background does.  But this seldom happens because it makes people look

either seasick or dead; usually, the people who scan the film adjust the
colors.This brings up the conditional probability issue.  This program is
slightly better at identifying pictures of puddings than it is at detecting
pictures of naked people, because an apple tart looks like skin arranged in
lines and strips.  Generally, if a Web page contains pictures of puddings,

then the program says each picture is a problem and, therefore, the Web
page is a problem.  This is a common conditional probability issue that

arises in different ways with different programs.  There is no reason to

believe that computer vision technology will eliminate it.Mike Jones and Jim Ray did some work on skin detectors.  When they
found skin, they looked for a big skin blob and, if it was big enough, theyTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.14CATEGORIZATION OF IMAGESsaid the picture was a problem.  The program cannot tell if a person is
wearing a little bathing costume or if the skin belongs to a dog instead of
a human.  They plotted the probability of a false positive against the prob-

ability of detection. If you wanted only a 4 percent probability of a false
positive, for example, then you would mark about 70 percent of porno-
graphic pictures.  I am not sure whether they used as many pictures of

puddings or the Colorado desert in their experiments as I did.  Density

also affects the results; doing these experiments right is not easy.  They

analyzed text as well as images.  I think they used a simple bag-of-words
model with perhaps some conditional probability function.  To mark

about 90 percent of the pornographic pictures, you would get about
8 percent false positives, which might be a very serious issue.  Unless you

are in the business of finding out who is looking at rude pictures, then 8
percent false alarms would be completely unacceptable.Several things make it easier to identify pornography than you mightthink.  First, people tend to be big in these pictures because there is not

much else.  There are also wild correlations among words, pictures, and

links.  Most porn Web sites are linked to most others.  What you think

about a picture should change based on where you came from on the
Web.Filtering, or at least auditing, can be done in close to real time.  A
Canadian product called Porn Sweeper audits in close enough to real time
that the producers claim that someone transmitting or receiving large

numbers of these pictures will get a knock on the door within the next day
or so, rather than the next month.  But this is not fast enough to meet

everyoneÕs needs.3.3THE FUTURE
Face detection is becoming feasible.  The best systems recognize 90
percent of faces with about 5 percent false positives.  This is good perfor-
mance and getting much better.2  In 3 to 5 years, the computer vision
community will have many good face-detection methods.  This might help

in identifying pornography, because skin with a face is currently more of
a problem than skin without a face.  Face detection technology probably

can be applied to very specific body parts; text and image data and con-
nectivity information also will help.2Milo Medin said that security software now on the market uses a camera in the computerto identify the user during sign on.  Bob Schloss commented that it is much easier to com-
pare an image to one or more known, authorized users than to an arbitrary person.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DAVID FORSYTH15However, I do not believe that the academic computer vision commu-nity will be highly engaged in solving this problem, for three reasons.
First, it embarrasses the funding agencies.  Second, my students have been

tolerant, but it is difficult to assign a job containing all sorts of problematic
pictures.  Third, it embarrasses and outrages colleagues, depending on

their inclinations.Technical solutions can help manage some problems.  I am convinced
that most practical solutions will have users in the loop somewhere.  The

user is not necessarily a child trying to avoid pornography; he or she may
be a parent who backs up the filter and initiates a conversation when prob-
lematic pictures arise.  What is almost certainly manageable, and going to

become more so, is a test to determine whether there might be naked
people in a picture.  The intimidation scenario described above could work

technically in the not too distant future.What will remain difficult are functions such as distinguishing hard-core from soft-core pornography.  These terms are used as though they

mean something, but it is not clear that they do.  Significant aspects of this

problem are basically hopeless for now.  There have been reasonable dis-
agreements about the photographs of Jock Sturgess, for example.  Many

depict naked children.  They are generally not felt to be prurient, but

whether they are problematic is a real issue.  There is no hope that a com-

puter program will solve that issue.Another example of a dilemma is a composite photograph preparedby someone whose intentions were clearly prurient.  One side shows chil-

dren on a beach looking in excited horror at the other side of the frame,
where a scuba diver is exposing himself.  There was a legal debate over

this photo in the United Kingdom and a legal issue in this country as well.
One part of the photo showed kids pointing at a jellyfish on the beach; the
other part was a lad with his shorts off.  Real people might believe that the

intention of that photograph is prurient and seriously problematic, but
there is no hope that a computer program will detect that.  It is not even

clear whether pictures such as this are legal or illegal in this country; rea-
sonable people could differ on that question.Based on my knowledge of computer vision and what appears to bepractically possible, any government interested in getting around filters
designed to censor things like Voice of America is wasting its money.
Either that, or it is engaged in the essentially benevolent activity of sup-
porting research.  Something like this could be regarded as a final course

project in information-retrieval computer vision for a statistical English
program.  This will remain true for the foreseeable future.
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.164The Technology of Search Engines
Ray Larson4.1OVERVIEW
Most search engine companies do not want to reveal what their tech-nology is or does, because they consider that to be a trade secret.  Every

company claims to do retrieval better than every other company, and they

do not want to lose their competitive edge.  I will provide a broad over-

view of how search technology works in current engines, based on the old
standard models of information retrieval.Two players are involved: the information system and the people whowant the information stored in the system.  The searchers go through a

process of formulating a query, that is, describing what they seek in ways
that the system can process.  The same sort of thing happens on the other
end, where the system has to extract information from the documents
included in its database. Those documents need to be described in such a
way that someone posing a query can find them.In general, the emphasis in the design and development of searchengines has been to make the document finding process as effective as
possibleÑtoday, however the goal seems to be to exclude some searchers.
The idea is to prevent some people from getting things that we think they
should not get.  This is anathema to someone from a library background,

where we tend to think that everyone should have access to everything
and that it is up to Mom and Dad to say no.In between the information system and the searcher are the searchengineÕs processing functions (the Òrules of the gameÓ)Ñhow the lan-
guages are structured, all the information that can be acquired from theTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.RAY LARSON17documents that come in, and how that gets mapped to what a searcherwants.  The usual outcome is a set of potentially relevant documents.  The

searcher does not know whether a retrieved document is really relevant
until he or she looks at it and says, ÒYes, that is what I wanted.ÓMuch of what happens in search engines, which generally use theÒbag-of-wordsÓ model for handling data, is structure recognition.  Search
engines often treat titles differently than they do the body of a Web page;
titles indicate the topic of a page.  If the system can extract structure from

documents, it often can be used as an indicator for additionally weighting
the retrieval process.Often the search engine normalizes the text, stripping out capitaliza-tion and most other orthographic differences among words.  Some sys-

tems do not throw this information away automatically but rather attempt
to identify things such as sequences of capitalized words possibly indicat-
ing a place or personÕs name.  The search engine then usually removes
stop words, a list of words that it chooses not to index.  This would be a

likely place to put a filter.  But this can become problematic because, when

using a bag-of-words model, one occurrence of a word does not indicate
other nonproblematic occurrences of the same word.  If the usual suspect

words were placed on the list of stop words, then suddenly the American
Kennel Club Web site no longer would be accessible, because of all of the
words that refer to the gender of female dogs, and so on.  Rarely, the

search engine also may apply natural language processing (NLP) to iden-
tify known phrases or chunks of text that properly belong together and
indicate certain types of content.4.2BOOLEAN SEARCH LOGIC
What is left is a collection of words that need to be retrieved in someway.  There are many models for doing this.  The simplest and most

widely availableÑused in virtually every search engine and the initialcommercial search modelÑis the Boolean operator model.  Simple Bool-
ean logic says either Òthis word and that word occur,Ó or 
Òthis word orthat word occur,Ó and, therefore, the documents that have those words
should be retrieved.  Boolean logic is simple and easy to implement.  Al-

most all search engines today, because of the volume of data on the
Internet, include an automatic default setting that, in effect, uses the AND
operator with all terms provided to the search engine.  If the searcher

turns this function off, then the search engine usually defaults to a rank-
ing algorithm that attempts to do a Òbest matchÓ for the query.
All of these combinations can be characterized in a simple logic modelthat says that this word either occurs in the document or that it does not.
If it does occur, you have certain matches; if not, you have other matches.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.18THE TECHNOLOGY OF SEARCH ENGINESAny combination of three words, for example, can be specified, such thatthe document has this word and not the other two, or all three together, or
one and not the other of two.  You can specify any combination of the

words.  But if you do not specify the word 
exactly as it is stored in the
index, then you will not get it.  It cannot be a synonym (unless you supply

that synonym), or an alternative phrasing, or a euphemism.4.3THE VECTOR SPACE MODEL
Another approach is the vector space model.  This model was devel-
oped over 30 years of intensive research into a finely honed set of tools.
Probabilistic models are also being used much more commonly these
days.  Many other models combine many of the same aspects, including

attempts to automatically recognize structures of information within
documents that would indicate relevance.  Alternatively, one could look

at all of the documents in a collection and consider each individual word
that occurs in any of those documents.  But most large collections have

tens of thousands of words, even hundreds of thousands.  A large propor-

tion of those words are nonsense, misspellings, or other problems that
occur once or twice, whereas other words occur often (e.g., the, and, of).The vector space model attempts to consider each term that occurs ina document as if it were a dimension in Euclidean space.  (This is why we

use three terms as an example; if there are more than three dimensions, it
becomes difficult for people to think about.)  In a vector space model, each

document has a vector that points in a certain direction, depending on
whether it contains a term or not.  The documents are differentiated on

this basis.  This example shows a system where there is a simple yes/no

process; a document either has the term or does not have it.  You also can

consider each term as having a particular weight, which can be measured
in a variety of ways, such as how frequently the word occurs in a particu-
lar document.In this model, you are calculating the cosine of the angle between twovectors in imaginary space.  The smaller the angle between the vectors,

the more similar the document is to the query.  You can rank documents

based on that closeness or similarity.1  Therefore, in most vector space
models, you do not need to match all the words.  As long as you match
1Nick Belkin said that similarity in text documents is relatively easy to compute, assumingconstant meaning of words, whereas similarity of images is very difficult to compute.  David
Forsyth gave the example of the Pope kissing a baby versus a picture of a politician kissinga baby; they are the same picture in some ways, but different in others.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.RAY LARSON19many or even some of the words, you will get closer to a particular docu-ment that has those words in it.This model uses Òterm frequency/inverse document frequencyÓ (TF-IDF), a measure of the frequency of occurrence of a particular term in a
particular document, as well as how often that term occurs in the entire
collection of interest.  If a term occurs frequently in one document but also

occurs frequently in every other document in the collection, then it is not
a very important word, and the TF-IDF measure reduces the weight
placed on it.  A common term is considered less important than rare terms.

If a term occurs in every document, then the inverse document frequency
is zero; if it occurs in half of the documents, it will be 0.3; and if it occurs in
20 of 10,000 documents, it will be 2.6.  If a term occurs in just one docu-

ment, then the IDF measure would be 4Ñthe highest weight possible.Unfortunately, most pornographic words, given the distribution of porn
on the Internet, are not rare.Once you have extracted the words from the documents, you have toput the words somewhere.  They usually are placed in an inverted file,

which puts the words into a list with an indication of which documents
they came from.  Then the list is sorted to get all the terms in alphabetical

order, and duplicates are merged; if there are multiple entries for a par-
ticular document or term, then you increment the frequency for that item.
This is the simplest form of an inverted file.  Many search engines also

keep track of where a word occurs in a document, to provide proximity
information.  They also keep track of many other things, such as how

many links there are to the page that a word is on.Finally, you differentiate the file to make a unique list for every termthat occurs in the entire database, with pointers that say in which docu-
ments they occurred and how frequently.  With that information, you can

then calculate the magical-looking formulas that provide a ranking for a
document.4.4SEARCHING THE WORLD WIDE WEB
Most Web search engines use versions of the vector space model andalso offer some sort of Boolean ranking.  Some search engines use proba-

bilistic techniques as well.  Others do little more than a coordination-level

matching, looking for documents that have the highest number of speci-
fied terms.  Some use natural language processing (Lycos, for example,

was based on some NLP work by Michael Mauldin).  Excite
Õs concept-based search may be a development of Latent Semantic Indexing (devel-
oped at Bell Labs).  The Inktomi search engine formerly used a form of

retrieval based on logistic regression.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.20THE TECHNOLOGY OF SEARCH ENGINESVirtually all search engines use the bag-of-words of model.2  Some
use additional page weight methods, looking not only at frequency of a
word in a document, but also at other things like the number of links to a
page.  Google uses in-links, for example.  If no one links to your page,

then you would get a lower rank than someone who had the same words
but many in-links.  Most search engines also include every string of char-

acters on a page, even if they are total garbage.  Therefore, in addition to

comparing one word to another, you have to compare all of the numbers,
which is difficult.Exact algorithms are not available for most commercial Web searchengines.  Most search engines appear to be hybrids of rank and Boolean

searching.  They allow you to do a guess-match symbolized by the vector

space model and also very strict Boolean matching.  But most users never

click to the Òadvanced searchÓ page, which explains how to do all of these
things; they usually just type in what they think would be an appropriate
search.  Most people looking at search logs would say, 
ÒThatÕs ridiculous.How are they ever going to find anything?ÓThe search engine obtains this material by sending out a ÒspiderÓ to
retrieve the pages from Web sites.  They retrieve only static pages, not

pages that are hiding as databases or are dynamically generated.  Most

crawlers also obey the robot.txt file on a Web site; if the file says, ÒDo notindex this site,Ó they do not index that site.  They can store millions of
words and hundreds of sites.There are different methods of crawling.  In a depth-first crawl, you
go down as deep as you can within any particular site before going on to
the next site.  Another way is a breadth-first search, where you start across

many different sites and work your way down slowly.3  Part of the reason
2David Forsyth observed that it might be logical to ask why people use the bag-of-wordsmodel, which they know to be bad.  The answer is, it is very difficult to use anything else.
Most reasonable people know about 60,000 words.  You need to count how often each one
appears in text.  You need a lot of text to do this.  If you are modeling the probability of

seeing a new word, given an old word, there are 60,000 choices for the old word and 60,000choices for the new word.  The table would be 60,000 by 60,000, and it would be difficult to
collect enough data to fill the table.  Ray Larson noted that 60,000 words is a very small size

compared to the indexes used by search engines.3Nick Belkin noted that a crawler is limited by the size of its own memory.  As soon as it
finds as much as it can hold, it stops.  Milo Medin observed that this is not an ideal approach.

Rather, you want to rank order the types of things that you will either archive or not.  If you
cannot store all the useful things, then, rather than stop, a better approach is to go back andprune out some of the duplicate or irrelevant material.  Ray Larson said finding duplicates

is a big deal, because many things either have the same name or have different names butare on the same pages.  For database storage and efficiency reasons, it is important to find
those things.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.RAY LARSON21for this is, if a spider comes to your Web site and hits you 50,000 times ina row to get every single page that you have, you will get upset.  Instead,
breadth-first spiders spread out the hits over time among a number of
sites.  The main message here is that the pages have to be connected some-

how to the starting points or else you never will get themÑthat is, unlesssomeone has sent you a pointer saying, ÒHere is the new starting point.HereÕs our site, please index it.Ó4  Some people sell algorithms that ensure
that a given page gets ranked higher than others.  Search engine compa-

nies spend a lot of their time figuring out how to identify and counteract
the ÒspammedÓ pages from those people.  It is an 
Òarms race.Ó5A paper published in Nature in 1999 estimated the types of materialindexed, excluding commercial sites.
6  Scientific and educational sites
were the largest population. Health sites, personal sites, and the sites for
societies (scholarly or other) are all larger than the percentage estimated
for pornography.7  No search engine has 100 percent coverage, and they
often cover quite different things.  There can be overlap, as well.  There

are also issues of numbers of links.  If one site indexes something, then
4Milo Medin said that some sites generate indexes by asking other search engines andindexing what they already have.  He also said that no catalog inventories show up in
searches because the inventory is designed for a database query.  The exception is when that
site has created an index page with a set of stored queries.5Winnie Wechsler said that there seems to be a fundamental tension between search en-gines striving to provide the greatest accuracy to users in terms of retrieval or filtering andWeb publishers trying to trick or mislead the search engines to make sure their sites are
listed as much and as high in rank as possible.  How does this tension resolve itself?  It does
not seem resolvable, certainly in the case of pornography.  Nick Belkin said one approach is
to use more words in a query to make the conditions more restrictive.  A query with 10

words will get a much better result than one with only 2 words because it defines muchmore context.  The difficulty is that, even though the average number of words per query on
the Web has been going up, it is still only about 2.3 words, up from 1.7 words a few years
ago.  With very simple search engine technology, it may help to encourage people to use
more words in their queries.6Steve Lawrence and C. Lee Giles, ÒAccessibility and Distribution of Information on theWeb,Ó Nature 400(6740): 107-109, July 8, 1999.
7Milo Medin said that the declining cost of Web servingÑgenerally a good thingÑhasmade it easier for amateur pornographers to get published.  Medin
Õs service offers free Webhosting for a certain amount of material.  Subscribers are not allowed to post pornography
or objectionable material, but there is no cost or punishment if they do, so they take advan-tage of this situation.  The company audits sites based on the amount of traffic to them.

When a site attracts a certain amount of traffic, it triggers a red flag and generates a query tothe people in charge of investigating abuse.  Medin recalled that, when he worked for NASA,
data on international links had to be controlled.  When someone put up a porn site, the link
utilization to that region would rise.  A wiretap would reveal where the traffic was going.
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.22THE TECHNOLOGY OF SEARCH ENGINESanother site will index it.  Things that are unique tend to stay unique
within a particular search engine.8In looking for images, text retrieval technology looks for text that isassociated with images.  It looks for an image link tag within the HTML

and the sentences that surround it on either side.  This can be highly de-

ceptive.  The words 
ÒOh, look at the cute bunniesÓ mean one thing on a
childrenÕs Web site and something entirely different on PlayboyÕs site.Thus, the words alone may not indicate what those images are about.8Milo Medin emphasized the business dynamic, noting that creating the search capabilityto find an obscure Web page may not be worth the cost in terms of its impact on the sub-scriber base.  Say a search engine fails to find 5 percent of the material on the Internet.  To

some people whose content is in that 5 percent, this is important.  But if the cost of finding
that 5 percent is double the cost of finding the other 95 percent and the bulk of searchers aresatisfied with that performance, it may not be worth it.  Search engines are not librarians;

they exist for a business purpose.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.235Cyber Patrol: A Major Filtering Product
Susan Getgood5.1INTRODUCTION
SurfControl, Inc., is the worldÕs largest filtering company, with officesand companies throughout the world.  The company attained this posi-

tion through a combination of organic growth and growth by acquisition.
In 1998 it got into the corporate filtering business, and in 1998 and 2000 it
acquired both SurfWatch and Cyber Patrol, the pioneers in filtering to
protect kids from inappropriate content.I will tell you what filtering software is and what it is not.  It is safetytechnology, like a seatbelt for Internet surfing.  Seatbelts are not 100 per-

cent guaranteed to save a childÕs life, but there is no responsible parent in
America who does not buckle up a child in the car.  We believe the situa-
tion is the same in protecting kids from inappropriate content online.  Fil-

tering software puts the choice of how and when children can use the
Web in the hands of the people who should have it:  parents and educa-

tors.  It is also the most effective way to safeguard kids from inappropri-

ate Web content without compromising First Amendment rights, which is
important.  We are creating a solution that puts choice in the hands of the

people who need it, while keeping the government out of those choices.Filtering software is not a replacement for the guidance of parentsand educators.  I doubt any filtering software company would suggest

that parents, teachers, educators, administrators, business people, or any-
one use filtering software without clearly providing the guidance that
children need to understand what they see on the Internet.Web filtering products either block or allow access to Web sites byTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.24CYBER PATROL: A MAJOR FILTERING PRODUCTeither IP addresses or domain names.  Most of the widely available com-
mercial products are list based, with human reviewers.  These products

also use some artificial intelligence (AI) tools but not as the primary
mechanism of filtering.  Technologies work for us in the research process,

but they do not replace human review, which verifies that the content on
a page is about, for example, a marijuana joint and not the Joint Chiefs of
Staff, or that a woman in a picture is not wearing a tan bathing suit.  We

need human reviewers to make sure that content really is inappropriate.5.2WHY FILTER?
About 30 million children in this country have access to the Internet,and about 25 percent of them are exposed to some type of unwanted or
inappropriate online content.  Although we are mostly concerned here

with sexually explicit content and pornography, it is important to remem-
ber that parents and educators are concerned about broader types of con-
tent, from hate sites and intolerance material to how to build a bomb and
buy a gun.  Parents and educators are the people with whom I deal most

in my job, which is running the Cyber Patrol brand.Parents want this type of technology and they want it used both inschools and at home.  In 2000, a study by Digital Media found that 92

percent of Americans want some type of filtering to be used in schools;
they are concerned about the content that their children see.  Our job is to

find a way to make filtering an effective technology solution that does not
get in the way of the educational experience, whether at home or in school.Interestingly, we found that people do not always realize there is aproblem until they look at their hard drives and find Miss April or Miss
May.  As reported in the press recently, a teacher (a customer of one of our

competitors) checked the history of each computer and was appalled at
what the students were able to access.  They were accessing sexually ex-

plicit material, gambling, applying for credit cards, buying products with-
out parentsÕ permission
Ña whole host of things.  There is clearly a prob-
lem out there in the world, and parents and schools want to do something
about it.Corporations filter for four basic reasons:  (1) productivity of employ-
ees; (2) legal liability for inappropriate content being available on net-
works; (3) issues of inappropriate surfing, which takes up room in the
information pipeline; and (4) increasing demand for security to prevent
compromise of confidential information.  In schools, we tend to focus on

filtering to protect children from inappropriate content.  But we have

found that network bandwidth increasingly is an issue in schools, espe-
cially with respect to federal mandates for filters, which we oppose.  We

believe that schools purchase filtering software because it solves a wideTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.SUSAN GETGOOD25variety of problems, not just the simple, single problem of protecting kidsfrom inappropriate content.We mailed a quick e-mail survey out last week to 1,200 customers andgot a 2.64 percent response rate, which is fairly good in this time frame.
We asked them how important Internet bandwidth was to them last year
versus this year.  Fifty-five percent said it was very important or impor-

tant last year, compared to 70 percent this year.  Similarly, 37 percent

were either neutral or thought it was an unimportant issue last year, com-
pared to only 24 percent this year.  This is what our customers are telling
us, both anecdotally and numerically.  The bandwidth issue arises when

kids in the library go off to look at Napster,1 free e-mail accounts like hotmail and Yahoo mail, and anything else not on task.  Even something

otherwise appropriate, such as checking out sports scores, is not on task
at work or school.  If Napster is regulated, something else will come along

to replace it as the next big thing on the Internet.  We try to stay ahead of
what our customers need, and Internet developments like Napster prove
to me that educators are looking at the whole issue of managing the

Internet in the classroom, not just the management of sexually explicit
content.5.3SUPERSCOUT AND CYBER PATROL
We have two brands, SuperScout and Cyber Patrol.  I will describe
SuperScout briefly and then concentrate on Cyber Patrol.SuperScout was developed to do filtering, monitoring, or reporting ina corporate environment.  It uses an extensive list of nonbusiness-related

Web sites.  It has an optional AI tool that provides dynamic classification
of content, looking at the sites employees visit.  Some sites are on the

SurfControl list, and some are not.  If a site is not on the list, then the AI

program uses pattern recognition and textual analysis.  It can run this

information against the category definitions of the business product and
give the corporation an additional list that can act as a buffer against the
content that people actually see.  We do not plan to add this technology to

the home filtering products, although we use it in research before the re-
viewers look at something. We see a trend, especially in institutional set-
tings but also in homes, toward managing access to the content that people
actually are trying to seeÑas opposed to having huge category lists ofwhich employees are trying to access only 1 percent.1Milo Medin said that the bandwidth issue is driven primarily by multimedia.  Many
Internet service providers have issues with Napster traffic; about 10-15 percent of band-width traffic on his companyÕs interconnects is Napster traffic.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.26CYBER PATROL: A MAJOR FILTERING PRODUCTCyber Patrol, which keeps kids safe, comes in stand-alone versionsfor the home and network versions for schools.  The network version op-
erates either on local area networks or through proxy servers.  Cyber Pa-

trol for schools focuses on blocking Web access, and it goes through the
Microsoft proxy server, Microsoft Internet Security and Acceleration
Server 2000, or Novell Border Manager.  We incorporate elements within
the software that address the whole scope of what parents are trying to do
to protect their kids.  We enhanced security and improved tamper resis-

tance in the latest version for the home.  Parents can customize settings

for multiple children or multiple grades.  We also provide information

about why a site is blocked, so that parents can explain to their children
why they were not allowed to access something.Cyber Patrol works the same way if you are a subscriber to AmericaOnline (AOL).  Typically it is used in addition to AOL
Õs parental controls,which are based on work that we did.  Other Internet service providers

also offer these types of controls.  An advantage to using a stand-alone

filter is that it works regardless of how children access the Internet.  It

follows the same set of rules regardless of whether a child uses AOL, your
dial-up modem to work, or a dial-up modem they got from a friend, be-
cause the software is installed on the computer.  We have many custom-

ers who use AOL but also use Cyber Patrol specifically because they want
the same settings and time management across multiple services.Server-based filters, the primary design used in schools and busi-nesses, tend to be integrated with networks and users.  When you log in

as Jimmy Smith in the seventh grade, the filter knows that you are Jimmy
Smith and how to apply the filtering rules.  Different rules can be applied

for different users within a school system.  In our user base, school dis-

tricts have different rules in elementary school versus middle school ver-
sus high schoolÑexcept for sexually explicit material, which tends to beblocked throughout the whole school system.  As an example, you may

not want the fourth graders to access material about intolerance, but the
seventh graders may be doing a project on hate groups.  (Setting rules in

different ways is consistent with the new law against disabling filters.)
Eventually, as student identification (ID) cards move toward becoming
smart cards, a childÕs filter rules, lunch money, and library books will allbe on the ID card.22Milo Medin said that user identification and sign-on always have been complicated be-cause they involve sharing a password.  But fingerprint scanners are becoming less expen-

sive and are starting to appear in keyboards.  This enables a user-friendly level of identifica-
tion, because you no longer need to worry about getting your password right.  This will
become more common in the marketplace.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.SUSAN GETGOOD27We block lists of specific pages (identified by their uniform resourcelocator designations (URLs)); we do not analyze the content of a page as it
is downloaded to a subscriberÕs computer.  Playboy.com is blocked be-
cause it is Playboy, not because the program senses nude pictures or for-
bidden words.  We can block an entire site or by page level.  Cyber Patrol

for homes is based on a list called the CyberNOT List, reviewed in its
entirety by human reviewers.  Our team of professional researchers is

made up of parents and teachers.  Parents can then select the categories of

lists that they want to use.  We tailor the filtering levels to meet the needs

of different children.  Age-appropriate filtering is possible; for example,

we have a sex education category so that material that otherwise would
be considered sexually explicit can be made available to older children en
masse.There are 13 CyberNOT categories to choose from:  violence and pro-
fanity, partial nudity, full nudity, sexual acts, gross depictions, intoler-
ance, satanic and cult, alcohol and drugs, alcohol and tobacco, drugs and
drug culture, militant and extremist, sex education, and questionable/
illegal material and material related to gambling.  The definitions are pub-

lished on our Web site and in the product itself, so that parents can review
the definitions as they decide how to tailor the softwareÕs settings to fittheir needs.  About 70-80 percent of the list content is violence or profan-

ity, partial nudity, full nudity, sexual acts, and gross depictions.  The other

categories make up 20-30 percent; these categories are more difficult to
research and much less obvious.We publish our content definitions and categories.  We give you the
ability to override or allow based on your own preferences, but we do not
publish the sites that are on our category list.  We have spent thousands of

dollars to build a proprietary list that cannot be duplicated by anyone; I
have yet to hear a commercial reason that makes sense why we should
allow that.  As a company devoted to protecting kids from inappropriate
content, we will not publish a directory of dirty sites.We do not filter URLs or Web sites by keyword, which is an impor-tant point.  We do use keywords as part of the research process to get

suspect material to look at.  The training process is done on the job using

a shadowing technique.  That is, a new researcher works with someone

who has been doing it for a while to understand the process.  Researchers

work in teams, which is important in identifying material, particularly
when the material is difficult to classify and a discussion about it is help-
ful.  Most researchers have child development backgrounds, typically

with some type of training, whether teaching certification or on-the-job-
training as a parent.  They are not child development specialists or psy-

chologists, but they have an appreciation for why and how to classify the
material.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.28CYBER PATROL: A MAJOR FILTERING PRODUCTCyber Patrol does not interfere with, or get involved in, the searchengine process.  The software works purely in the browsing process.  We
can block a search on sex if that is what the parent wishes, but we do not
filter search results.  If a child tries to visit a blocked site, Cyber Patrol

shows you that the site does exist but that you were not allowed to access
it, and tells the parent why.  If you are trying to make this site available for

your family, you can go back and change that particular siteÕs setting andknow that you are fixing the right thing, as opposed to stumbling around
blindly, trying to figure out why a site was blocked.We deal with two kinds of chat.  One is Web-based chat, which we
block specifically by blocking the category of Web-based chat.  Alterna-

tively, you can use privacy features, which allow kids to go into chat
roomsÑif you want them to be allowed to talk about bird watching orwhateverÑbut not to give out their names, addresses, or phone numbers.It cannot do anything about a 15-year-old who is determined to tell some-
one his address.  But if a naive 12-year-old inadvertently gives out his

number, then the feature replaces it with a set of nonsense characters.  We

also can block Internet relay chat, which is used much less often now than
in the past, either completely or based on the chat channel name.SurfControl gets a lot of feedback from customers.  When a customerasks us to look at a site to see if it should be blocked for the larger popula-
tion, not just for his or her own family, we spend more time on it than we
otherwise might.  Often, however, such sites do not warrant being added

to a list that a large population uses.Consumers can decide how well we make decisions by trying theproduct before they buy it.3  Parents using Cyber Patrol can try to go to a
Web site that is blocked and, if they think it should not be blocked, bypass
the filter and look at the site and make a personal decision about whether
Cyber Patrol was right or wrong in putting that site on the list.  (Parents

can override the system, but children cannot, because, hopefully, they do
not have the necessary password.  Picking the family dog
Õs name as thepassword is probably not a good idea.)  There is an element of trust.  If

they believe that we offer them a good place to startÑfiltering software isnot a replacement for parents, nor is it a solution for everythingÑthen it isa reasonable place to start to protect their kids.  We try to provide parents

with a solution that gives them the ability to implement their own choices.3David Forsyth argued that it is easy to determine whether a dishwasher works becausethe plates either come out clean or dirty, but it is difficult to tell whether Cyber Patrol works,so the choice issue becomes problematic.  Milo Medin noted that the average housewife is
not likely to figure out the difference between good and poor dishwashing fluid.  Rather, she

makes decisions based on brand, consumer reports, and other evaluations.  Medin said he
does not make decisions about highly technical matters based only on his own experiments;third parties do these lab tests.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.SUSAN GETGOOD29We cannot guarantee 100 percent true positives, but we do the bestjob we can to build the tool.  If there is a metric for deciding how much

accuracy is enough, it is the market.  The market decides what level of

accuracy it wants by making product choices.  If we have a good product,

then presumably parents, schools, and businesses will continue to buy it.
If we did not have a good product, then I truly believe that Joe in his
garage would come up with something better.One reason why we oppose mandatory filtering is that we believe theuse of these products should be a choice that parents and educators make,
just as it is a choice for businesses.  When you select and evaluate a prod-

uctÑin our case, you can try it for 14 days before you buy itÑthen thechoice is yours.  If it is mandated, then it is not a choice.
5.4THE REVIEW PROCESS
To clarify, we have two review processes.  One is the process of find-ing new material that comes onto the Internet.  We use a variety of mecha-

nisms, from search engines to crawlers.  That same group of people is

involved in the re-review process to make sure that once something is on
the list, it should remain on the list.The Cyber Patrol team consists of about 10 people; most have beenwith us for at least 2 years and some more than 4 years.  It is a good job for

a parent who wants a part-time or supplementary job.  We have worked

hard to ensure that the job entails more than just looking at inappropriate
content all day, which would be absolutely mind numbing.  We also build

positive lists.  We have a Yes list that we use.  The job also has responsibil-

ity in the technical side of building these lists.It might sound like a great job, looking at porn all day.  But after
about a day, it becomes less fun.  To understand what they are reading,

the reviewers can spend anywhere from a minute or less on pornographic
material to upwards of 10 minutes on intolerance material or something
that requires textual analysis.  A sexually explicit site can be judged fairly

quickly; a picture is a picture.  If deeper probing into a site is required,

that takes longer.  We do not block sites simply because they do

mousetrapping,4 and we do not view this technique as a red flag for sites
to be reviewed.  (I plan on suggesting it, however.)
4MousetrappingÑa technique in which clicking on an item causes a second item to popupÑis used by pornography and gambling sites.  Milo Medin said that he would pay for
blocking of sites that use mouse trapping, especially when it has multiple levels.  Herb Lin

noted that the underlying technology has legitimate purposes, such as in making surveys orquestionnaires pop up on consumer sites.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.30CYBER PATROL: A MAJOR FILTERING PRODUCTIt is a mistake to attribute political motives to SurfControl or any other
major filtering company.  We add sites to our list based on their content.

In the case of the gossip site The Register,5 my understanding is that it
published a detailed explanation of how people could use a loophole in
anonymous proxies to get around the use of filtering softwareÑto let kidsget pornography.  This is why the site was added to the list.
6  The ulti-
mate example of a difficult case might be determining whether an image
is art or nudity.  We would not consider work by Rubins Wake to be

nudity, because it is art.  However, if you duplicated one of those images
on your own personal Web page, using your own friends and family, then
that probably would not qualify as art.We make sure that we re-review material, so that Web sites that goout of existence do not stay on our list.  We have regular re-reviews of the

list categories, both as projects within the research department and as part
of the customer feedback process.  On average, we probably cycle through

the whole CyberNot List about once every year.  Some categories get more

frequent reviews.  We look at some sites every month.  A couple of orga-

nizations ask us to look at sites every month, and we do.  After the

HeavenÕs Gate incident,7 we made an effort to go back through all the
material on the cult.  The same thing was done after the Columbine High
School shooting.8  We do re-reviews of the categories that are particularly
relevant to these sorts of issues.  The software comes with a year
Õs sub-scription to daily updates, so it is updated on a regular basis.We are looking at AI to speed up some of the review processes.  One
approach is dynamic pattern matching.  Internal tests reveal up to 85 per-

cent accuracy or agreement between what our reviewers find and what
the tool finds.  As that number starts to improve, we will be able to start

relying more on this tool.  Right now we do not believe that eliminating

the human review process in Cyber Patrol is the right thing to do.Here are two paraphrases of what reviewers say about their jobs.They take this job very seriously, which is one reason why we have been
able to keep some of these people for upwards of 4 or 5 years.  They really
5David Forsyth said The Register claimed it was blocked because it had said the financialbasis of the filtering market was not as sound as it looked and that SurfControl might betaken over.6David Forsyth said that this is a situation in which a legitimate discussion of a technologi-cal issue was cut short because useful, retrievable information was taken out of the publicdomain.  Susan Getgood said that the company does not claim it never makes mistakes and
that perhaps the researcher who added the site to the list was being overzealous.7In March 1997, 39 members of the HeavenÕs Gate cult committed suicide.8In April 1999, two students went on a shooting spree in their suburban high school inJefferson County, Colorado. Thirteen people were killed and 21 were wounded.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.SUSAN GETGOOD31do believe that they are doing something that helps parents do their jobbetter.¥ÒBeing a researcher demands an open mind and an objective out-look at all times.  We try to protect children and many adults from offen-

sive and harmful material without encroaching on anyoneÕs right to freespeech.Ó¥ÒIt can be both difficult and rewarding.  At times, seeing the worst
of what is on the Internet can be difficult, but the reward comes when you
know that a small child, whose parents are responsible enough to use
filtering, will not ever have to see what I just saw when I put it in the
database.Ó5.5THE FUTURE
As part of SurfControl, we take advantage of an active research anddevelopment department.  We now have 40 researchers around the world,

an increase from the time when Cyber Patrol alone had 10.  This gives us

an ability to deal with international content in a cultural context, rather
than as Americans looking at something in German or Dutch or Spanish.
We are looking at the next generation of filtering and what we need to
continue to do to build these products.  We do not create the need for

these products; the need is out there.  We are doing our best to develop

software and products that meet the need.Forty reviewers might seem like a small number if you were startingtoday.9  If you started this year and tried to do the whole Web in 365 days,
you probably would have a tough time.  But we have been doing this for

6 years, so there is a base that we are not repeating.  We focus on the

inappropriate content; we do not try to look at every single page on the
Internet.  To increase accuracy in dealing with material that is difficult to

categorize, it is not a question of hiring more people but rather of looking
at tools such as image recognition.  We can manage the human costs and

also improve the front-end part of the research.Clearly, there will be more bandwidth to homes in the future.  This
will allow us to use more robust AI technologies in these products.  Com-

mands such as ÒDonÕt show me more like this oneÓ rely on dynamic cat-
egorization.  Modems cannot handle this effectively; you need high-speed,
9Marilyn Mason said that there are more than 1 billion sites total.  Winnie Wechsler said
that a couple of million new sites are added each year.  David Forsyth said that, given 1
million new Web sites a year (not an unreasonable number), then 40 reviewers have to re-view 25 sites an hour in a busy year to get them all done.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.32CYBER PATROL: A MAJOR FILTERING PRODUCTbroadband connections.  Image filtering also is clearly part of the future,
but there is not, as yet, a solution for this.  We think the use of filtering

also will be changed by e-mail, which is now available to just about every-
one, and instant messaging.  We will start looking at how to incorporate

ways to keep these methods safe for kids.Privacy is of great interest to us, because protecting kidsÕ private in-
formation goes hand-in-hand with protecting them from inappropriate
content.  We already pay attention to both children
Õs rights for privacyand parentsÕ decisions about their children
Õs privacy.  We chose not to put
a logging or monitoring feature into the Cyber Patrol home product be-
cause children have a right to privacy if they are looking at appropriate
material.  As rules on privacy preferences
Ñrules about going to Web sitesthat collect information on kidsÑbecome finalized, we will be able toimplement those rules in a technological fashion, so that parents can pre-
vent kids from going to Web sites that, for example, publish surveys.  We

will be able to implement those types of thingsÑif the market wants them.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.336Advanced Techniques for
Automatic Web Filtering
Michel Bilello6.1BACKGROUND
As of 1999, the Web had about 16 million servers, 800 million pages,and 15 terabytes of text (comparable to the text held by the Library of
Congress).  By 2001, the Web was expected to have 3 billion to 5 billion

pages.1To prevent kids from looking at inappropriate material, one solutionis to have dedicated, pornography-free Web sitesÑsuch as Yahoo!Kids
and disney.comÑand assign reviewers to look at those particular Web
sites.  This is useful in protecting children too young to know how to use

a Web browser.Filtering is mostly text based (e.g., Net Nanny, Cyber Patrol,CYBERSitter).  There are different methods and problems; for example,

Cyber Patrol looks at Web sites but has to update its lists all the time.  You

can also block keywords, scanning the pages and matching the words
with keywords.  But keyword blocking is usually not enough, because

text embedded in images is not recognized as text.2  You could block all
images, but then surfing an imageless Web would become boring, espe-
cially for children.  A group at the Nippon Electronic Corporation (NEC)
1Steve Lawrence and C. Lee Giles, ÒAccessibility and Distribution of Information on theWeb,Ó Nature 400(6740): 107-109, July 8, 1999.2Michel Bilello said that his group has used a technique that pulls text off images, such aschest X-rays used for research purposes.  They process the x-ray image, detect the text, and
then remove, for example, the name of the patient, which the researcher does not need toknow.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.34ADVANCED TECHNIQUES FOR AUTOMATIC WEB FILTERINGtried to recognize the clustering communities within the Web. You could,for example, keep the user away from particular communities or exclude
some communities from the allowed Web sites.6.2THE WIPE SYSTEM
In the Stanford WIPE system,3 we use software to analyze image con-
tent and make classification decisions as to whether an image is appropri-
ate or not.  Speed and accuracy are issues; for example, we try to avoid

both false positives and false negatives.  The common image-processing

challenges to be overcome include nonuniform image background; tex-
tual noise in foreground; and a wide range of image quality, camera posi-
tions, and composition.This work was inspired by the Fleck-Forsyth-Bregler System at theUniversity of California at Berkeley, which classifies images as porno-
graphic or not.4  The published results were 52 percent sensitivity (i.e., 48
percent false negatives) and 96 percent specificity (i.e., 4 percent false posi-
tives).  The Berkeley system had a rather long processing time of 6 min-

utes per image.In comparison, the WIPE system has higher sensitivity, 96 percent,and somewhat less specificity (but still high) at 91 percent, and the pro-
cessing time is less than 1 second per image.  This technology is most

applicable to automated identification of commercial porn sites; it also
could be purchased by filtering companies and added to their products to
increase accuracy.In the WIPE system, the image is acquired, feature extraction is per-formed using wavelet technology, and, if the image is classified as a pho-
tograph (versus drawing), extra processing is done to compare a feature
vector with prestored vectors.  Then the image is classified as either por-

nographic or not, and the user can reject it or let it pass on that basis.
There is an assumption that only photographsÑand not manually gener-ated images, such as an artistÕs renderingÑwould be potentially objec-tionable.  Manually generated images can be distinguished on the basis of

tones: smooth tones for manually generated images versus continuous
tones for photographs.  Again, only photographs would require the next

processing stage.3For a technical discussion, see James Z. Wang, Integrated Region-based Image Retrieval,Dordrecht, Holland: Kluwer Academic Publishers, 2001, pp. 107-122.  The acronym WIPE
stands for Wavelet Image Pornography Elimination.4Margaret Fleck, David Forsyth, and Chris Bregler, ÒFinding Naked People,Ó Proceedingsof the European Conference on Computer Vision, B. Buxton and R. Cipolla, eds., Berlin, Ger-many: Springer-Verlag, Vol. 2, 1996, pp. 593-602.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.MICHEL BILELLO35This work was based on an information-retrieval system that finds ina database all the images ÒcloseÓ to one selected image.  From the selected
image the software looks at thousands of images stored in the database
and retrieves all the ones that are deemed ÒcloseÓ to the selected image.
The images were tested against a set of 10,000 photographic images and a
knowledge base.  The knowledge base was built with a training system.

For every image there is some trusted element, a feature vector can be
defined that encompasses all the information, texture, color, and so on.
Then images are classified according to the information in this vector.The database contains thousands of objectionable images of varioustypes and thousands of benign images5 of various types.  In the training
process, you process random images to see if the detection and classifica-
tion are correct.  You can adjust sensitivity parameters to allow tighter or
looser filtering.  You could combine text and images or do multiple pro-

cessing of multiple images on one site to decrease the overall error in
classifying a site as objectionable or not.A statistical analysis was done showing that, if you download 20-35images for each site, and 20-25 percent of downloaded images are objec-
tionable, then you can classify the Web site as objectionable with 97 per-
cent accuracy.6  Image content analysis can be combined with text and IP
address filtering.  To avoid false positives, especially for art images, you

can skip images that are associated with the IP addresses of museums,
dog shows, beach towns, sports events, and so on.In summary, you cannot expect perfect filtering.  There is always a
trade-off between performance and processing effort.  But the perfor-

mance of the WIPE system shows that good results can be obtained with
current technology.  The performance can improve by combining image-

based and text-based processing.  James Wang is working on training the

system automatically as it extracts the features and then classifying the
images manually as either objectionable and not.75To develop a set of benign images, David Forsyth suggested obtaining the Corel collec-tion or some similar set of images known to be not-problematic or visiting Web news groups,where it is virtually guaranteed that images will not be objectionable.  He said this is a rare

case in which you can take a technical position without much trouble.6David Forsyth took issue with the statistical analysis, because there is a conditional prob-ability assumption that the error is independent of the numbers.  In the example given ear-

lier with images of puddings (in ForsythÕs talk in Chapter 3), a large improvement in perfor-mance cannot be expected because there are certain categories in which the system will justget it wrong again.  If it is wrong about one picture of pudding and then wrong again about

a second picture of pudding, then it will classify the Web site wrong, also.7For more information, see <http://WWW-DB.Stanford.EDU/IMAGE> (papers) and<http://wang.ist.psu.edu>.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.367A Critique of FilteringBennett Haselton7.1INTRODUCTION
I have been running the Peacefire.org site for about 5 years, and wehave become known as a source of mostly critical information about block-
ing software and filtering.  I am biased in general against the idea of filter-

ing, as well as the existing limitations, but that is fair because all intelli-
gent people should have opinions about what they study.  They simply

need to design the experiments so that the person with the opinion will
not influence the outcome.The earlier presentations provided a general idea of how differenttypes of programs work.  Some programs examine the text on a down-

loaded page to look for keywords in the Web page address (the uniform
resource locator, or URL) or in the body of the page.  Other programs are
mainly list based; they do little analysis of the text on a page but have a
built-in list of sites that are blocked automatically.  All the programs that

I know of are some combination of the two types.  They have some key-

word filtering and some list filtering, but they can be slotted easily into
one of these categories.Most mainstream commercial programs, such as Cyber Patrol, NetNanny, and SurfWatch, are list based.  People often talk about a scenario

in which a site might get blocked if the word ÒsexÓ is in the title or first
paragraph.  This scenario has not been accurate for years.  Sites can be

blocked inaccurately, but this is not a correct way to describe what hap-
pens, because the most popular programs that look at words on the page
also work off built-in lists of sites.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BENNETT HASELTON377.2DEFICIENCIES IN FILTERING PROGRAMS
The mainstream commercial programs used in the homeÑwhich fil-ter and block pages on the fly (not for auditing or later review)Ñdo notfilter images.  We did a study involving the only commercial program at

the time that claimed to filter images on the fly, using 50 pornographic
images taken from the Web and 50 nonpornographic images.  We found

that the software performed no better than random chance if the images
were placed in a location that the software did not know about in ad-
vance.  All the pornographic and nonpornographic images in the test re-

mained accessible, so the claim of filtering based on image contents turned

out not to be true.The company later came out with some fixes so that the program be-gan to filter based on skin tone, but it could not do complex object recog-
nition.  The best it could do was to count the number of pixels in the

picture that were skin toned and then block based on that.  We did an-

other test involving the 50 pornographic images and 50 nonpornographic
pictures of peopleÕs faces, and the software scored exactly the same foreach type; it was not able to tell the difference.CYBERSitter is mostly a content-based program.  Cyber Patrol is
mainly a list-based program.  The content-based programs are notorious

for errors that arise if you block sites based on keywords on the page or in
the URL.  It is nowhere near as advanced as the vector space model de-

scribed earlier.  Yet, even though these programs are so sloppy, the ex-

amples of what they block are not very controversial, because the com-
pany justifiably can say it has no control in advance over what will be
blocked.  There is a certain phrase in the word filter, and if a site uses that

phrase, then it is not really the companyÕs fault.  Blocking software got a
bad reputation initially because of examples like a page about the explo-
ration of Mars being blocked because the title was ÒMars Explore,Ó or
marsexpl.html.I have a friend named Frank who made a Web page about Cyber Pa-trol, and he later found that his page was blockedÑnot because he wascriticizing the software, but because his name was Frank, and ÒankÓ was
on Cyber PatrolÕs list of dirty phrase keywords.  The list of blocked sites
could not be edited, but the list of dirty phrases was viewable and you
could add and remove terms from it.  Presumably to avoid offending the

parents who had to deal with it, the company put in word fragments in-
stead of whole words.  The list contained phrases such as 
ÒuckÓ and 
Òank,Óthe latter apparently an abbreviation for ÒspankingÓ because the com-
pany wanted to block pages and chat channels about spanking fetishes.There are many other examples, some involving programs that evenremove words from the pages as they download them, without making itTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.38A CRITIQUE OF FILTERINGobvious that words were removed.  Sites blocked by these programs are
much more controversial, because the company can control exactly what
is on the list.  If you find something that is blocked, then they cannot claim

they did not know in advance.  Supposedly, everything on the list was

checked for accuracy in advance.We periodically do reports, published on the Peacefire.org site, aboutwhat types of sites we have found blocked.  We focus on sites blocked by
the list-based programs; finding sites blocked by the keyword-based pro-
grams is not very interesting, because you almost always find some part
of almost every site blocked by something like CYBERSitter.  If someone

wants to know if they have standing to challenge a local library filtering
ordinance, and they want an example, I say: ÒWell, if you have 20 or moredocuments, I will just run it through CYBERSitter and one of them will be
filtered.ÓThe main controversy regarding list-based programs is how they cre-ate the list of sites to block.  The lists are divided into categories.  If a site

is classified into one of these categories, then the site will become inacces-
sible.  This gives the illusion of more flexibility than really exists.  If you

are using, say, SurfWatch and you elect to block only sex sites, then you
block sites that SurfWatch has classified under its sex category, which
may or may not be accurate.  Even if it were accurate, it might not agree

with your views on what a sex site is.  Even if you did agree with the

company on what qualified as a pornography site, the actual review pro-
cess might not be accurate.7.3EXPERIMENTS BY PEACEFIRE.ORG
We are one of the third parties that designed experiments to test theaccuracy of the lists used by these companies.  There are a couple of ways

to do this.  The list of blocked sites is supposed to be secret and is not

published, but it is always stored in a file that comes with the software.  A

client-based program has a local list, and periodically you update the list
by downloading the latest version from the company that makes it.  You

can try to break the code on the file and decrypt it, using either Unsoftware
or something else.  I wrote a decryption program for CYBERSitter in 1997,

and two other programmers wrote a decoding program for Cyber Patrol
in 2000.  You run one of these programs on a computer that has

CYBERSitter or Cyber Patrol installed, and it reads the file, decrypts it,
and prints out the list of blocked sites into a text file.The Digital Millennium Copyright Act (P.L. 105-304) was passed in1998.  The Library of Congress was designated to set out regulations for

how parts of that act would be enforced.  Part of the act prohibited
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BENNETT HASELTON39decryption of certain files perceived to be storing trade secrets of the com-pany that produced them.  The Library of Congress, which had been fol-

lowing the controversy regarding third parties decrypting lists of sites
blocked by blocking software and criticizing them, specifically said that
the act of decrypting the list of sites blocked by a blocking program would
be considered exempt from this law.  But at the time these programs came

out, there was no such exemption, so many people were worried about
the consequences.If you have a server product installed on the Internet serviceproviderÕs system, then you do not have access to the file where the list ofblocked sites is stored.  In that case you need to do a traffic analysis in-

stead of decrypting.  The hard way is trial and error, looking at your fa-

vorite sites in a directory like Yahoo.  The easier approach is to run a list of

sites through the program.  I have written scripts that run a large number

of URLs through one of these programs and record exactly which ones
are blocked.  This takes some programming skill, and third parties who

review this type of software generally do not go to this much trouble.
Reviewers for Consumer Reports or 
PC Magazine usually just use the trial
and error approach.  The flaw in that approach is that if you want a small

sample of sites and you get them from a place like YahooÑperhaps sitesin one of YahooÕs pornography categoriesÑthen you will get an overlygood impression of the software, because the software gets its list of por-
nography sites from the same type of place.  Any good program should

block 100 percent of those sites.  You want to test a larger sample of sites

to get a more reliable accuracy rate.In one study, we took a cross section of 1,000 dot-com domain namesfrom the files of Network Solutions, which keeps track of all 22 million
(and counting) dot-com sites.  We wanted to do a random selection.  The

problem was that if the blocking error rate came out too high with a ran-
dom selection, then anyone could claim that we stacked the deck by not
taking a really random sample.  This is a deeply politicized issue, and the

companies knew me as someone who had strong feelings about it.  It

would be too easy for them to say that we must have cheated by using a
disproportionate number of sites that we knew were errors.  Therefore,

we took the first 1,000 dot-com sites in an alphabetical list of all of the
sites, because the first ones are not any more or less likely to contain er-
rors than the rest of the list.  They all began with 
ÒA-1,Ó I think.
This report is linked to my subpage.  You can see the 1,000 sites that
we used and the ones that are blocked and which ones of those we classi-
fied as errors or nonerrors.  The sites that we classified as inaccurately

blocked were cases in which we believed that no reasonable person could
possibly believe that they were accurately blocked.  These sites were about

things like plumbing, aluminum siding, or home repair toolkits.  There
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.40A CRITIQUE OF FILTERINGwas absolutely no doubt that these were errors; we did not encounter anyborderline cases at all.  I did the analysis again using 1,000 random dot-

com sites, and, for all cases, it looked like the result was within 10 percent
of the error rate we got doing it the alphabetical way.We publicized this report with a strong caveat that the second digit ofthe error rate should not necessarily be taken as accurate.  For example, if

the error (false positive) rate is 50 percent, we are saying that 50 percent is
likely to be close to the actual error rate.  If a company claims that it is 99

percent accurate, and we get 30 blocked sites and 15 of them are errors,
we can determine with almost 100 percent accuracy that their 99 percent
figure is false.  Our 50 percent figure could indicate an error rate any-

where from 30 percent to 70 percent, but we definitely can say that 99
percent accuracy is a false claim.Of the 1,000 dot-com sites in the study, programs blocked anywherefrom 5 to 51 sites.  Of those blocked sites, how many do we feel were

errors?  In the case of the five blocked sites, the error number is not mean-

ingful.  In the case of 50 blocked sites, there is a certain spread of error.

The intent was not so much to come up with a hard number for accuracy
but rather to address the question of whether the Ò99 percentÓ claims are
true.Here is what we found.  Cyber Patrol blocked 21 sites, and 17 of them
were mistakes.  These were not borderline cases at all; these were sites

selling tool hardware, home repair kits, and stuff like that.1  The examples
of blocked sites are listed on our page, so you can verify which sites from
the first 1,000 were recorded as blocked or not blocked.  We took screen

capture images of the sites being blocked, showing the message, ÒThis sitehas been blocked by this software.Ó  Obviously, screen capture is not
proof, because it is trivial to fake an image.  But there is a danger of people
1Bob Schloss asked whether the same host might be hosting both a pornographic site anda hardware site, and, because of the way in which domain names, IP addresses, and portnumbers are mapped, the hardware site ends up blocked along with the pornographic site.Susan Getgood said Cyber Patrol formerly contained a bug that allowed this to happenÑwhich Peacefire.org may have known about and used in designing the test.  She said the
technical problem involving hosted servers has been solved in all network versions used inschools and libraries.  Bennett Haselton noted that the company
Õs Web page specifically saidthat material does not have to be blocked because it shares an IP address with anotherblocked site; if it is true that IP address sharing is the cause of blocking, then this is a falseclaim.  The Web hosting issue has been around for several years and also applies to proxy

servers.  The BESS filtering system and the parental controls of America Online see the hostname, not the IP address, of the site that a user tries to access, so they should not have thisproblem.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BENNETT HASELTON41being suspicious that the study was done incorrectly, that there was a bugin our scripts to record the number of sites blocked, or maybe a site was
down at the time and we mistakenly entered it as being blocked.A rate of 17 errors out of the first 1,000 dot-com sites on the list ex-trapolated across the entire name space of 22 million dot-com sites yields
a figure of several hundred thousand incorrectly blocked sites in the dot-
com name space alone, not even counting dot-org and dot-net name

spaces.SurfWatchÕs error (i.e., false-positive) rate was 82 percent; it blocked42 sites incorrectly and 9 correctly.  Even though the same company

owned SurfWatch and Cyber Patrol by that time, the lists of sites they
blocked turned out to be different.  AOL
Õs Parental Controls, which sup-posedly uses Cyber PatrolÕs list, blocked fewer sites, possibly because itwas using an older version or because the list was frozen after they li-
censed it from Cyber Patrol.  When we found the Surf Watch number, we

knew that we had better get all the back-up documentation we could pos-
sibly get, because there was such a high error rate.  The reason that people

do not get these high error rates when casually testing the software is that
they test their favorite sites or sites that they know about, and errors in
popular sites already have been spotted and corrected.  They get an overly

good picture of how well the software works.People spend a certain amount of time on sites that everyone elsespends time on; however, people also spend time on sites that are less
popular.  Therefore, we are concerned about errors in the less popular

sites, even though we know that the popular sites contain fewer errors.
Moreover, the SurfWatch error rate is not okay if you are one of those 42
sites blocked incorrectly.  We plan to do a follow-up study in which we

look at the error rates in a sample of 1,000 sites returned from a search on
Google or Alta Vista, in which the more popular sites are pushed to the
top.  I expect that the error rate in that sample will be lower, because the

popular sites are weighted more heavily.This study measured only the percentage of blocked sites that aremistakesÑfalse positives.  It did not measure the percentage of porno-
graphic sites that are blocked, or the percentage of nonpornographic sites
that are not blocked.  If we use either of those numbers to judge a pro-

gram, then we run into a problem.  To determine how good the programs

are at blocking pornography, we first would have to find out how many
of the 1,000 dot-com sites are pornographic and then see how many are
blocked.We used the same 1,000 dot-com sites for every program except BESS
(a filter made by N2H2), which blocked 26 of 1,000 sites, 19 appropriately
and 7 by mistake.  We did the experiment first with SurfWatch, and that

one was published first last August.  We thought the other companies
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.42A CRITIQUE OF FILTERINGmight have heard about the first study and perhaps fixed their programsto block fewer sites incorrectly in that small 1,000 site sample.  It turned

out that none of them apparently had heard about it, because their error
rates were the same as beforeÑexcept for BESS.  In BESS, we observed a
clean break in the error rate pattern.  We took the first 2,000 dot-com sites,

and the first 1,000 contained no errors; but right after that, the error pat-
tern appeared.2  Technically, all they did was fix errors in their software,
so can we accuse them of cheating or not?  They removed errors from the

sample that they knew we were using, so we used the second set of 1,000
dot-com sites.Our conclusion from this study was that the people are not actuallychecking every site before they put it on a list.  If there are 42 errors in the

first 1,000 dot-com sites in a list, then there is no way of knowing how
many errors will occur throughout the entire space of 22 million.  This

does not necessarily mean there is a conspiracy at the highest levels in the
company.  The most innocent explanation may be that some intelligent,

lower-level employee whose job it was to find these sites may have writ-
ten a program that scoured these sites and added them to the list auto-
matically, without the person having necessarily having to look at them
first.  There is not necessarily an explanation for how someone could have

looked at one of these sites and determined that it was offensive.The borderline cases receive a lot of attention, because someone bringsthem to the companyÕs attention and they have debates about whether ornot the blocking is appropriate.  This happened with an animal rights

page that was blocked by Cyber Patrol, for example.  There was a discus-
sion about whether the depictions of victims of animal testing were ap-
propriate.  But the vast majority of blocked sites that have not been viewed

are moving targets, because if you raise the issue of these sites, then gen-
erally the company will fix the problems right away.  Then it becomes a

question of finding more blocked sites.  That was why we did the study

using 1,000 dot-com sites, so that, even if these specific errors were fixed,
the fact that we found them in this cross-section says something about the
number of errors that exist in the list as a whole.Sites can be blocked erroneously for reasons other than a lack of hu-man review.  In an incident that became the baseline in discussions about

the appropriateness of blocking software, Time magazine wrote an onlinearticle about CYBERSitterÕs blocking policies and the controversy over2David Forsyth suggested that the substantial difference in results between tests of 1,000sites and tests of 2,000 sites means that 1,000 sites is too small a set with which to conduct anexperiment like this.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BENNETT HASELTON43the blocking of a gay rights advocacy groupÕs Web pages.  CYBERSitter
put pathfinder.com, 
Time magazineÕs domain name, on its list.  The
magazineÕs Web site has an article written after CYBERSitter blocked thesite, which is good, because otherwise nobody would believe me.  At the

other end of the spectrum, I sent e-mail to Cyber Patrol saying that the
American Family Association (AFA) Web site, the home page of an ex-
tremely conservative organization, should be blocked as a hate site be-
cause of the amount of antigay rhetoric.  Because most programs that pub-

lish definitions of hate speech include discrimination based on race,
gender, or sexual orientation, Cyber Patrol agreed to block the site.  It is

still on the list today.This is an example of controversial blocking.  Many of Cyber PatrolÕscustomers would not block this type of site themselves.  Many filtering

companies, in their published definitions of hate speech, have painted
themselves into a corner by including discrimination based on race, gen-
der, and sexual orientation.  There are many extremely conservative reli-

gious organizations, reasonably well respected, that publish speech deni-
grating people based on sexual orientation.  It does not have to be hateful;

it just has to meet the discrimination criteria.  (
ÒI Hate Rudy GiulianiÓ is
not a hate site.)  Even though anti-gay hate speeches generally are consid-

ered politically incorrect, it is not so politically incorrect that many people
favor blocking it in a school environment, the way they might favor block-
ing the Ku Klux Klan Web site.We did an experiment a couple of months ago in which we nomi-nated some pages on Geocities and Tripod to be blocked by SurfWatch,
Cyber Patrol, Net Nanny, and some of the other companies, saying that
the quotes on the pages constituted antigay hate speech.  The quotes said
things like, ÒWe believe that homosexuality is evil, unhealthy, and im-moral and is disruptive to individuals and societies.Ó  The companies
agreed to block the pages.  Then we said we had created these pages, and

they consisted of nothing but quotes taken from the Focus on the Family
Web page or the Dr. Laura Web page.  We asked the companies if, to be

consistent, they also planned to block these sites as well.  So far, all the

companies have declined to do this.  Net Nanny was the only one that

responded, saying it would consider blocking the subpages of sites that
contained the material that was blocked when copied to the other page.
But about 6 months have passed since then, and the company still has not
done it.We concluded that an unspoken criterion for whether or not to blocka page is how much clout the organization that owns the page has and
whether it could incite a boycott against the filtering company.  If Dr.

Laura talked on her radio show about how Cyber Patrol or SurfWatch
blocked her Web site, this has the potential to alienate a good proportionTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.44A CRITIQUE OF FILTERINGof potential customers, as well as possibly leading to a situation in whichsomeone sues a local school or library for blocking access to political
speech.  If conservatives join forces to raise a legal challenge to speech

blocked in a school or library, then it becomes a larger problem.  Even

without that experiment, the point is still valid.  The companies say they

block speech that is discriminatory based on race, gender, or sexual orien-
tation.  Yet we have examples of unblocked sites run by large or well-

funded groups thatÑno reasonable person could disagreeÑmeet thatdefinition.3We recently published two reports about Web sites blocked by vari-ous programs.  These reports are linked to our main page.  One is Blind

Ballots, about candidates in the U.S. elections in 2000 whose Web sites
were blocked; these candidates included Democrats, Republicans, and one
Libertarian, blocked by BESS and Cyber Patrol.  The other report is Am-

nesty Intercepted, about Amnesty International Israel and other human-
rights-related Web pages blocked by programs such as SurfWatch, BESS,
Cyber Patrol, CYBERSitter, and some of the others.These reports were published just before the U.S. Congress passed alaw requiring schools and libraries to use blocking software if they re-
ceive federal funding.  I think the reports will still come in handy later as
the debate continues about the appropriateness of blocking software.  Just

because these reports did not stop passage of the law does not mean that
they will not be used as evidence in the court cases to be filed regarding
the legality of the law.There is a question about whether some of the more obvious mistakesmade by blocking software can be avoided if you disable the function that
dynamically examines pages as they are downloaded and blocks them
based on certain keywords.  If the list of blocked sites was assembled

using keyword searches, and if the pages were not necessarily reviewed
first, then the keyword blocking cannot be turned off if the software is
installed in an environment (such as a library) in which the administrator
wants to be extra careful about not blocking sites that should not be
blocked.3Susan Getgood said that Cyber Patrol reviewed the four pages that Peacefire.org createdand blocked them.  The company also reviewed the four source sites but decided not to put
them on the list.  Cyber Patrol does block afa.net and will continue to do so; AFA promotesa boycott of Disney because it offers same-sex partner benefits.  Getgood said that Cyber
Patrol is not afraid of an organizationÕs clout; she receives mail from the AFA every 2 monthsasking for a site re-review, which is done.  Bennett Haselton said that the AFA is less main-stream than other groups focusing on the family, such as the Family Research Council, whichhas a large lobbying group in Washington, D.C.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BENNETT HASELTON457.4CIRCUMVENTION OF BLOCKING SOFTWARE
Blocking software can be circumvented.  The easiest way is to find
pornography that is not blocked.  If you run a search, it is not difficult to

find unblocked sites.  Everyone who runs a search, with small changes in
the query, will get a completely different list of results, so you often find
at least one site that is not blocked.  You also can disable the software,

either by moving files around or by running programs to extract the pass-
word.  I have written some of these programs.  I wrote them because the

standards that people use to determine what is indecent and pornographic
strike me as arbitrary and silly.  I have never heard an explanation for

why a manÕs chest, but not a womanÕs chest, can be shown on TV.  The
companies that make the software are reinforcing those standards of de-
cency.Whether parents should have a right to filter is still a political issue.  I
think that rights are more abstract; it is difficult to talk about them.  I

wrote these programs because I believe that no harm is done if you see
something that your parents do not want you to see. All of us can think of
things that our parents did not want us to see when we were growing up.
All of us can think of examples of when we thought they were wrong, and
some of us still believe that they were wrong.People would not use a program like this just to find pornography,because it is trivially easier to find pornography than to disable the soft-
ware.  People use such a program if they need to access a specific site that

happens to be blocked.  This is either a borderline case, like a sex educa-

tion site, or something that you do not think should be blocked at all.
People have asked me whether I think nothing ever should be blocked.  I
usually give the example that, if I had a friend whom I thought was de-
pressed and likely to read something that might provoke suicide, then I
might go out of my way to try and stop him or her from reading that
material.  What I would not do is say, 
ÒIf theyÕre under 18, then I have theright to interfere, but if theyÕre over 18, I canÕt stop them.Ó  I think that
criterion is arbitrary and silly, and that itÕs a red herring people use toavoid thinking about the real censorship issues at stake.Anonymizer.com is a site that enables you to circumvent blockingsoftware.  You can connect to a third-party Web site through Anonymizer,

which has a policy of not disclosing who is being redirected to connect to
a site.  Anyone can circumvent blocking software by going to Anonymizer

and typing in the site that they want to access, because blocking software
looks at the first site you connect to, not the URL.  However, all blocking

software blocks Anonymizer.  We never make a big deal out of this, be-

cause it is not something worth complaining about. SafeWeb is a site that
does the same type of thing.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.46A CRITIQUE OF FILTERINGTranslator services also are blocked.  Babelfish.AltaVista.com is a site
where you can type in the URL of a foreign language site and the words
from that language will be translated to English, or vice versa.  The ratio-
nale behind blocking this site was that otherwise the pictures would come
through.  But Babelfish cannot be used to access images because it does

not modify the image tags.  (The images are loaded from the original loca-

tion because Babelfish does not want that data traffic.)  The text comes

through translated (poorly) but the images are blocked.  We published a

short piece on why this was probably an unnecessary overreaction on the
part of the blocking software, because the text is converted and the im-
ages are not accessible.The third example is Akamai.com, a content distribution service.  If
you sign up, then the images on your siteÑinstead of being loaded fromyour siteÑcan be loaded through AkamaiÕs server to save on your band-width costs.  It is a caching service with servers distributed around the

country.  A person who requests one of these images will get it directly

from the server closest to them.  It is a complex scheme that can shave

seconds off the load time of a page, so many people place a high value on
it.  The catch is that a loophole in the software allows you to put any URL

on the end of the page, and it will fetch the page through Akamai and
deliver it to you.4We pointed this out last August, but it still works.  Some people knew
about it before then; they had just published a page on how to use this
technique and how often it works to unblock a blocked site.  The problem

is that if the blocking software companies were to block it, they also would
block many banner ads served by Akamai.  It is used mostly for banner

ads to save on bandwidth costs.  Large sites, such as Yahoo, also use it to

serve their own images.Programs installed on a network are more difficult to circumvent bymoving files around or disabling the software locally, but you can circum-
vent them by finding unblocked pornography or using the Akamai trick.
In addition, if you have the cooperation of someone on the outside willing
to set up an Anonymizer-type program on a server, then you can go
through that program to access whatever you want.  This is becoming

easier to do, and people are starting to publish smaller and more light-
weight versions of Anonymizer that anyone can put on a Web page as a
secret source for them and their friends to use to tunnel through and ac-4Milo Medin emphasized that this is a bug, which should be fixed, as opposed to a genericissue.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BENNETT HASELTON47cess blocked sites.  We are working on one of those.  It does all kinds of
fancy things, such as scrambling the text on the source page and using
Java script code to unscramble the text and write it.  The censoring proxy
server cannot block the page unless it parses the Java script to figure out
what the actual text is.To summarize, two points are important.  First, a significant percent-age of blocked sites have not been reviewed by humans.  This situation

may be due to honest errors, such as IP address sharing or employees
whose eyes are glazing over.  But one way or another, significant amounts

of content are blocked that should not be.  Second, it is easy to circumvent

blocking software.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.488Authentication Technologies
Eddie ZeitlerI work in information security and would like to provide a businessperspective on the difficult questions this committee is addressing.  Secu-

rity implementations must resolve whether the measures are to protect
honest people from honest problems or are to provide ironclad solutions.
The answer makes a big difference in what we implement.  In addition,

we are chasing technology.  If I were trying to subvert a secure system, I

would wait for the next communications protocol to be implemented or
the next revision to the operating system to be installed.  We have unlim-
ited opportunities with computer systems to change whatever works to-
day into something that will not work tomorrow.8.1THE PROCESS OF IDENTIFICATION
I will approach identification and authentication from the perspectiveof the individual, that is, how a child or person is identified to a system.
We prove who we are in a number of ways, such as with a driverÕs license,
passport, badge, signature, or fingerprint.  When I provide an identifier to

you (or tell you who I am), that identifier needs to be authenticated.  In

the computer world, we use something you know (e.g., a password),
something you have (e.g., a credit card with a magnetic stripe), or some-
thing you are (e.g., a face, a fingerprint, a retinal scan) to authenticate an
identity.  Note that, usually, my possession of an identifier does not au-

thenticate my identity.Some authenticators are much more secure than others.  We all know
and love our four-digit personal identification number (PIN) and pass-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.EDDIE ZEITLER49word authenticators.  However, administrators of multigigabyte or
terabyte databases have password authenticators that are necessarily 20
or 30 characters long.  The authenticator, whether weak or strong, needs

to be verified.1  This is where we tend to run into trouble.  The process ofverifying the authenticator requires a trusted source.  In the example of a
driverÕs license, we trust the Department of Motor Vehicles (DMV).  The
picture on your driverÕs license is the authenticator.  To identify a personyou look at the picture on the license, you look at the person presenting it,
and say, ÒYes, I have authenticated that this is your license and I nowbelieve your identity.Ó  The reason this works is that I trust the license
because I trust the DMV.  If we did not trust the DMV licensing process,

then we would not use a license for identification.If you sign something to authenticate yourself, I have to verify thatsignature against a trusted copy of your signature.  The trusted copy I use

to verify it against gives me the confidence that you are who you say you
are.  For example, a bank
Õs trust is based on properly issued signaturecards.A token typically is not a sufficient authenticator by itself because itcan be passed aroundÑit is too mobile.  But if implanted permanently in
someoneÕs head, that token probably would have some validity.  If I have
a microchip embedded in my skull at birth by a National Security Agency
(NSA) surgeon, and the NSA verifies the chip when I walk through mag-
netic readers, then I would trust it.  But I cannot think of anything less

draconian that would suffice to make a token a valid independent au-
thenticator (we tend to use them in conjunction with other authenticators
such as PINs).In summary, the ability to identify a person depends on confidence.You have to have confidence in the authenticator, the issuer and issuing
process of the authenticator, the source of the information used to verify
the authenticator, and the process used to verify the authenticator.  A

system that identifies millions of people must have very high confidence.
For example, in the case of automated teller machine transactions, a very
small error rate in identification would make them unacceptable.  If you

do not have enormous confidence in the identification process, it is not1David Forsyth gave the following example:  He has a piece of paper given to him by
someone trusted that says, ÒDavid Forsyth knows the factors of this very long number.Ó He
gives someone else that piece of paper and tells the person these factors.  In the authentica-
tion, that person says, ÒWell, if you cannot trust the person who gave you the piece of paper,then the whole thing will not work.Ó Eddie Zeitler added that verification means that he
knows that the piece of paper actually came from the person from whom Forsyth said itcame.  He has verified the 
Òsignature.ÓTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.50AUTHENTICATION TECHNOLOGIESappropriate for use by a large population (including some who may betrying to defeat the system).8.2CHALLENGES AND SOLUTIONS
In the digital world today, technology is rarely the problem.  Technol-
ogy is changing so fast that, if a problem is not solved today, then it will
be solved next week.  Note that the opposite is also true.  A technology

that is secure today may not be secure tomorrow.  Today we have very

high confidence in digital signatures based on public key cryptography.2The digital signing processes are good.  We are able to identify, authenti-
cate, and verify a person and his or her age very easily using digital signa-
tures.  However, the authentication and verification processes are

problematic.  If they really worked, then the banking community, the bro-

kerage community, and the rest of the financial world would have imple-
mented them years ago.  We have the technology to create digital signa-

tures that we all trust, but we do not have an infrastructure in place that
makes this process workable.The private key that you use to create your digital signature will be1,000 to 2,000 characters long.  Where will you put it?  It has to be stored in

an automated device of some sort.  To date, smart tokens, or smart cards,
are the best answer.  Note that if I put my private key in my computer, we

would be authenticating the computer, not me.  What I want is something

that, wherever I am, can be plugged into any machine to identify me.  I do

not want it to identify the machine, because then others using that ma-
chine could also identify themselves as me if they knew how to use the
signing software, which, if they have possession of the machine, they can
figure out how to do.If we use cards, there must be universally compatible software, cardreaders, and signing processors.  I have been involved in writing Ameri-

can National Standards Institute (ANSI) standards for banking, and Òuni-versally compatibleÓ is more difficult to accomplish than it is to specify in
a standard.  We rarely achieve it.  In software today, the signature process

is fairly standard but the interfaces tend to be different.Another thought is that if I have my secret key in a personal device(smart card), then I can use that secret key to create a signature.  To au-
2A question was raised as to the applicability of zero-knowledge proofsÑproving some-thing to someone without revealing anything that you know.  But this has not proved to be
practical.  Some years ago, I (Zeitler) delved into zero-knowledge systems and found out
that, at least for the Bank of America, they did not make a lot of sense.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.EDDIE ZEITLER51thenticate the person using that card to the signing system, we typicallyrequire a PIN (usually four or six digits).  Remember, security is only as

good as its weakest link.  We have sophisticated software, complex tech-

nology, and great cryptography, and it all depends on a PIN.Then we need a trusted authority to verify the digital signature, some-one to say, ÒYes, that really is Ed ZeitlerÕs signature.Ó  Since it is a digital
signature, it must be something more than comparing one piece of paper
to another piece of paper.  You would go to the agency that issued the

secret key and ask, ÒIs this signature based on this personÕs secret key?ÓThe agency would respond.  Note that I have to trust that agency.
If I am the agency giving you a private key to use to create your signa-ture, I had better know to whom I have given it.  So far, the only way we

have found to accomplish this is in person.  That is how you get a driver
Õslicense.  Banks want some verifiable form of identification from you in

their branch office.  In the financial world, there are many stipulations

that you know your customer.  However, in the online world, banks and

brokerage firms do not strongly verify the identity of their customers any-
more; they have necessarily resorted to less secure verification processes.A very secure process and database are necessary to assign crypto-graphic keys.  The people who assign those keys had better have them

locked up tight and require strong authentication of a person requesting
them.  A digital signature cannot be created with a four-digit PIN for

authentication.  If we do not have a lot of trust in this process, it becomes
a house of cards that comes apart, regardless of the zippy technology used.Today we have digital signature software on all browsers, which isgreat.  We were all applauding when that happened.  But we still do not

have card readers.  We do not have a practical way to issue private keys to

millions of people or a practical way to store those keys.  The NSA and

National Institute of Standards and Technology (NIST) have ventured into

this area and have not been successful.We do not have a trusted party to issue cryptographic keys and verifydigital signatures at the national level.  U.S. government intelligence agen-

cies would not be satisfactory to the private sector. The trusted party does
not have to be a government agency, but what other organization has the
presence?  When we started developing public key cryptography, we

talked about the U.S. Postal Service issuing keys.  There are also liability

issues.  For example, if the Post Office managed the keys and a major

break-in occurred and the whole country lost the ability to process public
keys (or digital signatures), whom would you sue?  On the other hand, if
it were a private concern, that probably would be the end of that private
concern.  What type of liability do companies such as Verisign, which

issues cryptographic keys to the public, have?  They have been address-

ing this issue for years and are comfortable that they have a workableTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.52AUTHENTICATION TECHNOLOGIESsolution.  But I am not comfortable with that, because if Verisign
Õs datacenters were to blow up, people would have little recourse.Despite the security flaws, electronic banking works fairly well.  I
worked in a retail company as the chief technology officer years ago, and
I moved to a bank from there.  I was amazed to find that the retail data-

bases and systems had much more security than the banking systems at
that time.  Interbank wire transfers and the like were done in a rudimen-

tary fashion.  Anyone who knew the system could break it or cause dam-
age.  But the reality is that there was very little loss.  There were reciprocal

agreements between banks.  If I sent you a $100 million transfer and real-
ize this afternoon that, oops, it was fraudulent, then the receiving bank
will give it back, in most cases.  In banking, when you get to the top, only

a few people are necessary to make a phone call to gain agreement that,
ÒYes, weÕll take care of that.Ó  Although real attacks have been made
against our systems, if you want to steal a million dollars, it is still much
easier to make friends with the branch manager than to figure out how to
break into the automated money transfer systems.  Security technology

has tended to stay a step ahead of what is practical in the world of finan-
cial fraud.To get back to the beginning of this talk, the definition of ÒgoodenoughÓ security depends on the problem to be solved
Ñfour-digit PINsmay be sufficient in many cases.  However, for the purpose of this study,
limiting the solution to school or public library computers is vastly differ-
ent from the problem of identifying a 9-year-old using any computer to
access the Web.  Most of the computers to which children have access

probably will not be run by federal, state, or local governments.
3  A strong
identification process will be required.3Bob Schloss suggested that there are more incentives for people to steal $100 million or toget the right to launch a nuclear weapon than there are for a 9-year-old to use a schoolcomputer to see something that his teacher does not want him or her to see.  Ordinarily, the
school district gives the smart card to the teachers, who use it to set filters.  You cannot forge

the PIN.  But will one kid who is a computer genius write a device driver that he loads intothe computer so that it steals the secret number?  Milo Medin suggested wryly that he could
simply download it from Peacefire.org.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.539Infrastructure for Age Verification
Fred CottonMy background is predominantly law enforcement, so I come to thisissue having tried to clean up the results of many societal problems, and I
see what is going on in the streets.  I agree with Eddie Zeitler about au-

thentication and verification.  You have to watch it work in the real world

with driverÕs licenses.  You can book an individual into the county jail and

rely on fingerprint information that does not come back to the right per-
son.  You will face these problems anytime you try to superimpose au-

thentication of age onto the real world.9.1THE REAL WORLD VERSUS THE INTERNET
How, and to what extent, is interaction with a human being needed tovalidate identity?  Who will validate the validator?  Who is it that you

trust to say who somebody else is?  That level of trust does not exist in any

level of government these days.  What level of confidence is needed for

the accuracy of an assertion of age to pass the legal requirements?  The

law will define that for you.  If you foul it up, you will know.  Just as with

any other problem in society throughout history, the lawyers will solve it.
They will find the tort in the problem, find the person or persons respon-
sible for the tortÑeither directly or vicariouslyÑand then sue their shorts
off.  The necessary level of confidence will be defined rapidly as soon as

the legal community determines that there is money to be made from it.What infrastructure is needed to support age checks outside the
Internet?  We have an existing infrastructure for dealing with credit cards,

fingerprints, biometrics, chips in your head, and other things that can beTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.54INFRASTRUCTURE FOR AGE VERIFICATIONused today.  But these things are cost prohibitive and not widely dissemi-
nated.  To have any kind of authentication process, it has to be globally

disseminated; otherwise, there is no standardization.  The problem is dis-

semination.  Credit cards are great in the United States but not in the

middle of Africa and other places around the global Internet where the
infrastructure does not exist.  In Third World countries that are develop-

ing sites that deal with child pornography and child exploitation, imple-
menting online authentication and age verification technologies is a whole
different business.Cops dealing with problems online tell us that the problem is that ourlaws only extend as far as our borders, and, historically, our ability to
regulate or influence things extends only as far as our laws.  Our laws are

based on how much territory we can hold with a standing army.  This has

no application on the global Internet.  It is a totally new environment
Ñabrave new world.  There is little we can do other than talk about it, be-

cause nobody owns the Internet and nobody runs it.  Nobody has any say

over it other than the people who use it.  It is truly a democratic society.

When the people who use the Internet get tired enough of something,
they will do something about it, independent of government.Has the Internet environment changed the necessary infrastructure?Obviously, we cannot superimpose the existing structure on the Internet,
because of its global and nebulous nature.  If you are going to validate

identification online, then it has to be standardized to some extent.  If you

are validated through ABC signature company, and I am a retail mer-
chant who subscribes to XYZ but not ABC, does that mean that you do not
get to buy from me?  This is probably not going to work well, and some-

thing will need to be done about standardization.What are the costs to the user and to the government?  Who will main-
tain the database of validation?  This is a huge responsibility, a huge cost,

and a huge security risk.  If you blow that one, you are guaranteed to get

the legal community involved.How reliable is the technology?  It is reliable today, but tomorrow
brilliant little Johnny in the class will figure it out.  It only takes one little

Johnny to figure it out, and then he automates it and gives it to all the
others.  We have seen this in computer security for years.  It does not

require much skill to hack.  All you have to do is download the tools that
somebody who had the skills to write them made available.  It is a point

and shoot operation.Other things that we do in the real world in age verification may ormay not have application here.  A driver
Õs license is an official ID becausewe have an official government entity.  It is well funded and well staffed,

and it requires that you show up to prove who you are before you get the
token or identification.  It is very difficult to do that on the Internet.  I can
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.FRED COTTON55apply for a credit card through the mail, and I call the issuing company toactivate it, and no one there ever actually sees my face.  But credit card

fraud is easy to commit.  People just throw those forms in the garbage.  I

could go through your garbage and pick up those applications, fill them
out, put in a change of address, and charge things in your name.  This

happens daily.  Identity theft is huge.  Once you are in that particular

loop, getting out of it is next to impossible.Biometric technologies and fingerprint scans are possible, but it is costprohibitive for both the user and authentication organization at this time.
In addition, the initial validation is always a problem with anything that
you superimpose here. Tokens are too mobile.  We see that with identities

now.  We have juveniles buying alcohol over the counter with false IDs,

which are not difficult to forge.Historically, law enforcement protection is a three-legged triangle.  It
involves enforcement, education, and prevention.  Of the three, education

is probably the cheapest.  This is where you get the most bang for the

buck.  You simply get people to change their ways by telling them that

something is not right, and that it is not in their best interests.  So far we

have not been very successful with things like narcotics.  If we could get

people to stop wanting children to access pornography on the Internet,
then it would go away.That leaves you with the other two legs of the triangle.  Prevention
involves giving parents and teachers some tools that they can use to try to
stem the flow.  The tools will not stop it but will give them some control

over their own part of the environment.  The third aspect is enforcement.
We find the people who are bringing this grief on us and we bring grief on
them, or we find the biggest offenders and put their pelts on the fence as a
warning to others.  Historically, that is what enforcement is about.  We get

them to the point where they do not know if they will be next, and they
keep their heads down.  If they all decide to do bad things at once, there is

no law enforcement agency in the world that can prevent it.  But we can

keep them on their toes enough that they will think twice before they do
it.Everything I have talked about so far deals with the Web, the leastoffensive of the content problems.  How does any of this technology affect

e-mail or Usenet?  The worst offender is Internet relay chat (IRC), when

kids are involved in that arena.  I train 30 task forces around the country

to do nothing but go after online predators, people who will get on an
airplane and go find a child for sex.  They spend months and months

cultivating that situation.  You would not believe the astronomical num-

bers involved.  In that type of environment, all of the screening software

and age verification do no good.  Technology will not solve this particular
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.56INFRASTRUCTURE FOR AGE VERIFICATIONproblem.  Right now, the only thing that is having an effect is enforce-
ment.  We are at least identifying the offenders and taking them out of

circulation as fast as we canÑsurgically removing them from society bywhatever means is currently socially acceptable.If you could keep kids off e-mail and Internet Relay Chat1Ñthat is, ifkids accessed the Internet in a way that worked only through the Web,
but portedÑthen it would eliminate access to children for most of thesepreferential sexual offenders.  But you would also eliminate a lot of things

that kids use the Internet for; it would be like keeping kids out of the park
or off the telephone.  IRC has replaced the telephone after school, and that

global circle of friends is a strong social draw.  For latchkey kids after

school, this is their way of communicating nowadays. With Usenet, if they
want to surf for porn, then they will find a public news server and pull off
whatever they want.  Screening does little about that, particularly with all

the things that are mislabeled.9.2SOLUTIONS
Any successful effort to keep pornography away from children willhave to draw from all available solutions; you need a bit of everything to
make it work.  No one model will be successful by itself, but, when com-

bined, they likely will have some impact.  The degree of impact will de-

pend on the social acceptance of this effort in the long run.  The available
models include the following:¥Age verification and validation is a positive ID model.  Before I can
get in somewhere, I must prove that I am an adult.  This lends itself to the

use of tokens, or what I have and what I know.  But this leaves us with the

problems mentioned earlier concerning who controls that database and
who keeps track of that information.¥The supervision model does nothing at the technological level, butrather has parents supervise kids online.  If you put your kids online, then

you do not throw them into an electronic pool hall without supervision.
You move the computer out into the family room; you do not let kids sit
in the back room and do these things all by themselves.  Unfortunately,

the reality is that most parents do not take the time to do this.¥The software model involves the screening softwareÑNet Nanny,Cyber Patrol, and the others.  With the false positives and so on, this is
1Milo Medin said that he could build a system to do this; the question is whether anyonewould want such a product.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.FRED COTTON57problematic, but, when combined with the two approaches mentionedabove, it may offer some reassurance.¥The law enforcement model says we go out there and increase ourpresence online, so it keeps the predatorsÕ heads down and keeps them
from doing what we want to prevent.  They will think twice before engag-

ing someone online, for fear that they are engaging me.  This keeps them

guessing.  This is a fear model.
¥The intervention model says we identify the people causing theproblem and enlist the aid of the cyber-network neighborhood and crime
prevention types so that people who see this activity do not ignore it.
They step in and do something about itÑthey report it, and somethinghappens as a result.  This works with burglaries and territorial crimes.

We have to rely on the community to tell us how things are going.¥The education model involves improving education to the pointwhere people see that something is wrong and change their behavior.
When you change the behavior pattern, it no longer will be socially ac-
ceptable or tolerated by the majority of society.We also need to remove roadblocks in law enforcement that severelylimit what I can do online. The rules currently applied to online situations
were written for telephones, not the Internet.  We work within very nar-

row parameters.  For example, a recent case in the Ninth Circuit dealt

with a supervisor going onto a password-protected Web page under the
auspices of a pilot during a pilotÕs strike.  The Ninth Circuit said that was
not right.  If I am a law enforcement officer working undercover, what

does that mean for me when I try to access a child pornography Web site?
They do not think about the ramifications and how it affects our ability to
function online.I cannot just take your computer, go through it, and find out what ison it.  I have to write a search warrant, convince a judge that I have prob-

able cause to believe that what I seek will be there, and show proof of that
before someone will give me a search warrant.  This is wise, of course.  We

have these protocols and procedures because you do not want us running
amuck and grabbing everything.  However, at some point you have to

remove some roadblocks if we are to address new technologies based on
laws for old technology.  We have to remove some of roadblocks so that

we can become effective; but we also have to keep parameters in place to
keep it from getting out of hand.  There is a balance.
The roadblocks have not been collected and presented in an article orpublication.  They are buried in case law
Ñnot even codified law.  They
are buried in the decisions of the U.S. Supreme Court, district courts, andTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.58INFRASTRUCTURE FOR AGE VERIFICATIONcourts of appeal, in a variety of cases, and in civil lawsuits.2  Agencies are
less concerned about protecting you as a citizen than about getting sued.
But we have advocates for change.  The U.S. Department of Justice has the

tools to do that.The first thing I would do is to protect children online.  We have to
find the most egregious cases out there of the providers.  I would identify

who is causing the problem.  Second, if I cannot arrest that person for a

violation of law, then I would sic a whole battery of attorneys and law
firms on them for a tort violation, basically a violation of my rights.  At

some point, they will get a clue that this is not acceptable behavior.  All of

this has to be done within the parameters of the law, but the people caus-
ing this problem have to fix the problem.  They are causing a problem for

the rest of society and they will have to own up to their part and face the
consequences.Criminal prosecution is generally the least effective approach.  Usingthe law is always available; the pen is mightier than the mouth.  But the

bottom line is, you need to change behaviors.  There is no law west of the

modem.  Look at the development and rapid growth of the Internet, and

compare it to the westward expansion of this country in the early 1800s.
The same type of thing is happening.Behavioral changes will be required on both sides.  It will require
different behavior on the part of people being victimized now.  They need

to realize that they cannot continue to do these things online without the
potential of being a victim.  The other behavior we have to change is that

of people who look at the Internet as the wild and woolly west, who do
not care what they do to anyone else online. You have to change the be-
havior of children who use the Internet at some point and, by default,
change their parentsÕ behavior.  I am not picking on any one group.  Soci-
ety as a whole will have to look at this problem and say, ÒDo we reallywant this to continue?Ó3The group causing the biggest problem right now are the offenders,2Dick Thornburgh said that someone should read all the cases, collect them, and developa strong argument for a remedy.3Eddie Zeitler said that, as long as society keeps developing new technologies, these prob-lems will arise.  A problem is created when someone puts digitized music in a file and then
says you cannot copy it.  You cannot commercialize it in the United States, but you can go
somewhere else where there is no law against this.  No one can tell you that you cannot
make copies, because you can, and no one can tell you that you cannot use the Internet,
because you can.  Fred Cotton noted that, if you send a picture of women without veils to
Saudi Arabia, you have sent pornography.  In other words, there is also a nebulous commu-
nity standards issue.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.FRED COTTON59the ones sending material to kids unsolicited, targeting kids, going afterthem in a planned and concerted manner.  That is the first behavior to be
changed.  They need to wise up and realize that this is not appropriate or

face the consequences, because what they are doing is a violation of the
law.  Sending 12- or 13-year-old kids horrific graphic images is unaccept-
able to me because the kids do not get a choice.  If you tell them, 
ÒHey, donot go over there, because there is bad stuff,Ó and they stay away, then it
is fine.  But keep the bad stuff over there.
You cannot dry up the supply by somehow taking the money out ofit.  The sexual predator is not motivated by money but rather by access to

children.  This cannot be managed like the banking model, in which a

concerted effort is made in multiple areas that largely prevents a problem.
There are few predators within the banking community, and we tend to
get our wagons in a circle when under attackÑwe control where moneygoes electronically.  On the Internet, nobody controls the pornography

supply.  You have a widely dispersed supply and a widely dispersed de-
mand, with no central point at which you can install controls.9.3THE EXTENT OF THE PROBLEM
When talking about protecting children online, it makes no differencewhether it is protection from a sexual predator or a pornographer,4 be-
cause predators use pornography as a tool to lower the inhibitions of chil-
dren.  I have seen them with cartoons of Homer Simpson and Fred

Flintstone, telling little kids,  ÒSee, Wilma thinks itÕs okay.Ó  There is no
difference; pornography is still being put out there and accessed by chil-
dren.  If children are hooked into it and able to go to another site and feed

that paraphilia (i.e., unusual sexual preference), then it simply serves to
lower the inhibitions further.  (Some sexual preferences are illegal; some

are not.  Child pornography paraphilia happens to be illegal.)
This is like watching violence on TV; eventually, you get numb to it.Most law enforcement officers see the same thing.  Finding a dead body

on the street is not horrific to me any longer; I have seen too many of
them.  To the average citizen, it is absolutely horrific, but I have been

desensitized to it over the last 27 years.  This is sad to say, but it is true of4Marilyn Mason asked whether there are two different, but related, aspects to the pornog-raphy issue.  On the one hand, there are sexual predators who are trying to make contact

with juvenilesÑthe scariest part.  On the other hand, there are creators of sexually explicit
material who are trying to make a buck by selling it, presumably to adults, but sometimesthey solicit children as well.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.60INFRASTRUCTURE FOR AGE VERIFICATIONmany people who are exposed, over and over again, to things that societydoes not wish to deal with.  Pornography is one of those things.
Just because someone possesses or distributes child pornography does
not necessarily make them a predator.  But every single predator whom I
have dealt with in my 27 years in law enforcement had child pornogra-
phyÑthey possessed it, collected it, and used it to entice someone.  Preda-
tors also use sexually explicit material that is not illegal.  The process does

not take place overnight.  My investigators work on these cases for

months.  A predator meets a child in a chat room and becomes a friend
Ñtalking about things that they cannot talk about with their parents, lower-
ing their inhibitions.  The whole object is to get physical access to the kid.

These are the people whom I would go after first, because they are the
most dangerous.  But there is also a group of them who have set up an

industry that supports this paraphilia.  When they cannot get access to

children, they get access to child pornography, because it is the next best
thing.David Finkelhor5 put together a study for the Office of Juvenile Jus-
tice and Delinquency Prevention, published early this year.  It was an

empirical study of young teenagers online and their contact with sexual
predators.  Of young girls in the 14-year-old age range that were online,

90 percent of those interviewed had been contacted with unwanted sexual

advances.  Several went on to further levels.  They were interviewed in

control groups, too.  The numbers were shocking, amazing.
How much of this is unique to the Internet, and how much is justreflective of society in general?  For about the first half of my 27 years, I

could count on my hands the number of child sexual abuse cases that I
handled.  With the advent of the Internet, it has grown exponentially.  I

handled 10 to 15 cases in 1989, the first year that I realized there was a
problem.  When we started looking at the agencies dealing with it, every-

one thought that they were the only one.  A segment of our society has

this paraphilia or would like to explore it or act it out.  They use the

Internet as the mask they hide behind.  They can play whatever persona

they want online, because there is no validation of who they are.I think we have had child sexual offenders in our society from thebeginning, but they used to have to go to extraordinary measures to get
access to children.  The Internet has made it easy for them.  Those who

may never have thought of acting out in the real world now have no com-5Finkelhor, of the University of New Hampshire, testified at the committeeÕs first meeting,in July 2000.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.FRED COTTON61punction about doing it on the Internet.  It is the borderline cases that arecoming out now; this is part of the problem.There is also a phenomenon called validation.  If you are into sexually
assaulting children, then you are universally disdained in almost every
society in the world.  You are the lowest form of bottom feeder; if you go
to prison, murderers will kill you because you went after a kid.  There-

fore, when child sexual offenders have the ability to get together in affin-
ity groups they say, ÒOh, IÕm not the only one.  I thought I was the only
one, but there are thousands of me out here.  And now we can validate it.

We can exchange information about children and target children online.
We can find out where they live and go and meet with them.  This is a

wonderful tool.ÓJust because they talk to one another does not make them easier tocatch.  It has made for an interesting enforcement environment, but we

still have roadblocks that prevent us from catching them.  Their Internet

communications are in transit, so, technically, we are using forms of wire
intercepts.  The law was written for the old days of wiretapping the tele-

phone; it does not apply to an Internet chat room.  The courts have not

defined this well enough.  They have not told us what we can and cannot

do as far as this new communications medium.  As a result, law enforce-

ment is more concerned about getting sued over these types of things.
We have to be careful how we proceed.But these people are coming out in droves.  The numbers are astro-
nomical; I have never seen anything like it, and I see no end to it.  Chil-

dren are at risk.  Can the risk be managed?  Yes, if we implement a variety

of different approaches, not just technology, we may be able to manage or
limit that risk.  But can we eliminate it?  Absolutely not.  Can we control

the global Internet?  Probably not.  Can we change how people use the

Internet through education, prevention, and enforcement?  Probably.
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.6210Automated PolicyPreference Negotiation
Deirdre MulliganI worked for a long time on the Platform for Privacy Preferences (P3P),which gives parents some control over the data collection practices at Web
sites visited by their children.  There are instances in which children dis-

close information about themselves that can be used to contact and com-
municate with them.  P3P has no application in the context of limiting

childrenÕs access to pornography and other content that might be consid-
ered inappropriate.P3P is a project of the World Wide Web Consortium (which also de-veloped the Platform for Internet Content Selection (PICS)), which en-
ables Web sites to express privacy practices in a standard format.  This

means that a Web site can make an extensible mark-up language (XML)
statement about how it uses personal data.The basic functionality of P3P is as follows.  Say that a Web site col-
lects information such as name, address, and credit card number for the
purchase of goods, or it uses clickstream data (i.e., the data left behind
when surfing a Web page) to target or tailor information on the Web site
to your interests.  On the client site, either through a browser or some

plug-in to a browser, P3P allows individuals to set parameters for the
types of Web sites their kids can visit based on the siteÕs data collection
practices.  For example, a child might try to enter a Web site that collects

data from children and sells itÑwhich is generally illegal in this country
without parental consent, under the ChildrenÕs Online Privacy Protection
Act (COPPA).1  The browser could be set up either to limit access to Web
1COPPA, which regulates the collection of personal information from children under age13, was signed into law in 1998 and went into effect in 2000.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DEIRDRE MULLIGAN63sites that engage in that type of data collection or to supply a prompt,notifying the child that ÒThis Web site collects data that your parents havedecided you should not disclose.ÓSeveral products incorporating P3P are being developed.  Most are
browser plug-ins.  Microsoft will have some P3P functionality in the next
generation of Internet Explorer.  As with other Web standards, P3P can be

combined with other tools and you can plug in certain things, such as
trust symbols.  You can envision a digital certificate built as an add-on to

a P3P application.  But the P3P specification itself deals with data collec-

tion, not access to different types of content.The adoption of P3P had little to do with COPPA.   Tim Berners-Lee
and I gave the first public presentation on P3P at a Federal Trade Com-
mission (FTC) meeting in 1995, several years before the enactment of
COPPA.  The technology was not specifically designed to deal with

childrenÕs privacy issues; rather, it was designed to address the need forWeb sites to be up front about how they handle data, and the need to
implement, on the clientÕs side, tools for individuals to make informeddecisions about the disclosure of personal information without having to
read all the fine print. P3P is an effort to use the interactivity of the Web to
get around some of the barriers and costs associated with privacy protec-
tion in the offline world.The notion of rating is not part of the P3P specification.  There is a
standard way of talking in a descriptive fashion, which is different from a
normative fashion, about privacy.  A P3P statement allows a Web site to

make descriptive statementsÑnot that their privacy policy is good, bad,or the best, but simply, ÒWe collect this type of information, and we dothis with it.Ó  Clearly, someone could build a program that makes a judg-
ment.  For example, a Web site could say,  
ÒWe collect everything that wepossibly can about you and sell it to everyone in the world.Ó  Someone
could develop a tool that says that statement equals a bad privacy policy.
That tool, in effect, could make a rating based on the descriptive state-
ments.In many ways, PICS was an effort to provide the capability to makedescriptive statements about content.  P3P does not provide anything new

or special in that area.  But descriptive information is not necessarily what

people are looking for in the content context; they are looking for norma-
tive judgments about what is appropriate, and this is much more difficult
to build into a specification.  There are constitutional, cultural, and hege-

mony reasons that make such decisions suspect.  It is not as straightfor-

ward or factual as statements about what data are collected and how they
are used.Whether P3P leads to more negotiation and customization of contentTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.64AUTOMATED POLICY PREFERENCE NEGOTIATIONdelivery2 will depend on the implementations.  There are a wide variety
of implementation styles, and it is unclear how the products will work.
Part of it will be driven by consumer demand.  Survey after survey has

documented enormous public concern with privacy and a real anxiety
about disclosing personal information, because people feel that Web sites
are not forthright about what they do with data.A tool that allows people to gain better knowledge about how thedata are used certainly may allow more personalization.  Some people

will choose personalization because they are comfortable having certain
types of data collected; if data collection and the personalization it en-
ables are done with the individualÕs consent, it will advance privacy pro-tection.  If a Web site offers the news or sports scores, you might be com-

fortable telling it which state or county you live in, or your zip code,
because the site provides a service that you think is worthwhile.  But to-

day you might be anxious about what the site does with the data.  If there

were a technical platform that allowed you to know ahead of time that
only things you were comfortable with would be done with your data,
then certainly it might facilitate personalization.  But it would be person-

alization based on your privacy concerns and your consent to the data
collection.With regard to the truth of a siteÕs privacy statements, the question ofbad actors is one that we have in every context.  There is nothing about

P3P that provides enforcement, but it does provide for some transpar-
ency, which could facilitate enforcement.  In this country, people who say

something in commerce that is designed to inform consumers run the risk
of an enforcement action by the FTC or a state attorney general if they fail
to do what theyÕve said.  In other countries, there are similar laws prohib-
iting deceptive trade practices, and, in addition, many countries have laws
that require businesses to adhere to a set of fair information practices de-
signed to protect privacy.  Collaborative filtering
Ña process that auto-mates the process of Òword-of-mouthÓ recommendations by developing
responses to search queries based on the likes and dislikes of others who
share interests, buying habits, or another trait with the searcherÑis inde-pendent of P3P. I have not seen a discussion of its applicability in the
privacy area.2Bob Schloss gave the hypothetical example of Sports Illustrated warning that some of its
content shows people in skimpy bathing suits, and a user agent (or client) saying it does notwant to see sites like this.  Sports Illustrated could offer to present a subset of its content
honoring the request.  But why would the magazine go through such complex program-

ming if only 10 people had user agents that could negotiate?  To what extent would there be
negotiations in which a site would either collect data or provide a subset of its functionwithout collecting data?Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.6511Digital Rights Management Technology
John BlumenthalI am a security architect specializing in digital rights management(DRM) systems.  I am engaged now in the music and publishing space,

but I have a history of looking at rights management in terms of digital
products and messaging, e-mail in particular, dealing with issues such as
the unauthorized forwarding of e-mails in the sense of how conversations
are considered under copyright law and the ability to abuse conversa-
tions.  I have both a technological hacker perspective and a policy ap-

proach that includes a focus on risk management in terms of how to con-
trol content.11.1TECHNOLOGY AND POLICY CONSTRAINTS
How do we prevent particular types of content floating around on theInternet from reaching certain classes of users?  We would like to imple-

ment a technological restriction.  How do we implement these controls on

contents to contain propagation?  The Internet is all about propagation.

This question raises not only the issue of viewing but also the issue of
ownership and super-distribution or forwarding.  On the policy and legal

side, can this be implemented in a legal structure once you achieve this
ÒnirvanaÓ of a universal technological solution?Is this really any different from the MP3 debate?  There may be social
or psychological issues as to why people consume and propagate this type
of content, but fundamentally, to look at the MP3 debate is to stare in the
face of the problem.  The current crisis in the music industry is that this
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.66DIGITAL RIGHTS MANAGEMENT TECHNOLOGYformat, MP3, which compresses and renders audio,1 is not associated with
any type of use controls.  Napster posts these files, or references to them,

such that users can send and swap the files without any control, effec-
tively undermining the music distribution channel, typically compact disk
with read-only memory (CD-ROM).  The publishers chose not to encrypt
the data on CDs, for cost and other reasons.2  Music on a CD is stored
digitally in a totally unencrypted way, which is why you can make copies
to play in your car.There is no way to control this problem technologically; we can onlycontinue to raise the bar, effectively placing us in the domain of risk man-
agement.  This is the core problem, which I refer to here as the trusted

client security fallacy.  I have complete ownership of this device, literally,

physically, and in every aspect, when it is on a network.  This means that,

with the proper tools, I can capture that content no matter what type of
controls you place on me.  There are people within @stake who are ex-

perts in reverse engineering, which allows them to unlock anything that
has been encrypted.  If we attempt a technological solution, then there

will be ways to circumvent it, which then will propagate and become
much easier for the masses to use.I believe that policy drives technology in this problem, simply be-cause technology does not offer a complete solution.  The only way to

attempt a solution to mitigate risk is to adopt a hybrid approach, mixing
technology and policy.  Whatever system you come up with in the digital

rights space must be sensitive to these policy constraints.  You have to

distinguish the type of content in attempting to invoke rights on it and
control it.  This is a fundamental premise of the way a DRM system is

designed and applied.3These policy constraints create the archenemy of security and contentcontrolÑsystem complexities.  There are serious economic consequences
for the technology industry in general, because you are imposing on the
end user experience.  You are disrupting and removing things, such as

free use of and access to information, that I have become accustomed to
using on the Internet.  Decisions regarding how to implement the policy

and technology will affect this industry.1To render means to convert a format into a human-consumable elementÑdisplaying dataas images, playing data as sound, or streaming data as video.2Milo Medin pointed out that the music publishers themselves created the unencryptedformat in which CDs are published, effectively creating this problem.  He said we cannot
expect people to use a digital management format that offers them fewer capabilities than
the native format in which the material originally was published.3References for DRM and client-side controls can be found at <http://www.intertrust.com>, <http://www.vyou.com>, and <http://www.oracle.com>.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.JOHN BLUMENTHAL67The policy constraints causing these problems are privacy, the FirstAmendment and free speech, censorship, the legal jurisdiction issue, rat-
ing systems (which will become difficult to implement and maintain),
copyright and fair use, and compliance and enforcement.  These are all

difficult issues.11.2DESIGNING A SOLUTION TO FIT THE CONSTRAINTS
This is how I would approach designing a system that conforms tothe policy constraints.  Some of this is very technical.  First, we have to

design a system to operate across all the consuming applications:  chat, e-

mail, Web browsers, file transfer protocol, and so on.  This is a massive

infrastructure.  Then, given all of the policy constraints, how can we au-

thenticate ageÑto determine if a user is 18Ñand only age without stomp-ing on privacy issues?  The only thing that I could come up with is bio-

metric authentication.  A biometric approach can detect who you are.  I

have heard that devices exist that can take a biometric measurement and
determine the age of that measurement, but I do not believe it.4The collector of the information is responsible for enforcing the pri-vacy issues.  If you are willing to go deeper into the privacy issue and

maybe involve so-called trusted third parties, porn sites often perform
age authentication through the submission of a credit card number.  Thus,

if you release some of the constraints, you get more of what you want to
achieve.  But the problem of hacking is inescapable.
5  Gaining access to
pornÑsomething forbiddenÑis probably one of the most deep-rootedpsychological motivations for becoming a hacker in the early stages.  Talk

to any hacker; if there is lurid content, then they want access to it.  Music

probably brings them into the same psychological realm.The bigger issue is, now that you provide access, do you permit propa-gation?  In other words, is the authorized user allowed only to view the

content?  This issue has more to do with content consumption than con-
4Herb Lin said that he does not believe this; his 6-year-old daughter just had a bone-agescan, which said she is three-and-a-half.  Milo Medin suggested that a blood test probably

could determine age.  David Forsyth suggested counting the rings in a section of a long
bone.  Herb Lin noted that, to be useful legally, a biometric would have to change suddenly
in a significant way between age 17 years and 364 days and age 18 years and 1 day.  Milo

Medin countered that a real-world system need not be accurate to within 1 day.  Gail
Pritchard summed up the problem by saying, ÒThe minute I turn 18, I want access.Ó  She
noted that there are other means for checking a personÕs birthday.5David Forsyth pointed out the conundrum of Òanything I own, I can attack.Ó In other
words, if a parent has an age verification system and a technically creative offspring, thenthe system is essentially meaningless.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.68DIGITAL RIGHTS MANAGEMENT TECHNOLOGYtent access.  You want to prevent the propagation of certain types of
Internet material.  There is a subtle, more hidden issue here.  If content is

provided to someone who is authorized and authenticated, and it is ren-
dered, then you are heavily into DRM.  Should the user be permitted to

propagate that material to another party such that it is rendered, in effect,
in an uncontrolled fashion?  The system needs to consider both consump-

tion and propagation issues to provide a whole solution.In the system that I am designing, I will install a virtual V-chip.  Some
of you may be familiar with the V-chip Initiative,6 which led to many
debates and various laws.  As of January 2000, new television sets have

this capability.  There is a twin effort in the V-chip analogy, in which the

so-called client side (i.e., the television, desktop) and the publisher side
(i.e., the broadcasters) are driven by policy makers not only to implement
this bar to maintain risk on the client rendering side, but also to come up
with a rating system so that the V-chip can look at a stream of art or video
and say whether it is inappropriate content.  The parents have set up this
virtual ratings wall to prevent the rendering of, and access to, the content.As applied to television, the V-chip impedes the user experience soonerously that people do not use it.  Instead, they police the use of televi-
sion by simply physically being in their childrenÕs presenceÑor they donot police it at all.7  A lot of work would need to be done with both the
6See <http://www.cep.org/vchip.html>, <http://www.fcc.gov/vchip.html>, <http://www.webkeys.com>.7Janet Schofield said that parents typically do not police their childrenÕs television usesystematically.  Linda Hodge said that parents do not trust the filtering system because the

broadcasters themselves set V-chip ratings, which are voluntary, and they have no incentiveto use them.  Janet Schofield said that many parents do not believe that the violence seen on
television is really a problem, at least not to the degree that they donÕt watch things theywant to see because their children will be exposed to it.  When she talks to kids about experi-
ments on the connection between television violence and kidsÕ behavior, she loses their in-
terest.  She said parents or adults would take pornography issues more seriously than they

do violence, so there may be a difference in motivation to use the filter.  Sandra Calvert
noted that the V-chip is not designed to censor violence only; it also screens sex and lan-guage.  It has about five different ratings:  fantasy violence, real violence, sex, language, and

so on.  Robin Raskin said parents are not using filters on their PCs or AOL
Õs parental con-trols either, because they do not see the link between entertainment and behavior.  Part of
the problem is that the research on this link is 20 years old and not very good.  Sandra

Calvert said that people who watch violence but are not incited to kill by it tend to disbelievethe general findings in the literature about the connection, which depends on the individual.But there is a new review article showing a link between playing aggressive video games
and being aggressive personally, for both males and females.  People can become desensi-
tized to violence and no longer pay attention to it.  At this time, the culture is not so desen-
sitized to pornography, but this could become a problem.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.JOHN BLUMENTHAL69purveyors of this technology (Microsoft and Intel) and the publishers onthe server side offering up the content.  The complexity and impossibility

of this problem starts to avalanche here.A precedent to frame thinking in this debate is encased in an interest-ing act of 1990 that ultimately led to this technology.  The first initiative to

look at is the Platform for Privacy Preferences (P3P).8  I argue that exten-sions to this initiative, in effect, could implement a rating system.  This

would be done using the extensible mark-up language (XML), a revolu-
tion in the industry and the treatment of content.  XML is a natural evolu-

tion from HTML.9  It provides more power and will be the native formatin which all Microsoft documents are stored.  (Today, Word is stored in a

format proprietary to Microsoft.)  The XML processing engines sit inside

the operating system, at least in forthcoming versions of Windows; virtu-
ally every device in the world will be capable of parsing that type of con-
tent.  The idea is to modify the processing engine to require a P3P rating.

If the description of the P3P rating is not in the content, the processing
engine will not render it.  This would force everyone in the industry to

adopt this standard on a global basis.This idea is not that farfetched.  HTML achieved global status over a
period of time; XML will achieve similar status over a period of time.
XML already is being applied in various ways that have a global effect.
The idea of modifying client applications that already use the underlying
XML processing engine is not a stretch either.  XML even could be ex-

tended to handle commerce material (e.g., from Napster).  This initiative,
which is in front of the World Wide Web Consortium, is achieving stan-
dards that are unprecedented.  P3P is not a burdensome implementation,

either, technologically.  It is in line with where the vendors are going with

a whole slew of other initiatives.Next, you would need to start applying pressure on software indus-try giants and possibly hardware industry giants, too.  In doing so, the

entire client-side security fallacyÑthat you can control the rendering ofcontent on an untrusted and unsecured hostÑmust be recognized.  The
only way to compensate for it is through policy, by going after the people
who create compromises in reverse engineering of the system itself.  The
8See <http://www.w3c.org/P3P>.9Nick Belkin said that, so far, XML has done only what HTTP has doneÑformal character-ization.  No one has had any significant experience with content characterization.  If this is

done, then a database is needed that incorporates ontology that describes the whole thing,and someone has to construct and maintain the database.  Bob Schloss said there would be
an announcement soon related to this issue by a consortium of companies.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.70DIGITAL RIGHTS MANAGEMENT TECHNOLOGYDigital Millennium Copyright Act of 1998 outlaws some of these tech-niques.  It did not stop the DeCSS
10 model, but it did end up in court.
Reverse-engineering techniques would permit me to create controlsaround the content of any type of system.  Reverse engineering unleashes

content across all of computing.  It is one of those difficult problems that

have not been solved in the computer science field.  Embedded systems

raise the bar,11 but you create a cottage industry of reverse engineers who
will get down to assembly level code and remove the actual execution set
on the chip and replace it.  This is done widely now.  There are ways of

raising the bar continually;12 the question is, how far you want to raise
the bar and, in doing so, affect the industry in many different ways.If we implement such a solution in the turbulent waters of the indus-try now, we would create an interesting and difficult problem.  Some gi-

ants, such as Microsoft, want to dominate the content-rendering space,
and whoever wins that battle effectively dominates digital entertainment.
Microsoft is the best positioned to do this, as America Online and every-
one else knows.  The interesting economic and political issue is that the

operating system vendor would dominate this area.  If this solution were
implemented in the interests of policy, then the vendors would scramble10DeCSS is software that breaks the Content Scrambling System (CSS), which is weak en-cryption used for movies on digital versatile disks (DVDs).11Herb Lin said it would be very difficult, although not impossible, to do on-screendecryption.  In principle, you could build into the display processor some hardware that
decrypts data on the fly before they are put on the screen.  Milo Medin noted that such
technology is used for high-definition television.  David Forsyth said the problem with rais-

ing the bar is that you only raise it for one person.  The federal courts say that DeCSS is
naughty, but he has DVDs stolen from a Macintosh that required no programming to obtain.12Milo Medin said the problem with standards is that computer power increases.  A DVD
player cannot send out raw, high-depth material; it has to be encoded in some way.  (A PC
does not have this constraint.)  This requirement is in the license signature process for DVDs.
All consumer devices have the same fundamental issue.  You want to build a standard that

consumer electronics companies can blast into hardware, make cheap, and make widelyavailable.  You want that standard to last for 10 to 20 years.  To make an affordable device
when the standard is released, there must be a manageable level of complexity and security.
But 10 years later, a computer is much faster, and the standard cannot change.  Anything
that uses a fixed standard for cryptography is doomed.  DirecTV dealt with this problem inthe right way.  People often steal the modules and clone them.  One Superbowl Sunday, the

company turned off about half a million to 1 million pirate boxes.  Over time, the company
sent down little snippets of code and then, all at once, decrypted the code and ran it, and itchanged the way the bits are understood.  A flexible crypto scheme is the only way to ad-

dress this problem.  However, it is very difficult to implement in consumer electronics when
you do not have a data link; it may be easier in the future when everything is Internetconnected.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.JOHN BLUMENTHAL71to provide a solution, not so much to solve this very ugly problem,13 but
rather to control the rendering of music, documents, and images.Of course, the client security fallacy continues to hold.14  Once some-one has developed a way to circumvent the system, he or she can package
it into an application or executable and put it on the Internet, and anyone
else who wants to shut the whole system off just clicks on this applica-
tion.15  The goal is to raise the bar to a level of hassle so high that only a
very motivated individual would engage in cracking it.  Such safeguards

are all hardware related.16  Any solution not hardware related will end
up with a one-click compromise.  When you have to crack open a device
13Milo Medin said many stupid ideas are circulating in this space.  One idea is to put
controls in the logic of hard drives so that they will not store or play back files.  But as long
as the industry wants a cheap, easy-to-display, and easy-to-implement consumer electronicsstandard, security will remain elusive, because you cannot have all these things and security
too.  This is a problem that the industry has made for itself.
14Milo Medin noted that, as long as a general-purpose operating system is used, someonecan circumvent the system by changing a device driver.  In fact, a network makes such

changes automatically.  As long as people can make a change between the XML rendering
engine and the underlying hardware, they can get around anything.  Dan Geer said another
future trend is automatic updating by manufacturers on a regular basis.  This is done for two

reasons:  to ease the burden of updating on the average user, and to handle security prob-
lems that cannot wait for system updates.  The question of whether the software will run on
a desktop internally and belong to the user, or whether there has to be an opening for others
elsewhere to reach in and change it as part of a contract or lease, is outside the scope of thepresent discussion.  Herb Lin noted that automatic updates already are made to Norton
AntiVirus, Word, and Windows.  Milo Medin emphasized that both the software programs
and users can do automatic updates.  A provider can trigger an update on the desktop of a
subscriber at homeÑa capability built into the software.  But the provider cannot prevent
the user from also doing an update.15John Rabun said this would be a problem for law enforcement, because many pedophileswould get the chip needed to circumvent the system.  However, the system would prevent
normal exposure of children to pornography.  Milo Medin disagreed.  Unless the industry

changes the architecture of PCs completely, there will be a way to intervene in instructionsby loading executables into an operating system and running them between the hardwareand renderer.  By contrast, a cell phone is an intelligent device running software that is

relatively secure.  People cannot make calls with someone else
Õs cell phone because theycannot download programs into it.  In the case of the cable modem, the network operator,
not the user, controls the code.  The problem with PCs is that the user controls the code, and

the operating system does not have trusted segments that interplay with the hardware toprevent circumvention.  The situation is different with a set-top box, because the operating
system is embedded and is managed and downloaded remotely.  A user cannot get around
it because there is no hook to execute.16David Forsyth gave the example of region codes in the DVD world.  If someone wanted
to convert a DVD player into a non-region-coded player, he or she would have to fiddle
around in the guts of the device.  Clear instructions can be obtained from the Internet on
how to do this, but most people are inhibited from changing the firmware on their DVDplayers.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.72DIGITAL RIGHTS MANAGEMENT TECHNOLOGYcase and replace a chip or do something else that involves hardware, youraise the bar pretty significantly.  But this is a general-purpose computer,
and the idea of shipping a chip associated with digital rights, which Intel
tried to do, has not worked.17I am creating a futuristic scenario, drawing on themes in the industryand technology that are moving toward what I am describing.  The older
systems that remain in legacy states would not be able to participate in
the system; they would not be able to render content as easily as newer
systems.  The king holding all the cards is Microsoft, because it is the one
entity that can modify the operating system to require tags on content for
rendering.  If Microsoft took that step, then, in effect, you would drive the

pressure back to the publishers, who are saying, ÒIf I donÕt rate, then IdonÕt render.Ó  Microsoft can drive this issue, but this brings you back full
circle to the question of whether you give it the power to do that.Let us fantasize about this world in which content is legislated andrated, effectively much like the V-chip.  The whole argument over ratings

already has been conducted on Capitol Hill, so you would end up with an
interesting and difficult technological problem.  How do I know that con-

tent is accurately rated and that my P3P profile on my browser renders
that?  How do I enforce the association between the content being posted
and the rating that it is purported to have?18There would need to be a law that defines the answers.  Technology is
part of the solution, but this is difficult technologically.  A crawler or piece

of software could wander around the Internet, looking at your P3P rating
and then descending into your Web site to determine what that content
really is and whether it is accurately rated.  This is feasible, and it is prob-

ably an interesting project for some of the best computer scientists in this
country.  There are things like this on the Internet today, not necessarily

looking at porn, but providing other search engine capabilities.  This tech-

nology will improve over time.  You would have to build a component

that is highly complex and globally capable of crawling around the
Internet.17Milo Medin said Intel would still fail if it tried this approach again today, because peopledo not want someone else controlling their computers.  Robin Raskin argued that it is a
trade-off between service and privacy; if Intel can make the usersÕ lives easier, then users
will comply.  Milo Medin said the problem is that consumer electronic companies want to
build cheap devices without elaborate internal workings.  All it takes is for one or two people
to crack the code and post it to Usenet, and it will be replicated all over the place.  Providing

access to the content (as opposed to the algorithm) is illegal because of copyright.18Milo Medin said this is a Federal Trade Commission (FTC) issue.  There must be a nega-
tive consequence for rating aberrations to change behavior.  In the privacy arena, everyone

posted something in the deal with the FTC, and the FTC said it would pursue anyone whoviolated the agreement.  Bob Schloss suggested a default rating, so that if actual rating infor-
mation is absent, the content is assumed to be X rated and for adults only.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.JOHN BLUMENTHAL73Realistically, to achieve this system, you would go after Microsoft
based on its market dominance in the rendering device itself.  If you con-
trol that, then you effectively control how things get published to those
devices.  This would be the creation of a V-chip-like initiative that goes to

the heart of a much more homogeneous environment than what the V-
chip vendors were concerned about.  Technologically, it fits with the P3P
protocol and borrows from classification models, such as Label Security
implemented by Oracle 9i, that define data and how the rendering client
should treat them.  But it is still futuristic and requires huge global change.

Another layer you can add is policy-based filtering in the network itself.
The only way you can approach this problem holistically is with a model
that layers additional components of control from the network to the cli-
ent application and operating system to the publisher.The publishers will oppose this because it will limit their market reach.Yet they have an incentive to protect copyrights and to have a control
model in place.  They are all trembling in the wake of the Napster crisis.

This is why I hold out hope that solving this problem also solves some of
those issues for them.11.3PROTECTING CHILDREN
I say it is up to the parent to define a childÕs user profile during theinstallation of an application.  Many applications do this today:  AOL

accounts, Netscape, and Internet Explorer offer a profiled login.  This way,

when a child sits down to use that computer, he or she is constrained by
the user profile, which technically becomes intertwined with the P3P pro-
file.  Once the child gets past a profile login, his or her Internet world is

constrained by the definition of that profile.This is in line with how you operate today.  The difference is that thecontent you would access in my system would be controlled by the defi-
nition of your profile.  This link is not strong today; there are no preset

rules as to what renders in a browser.  I am suggesting that you have to

deal with the login issue to gain access to a profile based on your age.
This comes back to the question of how you authenticate just age without
violating other policy constraints and privacy and so forth.  The P3P ne-

gotiation occurs at the machine level.  For the level of detail in the profile,

imagine a sliding bar representing content acceptable to the parent.1919Robin Raskin said the more granular the P3P negotiation, the less it will be used.  Sys-
tems do not work when they ask parents to make distinctions among, for example, full
frontal nudity, partial nudity, and half-revealed nudity; in such cases, parents decide to lettheir kids see everything.  A good profile requires a lot of granularity, but to convince a
parent to use it, it cannot have any nuances.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.74DIGITAL RIGHTS MANAGEMENT TECHNOLOGYThe privacy issue arises not when a person provides access to per-sonal information but rather when someone else records it.  If you focus

on the client side, then at least you can throw to the privacy advocates a
bone that says, ÒAll of that information is stored locally.Ó  But there are
systems in which you need a connection to a remote server, and your
private informationÑlike a credit card number or some other authenticat-ing tokenÑgoes somewhere else.  Once you do that, the privacy advo-
cates will descend on this like vultures and pick it apart.The adult entertainment industryÕs age verification services move theissue of trust somewhere else.  When you give your age, you get a chal-

lenge response asking you to prove your age by filling out a form.  You

might do that with a credit card number or other personal information.
You repose this information with the trusted third party.  This informa-

tion could be loaded to say, ÒYour P3P profile now permits you to see thistype of material.Ó  But because you send your private information some-
where else, this age verification service, over time, now becomes a list of
names of people who want access to porn.20  You can see the privacy
people going crazy about the fact that this database is being used for that
purpose.There is another industry trend that relates to age verification.  Dan
Geer is probably one of the worldÕs leading experts on this, because hedesigned the system that Wall Street uses, Identrus, which issues digital
certificates to own identity.  The forms that describe the identities in those

certificates have an age field.  There are initiatives concerning the issu-

ance of multiple certificates based on multiple types of identities and use
of identity.  There is talk in various committees in front of the Internet

Engineering Task Force about the issuance of age-specific certificates.To obtain an age-specific certificate, you would prove to VeriSign thatyou were born on the following date and your Social Security number is
x.  Then you can be issued a certificate to be loaded onto your computer.
There is discussion in the public key infrastructure community that
VeriSign might fill the trusted third-party role, in which it would gain no
further knowledge about you other than your age.  VeriSign has a bunker

that enforces the limits in physical and legalistic ways.  I would feel com-
fortable proving my age to VeriSign, knowing that it is legally bound.  In20Herb Lin noted that whoever is verifying the age information does not have to keep alist, even though it would be valuable.  If people could be sure that no list was being kept,
then the privacy issue would disappear.  The difference between cyberspace and the real

world is that, if a person goes into an adult bookstore and shows a driverÕs license as proofof age, then the clerk just looks at it and says, ÒOK.Ó  The clerk does not make a photocopy
of it and file it away.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.JOHN BLUMENTHAL75fact, VeriSign exists on a foundation of trust that is assumed when youuse and obtain its certificates.This system might indeed provide the trusted third party for age au-thentication, and it fits with the public key infrastructure.  The problem
Ñand Simpson Garfinkel and others have pointed to this in the privacy
debatesÑlies in the meta-aggregation that will come in the future.  I will
get that database; VeriSign sells data like that.  I also will get the

clickstream from all the porn sites, and interesting data mining techniques
will be used to aggregate and combine these data to trace it back to me
and say, ÒYou were the person who did this.Ó  There is widespread com-
promise on the server sideÑlook at Egghead and CD Now.  This is an
uncontainable problem that you do not encounter until after the compro-
mise has occurred.2111.4SUMMARY
There are many threats to the system I just designed.22  Compliance is
a major issue, which the search engine industry is addressing to some
extent.  Bots will be required to crawl the Internet for server-side ratings

implementation; anti-bots can be created to defeat compliance checking.
Client-side Trojans, worms, and viruses all can be injected into this ma-
chine to modify the XML processor.  If it has memory, then I can hack it.  If

it has a processor, then good reverse engineers can create a one-click com-
promise.  Ratings can be stripped off of content, or interesting techniques

can be used to create content that appears G-rated to the rendering engine
but is actually X-rated.  In the Secure Digital Music Initiative, they tried to

watermark the content to control it; this was hacked within days.  The

same thing would happen here.  Finally, you would face widespread dis-

semination of a one-click compromise created by one hacker.  ÒScript kit-tiesÓ enable people to click on an attack that someone else created to auto-
mate everything I described.  The scenario is not very hopeful.
21David Forsyth said you could prohibit people from possessing certain types of data orusing them in certain ways.   You also could punish violators.  But the chances of actually

catching them might be very small.  Someone could keep a database in a way such that it
would be difficult to find.22Herb Lin summarized the presentation as follows: To control distribution of content toonly age-appropriate people, you would have to make many changes in the existing tech-nology and policy infrastructure, going far beyond the issue of age verification for inappro-priate content.  This would offer some benefits but would not necessarily solve the problem.
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.7612A Trusted Third Party in
Digital Rights ManagementDavid MaherI designed the secure telephone unit that first used the infamous Clip-per chipÑwhich further illustrated, to me, many of the issues involved
with trusted third parties.  I agree that there are major problems with

trying to control what people do on their open-system PCs.  But we should

not give up just because we cannot design a perfect system to prevent a
hacker from hacking PCs.  There are techniques that can make hacking

difficult, and in particular techniques that can allow business models to
be supported in spite of security breakdowns.  When I saw CSS several

years ago, my colleagues and I in the secure systems world shook our
heads and said, ÒAs soon as itÕs rolled out, it (the crack) will be on a T-
shirt.Ó  In fact, it was. But bad security design does not have to be the rule.
I agree that a lot of infrastructure will have to be rolled out to takeadvantage of some of the methods and techniques discussed here at these
meetings, and many things will have to change. We will become more
oriented to digital rights and responsibilities and policies. There will be
motivation to roll out some of these techniques, methods, and standards,
not only because of digital rights management for the control of copy-
righted material in the media and entertainment industry, but also practi-
cally for asset management (in enterprises), where some of the challenges
are not quite the same.  There is a lot of movement and demand to set up
the infrastructure for policy and control of the deployment of assets, both
within an enterprise and among enterprises.The context for digital rights management (DRM) has a lot to do withcommerce automation, where you have a publisher who wants to publish
information, which could be entertainment, pricing information, or a con-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DAVID MAHER77tract, and the publisher wants to give access to the right people, who areallowed to exercise the provisions of the contract.  Just about any piece of

information that has some value that someone can exercise some right
with regard to is the type of thing that you want to be able to control in
this sort of system.12.1INTERTRUST TECHNOLOGIES
At InterTrust Technologies, we give the publishers tools that allowthem to place the content in a container that provides any type of protec-
tion that the publisher wants.  It can be encrypted or not; it can have integ-

rity protection or not.  There could be rules associated with the informa-

tion placed in the container.  There also could be other containers linked

to that first container that contain additional rules, such as rules that the
publisher thought of later on or rules that say that the previous rules are
revoked.Then you go through a distribution chain, which may have severaltiers.  According to the rules, people can do various things.  They could

change the unit price of an object that has commercial value, for example,
or they could decide that you can forward it to someone else.  Just about

any action can be controlled at any level of the distribution chain.Eventually, however, these things get back to the consumer.  In our
space, the consumer has to agree to rules, either implicitly or en masse.
For example, if there is a license associated with something, then the user
must agree to the license, which may make an implicit agreement for
many other transactions that might happen down the road.  But somehow

or other, the consumer must be informed about the rules associated with
the things that impinge on the consumer.As an example, a rule might say that an audit record will be created ifyou engage in a specific transactionÑan audit record that itself becomesprotected content.  This is done in a way such that the consumer is told,

ÒYou can have this piece of content for free.  We will collect some un-
linked, anonymous information about it, but we need to aggregate that
information with information from other people.ÓInterTrustÕs role is to ensure that such things are done in a fair andaccurate manner.  For example, if someone says, 
ÒI will not collect data foran audit record about your use of this,Ó we can tell whether that state-
ment is true, because we designed many of the mechanisms.  The rules

say that if an audit record is supposed to be created but instead an

anomaly occurs, then the transaction will not go through. The idea is to
have automation not just within the Web, but within any local area net-
works or personal area networks, such that the consumer could, for ex-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.78A TRUSTED THIRD PARTY IN DIGITAL RIGHTS MANAGEMENTample, have some of this content moved into various other types of de-vices.Thus, the commerce networkÑat least in the way that we representDRMÑcontains just about any type of digital information.  There are also
loosely coupled rules, meaning the rules do not have to be packed with
the information in the same file.  The file can be delivered in one space

and the rules delivered in another.  In addition, the rules can change; they

can expire and things of that sort.Another important concept is identity attributes, which are applied toprincipals who may use the information.  Rules can refer to those identity

attributes.  There is a coding system for identity attributes, and a trust

management system for determining which identity attributes are associ-
ated with what.  The identity attributes also could be associated with

pieces of information.  For example, a rule might say that if you are a Book

of the Month Club member, you get a 25 percent discount.  There also has

to be something, such as labels, that identifies Book of the Month Club
selections.  These labels are identity attributes in that space.
Events and consequences are an essential part of the DRM system.The content owner identifies the events; for example, if you want to play
this particular game, then you have to pay for it.  In such cases, content

owners might want to see proof of authorization or payment, or they
might prefer to say that a meter in some device is decremented or
incremented.  Or they may want to have, anonymously or explicitly, the

identity-linked information or a record of what happened.  Some of these

events and consequences are practical.  In the medical information arena,

for example, people are resistant to hard-coded policies on access to medi-
cal records, because in emergency situations these policies would not be
appropriate.Therefore, you need exception mechanisms, which are difficult toimplement.  The exception mechanism might say, 
ÒYou can have emer-gency access if you say who you are; then an audit record will be collected
and will flow upstream to a clearinghouse, and later on someone may ask
you why you did this.Ó  At least this approach tends to ensure that the
exception mechanism is not abused.  Such a mechanism could be useful in

the context of labeling content so that children can have access to some-
thing on which they are doing a report, even though something like P3P
or some browsing policy enforcement software, or whatever, otherwise
would deny them access.  Creating an audit record is problematical, but

at least the parent can say, ÒI understand that you exercise that exceptionin a fairly straightforward way and I am still monitoring what you are
doing in absentia.Ó  When these techniques are applied, the recording of
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DAVID MAHER79events, logging, and especially exception mechanisms are absolutely re-quired.An audit mechanism can be defeated by an attack on the communica-tion between the auditor and desktop.  The mechanism that we use as-

sumes that you are not always online (most people are not).  We can tell

whether or not people tamper with the protected database, up to certain
limits.  There are thresholds that say, 
ÒI must deliver my cache of auditrecords to wherever their destination is.Ó  The audit server could be part
of an enterprise, or you could contract with an ISP to host the clearing-
house for the audit records.  Or it could be part of a home network or part

of the same machine such that the parent has access to the audit records
but the children do not.  It is difficult to implement but conceptually

straightforward.We have a network of protected processing environments.  We work
directly with chipmakersÑsuch as Texas Instruments, and chip platformmakers, such as ARM, and other companies making chips that go in set-
top boxes, cell phones, or personal digital assistantsÑto put in securitymechanisms (e.g., trust management) so that we can have a protected pro-
cessing environment.  This is highly problematic for a PC, as observed by

others earlier.  The mechanisms that we use for the PC are quite different;

they have to do with the concept of renewability, also alluded to earlier.Trust management, or delegation of trust, involves who and what aretrusted to do what, and who determines policy.   This has do with, for

example, those things you delegate to a parent versus a child, and how
you arrange the user interface so that people actually understand the
policy on what might be delegated to themÑa difficult problem in thisspace.  A couple of years ago, AT&T Labs did a demonstration of P3P

policy with a user interface, which I thought was the most crucial aspect
of the research done at AT&T labs on P3P.  A user interface is how you

make all of this material understandable.  They made a few policies vis-

ible.  But these were not granular policies, which are difficult to make

people understand.  Straightforward policies might be difficult to change

on a daily basis, but they can at least be tuned, perhaps when installed,
using a somewhat more complicated user interface.There is also the distribution of policies and rules, which can be bro-ken up into three areas of intent:  what you want to do with the content,

under what conditions you are allowed to do those things, and what the
consequences are.  Another important concept is action inquiries, that state

the conditions under which I even ask the question, ÒAm I allowed to dothis?Ó  There is also governance of transactions, the overseer that ensuresthat a transaction is carried out.  When the answer to an action inquiry is,

ÒYes, this is allowed, but . . . ,Ó then often it is allowed if you pay or if an
audit record is created or whatever.  This is the concept of a transaction.
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.80A TRUSTED THIRD PARTY IN DIGITAL RIGHTS MANAGEMENTConcurrent events either all occur or do not occur together.  There are
two-phase approaches to ensuring that governance is enforced that are
part of the DRM system but distinct from the trust management system.12.2COUNTERMEASURES AND HACKERS
Another part of DRM is renewability, which I think is key to trying todefeat someone who is determined to circumvent the system.  I have been

involved in the design of protection for satellite entertainment systems,
and the sophistication of attacks on these systems might astound some
people.  One of the best books on defeating these systems is 
The BlackBook, which has a skeleton and crossbones on the cover.  You can order iton the Internet and it is freely available, published by a charming Irishman
named John McCormac.  It is humorous, but it also has a lot of code and

diagrams of how to defeat various satellite receivers.  He also publishes a

Web site, the Hack Watch News (at <http://www.iol.ie/~kooltek/>),
which has been up for years and is probably still there. At one time this
site was filled with hacks and boasts of hacks, but now the hacking is
uninteresting, and the hackers seem to be having far less fun.A number of these satellite systemsÑthe predecessors of DirecTV, forexampleÑwere mercilessly attacked.  I asked them how they designed
systems that could be attacked so easily.  The answer was something like
the following:  ÒOur contract with the service provider just says to keepthe piratesÕ success rate below a certain level.
Ó  This is all they really
needed to do.  More aggressive approaches were either more expensive or

more intrusive to the legitimate consumers.  For years, they have been

playing that game of keeping the piracy below a certain level while ensur-
ing that the protection measures are not that expensive in a generalized
sense, and that includes intrusion on legitimate rights.The Hack Watch News, which I used to monitor quite a bit, coveredwhat happened when the purveyors of one of these protection systems
tried using a renewal technique.  As described in an exercise recently with

DirecTV, some people had businesses selling hacker versions of smart
cards, which were better designed than some of the legitimate smart cards.
They gave you access to material that you should not have been allowed
to access.1  Then the algorithms were changed, and the hackers defeated
the countermeasure.  The algorithms were changed again the second

month.2  After the third time, the Hack Watch News said there was a pall
of defeat.  The hackers basically gave up.
1Milo Medin said there was a market for these cards in Canada, because residents therecould not subscribe to the programming legally.2Bob Schloss noted that this approach works for new content only.  New content requiresthe new algorithm, which may never be broken or may take a few months to be broken.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DAVID MAHER81I taught a course on some of these things, and I had a cartoon in whicha little kid is crying, ÒMommy, mommy, I canÕt get the Cartoon Channelany more.Ó  The mother says, 
ÒWell, weÕll just have to wait until nextmonth when the solution to the next countermeasure is available.Ó  The
idea is to keep the legitimate service level, for most people, better than
that available from the pirate.  There are things that we can learn from

that approach, although this problem was different from the one at hand
here.3  The satellite pirates were commandeering part of the legitimate
system, either for their own benefit as individuals or, in some cases, as
part of a business selling smart cards.We use a secured virtual machine that is independent of the browser.4We keep changing it to defeat the hackers.  This method is problematic

because we have to get that thing on the desktop.  We are arranging to get

that capability in all of the forward-looking systems, but we do not have a
deal with Microsoft so it is problematic within Internet Explorer.  There is

reasonably good technology such that, as long as you are connected inter-
mittently, it will allow you to do that.  Marimba
Õs Castanet software doesa good job; you tune in to an upgrade channel.  I think Real Network uses

either Castanet or something similar.  It tells you if an upgrade is avail-

able, and then gives you an option, which is the standard way of dealing
with this.  To make our system effective, you would not allow the option

for the upgrade.  The problem is raising the stakes on who gets the up-

date, so renewability and tamper resistance are essential.Napster is having a problem now with legacy content.  They are try-
ing to put together a system that will use name tagging to prevent distri-
bution of copyrighted material through Napster.  Of course, there are al-

ready dozens of ways to counteract that approach. But there is also the
concept of requiring proof of origination.  There are sophisticated systems

that check for proof using cryptography techniques.  (Hackers do not tar-

get these techniques, but rather try to turn off the structure of the secure
system, the key management and things like that.)  In the case of some-

thing like proof of origination, you must have a policy that says, ÒThissystem will not read or present any data that lacks proof of origination.ÓIn which case you would have secure labels and so on.  You will still have
Thus, even with a great system, all the old pornography produced before a certain dateÑalot of materialÑstill would be available for everyone to see.3David Forsyth said the problem is different because the satellite pirates are ÒvicariousÓcontent providers who are not doing anything to their own satellites.  They might hack your
chips, whereas anyone who gains access to pornography on the Internet can distribute it.4David Forsyth suggested that software vendors might give out new browsers everycouple of months to defeat the hackers.  But it is not clear that everyone is jumping on the
rendered software bandwagon.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.82A TRUSTED THIRD PARTY IN DIGITAL RIGHTS MANAGEMENTthe issue of what to do about unlabeled content, whether legacy materialor not.  You need a policy that deals with unlabeled content.
People believe they should get satellite programming or music fromNapster for free5 because the data are not stored in any encrypted way
when someone buys it.  This is a fundamental issue.  For new things, you

can use the lack of an Òin the clearÓ distribution path as the exclusion
mechanism; this is the issue with the record industry.  But from the per-

spective of media, do you believe that this type of structure, which, in
essence, rents content or distributes rights according to content, will be
any more successful outside of the commerce space, where you can basi-
cally say, ÒIf you want to do this, then you have to do it this wayÓ?  Do
you believe that this will ever be successful given all the history?I am making an actual personal bet that it will.  But the path to getting
there will not be easy.  I look at the forces that resist success and wonder

whether they can be overcome.  I have spent a lot of time thinking about

privacy because of the issue of collecting information about events in dis-
tributed systems.  I do not think we will have a truly productive distrib-

uted computing system unless we know how we can collect information
about those events.  We are dealing with that in the embedded systems

committee.  At my company, we say, ÒCollected information about thosethings is protected, and we have techniques and policy mechanisms to do
that.Ó  How effective we can make them and how can we use distributed
trust mechanisms?   We know that we cannot do it perfectly, but this does

not mean we do not try.There is also interplay between law enforcement and policy at thegovernment level. In the DRM field, we depend on things such as the
Digital Millennium Copyright Act, with which I was not completely
happy because of its impact on research.  But certain aspects of it are rea-
sonable.  Its provisions are important
Ñaddressing issues associated withcountermeasures, and what risks you take when you try to defeat a coun-
termeasure.  If we could get the research aspect right, then I would be

happy.  There are also other things, such as copyright and patent law.
If you are a purveyor of mechanisms that defeat countermeasures,what consequences do you face?  What are the risks?  My house does not5Winnie Wechsler said that in the mid-1980s, when encryption was introduced to the back-
yard satellite dish market for the first time (before DirecTV), there was an uproar amongpeople who owned e-band satellite dishes, because they felt it was their right to have accessto this programming, which had always been free.  They bought the dish, and the free pro-

gramming was part of the proposition.  Then suddenly programmers started to use encryp-
tion, and there was a huge backlash involving piracy.  She suggested that this is a funda-
mental hurdle in developing any solution to piracy.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DAVID MAHER83have a lot of security systems.  Many other people have all types of secu-
rity systems on their houses.  Yet it is very simple to deal with them; you

could level a house with a bulldozer, for example, and grab the jewelry.
This does not happen because we have laws and law enforcement.  The

same type of situation will occur here.  The cost of the systems clearly has

to fall,6 and you need a shared infrastructure so that, instead of just a few
people paying for it, a lot of people pay a much smaller per-person price
for it.  This is why the techniques will not be rolled out just yet.
There are solutions coming in a couple of years that will use moresophisticated distributed trust management techniques to increase the
barriers to unauthorized redistribution of content.7  This will be done on
the basis of actions that firms can insist that you do as a condition of re-
ceiving their material.  I believe this to be true because many larger pub-

lishersÑincluding entertainment publishers, such as Time Warner, Uni-versal, Bertelsmann, and ReutersÑare funding the establishment of someof these mechanisms.6Robin Raskin said the cost of the system would exceed the costs of the music or televisionshow that one tried to protect.  He gave the example of publishers dealing with authors
Õcontracts.  In looking at DRM, he decided it was cheaper in the short term (the next 2 years)to pay all the authors more money than to implement a rights management system, the costsof which, for a big publishing company, would be astronomical.  Herb Lin said representa-

tives of the adult online industry told the committee that they have problems with peoplecopying their content and redistributing it without paying.  He said it seemed doubtful that
any single provider could afford to implement a DRM system.  Bob Schloss said DRM would

work in the music industry because the major labels believe that each artist is unique, suchthat almost nothing is a substitute.  This may not be the case for other types of content,
including pornography.  If Danni Ashe (who testified before the committee at a previous

meeting) required a special browser plug-in or keyword every time someone visited her site,and no one else had such a requirement and her competitors were comparable, then peoplewould go elsewhere.  John Rabun said most of Ashe
Õs images are copied all over the place.The people who copy them do not even bother to change the titles, even though you wouldexpect that someone violating a copyright would at least do this.  Rabin said Ashe expressed
concern about new talent, but this constitutes probably less than 1 percent of all adult por-
nography sites.7John Blumenthal said he checked the Web site of Danni Ashe to see how she did ageverification and how she contained her content to her site.  Then he went to Usenet, where

some news groups focus on her.  The news groups
Ñat least three or four different UsenetserversÑcontained no images of her.  Somehow she is creating a barrier between her Web
site and Usenet.  Herb Lin said he asked Ashe these questions and she is very concerned

about redistribution; she also hired her own technical staff to deal with the issue.  David
Forsyth said he does not understand why she does this, because it is valuable when peopleredistribute low-resolution or inconvenient versions of good content.  Forsyth is finishing a
textbook, which can be downloaded in PDF format and printed.  It is much less convenient
to print an 800-page book than to buy it, but availability of the PDF version means thateveryone gets to look at it.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.84A TRUSTED THIRD PARTY IN DIGITAL RIGHTS MANAGEMENT12.3SUMMARY
Carrying out the concepts of trust and policy management is nottrivial.  We need languages and ways in which we can identify principals.

In some of this space, we need to identify principals in an anonymous
way.  P3P addresses some of this, but I am not sure whether it will do

everything that we want without things like exception mechanisms.  We

need credentials and an artificial intelligence compliance checker.  These

are not universally available, but there is a drive to make them more avail-
able because of their usefulness in commerce.  Until these things are em-

bedded in such a way that people interact with automated systems in a
natural fashion, it is difficult to believe that the mechanisms will have
widespread effectiveness.  Some of the research needs to focus on how

people interact with these systems.InterTrust has embedded a trust management system that adheres tothese principles into the systems deployed on behalf of its partners.  We

also play another important role.  There must be an administrator; some-

one has to be copyrighted as the root source of trust.  This must be a

utility-like function, that is, carried out by someone who specializes in
doing these types of things and does not compete with the people for
whom these mechanisms are deployed, because there could be bias.Do we have competitors? Yes, we have competitors.  In spaces such as
music, our main competitor is Microsoft, which, interestingly enough,
does not have the utility-like attribute.  Microsoft competes with many

service providers, which is what they (the service providers) are afraid of
(in making Microsoft a gatekeeper, through their DRM).  People expect

InterTrust, as an impartial trusted party, not to compete with them as we
deploy these types of mechanisms.  We are putting legal structures in

place to ensure that this happens.  DRM is all that we do. We charge a

utility fee, which I think is 60 basis points on transactions that use the
technology.  The reason the Universal music group, Bertelsmann, and a

few others have looked kindly on us is because of our impartiality in that
we do not compete with them.  But we have also heard that they think

that 60 basis points is a Òcheap date.ÓTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.8513Problems with a Dot-xxx Domain
Donald EastlakeI co-wrote a personal Internet draft in the Internet Engineering TaskForce (IETF) about the problems with mandatory labeling.1  People oftencome up with ideas about how to segregate or label all bad material to
magically solve the content selection or child protection problem.  This

idea is simple and easy to understand, but it does not work.  There are a

lot of problems with it, which this draft tries to summarize.  The problems

can be divided into several categories.The first category of problems is philosophical.  The idea of finding away to categorize content in the global context of the Internet is absurd.
There are 200 countries and they all have different laws.  For example,

laws on nude modeling differ.  In one country you can have a magazine

consisting entirely of nude pictures of 17-year-olds, but this is obviously a
felonious and criminal act in another country, where nude models have to
be 18.  Yet another country might not permit any noticeable amount of the

female body under any circumstances in a magazine or publication.  There

is no hope of getting a consistent point of view on this sort of thing.  And
this is just one criterion.Moreover, there are more cultures than there are countries.  There
are literally thousands of cultures, all of which have their own particular1See <ftp://ftp.ietf.org/internet-draft/draft-eastlake-xxx-00.txt>.  Personal Internet drafts
have no formal status and are not endorsed by the IETF or any other group. The draft isintended to become an informational request for comments (RFC), a document that is issuedunder the auspices of the IETF.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.86PROBLEMS WTH A DOT-XXX DOMAINquirks and ideas regarding what sorts of things children should be al-lowed to access or the age at which children become adults.  Going one

step further, the concept of community has made it easier to develop
standards, one way or another.  But there are literally millions of

communities.Another category of problems is legal.  If you require everyone who
has a certain type of content to be in the dot-xxx name space, then you are,
in effect, forcing speech on them.  This seems to be a problem with respect

to certain legal rights in the United States and some other countries.  It

obviously depends on the circumstances and whether this sort of speech
is commercial or noncommercial, and so on.  But, in effect, you are requir-

ing people to label themselves, which runs into legal problems and effec-
tively limits their free speech.One difficulty in thinking about this sort of thing is the malleable na-ture of the Internet.  Some parts of it are similar to commercial broadcast

television, which, at least in the United States, currently has a system of
labeling.  But other parts of the Internet are more like someone strolling

through a park and talking to whomever they bump intoÑactivities that
are entirely noncommercial, spontaneous, and unorganized.  Imagine, if

you are strolling through a property and bump into someone and you
want to say something that some people could construe as objectionable,
that you had to wear a large, yellow star.  I think people would consider

this to be objectionable.  In some respects, labeling of Internet content

could be considered similar to the yellow star.Another category of problems is technical.  The labeling system has to
be realistic.  The use of dot-xxx is not linguistically complicated.  But if

you try to label in an understandable way the various different axes of
heresy or derogatory speechÑwhatever people object toÑthen you would
have problems with the language from which to select the labeling.  In

addition, the Internet is not technically structured for things to be done in
this way.  The Internet has a hierarchically distributed control structure,

so that one entity controls dot-com, for example, and other entities control
the subzones below dot-com.  There are multiple levels.  Typically what is

identified by one of these names is an IP address for some machine that
can store data.  Of course, we worry about causing a name to somehow

correspond to some characteristic of the data in that machine.  In fact, the

people controlling these different name zones are likely to be indepen-
dent organizations, and there is no way to stop other people from point-
ing at your material.In other words, if you post material on a Web site with a name, thereis no technical mechanism to stop someone else who has independent
control of a different zone on the Internet from posting a pointer to your
IP address under any name that they choose.  If you have innocent mate-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DONALD EASTLAKE87rial, there is no way to stop someone from creating a dot-xxx name thatpoints to your project.  Similarly, if you have material that is placed cor-

rectly in dot-xxx, there is no way to stop someone from creating an inno-
cent-sounding name that points to you.  If we had global laws, we could

make this practice illegal and go round up all of the people who do it and
fine them.All of these tricks are affordable.  It is very simple, for example, to
take an arbitrary mailing list, one that is entirely innocent and devoted to
some light topic, and create an alternative address that you can send mail
to, an address with terrible things about ÒxxxÓ in its name.  You can have
this bad sounding address automatically forward messages to the real,
innocent mailing list and change the envelope informationÑthings notnormally seen around a messageÑand the headers.  There is no softwarethat checks on these functions, so it is easy to cause things to be distrib-
uted to individuals or mailing lists while making it appear that the mail-
ing list has a name that is actually forged.  In principle, a few of these

problems could be solved by globally distributing changed software, but
this is unlikely to happen.There are other things on the Internet that have domain names thatare not really domain names.  For example, there is Net News, which has
news groups that are hierarchically named but not hierarchically struc-
tured.  They are more anarchic than domain names because they do not

have a root and so on.  They are more like a conversation, in that anyone

could post anything to any of these news groups and, except for the few
that are moderated, it is not clear how you can enforce much control over
the names.  Similarly, names are used in Internet relay chat and chat rooms

that are also very conversation-like.   Given all of this, you wonder if you
can reasonably come up with an approach that would meet reasonable
linguistic criteria and somehow affect all of these different naming
schemes in any reasonable fashion.There is nothing wrong with the mere existence of a dot-xxx domainname,2 or with just anybody getting a dot-xxx site. But I feel that, if such
a category existed, it would greatly increase the probability of laws re-
quiring people to register there.  This is not a technical problem, and there
2Milo Medin said that some companies want to brand themselves in such a way, and thismechanism is convenient.  Logically, if there were a generic law that said people had to label
themselves, it would be universally agreed that, if people put their content into dot-xxx,they should not be prosecuted if a child happened to get in there and the filtering software
failed.  Dot-xxx is not the way to enforce mandatory labeling; this should be done with PICS
or something page dependent.  However, someone could be prosecuted, either civilly or
criminally, if they put not-for-minors content into a dot-kids domain.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.88PROBLEMS WTH A DOT-XXX DOMAINis certainly no technical difficulty with the mere existence of that utilityand the ability of people to get names there, as long as some organization
runs a registry for it.  There is a slippery slope argument, but it is not

currently mentioned in our draft.  The main thrust of our draft is to pro-

vide a convenient, precompiled answer for people who assert that a man-
datory dot-xxx domain name will magically solve the problem they per-
ceive in the categorization of Internet content.The idea of a dot-kids domain may have a different spin in variousways.  It still has the problem that the criteria for what kids are and what
is appropriate material for them differ widely among nations, cultures,
and communities.  But in some sense it is a little better than dot-xxx.

Maybe if you put something in dot-kids that is not considered appropri-
ate for children, you would be prosecuted.I also want to comment on the idea, which is mentioned less often, ofcategorizing content with a bit of the IP address.  All hosts on the Internet

have either 32-bit addresses under IPV-4 or 128-bit addresses under IPV-
6, which is not widespread but is getting some attention.  There are many

problems with this approach.  It is, in some ways, coarser than the domain

names (sometimes the main name structures can be used to address a
subset of material for the host).  In some sense, like the address of a build-

ing, it refers to everything in that building.  One problem is that there are

no extra bits in IPV-4.  Taking even one bit away would cause havoc; there

are not enough addresses to go around.  The whole reason for the creation

of IPV-6 was to overcome the limit of 32 bits in IPV-4.Another problem is that these bits are not arbitrary.  They are topo-
logically significant.  As packets are sent through the network, they are

routed by comparing the prefix bits on these numbers with a routing table.
Essentially, the longest match determines how the packet is sent.  I am

simplifying this a bit, but at the top level of the Internet, routing tables
currently have on the order of 40,000 or 50,000 entries, and this deter-
mines where things go at the top level, and they trickle down from there
until they get to a particular local machine.  If you assign addresses ran-

domly, then you need billions of routes at the top level or else it would
not work.  There is no feasible hardware today that understands how to

do this.  For the Internet to work and get the data around, the address bits

have to be assigned in a topologically meaningful way, directly related to
the actual structure of the Internet and how the IPs are connected to each
other.IPV-6 might sound more hopeful, but it is not.  One popular proposal,
intended to enable wide deployment, effectively would reduce the rout-
ing part of the IPV-6 address to half of the full size.  In this scheme, 64 bits

would be used for all of the routing control, and the other 64 bits would
be used as a unique end-point identifier.  Conceivably, you could some-
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.DONALD EASTLAKE89how get one bit out of the bottom of the 64.  But once you consider the
need to label things along all the different dimensions and categories you
might need on a globally meaningful basis, there is no way to do it in the
bits in an IP address.There is some hope for a technical solution.  PICS has multiple modes.
The mode in which you have to put a fixed label on your Web page or site
has all types of similar problems as does forced speech, and not enough
categories, and so on.  But PICS does have a mode in which you have

separate servers, like a separate rating service.  You can ask the servers

about certain data, certain sites, and so forth.  This, at least, seems not to

have the problems of forced speech or the limitations of other labels.  You

could have literally thousands or millions of different PICS servers that
painted the world in different ways, and they would enable you to ask
questions as to whether certain parts of the network are approved or not
by the vendor of that particular PICS rating service, which could be some
particular church, culture, or country.  I am not saying that this necessar-
ily would work wonderfully, but it does seem to have at least some tech-
nical practicality.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.9014Business Dimensions:The Education MarketIrv ShapiroI am the chief executive officer for Edventions, which provides a suiteof software services and training to introduce technology transparently
into schools.  Let me define ÒtransparentÓ very simply.  When you got

into your car this morning, all you needed to do was hold onto the steer-
ing wheel, push two pedals (maybe three, if you are an advanced driver),
and you were done.  You did not think about what type of engine was in

the car, why it worked, or any of those kinds of issuesÑthe car was trans-
parent technology.114.1THE ROLE OF TEACHERS
I am most interested in the role of teachers in elementary schools,which are very different from high schools or universities.  From a busi-

ness perspective, teachers are both an asset and a liability.  That asset and

liability may be the solution to some of the questions posed earlier today
(described earlier in these proceedings).For at least 2,600 years, from the time of the Greek academies, whenadults have wanted to introduce children to new material, they have sent1Sandra Calvert noted that driving a car is not transparent for a new driver.  When first
learning to drive, she was concerned about what to do if she had to sneeze.  This is some-

thing that requires thought; it is not an automatic skill.  Even today, she carries an American
Automobile Association card so that she can call emergency services if she runs into anyproblems.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.IRV SHAPIRO91them to school.  Teachers are expected to teach more than reading, writ-
ing, and arithmetic.  We also expect teachers to make decisions.  Teachers

have immense classroom autonomy.  In elementary schools, the number

of supervisory staff is small compared to the number of teaching staff,
and teachers in the classroom are mostly on their own.  They decide
Ñwetrust them to decideÑwhat our children should learn each day.  In that
process, they make many selections.The same processes are at work in the elementary school library.  The
library does not have a million books in it.  Even if the school could afford

a million books, having a million books would not be a good idea.  For

example, if a third grader is writing a book report on George Washington
and goes to the library and finds a thousand books on the shelf about him,
the student will sit on the ground and begin to cry.  I have four children; I

know this to be a fact.  School librarians and teachers select books for the

library under the direction of the school board, state and federal stan-
dards, and recommendations from organizations.14.2HISTORICAL PERSPECTIVE
The challenge is how to provide the tools that teachers need to leadand teach children in the Internet age.  The present rate of technology

change is unprecedented in history.  The impact of information technol-

ogy is comparable to the impact of GutenbergÕs printing press at the endof the 1400s, but today the impact is being manifested over several years
instead of several decades.How do we empower teachers?  Let us look at the last 30 years.  Over
the last 10 years, there has been universal agreement that the economy
has been robust.  Even with the adjustments occurring now (I am no ex-

pert, and I do not know if they are permanent or if this is a recession),
times have been good for 10 years.When economists looked at this period of time, they were baffled ini-tially, because, as I learned years ago in Economics 101, you cannot have
both low inflation and low unemployment.  You cannot have robust

growth, low interest rates, and a full employment economy.  Those things

do not happen together; they have to be kept in balance.  The Federal

Reserve Board kept them all in balance, and taxes kept them in balance.
Economists eventually concluded that there was a dramatic increase in
productivity over that period of time as a result of the introduction of
computer technology into the American economy.  That increase in pro-

ductivity allowed us to produce more goods for less cost.This sounds wonderful.  But I was in steel mills in the mid-1970s in-
stalling computers, and I guarantee you that there was no increase in pro-
ductivity.  When we walked into the mills, they laughed about all the
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.92BUSINESS DIMENSIONS:  THE EDUCATION MARKET
people they were going to have to hire to take care of the computers doingtheir payroll, general ledger, and accounts receivable.  Maybe the com-

puters were controlling a couple of machines, monitoring temperatures of
furnaces, and doing process control, but there was no increase in produc-
tivity.Let us assume, for the sake of argument, that there was no increase inproductivity in the 1970s.  Yet in the 1990s, the economy was robust.  What

changed?  Some very smart people and organizations, such as SAP,

Microsoft Corporation, Apple Computer, and Sun Microsystems, recog-
nized that the computers in the plants, factories, and offices of America
would not account for the difference.  Nor would it come from the infra-

structure.  No, the difference was that these companies began to build

specialized software for industry, and businesses invested hundreds of
millions of dollars in training their workforces.  In the 1970s, we put in

lots of wires and computers; in the 1980s, we introduced new software
designed to revolutionize the process of manufacturing.  The word pro-

cessor changes peoplesÕ lives.
I have two children in college who would not even know how to writea paper in longhand.  This is a technologically revolutionary time.  So

where does technology stand in the schools?  Because of the E-Rate and

other successful programs, we have put lots of computers and wires into
the schools.  But it seems to me that the schools are stuck in the 1970s

because we have not retrained our teachers.  We have not introduced new

software specifically designed for these marketsÑespecially for elemen-tary school.  Instead, we have taken software designed for the business

community, universities, or high schools, and tried to roll it downhill to a
second-grade classroom.Teachers in second grade do not have $5,000 projectors.  They may
have laptop computers, but PowerPoint and Excel are not tools for them.
The teachers need something different.  Thus, the opportunity for the busi-

ness community now is the same opportunity that existed at the end of
the 1970s for the traditional computer and software companies.  There is a

need for software and training in the schools.  There is a need for help

desks so that teachers can pick up a telephone and talk to a real person at
8:00 P.M. or 11:00 P.M. without being put on hold for an hour, as they try toprepare an assignment for the next day.  This is a wonderful business

opportunity, which is why I got involved with it 2 1/2 years ago.
14.3THE SCHOOL MARKETPLACE
The other side of the story is that teachers are scared.  They are under-
paid and overworked.  When a teacher gives our children more home-
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.IRV SHAPIRO93work, the teacher has more homework the next night, too.  They get calls
late at night.  They work in a complex environment.  Quite candidly, the

skills that make someone a phenomenal second-grade teacher are prob-
ably not skills that would enable them to deal with such complexity.
Change in the elementary school education marketplace is difficult, be-
cause teachers do not want anything to do with it.  Our company has been

involved in many districts where the superintendents and principals
brought in a program but the teachers dug in their heels and said, ÒNo,we will not use this stuff.  We do not even want to learn it.
Ó2It will take some time.  Unfortunately, the cost of time is dollars.  In
this economy that has just survived the dot-com world, think about time
in terms of months, maybe a year and a fraction.  When you talk to the

investment community about going into a marketplace in which you may
have to spend 2 years in a sales and educational process, providing edu-
cation at a subsidized rate, the investment community says, ÒThere areeasier places to put our money.ÓWhy should they do it?  Because switching costs
Ñto use economictermsÑin the schools are very high.  Once a program is in a school, it does
not go away.  If I had a magic wand, I would look at how to pump dollars

into teacher education and the creation of software and technology spe-
cifically targeted to this marketplace, even though we know the payback
probably will take 5 years instead of 18 months.  The reason to do it is that

the marketplace is very large.  Look at the PresidentÕs budget and see thelarge numbers going into education.  When you are in the market and are

successful, you provide very good returns to investors.Whether you do that as a nonprofit, whether the government does it,or whether the government provides funds to a for-profit to do it, it is a
fundamental issue.  As a for-profit attempting to address that need, we

find it very difficult to raise capital, because the return on investment
takes longer than the current capital markets want.  This is not just a pri-

vate market problem.  Look at the allocation of federal funds.  As an ex-

ample, E-Rate was strictly a program for lines and hardware.  The way

you get Title 1 dollars to apply to technology is to repackage the technol-
ogy as reading, math, and basic learning.  The overall challenge is to find
a way to retrain the teachersÑnot put dollars into curriculum, hardware,2Sandra Calvert said teachers today are expected to do much more than teach.  They are
expected to solve social problems, such as parents getting divorced.  Then the computer is

thrown in.  A teacher using a computer to give a presentation needs to become a technical
expert in case something goes wrong.  If it breaks down, then usually a whole classroom of
kids is left sitting there, because technical support is seldom available in the classroom.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.94BUSINESS DIMENSIONS:  THE EDUCATION MARKET
and lines, which is where I see the majority of the dollars going.  Some
federal money is targeted specifically to professional development, but
look at the order of magnitude difference between professional develop-
ment and hardware and infrastructure.Over the past 2 years, many businesses looked at the size of the pot inthe education marketplace and attempted to fill the gap by using adver-
tising revenues or other nontraditional revenue sources.  They failed.  We

are left with two models, which may be fine.  Very large corporations

have a vested interest in the current model.  They would like teachers to

use textbooks in the exact same way as in the past; they are not interested
in the technology changing too rapidly.  These parties have deep pockets,

which is okay.  There is also the continual opportunity marathon, in which

someone can start a small business and leave it as a small business.  In a

number of sectors of elementary schoolsÕ infrastructure, there are many
small Òmom and popÓ operations that never grow beyond serving the
technology needs of a couple of communities.TeachersÕ unions have no effect, positive or negative.  In the long term,
they could have a slight positive influence.  But in the situations that we

have seen over the past 30 months, this has rarely been packaged as a
union issue.  Every once in a while we hear, 
ÒOur contract is coming up in6 months and we do not want any change until the contract is renegoti-
ated.Ó  There are many fearful teachers out there, and getting them over
that fear is as massive an undertaking as the complete E-Rate undertak-
ing.  This is much more expensive than what we have done on the tech-

nology side.  Unions could be a positive force in helping their member-

ship to overcome this fear.3There is another positive force coming.  The statistics indicate that
about 50 percent of the teachers in America are approaching retirement
age, and as many as 50 percent will retire over the next 5 years.  The

people going into those jobs probably recently came from universities
where they got all of their homework online and computers were used
transparently, so they may demand this in the schools.  Teachers become
3Janet Schofield suggested that unions could be helpful in negotiating, for example, dis-counts for teachers buying home computers.  In studying teachers, she has found that, if
they have computers at home, they are more likely to get over their initial reluctance.  Maybe

sons or daughters train them, and they have more time in the home environment.  Unions
could reduce the economic barriers and create centers for their members to get home com-puters.  She also suggested that teacher training relates directly to other issues at hand.  For
example, teachers seldom know enough about the Internet to realize how they might pre-pare kids to surf safely and responsibly.  Teachers may not know how to locate good sites
that will draw the kids in.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.IRV SHAPIRO95obsolete because we have not done our job of training them.  If we had
done our job better as a society of providing teachers with the expertise
and training that they needed, then society would not have to solve this
other issue of kidsÕ access to inappropriate material.  Teachers are very
influential, at least with very young children.We need to develop a business model that takes a patient approach tothe retraining of the teacher workforce.4  Over the last 9 months, we have
held training in 200 schools in Internet access, how to select good sites,
how to use our particular tools, and a variety of related topics.  We no

longer will be doing on-site, in-service training.  Instead, we are moving

to a model in which we will train a trainer in the school and provide a
variety of multimedia materials for the teachers.  We have found that what

is most effective with teachers is Òjust in timeÓ training, rather than bring-
ing them into an in-service for a day at the beginning of the year and then
4 months later when they go to use the materials.  Providing that type of

training and support mechanisms is expensive.  It is a challenge to de-

velop business models that will support the teachers so that they can pro-
vide the education that will cut down on some of the bad things that hap-
pen in this networked world.4Marilyn Mason said that when libraries began using the Internet, entire staffs were re-trained.  Librarians are neither more nor less reluctant to use technology than are teachers.
But if a library had something very specific that it wanted the staff to do, and if librarianssaw this as a way to make their jobs easier and make themselves more effective, then they
could embrace the technology as a new tool.  The education profession has not sorted out
how the Internet can be a tool for improving education.  Mason suggested looking at where
one can intervene in a cycle.  One opportunity may be the emphasis on test scores, because

they provide some measure of effectiveness.  There are software packages that help children
learn to read, and they can be effective if used in libraries.  The key is to make sure there is
a common understanding of how teachers are supposed to use technology.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.9615Business Models:Kid-Friendly Internet BusinessesBrian PassUntil yesterday, I was president, chief executive officer, and co-founder of Passport New Media, which created a product called ÒYour
Own WorldÓ (YOW for short), stand-alone software designed to enable
children to experience third-party Internet content in a protected, offline
environment.  For parents, we offered peace of mind that their kids, when

using our software, would never be exposed to the dangers of the Internet.
For kids, we dramatically improved the performance of the Internet by
eliminating bandwidth constraints and putting all of the content on the
personal computer (PC).We founded the company in January 1999.  We were a year in devel-
opment, building this software from scratch.  We launched the product

last spring but, when we went to raise our third round of capital and
market the product nationwide, we were hit by the financing problems
that face many companies these days.  Bankruptcy papers were filed just

yesterday.  Nonetheless, we are proud of the product, which drew a lot of

praise from parents, especially, and from critics who covered the space.I am also a father of two girls aged 5 and 7, and many of my com-ments are informed by the fact that I am a concerned parent.15.1BUILDING AN INTERNET BUSINESS
What are the primary challenges of building a business based on theidea of attracting kids to safe and appropriate Internet content?  Building
any Internet-based business is difficult, but especially in the kidsÕ space.
The kidsÕ companies suffer from all the same problems that the adult-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BRIAN PASS97content companies do, but the problems are exacerbated.  The problems
are not necessarily different in nature, except for the safety area.The first and biggest challenge is the Internet itself, which is not nec-essarily an effective medium for young children aged 2 to 12, especially
for those under 10. The bandwidth constraints pose one of the most sig-
nificant problems.  Even at broadband speeds, children find content com-

ing over the Internet frustrating.  Adults do, too.  If you try to watch a

video or animation, especially over a dial-up connection but even over
broadband connections, the experience is not pleasant.  It is tolerable for

adults but becomes intolerable for kids.  This is a business challenge be-

cause of the competition.  You are competing with TV, video games that

perform extremely well, and PC software that works well.  When you

click on a PC game, something happens right away; the same cannot be
said for content coming over the Internet.A snowballing series of other business challenges arise out of thesebandwidth constraints.  There are creative limitations on what you can do

in a space.  If you want to do something that works well over the Internet,

chances are you will make creative sacrifices that make your content fare
worse than your competition.  This applies to entertainment-based con-

tent and educational content.  Our product was somewhere in the middle,

in the edu-tainment space.  The creative trade-offs pose real challenges.
Many companies have tried to develop original educational contentand deliver it exclusively over the Internet.  For example, MaMaMedia in

New York tried to create bandwidth-intensive educational (but fun) con-
tent for kids.  They were challenged from a business perspective because

they spent a lot of money marketing this product.  There was a major

mass-advertising campaign of which my kids were well aware; they asked

me if we could buy Fruit Roll-ups so that they could get the secret code
for a game on a MaMaMedia siteÑnotwithstanding the fact that they arenot allowed on the Internet and have never seen MaMaMedia.  This was a

successful campaign and it drove millions of unique visitors to the site.
But from a business perspective, those kids did not visit the site often or
stay very long, and the performance results were probably among the
worst in the industry of the companies that I am aware.At Passport, we tried to address this very issue by bringing the con-tent off the Internet and making it perform well.  As a consequence, we

did not have the same problems.  On average, our kids visited 10 times a

month and stayed 25 minutes each time they sat down, about 10 times the
industry rate (kids visiting less than twice a month and staying maybe 25
minutes during the entire month).  We had other problems, but band-

width clearly is holding kids back from embracing the Internet in impor-
tant ways.The other major limitations of the Internet include the safety and pri-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.98BUSINESS MODELS:  KID-FRIENDLY INTERNET BUSINESSES
vacy concerns.  I will address them from a business perspective.  The first
issue is the cost of complying with regulations.  The Children
Õs On-LinePrivacy Protection Act governs this space.  There have been many discus-

sions since the law was enacted about the costs, in dollars, that these regu-
lations impose on content providers.  These are just some of the costs of

doing business in this space.The more important cost is the primal fear factor.  I do not wish to
question parentsÕ judgment, because I share a lot of those concerns.  But
parentsÕ fear of the Internet makes it a less than great medium for the
simple reason they do not allow their younger kids online in great num-
bers.  (I am not referring to teenagers, who embrace the Internet in much

higher numbers.)  When you combine this fact with the unpleasant, band-

width-constrained online experience for kidsÑif they are allowedonlineÑit explains why fewer than one in three kids who have Internetaccess at home are actually online.  (This number does not include kids

who access the Internet from schools.)Another major challenge to a business seeking to provide content tokids in a safe way is financing.  This is obviously the biggest issue facing

Internet companies of any type today, but even when we got started in
early 1999, during the glory days of the Internet, the kidsÕ segment was
difficult for the venture capital community.  I cannot tell you how many

times I was in a venture capital meeting and was told, ÒIt is very difficultto monetize kids.Ó  As repugnant as that sounds, it gets to the heart of the
problem.  There is no bigger challenge than getting a business funded and

off the ground.  Even in the late 1990s, the industries serving children

were not doing especially well.  This includes television production, his-

torically a difficult business, and the CD-ROM business, which is very hit
driven and a difficult retail model.  The Learning Company, then under

Mattel, was struggling in those days, and I read just recently that, since
the company was sold, it has reached the break-even point.15.2COMPARING BUSINESS MODELS
After the stock market crash of last year, I did not hear about the issueof monetizing children anymore in meetings, because I was not getting
any meetings.  I could not have presented a worse business model to the

venture capital community last yearÑI think the same still holds for to-dayÑbecause the model embraces content for kids and has an advertis-ing-supported revenue stream.One might argue that the business case has not yet been made forproviding content to kids in a safe way.  But many people have tried.  The

business models today can be categorized by two variables.  The first vari-
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BRIAN PASS99able is the market that you are targeting, such as kids in the home, theconsumer market, or kids in schools.  These are different markets and are

dividing lines among business models.  The second variable is the rev-

enue model, whether ad-supported or fee-based subscription or licens-
ing.  I am excluding e-commerce.
If you constructed a matrix using those variables, you would haveconsumer ad-supported companies, consumer subscription-fee compa-
nies, school-based ad-supported companies, and school-based subscrip-
tion-fee companies.  We were in the first of those four categories, with a

consumer product for the home supported by advertising.  Other ex-

amples of this type are MaMaMedia, Zeeks, FreeZone, and probably a
host of others.The problems here with the business case are similar to those facingsites for adults:  the high cost of creating content, slow acceptance by ad-

vertisers, and limitations of the Internet medium with respect to advertis-
ing.  Not only does it make for a poor entertainment content experience,

but it also makes for a poor advertising experience.  The traditional form

of advertising on the Web is a banner ad, which you click and it takes you
to another site.  For a kid, especially over a dial-up modem, that form of

advertising is a nonstarter.  The kid gets lost when transferred to another
site.  Even the content provider loses out, because now the kid is no longer

at the original site.  It is a losing proposition all the way around.
We tried to address this problem with offline capability.  Instead of
kids clicking on a banner ad and going to another site, they got a rich
media pay-off right away.  They could play a game instantly.  They could

watch the full, 2 1/2-minute Rocky and Bullwinkle movie trailer behind abanner ad that played in real time with no bandwidth constraints.  Not

surprisingly, we got a very high response to that ad.  But with a small user

base, you cannot make a lot of money doing this.  This was our big chal-

lenge; we could not build a base big enough to get large advertisers on
board, even though they were excited about the product.  We did not

have enough kids for them to reach.  We did not build the base quickly

enough before we ran out of cash.  Timing is everything, and that had a

lot to do with it.There are many examples of the consumer-subscription model in thekidsÕ space, such as JuniorNet, probably the closest technically to what we
were doing, and Disney Blast.  These companies have tried to offer sub-

scription-based services to kids in the home, such that the subscription
takes the form of a monthly or yearly fee.  The problem is that the sub-

scription model never has worked for any Internet company, as far as I
know.  Many people have tried to charge for content, but people at home
feel that Internet content should be free of charge.  This has been the fun-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.100BUSINESS MODELS:  KID-FRIENDLY INTERNET BUSINESSES
damental problem of the Internet for all companies, not just those cateringto kids.An example of a school business model that adopted an advertisingapproach would be Zap Me, which offered to wire schools and build in-
frastructure in exchange for being able to advertise or market to children
in those schools.  This brings up difficult issues in terms of the commer-

cialization of schools.  Zap Me found that it was unworkable and the com-

pany no longer deals with schools or kids; it is now offering network ser-
vices under a different name, rStar Networks.The fourth model in the matrix is school-based services that use asubscription or licensing model.  This is the predominant model.  Class-

room Connect, Light Span, and others have developed online, fee-based
services for schools.  We have heard a lot about the obstacles and difficul-

ties of working in schools; I will highlight just a few.One difficulty is the great variability in how networks and computersare structured. Every school is a little bit different in ways that affect how
you bring content into that school.  Statistics show a very high penetration

of Internet access in schools, but I doubt that any one school is like any
other in the way that kids use and experience the Internet.  Some have

computers in the classrooms, others have them only in the library, and
still others have a separate computer lab.  This makes it very difficult to

create curriculum-based content.In addition, there is an underlying assumption that learning from thePC or the Internet is a good thing, especially in schools.  This remains to

be shown.  I believe that, on the whole, my kids are better off.  They are

learning to use software and have had positive experiences on computers.
But at least some studies suggest that this is not necessarily a good thing,
so this becomes a barrier to successfully putting content into those schools.
Ultimately, the successful model (if there is one) will do the followingthings:  It will work well within the bandwidth limitations of the Internet.

It will focus on what the Internet does well, which is deliver content and
exchange text.  It will meet the demands of parents.  It will be safe, secure,

and private.  And, above all, it will meet the demands of kids, the tough-

est ones to please in this market.  It will entertain, it will educate, and it

will be well done so that they will accept it.No one has tried yet to shrink-wrap a content-based Web productÑthe publisherÕs model.  CD-ROM developers are trying to incorporate the
Internet into their off-the-shelf products.  We could have shrink-wrapped

our product and put it on a shelf.  But at the time, we looked at the compa-

nies doing this and saw the difficulties that they were having.  The Learn-

ing Company and others in the educational space had difficult distribu-
tion models and had to provide incentives for purchases by offering very
substantial rebates.  The publishing model was not attractive to us at the
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BRIAN PASS101time.  Maybe Netscape tried this model when they first introduced the
Navigator.1There are also other issues.  One is whether a company in this space
can be grown organically while avoiding some of the venture capital fund-
ing issues.  It probably can.  Somewhere, there is probably someone cre-

ative enough to make their own educational or entertainment content,
post it on the Web, and build a business that can pay for itself over time.  I

was not smart enough to go about it this way, but I think someone may
succeed.Sadly, some of the best sites for kids on the Web are probably thecommercial ones pushing products.  Nabisco, LifeSavers, and Kellogg
Õsare examples of dynamic, well-done sites that exist purely to promote
products.  They have great activities.  The most popular game that circu-

lated around our office was a Tetris-like game with Fruit Loops; it was a
lot of fun.  Unfortunately, this is where the money is.  They have a differ-
ent purpose in bringing that content to kids, and they can afford to create
beautiful stuff.Businesses targeting 12- to 18-year-olds would face a lot of the samechallenges.  The Web applications are different
Ñmore chat, more instantmessagingÑand the content is different.  I have not seen as much educa-
tional content going to teens.  The content is more like the Back Street

Boys, surfing, and skateboarding.  The companies operating in this space
have had very mixed results.  A notable company in San Francisco, Kibu,
recently closed before it ever launched.  Bandwidth is less of an issue for

teens, who are more tolerant than younger kids and understand the me-
dium better.  They are looking to the Internet for different things.  There

are also more homework issues.  Teens who go home and do their home-
work want to do research and access those positive aspects of the Internet.
Any technology change has both good and bad aspects.21Marilyn Mason suggested that this model is going in the direction of a journal for a differ-ent level of reader.2Irv Shapiro said his company targets the ages between very young children and teens,primarily kids aged 6 to 12.  He uses a subscription model paid for by the schools.  His
motivation is simple:  He had good fortune in a previous career, planned to donate about

100 computers to schools, walked around to see how they were planning to use them, andwas appalled.  This led to the creation of Edventions.  The goal was to integrate computers
into schools just as calculators had been integrated into the math curriculum, based on the
idea that children will use calculators to do arithmetic when they become adults.  In the
early years, elementary school math teachers were against any use of calculators.  Now,
calculators are integrated into the curriculum.  A division of Texas Instruments is devoted to

selling calculators to schools.  Similarly, children will use computers as teens in high school
and as adults, so the societal motivation is to find the proper way to provide a safe, secureenvironment for these children to learn about computers.  Shapiro
Õs solution is to try toleverage the talents of teachers to do this.  Sandra Calvert said Dan Geer sent around a
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.102BUSINESS MODELS:  KID-FRIENDLY INTERNET BUSINESSES
There has been a lot more business activity in the teen space, and afew companies have gone public.  Sites like Bolt, Alloy, and Snowball are

really going after this market and these advertising dollars, because teens
have more disposable income.  They can make decisions.  Then the ques-
tions become whether they are staying away from pornography and

whether marketing to them is good or bad.I spoke about a year ago at a conference at which there was a heateddiscussion about the commercialization of the Web and kids.  Someone

asked why there is nothing like a Public Broadcasting System (PBS) for
kids on the Internet.  The discussion went on for about 5 or 10 minutes,

and it was heated.  No one pointed out that PBS is the PBS of the Web
Ñitis out there online.  Maybe not enough people know about it, but this may

be a good model going forward (it is one that I was toying with late in the
game).  We could create nonprofit organizations that license commercial

technology and work in that space, and corporations that want to do good
work can sponsor good educational content.  We can have something like

PBS; it is not out of the realm of possibility.In the course of licensing content from major media companies and indealing with their kidsÕ divisions in separate Internet operating groups, I
did not think those separate Internet groups did very well.3  My sense is
that Nickelodeon, for example, went through two or three massive
restructurings of its Internet group over the last 2 years.  Another example

is Warner Brothers, whose online site just folded itself back into the com-
pany.  Fox is withdrawing from having separate Internet divisions, in-

cluding Fox for Kids, and wrapping them back up in the network.  Televi-

sion is a great driver.  But it is interesting that sites like Nickelodeon or

Fox for Kids do no better than the industry averages in terms of repeat
visitation and total minutes of use.  The media company is making money

from the TV show and not necessarily from the Web.  They are not that

different from Life Savers, which is promoting products online and doing
it well.memo about the use of calculators, especially among minority children, who do not under-stand the fundamentals of math but can use a calculator.  This approach needs to be tem-

pered with more basic knowledge.  Calculators alone are not a magic bullet for doing math.
3Winnie Wechsler suggested that Web sites linked to television networks or other pre-existing media seem to do well.  Whatever her kids watch on television, they also use on the

Internet.  In other words, the business model that works involves a Web site that augments
viewership on television, which, in turn, draws traffic to the Web.  To address the problem
of drawing traffic, what is more powerful than a 24-hour ad on television?Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BRIAN PASS10315.3THE ROLE OF PARENTS
The question of how to deal with inappropriate material goes back tothe role of responsible parents.  This burden falls on parents, teachers,

and librarians by default because the technologies are not strong enough,
and the regulatory responses generally run into First Amendment issues
about free speech and have a tough time in the courts.  By default, respon-

sible adults have to stand up and take the lead in combating inappropri-
ate material.The central role of responsible adults is the reason why, as business-men, we made a product that would appeal to parents as the primary
decision makers.  We demonstrated with the product adoption rates that

there is a lot of demand for solutions from parents.  Parents are concerned;

they want their kids to have a positive Internet experience, and they are
searching for solutions.I do not let my kids go on the Internet without my presence.  Of
course, they are young (5 and 7), so we will see how vigilant I am in 2 or 3
years.  I have a cable modem, and my kids are examples of how band-

width constraints are a problem.  Even when my kids go with me online

and we look at something together, they get frustrated and go back to
their rooms to play with Barbie dolls.  The Internet is slow.
There is concern about whether we want 2- and 3-year-olds on theInternet. By being offline, we could make a completely simplified inter-
face that could be used by 2-year-olds, who did use our service without
knowledge of how to use the Internet.  I will not say whether this is right
or wrong, but the childrenÕs educational software industry targets kidsstarting at that age and even younger. A year or two ago, The Learning
Company introduced software that teaches toddlers how to bang on key-
boards.  My kids were using the computer with multimedia software at 18

months.  They are not gifted children.  But they happened to be the types
of kids who would just as soon be playing outside and would do a little of
both.  But this is a concern, and it goes back to the assumption that the

Internet is a good medium for educating kids.  That assumption should be

challenged.44Sandra Calvert said the issue should be researched.  The discussion points to the lack of
a database on whether and how little kids should use the Internet.  She has seen 4-year-olds
who have been online for 2 years, and they are not Òhunched over.Ó  They are curious; theywant to know where the ÒBackÓ button is.  They are knowledgeable about the Internet.  She
does not think it is damaging them, but she would pay attention to the sites they visited andwhether their parents were with them at the time.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.10416Business Models Based on Advertising
Chris KellyMy presentation will focus on the business models for advertisingand commerce on the Internet, still viable despite the general pessimism
about the way things are going on the Internet these days.  All of the big

players have had problems.  But there will be a workable business model;

the question is how to figure out what it will look like, and how those
models can be put to use in protecting kids online.16.1COMPARISON OF ADVERTISING MODELS
Advertising will continue to be a significant part of Internet businessmodels, despite what you may hear.  There are four basic models for the

sale of advertising.  The most common models are cost per impression

and revenue share, although cost-per-click and cost-per-acquisition deals
are gaining in popularity.Cost-per-impression (CPM) deals are usually experienced as bannerads while you surf the Web.  You go to a site such as Excite, and the

banner ad is presented to you as part of the page.  This is still the bread

and butter of the industry, the way most sites generate their major rev-
enues, but it is in serious trouble.  Every major Internet portal has seen a

serious decline in revenue coming from advertising, and offline businesses
dependent on advertising revenues have seen similar thinning.When banner ads first came out on the Internet, people clicked onthem 15 to 20 percent of the time, because nobody knew what they were
and everyone was trying to bounce around and figure out this exciting
new medium.  Things have stabilized now to below half a percent in terms
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.CHRIS KELLY105of click rates for a basic banner ad.  This has been a disaster in terms of
convincing offline advertisers to move some of their budgets onlineÑan
effect that everyone has seen on the Nasdaq.  In talking about these low

clickthrough rates, I am referring to run-of-the-mill ads; I will discuss tar-
geting later.Because of this lower perceived effectiveness, a few other models aregaining greater prominence, such as Òcost per click.Ó  Instead of paying

for the presentation of your product in a banner advertisement, you pay
for the actual clickthrough on the ad.  This is less popular and more diffi-
cult to negotiate, because Internet networks are reluctant to accept these
deals.  They say, ÒIf you pay us only on a conversion, on a move, on a

redirection to your site, then we cannot forecast what the revenue from
this deal is going to be.Ó  Advertisers (i.e., ad space owners) are looking

for guaranteed paymentsÑgenerally targeted banner ads.Cost per lead is a slightly different model.  A lead is a conversion so
that someone agrees to provide a service or to accept to further direct mail
or e-mailÑroughly analogous to the response card in a magazine that
says, ÒCircle here for more information.ÓThe revenue share, as I mentioned earlier, is also a popular type ofdeal.  The problem with revenue share deals is that you are depending on

actual commerce to pay the bill.  If there is no transaction at the end of the

day, then revenue does not flow back to the advertising presenter, who is
thus not happy about the way the ad space has been used.16.2PORTALS, ADVERTISING NETWORKS, AND TARGETING
In discussing advertising-based business models, itÕs important tonote that the big playersÑAmerica Online, Excite@Home, YahooÑsell
many of their own ads but not all of them, which is important.  We have

an ad sales force that spends a lot of time going to large advertisers and
saying, ÒFor x million dollars, you can get this many impressions on our
network.  They will be on these particular channels on the network.Ó

Smaller players and some of the big ones outsource that type of ad sales to
ad networks.  The biggest one is Double Click.  Other large ones are

MatchLogic, a wholly owned subsidiary of our company; Engage; and
24/7 Media.  These are third-party networks that operate on a variety of

sites across the Internet.  Double Click has 2,500 to 3,000 sites from which

it serves ads across the Internet.  Match Logic has about 1,000 sites.  A big

concern is the placing of cookies on userÕs browsers and computers, to
track behavior across those different sites.Targeting is, in many ways, the Holy Grail of the industry.   Most ad
targeters use profiles based on your behavior across a number of sites
within an hour.  If you visit 10 or 20 of the 2,500 sites within a Double
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.106BUSINESS MODELS BASED ON ADVERTISINGClick network, then you get scores associated with each site indicatingmale or female, likely age, presence of children in the household, and
other things like that.  Once that profile is established, when you visit a

site where ads are served by Double Click, it will read the cookie on your
browser and say, ÒThis person is probably between 24 and 35, probablyhas kids in the household, is probably female, and may have an interest in
X.Ó  Then you get served an ad that Double Click has sold to an advertiser
that matches this demographic profile.These are usually anonymous, which is an important point.  This is
one of the biggest sources of confusion and discussion in the privacy
arena.  The Federal Trade Commission (FTC) took action against Double

Click because the company had plans to start personally identifying with-
out user permission.  As it turned out, they never did that and the FTC

inquiry was properly stopped.  They had planned something that prob-

ably would have violated the law and it would have been a false incentive
advertising practice.  But they did not do it.
All of this happens because of the need to drive the click rates up, toactually reach the people that you are trying to target.  To the extent that

these things are done anonymously, they are, arguably, wonderfully ben-
eficialÑand one of the business models that will work.  If you can get to
the types of people that you want, then it is much easier to present to an
advertiser who has x number of dollars to spend to reach this audience,
and say, ÒYou should pay this rate, this CPM or whatever, to get thesepeople.  Because we know, based on the technology that we
Õve set up,that we can get to people who meet these characteristics.ÓA number of companies have tried to generate revenues this way.  I
am sure that a number of the big networks are very involved in ad target-
ing.  This is similar to what grocery stores have been doing by giving out
discount cards.  The major difference is that the grocery discount cards

have personally identifiable data, so that they can send you coupons in
the mail.16.3CHOICE OF MODELS
Different types of Internet content providers favor different ad mod-els.  The quintessential example of the lengths to which some companies

will go to drive traffic is that, if you end up accidentally on a porn site,
you cannot even close your browserÑthe site just keeps showing up.Mainstream advertisers are starting to use these technologies, too; if you
try to close a window, then ads pop up on a number of different sites.
Without having done a full economic study of the porn industry, I cannot
say this definitively, but my guess is that they will get hit with some of the
same advertising doldrums that everyone else has.  The ones making
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.CHRIS KELLY107money are probably the ones with subscription models.  Porn seems to be
one of the few things that people will pay for.  The problem in avoiding

the content is probably related to promo pages, which are designed to
draw people in to pay for a subscription.  Filters definitely need to catch

those pages.Most nonporn sites are not trying to show pictures or video, just ani-mations and banner ads, so there is less concern about bandwidth cost in
the presentation of screens.  One reason why the ad networks have man-

aged to prosper is precisely because their costs are so low.1  There is a
high cost to build servers to push things out and to negotiate the first
arrangements with Web sites to build them into the network.  But once

that happens, you can just serve it out.  You added potential customer

leads and lowered your customer acquisition cost by expanding your net-
work, because you can send a cookie when a new browser visits a site that
has, for example, a Double Click ad.  That unique identifier will be carried

across every site in the Double Click network and be registered in Double
Click.  High start-up cost and low marginal cost make a big difference in

terms of overall advertising cost.16.4ADVERTISING, REGULATION, AND KIDS
There are many questions to be asked about advertising as a modelfor paying for software or services that would protect kids.  The biggest

player in filtering in the schools has now abandoned advertising despite
the potential for real benefits in terms of a business model and potentially
modifiable ad space that could pay for technology that would help to
avoid indecent material.  What drives these choices are worries about pri-

vacy.  The Children
Õs Online Privacy Protection Act (COPPA) requiresparental permission for any personally identifiable information collected1Brian Pass said that, when his company delivered large, rich-media adsÑsuch as themovie trailer mentioned in his presentationÑbandwidth costs were an issue, because theentire file was shipped all the way to the userÕs computer on a nightly basis.  If rich-media
technology starts to take hold in advertising structures, then bandwidth costs will be a fac-
tor.  The myth that bandwidth is so inexpensive
Ñthat it is effectively unlimitedÑcausesengineering decisions to be made.  Milo Medin said market data show that retail pricing for
Internet transport runs about $400 monthly for one megabit per second.  A new entrant

might get a competitive price in the range of $200.  If a site draws a lot of traffic, then net-
work providers discount substantially.  For example, a Yahoo co-location facility might pay
only $50, even without fiber-optic systems.  If a company is willing to put content into a

hosting facility that a network charges for, then the network virtually gives the bandwidthaway because it provides leverage in interconnection discussions.  Over the long term, the
price probably will stabilize at about $150, Medin said.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.108BUSINESS MODELS BASED ON ADVERTISINGabout children under 13 and thus severely limits business models thatwould target kids.A number of other potential privacy laws and regulations also arecoming that could affect the choice of advertising-based models for online
safety efforts.  One is self-regulation by the industry through the Network

Advertising Initiative (NAI), part of a response to the Double Click rul-
ing.  A number of industry players, including Match Logic, Double Click,

24/7 Media, and Engage, got together to find a fair way to give people
notice if we want to merge personally identifiable data with ad informa-
tion.  The group came up with strict permission and self-regulatory stan-

dards.  They worked and negotiated with the FTC to establish these stan-

dards, which were unanimously approved by the FTC and sent to the
Congress and are now in force.In discussing the data models that advertisers use and particularlythe potential effect on a childrensÕ market, the meaning of 
ÒpersonallyidentifiableÓ is a huge issue.  The question is how far you can move back
up the chain to make data personally identifiable.  According to the NAI,
there will not be a move to make data nonanonymous without permis-
sion.  If a hacker took the information and could match it geographically,
then perhaps this could be done without permission, but it is difficult to
get all the crumbs together and link them back to an actual person.  Per-

sonally identifiable information usually is defined as information to be
used to contact an individual directlyÑsuch as full name and physicaladdress.  E-mail address generally is defined as personally identifiable as

well.  Some interesting discussions are going on in the European Union

about whether Internet Protocol addresses should be considered person-
ally identifiable information.  It is always difficult to figure out what will

happen in the EU and which body is acting on which day.Senators John McCain and John Kerry have proposed privacy legisla-tion that would require Web site notice, which would affect potential
childrenÕs advertisers along with everyone else, in terms of fully disclos-ing the facts and the privacy laws.  There are also a number of other pos-

sibilities.  Some in the industry favor a weakening of COPPA because of

its effects in cutting off under-13s from a socially beneficial communica-
tion source.  Our network does not favor a weakening of COPPA.  But it

has a real effect on our site.  We have completely cut off under-13s from e-

mail and chat, because these mechanisms can be used to spread person-
ally identifiable information, and the costs of getting parental permission
and maintaining verifiable parental permission were not justified by the
revenue.  Kids on our network can get to the personalization features and

use them, but we keep only the first name and birth dateÑeverything elseis deleted.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.CHRIS KELLY109On privacy, including kidsÕ privacy, the corporate position that we
have taken is that we are comfortable with further enforceable regulations
saying what companies can and cannot do, as long as they are done care-
fully and do not forbid legitimate consumer-serving uses of data.  Self-

regulation, in which companies talk about their practices and expose
themselves to both public scrutiny and government scrutiny for false and
deceptive trade practices, will also be a major part of coming up with a
privacy solution.  There also will be new technology, which is the 
x factor.
Some technologies will allow complete masking of information and cov-
ering of footsteps.  This is difficult to implement.  A number of advertisers

will rely on the fact that people will find it difficult to use.  Furthermore,

not everyone wants to be anonymous at the end of the day.  For instance,
you want toothpaste if you run out.  It is okay for most people that

Webvan knows that fact because you want it to bring the toothpaste so
that you do not have to leave home or worry about it.  You want your

refrigerator company to know when your compressor isnÕt operatingproperly so that it can come out and service it.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.11017Constitutional Law andthe Law of CyberspaceLarry Lessig17.1INTRODUCTION
I am a professor at Stanford Law School, where I teach constitutionallaw and the law of cyberspace.  I have been involved from the beginning

in this debate about how best to solve the problem of controlling childrenÕs
access to pornographic material.  I got into a lot of trouble for the posi-

tions I initially took in the debate, which made me confident that I must
be on to something right.This is, necessarily, a question about the interaction between a certaintechnological environment and certain rules that govern that environ-

ment.  This question about childrenÕs access to materials deemed harmful

to minors obviously was not raised for the first time in cyberspace; it was
raised many years prior in the context of real space.  In real space, as

Justice OÕConnor said in Reno v. ACLU,
 521 U.S. 844, 887 (1997), a majority
of the states expressly regulate the rights of purveyors of pornography to
sell it to children.  This regulation serves an important purpose because of

certain features of the architecture of real space.It is helpful to think this through.  You could suppose a community
that has a law that says that if you sell pornography or other material
harmful to minors, then you must assure that the person purchasing it is
above the age of 18.  But in addition to a law, there are clearly also norms
that govern even the pornographer in his willingness to sell pornography
to a child.  The market, too, participates in this zoning of pornography

from children; pornography costs money, and children obviously do not
have a lot of money.  Yet the most important thing facilitating this regula-

tion is that, in real space, it is relatively difficult to hide the fact that youTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.LARRY LESSIG111are a child.  A kid might use stilts and put on a mustache and dark coat,
but when the kid walks into a pornography store, the pornographer prob-
ably knows that this is a kid.  In real space, age is relatively self-authenti-
cating.This is the single feature of the architecture of cyberspace that makesthis form of regulation difficult to replicate there.  Even if you have ex-

actly the same laws, exactly the same norms, and a similar market struc-
ture, the character of the original architecture or technology of cyberspace
is such that age is not relatively self-authenticating.17.2REGULATION IN CYBERSPACE
The question, then, is how to interact with this environment in a waythat facilitates the legitimate state interest of making sure that parents
have the ability to control their childrenÕs access to this stuff, while con-tinuing to preserve the extremely important First Amendment values that
exist in cyberspace.  The initial reaction of civil libertarian groups was to

say the government should do nothing hereÑthat if the government didsomething, it would be censorship, which is banned by the First Amend-
ment.  Instead, we should allow the private market to take care of this

problem.Although the U.S. Congress passed the Communications Decency Act(CDA) of 1996, there is fairly uniform support among civil liberty organi-
zations to strike it down for that very reason.  When Bruce Ennis argued

this case before the Supreme Court, he said, ÒPrivate systems, these pri-vate technologies for blocking content, will serve this function just as well
as law.Ó  And the Court avers the fact that there exists private technology
that could serve this purpose as well as law.But the thing to keep in focus is that just as law regulates cyberspace,so does technology regulate cyberspace.  Law and code together regulate
cyberspace.  Just as there is bad law so, too, there is bad code for regulat-

ing cyberspace.  In my code-obsessive state of California, we say there is

bad East Coast codeÑthis is what happens in CongressÑand bad WestCoast code, which is what happens when people write poor technology
for filtering cyberspace.  The objective of someone who is worried about

both free speech in cyberspace and giving parents the right type of control
should be to find the mix between good East Coast and good West Coast
code that gives parents this ability while preserving the maximum amount
of freedom for people who should not be affected by this type of regula-
tion.In my view, when the civil liberties organizations said governmentshould do nothing, they were wrong.  They were wrong because it cre-

ated a huge market for the development of bad West Coast codeÑblock-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.112CONSTITUTIONAL LAW AND THE LAW OF CYBERSPACEing software, or censor-ware, which made it possible for companies tofilter out content on the Web.  The reason I call this type of technology

Òbad codeÓ is that it filters much too broadly relative to the legitimate
state interest in facilitating the control of parents over their childrenÕs ac-cess to materials that are harmful to them.There is a lot of good evidence about how poorly this technologyfilters cyberspace: how it filters the wrong type of material.  There are

also more insidious examples of what the companies that release this
software do.  For example, if you become known as a critic of that soft-

ware, mysteriously your Web site may appear on the list of blocked Web
sites, which becomes an extraordinary blacklist of banned books.  The

problem with this blacklist of banned books is that the public cannot look
at it.  It is a secret list
Ña secret list of filtered sites that is being sold to thepublic on account of parentsÕ legitimate desire to find a way to protect
their children.17.3POSSIBLE SOLUTIONS
My view is that there is a mixture of government and market actionsthat could help facilitate the type of control that parents deserve while
minimizing the bad effects of this West Coast code.  I will describe two

versions of it.  One is more problematic; the other is more invasive.
Imagine a browser that allows you to select G-rated surfing.  As the
browser perused the Web, the client would signal to the server that this
person wants G-rated browsing.  This means that, if you have material

that is harmful to minors on your site, you cannot serve that G-rated
browser this material.  The necessary law to make the regime work is

simply a requirement that sites respect the request that only G-rated ma-
terial be sent to a particular client.  All that is required is that you forbid

people from sending so-called Òharmful-to-minorsÓ material to a browser
that says, ÒI want G-rated material.ÓIf there were such a lawÑand only that lawÑthen there would be astrong incentive for the market to develop many browser technologies
that would signal efficiently, ÒI want G-rated material.Ó  A family in a
particular house could have many different accounts on the browser, so
that children have G-rated accounts and the parents do not.  The market

would provide the technology to make that system work.One problem with this system is that, by going around and raisingyour hand and saying, ÒI want G-rated browsing material,Ó you are also
saying, ÒI am likely to be a child.Ó  People who want to abuse children can
then take advantage of that hand-waving in ways that we obviously do
not want.  There is a way around this problem, but let us move to the

second solution, which I think solves it more directly.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.LARRY LESSIG113Imagine a law that says, ÒYou must, if you have a Web site, have acertain tag at the server or the page level that signals the presence of ma-
terial that is harmful to minors.Ó  This is the type of judgment that book-
stores have to make now.  It is not an easy judgment, but it is one already

entrusted to booksellers today.  An incentive is thereby created in the

market for the development of a G-rated browser, but this time it does not
signal its use by a child.  It simply looks for this particular tag.  If it finds

this tag, then it does not give the user access to the Web site.This, too, is a mixture of a certain amount of regulation, which saysÒyou must tag this content,Ó and a certain expectation about how the mar-
ket will respond.  To the extent that parents want to protect their children,

they will adopt versions of the browser that facilitate this blocking on the
basis of age.  To the extent they do not want to protect their children, they

will not use these types of browsers.  But the power either to adopt the

technology to block access or not will be within the hands of parents.
Obviously, browsersÑat least in the current browser warÑare inexpen-sive; Microsoft has promised they will be free forever.  Thus, the cost of

the technology implemented from the parentsÕ side is very low.
The advantage to this approach is that the only people blocked by thissystem are either parents who opt to use the blocking or schools that adopt
browsers that facilitate blocking to protect children from harmful content
while at school.  It does not have the over-inclusiveness problem that the

other solutions tend to have.  Because the incentive is structured so that

all we need to worry about is material harmful to minors, it does not cre-
ate an incentive to block much more broadly than what the law legiti-
mately can require.1If Geoff Stone2 were here, he would say, 
ÒYes, but arenÕt you forcingWeb sites to speak, by forcing them to put these little tags on their sys-
tems?  And so isn
Õt this a compelled speech, and isnÕt that a violation ofthe Constitution?Ó  I think the answer is no, because the relevant com-
pelled speech is not that you must display on your Web site a banner that1Milo Medin said he likes this scheme because there are many ways of implementing itÑnot only in a browser, but also as a service that a user could buy from a network provider.
The provider would be able to look at the tags as part of the caching process, and peoplewould not be subject to the usual workarounds on the software side.  Another appealing
aspect is that it puts all the people who want to cooperate on one side of the issue.  The other

people do not want to cooperate and do not want their stuff to be restricted.  The question is,
what incentives do these people have?  Many personal publishers, who publish just because
it is fun, would be affected directly by this.  It would not affect the large companies, because

they would act rationally.2Geoff Stone, from the University of Chicago, spoke on the First Amendment at thecommitteeÕs first meeting, in July 2000.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.114CONSTITUTIONAL LAW AND THE LAW OF CYBERSPACEsays, ÒThis is material harmful to minors.Ó  It is not that you must, in anypublic way, advertise this characteristic.  You simply enable the Web site

to label itself properly through the HTML code in the background.  The

Supreme Court has upheld the right of states to force providers of mate-
rial harmful to minors to discriminate in the distribution of this material.
It seems to me perfectly consistent with that opinion to say that sites that
have this type of material must put a hidden tag in it that facilitates the
type of blocking that would enable parents to regain some kind of control.Geoff Stone taught me the First Amendment, so I understand his per-spective toward it.  But I think he is undercounting how this action looks
in light of the other things Congress has done.  There is a certain prag-

matic character to how the Supreme Court decides cases; the court will
not say the Congress can never do anything until the end of time.  This

type of regulation seems to me to be a relatively slight intrusion that
would facilitate a better free-speech environment than would exist in the
absence of any federal regulation.  If we had no federal regulation at all,

the result would be, for example, the blocking of many sites about contra-
ception using private filters.  In this way, the First Amendment world is

worse without this regulation than with it.The necessary condition for success is not an agreement about whatmaterial is harmful to minors but rather what the language of the harm-
ful-to-minor tag would be.  The former would be left to the ordinary sys-
tem of letting people decide what the character of the material is and self-
rating.  The standard imposed by the Supreme Court is that you must

adopt the least-restrictive means.  CDA-1 failed because it was overly

broad in trying to regulate things that were clearly not speech harmful to
minors and because it created too much of a burden on users by requiring
them to carry IDs around if they wanted to use the Web.  I think CDA-2

will be struck down because it continues to require that you carry an ID.
These burdens would have to be borne by everyone who wanted to use
the Web, just so that children could be protected.In my scenario, the burden is borne by Web site administrators, whoalready are spending extraordinary amounts of money developing their
Web pages.  It is just one more tag.  No one can argue that the marginal

cost of one more tag is expensive.  What is expensive is making a judg-

ment about your Web site.  But if you are in the pornography business,

then it is an easy judgment.  If you are in the business of advising children

about access to contraception, then I think it is an easy judgment.  The

Starr report3 is not harmful to minors.  There would be difficult cases, but
the law passed by Congress requires these difficult decisions anyway.3This is a reference to the 1998 report by Independent Counsel Kenneth Starr on PresidentClintonÕs relationship with a White House intern.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.LARRY LESSIG115I envision the G-rating feature as an opt-in setting on a browser.  It
could be a default instead,4 but I contend that if parents do not know how
to turn on the G-rating feature, then they ought to learn.  Constitutionally,

opting out clearly is different from opting in.  The way to analyze the

constitutional balancing test is as follows: is the additional burden placed
on the 100 million people who do not have children and do not care about
protecting children worth the advantage of making sure that the 60 mil-
lion people who do want to protect children do not have to take any extra
steps?  I cannot predict how this type of judgment would be made.  But as

the market develops, people will start branding themselves, much like
AOL has done.  One reason why AOL likes the existing system so much is

that the company draws a lot of parents to its content, because it has taken
many steps to provide for them.Age verification would be performed by the family in switching thebrowser on or off the G-rating setting.  This is the big difference between

this type of a solution and the CDA type of solution, in which age verifica-
tion is done over the Internet.  With age verification over the Internet, the

incentives for cheating are big, so the system needs to be sophisticated
enough to prevent it.My proposal suggests a two-tier system in a library setting,5 with one
tier available to children and either available to adults.  Just as libraries

now might have an adult section that is not accessible to children, you can
imagine having some browsers that are G-rated and others that are not.  It

is difficult to know the libraryÕs role in enforcing the rule on children,however.  Some libraries have adopted the practice of requiring a child
Õslibrary card to be marked.  I am less concerned about libraries enforcing

this rule when only a tiny fraction of speech is being regulated, as op-
posed to many types of speech.  It does suggest some minimal role for

librarians.64Linda Hodge noted that most parents are not using filters and suggested that the G-rating feature be a default, requiring action to opt out.  To disable the G-rating feature, a user
could change the default setting.  Milo Medin said the ISPs supply browsers and provide an

option either at startup or in an upgrade panel that asks the user to Òcheck this or that.Ó5Marilyn Mason said that one of the most troublesome things about the current legislationis that it puts the burden of deciding what is harmful to minors on the shoulders of every
school and library.  She said aspects of LessigÕs proposal are appealing:  the least-restrictivesetting becomes the norm, the list of what is G-rated or not is public, a challenge is a publicevent, public agencies are removed from the middle, and millions of people are relieved of
the burden of deciding what Òharmful to minorsÓ means.
6Marilyn Mason said the tier system could be handled with a library card or smart card.An adult has an adult card so there is no problem.  Children have their parents sign for their

cards.  If a parent wants a child to have unlimited access, then the card can be so coded.  The
cards can be read by machine.  David Forsyth said librarians have told the committee that
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.116CONSTITUTIONAL LAW AND THE LAW OF CYBERSPACEThere are problems with the system I have described, but only withthose involving a state regulation that attempts to guarantee that material
harmful to minors is not handed over to children without the permission
of parents.  My concept is more complicated because it involves the

Internet, but it anticipates the same type of problem that exists in the ma-
jority of states now, when material like this is distributed.Sites would have to do self-rating.  Importantly, the self-rating would
not go beyond this category of harmful to minors.  PICS technology, the

Platform for Internet Content Selection, enables site rating in a wide range
of circumstances.  PICS is the same technology as P3P, the Platform for

Privacy Preferences Project, but is applied to material harmful to minors.
(I am skeptical of PICS because it enables general labeling, which is much
broader than the legitimate interest at issue when dealing with material
harmful to minors.  Its architecture is such that the label or filter can be

imposed anywhere in the distribution chain.  If the world turned out the

way the PICS author wanted, you would have many rich filtering systems
that could become the tools of censors who wanted to prevent access to
speech about China or the like.  My proposal involves a much narrower

label.)To avoid asking a site to slander itself, the label could be an equiva-lent to the one on cigarette packets.  This label does not say, 
ÒI think this isharmful to your health.Ó  It says only that the Surgeon General thinks
cigarettes are harmful to your health.  An equivalent entity could find

material harmful to minors.  The label would not actually say this
Ñitwould be a computer code, of course.  On the other hand, I could reveal

the code and see it, so you might say that this is equivalent to self-slander,
although I am not sure where the harm is.  The label means that the speech

is of a class that can be restricted.  We could make up a word and call it

ÒXYZ speech.Ó  I can be required to block children
Õs access to XYZ speech.The law cannot force me to keep the speech away from my own children.
All this does is improve the vocabulary of the space so that people can
make decisions in a relatively consistent frame.they already monitor library activity and discourage users who are making others uncom-fortable or behaving inappropriately.  It might not be necessary for a library to require chil-dren to identify themselves before using the Internet; the Òtap on the shoulderÓ mechanismprobably can deal with it.  Milo Medin said this approach moves the incentive for labeling or

doing the labor to the content publishers, as opposed to the people who do not want to beaffected.  This localizes the problem and trims a wide range of responsibility.  Labeling
provides the negative incentive needed for the system to work.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.LARRY LESSIG117We cannot simply create a dot-xxx space for material harmful to mi-nors because there are other types of potentially harmful speech besides
hardcore pornography.  Here Geoff Stone would appear in full force, and

I am behind him now.  The fact that you force me to go into a dot-xxx

space is harmful to me if I do not convey hardcore pornography but rather

other material that perhaps should not be given to children.  You are forc-

ing me to associate with a space that has a certain kind of meaning.  If that

were the only option, then maybe it would be constitutionally acceptable.
But there is no reason to force me to associate with the hardcore pornog-
raphers when an invisible filtering/zoning system, such as the P3P labels
in the HTML tag, can be employed instead.  I can be a dot-com and be

tagged.  Some of my Web pages would be blocked to a child, whereas

others would not.  Because I have both types of content, I contend that I

should be free to be a dot-org or dot-com and not be forced into the dot-
xxx ghetto.Of course, a site might take the position that the First Amendmentprotects it in delivering my material to children, regardless of what the
parents think.  The parents might have a different view, thinking they

should be allowed to block access to that site.  The point about this struc-
ture is that the question would be resolved in a public context.  If the

parents believe that this material properly is considered harmful to mi-
nors, and the site refuses to label it as such, then there would be an adjudi-
cation of whether this is material harmful to minors.  I am much happier

to have this adjudication in the context of a First Amendment tradition,
which does limit the degree to which you can restrict speech, as opposed
to a cyberspace board meeting, where the real issue is, ÒHow is this goingto play in the market if people think weÕre accepting this kind of speech?ÓIn my view, we can ensure more protection of free speech if we have that
argument in the context of adjudicators, who understand the tradition of
free speech that we are trying to protect.I want to emphasize that it would be stupid and probably unconstitu-tional to make the requirement to label punishable through a criminal
sanction.  We want to keep the punishment low in order to preserve this

proposed system against constitutional challenge.  To the extent that you
raise the punishment, the Supreme Court is likely to say, ÒThis is too dan-gerous, and it will chill speech if you threaten 30 years in jail because
someone failed to properly tag a site.Ó  Alternatively, I like causes of ac-
tion.  I push this in the context of spam all the time.  A cause of action

might be one in which bounty hunters were deployed to find sites that
they believe are harmful to minors.  They would then employ some sys-

tem for adjudicating this issue.  Then you would get lots of efficient en-

forcement technology out there, for people who really care about this is-
sue, and the enforcement would be enforced in a context in which theTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.118CONSTITUTIONAL LAW AND THE LAW OF CYBERSPACEFirst Amendment is the constraint as opposed to a corporate context inwhich the board worries about public relations.You have to implement this solution step by step.  You have to be
open to the fact that we do not understand well enough how the different
factors interact.  We can make speculations, but we need to use real data

to analyze it, and this requires some experience in taking one step and
evaluating it.  The Web is the first place to worry about.  You could play

with that for a year or more and see what works, and then decide where
else you need to deploy this solution.  Usenet is a network that uses an

NTP protocol.  An ISP can decide which protocols to allow across its net-
work.  It might say, I am a G-rated ISP and will not allow any Usenet

services to come across.  Sometimes people get access to the Usenet

through the Web.  In these cases, you can still require the same kind of

filtering.  It is only in the context of getting access to Usenet outside of the

Web that a problem arises.717.4PRACTICAL CONSIDERATIONS
Let me map out a sample proceeding.  Let us say there has been a
failure to properly tag something that is, in fact, harmful to minors. Imag-
ine that something like a bounty is available.  The bounty hunter brings

an action: hopefully not a federal court action.  In principle, anyone could

bring the action.  The person says, 
ÒThis site by Playboy has material thatis properly considered harmful to minors, and they have not implemented
this tag.Ó  Then there has to be a judgment about whether the material is,in fact, harmful to minors.  A court must make this type of judgment, as

they always have done.  It is difficult in some cases, but the public has

long survived this judgment being made in real space.  If the court finds

that this is material harmful to minors and the site has not put up this tag,
then there would be some sanction.  I think the sanction should be a civil

sanction, such as a fine, sufficient to achieve compliance, that is, set at a7Dick Thornburgh said the person doing the conversion from Usenet to the Web wouldend up doing the labeling, not the person who posts the content.  In this example, the prob-
lem is not difficult to solve.  But the generic issue is that there is some level of restriction onthe connection; it is not necessarily a complete removal of either an intermediary or softwareon the PC, although it greatly facilitates things.  There is no reason why you could not

enforce the same type of labeling requirement on the publisher.  There is usually a way of
labeling files available via file transfer protocol or other types of protocols, for example.  It
could apply to chat groups, instant messaging traffic, and so on.  The key point is to shift the

burden, make it general enough that people have an incentive to cooperate, and enablebounty hunters so the marketplace can police it and you would not necessarily need lawenforcement.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.LARRY LESSIG119level such that a rational businessperson thinks, ÒItÕs cheaper for us tocomply.ÓYou could assume that no one would comply with this law, that therewould be thousands of these prosecutions, and it would bog down the
courts and end up like the war on drugs.  This situation would be similar
to a denial-of-service attack8 and would prove that this system is terrible.
On the other hand, you could assume that people will behave rationally
based on what they expect the consequences and cost of compliance will
be.  Then the world segregates into a vast majority that are willing to

comply because it is cheaper and they do not wish to violate the law any-
way and a smaller number that we have to worry about controlling.A bounty action could be structured so that the first to file gets tolitigate, and, after a judgment is rendered, that is the end of it.  If a frivo-

lous action is filed, it should be punishable by a filing for malicious pros-
ecution.  A class action analogy is possible, but the cumbersome nature of

class actions now might make it simpler to have just a single action.  I do

not think it is possible to eliminate the possibility of a proliferation of
actions, but there are ways to try.  For instance, we could limit it by geo-

graphic district, for example, to avoid the problem of trying to sue some-
one across the country and imposing that type of burden.  A lot of creative

thinking will be needed.  A qui tam action
9 could be troubling constitu-
tionally.  There are people who believe that a party should be found to

lack standing unless there is a demonstration of harm.10  But there is such
a long tradition of qui tam that, like bounty actions, it will survive.The one area of this jurisprudence that has not been developed iswhether and how the community standards component of the traditional
obscenity doctrine applies in the context of material harmful to minors.
There is a need for the courts to figure out something new.  The decision

in the Third Circuit, ACLU v. Reno, 217 F.3d 162 (3
rd Cir. 2000), striking
8David Forsyth sought to draw an analogy to a denial-of-service attack in which a largenumber of people do a small inappropriate thing on a network and overload the system
administrator.  In the legal context, a sufficient number of small bounty-seeking actions
from enough different people would bring the system to a halt.9A qui tam action is one filed in court by a private individual who sees some misconductthat is actionable under the law.  If the individual prevails in court, he or she is entitled to
some of the proceeds that the transgressor must pay.10David Forsyth questioned whether bounty hunters could participate in civil actions,because he thought that some harm had to be demonstrated in order to sue.  Dick
Thornburgh said that, in a qui tam case, the evidence brought forth as the basis of the actionmust be something peculiar to the individual.  A person cannot walk in off the street and

bring a qui tam claim by showing a simple fact such as a lack of a tag on a program.  These
claims are numerous within an industry where evidence has been accumulated and there isonly one person or a small group of people who could bring an action.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.120CONSTITUTIONAL LAW AND THE LAW OF CYBERSPACEdown the most recent action of Congress made it sound as if there is nopossible way to get over the community standards problem when trying
to regulate this material in cyberspace, because there are so many differ-
ent communities and problems associated with applying different types
of tests.  What if the architecture requires you to label or unlabel depend-

ing on where, geographically, a person is coming from?  The way the

architecture is now, it is relatively difficult to figure out where a user is
located.  This is where the additional layer of community standards be-

comes difficult to architect.  I confess that I do not know how to solve this

problem.The Supreme Court is difficult to predict.  My confidence in predict-
ing what this Court will do has dropped dramatically in the last year, so I
will not predict how the Court will resolve this issue.  But I cannot believe

that it will decide that nothing can be done.  The resolution will not be

that one standard fits the whole nation either; the Court will instead at-
tempt to find some compromise.  In a sense, it has struck the same balance

in real space through the same legal standard applied to real-space mate-
rials.This leads to the question of how the community standards issuewould play out in a place like a library, which serves a wide range of
people, presumably with different ideas of what is harmful.  If there were

thousands of lawsuits, this could create a chilling effect on free speech,
because people would think, ÒWell, every time I have a certain type ofspeech on my site, IÕm going to get into a lawsuit.  It will be blocked, so
IÕm not going to have that speech on this site (without labeling).Ó  Yet weoften forget that, with the existing censorware, Web sites already make
the same judgment.  They say, 
ÒHmm.  I want to avoid getting on the
CYBERSitter list.  I want to include this interesting information about how

to get contraception in certain cases, but itÕs too dangerous, because thisspeech will be filtered.  When my speech is filtered in the context of

CYBERSitter, there is no court to which I can go to order that it is im-
proper to filter my speech.  I am stuck.
ÓIn other words, there is already a chilling effect on free speech createdby these invisible blacklists that spread across cyberspace.  I do not think

we can avoid some chilling effect.  The question is how to minimize it.

Focusing on a legal standard that is interpreted in a legal context is a way
to minimize the chilling effect and maximize the amount of speech that
can be protected.ÒChillÓ has a more precise meaning than just causing you to not post
material.  It means that you are uncertain and afraid of punishment, so

you choose not to post what otherwise you should be allowed to post.  It

is the variance (the uncertainty in application) that we are concerned
about.  Given the range of private censors, the variance that we need to
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.LARRY LESSIG121consider is much greater than it would be if there were a single standarddefining material harmful to minors.  Thus, I think that 
ÒchillÓ has greater
meaning in the private censorship context than in the government con-
text.  This is not to say that we could not imagine the court developing a

doctrine such that people are terrified and do not do anything. That is an
unavoidable consequence if you screw it up and would be terrible for free
speech.  Maybe this is a lawyer-centric view, but I am much happier if that

battle occurs in court, because then I have the right to argue that this stan-
dard is wrong and inconsistent.  When it is done in the private censorship

context, I do not have the right to make that argument.Here is the disingenuous part of my scenario.  It is extremely difficult
to say what the standard Òharmful to minorsÓ means.  The burden is on
the government or prosecutor to demonstrate that this material is harmful
to minors.  I have the right to free speech until the state can demonstrate

this.  But what does the government actually have to show?  The govern-
ment does not need to show data that demonstrate the harm.  The way

these cases are typically litigated involves comparisons to Òlike kindsÓ of
material.  Obscenity is harmful to minors.  As the court said, the sort of

sexually explicit speech that appropriately is kept from children is like
obscenity to children.To date, Òharmful to minorsÓ has been interpreted by the Supreme
Court to include sexually explicit speech only.  It does not include hate

speech, for example.  There is a lower court judgment that expands the

interpretation, but I donÕt believe that interpretation will be sustained.Therefore, in my view, the legitimate interest of the government has been
prescribed to include only sexually explicit speech.  I am sure that people

will try to bring other types of speech to the courts.  But I am also sure that

the Supreme Court would look at Ku Klux Klan (KKK) speech, for ex-
ample, and say, ÒIt is terrible speech, I agree, but this is the core of FirstAmendment type of speech that we must protect.Ó  We will get into an
argument about whether 6-year-olds should see KKK speech, and this
will be difficult for the court.I have no kids and I do not look at this material.  I have no way of
figuring out how to draw the line.  But part of the solution is to realize

that no one will have a complete solution.  We depend on the diversity of

institutions to contribute their parts.  Some part has to be contributed by

people making judgments.  In a paper that I wrote with Paul Resnick,

who was originally on this committee, we described techniques for mini-
mizing the cost of determining what Òharmful to minorsÓ means.  Geoff
Stone would look at some of these techniques and say, ÒNo, no, the Con-stitution would forbid them.ÓImagine a site asking a government agency, ÒCan you give me a signthat this material is okay?Ó  This is like a promise not to prosecute, and it
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.122CONSTITUTIONAL LAW AND THE LAW OF CYBERSPACEis done now.  It amounts to preclearance of material that is on the border-
line.  It is not saying that you cannot publish unless you get permission.  It

is not saying that if you do not get permission, you cannot publish.  All it
means is that if you get preclearance, there is a guarantee that you will not
be punished.  It is a safe harbor
Ñit takes care of the ÒchillÓ problem.  If the
government says, ÒWe canÕt give you a safe harbor here,Ó then you have a
problem.  Then you must decide whether it is worth the risk to speak.

But, again, this is a problem we face now.  People currently make this

decision when they decide how to distribute material in more than half of
the states.  We should minimize the cost of that problem, but I do not

think we can say the Constitution requires us to make that cost zero.As times and standards change, crude standards help, because a fine-grained system would become out-of-date.11  Because this discriminator
is so crude, I think that what happens in cyberspace would mirror what
would happen in real spaceÑpeople only worry about and prosecute theextreme cases.  There is a lot of material floating around that nobody

wastes time worrying about.  But, in principle, we would have to worry

about how things are updated over time.  In cyberspace, 10 years is a long

time.  I am not sure what the burden of that is.  My personal preference is

that we do as little as possible but enough to avoid the problem of too
much private censorship.  The system also needs to be sensitive to what

we learn about the consequences of what we do.This solution will not eliminate all private filtering.  But my view is
that a significant amount of demand for private filtering results from the
lack of any less-restrictive alternative.   If you asked the filtering compa-

nies, 90 percent of them would say, ÒWhat Lessig is talking about is ter-rible and unconstitutionalÓÑbecause it would drive 90 percent of themout of business.  But there still would be parents who are on the Christian

Right, for example, and who want to add another layer of protection on
top.  We will not go from a world of perfect censorship to perfect free

speech, but a balance is needed between the two.  Under the existing sys-

tem, we have so many examples of overreaching and private censoring
that some way to undermine it is needed.Given the international context for the Internet, this solution is not a11Bob Schloss asked who would label orphan content, which is floating around on theInternet or on hard disks but whose publisher is dead or not paying attention, and how thebinary indicatorÑa yes or no answer to the question of whether something is harmful tominorsÑwould hold up over time as community standards changed.  It might work for 10
years, but in the end, to deal with the problem of both shifting standards and orphan con-tent, the system could end up with a third-party rating process again.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.LARRY LESSIG123complete one.  But our nation is very powerful.  When you set up a simple
system for people to comply with, and there is some threat that they will
be attacked by the United States if they are not in compliance, then it will
be easier for most people to comply.  Tiny sanctions and tiny compliance

costs actually have a significant effect on convincing people to obey.Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.124Appendix:Biographies of Presenters
Nicholas Belkin has been professor of information science in the Schoolof Communication, Information, and Library Studies at Rutgers Univer-
sity since 1985.  Prior to that appointment, he was lecturer and then senior

lecturer in the Department of Information Science at the City University,
London, from 1975. He has held visiting positions at the University of
Western Ontario, the Free University of Berlin, and the Integrated Publi-
cation and Information Systems Institute of the German National Research
Center for Computer Science and was visiting scientist at the Institute of
Systems Science of the National University of Singapore and a Fulbright
Fellow at the Department of Information Studies, University of Tampere,
Finland.  Professor Belkin was chair of the ACM Special Interest Group on

Information Retrieval from 1995 to 1999 and is a member of the Steering
Committee for the ACM/IEEE Joint Conferences on Digital Libraries. He
received his Ph.D. in information studies from the University of London
in 1977 and a masterÕs in librarianship from the University of Washington
in 1970.Michel Bilello holds a Ph.D. degree in electrical engineering and an M.D.degree, both from Stanford University.  His most recent research includes

security mediation for secure dissemination of medical information.  He

has recently completed his internship in internal medicine.Fred Cotton is director of training services for SEARCH, the NationalConsortium for Justice Information and Statistics. He provides technical
assistance and training to local, state, and federal criminal justice agencies
nationwide in information systems, including assistance in computerTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BIOGRAPHIES OF PRESENTERS125crimes investigations and examining seized microcomputers. He instructsa variety of technology crimes courses that SEARCH offers at its National
Criminal Justice Computer Laboratory and Training Center in Sacra-

mento, California, and at other sites nationwide, and he oversees a train-
ing staff of eight. He has also taught advanced officer courses and officer
safety subjects in the Basic Police Academy and was an invited guest of
NorwayÕs National Bureau of Criminal Investigation, where he providedtraining on computer investigations.  Mr. Cotton has 13 years of full-time

law enforcement service as a field supervisor with experience in opera-
tions, investigations, records, training, and data processing. In addition to
his duties at SEARCH, he is a reserve police officer with the Yuba City,
California, Police Department, where he is assigned to the Sacramento
Valley High-Tech Crimes Task Force, and a specialist reserve officer with
the Los Angeles Police Department, where he is assigned to the Orga-
nized Crime and Vice Division.  Mr. Cotton is a member of the Florida

Computer Crime Investigators Association, the Forensic Association of
Computer Technicians, the Northern California Chapter of the High Tech-
nology Crime Investigation Association (HTCIA), the National Technical
Investigators Association, the Georgia High-Tech Crime Consortium, the
Midwestern Electronic Crime Investigation Association, the American
Society of Law Enforcement Trainers, and Police Futurist International.
He is a former member of the National Board of Directors of HTCIA. In
September 1999, the International Board of Directors of HTCIA selected
him as the first recipient of its Distinguished Achievement Award.  Mr.

Cotton is certified by the California Commission on Peace Officer Stan-
dards and Training as a Òcomputer / white-collar crime investigatorÓ forthe State of California through the Robert J. Presley Institute of Criminal
Investigation (ICI), and he is an ICI-certified instructor. He is also a gradu-
ate of and has been a guest instructor at the ÒSeized Computer EvidenceRecovery SpecialistÓ training course offered through the Federal Law En-
forcement Training Center in Glynco, Georgia, and he has qualified and
testified as an expert witness on computer investigations in both county
and federal courts.  Mr. Cotton holds a degree in administration of justice

and is an adjunct professor in the forensic computer investigation certifi-
cate program of the University of New Haven, Connecticut.Donald Eastlake has over 30 years of experience in the computer fieldand was one of the principal architects and specification authors for the
Domain Name System security protocol. He co-chairs the joint IETF/W3C
XML Digital Signature Working Group, chairs the e-commerce-oriented
IETF TRADE Working Group, is a member of and co-editor of the specifi-
cation for the W3C XML Encryption Working Group, and a member of
the Java Community group developing XML Security APIs. He is a mem-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.126APPENDIXber of the technical staff at Motorola and has previously worked for IBM,CyberCash, Digital Equipment Corporation, Transfinite Systems Com-
pany, Computer Corporation of America, and the Massachusetts Institute
of Technology.Susan Getgood is one of corporate AmericaÕs leading experts on educa-tion and Internet safety. She has testified on behalf of the industry before
Congress, the National Research Council, the Federal Trade Commission,
and the ChildrenÕs Online Protection Act Commission.    Prior to assum-
ing her current role, she served as the Learning CompanyÕs director ofcorporate communications, where she developed a broad knowledge of
the role of technology in teaching both children and adults.  She previ-

ously was director of marketing at Microsystems Software, the
company that developed Cyber Patrol Internet filtering software. She  rep-

resented Microsystems Software when it was among the coalition of plain-
tiffs who successfully challenged the constitutionality of the Communica-
tions Decency Act in 1996.  In December 1997, she was part of a panel on

Internet filtering and other technologies to protect children at the White
House-backed ÒInternet/Online Summit: Focus on Kids.Ó  She has been
involved in issues related to creating positive digital content for children
and the development of quality educational content for homes and
schools.  She is on the Board of Directors of Mass Networks and was re-

cently named to the Executive Committee of this public-private partner-
ship dedicated to enhancing education through technology in the state of
Massachusetts.Bennett Haselton has been publishing reports on the workings of Internetblocking software since 1996.  His Web site at Peacefire.org has functioned

as  a clearinghouse of information related to Internet blocking software

and has been featured in reports on CNN, CourtTV, CNNfn, MTV News,
and MSNBC.  Bennett holds an M.A. in mathematics from Vanderbilt

University and lives in Seattle.  He currently works as a 
Òcontract hacker,Ófinding security holes in Internet applications on a commission basis.Chris Kelly has served in a variety of business, educational, and govern-mental roles over the past 10 years aimed at bettering the ways we pro-
vide customer service, teach our children, and live our everyday lives
online.  Currently chief privacy officer for Excite@Home, the leading

broadband online service provider, he is responsible for the companyÕsprivacy policy and practices, working with public policy makers and in-
dustry leaders on consumer privacy initiatives and educating the public
regarding the companyÕs commitment to online privacy.  He brings ex-
tensive experience in information technology, law, and public policy to
his role at Excite@Home.  At Kendara, a 40-person next-generation digital
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BIOGRAPHIES OF PRESENTERS127marketing startup acquired by Excite@Home in 2000, he served as one ofthe online industryÕs first chief privacy officers, overseeing product archi-tecture development and data management practices to ensure consumer
privacy.  Prior to Kendara, he was an attorney in the Antitrust and Intel-

lectual Property groups at Palo Alto law firm Wilson, Sonsini, Goodrich &
Rosati, where he counseled numerous Internet companies on privacy poli-
cies, terms of service, and other Web site service concerns.  At Wilson

Sonsini, he also advised numerous Silicon Valley companies on the impli-
cations of the governmentÕs antitrust suit against Microsoft and on theapplication of intellectual property concepts in the digital age.  As a fel-

low of the Berkman Center for Internet and Society at Harvard Law

School, he worked on a variety of Internet public policy issues, including
spam prevention, privacy protection, and technologyÕs impact on educa-tion.  He has also taught cyberspace law as an adjunct professor at the

California Western School of Law in San Diego. In the community, he
serves on the board of directors of Greatschools.net, a not-for-profit online
ÒZagatÕs GuideÓ to every school in California and Arizona, assisting the
companyÕs nationwide expansion efforts.  He also participates in the Palo
Alto Area Bar AssociationÕs Lawyers in the Schools program, teachinghigh school students basic legal concepts through interactive role playing
exercises.   He is a member of the State Bar of California and the American

Bar Association.  He has served as a policy analyst for the White House

Domestic Policy Council and as a special assistant at the U.S. Department
of Education.  He holds a J.D. from Harvard Law School, a master
Õs de-gree in political science from Yale University, and a B.A. from Georgetown
University, where he was elected to Phi Beta Kappa.  At Harvard, he was
editor in chief of the Harvard Journal of Law and Technology and was part of
the founding team for the Berkman Center for Internet & Society.Ray Larson specializes in the design and performance evaluation of infor-mation retrieval systems and the evaluation of user interaction with those
systems.  His background includes work as a programmer/analyst with

the University of California (UC) Division of Library Automation, where
he was involved in the design, development, and performance evaluation
of the UC public access online union catalog (MELVYL). His research has
concentrated on the design and evaluation of information retrieval sys-
tems. He is the designer of the Cheshire II information retrieval system,
which is being used as a search engine at numerous sites in the United
States and Europe. The ranking algorithms developed in the Cheshire II
project are the basis of the Inktomi search engine used by Yahoo and other

World Wide Web search portals.  He was a faculty investigator on the

Sequoia 2000 project, where he was involved in the design and evaluation
of a very-large-scale, network-based information system to support theTechnical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.128APPENDIXinformation needs of scientists studying global change. He is also a fac-ulty investigator on the UC Berkeley Environmental Digital Library
Project (sponsored by the National Science Foundation (NSF), the Na-
tional Aeronautics and Space Administration, and the Defense Advanced
Research Projects Agency (DARPA)), where the work is continuing on a
very large environmental information system providing access to infor-
mation on the California environment. He is the principal investigator on
the International Digital Libraries Initiative sponsored by NSF and the
Joint Information Systems Committee in the United Kingdom.  He is a co-

principal investigator on other projects sponsored by DARPA and the In-
stitute for Museum and Library Studies.  He has consulted on information

retrieval systems and automatic classification methods with major corpo-
rations, including Sun Microsystems, American Express, and Inktomi. He
has also consulted on international information system projects in the
United States and the United Kingdom, including the Networked Social
Science Tools and Resources project and the ÒArchives HubÓ linking ar-
chival collections in U.K. research libraries.David Lewis is a consultant based in Chicago, Illinois.  He works in the
areas of information retrieval, machine learning, and natural language
processing.  Prior to taking up consulting, he was a researcher at AT&T

Labs and Bell Labs and a research faculty member at the University of
Chicago.   Lewis received his Ph.D. in computer science from the Univer-
sity of Massachusetts at Amherst in 1992 and has undergraduate degrees
in computer science and mathematics from Michigan State University.
He has published more than 40 papers, holds 5 patents, and helped to
design the U.S. Government Message Understanding Conference and Text
Retrieval Conference evaluations of language processing technology.David Maher has served as chief technology officer of InterTrust sinceJuly 1999. Before joining InterTrust, he was an AT&T fellow, division
manager, and head of the Secure Systems Research Department at AT&T
Labs, where he was working on secure IP networks and secure electronic
commerce protocols. He joined Bell Labs in 1981, where he developed
secure wideband transmission systems, cryptographic key management
systems, and secure communications devices. He was chief architect for
AT&TÕs STU-III secure voice, data, and video products used by the WhiteHouse and U.S. intelligence and military personnel for top secret commu-
nications. In 1992 Maher was made a Bell Labs fellow in recognition of his
work on communications security. He was also chief scientist for AT&T
Secure Communications Systems overseeing secure systems R&D at Bell
Labs, Gretag Data systems in Zurich, and Datotek Systems in Dallas.  In

1993, Maher designed the Information Vending Encryption System used
to provide a Òvirtual VCRÓ video pay-per-view system for cable networks.
Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.BIOGRAPHIES OF PRESENTERS129In 1995, he worked with AT&T Universal Card Services, where he de-signed and analyzed a number of electronic payment systems and served
as a member of the Mondex International Security Group.  He has pub-

lished papers in the fields of combinatorics, cryptography, number theory,
signal processing, and electronic commerce. He has been a consultant to
the National Science Foundation, National Security Agency, National In-
stitute of Standards and Technology, and the Congressional Office of
Technology Assessment. He has a Ph.D. in mathematics from Lehigh Uni-
versity, and he has taught electrical engineering, mathematics, and com-
puter science at several institutions and was an associate professor of
mathematics at Worcester Polytechnic Institute. He currently serves on
the Computer Science and Telecommunications Board committee investi-
gating networked systems of embedded computers.Deirdre Mulligan is acting clinical professor of law and director of theSamuelson Law, Technology, and Public Policy Clinic at the Boalt Hall
School of Law, University of California at Berkeley.  Prior to joining Boalt,

she was staff counsel at the Center for Democracy and Technology, where
she focused on privacy and First Amendment issues. She serves on the
Computer Science and Telecommunications Board committee studying
authentication techniques and their implications for privacy.Brian Pass is a partner with the law firm of Brown, Raysman, Millstein,Felder, and Steiner LLP, heading the firmÕs West Coast technology prac-tice from its Los Angeles office.  Mr. Pass represents clients in the licens-

ing, development, and distribution of computer software; hardware de-
velopment and OEM relationships; new media and Web site licensing,
development, and marketing; intellectual property and trade secret pro-
tection; broadband communications; interactive television; and e-com-
merce.  Mr. Pass counsels companies on start-up formation and venture

capital finance, joint venture formation, and mergers and acquisitions.  He

also advises companies on Internet privacy and other regulatory issues
affecting new media and e-commerce.  Before joining Brown Raysman, he

served as president, chief executive officer, and co-founder of Passport
New Media, where he led the development of PassportÕs critically ac-claimed childrenÕs Internet service, Your Own World.  At Passport, he
raised $7.5 million in venture capital and led a team of over 30 employees,
while concluding numerous third-party content partnerships and negoti-
ating key technology and distribution relationships.  He also served as

vice president and general counsel at Americast, a joint venture of the
Walt Disney Corporation and several of the Baby Bell telephone compa-
nies, to develop interactive digital television systems.  In addition to ad-

vising the Americast partnership and its board on all general corporate
matters, he negotiated and administered numerous technology purchas-Technical, Business, and Legal Dimensions of Protecting Children from Pornography on the Internet: Proceedings of a WorkshopCopyright National Academy of Sciences. All rights reserved.130APPENDIXing and licensing agreements, including a $1 billion set-top box purchaseagreement; an $80 million dollar hardware purchase agreement; a multi-
million dollar intellectual property licensing agreement; and numerous
software development and licensing agreements.  He graduated from

Wesleyan University in 1986 with high honors in the College of Social
Studies and received his J.D. from the UCLA School of Law in 1991.is chief technical officer and co-founder of Novation Bio-sciences, a data and text mining company serving the pharmaceutical in-
dustry. He was formerly co-founder and Vice president of advanced de-
velopment for Outride, Inc., where he applied state-of-the-art relevance
technology to the challenge of information retrieval.Eddie Zeitler was a senior vice president at Charles Schwab & Co., Inc.,through March 2001, where for 5 years he managed the Information Secu-
rity Department, which comprised six specialized units: Information Ac-
cess and Protection, Information Security Technology, Information Secu-
rity Risk Management, Information Security Strategy and Architecture,
Business Contingency Planning, and Security Awareness and Training.
Mr. Zeitler has a varied background in computers and information pro-
cessing.  Prior to Charles Schwab, he managed the information security

functions at Fidelity Investments, Bank of America, and Security Pacific
National Bank.  Other management positions include the capacity plan-

ning function for Security Pacific National BankÕs computer centers, tech-nical services (operating systems and software) and computer center op-
erations for the National Data Center of Federated Department Stores,
and data center performance and configuration for Transamerica Infor-
mation Services.  He began his career developing the operating system

used on the Shuttle Orbiter at Rockwell International and radar system
controls at ITT Gilfillan.  External activities include participation on vari-
ous committees such as the Los Angeles County Computer Crime Task
Force, the Department of the TreasuryÕs Financial Management ServicesSecurity Advisory Panel, the ANSI X9.E9 and X9.F2 Working Groups for
Security of Financial Systems, the U.S. TreasuryÕs Electronic Funds Trans-fer Task Force Subcommittee on Interoperability, the ABA Information
Systems Security Committee, the (ISC)2 Qualifications Review Commit-
tee, the National Computer System Security and Privacy Advisory Board,
and the National Research CouncilÕs Panel for Information Technologythat annually reviews the National Institute of Standards and
TechnologyÕs information technology program.  Mr. Zeitler is a registered
brokerage representative (Series 7 and Series 63) and is a certified infor-
mation systems security professional.  He holds a B.S. in mathematics and

an M.S. in systems engineering from the University of Arizona.  He also

completed his Ph.D. candidacy in computer science at the University of
Alberta.
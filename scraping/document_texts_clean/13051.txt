detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/13051wireless technology prospects and policy options112 pages | 6 x 9 | paperbackisbn 9780309163989 | doi 10.17226/13051committee on wireless technology prospects and policy options; computer scienceand telecommunications board; division on engineering and physical sciences;national research councilwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.committee on wireless technology prospects and policy optionscomputer science and telecommunications boarddivision on engineering and physical scienceswireless technologyprospects and policy optionswireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, n.w. washington, dc 20001notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.support for this project was provided by the national science foundation under award number cns0238131. any opinions, ndings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily re˚ect the views of the organization that provided support for the project.international standard book number13: 9780309163989international standard book number10: 0309163986copies of this report are available from:the national academies press500 fifth street, n.w., lockbox 285washington, dc 20055(800) 6246242(202) 3343313 (in the washington metropolitan area)internet: http://www.nap.educopyright 2011 by the national academy of sciences. all rights reserved.printed in the united states of americawireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprot, selfperpetuating society of distinguished scholars engaged in scientic and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientic and technical matters. dr. ralph j. cicerone is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. charles m. vest is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v. fineberg is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientic and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. ralph j. cicerone and dr. charles m. vest are chair and vice chair, respectively, of the national research council.www.nationalacademies.orgwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.vcommittee on wireless technology prospects and policy optionsdavid e. liddle, u.s. venture partners, chairyochai benkler, harvard universitydavid borth, motorola labsrobert w. brodersen, university of california, berkeleydavid d. clark, massachusetts institute of technologythomas (ted) darcie, university of victoriadale n. hatfield, university of colorado, bouldermichael l. katz, new york universitypaul j. kolodzy, kolodzy consultinglarry larson, university of california, san diegodavid p. reed, massachusetts institute of technologygregory rosston, stanford universitydavid skellern, national ict australiastaffjon eisenberg, director, computer science and  telecommunications boardwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.vicomputer science and telecommunications boardrobert f. sproull, oracle corporation, chair prithviraj banerjee, hewlettpackard company steven m. bellovin, columbia university seymour e. goodman, georgia institute of technology john e. kelly iii, ibmjon m. kleinberg, cornell university robert kraut, carnegie mellon university susan landau, radcliffe institute for advanced study david e. liddle, u.s. venture partners william h. press, university of texas, austin prabhakar raghavan, yahoo! labs david e. shaw, d.e. shaw research alfred z. spector, google, inc. john a. swainson, silver lakepeter szolovits, massachusetts institute of technology peter j. weinberger, google, inc. ernest j. wilson, university of southern californiastaffjon eisenberg, director virginia bacon talati, associate program ofcershenae bradley, senior program assistantrenee hawkins, financial and administrative manager herbert s. lin, chief scientistemily ann meyer, program ofcer lynette i. millett, senior program ofcer eric whitaker, senior program assistantenita a. williams, associate program ofcerfor more information on cstb, see its website at  http://www.cstb.org, write to cstb, national research council, 500 fifth street, n.w., washington, dc 20001, call (202) 3342605, or email the cstb at cstb@nas.edu.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.viithe use of radiofrequency communicationšcommonly referred to as wireless communicationšis becoming more pervasive as well as more economically and socially important. technological progress over many decades has enabled the deployment of several successive generations of cellular telephone technology, which is now used by many billions of people worldwide; the nearuniversal addition of wireless local area networking to personal computers; and a proliferation of actual and proposed uses of wireless communications. the ˚ood of new technologies, applications, and markets has also opened up opportunities for examining and adjusting the policy framework that currently governs the management and use of the spectrum and the institutions involved in it, and models for allocating spectrum and charging for it have come under increasing scrutiny. yet even as many agree that further change to the policy framework is needed, there is debate about precisely how the overall framework should be changed, what trajectory its evolution should follow, and how dramatic or rapid the change should be. many groups have opinions, positions, demands, and desires related to these questionsšre˚ecting multiple commercial, social, and political agendas and a mix of technical, economic, and social perspectives. the development of technologies and associated policy and regulatory regimes are often closely coupled, an interplay apparent as early as the 1910s, when spectrum policy emerged in response to the growth of radio communications. as outlined in this report, current and ongoing prefacewireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.viii prefacetechnological advances suggest the need for a careful reassessment of the assumptions that inform spectrum policy in the united states today. this report of the committee on wireless technology trends and  policy options (appendix a) thus seeks to shine a spotlight on 21st century technology trends and to outline the implications of emerging technologies for spectrum management in ways that the committee hopes will be useful to those setting future spectrum policy. speakers at the meetings held by the committee are listed in appendix b. the detailed statement of task for the study is given in appendix c.the committee was not in a position to examine details of the numerous specic areas of contention that are the subject of frequent debate today or to evaluate the merits of opposing claims. this report thus does not offer specic prescriptions for how particular frequency bands should be used or seek to resolve con˚icting demands for spectrum use for particular services. instead, the committee offers a discussion of the technology trends and related policy options relevant to addressing these con˚icts, both today and in the future.the development of this report was not without its own challenges, and the report was a long time in the making. early on, the committee™s work expanded in scope following a request from the national telecommunications and information administration to convene a forum on spectrum policy reform options.1 later, a variety of circumstances  unrelated to the substance or the work of the committee led to unexpected delays. throughout the project, there were also reminders that its subject is inherently complex and challenging. the technology and policy issues are tightly intertwined, and the study involved experts from multiple disciplines, including economics, law, public policy, electrical engineering, and computer science. the multidisciplinary approach sought yields a more comprehensive view of a problem, but more time and effort are needed to establish a common view of the issues, a common vocabulary, and so forth. finally, the technical and policy perspectives of the members of the committee were, by design, diverse. as a result, the technology considerations, enablers of a more nimble policy framework, and policy options developed by the committee are the products of a multidimensional examination of the issues and negotiation of agreements among members holding oftencontrasting opinions.1  national research council, summary of a forum on spectrum management policy reform, the national academies press, washington, d.c., 2004.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.ixthis report has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the national research council™s report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscript remain condential to protect the integrity of the deliberative process. we wish to thank the following individuals for their review of this report:vinton g. cerf, google, inc.,john m. ciof, stanford university,gerald r. faulhaber, university of pennsylvania,kevin c. kahn, intel corporation,teresa h. meng, stanford university,dipankar raychaudhuri, rutgers university,david h. staelin, massachusetts institute of technology,andrew j. viterbi, the viterbi group, andsteven s. wildman, michigan state university.although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the conclusions or recommendations, nor did they see the nal draft of the report acknowledgment of reviewerswireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.x acknowledgment of reviewersbefore its release. the review of this report was overseen by r. stephen berry, university of chicago. appointed by the national research council, he was responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and that all review comments were carefully considered. responsibility for the nal content of this report rests entirely with the authoring committee and the institution.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.xisummary 11 introduction: trends and forces reshaping  14 the wireless world advances in radio technology, 15 expansion in applications and users, 17 changing market dynamics, 20 the evolving policy and regulatory framework, 212 key technology considerations 33 technological advances in radios and systems of radios, 34 lowcost, portable radios at frequencies of 60 ghz and  above, 52 interference as a property of radios and radio systems,  not radio signals, 53 enduring technical challenges, 55 timescales for technology deployment, 57 talent and technology base for developing future radio technology, 58 measurements of spectrum use, 59 challenges facing regulators, 63 engineering alone is often no solution, 66contentswireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.xii contents3 policy options 67 pressures on today™s wireless policy framework, 67 key considerations for a future policy framework, 68 technologyenabled policy options, 76appendixesa biographies of committee members and staff 87b speakers at meetings  96c statement of task 99wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.1today™s framework for wireless policyšwhich governs the operation of devices that make use of radiofrequency (rf) transmissionsšhas its roots in the technology of 80 years ago and the desire at that time for governmental control over communications. it has evolved to encompass a patchwork of legacy rules and more modern approaches that have been added over time. although views vary considerably on whether the pace of reform has been commensurate with the need or opportunity, there have been a number of signicant policy changes in recent decades to adjust to new technologies and to decrease reliance on centralized management. these developments have included the use of auctions to make initial assignments (along with the creation of secondary markets to trade assignment rights) and the designation of open bands1 in which all users are free to operate subject only to a set of ﬁrules of the road.ﬂthere remains, nonetheless, much debate about how the overall framework should be changed, what trajectory its evolution should follow, and how dramatic or rapid the change should be. many groups have opinions, positions, and demands related to these questions re˚ecting multiple commercial, social, and political agendas and a mix of technical, economic, and social perspectives. 1  a variety of terms are used to describe this approach, including ﬁlicenseexemptﬂ or ﬁlicense by rule.ﬂ the approach is probably most familiar as the basis for operation of wireless lans, cordless telephones, and the like.summarywireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.2 wireless technology prospects and policy optionspressures on today™s wireless policy frameworkthe current framework for wireless policy in the united states is under pressure on several fronts:ł it continues to rely heavily on servicespecic allocations and assignments that are made primarily by frequency band and geographic location and does not embrace all of the spectrum management approaches possible with today™s technologies and expected to be available with tomorrow™s technologies. ł despite revisions aimed at creating greater ˚exibility, it continues to rely signicantly on centrally managed allocation and assignment, with government regulators deciding how and by whom wireless communications are to be used despite growing agreement that central management by regulators is inefcient and insufciently ˚exible.ł it will not be able to satisfy the increasing and broadening demand for wireless communications that is spurred by interest in richer media, seemingly insatiable demand for mobile and untethered access to the internet and the public telephone network, and growing communication among devices as well as people.ł it does not fully re˚ect changes in how radios are being built and deployed now or in how they could be built and deployed in the future in response to different regulations, given that technological innovation has expanded the range of potential wireless services and the range of technical means for providing those services and at the same time has dramatically lowered the cost of including wireless functionality in devices. today, the complexity and density of existing allocations, assignments, and uses, and the competing demands for new uses, all make policy change difcult. decisions will necessarily involve (1) addressing the costs and benets of proposed changes that are (often unevenly) distributed over multiple parties, (2) resolving con˚icting claims about costs and benets, and (3) addressing coordination issues, which are especially challenging if achieving a particular change requires actions by a large number of parties. moreover, some parties stand to gain by changingšor advocating for changešwhile others stand to gain by delay or retaining the status quo.forwardlooking policy directionsthe committee on wireless technology prospects and policy options believes that, moving forward, the unambiguous goal for spectrum policy wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.summary 3should be to make the effective supply of spectrum plentiful so as to make it cheaper and easier to innovate and introduce new or enhanced services. put another way, the goal should be to reduce the total costšwhich includes the cost, if any, of licenses, and the cost of equipment, both for the end user and the networkšof introducing or enhancing services. the nancial cost of adverse impacts to existing users and services should also be fairly evaluated and debated in advance of regulatory changes.given the plethora of existing allocations and assignments, and the multitude of existing services and users associated with them, it is not possible to take a cleanslate approach. achieving the goal stated above will thus involve several parallel efforts:ł leveraging advanced technologies, regulation, and marketbased incentives to support sharing, including overlay and underlay approaches, so that new services can share spectrum with legacy services.ł streamlining and modernizing the use of bands allocated or assigned to old services to free up new areas of ﬁwhite spaceﬂ that can be used for new services, by using market mechanisms, relinquishing governmentcontrolled bands used for obsolete services, and shutting down obsolete services (as has happened with analog television). ł establishing ﬁopenﬂ as the default policy regime used at 20 to 100 gigahertz (ghz). at these higher frequencies, sparser use and technical characteristics that signicantly reduce the chance for interference suggest that nontraditional management approaches can predominate. the likelihood of ongoing technological change also points to the value of establishing a more adaptive learning system for setting policy that would be better able to track and even anticipate advances in wireless technology and emerging ways of implementing and using wireless services. the sections that follow provide a brief description of key technology considerations and outline policy options, many enabled by new technology, that will be useful in achieving the goal of increasing the supply of spectrum for enhanced or new services.key technology considerationsradiofrequency communication has been transformed profoundly in recent years by a number of technological advances. this section outlines key recent advances and associated trends and their implications for the design of radios and radio systems and for regulation and policy. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.4 wireless technology prospects and policy optionsprofound changes in radiofrequency communication as a result of technological advances in radios and radio systemsdigital processing is used increasingly to detect the desired signal and to reject interfering signals. the shift to largely digital radios built using complementary metal oxide semiconductor (cmos) technology; (a highdensity, lowpowerconsumption technology for constructing integrated circuits) has made it much cheaper and easier to include wireless capabilities in consumer electronic devices. as a result of the reduction in costs for radio technology, the barriers to developing and deploying novel, lowcost, specialized radios have become much lower, and more rms and other organizations have become capable of and potentially motivated to participate. growth in the number of wireless devices of various types and in the demand for wireless communications is likely to continue. technological capabilities are also driving the introduction of new radio system architectures, including a shift away from centralized systems to more localized transmissions in distributed systems that use very small cells (the smallest of those being deployed today are called femtocells) or mesh networks, and a shift from centralized switching to more distributed, often internetprotocolbased, networks. another important shift in radios has been the ability to use new techniques to permit greater dynamic exploitation of all available degrees of freedomšfrequency, space, time, and polarizationšwhich makes it possible to take greater advantage in a dynamic, negrained, and automated fashion of all the degrees of freedom to distinguish signals. this capability offers the opportunity to introduce new options for assigning usage rights.the ability to leverage sustained improvements in the performance of digital logic also opens up opportunities to build radios that are much more ˚exible and adaptable. such radios can change their operating frequency and modulation scheme, can sense and respond to their environment, and can operate cooperatively to create new opportunities to make more dynamic, shared, and independently coordinated use of spectrum. (they cannot, however, directly sense passive users, which means that special measures such as registries or beacons are needed for detection of passive users.) the result is that radios and systems of radios can operate and cooperate in an increasingly dynamic and autonomous manner. although increased ˚exibility involves greater complexity, cost, and power consumption, it enables building radios that can better coexist with existing radio systems, through both underlay (lowpower use intended to have a minimal impact on the primary user) and overlay (agile use by a secondary user of ﬁholesﬂ in the time and space of use by the primary user). moreover, ˚exibility makes it possible to build radios with operating parameters that can be modied to comply with future policy or rule changes or future service requirements.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.summary 5the use of cmos to build radios and digital processing together with other advances in rf technology opens up a new set of opportunities in the form of lowcost, portable radios that are becoming increasingly practical at frequencies of 60 ghz and above. radios operating in this domain must confront a number of challenges, including limited freespace propagation distances (especially in the oxygen absorption bands around 60 ghz) and very limited penetration through and diffraction around walls of buildings or other obstacles. on the other hand, these characteristics make such radios very useful in providing very large bandwidths over short range. interference as a property of radio receivers and radio systems,  not radio signals it is commonplace to talk about radio signals interfering with each other, a usage that mirrors the common experience of hearing broadcast radio signals that are transmitted on the same channel overlay each another. however, radio signals themselves do not, generally speaking, interfere with each other in the sense that information is destroyed. interference re˚ects a receiver™s inability to distinguish between the desired and undesired signals. the cost of separating these signals is ultimately re˚ected in design complexity, hardware cost, and power consumption. as a result, any practical radio (i.e., one of practical size, cost, and power consumption) will necessarily throw away some of the information needed to resolve signal ambiguity. as the performance and capabilities of radios continue to improve over time, their ability to distinguish between signals can be expected to improve. however, power consumption will remain an especially challenging constraint, especially for portable devices, and even a modest additional device cost can jeopardize the commercial viability of a product or service. persisting technical challenges even as the capabilities and the performance of radios continue to improve, several hard technical problems can be expected to persist. these technical challengesšdiscussed in more detail below in this reportšinclude power consumption, nonlinearity of radio components, support for nomadic operation and mobility, and coping with the heterogeneity of capabilities, including both legacy equipment and systems that are inherently constrained, such as embedded network sensors and scientic instruments that passively use spectrum (e.g., for remote earth sensing and radio astronomy). wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.6 wireless technology prospects and policy optionsnonuniform timescales for technology replacement different wireless services are characterized by the different timescales for removal of old technology from service and deployment of new technology. the factors in˚uencing the turnover time include the time to build out the infrastructure, the time to turn over the base of enduser devices, and the time to convince existing users (who may be entrenched and politically powerful) to makešand pay forša shift, as well as the incentives for upgrading and the size of the installed base. considerable uncertainty about the rate at which new technologies can be deployed practically a particular challenge in contemplating changes to policy or regulatory practice is determining just how quickly promising new technologies will be deployable as practical devices and systems and thus how quickly, and in what directions, policy should be adjusted. as is natural with all rapidly advancing technologies, the concepts and prototypes are often well ahead of what has been proved to be technically feasible or commercially viable. at the same time, technical advances sometimes can be commercialized quickly, although deployment and use might also require adjustments to regulations, a process that historically has taken longer.spectrum use lower than allocations and assignments suggest,  especially at higher frequencies quantifying how well and how efciently spectrum is used is quite challenging. measurements may miss highly directional or periodic use and cannot detect passive uses such as radio astronomy. these caveats notwithstanding, measurements suggest that some allocated and assigned frequency bands are very heavily used whereas others are only lightly used, at least in certain places and at certain times. the published frequency allocation and assignment charts are thus potentially misleading in their suggestion that little spectrum is available for new applications and services. a good deal of empty space exists in the spectrum; the challenge is to nd ways of safely detecting and using it.enablers of a more nimble, forwardlooking spectrum policy frameworkthe committee identied the following approaches as enablers of a more nimble approach to spectrum policy. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.summary 7abandon the extremes in the  ﬁproperty rightsﬂ versus ﬁcommonsﬂ debatethe terms ﬁproperty rightsﬂ and ﬁcommonsﬂ are shorthand for particular approaches to spectrum managementšapproaches that re˚ect philosophical and ideological perspectives as well as technical and policy alternatives. the property rights approach relies on a wellspecied and possibly exclusive license to operate and on rights that can be established or transferred through an administrative proceeding, auction, or market transaction. it is intended to facilitate the creation of a market in infrastructure access and use rights. the commons or openaccess approach relies on establishing licensefree bands in which users must comply with specied rules, such as limits on transmitted power. it is intended to facilitate a market in devices and services based on symmetrically applied infrastructure use and access rights.each has advantages and disadvantages and associated transaction costs. each involves different incentives, and different and complementary loci, for innovation. when carefully specied, neither pure version can at present be determined to be uniquely ﬁbetterﬂ than the other. moreover, there is a much larger space of alternatives, and commercial forces can help drive their evolution and selection provided that the regulatory structure is not overly rigid. this suggests adopting a policy framework that avoids detailed allocation of spectrum in favor of one that uses  market mechanisms for spectrum allocation where they make sense and uses an openaccess mechanism in other instances. where to draw the line between the two general approaches (licensed or exclusiveuse allocations versus open access)šand which hybrids of the two approaches might be usefulšwill shift as technological capabilities, deployed services, and business models continue to evolve. leverage standards processes but understand their limitationsregulators often rely, either explicitly or implicitly, on standards  bodies to dene the technical standards that are ultimately needed to implement rulings for proposed new allocations and services. on the one hand, standardssetting organizations are viewed as being more nimble and better able than regulatory bodies to focus on technical issues. on the other hand, as standards take on greater importance, the number of competing players and con˚icting interests grows, raising the risks that a large player may try to dominate the process, that standards setting may deadlock, or that only certain societal interests are re˚ected. some ways to address these risks have been identied, such as the use of one company, one vote to deal with attempts to dominate by sending multiple delegates, but such an approach has tradeoffs as well. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.8 wireless technology prospects and policy optionscollect more data on spectrum usethere are many gaps today in knowledge about the use of spectrum. measuring use is difcult and has not been done systematically, leading to uncertainty for policy makers, who are not able to readily assess claims and counterclaims about the use or nonuse of spectrum. advances in radio technology, however, make it possible to contemplate new ways of collecting data on spectrum use, such as by the deployment of networks of sensors and the incorporation of sensing capabilities in equipment deployed for other purposes. such capabilities would enhance the ability of regulators to enforce compliance with operating rules, and to more quickly assess con˚icting claims about harmful interference and provide the data required to implement spectrum management schemes that depend on identifying unused spectrum. ensure that regulators have access to technology expertise  needed to address highly technical issues as this report argues, spectrum policy is entering an era in which technical issues are likely to arise on a sustained basis as technologies, applications, and services continue to evolve. the committee believes that the federal communications commission (fcc) would therefore benet from enhancing its technology assessment and engineering capabilities and suggests several ways to gain such expertise:ł make it a priority to recruit topcaliber engineers/scientists to work at the fcc, perhaps for limited terms. ł use an external advisory committee to provide the fcc with outside, highlevel views of key technical issues. (indeed, in the past, the fcc convened the technology advisory council to play just such a role.2) ł add technical experts to the staff of each commissioner. ł tap outside technical expertise, including expertise elsewhere in the federal government such as at the department of commerce™s institute for telecommunication sciences and the national institute of standards and technology (nist), or through a federally funded research and development center. 2  the fcc announced the appointment of a new technology advisory council in october 2010, as this report was being prepared for publication.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.summary 9sustain talent and technology base for future radio technology the opportunities described in this report rely on innovation in both technology and policy. innovation in wireless technology involves many areas of science and engineeringšincluding rf engineering, digital logic, cmos, networking, computer architecture, applications, policy, and  economicsšand often expertise in combinations of these areas that is difcult to obtain in a conventional degree program. research investments in wireless technologies by federal agencies such as the national science foundation, defense advanced research projects agency, national telecommunications and information administration, and nist help to build the knowledge base for future innovation and to educate and train tomorrow™s wireless engineering talent. research efforts can be buttressed by an infrastructure for implementing and testing new ideas in radios and systems of radios. test beds allow radio system architectures to be tested at scale, and access to facilities for integrated circuit design and fabrication makes it possible to build prototypes. forwardlooking policy optionsconsider ﬁopenﬂ as the default policy regime at a frequency range of approximately 20 to 100 ghzat frequencies of 20 to 100 ghz, the potential for legacy problems and for interference (in the classical sense) is lower, suggesting that nontraditional (open) approaches could predominate for use of spectrum at 20 to 100 ghz.3 adopting an open approach for a frequency domain that will become increasingly more technologically accessible and commercially attractive several years from now would set the stage for more ˚exible and adaptive future spectrum management. fcc policy has already moved in this general direction, with an unlicensed regime established in a band at 57 to 64 ghz and licensed access to bands at 80 and 95 ghz made available on a rstcome, rstprotected basis. spectrum use is relatively low at 20 to 100 ghz compared to use at frequencies below 20 ghz, but existing users are likely to argue vociferously for ongoing protection, and some exceptions to the open rule will probably be needed to protect certain established services and passive scientic uses. 3  it would be imprudent to recommend a particular regime for frequencies above 100 ghz given today™s limited understanding of how radios might be constructed or operated in that domain, and it would be prudent to review policy in this area every several years and make adjustments as appropriate.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.10 wireless technology prospects and policy optionsuse new approaches to mitigate interference and a wider set of parameters in making assignments protecting against harm from interference has both technical aspects (how well a radio or radio system can separate the desired from undesired signals) and economic dimensions (the costs of building, deploying, and operating a radio or radio system with particular technical characteristics that make it easier to separate the signals). provided that the transaction costs are low enough and that agreedupon protocols for coordination exist, usage ﬁneighborsﬂ can negotiate mutually satisfactory solutions to interference problems that take into account the nancial benets, costs, and technology opportunities.4 given the complexity of dening the technological options for any given communication in the context of other local attempts to communicate, as well the difculties of determining who is a ﬁneighbor,ﬂ particularly for mobile and nomadic systems, the transaction costs may be signicant.5 the size of these costs and their implications for solutions that rely on negotiations will depend on such factors as the number and diversity of systems and users and is a subject of ongoing debate.receivers are increasingly able to discriminate a desired signal from an undesired one, some technologies provide new tools for mitigating interference, and other new technologies make it possible to exploit all degrees of freedom in a dynamic fashion, opening new avenues for mitigating interference. mitigation of interference can also be addressed in terms of the behavior of systems of radios rather than of individual radios and by coordinating the behavior of multiple systems. a key question is how best to establish incentives for such cooperation. introduce technological capabilities that enable more sophisticated spectrum management the use of certain technologies, some of them emerging and some of them available but not widely deployed, would make it easier to introduce new services into crowded frequency bands. in particular it might be possible to overlay unlicensed use onto licensed use if receivers were suitably equipped. another enabling technology is smart antennas that could be used to focus transmitted power, scan the environment for other transmissions, and spatially separate transmissions to help avoid interference. migrating current nondigital services to more efcient digital 4  r.h. coase, ﬁthe federal communications commission,ﬂ journal of law and economics 2(10):140, 1959.5  y. benkler, ﬁsome economics of wireless communications,ﬂ harvard journal of law and technology 16(1):2583, 2002. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.summary 11transmission will be a major challenge, especially for services that have large and/or politically powerful legacy bases.migrating to higherquality receivers has a cost in dollars, design complexity, and power consumption. even small additional costs matter a great deal when service providers are ghting for pennies. but the additional investment could have a big payoff for those who seek to introduce enhanced or new services.trade nearabsolute outcomes for statistically acceptable outcomes although statistical models have long been used in spectrum analysis, the underlying conservative assumptions have emphasized avoidance of interference to an extent that has signicantly affected efcient use of spectrum. an alternative is to relax constraints so as to normally (but not always) provide good outcomes, as is done in both internet communication (besteffort packet delivery) and cellular telephony (which provides mobility in exchange for gaps in coverage and lower audio quality). with this approach, adverse impacts on users would be rare even though technical performance might be measurably but tolerably worse for users. a relaxation of requirements could signicantly open up opportunities for nonexclusive use of frequency bands through a rebalancing of the risk of interference and the benets of new services. this approach might not be appropriate, however, for services that demand guarantees of especially highquality service (e.g., for certain safetycritical systems). although regulatory proceedings could be used to implement such a shift, it might be preferable for licensees to negotiate mutually benecial arrangements. design for light as well as design for darknessmany systems, notably cellular phones, have been ﬁdesigned for darknessﬂšthat is, with the assumption that a particular band has been set aside for a particular service or operator and that there are no other emissions in that band. an alternative is to ﬁdesign for light,ﬂ with the assumption that the operating environment will be noisy and cluttered. both approaches are reasonable for certain applications and services, but there are tradeoffs between (1) the ease with which higher spectral efciency can be achieved under design for darkness, thus allowing for lower cost and reduced power consumption and (2) the greater ˚exibility to support multiple and diverse uses under design for light. the historical preference has been to design for darkness, but today technological advances suggest opening up more bands in the designforlight modality. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.12 wireless technology prospects and policy optionsconsider regulation of receivers and networks of transceiversmuch regulation has focused on transmitters, and rules have specied transmission frequency and bandwidth, geographical location, and transmission power. increasing use of new radio architectures (discussed above) suggests that the scope of inquiry can be broadened to look at the properties and behaviors of receivers and networks of transceivers. better receiver standards would create an environment in which receiver capabilities present a lower barrier than they do today for implementing new spectrumsharing schemes. expanding the scope of policy or regulation to include a system of radios rather than an individual radio would open up new opportunities, such as the possibility of exploiting a network of radios to reliably use a listenbeforesend protocol to avoid interference and thereby avoid the hidden node problem, in which one radio cannot detect transmissions from another radio.exploit programmability so that radio behavior  can be modied to comply with operating rule changes because radios can be made highly programmable, albeit with tradeoffs in complexity, cost, and power consumption, their operating parameters can be made modiable to comply with policy or rule changes. deployment of devices with such capabilities opens up new opportunities for more ˚exible regulation and more incremental policy making: (1) policies could be written less precisely up front, (2) policies would not have to be homogeneous and could be adapted to local environmental conditions such as signal density, (3) the operating rules of existing devices could be revised to accommodate new technology, and (4) devices could more easily be certied for international use because they can readily be switched to comply with local policy. one result could be greater speed of deployment for new technologies and services.6 over time, the introduction of such capabilities could be expected to impose a less onerous performance and cost penalty. future regulations could take advantage of this opportunity by specifying, for example, that licenses granted after a certain date would require use of devices with a certain degree of reprogrammability.6  caveat: this ˚exibility could also paradoxically represent a disincentive to deployment because it opens up the possibility of future forced sharing, potentially reducing the value of a particular license. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.summary 13use adaptive and environmentsensing capabilities to reduce the need for centralized managementas agility, sensing, and coordination capabilities improve and as  etiquettes and standards for these capabilities develop, opportunities will arise for scaling back centralized management. potential advantages of this approach include a lower barrier to entry (because neither engagement with a regulator for spectrum assignment nor negotiation with an existing license holder would be necessary) and greater ˚exibility of use (because operation would be dened primarily by the attributes of radio equipment rather than regulation). potential disadvantages of this approach include uncertainty about the technical feasibility and the costs of building more capable radios with the degree of agility, coordination, and environmental sensing required for effective decentralized operation. such a shift would also involve assessing tradeoffs between the more rapid introduction of services made possible in a decentralized regime and the signicant capital investment made and efciencies achieved, at least in some instances, under a centralized regime. establish enhanced mechanisms for dealing with legacy systemsin recent years, notable efforts to deal with legacy systems have included relocating pointtopoint microwave services to allow deployment of personal communications service cellular telephony and the relocation of nextel cell services out of public safety bands. more recently, relocation of government services as well as broadcast radio services and xed services has been undertaken to allow the introduction of new 3g/advanced wireless services bands. modifying infrastructure to accommodate such change can be difcult and expensive; an even bigger legacy challenge is the need to migrate potentially millions of devices owned and operated by consumers and other end users. this task has proven easier when the market dynamics are such that enduser technology is regularly refreshed (as in mobile telephony, where new handsets with new features enter the market frequently and where the cost of handsets is often partly covered in the services fees and regular upgrades are made available at little additional cost to the subscriber) and harder where retrotting is not practical and hardware has historically had a long lifetime (as in aircraft and public safety radios). the difculty of making changes also depends, of course, on the relative political clout of the incumbents and those seeking to introduce new services.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.141introduction:trends and forces reshaping the wireless worldthis report examines the evolution of radiofrequency communicationšcommonly referred to as wireless communication1šand the framework that governs its use (a framework that also extends to uses of radio frequencies for purposes other than communication). an avalanche of new technologies, applications, and markets for wireless communications is colliding with a wellestablished and comprehensive but increasingly obsolescent framework for the allocation, assignment, and utilization of the radio spectrum. even as demand for wireless services continues to grow, much of the radio spectrum has already been allocated and assigned by frequency band (and often by geographical location) for a multitude of privatesector and government uses. the more recent developments come on the heels of many decades of technological progress, notably marked by widespread deployment of existing wireless capabilities such as several successive generations of cellular telephone technology now used by billions of people worldwide and a proliferation of actual and proposed uses of wireless communications. signicant policy changes in recent decades re˚ect efforts to adjust to new technologies and to decrease reliance on centralized management. there is debate about how the overall framework should be changed, what trajectory its evolution should follow, and how dramatic or rapid the change should be. many groups have opinions, positions, and demands related to these questions, re˚ecting multiple commercial, 1  this report uses the terms ﬁradioﬂ and ﬁwireless deviceﬂ synonymously. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.introduction: trends and forces reshaping the wireless world 15social, and political agendas and a mix of technical, economic, and social perspectives. this report thus seeks to shine a spotlight, in ways the committee hopes will be useful to those setting future spectrum policy, on emerging technology trends and to outline policy directions that align with those trends. it aims to provide a cogent discussion of the overall rationale for changing policy, the opportunities afforded by new technologies for spectrum management, and some longterm directions for improvement in policy. the committee on wireless technology trends and policy options was not in a position to examine the details of the numerous specic areas of contention that are the subject of frequent debate today regarding use of the spectrum, or to evaluate the merits of opposing claims. this report thus does not offer specic prescriptions for how particular frequency bands should be used or seek to resolve con˚icting demands for spectrum for particular services. instead, the committee intends that its discussion of the relevant technology trends and policy options should be helpful in addressing these con˚icts, both today and in the future.advances in radio technologythe development of technologies and the associated policy and regulatory regimes that govern their use are often closely coupled. for example, from the late 19th century until recently, the roadways for communication and transmission of information (e.g., the telephone system, broadcast television, and radio) were, like those for transporting people and physical goods, owned, managed, and regulated by a relatively small number of institutions. the concerns and assumptions underlying policies were grounded in the technical realities and economic and political imperatives of the time. the interplay between technology and policy was apparent as early as the 1910s. the growth of radio communications and the spectrum policy that emerged re˚ected a compromise on a framework for spectrum management. when spectrum regulation began with the radio acts of 1912 and 1927 and the communications act of 1934, the primary obstacle to signal reception was noise. because of the quality of components available at that time and the nature of the most popular frequency bands of the day (which were selected for their longer propagation distances), noise was a signicant problem, and interference (i.e., humangenerated noise from other transmissions) from other sources was regarded as intolerable and something to be avoided. accordingly, a regulatory structure was set up that allocated frequencies with specic power levels and bandwidth masks uniquely to single broadcasters or services in a given geographic wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.16 wireless technology prospects and policy optionsarea. for the most part, the environment consisted of a small number of highpower transmitters separated by frequency and geography, and a very large number of mute receivers. licenses granted the right to broadcast using a few kilohertz of spectrum and also provided an ﬁaddressﬂ (in the form of, for example, am radio channel numbers) in addition to a means to avoid interference.today, radios routinely operate in frequency ranges where background noise is limited and dealt with rather easily. the very large number of active transceivers means that the primary challenge is separating the desired signal from the signals of all the other potentially interfering transmitters, not avoiding noise. the huge number of devices associated with many modern services means that frequencies must be shared (and that the particular frequencies in use at any given time are not apparent to the user). for example, many cell phones share a particular block of spectrum at any given time, with the sharing enabled by separation by code (code division multiple access) or time slice (time division multiple access) as well as location (which cell the phone is currently in). these challenges were not fully anticipated by traditional spectrum allocation and licensing schemes. moreover, in the past 50 years, a number of changesšincluding a fundamental new understanding of physics and information theory; vast increases in the computation that can be performed by a compact, cheap, lowpower device; and improvements in analog componentsšhave allowed for very inexpensive processing of signals in ways not contemplated when many spectrum polices were established and allocations were made.in short, radiofrequency communication today is being profoundly changed by a related set of technological advancesšboth in the capabilities and performance of individual radios and in the design of networks and systems of radios. these advances, which are discussed in more detail in chapter 2, include the following:ł a shift in favor of digital signal processing and use of lowcost complementary metaloxidesemiconductors integrated circuit technology for both digital and analog radio components;ł the advent of new radio systems architectures that rely on distributed (and often internetprotocolbased) control and on more localized transmission using microcells and mesh networks, rather than  traditional architectures that rely on centralized switching or wide area transmission; ł the development of a variety of techniques, including more robust receivers, antenna arrays, frequency agility, and new modulation techniques and coding algorithms, to permit dynamic, negrained, and wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.introduction: trends and forces reshaping the wireless world 17automated exploitation of all available degrees of freedomšthat is, not just static separation in frequency and space but also dynamic use of frequency, time, space, and polarizationšalong with ﬁcodeﬂ2što distinguish radio signals; and ł the development of technologies that permit ˚exible and adaptable radios that can sense and respond to their operating environment and can coordinate their operation in an increasingly dynamic, distributed, and autonomous fashion. the technological advances outlined above and discussed in more detail in the next chapter call for a careful reassessment of the assumptions that underlie spectrum policy.expansion in applications and usersthe transition from wired and xed placetoplace communications to wireless mobile persontoperson (and devicetodevice) communications has been under way for decades.3 radio, once conned to largely unidirectional transmissions from a small number of broadcasters to a large number of passive receivers, has blossomed to include bidirectional communication among a much larger numbers of devices. the number of people actively using wireless communications has grown dramatically: only a couple of decades ago, there were thousands of radio and television broadcasters, a half million amateur radio operators, and a few million mobile radio users worldwide; today there are billions of mobile telephone users, hundreds of millions of wireless local area network (wlan) users, and similarly large numbers of lowpower inhome and personal devices. many other services and products ranging from satellite television to global positioning systems (used, for instance, in automobile navigation systems) to public safety communications make use of spectrum licensed to specic companies, government agencies, or other entities.perhaps most familiar and notable is that there are nearly 300 million cell phone subscribers in the united states4 and 5 billion subscribers world2  although it is strictly speaking a technique for exploiting the other degrees of freedom, modulation or code is often referred to as another degree of freedom because it can be used to allow separation of signals that appear to be at the same frequency, time, and space.3  donald c. cox, ﬁwireless personal communications: what is it?ﬂ ieee personal communications, april 1995, pp. 2035. this paper notes the transition occurring already as far back as 1995 due to wireless communications. 4  ﬁctiašthe wireless association, wireless quick facts: midyear figures,ﬂ available at http://www.ctia.org/media/industryinfo/index.cfm/aid/10323.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.18 wireless technology prospects and policy optionswide.5 many everyday products that have been sold by the hundreds of millionsšsuch as cordless phones, baby monitors, security systems, garage door openers, keyless entry for automobiles, and a wide variety of wlan productsšmake use of socalled open bands for which individual licenses are not required and only lowpower transmissions are permitted. these two familiar examples are notable both for their success and for their distinct features. wlan technology enabled the rapid and ˚exible deployment of a wide variety of devices. cell phones became nearly  ubiquitous as a result of large capital investments and the spectral efciency achieved by their technology. the success of the cell phone industry was predicated on the solution of an extremely difcult (indeed nearly insurmountable) engineering problem in the presence of a huge, visible, obvious, wellunderstood market opportunityšuniversal mobile telephony. in contrast, wlans involved solving a simpler engineering problem for a market with considerable potential but less certain value. many wireless devices use multiple wireless systems and technologies. cell phones now often include bluetooth capability,6 allowing them to connect to wireless headsets and vehicle audio systems7 as well as the cellular telephone system. laptop computers today may contain wireless lan, bluetooth, and cellular communications capabilities. a digital video recorder might connect to a home wireless network to allow sharing photographs and music from other computers on the network while also receiving broadcast signals over the air and commercial satellite television signals. both wireless lan and cellular capabilities are being built into new types of consumer electronics such as electronic book readers.military applications of wireless technology have expanded well beyond voice communications and radar systems, and many applications initially developed for military purposes have found widespread commercial or civilian use. for instance, the global positioning system (gps) was launched as a military application and is now used by hikers, invehicle navigation systems, and even in golf carts. more recently, wireless technology has been applied to machinetomachine communications, with expectations that such communications will exceed those involving humans within the next few years.8 fleet 5  estimates were that by the end of 2010, there would be 5.3 billion mobile subscriptions worldwide. see international telecommunication union (itu), the world in 2010: ict facts and figures. geneva.6  bluetooth wireless technology is one of several shortrange communications technologies intended to replace the cables connecting portable and xed devices. 7  the increasing prevalence of laws requiring handsfree operation of cellular phones in automobiles in the interest of safety concerns is driving increased interest in this application of wireless technology.8  ﬁa world of connections: a special report on telecoms,ﬂ p. 5 in the economist, april 28, 2007. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.introduction: trends and forces reshaping the wireless world 19management, supply chain and logistics management, automated meter reading, security monitoring systems, vending machines, and sensor networks monitoring industrial process are just a few examples of the applications already in use and being developed. these distributed control systems made up of sensors, remote devices, and actuators are linked into wireless networks via wireless communications channels.9 radio frequency identication (rfid) uses wireless communication to identify tagged objects. although this prospect has been anticipated for some time,10 such applications are now being more widely adopted. applications of wireless technology are moving from any time and any place to include any thing.11 in short, wireless technology is spread broadly across all activities of daily life and is becoming an ever more integral and indispensable part of those activities. reports of how the wireless revolution is changing everyday life abound in the news, and they include news of the pervasive and ubiquitous computing enabled by wireless communications, making all sorts of previously impossible things possible. these changes are driven by technological advances and by the creation of new applications that make use of those advances to provide new services and create new markets. the potential is real, but realizing it, with all of its implications for more and more wireless communications of all types, will continue to strain the spectrum management regime.wired versus wireless communication (propagation versus backhaul)fiber optics nally led to the demise of grove™s law, which (contrasting the remarkable rate of improvements in computing performance with the slower rate of improvements in the performance of deployed communications capabilities) forecast a doubling of the bandwidth of the telephone system every 100 years.12 the effect of rebuilding the cable and telephone industries with an abundance of beroptic technology has been transformative, as has been the deployment of broadband local access infrastructure using ber, digital subscriber line,13 and cable modem technology. the most signicant impact for wireless of the investment in this 9  andrea goldsmith, wireless communications, cambridge university press, 2005.10 national research council, embedded, everywhere, the national academies press, washington, d.c., 2001. 11 international telecommunication union, internet reports 2005: the internet of things, united nations, 2005.12 see, for instance, national research council, dening a decade: envisioning cstb™s second 10 years, proceedings of computer science and telecommunications board™s 10th anniversary symposium, national academy press, washington, d.c., 1996.13 interestingly, digital subscriber line networks pose their own spectrum management challenges because wire pairs within the telephone wire plant radiate into each other.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.20 wireless technology prospects and policy optionsinfrastructure has been a signicant reduction in the need for medium and longrange propagation of radiospectrum signals. in effect, wireless technology has become an important (though not exclusively) local access technique for interconnection with a huge ber transport infrastructure for voice, data, and, increasingly, video transmission. fiberoptic connections frequently provide these ﬁbackhaulﬂ services, which are needed to connect distributed sites (such as cell towers) to the network. of course, a backhaul role remains for wireless links, such as microwave and satellite communications, but the tremendous breakthrough in the cost and capacity of beroptic technology has shifted the focus of wireless communications more toward ﬁlastmileﬂ and ﬁlastmetersﬂ issues. another consequence is that the market in wireless services is more closely linked to the market in lastmile wireline communications services. this shift increases the importance of wireless services that operate at shorter ranges. at the shortest ranges, neareld communication is used in such applications as touchless public transportation passes, and rfid is used for communication between, for example, vehicle transponders and tollbooths. changing market dynamicswireless technologies are making possible valuable new services and products. most largescale commercial applications of wireless technology have until recently operated using licensed spectrumšspectrum in which only the assigned user can operate and offer services according to the terms of its license. broadcast television and radio, satellite communications, and cellular telephone systems are prominent examples. as personal wireless communications and related data services are improved, demand for spectrum to be used by individuals and devices continues to increase. as previously discussed, a growing number of devices (including laptops, tablets, cell phones, electronic book readers, cameras using wifi, headsets and other devices using bluetooth, and sensors and controls using such protocols as zigbee) operate in open bands in which dened technical rules for both the hardware and the deployment methods are employed to enable shared use without license rights or guarantees of protection from interference. such capabilities are being deployed by individual users (households with wifi for sharing a broadband connection throughout their house); schools, other organizations, and rms (to provide connectivity within their premises); communications carriers (to complement their offerings using licensed spectrum or wireline connections); and local governments (for their own use or to extend communications within their communities). this complementary approach is often credited with having allowed the rapid development of new products and services. spectrum wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.introduction: trends and forces reshaping the wireless world 21policy, service offerings, and business models have all been evolving to take advantage of licensed operation as well as operation in open bands. some currently licensed spectrum uses are facing competition or replacement by technologyenabled alternatives. for instance, terrestrial broadcast television now competes with both cable and satellite transmission, and they all compete with video delivered (by streaming or download) over the internet. spectrum once dedicated to a particular use becomes less valuable as alternative uses become more valuable. an obvious example is the spectrum once reserved for analog television broadcasting channels and freed when broadcast television completed its transition to alldigital transmission. the question of what to do with the ﬁwhite spaceﬂ created by freeing spectrum previously allocated for television channels 2 to 51 has highlighted many of the arguments about the merits of licenses, the possibilities for using markets to shift spectrum to new uses, and the role of openband approaches.14 still another aspect of shifting market dynamics is related to the globalization of markets. global markets for wireless communications devices have been driven not so much by global travelers, which are relatively few, as by the global economies of scale associated with common components, common products, and consistent standards that make it possible to develop products and services for large markets. where differences do exist, decreasing component costs and increasing miniaturization have enabled multimode devices such as tri and quadmode cell phones that sidestep some of the harmonization issues. the evolving policy and regulatory frameworkthere appears to be a broad consensus that the current framework for spectrum policy is ripe for change.15 this attitude re˚ects recognition of the shortcomings of centralized government management of spectrum use as well as the need to accommodate present and emerging technological capabilities such as those discussed in chapter 2. a number of signicant policy changes re˚ect efforts to adjust to new technologies and to shift some control from central management to markets and open bands. this section reviews the origins of the present policy regime and some recent efforts to make changes.14 see testimony submitted to the federal communication commission, ﬁunlicensed operation in the tv broadcast bands,ﬂ et docket no. 04186, and ﬁadditional spectrum for unlicensed devices below 900 mhz and in the 3 ghz band,ﬂ et docket no. 02380.15 fcc, ﬁreport of the spectrum policy task force,ﬂ et docket no. 02135, november 2002, p. 11; government accountability ofce (gao), telecommunications: comprehensive review of u.s. spectrum management with broad stakeholder involvement is needed, gao03277, washington, d.c., january 2003, p. 3. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.22 wireless technology prospects and policy optionshistorythere are several potential historiographies of the emergence of wireless communications policy in the united states. each represents a particular perspective on the proper role for government and for markets in the management of spectrum. this section starts with a brief summary of the ofcial administrative storyšthat is, the legislative and regulatory actions beginning with the radio act of 1912. both the supreme court, when it initially upheld the role of the federal communications commission (fcc) in licensing wireless systems, and the fcc in various reports (such as the spectrum policy task force report described below in this report) re˚ect this perspective. three additional perspectives re˚ect actual or perceived motivations, priorities, and consequences from alternative points of view. often unstated or implied in current spectrum policy debates, these stories color the assumptions and arguments made by the diverse policy stakeholders, with numerous important implications for spectrum policy analysis. they also serve to reveal the many potential pitfalls for spectrum policy making. ofcial (administrative) storythe administrative story begins with the demise of the titanic and the sense that potential rescuers could not be reached because of a lack of coordinated communications. the radio act of 1912 was meant to address such issues, but a 1926 court decision in united states v. zenith radio corp. held that the 1912 act did not allow the secretary of commerce (under authority from the president) to refuse licenses.16 that decision led to an 8month period when the law broke down and a cacophony of signals was transmitted, so that no one could be heard, followed by the rapid passage of the radio act of 1927. the provisions of the 1927 act were mostly incorporated into the communications act of 1934, which unied the regulatory regime for nongovernmental use of spectrum for telephone, telegraph, and radio under the control of the fcc. regulation of governmental spectrum use was assigned to the executive branch, and eventually, in the 1970s, to the national telecommunications and information administration (ntia) of the department of commerce. this split addressed concerns about concentrating licensing authority, as re˚ected in the 1926 court decision.17 these two agencies, the fcc and the ntia, must coordinate to accommodate the full range of spectrum users since no spectrum is specically mandated for exclusive federal or nonfederal 16 united states v. zenith radio corp. et al., 12 f. 2nd 614 (n.d. ill., 1926).17 gao, telecommunications: better coordination and enhanced accountability needed to improve spectrum management, gao02906, washington, d.c., september 2002, p. 2. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.introduction: trends and forces reshaping the wireless world 23use.18 the system put in place in 1934 is largely the system that we have to this day.19 this historiography presents spectrum management as a straightforward technical problem, to be solved to the extent possible and necessary by the most direct and straightforward regulatory mechanism. government control storythe government story starts with a focus on the navy™s efforts to control the airwaves since the early 20th century, efforts that had been almost entirely successful as the united states entered the first world war. it then follows the battle over the following decade that resulted in direct control (through the independent radio advisory committee and the ntia) over much of wireless communications capacity, and indirect control through the privatepublic arrangement embodied in the fcc over the remainder. there are nuances to this story. early versions focused on overly zealous regulation and the scarcity of capacity it caused.20 newer versions focus more heavily on the positive political theory (i.e., the use of game theory and other formal methods) of legislation.21 the primary practical lessons of this perspective are that any form of regulatory solution, however well designed, can have undesired results, including corruption or failure, so that the institutional design of the regulatory system aims to minimize the role of selfconscious policy making. business storythe business story focuses on the moves of the industrial players in the rst quarter of the 20th century. it follows the path from marconi to de forest, the joining in of at&t and later ge and westinghouse, the formation of rca, and the patent pools of 1920.22 in this story, a series 18 u.s. department of commerce, spectrum policy for the 21st centuryšthe president™s spectrum policy initiative: report 1, june 2004, pp. 810. 19 fcc, ﬁreport of the spectrum policy task force,ﬂ et docket no. 02135, november 2002, p. 7. additional source: nbc v. u.s. 319 u.s. 190, 1943.20 r.h. coase, ﬁthe federal communications commission,ﬂ journal of law and economics 2(october):140, 1959; jora r. minasian, ﬁproperty rights in radiation: an alternative  approach to radio frequency allocation,ﬂ journal of law and economics 18(1; april):221272, 1975.21 thomas w. hazlett, ﬁthe rationality of u.s. regulation of the broadcast spectrum,ﬂ journal of law and economics 33(1):133175, 1990; thomas w. hazlett, ﬁassigning property rights to radio spectrum users: why did fcc license auctions take 67 years?ﬂ journal of law and economics 4(2):529576, 1998. 22 yochai benkler, ﬁovercoming agoraphobia: building the commons of the digitally networked environment,ﬂ harvard journal of law and technology 11(winter):287, 19971998.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.24 wireless technology prospects and policy optionsof business decisions by the primary manufacturers of transmission and reception equipment in the second and third decades of the 20th century led to the emergence of the broadcast model.through a variety of techniques, some developed in the market, some through the patent system, and some through the regulatory system, the broadcasting industry had settled by 1926 on the advertisersupported networks using governmentgranted exclusive licenses that dominated until very recently. the following years of industry consolidation saw a shift from what was primarily an equipmentmarketdriven phenomenon in the 1920s (e.g., the need to create demand for receivers as the economic rationale for the creation of the national broadcasting company) to an advertisersupported entertainment service by the 1930s. it also saw the shift from spectrum allocation by the secretary of commerce to allocation by an independent agency, the fcc. however, the basic structure was set in place even beforešand independent ofšformal legislation.23 the primary signicance of perspective as a guide to contemporary policy making is in regard to the need to pay particular attention to the business structure of the markets in wireless communications equipment and wireless services and their implications for proposed institutional designs. publicinterest advocates versus commercial broadcasters storya third, and nal, nonofcial story is the story of the battle between entrenched broadcasters and advocates concerned with a public interest in spectrum and publicly minded broadcast policy. in this story, much of the action that matters most occured later than in either of the two other nonofcial storiesšin the period between the advent of broadcast radio and passage of the communications act of 1934. during that time, a variety of education, labor, religious, press, and civic groups opposed the networkbased and advertisingsupported system that was emerging and advocated for setting aside signicant capacity for nonprot and noncommercial broadcasting.24 the story is important because its primary elements continue to describe a fairly broad perception of the political stakes in wireless communications policy. broadcast communications policy is perhaps the most visible of wireless policies for most americans. the construct of the ﬁpublic interestﬂ evokes strong political emotions and deeply held beliefs. the political power of broadcasters, coupled with 23 erik barnouw, a history of broadcasting in the united states: volume 1: a tower of babel: to 1933, oxford university press, new york, 1966; hugh g.j. aitken, ﬁallocating the spectrum: the origins of radio regulation,ﬂ technology and culture 35(4):686716, 1994. 24 robert w. mcchesney, telecommunications, mass media, and democracy: the battle for the control of u.s. broadcasting, 19281935, oxford university press, new york, 1994. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.introduction: trends and forces reshaping the wireless world 25the belief that this particular area of policy is especially important for, and amenable to, political action, creates important constraints on the range of policies practically open for reform. allocation, assignment, and licensingthe allocation of frequencies for a particular use (what is permitted to operate in a range of frequencies) is distinct from their assignment (who is permitted to use that range of frequencies). allocation was historically made through rule making; recent years have seen a shift from assignment by comparative hearing to auctions and the introduction of secondary markets to allow marketbased reassignment.the vast majority of licenses to operate wireless devices and systems in the united states are assigned in an administrative process either by the fcc, which has jurisdiction over use by private and state, local, and tribal users, or by the ntia, which has jurisdiction over use by federal agencies. the fundamental principal for regulation of transmitters is that it is impermissible to operate a wireless communications transmitter in the united states except by license, unless the device has very well dened technical characteristics that allow it to be operated under one of the fcc™s permissive frameworks for unlicensed operation. licenses typically include limits on the use of the equipment licensed which are typically designated in terms of the following:ł the frequency of signals transmitted by the system;ł the bandwidth of the signals;ł the power of the transmitter, given the bandwidth used; ł the antenna location and height or other design characteristics (such as direction);ł the number of other potential licensees to use equipment with equivalent characteristics; and ł the relations among licensees (e.g., license exclusivity and the presence of secondary and primary users).licenses typically also limit the types of services that can be offered; for example, a television band licensee cannot use that spectrum for any other use.25 devices that receive and decode but cannot transmit wireless communications are not subject to the same regulatory framework (although 25 the advantages of not specifying particular services are compellingly illustrated in the diversity of services that have been implemented in unlicensed bands.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.26 wireless technology prospects and policy optionssome, like police radar detectors, may be regulated in other contexts). note that because receivers contain local oscillators (to detect the signal or for their computational elements) that may interfere with other transmissions, they are subject to limits on these unintentional emissions.overview of recent policy developmentsstarting with changes made to the communications act in 1983, congress has sought to encourage competition and innovation and to recognize the evolving technological reality.26 today, increasing use is being made of less centralized mechanisms using markets in both spectrum rights and open bands. changes to the communications act authorize the fcc to collect license fees, conduct spectrum auctions, and provide for spectrum allocation ˚exibility.27 auctions have seen increasing use for making assignments, and secondary spectrum markets are emerging. the opening of new bands and the auctioning of spectrum rights, together with signicant technological developments, is credited, for example with having enabled tremendous growth in the number of cell phone subscribers. complementing these marketbased mechanisms has been growing use of open bands, in which all users are free to operate subject only to rules of the road.28 this development had its origins in the decision to establish the socalled industrial, scientic, and medical bands at 900 mhz and at 2.4 and 5.8 ghz as open bands, an action that helped pave the way for today™s widespread use of wlans. in recent years, two u.s. government initiatives aimed at stimulating broad reform were launchedšthe fcc 2002 spectrum policy task force report and associated ongoing activities, and the president™s spectrum policy initiative of 2004.29 recent specic policy changes have included approval of ultrawideband operation, which represents a new, fundamentally different way of thinking about wireless transmission and is also the rst instance 26 47 u.s.c. 157, ﬁnew technologies and services.ﬂ 27 fcc, ﬁreport of the spectrum policy task force,ﬂ et docket no. 02135, november 2002, pp. 78.28 a variety of terms describe this approach, including ﬁlicenseexemptﬂ or ﬁlicense by rule.ﬂ this approach is probably most familiar as the basis for operation of wlans, cordless telephones, and the like.29 fcc, ﬁreport of the spectrum policy task force,ﬂ et docket no. 02135, november 2002; fcc spectrum policy task force, report of the spectrum efciency working group, november 15, 2002; u.s. department of commerce, spectrum policy for the 21st centuryšthe president™s spectrum policy initiative: report 1, june 2004.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.introduction: trends and forces reshaping the wireless world 27of approval for the overlay of existing services;30 changes in licensing procedures to accommodate softwaredened radios and proceedings regarding adaptive radios;31 a decision to permit lowpower devices to operate on vacant broadcast television channels;32 issuance of a notice of inquiry for a spectrumsharing test bed to be shared among federal and nonfederal users;33 and adoption of rules and development of technical measures enabling the sharing of spectrum at 5 ghz between existing military radar systems and lowpower unlicensed devices.34two recent federal policy initiativesseveral major federal policy initiatives were launched in recent years. these include the two described belowšthe fcc spectrum policy task force (and a series of proceedings that followed) and the president™s spectrum policy initiativešas well as the fcc national broadband plan that was released in march 2010.fcc spectrum policy task force (2002)seeking to exploit the opportunity opened by new technological capabilities, the spectrum policy task force (sptf) approached not only the problem of the need for changes to spectrum management and allocation but also the longterm need to allow further change to happen readily in anticipation of such technological advance. the sptf report of 2002 introduced new models and ways of thinking about the rights of users and licensees, about the accommodation of market forces, and about the preparation for future radio technologies beyond the horizon.35 the fcc chair formed the sptf in 2002 to help the fcc improve spectrum policy management in recognition of the challenges it faces to ﬁkeep pace with the everincreasing demand for spectrum and the continuing 30 fcc, order fcc 0248, et docket no. 98153, february 14, 2002.31 an adaptive radio and radio technology are commonly referred to as a ﬁcognitive radioﬂ or a ﬁsmart radio,ﬂ dened in a 2005 fcc proceeding as a radio empowered to ﬁalter its transmitter parameters based on interaction with the environment in which it operates.ﬂ see fcc, report and order fcc 0557, et docket no. 03108, march 10, 2005, available at http://hraunfoss.fcc.gov/edocspublic/attachmatch/fcc0557a1.pdf. 32 fcc, et docket no. 04186, may 13, 2004. 33 fcc, et docket no. 0689, june 8, 2006, available at http://hraunfoss.fcc.gov/edocspublic/attachmatch/fcc0677a1.pdf. 34 fcc, report and order fcc 975, et docket no. 96102, january 9, 1997, available at http://www.fcc.gov/bureaus/engineeringtechnology/orders/1997/fcc97005.pdf. 35 fcc, ﬁspectrum policy task force report,ﬂ et docket no. 02135, november 2002, available at http://hraunfoss.fcc.gov/edocspublic/attachmatch/doc228542a1.pdf. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.28 wireless technology prospects and policy optionsadvances in wireless technology and applications.ﬂ36 the sptf report of november 2002 sought to provide a comprehensive and systematic review of fcc spectrum policy and to catalyze reform of that policy. the report offers a number of ndings and recommendations aimed at improving spectrum policy and ensuring that it is able to evolve with technology and applications. the 2002 sptf report summarizes the regulatory history of spectrum policy in the united states from its beginnings more than 90 years ago, covering both statutory and administrative aspects. it also notes that public interest use, such as for public safety communications and national defense, is an ongoing consideration of the regulatory process and is factored into policy decisions along with economic considerations driven by privatesector demand for services and applications. the sptf report makes the case for spectrum policy reform, stating that the dramatic increase in demand for spectrumbased services coupled with signicant and continuing technological advances makes reform not only possible but also necessary. it argues that these new and evolving dynamics are straining longstanding, outmoded spectrum policies that, unchanged, will fail to maximize the potential public benets of spectrumbased services and applications. specically, it notes the potential for ﬁsmartﬂ or ﬁopportunisticﬂ technology, such as softwaredened radios, to allow more ˚exible use of spectrum. additionally, the report notes that spectrum scarcity is of increasing concern. it refers to some evidence that allocated spectrum is being underutilized and calls for more comprehensive measurements of spectrum use to be undertaken. it sees better understanding of actual use as one means of identifying where scarcity might be mitigated through more efcient allocation and greater ˚exibility.the sptf report identies seven key elements for a new approach to spectrum policy:ł maximizing ˚exibility of spectrum use. a ˚exibleuse approach to spectrum policy, in contrast to the traditional commandandcontrol approach, allows licensed and unlicensed users maximum autonomy to determine the highestvalue use of their spectrum and allows them to make choices based on market factors.ł clear and exhaustive denition of spectrum rights and responsibilities. clarity in the rules governing use would create an environment for spectrum users to condently negotiate alternative arrangements for maximizing value. rules should be written to identify uses that are excluded, prohibited, or limited, allowing users to explore any options not explicitly prohibited. 36 ibid., p. 1. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.introduction: trends and forces reshaping the wireless world 29ł accounting for all dimensions of spectrum use. spectrum should be allocated using time in addition to traditional dimensions of frequency, space, and power. technology advances also make possible new approaches to allocation in these traditional dimensions. ł promoting efciency. three types of efciency are identied: spectral, technical, and economic. there are situations where spectral and technical efciency may take priority over economic efciency in order to promote public interest goals. however, economic efciency can be promoted by providing spectrum users with ˚exibility of use and ease of transferability. this could allow maximizing of the value of services provided.ł ﬁgood neighborﬂ incentives. to the extent possible grouping like systems or devices (e.g., lowpower systems with high sensitivity to interference) together in spectrum ﬁneighborhoods.ﬂł periodic review of rules. rules should be reviewed so that they can be adjusted in light of technological advances made since those rules were made. such reviews should be scheduled at intervals that permit adjustment of business plans and investments. ł enforcement. enforcement increases in complexity with the complexity of technology and applications. proper enforcement requires sufcient resources for monitoring use of the spectrum.the remainder of the 2002 sptf report focuses on approaches for avoidance of interference, alternative spectrum usage models, and promotion of access to spectrum. first, the sptf report addresses avoidance of interference, a problem that has been a major responsibility of the fcc from its beginning and has always been a challenge. the issues related to interference have increased in technical difculty and prevalence due to changes created by new technology and new applications. the sptf report argues that these changes will challenge the continued effectiveness of current approaches to managing interference avoidance. it states that a more quantitative approach to interference management should be pursued by the fcc. the sptf report recommends that the fcc move toward assessing interference based on realtime adaptation, actual spectrum use, and interactions between transmitters and receivers rather than on transmitter operations alone, as is currently done. control of interference could be improved by several methods other than measurement, including approaches that account for and promote receiver robustness, increased use of automated transmitter power and frequency, advanced antenna technology, tightening of outofband emission limits, harmonizing references to interference, developing technical bulletins explaining fcc rules regarding interference, and developing a bestpractices handbook. second, the sptf report examines alternative spectrum usage models. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.30 wireless technology prospects and policy optionsthree models are described, including commandandcontrol, exclusiveuse, and a commons (or openaccess) model. the sptf report concludes that spectrum policy is not generally best served by the traditional commandandcontrol approach but mostly requires striking a balance between the exclusive rights and commons models. the report presents the alternatives as offering a continuum over which elements of the different models may be incorporated in particular instances as necessary to best serve the public good. it identies factors that may favor the application of one model over another depending on circumstances. generally, the sptf report argues that the exclusiveuse model may best be applied where spectrum is relatively scarce and transaction costs associated with market mechanisms are relatively low. this contrasts with the commons model, which may best be applied where spectrum is relatively abundant and transaction costs associated with market mechanisms are relatively high. the sptf report views the commandandcontrol model as best only for fullling compelling publicinterest objectives such as conforming to treaty obligations (e.g., with respect to satellite transmissions), ensuring capacity for passive scientic observations, and supporting public safety communications. even in these cases other models should, according to the sptf report, be applied to the extent possible.37finally, the sptf report recommends approaches for promoting access to spectrum, which it views as essential to continued innovation. it notes the signicant market for unlicensed devices created in the relatively limited spectrum available for unlicensed use. it argues that further innovation is likely with additional available spectrum for such use. it also discusses how secondary markets involving the leasing of licensed spectrum rights might further promote access. in each of the three areas discussedšavoidance of interference, alternative spectrum usage models, and promoting access to spectrumšthe sptf report addresses transition issues that might arise.2008 president™s spectrum policy initiativethe commerce department has been leading an effort initiated by a presidential order to take a similar fresh look at the use and management 37 it is important to note that both the market and the commons approaches claim that they would reduce spectrum scarcity. the market approach would price spectrum to clear competing uses, and the commons approach would create the conditions for markets in more intelligent devices that can successfully communicate without displacing other communicationsšthat is, without ﬁusingﬂ spectrum. the primary differences, then, are whether transactions costs associated with market mechanisms are higher than those associated with commons approaches (e.g., dispute resolution) and whether devices can develop the ability to clear competing uses through coordination. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.introduction: trends and forces reshaping the wireless world 31of spectrum allocated to the federal government in various agencies and departments.the resulting federal strategic spectrum plan, released in march 2008 by the ntia, incorporates summaries of 15 agencyspecic plans and integrates planning needs of the ntia and other federal agencies.38 the plan™s aim is to support a new and evolving spectrum management system that enables more effective use of spectrum and allows dynamic access to it where feasible. according to the plan, the current system cannot readily accommodate innovations or new operational requirements. the plan states that a new model for spectrum management is required to meet the growing federal and privatesector need for spectrum. it recognizes that much of the growth will be below 5 ghz, implying that additional use must be supported in already heavily utilized spectrum space. the plan emphasizes the need for agility and an evolutionary model for spectrum management that can rapidly take advantage of technology advances, including advances in use of the various degrees of freedom. it notes that meeting the needs identied in the plan will require coordination among all stakeholders, including federal agencies, state and local public safety entities, and privatesector users as well as vendors and researchers developing and commercializing technology advances.the plan identies several specic future federal requirements for spectrum likely to drive spectrum policy and the methods needed to meet those requirements. first, more data and higher data rates will be needed for public safety communications and military applications, such as increased use of sensors and unmanned systems. increased application of wireless communications for law enforcement and other federal agency needs was a common theme in agencyspecic plans. second, the demand for satellite and spacebased services, including space research, global positioning systems, and remotesensing operations for meteorological services and climate research, is expected to increase, driving the need for spectrum to support them. use of highfrequency bands (between 3 and 30 ghz) and use of spectrum for radar and air trafc control were also identied by federal agencies as likely to grow over time. finally, the plan noted emerging applications above 30 ghz that may drive spectrum use in this frequency range over the long term. the 2008 plan outlines nearterm and midterm strategies for addressing federal spectrum policy needs and brie˚y discusses challenges and plans for developing longterm strategies. it notes that projection of future spectrum use is largely qualitative (based on anticipated require38 u.s. department of commerce, spectrum management for the 21st century: the president™s spectrum policy initiativešfederal strategic spectrum plan, march 2008, available at http://www.ntia.doc.gov/opadhome/opadwire.html.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.32 wireless technology prospects and policy optionsments) rather than quantitative. recognizing that privatesector spectrum needs are also likely to grow, the plan identies crucial improvementsšautomation and analytical tools, standardized generation of spectrum requirements, and spectrum forecasting methods. the nearterm strategy includes 10 elements for federal use of spectrum:ł use of commercial services where feasible;ł smart technologies such as softwaredened (cognitive) radios;ł flexible approaches to incentives for making underutilized spectrum available to other entities;ł a range of public safety issues, including interoperability, spectrum and infrastructure sharing, and expanded microwave backhaul;ł considerations for continuity of government;ł improving processing time for frequency assignment requests;ł improving methods for spectrum valuation and incentivizing economic efciency;ł improving technical efciency by such methods as optimizing sharing and tradeoff analysis;ł trend forecasting; andł better interagency and federal/private coordination.the plan identies two midterm strategies for improving spectrum management. first, it describes a unied approach to coordinating spectrum management at the federal level across the fcc, ntia, and dod. it also describes initial plans for creating a technology test bed to support exploration of new technologies and methods to share spectrum. the department of commerce spectrum management advisory committee, convened as part of the department™s spectrum policy initiative, issued a series of reports in late 2008 that examine denitions of efcient spectrum use, mechanisms for improving operational efciency, the transition of federal services to more efcient technologies, a spectrum sharing test bed, and federalnonfederal spectrum sharing.3939 see http://www.ntia.doc.gov/advisory/spectrum/csmacreports.html.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.332key technology considerationsradiofrequency (rf) communication saw a progression of innovation throughout the 20th century. in recent years, it has been transformed profoundly by technological advances, both in the capabilities of individual radios and in the design of networks and other systems of radios. this discussion presents some highlights of recent advances and their implications for the design of radios and radio systems and for regulation and policy. it does not aim to describe the full range of technical challenges associated with wireless communications; the interested reader is referred to the 1997 nrc report the evolution of untethered communications,1 which describes many of the fundamental challenges associated with wireless communications or, for a more recent view of the technology and its applications, several recent textbooks on wireless communications.2 1  national research council, the evolution of untethered communications, national academy press, washington, d.c., 1997.2  see, e.g., andrea goldsmith, wireless communications, cambridge university press,  cambridge, england, 2005; david tse and pramod viswanth, fundamentals of wireless communication, cambridge university press, cambridge, england, 2005; and theodore s.  rappaport, wireless communications: principles and practice, 2nd edition, prenticehall, upper saddle river, n.j., 2001. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.34 wireless technology prospects and policy optionstechnological advances in radios and systems of radiosdigital signal processing and radio implementation in cmosmodern communications technologies and systems, including those that are wireless, are mostly digital. however, all rf communications ultimately involve transmitting and receiving analog signals; box 2.1 describes the relationship between digital and analog communication.digital signal processing (box 2.2) is increasingly used to detect the desired signal and reject other ﬁinterferingﬂ signals. this shift has been enabled by several trends:ł increasing use of complementary metal oxide semiconductor (cmos) integrated circuits (box 2.3) in place of discrete components;ł the application of dense, lowcost digital logic (spawned primarily by the computer and data networking revolutions) for signal processing;ł new algorithms for signal processing;ł advances in practical implementation of signal processing for antenna arrays; and ł novel rf lter methods. the shift relies on an important tradeoff: although the rf performance of analog components on a cmos chip is worse than that of discrete analog components, more sophisticated computation can compensate for these limitations. moreover, the capabilities of radios built using cmos can be expected to continue to improve. the use of digital logic implies greater programmability.3 it is likely that radios with a high degree of ˚exibility in frequency, bandwidth, and modulation will become available, based on highly parallel architectures programmed with special languages and compilers. these softwaredened radios will use software and an underlying architecture that is quite different from conventional desktop and laptop computers, but they will nonetheless have the ability to be programmed to support new applications.high degrees of ˚exibility do come at a costšboth nancial and in terms of power consumption and heat dissipation. as a result, the wireless transceiver portion (as opposed to the application software that communicates using that transceiver) of lowcost consumer devices is unlikely to become highly programmable, at least in the near future. on the other 3  programmability of radio functionality is distinct from the increasing degree of application programmability being introduced into mobile phones and exemplied by smart phones for which a large number of userselected applications are available.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 35box 2.1  analog versus digital communicationsin common usage, the term ﬁanalogﬂ has come to mean simply ﬁnot digital,ﬂ as in ﬁanalog wristwatchﬂ or ﬁanalog cable tv.ﬂ but for the purposes of this report it is useful to trace the meaning to its original technical usage, in early computing. from about 1945 to 1965, an era when digital computers were very slow and very costly, differential equations describing a hypothetical physical system were solved (one might say modeled) by an interconnected network of properly weighted passive components (resistors and capacitors) and small ampliers, so that the smoothly timevarying voltages at various points in this network were precisely analogous to the time behavior of the corresponding variables (velocity, acceleration, ˚ow, and so on) of the system being modeled. today, we solve these same equations numerically on a digital computer, very quickly and at low cost.in a similar way, for roughly 100 years, signals were transmitted in analog form (over wires or wirelessly) with a smoothly varying signal, representing the changing level and pitch of voice; the hue, saturation, and brightness of each point in a video image; and so forth. but just as highspeed and lowcost  numerical representations and digital computations replaced analog computing, it likewise became much more reliable and less expensive to transmit digital coded numerical samples of a signal to be reconstituted at the receiver rather than to faithfully transmit a continuously varying analog representation. in digital communications, information is encoded into groups of ones and zeroes that represent timesampled numerical values of the original (voice, music, video, and so on) signal.ironically, in the wireless domain, once the analog signal has been encoded into a sequence of digital values, smoothly varying forms for the ones and the zeroes must be generated so that the transmitted signal will propagate. figure 2.1.1 shows a digital sequence of ones and zeros. the sharp onoff pulses that work so well inside a computer do not work well at all when sent through space between antennas. and so groups of ones and zeroes are represented by smooth changes in frequency, phase, or amplitude in a sinusoidal carrier, the perfect waveform of propagation. three schemes are illustrated in figures 2.1.2 through 2.1.4: amplitude shift keying of the carrier wave from 1 volt to 0 volts (figure 2.1.2), frequency shift keying of the transmission frequency from f0 to f1 (figure 2.1.3), and phase shift keying of the phase by 180 degrees (figure 2.1.4). these ones and zeroes are interpreted at the receiver in groups of eight or more bits, representing the numerical value or other symbol transmitted.continuedwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.36 wireless technology prospects and policy optionsfigure 2.1.2 amplitude shift keying. source: charan langton, ﬁtutorial 8šall about modulationšpart 1,ﬂ available at http://www.complextoreal.com. used with permission.figure 2.1.1 digital sequence of ones and zeroesš0010110010. source: charan langton, ﬁtutorial 8šall about modulationšpart 1,ﬂ available at http://www.complextoreal.com. used with permission.figure 2.1.1figure 2.1.2box 2.1 continuedwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 37figure 2.1.4 phase shift keying. source: charan langton, ﬁtutorial 8šall about modulationšpart 1,ﬂ available at http://www.complextoreal.com. used with permission.figure 2.1.3 frequency shift keying. source: charan langton, ﬁtutorial 8šall about modulationšpart 1,ﬂ available at http://www.complextoreal.com. used with permission.figure2.1.3figure 2.1.4wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.38 wireless technology prospects and policy optionsbox 2.2 digital signal processingfor the continuous sinusoidal signals that can be propagated from transmitter to receiver to be encoded, modulated, demodulated, and decoded using digital technology, they must be put into a digital form by using an analogtodigital converter (adc), and then a digitaltoanalog converter (dac) to return to analog form. for example, an adc might take 500 million samples per second, with a resolution of 10 bits (1 part in 1024 accuracy). then, the continuous signal being received would be represented by a series of samples each spaced 2 nanoseconds apart. a series of dots approximately represents the continuous function shown in figure 2.2.1.to nd the frequency domain representation of this function, we can calculate its fourier transform. but because it is now a sequence of discrete samples rather than a continuous mathematical function, we use an algorithm known as the discrete fourier transform (dft). it has the formxxemkjmknkn201.and the inverse dft has the formxnxekmjmknmn1201.in these two expressions, we use n time domain samples to compute n frequency components, and vice versa. a huge improvement on the dft is the fast fourier transform (fft) and the inverse fft (ifft). by always using n equal to a power of 2 (16, 32, 64, 128–), the calculation is greatly simplied. the fft and ifft are the foundation of modern digital signal processing, made possible by highspeed, lowcost digital cmos (see box 2.3).figure 2.2.1 representation of continuous function as series of digital samples. source: charan langton, ﬁtutorial 6šfourier analysis made easyšpart 3,ﬂ available at http://www.complextoreal.com. used with permission.figure 2.2.1wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 39box 2.3  complementary metal oxide semiconductor technologythe transformation of communications from analog to digital and the related dramatic reduction in costs and increased performance are a consequence of the revolution in semiconductor design and manufacturing caused by the emergence of the personal computer (pc) industry. in particular, the remarkable and steady increase in performance and reduction in feature size by a factor of two every 18 months, generally known as moore™s law, has driven aggressive innovation far beyond the pc industry. by far, the majority investment to enable this progress has been in the design and process development of complementary metal oxide semiconductor (cmos) technology. introduced in the 1960s, cmos is now used widely in microprocessors, microcontrollers, and other digital logic circuits as well as in a wide variety of analog circuits. this technology for constructing integrated circuits uses complementary and symmetrical pairs of ptype and ntype metal oxide semiconductor eldeffect transistors.investments also spawned a new industry structure: ﬁfablessﬂ companies, which design, market, and sell innovative products, along with silicon foundries, which manufacture the chips for these companies, spreading the capital investment in exotic equipment over large volumes.for example, today even a new, small company can design a complex part in cmos and have a foundry charge $1,000 to process a silicon wafer yielding, say, 5,000 chips (20 cents each). adding 10 cents for packaging and testing gives a cost of 30 cents for a part that is sold to a cell phone manufacturer for 40 to 60 cents. well over 1 billion cell phones are sold each year.hand, there are other applications, such as cellular base stations, where concurrent support of multiple standards and upgradability to new standards make transceiver programmability highly desirable. also, the decreasing cost of computation and memory opens up new possibilities for network and application design. the low cost of memory, for example, makes practical storeandforward voice instead of alwayson voice. this capability creates new opportunities for modestlatency rather than realtime communication and may be of increasing importance to applications such as public safety communications. digital signal processing of the audio can also, for example, be used to enhance understandability in (acoustically) noisy environments.44  note that some forms of digital signal processingšcompression and some algorithms used to encode and decode audio (vocoders)šcan adversely affect audio quality in certain applications. for example, the vocoders in early digital mobile phones did not cope well with wind and road noise, and there have been reports that vocoders in digital public safety systems poorly transmit such important sounds as sirens and gunshots.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.40 wireless technology prospects and policy optionsthe pace of improvement in digital logic stands in contrast to the much slower pace of improvement in analog components. one consequence of this trend is that it becomes potentially compelling to reduce the portion of a radio using discrete analog devices and instead use digital signal processing over very wide bandwidths. however, doing so presents signicant technical challenges. as a result, at least for the present, the development of radios is tied to the pace of improvements in analog components as well as the rapid advances that can be expected for digital logic, although promising areas of research exist that may eventually overcome these challenges. digital modulation and codingmodulation is the process of encoding a digital information signal into the amplitude and/or phase of the transmitted signal. this encoding process denes the bandwidth of the transmitted signal and its robustness to channel impairments. box 2.4 describes how waveforms can be constructed as a superposition of sinusoidal waves, and box 2.5 describes several modern modulation schemes in use today. the introduction of the more sophisticated digital modulation schemes in widespread use todayšsuch as cdma and ofdm, whereby different users using the same frequency band are differentiated using mathematical codesšhave further transformed radio communications (see box 2.6).many important advances have also been made in channel coding, which reduces the average probability of a bit error by introducing redundancy in the transmitted bit stream, thus allowing the transmit power to be reduced or the data rate increased for a given signal bandwidth. although some of the advances come from the ability to utilize everimproving digital processing capacity, others have come from innovative new coding schemes (box 2.7).low cost and modularitythe low cost and modularity (e.g., wifi transceivers on a chip) that have resulted from the shift to largely digital radios built using cmos technology make it cheaper and easier to include wireless capabilities in consumer electronic devices. as a result, developing and deploying novel, lowcost, specialized radios have become much easier, and many more people are capable of doing so. a likely consequence is continued growth in the number of wireless devices and in demand for wireless communications. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 41 box 2.4 power spectra and the frequency domainlate in the 1600s, josef baron fourier rst proved that any periodic waveform can be represented by a (possibly innite) sum of pure sinusoidal functions of various amplitudes. this result is surprising but true, however little the original waveform may resemble a smooth sine or cosine function. for example, a perfect square wave x(t) can be represented by the innite seriesx(t) = sin t + (1/3) sin 3t + (1/5) sin 5t + (1/7) sin 7t + –figure 2.4.1 shows that adding the waveforms of just the rst four terms of this equation already begins to approximate the square wave, an approximation that improves as more terms are added.this square wave can be composed by adding an increasing number of sine waves that are odd harmonics of the basic frequency of the square wavešthat is 3, 5, 7, and so forth times the frequencyšand 1/3, 1/5, 1/7, and so forth times the amplitude.needless to say, it is impossible in practice to combine an innite number of sine waves, but then it is also impossible to produce a perfect square wave, rising and falling in zero time. but we certainly can generate waves with very, very fast rise and fall times, and the faster they are the larger the number of harmonics they contain. consider just the simple case of the 3rd, 5th, and 7th harmonics.this collection of sine waves can be represented in another way, by showing the amplitude of each frequency component visually. this amplitude spectrum (figure 2.4.2) represents the signal amplitude in the frequency domain. a signal figure 2.4.1 representation of square wave (solid line) by the sum of 1, 2, 3, and 4 sinusoidal waveforms (dashed lines). amplitudetimefigure 2.4.1continuedwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.42 wireless technology prospects and policy optionsalso has a frequency domain representation of the power in a signal, which is proportional to the square of the amplitude. especially in the case of signals radiating from an antenna, we usually show the signal power spectrum as consisting of equal positive and negative frequencies or sidebands, with half of the power in each sideband. thus, the power spectrum of the signal from figure 2.4.1 would look like the spectrum shown in figure 2.4.3.these ideallooking spectra result from combining perfectly stable, pure sine waves of precise frequencies, which are also impossible to achieve in practice. nevertheless, the spectra do illustrate the relationship between the coefcients of the timedomain harmonics in the fourier series, and the frequencydomain components in the amplitude and power spectra. these are more clearly related by the fourier transform, which accepts a time domain representation of a  signal, such asxt, and returns a frequency domain representation:xxtedtjt.the inverse fourier transform accepts a frequency domain representation xand returns the corresponding time domain representation:xtxedjt12.these two transformations are extremely important in modern wireless,  because they allow information to be encoded by including or excluding different frequencies from a transmitted signal and then detecting these at the receiver, in order to symbolize data in a way that is very resistant to interference and noise. these continuous integral equations form the basis for the discrete computations described in box 2.2. this requires highspeed, specialized computations.figure 2.4.2 signal amplitude represented in the frequency domain.box 2.4 continuedfigure 2.4.2wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 43also has a frequency domain representation of the power in a signal, which is proportional to the square of the amplitude. especially in the case of signals radiating from an antenna, we usually show the signal power spectrum as consisting of equal positive and negative frequencies or sidebands, with half of the power in each sideband. thus, the power spectrum of the signal from figure 2.4.1 would look like the spectrum shown in figure 2.4.3.these ideallooking spectra result from combining perfectly stable, pure sine waves of precise frequencies, which are also impossible to achieve in practice. nevertheless, the spectra do illustrate the relationship between the coefcients of the timedomain harmonics in the fourier series, and the frequencydomain components in the amplitude and power spectra. these are more clearly related by the fourier transform, which accepts a time domain representation of a  signal, such asxt, and returns a frequency domain representation:xxtedtjt.the inverse fourier transform accepts a frequency domain representation xand returns the corresponding time domain representation:xtxedjt12.these two transformations are extremely important in modern wireless,  because they allow information to be encoded by including or excluding different frequencies from a transmitted signal and then detecting these at the receiver, in order to symbolize data in a way that is very resistant to interference and noise. these continuous integral equations form the basis for the discrete computations described in box 2.2. this requires highspeed, specialized computations.figure 2.4.3 power spectrum representation of the signal shown in figure 2.2.1. figure 2.4.3frequencyamplitudewireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.44 wireless technology prospects and policy options box 2.5  modern modulation techniquesgaussian minimum shift keying (gmsk) is the most widely used form of frequency shift keying (see box 2.1), as a result of its adoption in the global system for mobile communications (gsm), the standard secondgeneration (2g) air interface used by 80 percent of cellular phones worldwide. the sharp digital pulse used to perform the frequency shift is rst softened to a gaussian shape, reducing unwanted harmonics. the dominant worldwide cellular phone system gsm uses a simple constantamplitude sine wave, with all modulation done by gmsk.quadrature phase shift keying (qpsk) is a technique that allows two bits of information to be sent concurrently. two identical carriers 90 degrees out of phase (inphase, i and quadrature, q) are each modulated with a 0 degree or 180 degree phase shift to represent a one or a zero. these two modulated  carriers are then added and transmitted, giving four different values, or two bits of information, when received and decoded, as shown in figure 2.5.1.quadrature amplitude modulation (qam) is a technique that, like qpsk, uses two carriers 90 degrees apart (i, q). but instead of phase modulation, qam uses amplitude modulation. for example, 16qam has four amplitude values for i and four values for q. when the two are combined and transmitted, there are 16 possible combinations, corresponding to 4 bits, as shown in figure 2.5.2.qi11100001figure 2.5.1figure 2.5.1 possible pairs of bits transmitted using quadrature phase shift keying.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 45 box 2.5  modern modulation techniquesgaussian minimum shift keying (gmsk) is the most widely used form of frequency shift keying (see box 2.1), as a result of its adoption in the global system for mobile communications (gsm), the standard secondgeneration (2g) air interface used by 80 percent of cellular phones worldwide. the sharp digital pulse used to perform the frequency shift is rst softened to a gaussian shape, reducing unwanted harmonics. the dominant worldwide cellular phone system gsm uses a simple constantamplitude sine wave, with all modulation done by gmsk.quadrature phase shift keying (qpsk) is a technique that allows two bits of information to be sent concurrently. two identical carriers 90 degrees out of phase (inphase, i and quadrature, q) are each modulated with a 0 degree or 180 degree phase shift to represent a one or a zero. these two modulated  carriers are then added and transmitted, giving four different values, or two bits of information, when received and decoded, as shown in figure 2.5.1.quadrature amplitude modulation (qam) is a technique that, like qpsk, uses two carriers 90 degrees apart (i, q). but instead of phase modulation, qam uses amplitude modulation. for example, 16qam has four amplitude values for i and four values for q. when the two are combined and transmitted, there are 16 possible combinations, corresponding to 4 bits, as shown in figure 2.5.2.qi0000010011001000000101011101101100110111111110111110101000100110figure 2.5.2figure 2.5.2 possible groups of four bits transmitted using quadrature amplitude modulation.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.46 wireless technology prospects and policy optionsbox 2.6  code division multiple access and  orthogonal frequency division multiple accessbecause the efcient use of spectrum is important, particularly the expensively licensed cellular spectrum, it is important that as many users as possible be able to access a given frequency band without interfering with one another. traditionally, this has been accomplished by assigning a narrowband subfrequency ﬁchannelﬂ to each user (frequency division multiple access; fdma), or by providing brief, highdatarate recurring time slices or slots to each user in turn (time division multiple access). as both the number of concurrent users and the demand for higher data rates have increased, new techniques have emerged that exploit digital processing to support these requirements without increasing interference. code division multiple access (cdma) has become increasingly important as the basis for higherdatarate and more spectrally efcient thirdgeneration (3g) mobile phone systems. cdma is a spread spectrum multiple access technique. in cdma a locally generated pulse train code runs at a much higher rate than the data to be transmitted. data for transmission are added using an exclusive or logical operation with the fasterpulse train code. each user in a cdma system uses a different code to modulate the signal. the signals are separated by digitally correlating the received signal with the locally generated code of the desired user. if the signal matches the desired user™s code, the correlation function will be high and the system can extract that signal. if the desired user™s code has nothing in common with the signal, the correlation will eliminate it, treating it as noise (although it is actually rejected interference, the low correlation value makes it appear to be noise). thus a large number of users can occupy the same band of frequencies and still be able to uniquely separate their desired signals. these coded signals use various modulation techniques such as quadrature amplitude modulation (qam), phaseshift keying (psk), and the like. orthogonal frequency division multiple access (ofdma) uses a group of closely spaced, harmonically related subcarriers, each of which can be modulated by psk, qam, or other methods. they are then added and transmitted. because the subcarriers are harmonically related, they are said to be orthogonal and can easily be separated using a fast fourier transform and decoded. systems often use as many as a few thousand subcarriers in a single frequency band. rather than a particular subcarrier being assigned rigidly to each user, as in fdma, these subcarriers can be dynamically allocated among users, providing more subcarriers to different classes of users to give higher data rates, lower error rates, or other qualityofservice choices. also, transmitting a given payload in a given period of time over multiple channels at a lower data rate (e.g., 1 mbps over 16 channels) is much more power efcient than transmitting over a single channel at a higher data rate (e.g., 16 mbps over a single channel). ofdma is the access method in the 3gpp long term evolution and ieee802.22 (wimax) standards. the basic ofdm technique is also used in wlan ieee 802.11 (wifi), terrestrial radio digital audio broadcast, terrestrial digital television (dvbt and tdmb standards), and mobile digital television (dvbh and isdbt standards). wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 47new radio system architecturesnew networking technologies are transforming the architectures of radio systems, as seen in the introduction of more distributed, often internetprotocol (ip)based networks in addition to networks that rely on centralized switching. this shift is suggested by cell phones, in which every mobile phone is a transmitter as well as a receiver, but the shift goes further to architectures that do not have the centralized control of cellular systems. what was once a population of deployed radios consisting of a small number of transmitters and many receivers (which placed a premium on lowcost receivers and did not impose tight cost constraints on transmitters) is changing to a population that contains many more transceivers. especially in more densely populated areas, services that have box 2.7  evolution of coding schemesa major advantage of the increased use of digital cmos technology is the ability to encode information before modulation and transmission so that errors introduced into the radio signal during transmission and reception by noise or interference can be detected and corrected during decoding at the receiver. use of these techniques has made possible the accurate recovery of very tiny signals from heavy interference and noise. although deepspace communication was the original source of these ideas, they have since been incorporated as a fundamental enabler of modern wireless communications ranging from wireless local area networking to mobile phones and to satellite radio and television.for four decades, the workhorse combination making these advances possible has been the convolutional coder and the viterbi decoder, which remain the mainstay of many systems. the convolutional coder is a simple linear state machine of shift registers and exclusiveor gates, which can be made longer and more elaborate as needed by the expected transmission environment. it expands the data prior to modulation to include additional bits that, when decoded, permit error detection and correction. the viterbi decoder is a state machine that calculates metrics based on the current and prior received signals and makes the most likely decoding choice among the possible transmitted symbols. this scheme is the basis for cdma and global system for mobile communciations (gsm) digital cellular coding, as well as wifi and various modems.for the extremes of deepspace missions and certain other applications where very long codes are required, the viterbi algorithm becomes too complex, and a more recent technique of turbo coding is combined with reedsolomon errorcorrecting codes. it is likely that turbo coding will gradually assume a central role in broadband mobile applications. in applications that are tolerant of latency, closely related lowdensityparitycheck codes can provide even lower error rates.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.48 wireless technology prospects and policy optionsrelied on transmission from highpower central sites are giving way to more localized transmissions using eversmaller cells5 and mesh networks (box 2.8) that provide much greater capacity by enabling frequencies to be reused at a negrained level. dynamic exploitation of all degrees of freedomanother important shift in radios has been the ability to use new techniques to permit greater dynamic exploitation of all available degrees of freedom. theoretic communications capacity is the product of the number of independent channels multiplied by the shannon 5  for example, cellular carriers have introduced eversmaller cell sizes (e.g., micro, pico, femto) to optimize cost, capacity, and coverage, and broadcasters use local repeaters to extend coverage.box 2.8  capacity scaling of mesh networksmuch has been claimed about the scalability of mesh networks (wireless networks with no central control). leaving aside issues of cost and latency, the possibility that they could scale linearly is enticingšthat is, that adding more nodes would increase network throughput in proportion to the number of nodes added. if mesh networks were to scale linearly, they would offer innite capacity, which would have profound consequences for spectrum management policy. however, considerable doubt has been raised about such claims by research showing limitations to their scalability.research by kumar and gupta (2000) examined the question of capacity in mesh networks.1 their research showed that, making certain assumptions about how current technology operates, there was indeed a constraint on the ability of mesh networks to scale and that the ﬁcause of the throughput constriction is not the formation of hot spots, but the pervasive need for all nodes to share the channel locally with other nodes.ﬂ2 it considered networks where nodes were arbitrarily located as well as networks where nodes are uniformly distributed. these results were obtained assuming perfect information about node location and trafc demand and with stationary (nonmobile) nodes. capacity would decrease further should any of these assumptions not hold. the capacity limitations did not change when the network nodes were located in a two dimensional plane or on the surface of a sphere. the research further showed that splitting the communication channel into several subchannels did not change any of the results. although scaling was not linear, the results did show that networks composed of nodes with mostly closerange transactions and sparse longrange demands, such as those envisaged for smart homes, are feasible. that is, where nodes need to communicate only with nearby nodes, then all nodes can transmit data to nearby nodes at a bit rate that does not decrease with the number of nodes. further research has explored theoretical limits of scaling wireless networks if current technological limitations could be eliminated and optimal operational strategies could be devised.3 for instance, no assumptions were made that interference must be regarded as noise or that packet collision between nearby transmitters must necessarily be destructive. one result of this research was to show that better scaling can be achieved by different network architectures for information transport depending on attenuation. for relatively high attenuation, a multihop transport mode where load can be balanced across nodes appears to have the best scaling characteristics. for relatively low attenuation, multistage relaying with interference subtracted from the signal at each stage could theoretically attain unbounded transport capacity. yet achieving these theoretical limits would require not only overcoming existing technical limitations but also achieving fundamental advances in information theory to understand complex modes of cooperation between nodes in a network. 1 piyush gupta and p.r. kumar, ﬁthe capacity of wireless networks,ﬂ ieee transactions on information theory 46(2; march):388404, 2000.2 ibid. p. 391.3 liangliang xie and p.r. kumar. ﬁa network information theory for wireless communication: scaling laws and optimal operation,ﬂ ieee transactions on information theory 50(5):748767, 2004.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 49box 2.8  capacity scaling of mesh networksmuch has been claimed about the scalability of mesh networks (wireless networks with no central control). leaving aside issues of cost and latency, the possibility that they could scale linearly is enticingšthat is, that adding more nodes would increase network throughput in proportion to the number of nodes added. if mesh networks were to scale linearly, they would offer innite capacity, which would have profound consequences for spectrum management policy. however, considerable doubt has been raised about such claims by research showing limitations to their scalability.research by kumar and gupta (2000) examined the question of capacity in mesh networks.1 their research showed that, making certain assumptions about how current technology operates, there was indeed a constraint on the ability of mesh networks to scale and that the ﬁcause of the throughput constriction is not the formation of hot spots, but the pervasive need for all nodes to share the channel locally with other nodes.ﬂ2 it considered networks where nodes were arbitrarily located as well as networks where nodes are uniformly distributed. these results were obtained assuming perfect information about node location and trafc demand and with stationary (nonmobile) nodes. capacity would decrease further should any of these assumptions not hold. the capacity limitations did not change when the network nodes were located in a two dimensional plane or on the surface of a sphere. the research further showed that splitting the communication channel into several subchannels did not change any of the results. although scaling was not linear, the results did show that networks composed of nodes with mostly closerange transactions and sparse longrange demands, such as those envisaged for smart homes, are feasible. that is, where nodes need to communicate only with nearby nodes, then all nodes can transmit data to nearby nodes at a bit rate that does not decrease with the number of nodes. further research has explored theoretical limits of scaling wireless networks if current technological limitations could be eliminated and optimal operational strategies could be devised.3 for instance, no assumptions were made that interference must be regarded as noise or that packet collision between nearby transmitters must necessarily be destructive. one result of this research was to show that better scaling can be achieved by different network architectures for information transport depending on attenuation. for relatively high attenuation, a multihop transport mode where load can be balanced across nodes appears to have the best scaling characteristics. for relatively low attenuation, multistage relaying with interference subtracted from the signal at each stage could theoretically attain unbounded transport capacity. yet achieving these theoretical limits would require not only overcoming existing technical limitations but also achieving fundamental advances in information theory to understand complex modes of cooperation between nodes in a network. 1 piyush gupta and p.r. kumar, ﬁthe capacity of wireless networks,ﬂ ieee transactions on information theory 46(2; march):388404, 2000.2 ibid. p. 391.3 liangliang xie and p.r. kumar. ﬁa network information theory for wireless communication: scaling laws and optimal operation,ﬂ ieee transactions on information theory 50(5):748767, 2004. limit for a channel. in practice, the capacity (data rate) of an individual channel will be limited by the particular choice of modulation, coding scheme, and transmission poweršfor any particular prole of background channel noise. four independent degrees of freedom can be used to establish independent channelsšfrequency, time, space, and polarization.6 in the past, technology and the regulatory schemes that govern it have relied principally on a static separation by frequency and space. advances in digital signal processing and control make it possible for radios to exploit the available degrees of freedom on a dynamic basis and to coordinate their own use of the various degrees of freedom available so as to coexist with 6  polarization has seen practical application only for separating wireless signals for satellite and pointtopoint microwave services.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.50 wireless technology prospects and policy optionsone another and with uncoordinated spectrum occupants. antenna arrays enable more sophisticated spatial separation through beam forming in all three dimensions. today™s radio technologies can thus, in principle, take greater advantage of all the degrees of freedom (frequency, time, space, and polarization) to distinguish signals and to do so in a dynamic, negrained fashion. an important consequence is that a wider set of  parameters (beyond the conventional separation in frequency and space) can be used to introduce new options for allocating usage rights (i.e., dening what a user can do and what the user must tolerate) based on all of these degrees of freedom.flexibility and adaptabilitythe agility and the ˚exibility of radios are improving along with advances in the ability to more accurately measure communication channels (sensing), share channels (coordination), and adapt to the operational environment in real time (adaptation). more agile radios can change their operating frequency or modulation or coding scheme, can sense and respond to their environment, and can cooperate to make more dynamic, shared, and independently coordinated use of spectrum. digital logic advances make it possible for radios to incorporate signicant and growing computing power that enables them to coordinate their own use of the various degrees of freedom available so as to coexist with each other and with uncoordinated spectrum occupants. since much of the processing is performed digitally, the performance improvements popularly associated with moore™s law that characterize the computer industry are likely to apply to improvements in this type of processing. the result is that radios and systems of radios will be able to operate in an increasingly dynamic and autonomous manner. finally, increased ˚exibility poses both opportunities and challenges for regulators. although it is much more complex, costly, and power consuming, ˚exibility makes possible building radios that can better coexist with existing radio systems. coexistence is sometimes divided into underlay (lowpower use intended to have a minimal impact on the primary user) and overlay (agile utilization by a secondary user of ﬁholesﬂ in time and space of use by the primary user). such overlays and underlays might be introduced by rules requiring such changes or by rules that enable  licensees to agree to such sharing in exchange for a market price. moreover, ˚exibility allows building radios with operating parameters that can be modied to comply with future policy or rule changes or future service requirements. that is, devices are able to instantiate and operate on specied policies, and the policies (and the devices™ operation) can be modied. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 51besides providing regulators and system operators with a valuable new tool, this malleability poses new challenges, such as how to assure a radio™s security in the face of potential (possibly malicious) attempts to modify its software. possible scenarios include rogue software silently placing calls constantly (thus congesting the control channel) or altering the parameters of a cell phone™s transmitter so as to jam transmissions of cellular or other services. information system security experience from other applications suggests that it will be possible, with signicant effort, to provide reasonable security (i.e., against casual efforts to break it) but that it would be quite difcult using today™s state of the art to provide highly robust security against a determined attacker.7antennaswork has been done for many years on antennas that can operate over very wide frequency ranges. early theoretical work in this area on mode coupling of radiation into materials by such authors as chu,8 harrington,9 and hansen10 still stands today, and advanced research continues on such topics as fractal and nonresonant antennas. commercial products approximating wideband antenna technology include patch antennas, meander antennas for use at 2.4 and 5 ghz, and extreme spectrum antennas in the 2 to 6 ghz bands. in the past decade, an interesting new approach to improved wireless communication began to develop, based on using multiple antennas at both transmitter and receiver. advances in analog and digital processing have made it possible to individually adjust the amplitude and phase of the signal on each member of an array of antennas. when the approach is used to increase data rates, it is called multipleinput, multipleoutput (mimo), and when it is used to extend range, it is called beam forming. the most basic form of mimo is spatial multiplexing, in which a highdatarate signal is split into lowerrate streams and each is broadcast concurrently from a different antenna. (more generally, multiple antennas can be used to obtain the desired degree of enhancement in both data rate and range.) these schemes require signicant ﬁbasebandﬂ (i.e., digital) 7  for a general discussion of cybersecurity challenges, see, for example, national research council, toward a safer and more secure cyberspace, the national academies press, washington, d.c., 2007.8  l.j. chu, ﬁphysical limitations of omnidirectional antennas,ﬂ journal of applied physics 19(december):11631175, 1948.9  r.f. harrington, ﬁeffects of antenna size on gain, bandwidth and efciency,ﬂ journal of research of the national bureau of standards 64:112, 1960.10 r.c. hansen, ﬁfundamental limitations in antennas,ﬂ proceedings of the ieee 69(2):170182, 1981.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.52 wireless technology prospects and policy optionsprocessing before transmission and after reception, but are able to provide increased range or data rates without using additional bandwidth or power. they provide link diversity, which improves reliability, and they enable more efcient use of spectrum. this approach is used in a number of commercially deployed technologies including 802.11n (a wireless lan standard), wimax (a lastmile wireless localaccess technology), and longtermevolution (lte; a technology for fourthgeneration mobile telephony). lowcost, portable radios at frequencies of 60 ghz and abovethe use of cmos and digital processing together with other advances in rf technology opens up opportunities in the form of lowcost, portable radios that are becoming increasingly practical at frequencies of 60 ghz and above. technological progress may extend this up to 100 ghz and beyond. radios operating in this domain confront a number of challenges. at these frequencies, propagation distances are very short in free space and even shorter where there is foliage. penetration through and diffraction around building walls or other structures are also very limited. on the other hand, operation at these frequencies also has some attractive properties. only at these frequencies are very large bandwidths available, making them the only practical option to support wireless applications that require extremely high data rates. for example, technology developed for inroom video transmission uses data rates of up to 4 gigabits per second (gbps). another attractive feature of operation at these frequencies is diminished potential for interference. short propagation distances and limited penetration of buildings are one reason. the high path losses could have another advantage with respect to interference. a likely solution to the pathloss problem is to use adaptive beam forming to provide high antenna gainšthat is, directing transmitted energy along a chosen path and preferentially receiving a signal from a chosen path. if transmission sensitivity and receiver sensitivity are thus tightly focused, the potential for interference among different pairs of transmitters and receivers is markedly reduced.11 using these frequencies for mobile devices therefore becomes technically challenging because very narrow beams must be dynamically 11 note, however, that this situation can also exacerbate the ﬁhidden nodeﬂ problem in which a transmitter using a ﬁlisten before talkﬂ protocol before transmitting to a receiver cannot necessarily detect another transmitter that is already using the same channel to communicate with the receiver.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 53steered, a capability that is now being deployed in commercial products providing links up to 4 gbps.12 note that technological advances in these areas, which open up the bands between 20 and 100 ghz to practical use, will also open up other attractive options for using antenna arrays at lower frequencies, such as the use of mimo, or the adaptation of 802.11n technology to operate at higherthanpresent frequencies. what applications might operation in these newly accessible frequencies have? in the short term applications that require very large bandwidth over short range appear to be promising, such as devices that allow computer devices to transfer data at high speed across a desktop or devices that can transmit highdenition video from one side of a room to another. short propagation distances make these frequencies less viable for widearea infrastructure or applications where inbuilding signal propagation is important. looking ahead, it is possible that new architectures, such as very small cells, could make it possible to use these frequencies to provide widercoverage services. realizing this vision would depend on several factors not yet presentšdevices that operate at 20 to 100 ghz becoming cheap enough to be ubiquitous, a sufciently widespread and cheap wired network infrastructure that would connect these devices, and the development of new business models for such services. interference as a property of radios and radio systems, not radio signalsit is commonplace to talk about radio signals interfering with one another, a usage that mirrors the common experience of broadcast radio signals on the same channel interfering with each another. thus, the term ﬁinterferenceﬂ might suggest that multiple radio signals cancel each other out, making their reception harder or impossible. however, this view is misleading because radio signals themselves do not, generally speaking, interfere with each other in the sense that information is destroyed. in fact, interference is a property of a receiver, re˚ecting the receiver™s  inability to disambiguate the desired and undesired signals. radio signals are electromagnetic waves whose behavior, as described by maxwell™s equations, is linear. one consequence of this behavior is that radio signals do not, in general, cancel each other out. each new communication signal is superposed on the entire eld.13 actual destruction of information requires energy input at the point of destruction, and this 12 for example, sibeam, which is currently offering chip sets for wireless highdenition television links.13 in principle, a precisely applied signal could literally cancel out another eld, but only at a single point in space.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.54 wireless technology prospects and policy optionsenergy must be applied very precisely to cancel out the signal™s vector eld in all six dimensions, which is a lowprobability event, and applies only at a single point in space.as a result, the superposition of any number of radio signals should be thought of not in terms of destroying information but rather in terms of the ambiguity it creates for a radio trying to receive any one specic signal. the difculty of resolving the ambiguity relates to the energy emitted by other radios (with the implication that each radio sees multiple signals) and the unpredictability of the signals (which makes the individual signals harder to separate). even though it is available, information is discarded in the receivers primarily for the following reasons:ł dynamic rangešlarge interfering signals inhibit a receiver™s ability to detect a small signal. a small signal cannot be amplied above certain noise levels without the larger signal saturating the receiver. moreover, desensitization circuits will reduce gain in the presence of strong interfering signals. finally, the resolution of analogtodigital converters is limited, which means that a weak signal cannot be digitally represented when a strong interfering signal is also present.ł nonlinearity of receiver componentsšthe desired and the interfering signals will interfere with each other inside the receiver. (see below.)ł inadequate separation in signal space (within the various degrees of freedom and code).moreover, the extent to which signal processing can be used to separate signals with the required sensitivity, accuracy, and latency is limited by the computational power available in a radio. removing signal ambiguity thus entails investment in one or more of the following: better radio components, additional radio complexity, additional integrated circuit area, additional antennas, additional computation, and/or additional power consumption. another area for potential improvement is in systems of radios. with more and more radios capable of transmitting and receiving, behavioral schemes can be used to mediate among radios. also, because it is fundamentally easier to separate out known (and thus predictable) signals as opposed to random signals, mechanisms that allow waveform and modulation information to be registered or otherwise shared may prove useful. however, there will always be practical limits to what can be shared or coordinated. the costs of disambiguating signals are, thus, ultimately re˚ected in a number of ways, including in the complexity of a radio™s (or system™s) design, the cost of its hardware, its size, the power it consumes, and (for wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 55mobile devices) the lifetime of the battery. disambiguation thus involves tradeoffs, given that a radio is built to meet many requirements, only one of which is dealing with signal ambiguity.enduring technical challengeseven as the capabilities and performance of radios continue to improve, a number of hard technical problems can be expected to persist. power consumptionthe power required to operate increasingly complex and sophisticated radios will continue to represent an important boundary condition, especially for mobile devices, where it dictates the cost, capacity, dimensions, and weight of their batteries as well as the interval between charges. even for nonmobile devices, excess power results in heat that requires space or costly cooling components to dissipate. the design of practical radios will continue to re˚ect difcult tradeoffs between power consumption and other desired attributes and capabilities. nonlinearityrealworld radio elements are not perfectly linearšthat is, the output of an element is not exactly proportional to the input. nonlinearity results in signal distortion and, when more than one signal is present in a nonlinear element, the creation of new, unwanted products of the original signalsšan effect known as intermodulation distortion. the result is a degraded ability to separate a desired signal from other signals, which constrains the extent to which a receiver can mitigate interference. radio designers use several strategies to mitigate these effects. one is to use lters that separate out signals at other frequencies from the range of signals that are to be detected. in particular, lters allow relatively strong signals to be separated out so that a relatively weak signal can be detected. another is to use components that are close to linear over a wider range of signal strengths. nonlinearity has always been a signicant challenge to radio designers. it is a particular challenge to realizing the vision of radios that dynamically adapt to the presence of other radios by changing their frequency and other operating parameters. one might imagine building a radio that uses digital signal processing over very wide frequency ranges to separate out desired signals from potentially interfering signals. doing so would allow one to leverage improvements in digital logic and better digital signal processing techniques to mitigate interference. however, the extent to which this wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.56 wireless technology prospects and policy optionsapproach can be used is constrained by the intermodulation distortion associated with realworld radio components, which limit the bandwidth that can be handled practically using digital signal processing alone.a variety of avenues are being pursued by researchers to overcome these constraints. one of them has long been of interest but has not been realized in commercial products: the use of narrow lters that are tunable under digital control over a wide range, perhaps using microelectromechanical systems (mems) technology. nomadic operation and mobilitysupporting nomadic operation and mobility requires more dynamic adaptation of radio operating parameters than is needed for xed radios, which only need to cope with changes in environmental conditions. moreover, nomadic operation and mobility make it more difcult to neatly segment space or frequency, and they complicate dynamic market approaches because they make it more difcult to buy and sell rights at the rate at which radios can move between segments. heterogeneity of capabilitiesas more sophisticated radios are deployed, the heterogeneity of  capabilitiesšespecially the existence of radios with much poorer performance than othersšwill present growing challenges. at any point in time, there will be a legacy in terms of deployed equipment, existing frequency allocations, and existing businesses and government operations that are being made obsolete, in some sense, by new capabilities. the problem is not new, but a rapid pace of technological advancement and concomitant explosion of applications, especially applications with different purposes and capabilities, magnies the challenges.not all heterogeneity will arise from legacy systems. some applications will have cost and/or power requirements that preclude the use of highly sophisticated radios that coordinate their behavior. for example, the constraints on cost and power consumption for embedded networked sensors preclude the use of highly sophisticated radios that are able to do very sophisticated signal processing or complex computation to coordinate their behavior. another manifestation of heterogeneity is the contrast between active use, which involves both transmitter(s) and receiver(s), and passive spectrum use (e.g., remote sensing and radio astronomy), which involves receivers only.14 figuring out how to simultaneously 14 for a detailed discussion of passive scientic uses, see national research council, frequency allocations and spectrum protection for scientic uses, the national academies press, washington, d.c., 2007. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 57accommodate more sophisticated and adaptable radios with those that are necessarily less sophisticated will be an ongoing challenge. timescales for technology deploymenta particular challenge in contemplating changes to policy or regulatory practice is determining just how quickly promising new technologies will actually be deployable as practical devices and systems and thus how quickly, and in what directions, policy should be adjusted.rate for deployment of new technologies as practical devices and systemsas is natural with all rapidly advancing technology areas, concepts and prototypes are often well ahead of what has been proven feasible or commercially viable. the potential of adaptive radios, for example, has been explored (particularly for military use), but the technology has not yet been used in mainstream commercial devices or services. as described above, there is reason to expect the capabilities of radios to improve and their hardware costs to steadily decline, but many important details of operation and protocols must be worked out in parallel with technical development and regulatory change. moreover, although great technical progress has been made in recent years, resulting in the deployment of new wireless services, wireless communications will remain a fertile environment for future basic research as well as product and service development.timescales for technology turnoverdifferent wireless services are characterized by the different timescales on which technology can be upgraded. the factors in˚uencing the turnover time include the time to build out the infrastructure and the time to convince existing users (who may be entrenched and politically powerful) to make a shift. for instance, public safety users tend to have a long evolution cycle, as government procurement cycles are long and products are made to last a long time. cellular turnover is rapid by comparison, and technology can be changed out relatively readily (a 2year handset halflife and a 5 to 7year time frame for a shift to new technology are typical). the digital television transition that nally occurred in the united states in 2009 is emblematic of the challenge of making a transition where technology turnover is very slow, in part because of expectations raised by static technology and services that were developed over many decades. importantly, the rate at which turnover is possible depends on the incentives for upgrading as well as the size of the installed base. for wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.58 wireless technology prospects and policy optionsinstance, rms operating cellular networks have demonstrated an ability to upgrade their technology fairly quickly despite having an enormous user base, whereas aviation has a relatively small set of users but a very long turnover rate, having yet to transition from essentially 1940s radio voice technology. the primary driver of successful upgrades is for users to see tangible benets and for service providers to have an incentive to push for the switch. cellular subscribers gain tangible benets from newer capabilities commensurate with the added costs. (also, u.s. mobile operators generally subsidize handset cost, because it makes it easier to upgrade their network technologies and increase system capacity, somewhat offsetting the visible costs to the end user),15 whereas private pilots would incur a large capital cost and have to learn a new system even though the existing technology already meets their requirements.talent and technology base for developing future radio technology the changing nature of radios is creating new demands for training and education. research and development (r&d) for radios depend on skills that span both the analog and the digital realms and encompass multiple traditional disciplines in electrical and computer engineering. similarly, making progress in wireless networks often requires expertise from both electrical engineering and computer science. it is thus not straightforward for a student to obtain the appropriate education and training through a traditional degree program. the nature of modern radios presents another barrier to advanced education and universitybased research, because the cmos chips that lie at their heart require very largescale fabrication facilities, presenting a signicant logistical barrier to universitybased groups that seek to test and evaluate new techniques. this report assumes a continued stream of innovation in radio technology. such sustained innovation depends on the availability of scientic and engineering talent and on sustained r&d efforts. considerable attention has been focused in recent years on broad concerns about the declining base of scientic and engineering talent and levels of research support in the united states and its implications for competitiveness, including in the area of telecommunications. for a broad look at trends and their implications for science, engineering, and innovation, see rising 15  incentives may differ across markets and regulatory regimes. for example, cellular upgrades have been marketdriven in the united states and governmentdriven in the european union (eu). the effect has been mixed. on the one hand, the eu push for third generation arguably got ahead of actual market demand, whereas the u.s. market moved slowly from analog to secondgeneration digital services, arguably giving the eu higherquality wireless voice services sooner.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 59above the gathering storm: energizing and employing america for a brighter economic future;16 for a study focused on telecommunications research, see renewing u.s. telecommunications research.17the issues and opportunities described in this report involve considerations of many areas of science and engineeringšincluding rf engineering, cmos, networking, communications system theory, computer architecture, applications, communications policy, and economics. addressing the challenges and realizing the opportunities will require a cadre of broad systemsoriented thinkers. building this talent will be a major national advantage. radio engineering is an important area for consideration in this context, given that wireless is a fastmoving, hightechnology industry that is economically important in its own right and that has much broader economic impacts. moreover, wireless engineering encompasses an extensive skill setšincluding rf engineering, an ability to do rf work in cmos technology, and an ability to work on designs that integrate rf and digital logic componentsšthat is difcult to learn in a conventional degree program. similarly, wireless networks involve expertise that spans both electrical engineering and computer science. finally, for r&d to be effective, it is important to be able to implement and experiment with new ideas in actual radios and systems of radios. work on new radio designs requires access to facilities for ic design and fabrication. work on new radio system architectures also benets from access to test beds that allow ideas to be tested at scale. given the high cost of such facilities, university r&d can be enhanced by collaboration with industry.measurements of spectrum usethe standard reference in the united states for the use of spectrum is the u.s. frequency allocation chart that is published by the ntia. the chart separates the spectrum from 30 mhz to 300 ghz into federal or nonfederal use and indicates the current frequency allocations for a multitude of services (cellular, radiolocation, marine, land mobile radio, military systems, and so on). although this chart is an invaluable reference in providing a comprehensive view of what frequencies are potentially in use for various 16 national academy of sciences, national academy of engineering, and institute of medicine, rising above the gathering storm: energizing and employing america for a brighter economic future, the national academies press, washington, d.c., 2007.17 national research council, renewing u.s. telecommunications research, the national academies press, washington, d.c., 2007.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.60 wireless technology prospects and policy optionsservices and in giving some indication of the complexity of frequency use, it does not shed light on a particularly critical issuešthe actual density of use of the spectrum. that is, are there blank spaces in frequency, time, and space that could potentially be used for other purposes?it is increasingly asserted that much spectrum goes unused or is used inefciently. yet relatively little is known about actual spectrum utilization. licensees and users are not required to track their use of spectrum. there are no data available from any sort of ongoing, comprehensive measurement program. and when spectrum measurements have been made, they were often aimed at addressing a specic problem. proxy measurements, such as the number of licenses issued in a frequency range, have been used to characterize trends and extrapolate likely use, but they do not measure actual use and do not, of course, yield any insight into unlicensed use.18why spectrum measurement is hardperhaps the greatest challenge is that any program of measurement will be limited in its comprehensiveness if all the degrees of freedom are actually to be measured. measurements can be made only at specic locations and times; measurements at one place may not reveal much about even nearby points. results obtained by one set of measurements are not easily applied to a different situation. the full scope of measurement is suggested by the electrospace model, in which one species the frequency, time, angle of arrival (azimuth, elevation angle), and spatial location (latitude, longitude, elevation) to be measured.19 other measurement considerations include polarization, modulation scheme, location type (e.g., urban, suburban, or rural),20 and which signals are being measured (known signals, unknown signals, or noise).many radio systems are designed to operate with very low average power levels, and naive spectrum measurement techniques may miss use by such lowpower devices.21 moreover, a directional signal will be missed if the receiver is not pointing in the right direction. often designed to operate with very low average power levels, pointtopoint microwave 18 robert matheson, spectrum usage for the fixed services, ntia report 00378, march 2000, p. xi. 19 robert matheson, ﬁthe electrospace model as a frequency management tool,ﬂ  addendum to the proceedings of the 2003 isart conference, 2003.20 allen petrin, ﬁmaximizing the utility of radio spectrum: broadband spectrum measurements and occupancy model for use by cognitive radio,ﬂ ph.d. thesis, georgia institute of technology, august 2005, p. 6. 21 robert matheson, letter to david liddle in followup to presentation to the committee, august 27, 2004. wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 61links and radar systems are examples of use that may be missed by spectrum measurements efforts. radar emits narrow highpower pulses infrequently, making them easy to miss. some uses, such as public safety communications, are inherently sporadic and random in time and location. because they are normally conned to military installations, defense uses may take place in welldened locations but will vary considerably over time. also, measurements by denition measure only active use of the spectrum; passive use of the spectrum and remote sensing cannot be detected and, worse, could be interpreted as nonuse of parts of the spectrum that would be seen as empty. similarly, without careful interpretation, guard bands established to mitigate interference for existing services could be interpreted as unused portions of the spectrum even though these bands are in a real sense being used to enable those services. these considerations suggest that spectrum measurement is a challenging endeavor that requires measurements at many points in space and time and the collection of a very large amount of data. they also suggest that spectrum measurement has an inherent element of subjectivity, because results may depend signicantly on the particular assumptions made and methods employed. looking forward, measurement might be improved over the long term by requiring systems to provide usage statistics, as might the development and adoption of a formal framework for measuring, characterizing, and modeling spectrum utilization. such a framework might provide  researchers a way to cogently discuss spectrum utilization and provide policy makers with evidencebased information about technical  factors affecting efcient utilization. 22 results from some measurement activitiesthe ntia has a long history of spectrum measurement work going back to at least 1973.23 those early efforts included federal land mobile radio measuring use in the 162174 and 406420 mhz range, and federal aviation administration radar bands in the 2.72.9 ghz range. these projects were generally considered successful because the measurements focused on a denite problem and were able to address specic questions, such as whether claimed interference was real and whether minor changes to receivers could mitigate the problem of overcrowded use. the 22 f. weidling, d. datla, v. petty, p. krishnan, and g.j. minden, ﬁa framework for r.f. spectrum measurements and analysis,ﬂ proceedings of ieee symposium on new frontiers in dynamic spectrum access networks, 2005, pp. 573576. 23 ibid.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.62 wireless technology prospects and policy optionsntia conducted a number of broadband spectrum surveys in different cities in the 1990s.24an ntia report from 1993 (and updated in 2000) used proxy information as a ﬁmeasurementﬂ of spectrum usage for xed services (e.g.,  common carriers).25 that report examined historical license data and observations about market and technology factors likely to affect spectrum use, in order to gain insight on the degree to which the existing xedservice spectrum bands would continue to be needed for their allocated services. one conclusion to be drawn from that report is that pointtopoint microwave bands are probably underused and that the growth expected when these bands were allocated decades ago did not occur.26 anticipated use of pointtopoint microwave has moved largely to optical ber instead, although it is still used in many rural areas where the trafc does not justify the cost of laying ber. a number of research projects have attempted to directly measure spectrum utilization.27 shared spectrum company, a developer of  spectrumsensing cognitive radio technology, has made several measurement studies since 2000, including occupancy measurements in urban settings such as new york city and chicago, suburban settings such as northern virginia, and rural environments in maine and west virginia.28 spectrum measurements for the new york city study were done during a period of expected high occupancy, the republican national convention.29 the studies aimed to determine how much spectrum might be allocated for more sophisticated wireless applications and secondary users relative to primary (licensed) users. some important conclusions can be drawn from these measurements. the measurements indicate that some frequency bands are very heavily 24 frank h. sanders and vince s. lawrence, broadband spectrum survey at denver, colorado, ntia report 95321, september 1995; frank h. sanders, bradley j. ramsey, and vincent s. lawrence, broadband spectrum survey at san diego, california, ntia report tr97334, december 1996; frank h. sanders, bradley j. ramsey, and vincent s. lawrence, broadband spectrum survey at san francisco, california, mayjune 1995, ntia report 99367, july 1999.25 robert matheson, spectrum usage for the fixed services, ntia report 00378, march 2000, p. 1. 26 robert matheson, letter to david liddle in followup to presentation to the committee, august 27, 2004.27 p.g. steffes and a.j. petrin, ﬁstudy of spectrum usage and potential interference to passive remote sensing activities in the 4.5 cm and 21 cm bands,ﬂ proceedings of the ieee geoscience and remote sensing symposium 3(2024):16791682, 2004; s.w. ellingson, ﬁspectral occupancy at vhf: implications for frequencyagile cognitive radios,ﬂ proceedings of the ieee vehicular technology conference 2(2528):13791382, 2005.28 mark mchenry, ﬁnsf spectrum occupancy measurements project summary,ﬂ shared spectrum company, august 15, 2005. 29 mark mchenry and dan mccloskey, ﬁnew york city spectrum occupancy measurements september 2004,ﬂ shared spectrum company, december 15, 2004.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 63used and that some other currently assigned frequency bands are only lightly used, at least over some degrees of freedom. above all, the picture that emerges clearly from the measurements made to date is that frequency allocation and assignment charts are misleading in their suggestion that little spectrum is theoretically available for new applications and servicesšprovided that the right sharing or interference mitigation measures could be put in place. one might legitimately quibble over the details or the precise level of use; the real point is that there is a good deal of empty space, provided that ways of safely detecting and using it can be found.another broad conclusion is that the density of use becomes lower at higher frequency. the advent of lowcost radios that can operate at frequencies in the tens of gigahertz points to a promising arena for introducing new services.finally, measurements of spectrum use do not capture the value of use. in addition, if a licensee internalizes the opportunity cost of underutilized spectrum and has a way to mitigate that cost, there is no need for centralized measurement and management; that empty space exists, but the best way to use it is not necessarily for the government to allow additional users.challenges facing regulatorstechnology advances bring new issues before regulators that require careful analysis. some require a subtle understanding of the ways in which new technology may necessitate new regulatory approaches and a challenging of past assumptions about limitations and constraints. several examples are discussed below.use of white space to increase spectrum utilizationthe basic goal of ﬁwhite spaceﬂ utilization is to let operators with lower priority use the space when higherpriority users leave the spectrum unoccupied. from a technical perspective this approach requires adding sensing capability to devices to determine if a higherpriority user is using the spectral band (or bands). such a dynamic use of spectrum has not been supported in past regulatory models.in the dynamic situation envisaged in the whitespace model, several new questions and considerations have to be addressed. for instance, ﬁoccupancyﬂ must be dened thoughtfully. higherpriority users opposed to the use of white space might say that any use of their spectrum could cause harm to their transmissions, so that only ﬁno interferenceﬂ is acceptable. yet achieving no interference has never been possible because all wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.64 wireless technology prospects and policy optionsradios transmit energy outside their allowed bands and generate interference with other adjacent users. therefore, the only question, ultimately, is the degree to which interference is allowed. in the absence of a clear technical analysis of when a given level of interference is actually causing signicant degradation of signal, it is difcult to determine an acceptable level. how best to do so is of importance in formulating rules to open up spectrum as well as for private parties to negotiate what level of interference they would accept in return for a market price. a clear technical analysis requires that several factors be considered. estimating the total interference load depends on a realistic statistical model for the number of likely secondary users, the transmitted power spectrum for each user, the susceptibility of the primary occupant™s  receivers to these secondary signals, and the ability of the primary user to adapt its transmissions to reduce the impacts of the secondary users. given that the analysis is statistical in nature, it may be useful to approach the question in terms of a probability of degradation that should not be exceeded. if the likelihood of degradation by secondary users falls below this probability, then those secondary users would be considered as not occupying the band of the primary user. an analysis done from this perspective would help avoid situations in which highly improbable scenarios (as opposed to situations that can reasonably be expected to cause a problem) lead to the rejection of sharing arrangements. second, considering frequency as the only degree of freedom available to separate users makes for simpler technical analysis but is highly limiting. radios built to perform dynamic beam forming, for instance, allow highly sophisticated spatial separation. also, if sensing is fast enough, then it is possible to exploit white spaces in time. thus frequency, time, and space could all be considered as tools to reduce the effects of interference to below the level of degradation dened as noninterference.third, spectral emissions regulations have historically considered each transmitter working independently. yet, considering sensing performed by the network might mean much greater opportunity for more efcient spectrum use. just how much might be gained from such an approach is not well understood, because it depends on an understanding of the statistical correlation between sensing at different locations. considering such an approach requires the same mindset change as described previously, which allows for statistically based improvements. finally, there is the issue of sensitivity of detection. greater sensitivity increases the probability of detection but also leads to a greatly increased probability of false alarms. in other words, at some point increasing sensitivity causes any random noise to appear as occupancy. to make a proper analysis requires a level of understanding about sensing that goes beyond just sensing the energy in a spectral band. most signals have distinctive wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.key technology considerations 65signatures that can be used to differentiate them from noise or other  spurious emissions. one opportunity to make use of white space is in the broadcast television bands. to that end, in late 2008, the fcc issued a set of rules30 under which devices use geolocation and access to an online database of television broadcasters together with spectrumsensing technology to avoid interfering with broadcasters and other users of the television bands. (alternatively, the ruling provides for devices that rely solely on sensing, provided that more rigorous standards are met.) debate and litigation ensued following the 2008 order on such issues as how to establish and operate a database of broadcaster locations. in a second order issued in 2010 to nalize the rules, the requirement was dropped that devices incorporating geolocation and database access also must employ sensing.31 adaptive antenna arrays and power limitsantenna arrays at transmitters and receivers are being used increasingly to provide greater range, robustness, and capacity. yet the basic regulatory strategy of dening an equivalent isotropically radiated power level for transmitters ignores many of the special characteristics of antenna arrays. as one example, this regulatory approach does not encourage the use of beam forming, which has considerable advantages in reducing interference over omnidirectional antennas.decreasing cost of microwave radio linksthe present report describes above how standard cmos technology can now be used to transmit in the microwave bands (60ghz links have been demonstrated). as desired data rates rise into the gigabitpersecond range, adaptive antenna arrays will be used to obtain the necessary received power for both mobile and xed devices. as with the previous examples, this technology is very different from what has been in use until now. 30 fcc, ﬁsecond report and order and memorandum opinion and order in the matter of unlicensed operation in the tv broadcast bands,ﬂ et docket no. 04186, and ﬁadditional spectrum for unlicensed spectrum for unlicensed devices below 900 mhz and in the 3 ghz band,ﬂ et docket 02380, fcc 08260, washington, d.c., november 14, 2008.31 fcc, ﬁsecond memorandum opinion and order in the matter of unlicensed operation in the tv broadcast bands,ﬂ et docket no. 04186, and ﬁadditional spectrum for unlicensed spectrum for unlicensed devices below 900 mhz and in the 3 ghz band,ﬂ et docket 02380, fcc 10174, washington, d.c., september 23, 2010.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.66 wireless technology prospects and policy optionsengineering alone is often no solutionthe previous section describes several specic issues where engineering insights would help to inform future policy and regulation. at the same time, it is important not to oversell the extent to which better engineering or understanding of the technology alone can yield solutions. in the end, an engineering analysis depends on a knowledge of possible scenarios and what the acceptable outcomes are. these inform a complex set of business, marketing, and political judgments about value and risk. for example,ł engineering alone does not determine whether a service supporting aviation merits greater protection from interference than a service delivering entertainment. ł the density and the distribution of a constellation of mobile devices (which affect their ability to interfere) cannot be determined fully a priori. they will re˚ect market and consumer behavior, and moreover they will change over time.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.673policy optionspressures on today™s wireless policy frameworkthe current wireless policy framework is based on the technology of more than 8 decades ago and on the desire, at that time, for governmental control over communications. it has evolved to encompass a patchwork of legacy rules and more modern approaches that have been added over time. nonetheless, there is wide acceptance that the rules are ripe for change, to better re˚ect the technological options available today and in the future. the current framework is under pressure today on several fronts:ł the current framework continues to rely heavily (with a few exceptions) on servicespecic allocations and assignments that are made primarily by frequency band and geographic location and does not encompass all of the spectrum management approaches possible. allocation and assignment of services by frequency band were historically seen as the only technologically feasible way of allowing multiple wireless systems and services to coexist. today, technology advances make it possible to use additional degrees of freedom to separate transmissions, introducing new options for allocating usage rights. in addition, new frontiers are being opened by the emergence of inexpensive, small devices that operate at 20 to 100 ghz. ł despite revisions aimed at ensuring greater ˚exibility, the current framework continues to rely signicantly on centrally managed allocation and assignment, with government regulators deciding how and by whom wireless communications are to be used. spectrum policy has become more ˚exible over the past several decades in such areas as permitted modulation waveforms and types wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.68 wireless technology prospects and policy optionsof use and the adoption of less centralized models such as unlicensed bands and white space. nonetheless, the past decade has seen widespread agreement that central management by regulators is inefcient and insufciently ˚exiblešan agreement that re˚ects the complexity of the problem and the dispersion in the economy of the information that is required to make decisions.1 it also re˚ects concerns about whether government institutions are sufciently nimble to make efcient and timely decisions.ł the current framework will not be able to satisfy the increasing and broadening demand for wireless communications. one source of this demand is greater use of richer media (such as video) that requires higher data rates. another is the continued growth in internet applications and services and the growing demand for untethered and mobile access to them. demand for mobile access to the public telephone network has continuedšthe leading example of a more general shift toward mobile interpersonal communication. together, these have resulted in rapid growth in the number of users of wireless devices and services. increasingly, communications are between devices as well as people, notably re˚ected in growing interest in sensor networks, and together, these trends may overwhelm the ability of the existing framework to enable introduction of new communications services to meet demand. ł the current framework does not fully re˚ect changes in how radios are built and deployed now or in how they could be built and deployed in the future in response to different regulations. technological innovation has expanded the range of potential wireless applications and services and the technical means for providing them. at the same time, it has dramatically lowered the cost of including wireless capabilities in devices. the old regime and technology placed a premium on simple, lowcost receivers and did not impose tight cost constraints on transmitters. new technology enables the deployment of many more, and more capable yet inexpensive, transceivers. today, the population of deployed radios has shifted from one dominated by a small number of transmitters and many receivers to a population that also contains many more transceivers (e.g., every cell phone is a transmitter as well as a receiver). key considerations for a future policy frameworkenabling more nimble evolution of spectrum policythe current spectrum plan re˚ects decades of historical practice and in its myriad allocations and assignments re˚ects many stages of tech1  for example, this point arose repeatedly in the remarks of those who briefed this committee.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.policy options 69nology and policy development. it thus encompasses not only the xed allocations made years ago for services that use nowoutdated technology (e.g., am radio) but also new regulatory and technology approaches (e.g., ultrawideband).generally speaking, allocations for services re˚ect the frequency range that was practical at the time a particular service was introduced, and many services introduced decades ago persist today. it was often possible to fulll demand for new services by exploiting the higher frequencies that would become available as advances were made in device and radio technology. today, propagation and penetration considerations constrain, for many applications, the utility of the higher frequencies that are less crowded, and so it is no longer possible to free up spectrum for those applications by simply moving to higher frequencies. in addition, following many decades of tting in new services wherever there was free space, there is little or no unclaimed spectrum at lower frequencies. much of the current spectrum frame work also re˚ects a time when operating rights were fairly well dened and when there were relatively few systems, system operators, and transmitters. the complexity and density of existing allocations, assignments, and uses, coupled with competing demands for new uses, especially at lower frequencies, mean that any change will be difcult. it will involve careful consideration of the specics of allocations, assignments, and uses in specic frequencies as well as the particular technical characteristics of particular frequencies and proposed applications. regulators must approach this evaluation carefully lest they end up simply reinventing old commandandcontrol approaches. change may also involve addressing the costs and benets of proposed changes that are (often unevenly) distributed over multiple parties, resolving con˚icting claims about costs and benets, and addressing coordination issues, which are especially challenging if achieving a particular change requires actions by a large number of parties. moreover, some parties gain by changing while others gain by waiting. as a result, decision making ends up, broadly, being a political question.today, more ˚exible and adaptable radios and a world in which these systems are in the hands of millions of people suggest the need for correspondingly nimble and ˚exible processes for developing and evolving future wireless communications policy. in essence, what will be needed is an approach that is not necessarily completely right the rst time, but right over time. that is, the approach should allow for experimentation and feedback, and the regulatory system should be able to track and even anticipate advances in wireless technology and emerging ways of implementing and using wireless services. developing such a system in detail is beyond the scope of this committee™s charge, but it is with such an objective in mind that the following items are offered as promising avenues for progress.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.70 wireless technology prospects and policy optionsavoiding the extremes in the ﬁproperty rightsﬂ  versus ﬁcommonsﬂ debatethe terms ﬁproperty rights,ﬂ ﬁcommons,ﬂ ﬁand greater public goodﬂ are used as shorthand for particular approaches to spectrum management. ﬁproperty rightsﬂ refers to an approach that relies on a wellspecied and possibly exclusive license to operate a service using a set frequency range, location, transmitted power, and so forth. these rights can be established or transferred through an administrative proceeding, auction, or market transaction. ideally, any of the dimensions along which the rights are dened can then be redened through market transactions.2 ﬁcommonsﬂ refers to an approach that establishes a band in which those who operate devices do not need to obtain a license and instead must comply with rules that are applied to all devices operating in that channel, such as limits on transmitted power. this approach is intended to incentivize development of devices that perform better in a noisy, shared environment as an alternative to the use of market incentives for prioritizing potential sources of radiation in any given channel by the value of the communication carried in that channel.3 ﬁgreater public goodﬂ refers to governmentsponsored free use of the spectrum for such purposes as national security, public safety, and science.each approach has advantages and disadvantages, transaction costs, incentives for and loci for innovation, and so forth. no one of the approaches can at present be judged to be better than the other two. moreover, there is a much larger space of alternatives that combine attributes of these approaches, and the dividing lines between the approaches will shift as technological capabilities, deployed services, and business models evolve. these observations suggest avoiding building an overly rigid regulatory structure or relying solely on a single approach. instead, they suggest using each approach where practically and politically feasible, and measuring and monitoring their performance, and using those results 2  r.h. coase, ﬁthe federal communications commission,ﬂ journal of law and economics 2:140, 1959; arthur s. de vany, ross d. eckert, charles j. meyers, donald j. o™hara, and richard c. scott, ﬁa property system for market allocation of the electromagnetic spectrum: a legaleconomicengineering study,ﬂ stanford law review 21:1499, 1969; gregory l. rosston and jeffrey s. steinberg, ﬁusing marketbased spectrum policy to promote the public interest,ﬂ ofce of plans and policy™s working paper, january 1997; gerald faulhaber and david farber, ﬁspectrum management: property rights, markets, and commons,ﬂ  ofce of plans and policy™s working paper, 2002.3  yochai benkler, ﬁovercoming agoraphobia: building the commons of the digitally networked environment,ﬂ harvard journal of law and technology 11(winter):287, 19971998; y. benkler, ﬁsome economics of wireless communications,ﬂ harvard journal of law and technology 16(1; fall):2583, 2002; kevin werbach, ﬁsupercommons: towards a unied theory of wireless communication,ﬂ texas law review 82:863870, 2004.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.policy options 71to inform future allocations. regulators and policy makers will need to be able to track these developments and guide where the dividing lines should be in the future.leveraging the role of standards setting in regulatory decision makingstandards are stable and wellmaintained specications that are provided by vendors, service providers, nonprot organizations, or ad hoc organizations. they specify attributes such as interoperability and compatibility, which are often important in regulatory proceedings. regulators often rely, at least implicitly, on technical standards that guide those building devices and services in how to comply with the regulations. this reliance on standards setting re˚ects two related ideasšrst, that regulators may be better positioned to review technical proposals already captured in standards than to recommend specic technical approaches, and second, that it may be best to leave some of the technical details needed to dene a service to standards rather than rules. however, the process of forging consensus on standards is not easy. as in other domains, standards for wireless technologies have tended to be characterized less by engineers seeking consensus resolution of largely technical matters and more by contention among players with signicant stakes in the outcome (e.g., incumbents seeking to protect their position, participants who have investments in intellectual property, or participants who have differing business interests with respect to proposed services or applications). as standards have taken on greater importance, the number of competing players and con˚icting interests has grown, making the processes more cumbersome. one risk is that the large incumbent players can dominate by virtue of their greater resources and their greater participation in standards bodies, although this risk can be partly mitigated by moving to a one company, one vote formula, but with tradeoffs. another risk is that standards efforts could degenerate into a battle between two camps with disparate proposals that can end in a deadlock if a standard accommodates both campsšessentially a nondecision that can, at the extreme, prevent a product from coming to market or necessitate the establishment of another industry group that creates another standard that is narrow enough to be implementable.44  one recent example of such a deadlock was ieee 802.153a, an effort to establish a standard for a highdatarate ultrawidebandbased wireless personal area network. the effort ended when the standards working group was unable to decide between two competing proposals that were backed by different industry groups.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.72 wireless technology prospects and policy optionsanother challenge for standards setting is that standards are most useful once a new service has already seen at least some use. that is, standards processes are most useful in helping a set of actors forge a common approach for a service that has already been developed and, at least experimentally, deployed. they are much less effective where new solutions are being sought. related to this is the risk that some standards that are developed may never see signicant adoption. nor is the standards process always nimble or rapid; a recent example is the ieee 802.11n standard for wireless local area networks that took many years to nally adopt, by which time interim solutions had already been widely deployed to meet the market demands for faster networking. understanding the sensitivity of innovation to policy decisions the innovation process involves a number of actors, including academic researchers, small and large rms, and end users. policy and standards setting play an important role in shaping decisions that ultimately affect innovation. understanding the interplay between technology and policy is critical to creating effective policy. considerations include the tension between efciency and innovation and between the various stages of innovation. another important consideration is that innovation depends on inputs from basic research (see below).ensuring technology expertise in the regulatory processwhen matters requiring an evaluation of technical claims or options come before the federal communications commission (fcc), the technical basis for its decisions rests on information provided in comments to the fcc and assessments made by its engineering staff (box 3.1). the technical analyses in the submitted comments will of course tend to re˚ect the interests of the parties submitting those comments. the expertise of its engineering staff allows the fcc to address many specic technical issues it must grapple with regularlyšfor example determining the right noise gure for a particular system or the appropriate specication for adjacent channel interference. spectrum policy has entered an era in which many critical and strategic technical issues are likely to arise as technologies, applications, and services evolve. the fcc confronted many novel technical issues in its early days. over time its focus has broadened and now encompasses economic and legal issues as the industries it regulates mature and broaden, issues such as broadcast media ownership and common carrier regulation. today, qualitative and quantitative technology shifts of the sort discussed in this report and their complexity and interactions mean wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.policy options 73box 3.1  the federal communication commission™s office of engineering and technologythe ofce of engineering and technology (oet), the technical advisory arm of the fcc,1 has three divisions: policy and rules, electromagnetic compatibility, and a laboratory. the policy and rules division has three branches. the spectrum policy branch covers regulations and procedures for spectrum allocation and utilization. the technical rules branch develops technical rules and standards for the operation of unlicensed rf devices. the spectrum coordination branch monitors the activities of other government agencies, particularly the national telecommunications and information administration (ntia), as well as activities of the communications industry. it is also the liaison between the fcc and the interdepartment radio advisory committee, which advises the ntia. the electromagnetic compatibility division of oet studies radiowave propagation and communications systems characteristics; it also issues and manages experimental licenses. the laboratory division focuses mainly on testing, evaluation, and compliance. it has a technical research branch, a measurements and calibration branch, an equipment authorization branch, and a customer service branch. in 1998, oet convened a technology advisory council drawn from a range of technical experts, including manufacturing, academia, communications services providers, and researchers. the council, which met regularly until july 2006, was intended to provide a means for the fcc to stay abreast of rapid advances in telecommunications technology and help inform fcc regulations in light of those advances.2 in october 2010, a new council was appointed.1 see the ofce of engineering and technology web site at http://www.fcc.gov/oet/.2 technology advisory council charter, fcc, november 2002, available at http://www.fcc.gov/oet/tac/taccharter112502.pdf.that the fcc faces new challenges of a technological nature. examples of these complex issues that were grappled with during the work of this committee included how best to use the white space of (unused) tv channels and how best to use the 700mhz spectrum for public safety communications.because it believes that the fcc would greatly benet from enhancing its technology assessment and engineering capabilities, the committee offers several options for obtaining access to such expertise.one option is to recruit additional topcaliber engineers and scientists to work at the fcc, perhaps for limited terms. programs could provide early or midcareer professionals with an opportunity to gain experience in its policy and regulatory environment or could establish rotating posiwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.74 wireless technology prospects and policy optionstions to bring in senior academic and industry experts. there is, of course, a potential for con˚icts of interests to arise when staff move between government and industry, and these con˚icts of interest must be carefully avoided. on balance, however, the increased ˚ow of expertise, ideas, and perspectives seems likely to bring net benets. the fcc has used the position of chief technologist, which has been held by several senior experts from academia and industry, as one way to bring in such expertise. the committee believes, however, that it will be necessary to create an environment that attracts more of the right talent. as things stand, for example, the committee™s impression is that many in the technical community do not appear to be convinced that working at the fcc can help advance an engineering career in industry or academia. another option is to convene an external advisory committee that could give the fcc outside, highlevel views on key technical issues. the fcc announced the appointment of a new technology advisory council in october 2010, as this report was being prepared for publication.another option would be to add technical expertise to the staff of each commissioner. the staff members are regarded as highly competent, but most are legal professionals, not technologists. that is, although the staff members are generally knowledgeablešand often very much sošabout technology, they typically do not have the advanced engineering background that may be necessary to understand and resolve complex, deeply technical issues. also, the fcc could tap outside technical expertise, including expertise available elsewhere in the federal government. notably, the ntia institute for telecommunication sciences (its; box 3.2) already provides considerable technical assistance to federal agencies on a cost reimbursement basis and has done a limited amount of work for the fcc in the past. over the years its has developed and maintained a strong competency in a number of technical areas related to rf communications. strengthening the relationship between the fcc and its would give the fcc access to another source of independent scientic and engineering expertise on an asneeded basis. nist, which has considerable expertise and resources for technology evaluation and is currently working in such areas as the performance of land mobile radios and their use for public safety, is another potential source of expertise. (one caveat is that the fcc™s status as an independent agency rather than an executive branch agency may limit work done by the ntia or nist to technical and not policy matters.)finally, another source of outside technical expertise might be a federally funded research and development center (ffrdc). these are organizations managed by universities, industrial rms, or nonprots and chartered to provide federal agencies with technical expertise. ffrdcs wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.policy options 75box 3.2 institute for telecommunications sciencesthe institute for telecommunication sciences (its) is the research and engineering arm of the national telecommunications and information administration (ntia) in the department of commerce.1 its stated mission is to be the federal government™s primary technical resource for telecommunications issues. a liaison ofce coordinates its technical research with other federal agencies. as part of its broader mission it has supported several other federal agencies, including the departments of defense, homeland security, and transportation as well as state and local government.2 it works through cooperative research and development agreements with the private sector (e.g., american automobile association, intel, lucent, and motorola) and academic institutions (e.g., university of  colorado, university of pennsylvania). its has also provided technical support to the fcc for specic issues such as evaluation of propagation models necessary to implement the satellite home viewer act. 3its performs fundamental research and engineering with technical programs several areas directly related to wireless technology: broadband wireless, digital land mobile radio, information technology, propagation measurement and models, and spectrum research. it provides the technical resources from the united states in developing international telecommunications standards. the staff of its is composed mostly of scientists and engineers across a number of disciplines, including electronics engineering, math, physics, and computer science. its stated goals re˚ect its engineering focus. those goals include optimization of federal spectrum allocation methods, support for systems engineering and planning of interoperable public safety radio systems and standards (not frequency allocation, which is the purview of the fcc), improvement of network operation and management of national defense systems, and providing practical telecommunications performance measurement methods. its also hosts the international symposium on advanced radio technologies (isart) conference, which annually brings together researchers, business leaders,  policy makers, and regulators to discuss the future development and application of radio frequency technologies.1 its, ﬁfy 2007 technical progress report,ﬂ december 2007, available at http://www.its.bldrdoc.gov/pub/ntiarpt/tpr/2007/07tpr.pdf.2 its overview brochure, available at http://www.its.bldrdoc.gov/itsbrochure/itsbrochure.pdf.3 its, ﬁpropagation model development,ﬂ 1999, available at http://www.its.bldrdoc.gov/tpr/1999/itse/propmodel/propmodel.html.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.76 wireless technology prospects and policy optionsare able to bring in expertise on a projectbyproject basis and to engage expertise that may not be available within the constraints of civil service salaries. the committee™s view is that whatever mechanisms the fcc uses to tap outside technical expertise, the goal is to strengthen capabilities for establishing appropriate highlevel guidance, and not to build up an infrastructure for more detailed commandandcontrol regulation. technologyenabled policy optionsconsidering ﬁopenﬂ approaches in the range of 20 to 100 ghz use is relatively sparse at frequencies of 20 to 100 ghz; commercial services in that range represent a small fraction of the services that operate below 20 ghz. the relatively high attenuation in materialsšand short free space propagation in the oxygen absorption band around 60 ghzšmeans that propagation distances are relatively short. the ratio of antenna size to wavelength makes it practical to form very narrow beams. together, these factors make interference inherently unlikely. these frequencies thus represent an opportunity that stands in marked contrast to the very difcult transition problems associated with introducing new services, allocations, and sharing arrangements at lower frequencies. (increased use of higher frequencies would, however, do little, at least in the short term, to alleviate pressures to also introduce new services at lower frequencies.) for these higher frequencies, the reduced legacy problem and lower chance for interference (in the classical sense) indicate that nontraditional (ﬁopenﬂ) approaches can predominate. although it is an oversimplication to say this, at lower frequency the problem is dealing with the legacy, while at the higher frequency it is difcult for radios to interfere. these factors suggest that the two domains be approached differently, but the distinction has so far not been clearly articulated or incorporated into the policymaking process. the lower bound of the range proposed for open use, 20 ghz, was selected on the basis of two factorsšfrequencies above 10 ghz have only recently become practical in small devices at low cost and the region between 10 and 20 ghz is already heavily allocated, such as for kuband satellite transmissions between 12 and 18 ghz.the upper bound of this range, 100 ghz, re˚ects what can reasonably be expected to be practical today or in the near future and the upper limit at which it is possible to have a reasonable sense of how the technology might be employed. it would thus be imprudent to recommend a particular regime for frequencies above 100 ghz, given the limited understanding of how radios might be constructed or operated in that domain, and it wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.policy options 77would be prudent to review policy in this area every few years and make adjustments as appropriate.fcc policy has already moved toward a more ˚exible and adaptive approach in this frequency domain, with an unlicensed regime established at 57 to 64 ghz and licensed access to bands at 80 and 95 ghz on a rstcome, rstprotected basis. these measures may stimulate commercial activity and speed the deployment of new services. at the outset, these frequencies most likely would be used for very short distances and veryhighbandwidth applications, such as inroom video distribution, because the bandwidth for gigabit and higherrate applications is not available elsewhere. this is not to say that existing applications in those ranges would be quickly or easily replaced, but rather that over time it would be attractive to introduce new applications at 20 to 100 ghz rather than carving out the rights to introduce them at lower frequencies. finally, although usage at 20 to 100 ghz is relatively low compared to usage at frequencies below 20 ghz, existing users at the higher frequencies are likely to object, and some exceptions to the open rule would probably be needed to protect some existing services. for example, many satellite and military services operate in this range, mostly under ntia jurisdiction.5 there are also noncommunications uses in this frequency range, such as radar, navigation, and other industrial, scientic, and medical uses. recent experience in working out a sharing arrangement between wlan and military radar use at 5 ghz suggests, in the view of the committee, both the possibilities and the potential pitfalls; an accommodation was ultimately reached but not without considerable study and delay. because many of the existing uses above 20 ghz are for government services, the participation of and cooperation between the ntia and the fcc will be required to sort out the issues. using a wider set of approaches to mitigate interference and a wider set of parameters in making assignmentsinterference should not be viewed simply as an overlap in frequency and space between two radios but also in terms of the ability of particular radios and radio systems to separate desired from undesired signals. harm from interference has both technical dimensions (how well a radio or radio system can separate desired from undesired signals) and economic dimensions (the costs and distribution of costs of building, 5  see, for example, bennett z. kobb, wireless spectrum finder: telecommunications, government and scientic radio frequency allocations in the us 30 mhz300 ghz, mcgrawhill, new york, 2001.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.78 wireless technology prospects and policy optionsdeploying, and operating a radio/radio system with particular technical characteristics that make it easier to disambiguate the signals). today, technology is enabling new ways of mitigating interference. the degrees of freedom available for managing interference go beyond the traditional parameters of frequency and geographical area and include amplitude, frequency, space, time, and polarization. interference mitigation can also be thought of in terms of the behavior of radio systems rather than individual radios. in the future, coordination and cooperation are more likely to be winwin situations; a key question is how to motivate such cooperation.regulation is beginning to re˚ect these opportunities. historically, interference between adjacent bands has been mitigated by inserting guard bands. under recently adopted rules for the 700mhz band, for example, there are no guard bands, leaving it up to users to gure out how to mitigate interference, whether by cooperation among users, investment in better receivers, or by other means. this is a good example of a more technology and serviceneutral approach to regulation. rather than mandate a particular technical solution, the idea is to be ˚exible and allow users to nd the best ways of increasing overall efciency.introducing technological capabilities for  more sophisticated spectrum management some current and emerging technologies could make it much easier to introduce new services into crowded frequency bands. given sufcient motivation, ingenuity, and investment, it is not possible to obtain signicant improvements in communications capacity in a particular piece of spectrum, but migrating current nondigital services to digital transmission will be a major challenge, especially for specic applications like aviation radios, which have a large, politically powerful legacy base. improvements are more feasible in bands where the disadvantages of migration are not so widely distributed across so many users, where the user base is a less potent political force, or where the market dynamics are such that enduser technology is regularly refreshed.smart antennas, for example, could mitigate interference problems in an overlay system. by focusing a beam from the transmitter to a receiver, devices with smart antennas can signicantly reduce their overall transmission power. they could also scan their environment for other transmissions and transmit in directions that help avoid interference. these technologies are not very practical at lower frequencies but become more so at somewhat higher frequencies.moreover, it may be possible to incorporate more sophisticated approaches into receiver specications established through either stanwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.policy options 79dards or regulations. adaptive radios need to be able to sense their environment, negotiate with other radios, and adjust their operation accordingly. doing so requires radios that can listen to a much wider range of signals and distinguish among various signals more accurately than is required for a conventional radio. a receiver™s ability to sense a small signal in the presence of a nearby larger signal is limited both by noise, which tends to corrupt measurement of the received signal, and by the receiver™s dynamic range. thus, adaptive radios are viable only if radios meet demanding specications for both dynamic range and noise. the problem remains of how to deal with legacy hardware, which does not have this capability built in because it was made before receiver performance was improved to exploit these opportunities. such higherquality receivers also cost more, have a more complex design, and consume more power. even small additional costs matter a great deal when service providers are ghting to save pennies. the additional investment can have a big payoff, however, if it enables new applications that are not otherwise possible.developing complementary policy to allow negotiation among usersa complement to the introduction of new technology is the creation of a policy environment in which neighbors (and others whose services experience interference) are free to negotiate a mutually acceptable outcome. this notion, rst proposed by coase,6 provides for market negotiations to complement or replace regulatory mandates. a new arrangement may not be optimal for a given set of parties and might run the risk of becoming obsolete as technologies emerge, but such negotiations allow for ˚exibility in situations such as the following:ł operator a spills over into neighbor b™s spectrum in a manner that is acceptable under current regulation but is costly to neighbor b, who should be free to pay a to not spill over.ł operator a seeks to implement a service that will interfere with operator b™s service unless operator b improves the interferencerejection capabilities of its receivers. operator a should be free to pay b for these improvements.it is important to recognize, however, that if the transaction costs such as for bargaining are high, the bargains are likely to be less efcient. for 6  r.h. coase, ﬁthe federal communications commission,ﬂ journal of law and economics 2(1):1, 1959.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.80 wireless technology prospects and policy optionsexample, the introduction of more sophisticated devices and network architectures could make it more difcult to know who is spilling over into a neighbor™s usage rights, and who is not. one can also envision scenarios in which such bargaining might not improve the overall efciency of spectrum use. if license holders can negotiate with others to shut down interfering transmissions, the former will have less incentive to invest in innovative devices that can operate well in the presence of noise. similarly, to the extent that device manufacturers know that their customers will not be able to protect themselves from interference, they will be motivated to invest in more robust, smarter devices that can give their purchasers better communications irrespective of whether or not there is an interferencereducing agreement. trading absolute outcomes for statistically acceptable outcomes approaches that use a statistical probability of interference of less than 100 percent do not necessarily lead to ruinous outcomes that will destroy service. rather, these approaches seek to relax constraints so as to normally (or almost always) provide good outcomes but accept poorer outcomes with acceptable probabilities and consequences. that is, the system attempts to offer optimal performance most of the time to most users and degrades softly under less optimal conditions. the difference between the approaches emphasizing absolute and acceptable outcomes regarding interference is somewhat analogous to the difference between personal auto safety (which ﬁacceptsﬂ a certain number of accidents) and common carrier air safety (which has an explicit albeit unrealizable goal of zero accidents). the latter approach has already been embraced in some aspects of telecommunications. the internet™s besteffort design, for example, does not guarantee quality of service yet generally provides an acceptable overall experience. acceptance of a similar tradeoff was re˚ected in the market™s favoring ethernet over token ring technology in the early days of local area networks. already in the wireless space, such imperfections as holes in coverage area or lowerquality audio (compared to a landline) are accepted in exchange for the convenience of mobility.such a relaxation of requirements could signicantly open up opportunities for nonexclusive use of frequency bands. rather than have regulators decide on acceptable quality, it might be desirable to allow licensees ˚exibility to negotiate mutually benecial arrangements even though the result at times might be degraded quality.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.policy options 81embracing ﬁdesign for lightﬂ and ﬁdesign for darknessﬂ  more broadly in design concepts and regulatory frameworksmany systems have been ﬁdesigned for darknessﬂšthat is, under the assumption that a particular band has been set aside for a particular service or operator and that there are no other emissions in that band. cellular systems are a notable example of this approach. an alternative is to design for light, with an assumption of a noisy, cluttered environment. both are reasonable design approaches for certain applications and services, but it is important to be clear about which mode is appropriate under what circumstances. the historical preference has been to design for darkness, whereas today, technological advances suggest opening up more bands in the designforlight modality. these techniques include beam steering, enhanced signal processing, and network coordination. to design for light will require better information than is available today on sources of potential interference. broadening the scope of inquiry to encompass receivers and networks of transceiversmuch regulation has focused on transmitters, with specications for transmission frequency and bandwidth, geographical location, and transmit power. increasing use of new radio architectures (discussed above) suggests that the scope of inquiry be broadened to look at the properties and behaviors of receivers and networks of transceivers. better receiver standards would create an environment in which receiver capabilities present less of a barrier than they do today for implementing new spectrum sharing schemes. for example, it might be possible to overlay unlicensed use onto licensed use with receiver specications written to these standards. expanding the scope for policy or regulation to a system of radios rather than an individual radio also would open up new opportunities. for example, a network of radios can help avoid the hidden node problem because it can use multiple network elements to listen from multiple points for transmissions. also, a network of radios would be able to relay a transmission through hops at lower power at each node rather than directly at higher power, thus decreasing the chance it would interfere with another system. receivers could also report on their positionšfor example, via embedded gps receiversšalthough this capability has cost and potential privacy implications.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.82 wireless technology prospects and policy optionsexploiting programmability as discussed above, technology has enabled highly programmable radios. to be sure, such radios are not practical in many circumstances today because of their complexity, power use, and dollar costs, especially for mobile devices. nonetheless, programmable radios are being used for some applications today (such as cellular base stations), and it is reasonable to expect wider use in the future. one implication of this programmability is that the radio operating parameters can be made modiable to comply with policy or rule changes. deployment of devices with such capabilities opens up new opportunities for more ˚exible regulation and for policy makers to safely work more incrementally. namely, (1) policies would not need to be homogeneous and could be adapted to local environmental conditions such as signal density, (2) the operating rules of existing devices could be revised to accommodate new technology, and (3) devices could more easily be certied for international use because they can readily be switched to comply with local policy. although revisability may sound attractive, the opportunity must be weighed against some signicant drawbacks. paradoxically, rules that require revisability could actually have the effect of discouraging deployment and investment if they are seen as weakening the commitments made by regulators. the most likely scenario, if such a policy were poorly drafted, would be that most industry participants would take a waitandsee position, which defeats the purpose of providing ˚exible and revisable rules for quick adoption. there are possible mechanisms to address this concern, such as offering investors compensation if the rules on which they relied are materially changed. such mechanisms would need to be carefully considered as part of any rulemaking that sought to exploit revisability. exploiting adaptive and environmentsensing capabilities that can help reduce centralized managementas agility, sensing, and coordination improve, and as etiquettes and standards for these capabilities develop, opportunities will likely arise for reduction of centralized management. potential advantages to this approach include a lower barrier to entry (because entry either will not require engagement with a regulator for spectrum assignment or will entail negotiation with an existing license holder, or it will be easier and less costly to nd an existing license holder willing to share its spectrum assignments) and ˚exibility of use (because operation is dened primarily by the attributes of radio equipment rather than by regulation). potential disadvantages to this approach include uncertainty about the technical feasibility and the added costs of building more capable and robust wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.policy options 83radios. such a shift is also predicated on resolving the issues discussed above about more robust receiver design. some current proposals would maintain a form of centralized control but would replace regulation with much more nimble and dynamic approaches, such as services that collect and distribute information about or grant access to open channels. establishing mechanisms for dealing with legacy systemsin recent years, notable efforts to deal with legacy systems have included relocating microwave services to allow deployment of pcs  cellular telephony and the relocation of nextel cell services out of public safety bands. more recently, relocation of government services as well as broadcast radio services and xed services has been undertaken for new 3g advanced wireless services bands. having an easier process for making such changes is a critical enabler of more dynamic policies to meet changing technologies and market needs. although there are costs and difculties associated with relocating infrastructure elements, an even bigger legacy challenge is the need to migrate potentially millions of userowned or useroperated devices. among the options for dealing with legacy systems are the following:ł commissioning independent neutral analyses to support decision making about potential interference with legacy services based on actual harm rather than political claims.ł establishing streamlined recovery procedures. claims of interference are inevitable where old and new systems coexist. a streamlined process would help identify, report, and resolve such claims.ł establishing databases of legacy equipment. it is far easier to  coexist with legacy systems if details about their operation are known. a lightweight system for registering systems would help to facilitate the creation of a useful database. ł exploiting technological improvement. as radios become more capable, they will be increasingly able to coexist with existing users and services. future policy should require or incentivize new users to coexist with existing usersšfor example, by making future devices more ˚exible (e.g., adaptable lters and oscillators and reprogrammability) so that their operation can be relocated more readilyšand should avoid rules that inhibit this.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.appendixeswireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.87appendix abiographies of committee members and staffdavid e. liddle, chair, is a general partner in the rm u.s. venture partners (usvp), a leading silicon valley venture capital rm that specializes in building companies from an early stage in digital communications, networking, wireless communications, semiconductors, technical software, and ehealth. he retired in december 1999 after 8 years as ceo of interval research corporation. during and after his education (b.s., electrical engineering, university of michigan; ph.d., computer science, university of toledo, ohio), liddle has spent his professional career developing technologies for interaction and communication in research, development, management, and entrepreneurship. first, he spent 10 years at the xerox palo alto research center and the xerox information products group, where he was responsible for the rst commercial implementation of the graphical user interface and local area networking. he then founded metaphor computer systems, whose technology was adopted by ibm and the company ultimately acquired by ibm in 1991. in 1992, liddle cofounded interval research corporation with paul allen. during his tenure, the company formed six new companies and several joint ventures based on the research conducted at interval. he is a consulting professor of computer science at stanford university. he has served as a director at sybase, broderbund software, metricom, starwave, and ticketmaster; he is currently a director with the new york times and numerous earlystage companies. he was honored as a distinguished alumnus from the university of michigan and is a member of the national advisory committee at the college of engineering of that university. he is also a member of the wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.88 wireless technology prospects and policy optionsadvisory committee of the school of engineering at stanford university, and of the college of engineering at the university of california, berkeley. he has been elected a senior fellow of the royal college of art for his contributions to humanœcomputer interaction. his current technology and investment interests are focused on signal processing, with an emphasis on wireless communications.yochai benkler is the jack n. and lillian r. berkman professor of entrepreneurial legal studies at harvard law school and faculty codirector of the berkman center for internet and society at harvard university. his research focuses on the effects of laws that regulate information production and exchange on the distribution of control over information ˚ows, knowledge, and culture in the digital environment. his particular focus has been on the neglected role of commonsbased approaches toward the management of resources in the digitally networked environment. his books include the wealth of networks: how social production transforms markets and freedom (2006), which received the don k. price award from the american political science association for best book on science, technology, and politics; the american sociological association™s citasa book award for an outstanding book related to the sociology of communications or information technology; the donald mcgannon award for best book on social and ethical relevance in communications policy research; and was named best business book about the future by the magazine strategy+business. in civil society, benkler™s work was recognized by the electronic frontier foundation™s pioneer award in 2007 and by the public knowledge ip3 award in 2006. previously, benkler was a professor at yale university and new york university school of law. david borth is an expert on wireless communications, with insight into both national security and commercial needs. he is corporate vice president and director of the communications research laboratories of motorola, inc., a part of the company™s research arm, motorola labs. borth joined motorola in 1980 as a member of the systems research laboratory in corporate research and development in schaumburg, illinois. as a member of that organization, he has conducted research on digital modulation techniques, adaptive digital signal processing methods applied to communication systems, and personal communication systems including both cellular and pcs systems. he has contributed to motorola™s implementations of the gsm, tdma (is54/is136), and cdma (is95) digital cellular systems. in his current role, he manages a multinational (united states, australia, france, japan, united kingdom) organization focusing on all aspects of communication systems ranging from theoretical systems studies to system and subsystem analysis and implementation wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.appendix a 89to integrated circuit designs. borth received his b.s., m.s., and ph.d. degrees in electrical engineering from the university of illinois at urbana champaign. previously, he was a member of the technical staff of the systems division of watkinsjohnson company and an assistant professor in the school of electrical engineering, georgia institute of technology. borth is a member of motorola™s science advisory board associates and has been elected a dan noble fellow, motorola™s highest honorary technical award. he has been issued 31 patents and has authored or coauthored chapters of ve books in addition to 25 publications. he received the distinguished alumnus award from the university of illinois electrical and computer engineering alumni association and was elected a fellow of the institute of electrical and electronics engineers for his contributions to the design and development of wireless telecommunication systems. he is a registered professional engineer in the state of illinois. borth was a member of the computer science and telecommunications board from 2000 to 2003. he also served on the cstb committee that produced the report information technology for counterterrorism: immediate action and future possibilities (2003).robert w. brodersen is the john r. whinnery distinguished professor in the department of electrical engineering and computer science at the university of california, berkeley. he is also the coscientic director of the berkeley wireless research center, where he works on the application of integrated circuits as applied to personal communication systems, with an emphasis on wireless communications and low power design. brodersen™s research is focused in the areas of lowpower design and wireless communications and the cad tools necessary to support these activities. he has won best paper awards for a number of journal and conference papers in the areas of integrated circuit design, cad, and communications, including the w.g. baker award in 1979. in 1982 he became a fellow of the ieee. he was corecipient of the ieee morris k. liebmann award in 1983. he received technical achievement awards from the ieee circuits and systems society in 1986, from the signal processing society in 1991, and in 1999 from the acm special interest group in mobile computing. brodersen was elected a member of the national academy of engineering in 1988. in 1996, he received the ieee solid state circuits award. brodersen was awarded an honorary doctorate from the university of lund, sweden, in 1999, and in 2000 he received the millennium award from the circuits and systems society and the golden jubilee award from the ieee. in 2001 he was awarded the lewis winner award for outstanding paper at the ieee international solidstate circuits conference. he has served on the editorial board or as a reviewer for numerous scholarly journals and publications including the ieee jourwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.90 wireless technology prospects and policy optionsnal of solidstate circuits, ieee transactions on vlsi systems, ieee personal communications magazine, and wireless personal communications (kluwer press). he is the author or coauthor of more than 60 journal publications and 120 published conference papers and is the author, coauthor, editor, or contributor to 14 books, including an anatomy of a silicon compiler (1992, kluwer academic publishers) and low power digital cmos design (1995, kluwer academic publishers). he received a ph.d. degree in engineering from the massachusetts institute of technology (mit) in 1972.david d. clark graduated from swarthmore college in 1966 and received his ph.d. from mit in 1973. he has worked since then at the mit laboratory for computer science, where he is currently a senior research scientist in charge of the advanced network architecture group. clark™s research interests include networks, network protocols, operating systems, distributed systems, and computer and communications security. after receiving his ph.d., he worked on the early stages of the arpanet and on the development of token ring local area network technology. since the mid1970s, clark has been involved in the development of the internet. from 1981 to 1989, he acted as chief protocol architect in this development and chaired the internet activities board. his current research area is protocols and architectures for very large, very high speed networks. specic activities include extensions to the internet to support realtime trafc, explicit allocation of service, pricing, and new network technologies. in the security area, clark participated in the early development of the multilevel secure multics operating system. he developed an information security model that stresses integrity of data rather than disclosure control. clark is a fellow of the acm and the ieee and is a member of the national academy of engineering. he received the acm sigcomm award and the ieee award in international communications, as well as the ieee hamming award for his work on the internet. he is a consultant to a number of companies and serves on a number of technical advisory boards. he chaired the committee that produced the cstb report computers at risk: safe computing in the information age and served on several committees that produced several cstb reports.thomas (ted) darcie received his ph.d. degree in aerospace physics from the university of toronto in 1982. currently, he is a professor at the university of victoria, british columbia, holding a tier 1 canada research chair in optical systems for communications, imaging and sensing. previously he worked at at&t bell laboratories at crawford hill, holmdel, new jersey, where he joined the technical staff to study a wide variety of topics related to lightwave telecommunications, including ber fabrication processes, semiconductor lasers, optical ampliers, and wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.appendix a 91numerous modulation and multiplexing techniques. he has been a lead gure in the development of lightwave systems for analog applications in cable television and wireless systems. as head of access communications research at at&t bell laboratories (19891995), he was responsible for technology innovation in wireless, lightwave, and hybrid bercoax systems. he has authored more than a hundred technical publications and 25 patents spanning this broad set of technologies. from 1995 to 2002, he was vice president at at&t laboratories, in charge of communications infrastructure research. his research laboratory provided technology support for at&t™s diverse requirements in optical networking, broadband access, xed wireless access, wireless lan, and cellular systems. his team worked closely with at&t businesses to provide technical expertise and vision and had numerous programs devoted to the evolution of mobile and broadband services, applications, and technologies. in 2002 and 2003, he was vice president for at&t labs research network architecture and strategic operations planning vice president, with responsibility for connecting innovative network technologies with opportunities within at&t™s network. darcie is an at&t fellow and a fellow of the ieee. dale n. hateld is an independent consultant and adjunct professor in the department of interdisciplinary telecommunications at the university of colorado at boulder. between december 2000 and april 2002, hateld served as chair of the department. prior to joining the university of  colorado, he was the chief of the ofce of engineering and technology at the federal communications commission (fcc) and immediately before that was chief technologist at the agency. before joining the commission in december 1997, he was ceo of hateld associates, inc., a multidisciplinary telecommunications consulting rm in boulder, colorado, for 15 years. before that, he was deputy assistant secretary of commerce for communications and information and deputy administrator of the ntia. before moving to the ntia, hateld was chief of the ofce of plans and policy at the fcc. in 1973 he received a department of commerce silver medal for contributions to domestic communications satellite policy and in 1999 received the attorney general™s distinguished service award. in 2000, he received the personal communications industry association (pcia) foundation™s eugene c. bowler award for exceptional professionalism and dedication in government service and the fcc™s gold medal award for distinguished service. more recently, he received the distinguished engineer award from the university of colorado at boulder. he is a fellow of the radio club of america. in february 2001, the federal trade commission appointed hateld as a monitor trustee for the aol/time warner merger. he also serves on the board of directors of crown castle international and kbdi tv12 public television in denver. hateld wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.92 wireless technology prospects and policy optionsholds a b.s. in electrical engineering from case institute of technology and an m.s. in industrial management from purdue university.michael l. katz is the edward j. and mollie arnold professor of business administration of the haas economic analysis and policy group and director of the center for telecommunications and digital convergence at the university of california, berkeley. in 2001 and 2002, he was deputy assistant attorney general for economic analysis in the antitrust division of the department of justice. from 1994 to 1996, he was chief economist at the federal communications commission. he is coeditor of the california management review and journal of economics and management strategy. he is a former member of the cstb of the national research council. he received his ph.d in economics from oxford university. paul j. kolodzy is currently a technology consultant in advanced wireless and networking technology, drawing on 20 years of experience in technology development for advanced communications, networking, electronic warfare, and spectrum policy for government, commercial, and academic clients. before becoming a consultant, kolodzy was the director of the wireless network security center (winsec), a research facility at stevens institute of technology that draws on wideranging expertise to design, develop, and evaluate technology for the secure transmission of voice, video, and data. previously, kolodzy had been appointed as the chair of the fcc™s spectrum policy task force, which was charged with examining spectrum allocation processes and other issues so that spectrum could be put to the best use in a timely manner. before joining the fcc, kolodzy served as a program manager within the advanced technology ofce of the defense advanced research projects agency (darpa) at the department of defense. at darpa, he oversaw the initiation of nextgeneration communications technology, which included the next generation (xg) communications initiative. the xg project developed technology that has the potential to fundamentally change the manner in which spectrum is allocated and assigned. kolodzy has also held positions at mit™s lincoln laboratory and lockheed martin corporation in the development and management of advanced signal processing, rf, and eo systems. kolodzy received a b.s. in chemical engineering from purdue university and an m.s. and a ph.d. in chemical engineering from case western reserve university.larry larson is a professor of electrical and computer engineering and director of the center for wireless communications at the university of california, san diego (ucsd). his research ranges from electronic circuits and systems to electronic devices and materials. larson develops wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.appendix a 93highspeed circuits based on inp (indiumphosphide) and gaas (gallium arsenide) as well as silicon germanium and cmos technology. he also explores applications for micromachining technology in the manufacture of highspeed integrated circuits and studies new packaging technology for them. larson™s current research is specically focused on lowpower circuit design and rf design techniques for wireless communications. he recently completed cdma mobile radio design, a book on how to design the hardware and software for wireless handsets based on codedivision multiple access technology. cdma is the foundation of all 3g wireless technologies, including europe™s wcdma standard and cdma2000. as director of the industrysponsored center for wireless communications (cwc) at ucsd, he oversees a wide range of ongoing research projects, with funding from cwc™s 17 corporate members. he is the rst holder of the communicationsindustryendowed chair at the jacobs school. he joined the ucsd faculty in 1996 after a 16year career at hughes research laboratories. there, he pioneered the development of analog integrated circuits and lownoise highelectronmobility transistors in iiiv technology, as well as microwave integrated circuits in sige hbt technology and rf mems technology. larson received a ph.d. from the university of california, los angeles, in 1986. he is an ieee fellow and cowinner of the 1996 hughes electronics lawrence hyland patent award and the 1999 ibm microelectronics excellence award. david p. reed is a senior vice president in the chief scientist group at sap labs and an adjunct professor at the mit media laboratory. he was previously a fellow at hp labs. reed™s work focuses on using digital technology to transform the design of technological, business, and social systems. his explorations center on exploiting new information technologies that enable people to be more effective, including mobile computing; highly scalable wireless networking; group information sharing; pervasive  networking; video media processing; and infrastructures for electronic commerce. reed spent 4 years at interval research corporation exploring portable and consumer media technology. for 7 years before joining interval, reed was vice president and chief scientist for lotus development corporation, where he led the design and implementation of key products, including 123, and technical business strategy. reed was also a professor in mit™s laboratory for computer science. he is coinventor of the endtoend argument, often called the fundamental architectural principle of the internet. he holds a b.s. in electrical engineering and m.s. and ph.d. degrees in computer science and engineering from mit.gregory rosston is the deputy director of the stanford institute for economic policy research. his research focuses on industrial organization, wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.94 wireless technology prospects and policy optionsantitrust, and regulation. he has written numerous articles on competition in local telecommunications, implementation of the telecommunications act of 1996, and auctions and spectrum policy. he has also coedited two books, including interconnection and the internet: selected papers from the 1996 telecommunications policy research conference. before joining stanford university, rosston served as deputy chief economist of the fcc, where he helped to implement the telecommunications act. in this work, he helped to design and write the rules that the fcc adopted as a framework to encourage efcient competition in telecommunications markets. he also helped with the design and implementation of the fcc™s spectrum auctions. rosston received his ph.d. in economics from stanford university and his a.b. in economics with honors from the university of california, berkeley.david skellern is ceo of national ict austrialia (nicta). skellern began his career in 1974 at the university of sydney, where he spent a decade designing, building, and commissioning instrumentation and extensions for the fleurs synthesis radiotelescope, one of australia™s pioneering giant radiotelescopes. from 1983 to 1989 he held various academic appointments as a staff member of sydney university™s electrical engineering department. in 1989, skellern took up the chair of electronics at macquarie university. he also spent considerable time working in industry as a visiting researcher, including more than 2 years at hewlett packard laboratories. in 1997 he cofounded the radiata group of companies in australia and the united states, established to commercialize the results of the wlan research project that he led at macquarie university in collaboration with the commonwealth scientic and industrial research organization. over the next 3 years he played an integral role in building a successful company with a team of 65. in september 2000 radiata demonstrated the world™s rst chipset implementation of the 54 mbps ieee 802.11a, a highspeed wlan standard. radiata was acquired by cisco systems, inc., in 2001 for 565 million australian dollars, at which time skellern joined cisco and subsequently moved to the united states as technology director of the wireless networking business unit. skellern was appointed to the nicta board in 2003. he received a b.sc. (computer science and mathematics) in 1972, a b.e. (electrical engineering) in 1974, and a ph.d. in 1985 from the university of sydney.staffjon eisenberg is director of the computer science and telecommunications board of the national academies. at cstb, he has been the study director for a diverse body of work, including a series of studies explorwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.appendix a 95ing internet and broadband policy and networking and communications technologies, and a study of how to use information technologies to enhance disaster management. from 1995 to 1997 he was a aaas science, engineering, and diplomacy fellow for the u.s. agency for international development, where he worked on environmental management, technology transfer, and information and telecommunications policy issues. he received his ph.d. in physics from the university of washington in 1996 and a b.s. in physics with honors from the university of massachusetts at amherst in 1988.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.96appendix bspeakers at meetingsalthough the briefers and workshop speakers listed below provided much useful information of various kinds to the committee, they were not asked to endorse the report™s conclusions or recommendations, nor did they see the nal draft of this report before its release.october 2324, 2003 the national academies washington, d.c.joseph b. evans, national science foundationmichael d. gallagher, national telecommunication and information administrationpaul kolodzy, wireless network security center, stevens institute of technologyjames a. lewis, center for strategic and international studiesjames h. snider, new america foundationpeter tenhula, federal communications commissionjanuary 2930, 2004 stanford universitys palo alto, californiabob brodersen, university of california, berkeleymichael howse, packethopdevabhaktuni srikrishna, tropos networkswireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.appendix b 97february 1213, 2004 workshop the national academies washington, d.c.siavash alamouti, vivatorichard barth, department of commercesamuel w. bodman, department of commercedavid g. boyd, safecom program, department of homeland securitythera bradshaw, city of los angeles information technology agencycharles n. brownstein, computer science and telecommunications boardduane buddrius, alvarion, inc.jim bugel, cingular wireless llcleigh chinitz, proxim corporationmark cooper, consumer federation of americadiane cornell, cellular telecommunications & internet associationthomas cowper, statewide wireless network, new york™s ofce of technologydavid donovan, association for maximum service television, inc.tyler duvall, department of transportationharold feld, media access projectbruce fette, general dynamics decision systemsmichael gallagher, national telecommunications and information administrationmerri jo gamble, department of justicemichael green, atheros communicationskalpak gude, panamsatrobert gurss, association of publicsafety communications ofcials internationaldewayne hendricks, dandin group, inc.bradley holmes, arraycomm, inc.nancy jesuale, net city engineering, inc.kevin kahn, inteljulius knapp, federal communications commissionrobert legrande, ofce of the chief technology ofcer, district of columbiapat mahoney, iridium satellite llcpreston f. marshall, advanced technology ofce, defense advanced research projects agency william moroney, united telecom council and united power line counciljohn muleta, wireless telecommunications bureau, federal communications commissionwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.98 wireless technology prospects and policy optionsglen nash, department of general services, state of californiascott pace, national aeronautics and space administrationcarl panasik, texas instrumentsandrea petro, ofce of management and budgetmarilyn praisner, montgomery county councildipankar raychaudhuri, wireless information network lab, rutgers universitypaul rinaldo, american radio relay leaguegeorge (gee) rittenhouse, lucent technologieskenneth ryan, comsearchgreg schmidt, lin television corporationdavid siddall, paul, hastings, janofsky, and walker, llpjim smoak, verizon wirelesscarl stevenson, agere systemskaren st. germain, national oceanic and atmospheric administrationthomas walsh, boeing space and communication spectrum managementjennifer warren, lockheed martincharles wheatley, qualcommdonald willis, federal aviation administrationmoe z. win, massachusetts institute of technologybadri younes, department of defensejuly 2223, 2004 university of california, san diego san diego, californiabob brodersen, university of california, berkeleymichael chartier, intelrobert matheson, national telecommunications and information administrationallen petrin, georgia institute of technologychuck wheatley, qualcommwireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.99appendix cstatement of taskan expert committee will be convened to conduct a comprehensive assessment of wireless technology and application trends and their implications for spectrum management and policy. the study will be grounded in an assessment of how technology capabilities are evolving, including the implications of emerging technologies (such as software radios, smart antennas, and other intelligent signal processing), architectural alternatives (such as base stationbased and peertopeer), services (such as 3rd and 4th generation mobile, local area networking, and xed broadband), and applications. building on this technology assessment, the study will also examine the interplay between the technical, economic, and  policy issues. key policy issues to be considered include spectrum supply and demand, alternative spectrum management approaches (including  unlicensed approaches), standardssetting processes and forums, and how the international environment is evolving and affecting u.s. policy options. the committee is seeking broad input on these issues from academic and industry experts and diverse stakeholders.in addition, the committee will convene a workshop examining the present and prospective needs of public and privatesector spectrum users and technology and policy options for more efcient and effective use of spectrum. the scope of topics to be considered in the workshop will be very similar to that of the broader study, but with additional emphasis on federal, state, and local government spectrum uses. the committee will issue a brief report of the workshop. the committee will also produce a nal report with consensus ndings and recommendations.wireless technology prospects and policy optionscopyright national academy of sciences. all rights reserved.
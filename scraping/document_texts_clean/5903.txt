detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/5903defining a decade: envisioning cstb's second 10 years116 pages | 8.5 x 11 | paperbackisbn 9780309059336 | doi 10.17226/5903computer science and telecommunications board, national research councildefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.proceedings of cstbõs 10th anniversary symposiummay 16, 1996 ñ washington, d.c.computer science and telecommunications boardcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1997definingadecadeenvisioning csibõs second 10 yearsdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.national academy press ¥ 2101 constitution avenue, n.w. ¥ washington, dc 20418notice: this report derives from cstbõs core program, which was approved by the governingboard of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine.this report has been reviewed by a group other than the authors according to procedures approved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance ofscience and technology and to their use for the general welfare. upon the authority of the chartergranted to it by the congress in 1863, the academy has a mandate that requires it to advise thefederal government on scientific and technical matters. dr. bruce alberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the nationalacademy of sciences, as a parallel organization of outstanding engineers. it is autonomous in itsadministration and in the selection of its members, sharing with the national academy of sciencesthe responsibility for advising the federal government. the national academy of engineering alsosponsors engineering programs aimed at meeting national needs, encourages education and research,and recognizes the superior achievements of engineers. dr. william a. wulf is president of thenational academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to securethe services of eminent members of appropriate professions in the examination of policy matterspertaining to the health of the public. the institute acts under the responsibility given to the nationalacademy of sciences by its congressional charter to be an adviser to the federal government and,upon its own initiative, to identify issues of medical care, research, and education. dr. kenneth i.shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 toassociate the broad community of science and technology with the academyõs purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both thenational academy of sciences and the national academy of engineering in providing services tothe government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. bruce alberts and dr. williama. wulf are chairman and vice chairman, respectively, of the national research council.support for this project was provided by core funds of the computer science and telecommunications board. core support for cstb is provided by its public and private sponsors: the air forceoffice of scientific research, defense advanced research projects agency, department of energy,national aeronautics and space administration, national library of medicine, national sciencefoundation, office of naval research, apple computer, inc., at&t laboratories, digital equipment corporation, hewlettpackard company, intel corporation, lucent technologies, inc., andmotorola, inc. any opinions, findings, conclusions, or recommendations expressed in this materialare those of the symposium presenters and do not necessarily reflect the views of the sponsors.international standard book number 030905933xadditional copies of this report are available from:computer science and telecommunications board2101 constitution avenue, nwwashington, dc 20418www2.nas.edu/cstbwebcopyright 1997 by the national academy of sciences. all rights reserved.printed in the united states of americadefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.iiicstbõs 10th anniversary symposium planning committeemichael l. dertouzos, massachusetts institute of technologyleonard kleinrock, university of california, los angelesjoseph f. traub, columbia universitywilliam a. wulf, university of virginiastaffmarjory s. blumenthal, directorjean e. smith, program associatedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.ivcomputer science and telecommunications boarddavid d. clark, massachusetts institute of technology, chairfrances e. allen, ibm t.j. watson research centerjames chiddix, time warner cablejeff dozier, university of california at santa barbaraa.g. fraser, at&tsusan l. graham, university of california at berkeleyjames gray, microsoft corporationbarbara j. grosz, harvard universitypatrick m. hanrahan, stanford universityjudith hempel, university of california at san franciscodeborah a. joseph, university of wisconsinbutler w. lampson, microsoft corporationedward d. lazowska, university of washingtonmichael lesk, bellcoredavid liddle, interval researchbarbara h. liskov, massachusetts institute of technologyjohn major, qualcomm incorporateddavid g. messerschmitt, university of california at berkeleydonald norman, hewlettpackard companyraymond ozzie, iris associates, incorporateddonald simborg, knowmed systemsleslie l. vadasz, intel corporationmarjory s. blumenthal, directorherbert s. lin, senior staff officerjerry r. sheehan, program officeralan s. inouye, program officerjon eisenberg, program officermark e. balkovich, research associateleslie m. wade, research assistantlisa l. shum, project assistantsynod p. boyd, project assistantdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.vcommission on physical sciences, mathematics, and applicationsrobert j. hermann, united technologies corp., cochairw. carl lineberger, university of colorado, cochairpeter m. banks, environmental research institute of michiganlawrence d. brown, university of pennsylvaniaronald g. douglas, texas a&m universityjohn e. estes, university of california at santa barbaral. louis hegedus, elf atochem north america inc.john e. hopcroft, cornell universityrhonda j. hughes, bryn mawr collegeshirley a. jackson, u.s. nuclear regulatory commissionkenneth h. keller,university of minnesotakenneth i. kellermann, national radio astronomy observatorymargaret g. kivelson, university of california at los angelesdaniel kleppner, massachusetts institute of technologyjohn kreick, sanders, a lockheed martin companymarsha i. lester, university of pennsylvaniathomas a. prince, california institute of technologynicholas p. samios, brookhaven national laboratoryl.e. scriven, university of minnesotashmuel winograd, ibm t.j. watson research centercharles a. zraket, mitre corp. (retired)norman metzger, executive directordefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.viiprefaceas the computer science and telecommunications board (cstb) approached its 10th anniversary in 1996, anumber of current and former members began to entertain the idea of convening a special event to celebrate theoccasion. taking the lead were the first chair of cstb, joseph traub; then chair, william wulf; and former boardmembers michael dertouzos and leonard kleinrock. the planning committee expanded the customary afternoonopen session of the boardõs spring meeting into a oneday symposium on may 16 in washington, d.c. two goalswere identified for the event: to celebrate 10 years of cstbõs achievements and to begin to define its goals for thenext decade.in order to involve as many cstb alumni as possible, the committee enlisted many former board andcommittee members as presenters. in addition, two panels were convened: one to address ways in which cstbinteracts with its sponsors and one to identify the emerging issues likely to be on cstbõs agenda during the next10 years. to expand the audience for these exciting presentations and to broaden the discussions begun on may16, 1996, the proceedings have been compiled. this volume represents the edited versions of the presentations,which were also reviewed by several cstb alumni for accuracy and meaning. it reflects the events of a day thatwas characterized by thoughtful presentations and lively discussion. although some speaker affiliations havechanged since the symposium, in general, those in effect then are presented to preserve the context for thediscussion. the groundwork was laid for setting cstbõs agenda for the next 10 years, opening the door for theboardõs further thought and refinement over the following months.this publication is part of a body of publications produced by the computer science and telecommunicationsboard in its 11year history. the board, part of the national research council within the national academy ofsciences, was established in 1986 to provide independent guidance to the federal government on technical andpublic policy issues in computing and communications. composed of leaders from industry and academia, cstbconducts studies that recommend actions by government, industry, and academic researchers on critical nationalissues. these studies, prepared by principals from the public and private sectors, provide a balanced perspectiveon the issues at hand. cstb also provides a neutral meeting ground for the consideration and focusing of complexissues where resolution and action may be premature. it convenes invitational discussion sessions that bringtogether principals from the public and private sectors to share perspectives on all sides of an issue, ensuring thatthe debate is not dominated by the loudest voices.cstb is grateful to the symposium presenters, moderators, and participants for their contributions. asalways, special thanks go to cstbõs sponsors, whose ongoing support made this symposium possible. thesedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.viiiprefaceinclude the air force office of scientific research, defense advanced research projects agency, department ofenergy, national aeronautics and space administration, national library of medicine, national science foundation, office of naval research, apple computer inc., at&t laboratories, digital equipment corporation,hewlettpackard company, intel corporation, lucent technologies inc., and motorola inc. cstb also wishes tothank sematech for a contribution that helped to defray symposium costs.david d. clark, chaircomputer science and telecommunications boarddefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.1from infoware to infowar................................................................................................................1joseph f. traub2linking the cstb community to the federal government:expert advice for policymakers...................................................................................................8michael r. nelson (moderator), david b. nelson, paul r. young, and howard frank3the global diffusion of computing: issues in development and policy.............18seymour e. goodman4engines of progress: semiconductor technology trends and issues...................22william j. spencer and charles l. seitz5computing and communications unchained: the virtual world...........................36leonard kleinrock and john major6picture this: the changing world of graphics..................................................................47henry fuchs, donald p. greenberg, and andries van dam7computational biology and the crossdisciplinary challenges...........................53federal research and funding policies, deborah a. josephfinding a home in academia, edward h. shortliffe8visions for the future of the fields..........................................................................................63david d. clark (moderator), edward a. feigenbaum, juris hartmanis,robert w. lucky, robert m. metcalfe, raj reddy, and mary shaw9unique challenges: computing and telecommunicationsin a knowledge economy..................................................................................................................73ellen m. knappcontentsixdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.10ancient humans in the information age.................................................................................83michael l. dertouzosappendixesaletter from dr. bruce alberts, president, national academy of sciences....................................................89bsymposium attendees......................................................................................................................................91cbiographies of presenters................................................................................................................................96xcontentsdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.1at its first meeting on may 7 and 8, 1986, the computer science and technology board (cstb) identified sixcritical national issues. i will tell you how those issues looked in 1986 and how they look today from my 1996perspective. then i will turn to a seventh issue that i did not want to raise publicly in 1986.from its inception, cstb deemed it important to look at both technical and policy issues. in 1986, it wasfairly unusual for national research council (nrc) boards to consider policy issues. therefore, at that firstmeeting we invited senior federal officials to give their views of the critical national issues. the officials includedcongressman donald fuqua, chairman, u.s. house of representatives committee on science and technology;john mctague, acting science adviser to president reagan; gordon bell, assistant director, computer and information science and engineering, national science foundation; robert c. duncan, director, defense advancedresearch projects agency (darpa); and alvin trivelpiece, director of research, u.s. department of energy.i will discuss the six critical areas identified in may 1986. when i speak about them from my 1996 vantagepoint, i am giving only my own view; i have not had the benefit of discussing these opinions with the board.in may 1986, the boardõs number one concern was competitiveness. how can the united states best ensurethe continued leadership of its computer science and technology enterprise in the face of intensified globalcompetition?competitiveness was a big issue in 1986. indeed, the opening sentences of the nrcõs press release announcing formation of the board were these: òu.s. leadership in computer research and manufacturing has beenseriously eroded. the nrc has established a computer science and technology board to advise federal agenciesand private firms on ways to strengthen u.s. international competitiveness in this field and to ensure that the fullpromise of this area is realized.ó we were deeply concerned about our competitive position, particularly vis ‹ visthe japanese, in areas such as computer chips, artificial intelligence, and software. for example, we had heard thatsoftware from japanese factories had unusually few errors.today, we must remain vigilant regarding our competitive position. the economic and national securitystakes are higher than ever, but our worst fears of 1986 were not realized. for example, according to williamspencer, president of sematech, in the mid1980s we were losing 3 to 4 percent of market share per year.today, the united states and japan have about 40 percent of market share each, while the rest of the world has 20percent of the $150 billion semiconductor market. business is booming for american companies in software,semiconductors, internetrelated infoware, and above all, in producing content. we must maintain and leveragethese positions in an economy that is increasingly international.1from infoware to infowarjoseph f. traubdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.2defining a decade: envisioning cstbõs second 10 yearsthe second item on cstbõs list was talent. how can the gap be closed between the small number of u.s.citizens graduating with ph.d. degrees in computer science and computer engineering and the large demand forgraduates with these skills? how can high school graduates be taught to deal with the computers and hightechnology they must use on the job in fields that range from the military to banking?i will divide this issue into two parts: ph.d. production and k12 education. we have succeeded in buildingph.d. production from some 200 a year to about 1,000 a year in computer science and engineering. in the 1990s,there has been concern about overproduction of ph.ds in some specialties as universities and research laboratoriesdownsize and as the field matures. i am not convinced the country needs 1,000 ph.d.s a year in computer scienceand engineering. i have seen the pain in physics and mathematics since about 1970; can we learn from history?incidentally, after a quarter of a century, and due to a number of circumstances, the crunch in physics andmathematics seems worse than ever. since most of these ph.ds will not be appointed to faculty positions in the topresearch universities, questions have also been raised about possible changes in the ph.d. program to producepeople better suited to positions in industry and colleges.problems in k12 education seem more serious and overwhelming than ever. there is a widespread feeling,which i certainly share, that americans do not get the education required for an informed populace in a democracy.they do not get an education that will enable them to fill many service sector jobs or to function in our hightechnology armed forces. students are not adequately prepared in analytic and writing skills to do well inuniversities. if we believe that educated people will be the key resource in the twentyfirst century, we have muchto be concerned about. bruce alberts, president of the national academy of sciences (nas), has identified k12education as the most important problem of his presidency.the third item on the 1986 list was scope and support. what will be the nature of computer science andtechnology in the 1990s? how can its health and vitality be sustained during a period of uncertainty andstringency in federal research and development budgets?i will discuss this issue in two parts also, beginning with nature and scope. it is typical of nrc boards to dostudies on the nature of their fields. some have become classics, such as the bromley report for physics1 and thepimentel report for chemistry. 2 these studies set the standard for other studies. since we were the ònew kids onthe block,ó cstb decided that beginning with a report on the nature of the field would be selfserving. we wantedfirst to build a record of reports dealing with critical national issues. computing the future, cstbõs study of thescope and direction of computer science and technology, was published in 1992. the committee was chaired byjuris hartmanis.the second part of this item asked, how can the fieldõs health and vitality be sustained during a period ofuncertainty and stringency in federal research and development budgets?oh, my prophetic soul! since there are people in this audience who have been grappling with this issue duringa period of uncertainty and stringency, i will not pursue it here.item 4 on the 1986 list was supercomputers. how can the power of supercomputers be exploited to promotescientific and technological advances, and how can u.s. leadership in this area be maintained?supercomputers continue to have economic and symbolic importance. at the time that i write this paper, amajor battle is under way to determine whether the national center for atmospheric research will purchase anamerican or a japanese supercomputer. the contract is valued at $13 million to $35 million; the amount ofinterest being generated must be due to more than just the amount of money involved. a teraflop computer willsoon be installed at sandia national laboratories. faster, more powerful machines are needed for aircraft designto ensure the safety and effectiveness of nuclear weapons with zero testing, for molecular dynamics in biology, andfor cosmological computations.the federal governmentõs interest in supercomputers has evolved since 1986. a program plan for highperformance computing was published by the office of science and technology policy (ostp) in 1989. thehigh performance computing act of 1991 authorized a fiveyear program known as the high performance1board on physics and astronomy, national research council. 1972. physics in perspective. national academy press, washington, d.c.2board on chemical sciences and technology, national research council. 1985. opportunities in chemistry. national academy press,washington, d.c.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.from infoware to infowar3computing and communications initiative (hpcci). in 1995, a cstb study, chaired by frederick brooks andivan sutherland, responded to a congressional request for an assessment of hpcci with a study titled evolving thehighperformance computing and communications initiative to support the nationõs information infrastructure.although high performance continues to be extremely important, there is less focus on supercomputers since theparallel processing paradigm has been accepted and higher performance has become pervasive.issue 5 was software. what can be done to promote the economical production of reliable software, whichrepresents a major portion of the cost and effort in the design and use of new computer systems?i was looking through my cstb files and found the following note from 1987: òsam fuller says bill gateshas really thought about software; he would be a good person to talk to the board.ó quite so, sam.in april 1988, cstb met at stanford university rather than in washington, d.c. edward feigenbaumarranged for john young, president and chief executive officer of hewlettpackard, to host a breakfast at whichleaders of silicon valley companies could tell us what national issues most affected them. attending the meetingwas the chairman of hambrecht and quist, one william perry. i was struck by the consensus among theseindustrial executives on the two most important issues. they identified k12 education because they wereconcerned about finding good employees, and they identified software because they recognized that the economical production of reliable software was a crucial problem for their businesses. this meeting underscored theboardõs concerns about software.the final item on the initial list was infrastructure. what are the important underlying capabilities, such asnational networks and electronic libraries, that are needed to support the healthy evolution of computing? howcan they be provided in a timely and effective fashion and integrated into daily activities to enhance nationalproductivity?the first two major studies the board published were toward a national research network in july 1988 andthe national challenge in computer science and technology in september 1988. the national challenge wasunique in that the study was done by the board rather than by a committee specifically appointed for the purpose.since it was a board report, we were all equally involved. however, michael dertouzos was far more equallyinvolved than the rest of us!  the national challenge presented two recommendations. here is the first in itsentirety:enhanced, nationwide computer networking should be seen as essential to maximizing the benefits in productivityand competitiveness that are created by computers. networking will facilitate the application and delivery ofdiverse advances in computer science and technology to the benefit of all segments of society. the board envisionsan enhanced national information networking capability, and it has already begun to examine a host of relatedquestions about how physically to improve data networking infrastructure; associated costs, impacts, and benefits;and the roles of industry, government, and other interested parties.seems rather prescient from the vantage point of 1996.during the past 10 years, infrastructure certainly became prominent in the boardõs portfolio. indeed, cstbwas initially the abbreviation for computer science and technology board. after the board was asked by frankpress, then the president of the nas and chairman of the nrc, to add telecommunications to its responsibilities,marjory blumenthal and i decided to rename it the computer science and telecommunications board to preservethe abbreviation.the boardõs list of critical issues in 1996 can be ascertained by looking at the list of current and futureinitiatives listed in cstbõs new brochure. you might want to compose your own list of the six most criticalnational issues in computing and telecommunications in 1996.in 1986, there was also a seventh topic on my mind. in 1985, i began to notice various ways in which theinformation infrastructure was vulnerable to electronic or physical attack. i imagined myself to be a terrorist or anenemy country, and targeted aspects of what today we would call òthe national information infrastructureó (nii).i did not go public with this concern because i feared it would do more harm than good. there is now considerableattention being paid to our vulnerability, especially by the department of defense and the media. however, i havestrong concerns, and i feel now is the time to express them. i will focus on the civilian infrastructure, although itis sometimes difficult to separate military from civilian in this domain.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.4defining a decade: envisioning cstbõs second 10 yearsit is just because the united states is the most advanced country in the world when it comes to the use ofinformation technologies that we are also the most vulnerable to attack. i will give you one illustration. financialsecurity is obviously very important to us as a people. we have two primary types of assets: real estate and whati will call virtual estate. i will confine my comments to virtual estate, which consists of bank accounts, equities,certificates of deposit, pension accounts, and so on. as you all know, this is òvirtualó estate because it is recordedin electrons. if you were a terrorist and wanted to do a great deal of damage to american institutions andindividuals, a natural target would be the virtual estate. although there are electronic backups and paper trails, iam not convinced that virtual estate is secure.then there are the electrons in the foreign exchange markets. according to the bank for internationalsettlement, turnover in this market is $1.2 trillion a day. this, of course, overwhelmingly dominates the value ofgoods moved around the world. furthermore, an amount of money that equals the annual gross national productof the united states moves through the foreign exchange markets in not much more than a weekñand it allconsists of electrons. these markets are international, but a successful attack could also have major domesticconsequences; the line between an international and a domestic attack has become blurred. this fading ofdistinctions is characteristic; we will see more examples later.our virtual estate is just one example of a potential target. others include the power grid, the air traffic controlsystem, and the communications system. i see the protection of information assets as a national security issue,although this view is not universally shared. in november 1993, i was one of seven civilians who participated ina seminar convened by andrew marshall, director, net assessment, in the office of the secretary of defense.marshall is a highly respected and influential pioneer in what is sometimes called revolution in military affairs(rma). mr. marshall organized the meeting because he believed that the world stands on the threshold of newinformation technologies that would have a profound effect on the u.s. military establishment. participating onthe military side was a group of general officers from all the services.i argued that there was a history of the military defending transportation assetsñsuch as rail lines, harbors,rivers, and airportsñduring wartime. should the armed services consider protecting the national informationinfrastructure? such a mission poses many questions and presents its own risks. it is further complicated by theblurring of wartime and peacetime activities that has already begun and will likely increase.there was a surprising split in views among the participants. a number of civilians, myself included, felt thatthe protection of civilian information assets might fall within the parameters of national security, whereas themilitary believed that responsibility for protecting these assets belongs in the realm of private industry and thepolice. military uses of infowar (iw), both defensive and offensive, are clearly the responsibility of the department of defense (dod). in january 1995, the secretary of defense established the iw executive board tofacilitate òthe development and achievement of national information warfare goals.ó3i want to focus here on protection of civilian assets, which, unfortunately, can be difficult to separate frommilitary assets. an enemy might attack the united states exactly by attacking the civilian infrastructure. this isjust an example of a more general tendency. in his book the transformation of war, martin van creveld arguesthat the lowintensity conflicts that have become the norm since world war ii will become far more prevalent andwill spread to developed countries, instead of being confined primarily to the third world. he writes: òas thespread of lowintensity conflict causes trinitarian structures to come tumbling down, strategy will focus onobliterating the existing line between those who fight and those who watch, pay, and suffer.ó by trinitarianstructures, van creveld means the division among the state, the military, and the people.4here are two specific examples of the difficulty in separating military assets from civilian assets. approximately 95 percent of all military communications are routed through commercial lines. we buy most of the chipsused in military systems from commercial vendors, many of whom are located in foreign countries. why might3to learn more, see berkowitz, b. 1995. òwarfare in the information age,ó issues in science and technology, fall, pp. 5966; andmolander, r.c., a.s. riddile, and p.a. wilson. 1996. strategic information warfare. national defense research institute, rand, santamonica, calif.4van crevald, m. 1991. the transformation of war. the free press, new york.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.from infoware to infowar5infowar be the weapon of choice of foreign or domestic terrorists, and of foreign countries, small and large? hereis a partial list of reasons:¥the united states is the only current superpower. as the gulf war showed, it is foolish to challenge us inconventional war.¥since we have the most advanced nii, it is the most vulnerable to attack. (of course, this would give us anadvantage in offensive iw.)¥the price of entering into iw is low.¥the learning curve is very steep.¥it might be difficult to detect who attacked us or even whether there was malicious intent.who should be in charge of protecting the civilian infrastructure against attack? i believe that there needs tobe strong government leadership and that it should be located in the executive branch. should the lead role beplayed by an existing entity, a combination of existing entities properly coordinated, or a government structurecreated for this purpose? this is an exceptionally complex and important question that i will not pursue here.it has been argued that the nii will acquire at least partial immunity due to repeated attacks, analogous to abiological organism. it has also been argued that the problem can be left to the private sector to solve. althoughi believe that the private sector has a very important role to play, i am not convinced it can do this on its own.coordination between the government and the private sector will be another difficult and important area toaddress.protecting ourselves against infowar may require the careful balancing of our desire for liberty and privacywith our wish for security. as the clipper chip illustrates, the conflicting demands of privacy, commerce, andsecurity can generate strong tensions. it is a particularly difficult issue for democracies, far more difficult than forcountries that do not place a high value on privacy and liberty.how imminent is a serious iw attack? how much time do we have to prepare? unlike most conventionalwarfare, in which there is a visible buildup of forces, an iw attack may come without warning. although weshould think carefully about how to meet the threat, i believe that time is of the essence. i suggest that no issue ismore important to the nation than the defense of our national information infrastructure.discussionjohn major: do you see a dichotomy between the militaryõs desire, on the one hand, to restrict the levelof security that private industry has access to, and on the other, to pass on to private industry the responsibility forprotecting its information from international attack?joseph traub: it is a minefield of difficulties. some of the major companies, such as at&t and mci,for example, say, òdo not worry, we can take care of it.ó you know how difficult it has been to conduct theboardõs study on encryption. i have no special wisdom on this subject, but i am sure there are people in the roomwho do.howard frank: i would like to react to the original question because it contains an assumption that is notnecessarily trueñthat the military has a desire to reduce or restrict the amount of protection afforded to the civilianeconomy and infrastructure. i do not think this is true. there has been an ongoing debate, which is certainly notuniform throughout dod, let alone among the major political leaders, about certain export policies and so on. ingeneral, there is no firm policy that says that the civilian sector should have less security or protection than dod.in the first place, i think the problem has been one of technology not responding to the needs because they arebasically invisible. we are more aware of this now. maybe you should have said something in 1986 or 1987,because it appears to me that this came out of the blue. for instance, if you look at the research community, it isdevoid of original ideas on this aspect of the problem. it is only now that people are beginning to think about whatwe might do as a nation in terms of a research agenda.second, the issue of privacy is one of the great perplexities in law enforcement. resolving this issue is goingto take quite a while, but it seems to be moving in the right direction across the government.third, the civilian sector, industry by industry, varies greatly in terms of what it believes the vulnerabilitiesdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.6defining a decade: envisioning cstbõs second 10 yearsare. in fact, i participated in an nrc study in 19891990 that looked at the security preparedness of the telephonesystem. when we began that study, there was vast disagreement about whether there were any vulnerabilitieswhatsoever. then, in the middle of the study, there was a major service disruption in hinsdale, illinois, and afailure in bedford, massachusetts, followed by another failure. so the reaction was, òoh, well, maybe we reallymade a mistake. maybe we are really vulnerable.óother parts of the infrastructure, like the power industry, have their heads buried in the sand; there is virtuallyno security at all. so this is a very complex question, and it is not a dichotomy. it is a continuum of issues, manyof which are economic. for instance, given the fact that we do not have a nationwide server for infrastructure, letus say telecommunications, no major trade can now be made that says this is in the national interest and thereforeyou must do itñthe way we could do in providing universal telephone service 50 years ago. this complicates theproblem a lot.sidney karin: i want to thank you for raising the information warfare issue. i think it is absolutelycritical. you said that determining who should be in charge is very complex. i think that answering the questionof whether anybody should be in charge is down the road. we have not gotten there yet.i think the threat is real. i think in some nontrivial way we are under attack today. there is lots of evidencethat security breaches have been taking place for the past several years. the major problem is that we, as a society,have not recognized that there is a threat, a real danger, and that the consequences could be quite serious.so i submit that the issue is not just a quibbling over civilian versus military security. although i have a verystrong position on this issue, i will not get into it at the moment. the issue is that, as a society, we have not agreedthat security is a serious problem. until we agree on this, there will not be any solutionsñno matter how hardanybody works at trying to implement anything, no matter what structures are imposed by anybody to deal with it.the first thing that needs to be done is to raise everybodyõs consciousness that something bad could happen,and that there are people trying to make it happen for various reasons. i commend you for raising the issue.butler lampson: the basic fact about security is that it is expensive; it is a pain in the neck; and peopleare going to implement it only in response to convincing evidence that there is a problem. the only way you aregoing to get that evidence is when someone you think is in a position similar to yours gets badly hurt.in my view, all this discussion about what we ought to do and how much we ought to worry about the threatis entirely beside the point because nobody is actually going to do anything until there is some serious damagedone. the fact is that no serious damage has been done. it is fine for us to think about how we might respond oncethe motivation is there to respond, but i think trying to raise the level of motivation is completely pointless.karin: what would constitute evidence in your judgment that somebody has been badly hurt? what wouldit take for your organization to decide that one of its competitors was badly hurt? what is a hypothetical incidentthat would convince you?lampson: something that happens that costs you a lot of money. bankruptcy would do it. that woulddefinitely have an impact.karin: how large a company would have to go bankrupt before your company would take notice, recognizethere is a serious threat, and decide to change its behavior?lampson: how large a company would have to go bankrupt? i do not know. it is an interesting question.since no companies have gone bankrupt yet, right now the question is academic. if you believe that somecompanies have gone bankrupt as a result of information warfare and you want to promote action, then i stronglyurge you to find out who those companies are and to publicize the situation clearly. this will have far more effectthan anything else that you or joe or anyone else might say.robert bonometti: i would like to make the suggestion that the problem actually is more extensivethan just a deliberate, determined attack against the infrastructure. as society becomes more dependent on thisinfrastructure, natural disasters also become of great concern because the social fabric will be ripped apart andbecome dysfunctional in the aftermath of a disaster.there are examples we can look at, such as the aftermath of hurricane andrew in florida, that provide somelessons learned. one very trivial but interesting fact is that people were unable to get cash because everyone wasso dependent on atms (automatic teller machines). after the hurricane the infrastructure was not there, and it wasa problem for some time.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.from infoware to infowar7andre van tilborg: i wonder if you might comment on how much of the security problem might berelated to not having the analogue of something like building codes for information and telecommunicationssystems, versus how much really requires deep, new insights and research. if you had, for instance, a nationalelectrical codeñnot for electricity or electrical appliances or underwriters laboratory, but for computing systemsand telecommunications systemsñyou might be able to cover a large fraction of the troublesome areas in ensuringthat your systems remain stable and work. you would still have that part where a very determined adversary canget through, even though you have a good building code. i wonder what your thoughts on that might be.traub: i think what you are suggesting is standards. it has been suggested that there be a core communications system in case of a national emergency, and there has been some discussion about that. the things we havewith which to protect our homes will at least keep out the amateurs, although they will not keep out a professional,determined burglar. the big problem is that everything is changing so quickly that this is an almost impossiblearea, i believe, to standardize. standardization, it seems to me, requires a certain maturity and a certain stability.i am not sure we can do that in this area.robert kahn: the good news is that the notion of the national information infrastructure is in the publicconsciousness. the bad news is that we really do not know what it is or might not recognize it if we saw it. tosome, the nii is 500 channels of cable tv and to others it is the library of congress on every desktopñin someways very mutually incompatible goals.some people think we have always had an information infrastructure, or at least maybe since 1844 when thetelegraph was invented, if you want to focus on electrons. others think we clearly have it now. still others willwake up in 10 years and be totally shocked that we do not have an infrastructure yet and have been talking aboutit all these years.it seems to me that the one thing that has really been missing, apart from understanding what it is, is anynotion about getting coherence among all the pieces, so that the infrastructure really becomes the mechanism tolower the barriers to productivity in a broad sense. i do not think we are there yet. the big objective over the nextdecade, perhaps many decades, is trying to figure out how to achieve interoperability to lower these barriers. bygetting coherence in the system, you make it more of a target for the kind of information warfare that you aretalking about.so my question to you is, how do we go about designing this coherence for interoperability into the system,while at the same time worrying about protecting against the kind of information warfare you are discussing as asocial process in this country?traub: that is a very good question, bob. i am sorry, but my time is up.william wulf: let me just make a couple of comments. first of all, the balancing of privacy and societalprotection was mentioned. probably the most sensitive report that cstb has ever undertaken is going to bereleased imminently, and it is one that was requested by congress.5 it addresses national cryptography policy. itwill be a completely unclassified report. this fact is very, very important. there is not going to be a classifiedannex to the report. we wanted the report to be completely unclassified.my second comment has to do with the national information infrastructure and natural disasters. we absolutely agree with dr. bonamettiõs remarks. if you look in the cstb brochure, you will see that one of cstbõs1996 studies (computing and communications in the extreme: research for crisis management and otherapplications) is looking at how we can use information technology to save lives and property. in his early remarks, joe pointed out that, from the outset, the board has been concerned with both technologyand policy issues. if anythingñand this is a personal perceptionñthe increasing recognition among people inboth the executive and the legislative branches of the relevance of information technology to virtually everyproblem that the country faces has reinforced the correctness of that original decision.5computer science and telecommunications board, national research council. 1996. cryptographyõs role in securing the informationinfrastructure. national academy press, washington, d.c.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.8michael r. nelsoni am mike nelson from the white house office of science and technology policy. i am one of the twopeople at the white house who work fulltime on information technology policy issues. i think it is probably thesecond most enjoyable job at the white house, after the presidentõs. he has the advantage of not having to reportto anybody, except to the american people.i am actually a geophysicist by training, and it is interesting that i have ended up where i am. i came towashington from the massachusetts institute of technology about eight years ago on a oneyear fellowship.apparently i contracted òpotomac fever.ó i hope it is not terminal. it might seem odd that a geophysicist wouldbe working in this area, but as i have told many of you, the training is actually perfect. having a sense of geologictime in washington is very important.my career in washington has nearly paralleled the existence of the computer science and telecommunications board (cstb) in terms of time. we have had a very interesting, fruitful, and i would say symbiotic,relationship. in washington, you must have a good mentor if you are going to accomplish anything. i have beenblessed with several, and several of them are on the board. in working on information technology issues duringthe past eight years, i have worked very closely with the board. i probably have relied on cstbõs reports as muchas anyone and have been able to accomplish a lot more because of the information provided to me by individualboard members and the reports that cstb has produced.when i first came to washington, i started working on earthquake issues. i spent two or three months doingthis and realized that these were really hard issues, and they were very depressing. i would start my morning byreading scenarios of a major earthquake in los angeles killing 20,000 people. so i started working on globalwarming issues, where i read scenarios in which 2 million people would die. then i decided it was much more funto work on information technology issues: we all know that information technology will solve all of our problems;so this is where i now spend all my time.the first hearing i organized for senator gore, then chairman of the science, technology, and space subcommittee in the senate, was on computer technology and highspeed networks. robert kahn, among others, testified.leonard kleinrock was our lead witness, testifying on cstbõs second report, toward a national researchnetwork (1988). it was a very influential, very important report. i know it was influential because i took largeportions and inserted them directly into the briefing memo, which was given to all the senators who came to the2linking the cstb community to the federalgovernment: expert advice for policymakersmichael r. nelson, moderatordavid b. nelsonpaul r. younghoward frankdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.hearings. i also inserted parts of that report directly into the legislation that i drafted for senator gore. luckily,plagiarism is legal on the hill! after that first hearing, several people were astonished at senator goreõs grasp ofboth the technology and the issues. this was due in part to the fact that he had read the report, which was releasedon the same day.since that time, i have worked with the board on a number of critical issues. cstb has helped us design thehighperformance computing legislation and keep the high performance computing and communications initiative on track. it has helped us deal with issues such as computer security. the board has helped us evolve thearpanet into the nsfnet, into the national research and education network, into the internet, into the net,into the national information infrastructure, into the global information infrastructure, and into whatever it is weare now creating.today, we are going to look at how the board has influenced policy and how it has worked effectively on theinterface between science and engineering and policy making. being one of the denizens of the interface, i knowit is a pretty turbulent, unpredictable place to work. i know that this board has been very effective in informingand enlightening those of us who have tried to help the policymaking process along. today we have an excellentpanel, and i should say that i am impressed with the quality of the entire symposium program.this panel is going to take a dickensian approach to its presentation by looking at cstbõs past, present, andfuture. the òspirit of cstb pastó will be provided by david nelson (no relation), who has been working in thisarea even longer than i have and has been one of cstbõs primary customers. the òspirit of cstb presentó willbe provided by paul young, who will talk about what is going on now and some of the ways in which the boardis influencing policy making. the òspirit of cstb futureó will be represented by howard frank, since the defenseadvanced research projects agency (darpa) is always 10 years ahead of the rest of us. he will discuss whatmight be ahead for the board. we are going to keep these remarks very short, about five minutes each, so that wecan have a full discussion. all of the panelists can talk about the past, present, and future, and they will do so inthe questionandanswer period. our goal is to provide a chronological look at where the board has been, whereit is now, and what directions it might take in the future.david b. nelsoni will cover cstb past. this does feel a little bit like a role in charles dickensõs a christmas carol. as i waspreparing this, i thought, òi am putting myself in the role of a peer reviewer.ó this reminded me of a story thati think most of you know, but it is so good that i will repeat it. this is the classic peer reviewerõs report. it readsas follows: òthis paper is novel, interesting, and correct. unfortunately, the part that is novel is not interesting.the part that is interesting is not correct. the part that is correct is not novel.ó fortunately, i do not make thatjudgment when it comes to the work of cstb.before i review some of the boardõs efforts from a government standpoint, let me remind you what the federalscene was like in 19861987. the lax report1 had been out for only a few years, and the various agencies werestruggling to implement its recommendations. i viewed that as an attempt to cure the vax disease. what i meanis that every chemistry department could afford a vax computer; so the standard of excellence in chemistrycomputation was one vax unit.next, darpa had just begun its strategic computing initiative. robert kahn smiles from the audienceñheremembers that one well. the national science foundation (nsf) was dealing with the question of establishingsupercomputer centers, particularly how to link them together in something that would be called nsfnet. toremind you how history is out of our control, remember the study that said that the nsfnet should run opensystems interconnection (osi) protocols as soon as practical. think back. there is a lot of water under the bridgeon that one.1report of the panel on large scale computing in science and engineering, peter lax, chairman, sponsored by the u.s. department ofdefense and the national science foundation, in cooperation with the u.s. department of energy and the national aeronautics and spaceadministration, washington, d.c., december 26, 1992. linking the cstb community to the federal government9defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.10defining a decade: envisioning cstbõs second 10 yearsthe department of energy (doe) had responded to the lax report by consolidating and extending all of itsenergy research computing, using what is now called the national energy research supercomputer center, andhad just begun the esnet that went with it. we were struggling with the question of what protocol to run. at justabout that time, cisco came along with multiprotocol routers that allowed us to sidestep this terrible politicalquestionñanother instance in which technology saves you from your political fate.the national aeronautics and space administration (nasa) was planning the national aeronautical simulator, the numerical wind tunnel. a lot of universities were trying to figure out how their budgets could affordadvanced computing. as we know, some could and some could not. on the hill, as michael nelson has noted,senator albert gore was thinking rather deeply about networking and about the things that networking could dofor the countryõs future.in the interagency setting, the department of defense (dod), doe, nsf, and nasa were working on thebeginnings of what was then called the high performance computing initiative. in the 1987 white house officeof science and technology policy (ostp) report, which is less well known than the 1989 report, it was referred toas highperformance computing. i believe it was allan bromley who saidñas the 1989 report was being preparedto launch the hpc initiativeñòshouldnõt there be a second c? isnõt communications becoming pretty important?ó as a result of such suggestions, it became known as the high performance computing and communications initiative (hpcci).with this introduction, let me review some of cstbõs work. in terms of quantity, cstb has been veryproductiveñmore than 40 projects since 1987; approximately three to five substantial products per year, includingone major study each year. some of the substantial products included influential colloquia as well as longtermstudies.next, cstb has usually been ahead of the issues. i think joseph traub already mentioned this fact, and iwould certainly agree. if we run very quickly through some of the products, we will see that it is true. as early as1988, in toward a national research network, cstb addressed issues in the gore bill and discussed the value ofthe research network.among cstbõs reports, there are several that resonated with me and that spoke particularly to policy issues.many of you probably have a different list, so this is not intended to be complete or in any way a judgment aboutthe ones that are not on my list.in computers at risk: safe computing in the information age (1991), cstb addressed the question ofhackers and penetration. it did so in a somewhat lowkeyed way because it considered system design and accidentsto be as important as hackers. i would say that these are still issues. many more people are killed in accidents thanin murders and wars. so, while we worry about malicious attacks, our own systems are fairly fragile. computersat risk pointed out that inadvertent problems can bring things down as much as malicious penetration.keeping the u.s. computer industry competitive was a series of reports (issued in 1990, 1992, and 1995) thatconveyed initially a sense of doom and gloom about the computer and electronics industry. the reports did notspeak particularly to government programs, although one of the major objectives of the hpcci was to keep theu.s. computer industry competitive.intellectual property issues in software was published in 1991. a house bill is in committee now onintellectual property issues in electronic media, which indicates that this issue does not go away. it also reaffirmsthe contribution of cstb in pointing out that we have patents and we have copyrights, and maybe software has tofit in somewhere either between or beyond them.computing the future: a broader agenda for computer science and engineering was released in 1992. wein government thought it was very helpful that the reportõs first recommendation was to continue to support hpcc.it also spoke to academics, saying that they should broaden academic computer science and engineering. prophetically, the report recommended broadening hpcc to mission agencies, and this certainly happened. in 19921993,several additional agencies signed on to the hpcci: the national institute for standards and technology (nist),national institutes of health (nih), national oceanographic and atmospheric administration (noaa), nationalsecurity agency (nsa), and environmental protection agency (epa) are examples.national collaboratories: applying information technology for scientific research (1993)ñi think williamwulf can claim a great deal of personal credit for this report. again, it set a direction that we are now followingdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved. linking the cstb community to the federal government11within agency programs and in the country as a whole. we are trying hard to follow many of the lessons coveredin the report.information technology in the service society: a twentyfirst century lever (1993) bemoaned the problemsof productivity and the absence of productivity improvements that could be traced directly to computing advances.we are still struggling with this issue. how do we even count, much less affect?then, in 1994, realizing the information future: the internet and beyond revisited the first report that cstbhad done about the internet, going back and saying, òokay, wow, this was really successful. now what do we dowith it?ó one of the things this report emphasized was the importance of the concept of the òbearer serviceó toallow a broad range of applications to be served by a hardware infrastructure.research recommendations to facilitate distributed work, released in 1994, is particularly close to my heartbecause the department of energy commissioned it. it provided doe with a good research agenda for how topromote telecommuting, which is, of course, becoming more and more ubiquitous. doe cares because we areworried about energy usage.evolving the hpcc initiative to support the nationõs information infrastructure, the famous brookssutherland report, was released in january 1995. out in the anteroom of this meeting is the chart from that report(reprinted here as figure 2.1) showing how research has affected practice and the long time frame required forsome of it. these were helpful lessons for those who thought that the research we do today should be in productstomorrow, and if it is not, it has somehow failed. the report also says, òdo not put all the computer science eggsin the hpcc basket,ó as well as, òadjust what you call hpcc so that you keep the books properly.óalso in 1995, information technology for manufacturing did an excellent job of laying out a research agendato address how computing can help on the factory floor. the report is, in a sense, a companion to informationtechnology in the service society; however, it argues that because the shop floor allows better productivitymeasurement, it is going to be easier to show that computing has actually helped.finally, i would say that the work of cstb past, as i have gone through these reports, has had a substantialinfluence on the administration, on congress, and on the research and development community. my little tourthrough these reports helped me to go back and support that statement by asking: well, what was really happeningin 1986 and 1988 and 1992 and so on? what were the important issues? did cstb address them in a productiveway? did its recommendations help?paul r. youngi was very interested in david nelsonõs introduction, particularly vis ‹ vis nsf in the past (see figure 2.2). hepointed out that 10 years ago we were struggling with what to do with supercomputing and networking, establishing the nsfnet. i arrived late this morning because i gave some introductory comments to the panel that is talkingabout the new supercomputer centersõ program, the partnerships for advanced computational infrastructure.things from the past continue in revised forms. i think a lot of cstbõs reports continue to influence the futureand what we are doing in the present.i was also amused because david mentioned networking, and we are in the middle of a major change in thensf networking program as we head to highend networking. as he pointed out, this program has been influencedby the kleinrockclark report (realizing the information future: the internet and beyond), which has had a verystrong impact not only on policy issues, but also on how we set a research agenda.finally, david mentioned that, 10 years ago, we were struggling with the issue of what would become of someinitiatives in computing and communications. in 1991, a bill was passed on highperformance computing andcommunications that goes through 1996 and expires at the end of this fiscal year. the administration and theinteragency process struggle with how to redefine this concept and with identifying the future role of federalinvestments in research in computing, communications, and information.so in some sense, policies evolveñthey do not change. cstbõs reports help us to define these policies and todecide what we are going to do about them. in this context, i was tempted not to discuss the national context inwhich some of these decisions are made. however, in talking with anita jones last night, she suggested somedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.12defining a decade: envisioning cstbõs second 10 yearsfigure 2.1governmentsponsored computing research and development stimulates creation of innovative ideas andindustries. dates apply to horizontal bars, but not to arrows showing transfer of ideas and people. reprinted from computerscience and telecommunications board, national research council. 1995. evolving the high performance computing andcommunications initiative to support the nationõs information infrastructure. national academy press, washington, d.c.,figure 1.2.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved. linking the cstb community to the federal government13things that might be useful. michael nelson spoke in his introduction about his experience with geological timeand how things get done in washington. there are some reasons for this.on the administration side of how policy is made, there are a lot of players, and nsf often sits in the middle.there is an office of science and technology policy, a national science and technology council that worksthrough the committee on information and communications, an hpcc process, and a lot of different subareas thatdefine research agendas. on the other side, through the department of commerce, there is the informationinfrastructure task force that addresses information technology implementation issues. somehow, the advice thatcstb provides has to influence and pull all of these together into a common framework.there is a similar method of arriving at priorities in the national science foundation that is unique to nsf.you should not think, of course, that cstbõs audience is just those of us who have the immediate responsibilityfor budgets. there are a lot of people to please: the national science board, an advisory committee, workshopsthat we hold, proposal pressure, and peer review.nevertheless, it is clear that external studies are important to us. peer review is important, and the mostprestigious of these reviews come from the computer science and telecommunications board. figure 2.3 addresses some issues that i think are important from an nsf standpoint. they provide some indication of placeswhere we might be looking for help from cstb, without saying exactly how these needs might arise.first, we continue to redefine the research agenda within nsf, but we do this across the federal governmentas well. i am sure howard frank will say more about this process. cstb really helps us to formulate this agendaand explain it to the various stakeholders. some issues that nsf is particularly concerned with include thefollowing: how can we do more interdisciplinary research, and what is the role of computer science and engineering, including computational science and engineering, in that? what is the proper home? what is the relationshipbetween computer science and engineering and computational science and engineering?within nsf, we are currently interested in crosscutting research that addresses human factors. people fromdifferent disciplinesñbiology, cognitive science, and engineeringñ are addressing questions such as, what doesit mean to learn? what does intelligence mean? what is the distributed nature of this? not all of this discussionbears directly on information technology, but much of it does, and we are trying to identify the relationships.we are interested in part of the current presidential initiative on learning technologies, a pet project of mine.figure 2.2nsf issues: then and now.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.14defining a decade: envisioning cstbõs second 10 yearswe all know that information technology is going to radically transform education in the twentyfirst century, buthow? do we have a road map for this? let me provide two particular examples. we know that computationalmethods have transformed how we do much of science and engineering. computation has joined experimentationand theory as a paradigm for many aspects of how we do science and engineering. when the technology becomescheap enough and available, it can similarly transform how students learn. it will change the way they think aboutabstract problems. do we know what the technical roadblocks are, and do we know what the psychologicalimpacts are? could cstb help us address these kinds of questions? i think so.i would like to come back to collaboratories, an area that william wulf has pushed and how they can helpscience and engineering generally. we are going to have some form of collaboratories across geographic boundaries that will influence education. are these going to be the same technologies? are children going to use thesein the same way as scientists and engineers? what are the technical roadblocks to achieving this by the year 2010?cstb could help nsf answer this kind of question.you see figure 1.2 of the brookssutherland report again and again (figure 2.1 in this volume). you knowthis figure, you love this figure; i know this figure, i love it. it helps to set policy because it shows a very nonlinearpicture of the role of federally funded research in the economy and the nation in general. i have found this slideenormously useful. it enables us to talk with policy makers and the general public about the fact that research hasa long life. it is still goal directed. it enables us to talk about the fact that the process is nonlinear and that theparticular goals may be diffuse in the interaction. this figure has been very effective.cstb has another study, òinnovations in computing and communications: lessons from history,ó currentlyunder way. we are looking forward to the results of this study in particular because the government increasingly ismoving toward performancebased budgeting. cstb can help us with this. the initial idea for performancebasedbudgeting for research across the federal government was that you were supposed to indicate what the output of aparticular program would be in the next two to three years. of course you could measure the number of publications,but this is not what you are after. you could measure impact. can you predict this in advance? i do not think so.figure 2.3illustrative impacts of cstb studies. note: nstc = national science and technology council, cic= committee on information and computing, hpccit = high performance computing, communications, and information technology (a subcommittee of cic).defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved. linking the cstb community to the federal government15we need more help in selling the research mission to the public as a whole, to congress, and to people downthe street. i think cstb can help us with that.howard franki have been involved with cstb and the earlier telecommunications board for quite a few years. having beenin government, i have been involved in two or three different ways. when mike asked me to speak about whatcstb should be doing in the future, at first i was taken aback a little because, well, how do i know? then idecided that the key to understanding what cstb should be doing is to understand what is happening in information technology in general.what i will do is give you a picture of what my personal feeling is. this is not darpaõs position or anybodyelseõs, but what i think the challenges of the future are. if you looked at the list of topics that david nelson hastalked about, cstb has been involved in all of the real issues of the past and presentñright on top of themñsometimes a year or two before the fact, sometimes a year or two after. the history of cstb has tracked thehistory of information technology.so i thought i would tell you where i think the problems are. i will point out that there is an anomaly here. weare living through the greatest revolution in information technology that the world has ever seen. the economy isrobust. the technology sector is leading the stock market. in the last week there was probably a 5 percent rise inthe stock market led by the technology shares, and so on. nevertheless, i think the future may be rather dim. ithink the first challenge is that, in this nation, computer science is basically an obsolete field. many things thatcomputer scientists have worked on have been wonderful and great. however, some critically important thingshave been missed.it was good that we had an introduction that talked about information warfare. when i first became directorof the computing systems technology office in darpa and began talking to my staff about survivability of theinfrastructure, i got the response, òwell, we have already solved that problem. the internet does adaptive routing.ói said, òwait a second, you do not really know what i mean.ó a year later, we tried to engage the academiccommunity on the topic. we discovered that there was no academic community engaged in the topic.this is just one example of the fact that computer scientists have become smug in the wonderfulness of thetechnology that existsñtechnology that has been created, by and large, in spite of them rather than because ofthem. it does not mean that there have not been great things that have come out of the academic community, butthe academic community certainly has not led.longterm information system technology research in this nation is in great danger. this is an area where wehave to cry for help right now. over the past five years, there has been a collapse of longterm research anddevelopment in the industrial sector. nobody knows exactly how big longterm research really is in the industrialsector, because when companies budget research and development, it may look like $20 billion, but the development part may be $17 billion, $18 billion, or $19 billion.we know that the bell labs of the past is gone. there is tumbleweed blowing down the halls of ibmõsresearch facilities and in many of the other major industrial organizations as well. in the federal government, weare under tremendous pressureñthere is vast misunderstanding of the relationship of longterm research anddevelopment to the nationõs prosperity and future. this is the same problem that cstb was looking at 10 yearsago, except now it is worse. we probably will not know the results of it for another decade or more.highend strategic computing is in danger of collapse. we have seen great technological success in terms ofthe highperformance computing program itself. it introduced the concepts of parallelism and scalability into thecommercial world. if you look at mediumscale computing, it now reflects the results of the government program.if you look at the very high end, however, the market is in danger of disappearing. the very high end of strategiccomputing is a government marketplace, not a commercial marketplace, but there is no government researchprogram that yet recognizes that fact. there is tremendous pressure from congress and from our own internalconstituencies to cut back on what had been considered highend strategic computing for homogeneous computingsystems. we really do not have a strategy for how to continue to acquire computers from a marketplace that cannotdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.16defining a decade: envisioning cstbõs second 10 yearsafford to spend the r&d dollars necessary for the highend computation required for many governmentuniqueproblems.finally, and this is an anomaly, we celebrate the wonderfulness of the internet when i believe it is now movinginto a period of decline in performance. the vast social phenomenon arising from the internet is yet to take place.we see just the beginnings of this phenomenon. yet yesterday, david nelson presented some initial results aboutperformance on the internet for the research community. we need another study called toward a nationalresearch network because we no longer have an adequate national research network. the exploding userpopulation has reduced performance below the levels needed by the research community.what are the implications of all of this? i think cstb is one of the only places on earth that can deal with thisclass of problem. it has the only group of people who understand not only the technology issues, but also some ofthe social and political issues. cstb needs to become much more proactive. i want to use one specific example.this was not cstb, but it was the national research council (nrc). when i was on the study committee thatlooked at the survivability of the u.s. telecommunications system, we came to some pretty significant conclusionsin 1988, 1989, and 1990. we decided to entitle our report the emerging crisis in the telecommunications systembecause we felt that this is what it was. the report ended up by being called the growing vulnerability of thepublic switched networks. that is my point. cstb needs to become a much more proactive organization that notonly helps give insight into policy, but actually helps to beat people on the head until policy is changed.discussionmichael nelson: before we go on, i want to add one thing that i forgot to mention. even though this isthe national academy of sciences and the national research council, cstb is having a growing impact internationally. i meet with a lot of people from other countries who want to know how to build up the internet in theircountry. they are learning about resources that cstb has provided, in part because of marjory blumenthalõsefforts, because cstb is now up on the web and because every time i travel overseas i take approximately 30pounds of your books and hand them out. i used to take only the little red book, realizing the information future(1994). now i have to take that and the unpredictable certainty (1996). in the past six months, i have handedout dozens of copies in russia, beijing, singapore, and jakarta. they have been read, and they have influenced alot of thinking. so i commend you for this effort, and i hope that you will continue to reach out.edward feigenbaum: cstb operates in the framework of the commissions and councils, and so on, ofthe nrc, which is in the framework of the national academy of sciences. all are extremely slow and conservative organizations, unwilling to say things that make anyone bristle. so a lot of what cstb might try to do is eithersquashed or squashed in advance by this elaborate structure. i want to point out how long it takes to get a cstb report out. it takes forever. bob lucky has a commentin the published abstracts about the fact that we are zero for one on predicting webs. the world wide web wasinvented and mosaic was invented and had 1 million usersñall within the time frame of one cstb report.michael nelson: i would like to second that. being in the policymaking process, i always want theanswer tomorrow, if not yesterday. the only counter to this argument has been that cstb has been ahead of thecurve in identifying the issue that will be hot in two years. this is good, because it takes two years to write thereport. i hope the process can move faster. part of this is our own governmentõs problem. the encryption study,which is coming out soon, has now completed the nrc review process. it would have been nice to have the reportsix months ago, but it is going to be incredibly timely. in a way, the need for that report is growing, and i think itis going to influence decisions that are just about ready to be made, so the report is right on target. some otherreports have been a little late, but i think the nrc, and especially cstb because it anticipates, has done better thansome other organizations in washington.i should add, however, that we are losing some of the other organizations that have supplemented the work ofthe board. the congressional office of technology assessment has disappeared. the annenberg program fromnorthwestern, which has done a lot of work on telecommunications policy, has been phased out. so the demandfor the products of this board, and the demands put on it, will be even greater.any other reaction to that comment?defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved. linking the cstb community to the federal government17paul young: i am very sympathetic to the time issue. it would be very nice to cut the time of producingreports in half. this said, there is a lot of policy made in washington by the close of business today, and there isdefinitely a place for a group that can take a deep, measured approach and think things through very carefully. ithink that this really has to be preserved while one speeds up the process because it is very easy to shoot from thehip. you see a lot of it.michael nelson: i should also say that there are reports that, although they are toned down a little bit,can still deliver a very powerful message if they are delivered personally by the people who helped write them.the other thing i would like to see more of is taking these reports and really pushing them in the policy process.people who get the reports often do not read them until somebody comes in the room and says, òhere it is. thisis why you have to read chapter five.ó this, i think, is something marjory and the team have been doing morefrequently. many of you have been in my office; you have been in a lot of offices. i think the broader communityneeds to take these messages and get them to policy makers.michael dertouzos: i am not sure that i heard what i heard from howard frank. did i hear sort of anostradamus thing? i would like to perhaps profoundly disagree or agree because i am not sure i got what wassaid.i agree our field has become narrow. i see tremendous opportunities ahead. some predict there will be 1billion interconnected machines by 2005. i see 15,000 independent software vendor artifacts going for thosemachines. i see the entire theory of computer science moving away from the single machine and addressing whathappens out there when you have billions of machines. we do not have turing theories for this. we do not havesystems theories for it. we do not have software for it. we do not have systems for it. all this has to happen. wetried doing artificial intelligence things and they did not pan out. this does not mean that they are wrong. it took250 years to progress from steam to jet power, and in computing we have had only 35 to 40 years.howard frank: mike, i think you are violently agreeing. the opportunities are there.dertouzos: but let me have my tantrum. i think there is just a wonderful world ahead, and i am certainlyexcited about it.frank: there is a fantastic world ahead. it would be nice if we had some of the theory now.david nelson: this is the perennial discussion, and it has taken place in mathematics and other fields.my personal view is that it helps to have very good academics working in the field, rubbing shoulders with thosewho are trying to apply it. in the department of energy, we are bringing together mathematics and computerscience in jointly funded projects that are trying to do applications. there is, of course, a danger that you maydegrade the quality of the research, and i keep my finger on it. i keep asking people who are managing thoseprograms, and the answer i am getting is, no. this is invigorating and stimulating.william wulf: i would like to say just one thing about the issue of the time it takes cstb to completereports. i think this is a serious issue, and i keep pushing the staff about it. part of the problem is internal to thenrc, and that is the part we have the potential to do something about. however, delays frequently occur at thefront end. sometimes cstb does not get a contract for a long time. this has been a problem on the ada study,where we are trying to do a fast turnaround.2 so i am going to put in a plug for why it is so important that agencieslike yours have provided sustaining fundsñcore funds, as they are calledñbecause those sometimes allow cstbto get a quick start on projects.2computer science and telecommunications board, national research council. 1996. ada and beyond: software policies for thedepartment of defense. national academy press, washington, d.c.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.18the global diffusion of computingtwenty years ago, computing was very much an american domain. a handful of american companiesmanufactured most of the computers in the world, and annual machine production of the biggest companiesmeasured in the tens of thousands. most of this equipment stayed in the united states, and little of the rest movedoutside of industrial countries, most of which were american allies. foreign manufacturers (mainly in westerneurope, japan, and the former warsaw pact nations) paled in comparison. the arpanet had about a hundredhosts, with minimal connectivity with england and norway, and no comparable network existed outside theunited states. computers were sparsely scattered outside the first and second world nations.todayõs computing world is profoundly changed. global production of microprocessors and computersamounts to tens of millions of dollars annually. capable manufacturers exist in many places around the world,including several nations that were technologically backward in the early days of computing. the united statesitself imports a lot of computing equipment, mostly from east asia and from the foreign manufacturing sites ofamerican companies. international sales of many important american companies amount to about 50 percent oftotal sales. the arpanet has evolved into the internet, which has about 10 million hosts. the nonu.s. shareof these hosts exceeded 50 percent for the first time in 1995 (the foreign share of computers in use passed thatmilestone about 1990). more than 170 countries have some worldwide network connectivity, including third andfourth world countries such as bangladesh, mongolia, mozambique, peru, and romania. today, there are abouthalf as many computers on the planet as there are cars, trucks, and buses.more generally, information technology (it) has become closely associated with modernization, with beingpart of the global economy, andñin one form or anotherñis considered necessary or highly desirable almosteverywhere. according to some, it will help bring the world together, paving the way for economic developmentand even sweeping aside the traditional borders (and governments) that divide people. (we might also note thatthe global diffusion of it is hardly uniform and that, in some ways, it is causing increased stratification.)3the global diffusion of computing:issues in development and policyseymour e. goodmannote: l. t. greenberg also contributed to the content of this presentation.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.players on a much expanded fieldthe united states has had an enormous role in stimulating and enabling this change. american contributionsinclude cheap and powerful hardware; great quantities of (often òfreeó) software; networking; american buyingand importing habits (which recently include a trade deficit in it products); the òmeccaó of the u.s. technicaleducation community; and the fruits of military and space research and competition (most notably, from a globalstandpoint, the internet and global positioning system). the united states still comprises about 40 percent of theworldõs computing market.although its relative position has declined, the united states still has almost as much computing as the rest ofthe world together. this country is now the most important player in a much bigger game. as such, the unitedstates is more prominent, accomplished, and extensively involved in computing, and more people in the rest of theworld have direct contact with the united states than ever before. thus, more foreigners than ever before careabout what is done by the united states.as important as the united states is in global it, there are also many other players. some are already quitesubstantial and others are becoming so. these players are not just governments of countries that now have aserious presence in the global it scene and are not just foreign it companies. rather, the spread and application ofit has been such that many nongovernmental, nonit industry stakeholders have emerged or have been strengthened. their involvement results from their interests in the applications of it (including its òapplicationó as theirexports), and they are empowered because of their wealth, constituencies, and newfound voices (often enabled byit). the interests and the numbers of these actors also create new, or exacerbate old, conflicts and issues. thesestakeholders cover a much broader spectrum of business, academic, nongovernmental organization, and privatecitizen actors than ever. there is even a larger set of stakeholders within governments. furthermore, the globaldiffusion of it has weakened some older stakeholders, and they have not all taken their diminution quietly.to illustrate how international changes in value conflicts and stakeholders have affected a major technologicalpolicy issue over the past 10 years, consider u.s. national security and foreign policy export control. this isarguably the earliest, longestlived, computerrelated u.s. public policy issue, and one with an enormous footprintacross the u.s. government. export control was the subject of one of the first major computer science andtelecommunications board (cstb) studies,1 as well as one of its most recently completed major studies. welimit this discussion to export controls on computers, particularly highperformance computers, but a similar shortanalysis might also be made with respect to the control of encryption.first and foremost, what has changed in the world is that with the demise of the soviet union, the peer threatpursuing a full spectrum of advanced military systems has all but disappeared. this is not to say that there are nosignificant threats to the united states from foreign sources who may benefit from enhanced computing capabilities, but these threats are not in the same league as the earlier possibilities for global superpower conflict. takentogether with the realities of technological progress and diffusion, this means that advocates of controls find itharder to justify the continuation of controls on the basis of specific applications that potential enemies might beable to pursue using computer products that might still be effectively controlled.to appreciate how difficult effective control has become, we note that in early 1993, a raw computingperformance threshold of 195 mtops (millions of theoretical operations per second; since appropriately revisedupward twice by the u.s. government) was used to define a control regime for òsupercomputers.ó today, millionsof microprocessors are produced each year with performance levels of about half to four times that value. thereis now so much madeintheu.s.a. computing in so many parts of the world that it is unfortunately inevitable, butessentially unpreventable, that some of it will be used against u.s. national security interests.furthermore, there is now a greater division of value judgments within the national security group of stakeholders. for example, some department of defense beneficiaries of cots (commercial, offtheshelf) highperformance computing (hpc) see more benefit to what they do and to u.s. national security from a strong,technologically vigorous, domestic industry than from continued export controls.the global diffusion of computing191computer science and technology board, national research council. 1988. global trends in computer technology and their impacton export control. national academy press, washington, d.c.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.20defining a decade: envisioning cstbõs second 10 yearsthe extent to which foreign governments, even those of our closest and former coordinating committee formultilateral export controls (cocom) allies, consider the value of export controls covering what are increasinglycots dualuse technologies is not obvious. none gives it the same level of visible attention. when computingwas much more exclusively in the u.s. domain and these allies shared a common threat to survival, it was easierfor all to agree to controls. in todayõs world, it is unimaginable how an effective unilateral control regime mightbe constructed.turning now to stakeholders in business and industry, we can find several factors that expand the set ofparticipants. one is that technological progress has been such that hpc, at least by the early 1993 export controldefinition above, is no longer limited to the extreme high end of the total industry product line. it now covers asubstantial fraction of the computers in the world, and thereby many of the computer makers, a significant fractionof which are not on u.s. soil. furthermore, about half of the new computer sales in the world are not made on u.s.soil.in many countries, the promotion of business interests is a basic part of national economic policy. it is evenfashionable for many countries to be formulating explicit national informatics policies. in the united states, whichdoes not have a comprehensive national industrial policy, there are, nevertheless, many government playerspromoting business interests at the national, state, and local levels. with many government stakeholders and theirconstituents, the concern for jobs and exports carries more weight these days than claims of computerbasedmilitary threats (although computerbased security problems have been attracting a much greater level of concernin the business and privacy arenas, partly as a result of global diffusion).todayõs world of computer sellers and buyersñincluding huge multinational corporations (mncs), manytechnologically capable small companies, and an immense consumer community ranging from governments toother large mncs down to millions of small organizations and individualsñconstitutes a large and growing, pushpull combination that is under pressure from intensifying global competition. as in the drug trade, there is a hugetraffic among willing and resourceful buyers and sellers, and it is difficult for third parties with conflicting intereststo interfere with this, for example, through export controls, tariffs, or import prohibitions. some vast and wealthyindustries (most notably the entertainment industry) depend increasingly on it as a vehicle to do their businessworldwide, adding a powerful òpulló element to the òpushó interests. as it diffuses more widely and becomesincreasingly pervasive in human activities everywhere, this buyerseller community gains in importance andinfluence.there are other parties with stakes in the international diffusion of computing. most are recent entrants, andalmost all favor the diffusion of computing and related technology transfers. these include various players indeveloping countries who see computing as necessary for development and as a way to join the rest of the world.others see it, in the words of ithiel de sola pool, as the òtechnologies of freedom,ó supporting openness, civil andhuman rights, and democracy; encouraging american values; and making it harder for tyrants to hide abuses. themedia also ride on this track, promoting freedomofthepress values (and for other reasons, for example, to sellentertainment and to provide better facilities for their reporting).people in these categories might argue that instead of controlling the infusion of computing into easterneurope and the soviet union in the late 1980s, the united states should have tried to pour it in. they might furtherargue that the infusion of it did more to bring about the decline of overall soviet power than export controls at thattime did to limit its military growth.the academic community tends to favor transferring more it. this viewpoint arises in a large number ofcontexts, including support for the reasons discussed above and for other reasons such as promoting academicfreedom, encouraging privacy and security through encryption, participating in efforts to bring useful technologiesto colleagues in other countries, and forming virtual scientific communities. american academic stakeholders arealso increasingly dependent on foreign student enrollments to keep their departments viable. foreign shares ofgraduate student enrollments in science and engineering departments in u.s. universities amount to 30 percent orhigher. simple demographics would indicate that faculty shares may follow suit. such training constitutes one ofthe most effective forms of technology transfer, and it is being done on a large scale. for example, it is estimatedthat on the order of 30,000 to 40,000 chinese students are studying in the united states. how does one justify adefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.the global diffusion of computing21major regime for controlling cots hardware products under such circumstances? it seems like locking doors ina building when the walls are falling down.export control is but one example of a major technology issue that has been dramatically affected by theglobal diffusion of computing and the attendant expansion of stakeholders. the spread of global networking invarious forms is creating or greatly exacerbating a number of issues that are attracting, or will attract, furtherattention. for example, many things from american political correctness to law enforcement practices to intellectual property rights to islamic religious tenets may be seen as local sensitivities that are increasingly to be batteredin a global multimedia freeforall. many forms of conflict, both civil and military, are finding fertile internationalor transnational breeding grounds in the itbased media.many will not like the redistribution ofñor compromise of, or assault onñmoral or legal sensitivities, jobs,markets, wealth, influence, military power, and governmental authority via the international use of it. many ofthese will be influential stakeholders who will seek help in various ways from their governments or internationalorganizations. there are notable cases in which this has already happened. it may also enable them, as well asthose they oppose, to band together to seek help or assert influence independent of their governments.cstb studies, and those of the national research council more generally, essentially have tried to bringscience and technologyrelated trends and issues to the attention of interested constituencies, and the attendantanalyses have tried to inform or guide choices for policy formulation. to a large extent, the latter is a matter ofbalancing or choosing between value conflicts among these constituencies. the primary funders, and the principaltargets as consumers of these studies, have been parts of the u.s. government.the activity levels and pressures on national policy makers in these matters are likely to increase, as will somefrustration levels. it appears possible that the relative power of national governments in these matters willdiminish, but certainly not disappear, and that governments will have to contend with and work with other playersas never before. the activity and frustration levels of advisers to governments, such as the cstb, may alsoincrease substantially.the main purpose of this short presentation has been to draw attention to the greater internationalization ofboth the issues and the affected stakeholders that is accompanying the global diffusion of it. this is not a shortterm or transient phenomenon; it is going to be forever.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.22william j. spencereveryone knows the adage (variously attributed to baseball managers and ancient chinese philosophers): òitis difficult to predict, especially the future.ó with that in mind, this paper focuses on how productive thesemiconductor industry has been over the past 50 years, its status in 1995, the road map for the industry for the next15 years, some of the challenges that the industry faces, a few predictions, and a final caveat.the productivity of silicon technology is usually measured in the cost reduction of memory or the increase inprocessing power. the cost of memory will have fallen by roughly five orders of magnitude from 1972, when thefirst 1,000bit static random access memory chip (sram) was available, to the year 2010, when the 64gigabitdynamic random access memory chip (dram) will be available. the cost of computing has also decreaseddramatically during this time frame. fortune, the impeccable source of reliable information, recently showed thatthe cost of mips has declined by a factor of 200,000 over 30 years. this dramatic reduction in cost or increase incomputing power over a period of several decades is unparalleled in any other industry. it is this continualimprovement in productivity in silicon integrated circuits, and the related lower cost of memory, computingpower, and communications bandwidth, that is leading the world into the information age.the productivity gains are made possible by continued technology improvements in the manufacture ofsilicon integrated circuits. the silicon transistor has been the principal element in this technology since 1957. thetypes of transistors have changed from grown junction to planar, from bipolar to metal oxide semiconductor(mos), but the transistor effect has been the principal element in all silicon integrated circuits for the past 40 yearsand probably will continue to be for at least another quarter of a century.the technology changes that have occurred are listed in table 4.1. these begin with the invention of thetransistor at bell labswestern electric in 1947 and continue through the introduction of ion implantation, reactiveion etching, optical steppers for lithography, ebeam mask making, and a variety of other technologies. except forsome of the design and simulation packages, these technology innovations have all come from industrial labs.some of these laboratories do not exist today; others have been downsized and redirected. this represents a majorchallenge for our industry that i want to come back to later in this presentation.in 1995, most leadingedge semiconductor manufacturers were using a 0.35micron technology for manufacturing mos devices in microprocessors, memory, and logic. a crosssection of a cmos transistor pair in thistechnology is shown in figure 4.1. silicon chips today contain as many as 20 million of these transistors. this4engines of progress:semiconductor technology trends and issueswilliam j. spencercharles l. seitzdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.engines of progress23table 4.1major semiconductor innovationsinnovationlaboratoryyearpoint contact transistorbell labswestern electric1947singlecrystal growingwestern electric1950zone refiningwestern electric1950grown junction transistorwestern electric1951silicon junction transistortexas instruments1954oxide masking and diffusionwestern electric1955planar transistor and processfairchild1960integrated circuittexas instruments, fairchild1961gunn diodeibm1963ion implantationplasma processingebeam technologysource: john tilton, brookings institution, washington, d.c.technology has several interesting characteristics. the interconnect is still principally aluminum, although it isnow alloyed with copper and titanium. the interlayer dielectric is still silicon dioxide. you will note that thisplasmaenhanced chemical vapor deposition of silicon dioxide leaves very uneven layers. there is a complexmetallurgy in the vias that consists of titanium and titanium nitride with tungsten as the principal conductor.today, the semiconductor industry is roughly a $200 billionperyear industry; $150 billion of this is in thesale of semiconductor devices, roughly $30 billion in the sale of processing equipment, and approximately $20billion in the sale of manufacturing materials, including silicon, mask blanks, photoresists, and production gasesand liquids (table 4.2). in the 1990s, the industry has grown at an extremely rapid rate, averaging more than 30percent per year over the past three years. the industry is projected to grow at an average rate of about 20 percentper year for the next 15 years.if we compare the growth rate of the semiconductor industry (about 20 percent per year) with the grossdomestic product (about 2 percent per year), by the year 2019 semiconductor sales will be equal to the u.s. grossdomestic product of $11 trillion. this leads me to prediction 1: the increase in semiconductor sales will flattenbefore the year 2019.the u.s. semiconductor industry has developed a roadmapping process that looks at technology needs for theindustry 15 years into the future if costs are to continue to decline. an example of the content of this road map isshown in table 4.3. the road map considers memory, highvolume logic, and lowvolume logic. the yearrepresents the first shipment of a product with the given technology. a major consideration of the road map iscost. the cost per transistor for each of these products is predicted to decline with each generation of technology.this leads to prediction 2:  there is no physical barrier to the transistor effect in silicon being the principalelement in the semiconductor industry to the year 2010.although the road map projects technology generations for 15 years in the future and the technology requiredfor that projection, it does not define solutions for all of the technology requirements. let us look at some of thechallenges that must be addressed to meet the road map projections to the year 2010.will it be possible to design 100 million transistor logic circuits and 70 to 75 billion transistors on memorychips? figure 4.2 describes what the semiconductor community has called the design productivity crisis. theupper curve shows the compound complexity growth rate in silicon integrated circuits at roughly 60 percent peryear. the lower curve shows the compound design productivity growth at about 20 percent per year. this leavesa considerable gap in 1995 and a growing gap into the twentyfirst century, which is the consensus of a group ofdesign experts in industry, government, and universities. there is no complete solution to this design dilemma.there is cooperative work on building an open infrastructure that will allow interoperability of commerciallyavailable design tools and of those tools that are developed as competitive capabilities in integrated circuitdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.24defining a decade: envisioning cstbõs second 10 yearsmanufacturers. the design costs, the size of design teams, and the time required to design products at the end ofthis century will be major barriers to continued growth of the integrated circuit industry. this uncertainty in designcosts and investment to manufacture new integrated circuits has led to numerous joint ventures focused on newproducts.supposing we are able to design integrated circuits in the twentyfirst century, will we be able to manufacturethem? here, i would like to focus on just one processing step, lithography. lithography and interconnectrepresent the two major costs in the manufacture of integrated circuits. figure 4.3 shows the road map forlithography and some potential solutions for lithographic requirements. at 0.35 micron and 0.25 micron, thetechnology choice has been made. deep ultraviolet (duv) exposure tools, operating at 248nm wavelength, willfigure 4.1device cross section, sematechõs 0.35mcmos process. source: sematech.table 4.2semiconductor industryñ1995salesbillion dollarssemiconductors~ 150equipment~ 30materials~ 20total~ 200pecvd si3n4/sio2 passivationti/tin linerw plugstisi2 salicideddiffusions and gatediffused nwellpmos transistorpbl isolationnmos transistordiffused pwellsio2 spacerpmd: sio2bpsgti/alcu/tinmetal 1pecvd sio2ti/alcu/tinmetal 2defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.engines of progress25be required for these two generations of integrated circuits. at 0.25 micron, there is a possibility of additionaloptical enhancement (oe) through offaxis illumination, phase shift masks, and other optical tricks. at 0.18micron technology, the solutions are less clear, and for technologies of 0.13 micron or less, it is even less clearwhich technology will be suitable for production of these devices.the development of this technology does not come for free. projected development costs for lithographicsystems (including exposure tools, resists, metrology, and masks) at 193nm exposure wavelengths total nearly$350 million for the 19952001 period (costs are highest in the first four years and peak in 19961997). this costis beyond the capability of any single company. this has been an area in which the u.s. semiconductor industryand the supplier industry have worked cooperatively to develop 193nm lithography and possible optical extensions down to 13 nm. multicompany consortia have focused successfully on technology development, whereas itis principally bilateral joint ventures that focus on new products.now, if we can design (still uncertain) and fabricate (possible) integrated circuits with 100 million transistors,will it be possible to package them? in 1995, 10 million transistors were being built on a single chip with a costper transistor of about 10 microcents for a total chip cost of roughly $10. the packaging costs in 1995 representsomewhere between $5 and $35 for packages with up to 500 pins. this supports personal computers that cost inthe range of $2,000 and provide moderate data capability, no voice, and very slow video. six years from now,there will be nearly 50 million transistors on a single chip at a cost roughly five times less, with a chip cost that isstill about $10. the packaging costs will rise to nearly $50 for packages with up to 1,000 pins. the question iswhether this technology will provide components at a cost low enough to keep personal computer prices at roughly$2,000 with better data capability, limited voice, and faster video. the challenge of programs in the governmentand at sematech is to bring packaging costs down so that they remain on the same level as chip costs. this willrequire a reduction of roughly two times the package cost.we have looked at the challenges in design, processing, and packaging of integrated circuits during the firstdecade of the twentyfirst century. what will the cross section of one of these integrated circuits look like? thecross section shown in figure 4.4 is of a 0.10micron cmos process for highperformance logic. there areseveral major differences between this and the 0.35micron schematic shown earlier. notice that the interlayerdielectrics are all flat. this means that the dielectric material has been planarized after it has been deposited. thematerial has changed from silicon dioxide to a low dielectric constant polyimide. metallization has gone tocopper, with copper plugs in the vias and copper in the liners. all of these new technologies will require extensivedesign and testing to ensure continued reliability and cost reduction.typically, the interdielectric layers are on the order of 1 micron in thickness, while the gate lengths are a tenthtable 4.3overall roadmap technology characteristicsñmajor marketsyear of first dram shipment199519982001200420072010minimum feature size (m)0.350.250.180.130.100.07drivermemorydbits per chip (dram/flash)64 million256 million1 billion4 billion16 billion64 billioncost per bit @ volume (millicents)0.0170.0070.0030.0010.00050.0002logic (highvolume: microprocessor)l (p)logic transistors per cm2 (packed)4 million7 million13 million25 million50 million90 millionbits per cm2 (cache sram)2 million6 million20 million50 million100 million300 millioncost per transistor @ volume (millicents)10.50.20.10.050.02logic (low volume: asic)l (a)transistors per cm2 (auto layout)2 million4 million7 million12 million25 million40 millionnonrecurring engineering0.30.10.050.030.020.01cost per transistor (millicents)defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.26defining a decade: envisioning cstbõs second 10 yearsfigure 4.2design productivity crisis. source: gwl etab strategic review, march 1996.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.engines of progress27figure 4.3lithography roadmapñpotential solutions. source: sematech.1992199519982001200420072010optics extension through¥ wavelength reduction¥ increased numerical aperture¥ optical enhancements (oe)1line + oe248 nm duv0.35um248 nm duv + oe0.25umnarrowoptionsleadingedge productionfurther study requiredpilot linedevelopment/most likely pathback up248 nm duv + oe193 nm duv + oe0.18um0.13umnarrowoptions193 nm duv + oeebeam projectionproximity xrayadvanced opticsdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.28defining a decade: envisioning cstbõs second 10 yearsof a micron. thus, the schematic in this figure is compressed in the vertical direction and highly expanded in thehorizontal direction. if we look at the actual cross section of this 0.10micron cmos process it looks more like askyscraper. this shows why the major complexity in future integrated circuits will be focused on interconnectsand the lithography to produce these interconnects. the basic transistor structure, while becoming more difficultto manufacture, will be a much smaller part of the total processing costs compared with the interconnect.finally, let us look at the total cost of building a manufacturing facility for future integrated circuits. the costsin 2000 are expected to be about $2 billion for each new fab. in 1970, intel built its first fab for less than $5million. the cost of fabrication facilities is growing faster than the growth of semiconductor revenue. this leadsto my third prediction:  the entire worldõs requirements for silicon integrated circuits will be built in a singlefabrication facility (probably korean) by the year 2050.let me go back to a point made earlier in looking at technology changes that have led to the continuedproductivity growth of integrated circuits and where those technology changes originated. most of the technologyused in the manufacture of modern semiconductors came from industrial laboratories such as bell labs, ibm,texas instruments, fairchild, and phillips. today, these laboratories either no longer exist or have been significantly downsized, particularly in physical science research. the remaining research is focused on corporate needs,figure 4.4device cross section, 0.1m cmos processschematic. source: sematech.high performance logicpecvd si3n4/sio2passivation cap.lok polyimidecu plugscu barrier/linerelevated cosi2 ortisi2 salicidedsource/drainand gatep+ poly gatelegendx is to scaley is 1/5 above sipmos transistorsoisilicon substratenmos transistorn+ poly gatesio2 spacerbpsgegcu metal 1cu metal 18low k, planarized(may have hard masklayers between levels)device crosssectiondefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.engines of progress29not industry needs, leaving a major technology gap. without these engines of innovation, the entire semiconductorindustry will stall in the first decade of the twentyfirst century, and this stalling will slow the growth of computersand telecommunications. there is certainly enough technology to keep the industry going over the next 5 andperhaps 10 years. beyond this time, there will be serious shortages in design, process, packaging, and othertechnologies that support the semiconductor industry. this is an issue that must be addressed by the industry, thegovernment, and the universities. there is no simple solution. the semiconductor industry is looking at triplingits investment in university research. even with this investment, there must be changes in commercialization,culture, management, and education to provide the future source of innovation for the semiconductor industry.this leads to my final prediction: all of the earlier predictions are wrong!as soon as a road map is written down, it becomes outdated. this industry moves so rapidly that it isimpossible in any document, talk, or paper to predict where it is going. usually, the industry has moved morequickly than predictions. engineers and scientists have typically found ways to overcome technology barriers, theindustry has continued to grow at an amazing rate, and productivity continues to increase at 25 to 30 percent peryear in silicon technology. this has led to the semiconductor industry as the economic driving force for theinformation age. if it stalls, the applications that are dependent on silicon technology will stall as well. thistouches every aspect of our lives, from education to work to leisure. silicon technology is pervasive in all ourlives. you probably have a half dozen or more integrated circuits on your person todayñin your watch, beeper,cell phone, and electronic notebookñand if you carry a laptop, of course the number goes up dramatically. thereare answers to most of the manufacturing productivity issues for semiconductor technology; the major questionwill be where our future innovations originate.charles seitza remarkable thoughtñcontinued progress through at least 2010. the progress described by william spencer reminds me of neighbors who ask, òshould i buy the new model xyz computer that just came out, or shouldi wait another year because they keep getting better and better?ó the only answer i have for that question is, òitdepends on whether you want a computer to use over the next year.óit has become part of everyoneõs expectations that computing devices will keep getting better and better. billand i are struck by the extent to which the real engine of progress is found in some of the lower levels of computingtechnology, even though the effect of these steady improvements is to be able to support larger and larger softwarepackages that do more and more.i am going to take you straight to a designerõs eye view of a chip. if you were to walk around the designlaboratories at places like intel and motorola, or at any of thousands of small companies, you would see peoplesitting in front of computer screens in cubicles, much like the dilbert cartoons, with pictures or portions of a chipslayout in front of them.these pictures, like most arcane art, may be a little difficult to interpret. what the designer is looking at is anumber of different layers, each in a different color, as if from a skyscraper looking down at a city. differentlycolored layers represent layers of metal on a chip. these metal layers are entirely for interconnection. specialsymbols, such as boxes with stipple or cross patterns, are used to indicate the connection from one of the metallayers down to another or from the metal layers down to the deeper layers. it would not surprise you, perhaps, thatthe wider metal layers carry power to the inner circuits of the chip. òwhere are the transistors?ó you ask. well,they are in the smallest features hidden way down at the lowest levels of the chip.to the designer, the transistor is formed where one of the wiresñcalled poly because they were oncepolycrystalline silicon, although they are now typically composed of tungsten silicideñcrosses another wire, thediffused area, down in the very, very tiny and deepest parts of the chip.the layout of all of these geometrical shapes creates a data file from which optical masks (or reticles) aremade, from which chips are made in turn. in this sense, all of the wonderful work that is done in semiconductormanufacturing can be thought of as similar to what the photography industry does. designers can produce anypattern they want and turn it over to a fabrication facility that can make chips with the same pattern. althoughdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.30defining a decade: envisioning cstbõs second 10 yearsprocesses for certain types of chips are somewhat specialized, for the most part the fabricator does not use adifferent process for one kind of chip or another, any more than a printer might use a different kind of paper for aphotograph of your daughter or your boat.òin actual fact,ó as they say in england, this picture [displayed at the symposium] is not an entire chip. theplot is of an area about a quarter of a millimeter on a side. one of the aspects of chip design that i want toemphasize is complexity, and i will use every opportunity to do so. remember, cato the elder kept repeating tothe roman senate that carthage must be destroyed. if you keep repeating something, it oftentimes happens.so, please excuse the repetition, but these chips are awesomely complex. a stateoftheart chip today isabout 2 cm on a side. thus, at the scale of this plot, it would take 6,400 of these pages to display the geometry ofthe entire chip. it is difficult for the designers even to find their way around such chips, let alone to design them.one of the beautiful things about semiconductors is that they are just physics at the lowest level. you mightsay, òwhy do bill spencer and sematech go to such great effort and expense to figure out how to reduce thefeature size a little bit more?ó let me try to explain, based on the tabulation along the left side of figure 4.5.starting in the early 1970s, device physicists recognized an ideal form of scaling of the socalled mosfet(metal oxide semiconductor, fieldeffect transistor) technologies, in which transistors are formed longitudinallyalong the surface of the chip. here is the basic story.for a scaling factor a, reduce the feature size x to some value x/a. if you do this, you should also reduce thevoltages in order to keep the electric fields from increasing; otherwise, you are asking someone to invent newmaterials that could withstand higher electric fields. (although chips operate from relatively low voltages, thedimensions are also very small, resulting in electric fields that approach the limits of the dielectric strength of theglass (silicon dioxide) insulator under the mosfet transistor gate.)with electric fields being constant in this scaling, and with the mobility of silicon being constant, the velocityof mobile carriersñelectrons or holesñis more or less constant. however, the carriers can traverse the smallertransistor in less time, making the smaller transistors (and circuits, if you go through the entire analysis) faster.every child knows that smaller things are faster.the reduction in feature size increases the circuit density quadratically with the scaling factor, but if thecurrent is also reduced in proportion, the power per device is reduced quadratically with the scaling factor. it isfortunate that these effects balance, so that the power per unit area remains constant in scaling.the switching energy, also known as the powerdelay product, is a technology metric that closely predicts thecost of a computation implemented in a given technology. the bottom line is that the switching energy, theproduct of the power per device and the transit time, scales as the third power of the scaling factor. thus, forexample, all of the effort of reducing the feature size by a small factor such as 1.26 pays off by reducing theswitching energy by a factor of 2, and this factor of 2 can be applied across the board in all kinds of computing andcommunications devices. it allows chip and computer designers to offer about twice as much computing at thesame cost, or the same amount of computing at half the cost.higher circuit density together with larger chips has led to the remarkable complexity scaling of microelectronics shown in the upper right of figure 4.5. this escalation of the circuits that can be put on a single chip,known as òmooreõs lawó (after gordon moore, chairman of intel), provides a smooth entry for yet another lessonin complexity appreciation.one nice analogy is to compare a chip with a city. the minimum spacing of the wires on a chip today is abouta micron, whereas city blocks are spaced about eight per mile, or five per kilometer. the difference between thespacing of wires on a chip and the spacing of blocks in a city is a factor of about 200 million.let us take one of todayõs chips, 20 mm on a side, and print a map on it. the multiple layers of wiring on thechip are of greater complexity than the generally single layer of roads found in a city, but you will see that the chipcan accommodate the map of a city 4,000 km on the side.this would be quite a city. if you use the figures from billõs charts of year 2007 technology, the wire spacingwill then be reduced to about a quarter of a micron, raising the scale factor to 800 million. how big a chip can youmake? that is determined largely by the defect density. today, we have defect densities of somewhat less thanone per square centimeter, so it is reasonable to build chips that are a centimeter or two on the side. if the defectdensity is reduced still further, you can achieve acceptable yields on chips that are even larger. a chip 50 mm ondefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.engines of progress31figure 4.5the underlying òengine of progress.ó source: myricom, inc.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.32defining a decade: envisioning cstbõs second 10 yearsa side is what you get if you crank out the math based on billõs defectdensity projections. applied to the cityanalogy, the complexity of a single chip would then correspond, roughly speaking, to the area of the earth coveredat urban densities at about 10 different levels, including the oceans. talk about an ecological disasterñthe òlosangelesizationó of the world!chip designs are, of course, done with the help of computeraided design and analysis tools (see box 4.1).the analysis tools provide reasonable assurance that the first time you fabricate a chip, it works well enough foryou to figure out what you overlooked. there is, by the way, no easy way to probe the signals in the interior ofthese chips. even if you could position a probe on the right point, the energies involved are so smallñmeasuredin femtojoulesñthat the probe would disturb the operation of the circuit.designers of processor chips always wish they could use the chip that they are designing to run their analysistools, but we have to run the design and analysis tools for each generation of chips using computers built from theprevious generation of chips. nevertheless, the technology is improving on exactly the same curve as the demandon the tools, an interesting example of technology feeding on itself.there was a fairly large brouhaha in the computer software field in the 1970s, triggered by edsger w.dijkstraõs 1968 article ògot to statement considered harmful.ó1 computer software was getting complicatedenough that people had to adopt complexity management schemes cast into disciplines such as structured programming for writing software.during the past decade or so, we have seen chip designers adopt analogous disciplines. frequently, thesevoluntary restrictions are tied into the design tools, just as programming disciplines are frequently incorporatedinto programming notations. my favorite rule is, òdo not design what the simulator cannot simulate.ó it may beperfectly possible to lay down some metal and poly and diffusion to produce a certain circuit that would work.however, if the simulator cannot divine that it would work, you had better not use this circuit because you wouldhave to treat it as too much of a special case.one fly trying to get into the ointment is that, as these devices get smaller and smaller, they are less ideal. ourpresent devices are what physicists think of as thermodynamic. their operation depends on aggregates of tens ofthousands of charges. the statistical fluctuations around 10,000 are in the sub1 percent range.as everything gets scaled down, you reach a regime in which, for example, the threshold voltage of atransistor, instead of being determined by something on the order of 10,000 impurity ions under the gate today, isdetermined by 100 or so. the statistical fluctuations around 100 may be 10 or 20. in addition, instead of havinga mere 108 devices on the chip, we may have 109 or 1010.1dijkstra, e.w. 1968. ògo to statement considered harmful,ó communications of the acm, march, pp. 147148.box 4.1how do chip designers cope?the òmechanicsó of chip designeach generation of chips is designed using computeraided design and analysis tools that execute on the previousgeneration of computers, an example of technology òfeeding on itself.órespect complexity. just as computer software hit complexity barriers demanding the introduction of òstructured programmingó and other design disciplines, chip designers have adopted analogous disciplines. òdonõt design what thesimulator canõt simulate!ósubmicron devices are not as ideal as the transistors and wires at larger feature sizes, thus additionally complicating thephysical design of chips today. in the future, designers may need to learn to cope with the statistical likelihood of a smallfraction of the transistors on a chip not working.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.engines of progress33one basic change in microelectronics technology to be expected is that chips will be made with no expectationthat all of the devices on the chip will work correctly. people doing the design and engineering have to startthinking in terms of systems such as the national power grid, in which one expects that some parts of it, at anygiven moment, will not be working correctly.finally, the question that most interests me is, how do designers innovate? (see box 4.2). i wish ivansutherland (a former member of the computer science and telecommunications board) were here. i heard a storyfrom ivan at the california institute of technology in about 1978 that i have carried in my head since and thoughtabout a lot. this is a story about bridge builders who build bridges out of stone. then steel, the exciting, newmaterial, comes along. the builders want to be modern and innovative, so they start to use steel. they use it bycasting the steel into blocks, which they then assemble into arches to make bridges.with new technologiesñand microelectronics is fundamentally quite a new technologyñthere is the samequestion about its real properties. in its early days, microelectronics was used as a substitute technology fortransistor logic. it came into its own when microprocessors and memories appeared on the scene. where do theinsights come from that let people build the trussandtrestle bridge, let alone the suspension bridge? apparently,one of the answers is from recognizing what properties the new material has. for example, steel will take tensionas well as compression, so it allows you to do new and different things.microelectronics differs from older electronic technologies, first of all, in being universal. in the olden days,you would have computers built with magnetic core memories, transistor or worse logic, special devices such aspulse transformers to shape pulses, and so on. today, the nucleus of the system is all created out of onetechnology. there is no escape into any specialized technology for some special purpose.microelectronics is severely communication limited. most of the area, power, and delay is caused by thewires. the transistors and active circuitry take up just a little bit of the lowest level. for reasons that i do not havetime to get into, some of which are described in cstbõs computing the future (1992) report, these communication limitations favor simplicity and concurrency. you can even present a mathematical argument that explainswhy. another effect is that it is easier to reduce costs than to increase speed. you will recall that the circuit densityimproves quadratically in scaling, whereas speed improves only linearly. these are some of the reasons people areresorting to unusual measures to get more speed, particularly parallelism.there have been an enormous number of success stories. if john hennessy (another former member) hadbeen here, he could have told you about risc (reduced instruction set computing) and cache memories. thissuccess story is another case of trying to fit technology to peopleõs needs through engineering insights thatrespected the limitations and exploited the specific capabilities of silicon.box 4.2how do designers innovate?ivan sutherlandõs (~1978) story about the bridge builders who knew all too well how to build bridges of stone: whenthe new material, steel, came along, they cast the steel into blocks from which they made arch bridges. how would theinsights come about that would result in the trussandtrestle bridge, let alone the suspension bridge? in part, fromrecognizing how the properties of steel differ from those of stone.how does microelectronics differ from earlier digital technologies? it is¥highly universal (no escape into specialized technologies);¥severely communication limitedñwires use most of the area and power or cause much of the delay (favorssimplicity and concurrency); and¥easier to reduce cost than to increase speed (favors concurrency).the òsuccessó stories in circuit, logic, and architectural innovationñthe dynamic ram, programmed logic arrays,riscs and cache memories, highly concurrent computers, and many other innovationsñcan be traced directly to insights that respected the limitations or exploited the capabilities of the medium.ram = random access memory; risc = reduced instruction set computing.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.34defining a decade: envisioning cstbõs second 10 yearsdiscussionmichael dertouzos: i have a question for both of you. i often hear this question about cost, and iwould like to have it answered by the experts. what if you really went after the cheapest possible reducedcapability of microprocessor chips that would still make a computer possibleñnot a computer that can supportmicrosoft word 6 and the programs of today, but something that is perhaps scaled back 10 or 12 years. if you gofor minimum cost, is the changed materialñthe bottom line materialñin the cost? would we see it reflected in thechip, and then in the computer?charles seitz: i have thought about that question and, to be realistic, the problem is economics. myanswer comes partly out of reading one of your books. there have to be good profit margins in this businessbecause it is extraordinarily expensive to keep developing new products on such a rapid cycle. most computercompanies are having to redesign their products about every 18 months. you sometimes wish that the recklesspace of our field would slow down a bit so we could all stop and figure out what has gone on, but with this pace,the profit margins on items such as desktop machines have to be reasonably large. one of the things hurting thecompanies right now is that the margins on desktop machines are very small. so i do not think you can cut theprices significantly below where they are right now.william spencer: we do an analysis, and i am sorry i do not remember the numbers now, on productcosts on silicon. if you look at personal computers or anything else today, a larger and larger percentage of the costis silicon. a major cost right nowñforget the profit for a minute, if you can break evenñis for capitalization onequipment. the depreciation costs are growing so large that they are going to dominate things. as an example,nextgeneration exposure tools, the most advanced ones, are now being sold. you may buy one for $5 million upfront, nonrecurring engineering costs and $10.5 million for the machine itself. it will be good for maybe twogenerations of technologies, five to six years. in this country, it has to be depreciated over that fiveyear period.that is turning out to be the major cost, the fabrication facility itself.robert kahn: there is a lot of good news and bad news in the stories that we have heard from both ofyou. on the good news side, things are going to get smaller. on the bad news side, it is getting harder, and eventhe design is going to get increasingly difficult quadratically or by the cube of whatever. there are clearly twoways that we can deal with this increasing complexity. one way is to make things smaller, so you get more intoa volume. or on a given feature size, make things bigger in an area or volume. the attendant challenges are large.when i first got involved with vlsi design in the 1970s, we were talking about 4 or 5 microns at most in size.in 20 years, bill spencerõs charts show we are now down to 0.35 micron, soon to be 0.25. this is a factor of almost40 or 50, some numbers like that, that we have experienced over the past 20 years. yet it looks as if theprojections for the next 20 or 30 years are maybe another factor of 2 or 3 at very large costs, pending some realinnovation that changes the linearity of these curves.i remember jim meindl put a report together 10 or 15 years ago predicting that somewhere around the 0.3micron range, transistors would no longer function, and that this was a natural block. somehow we have gottenthrough this. i do not know where the current limitations are, whether it is at 0.1 micron or whether we will getdown to atomic scale.the question i have is this. in the future, what is going to have the same factors of 50 or 100 or 1,000 inscalability that can generate the real interest and excitement in this field? are we just talking about a tenth of apercent here or a few halving of microns here and there?spencer: from the technology side, i believe the cost reduction will continue for another quarter of acentury. we do not need a breakthrough to do that. the physics of transistors looks as though they are good downto less than 0.05 micron. we can build transistors that switch at room temperature; we do not have to go to liquidnitrogen temperatures to do that.it would be great if the people in the ibm lab in zurich actually had gotten room temperature superconductors rather than liquid hydrogen temperature superconductors because, as chuck seitz pointed out, the majorproblem in the future is going to be 10 to 15 to 20 layers of metal or interconnect that may not be metal. this iswhere the controlling features are going to be.in the past, every time we have run up against a technology barrier, somebody at bell labs discovered reactivedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.engines of progress35ion etching to get rid of wet chemical etching. steppers came along and gave us a 10 or 15year respite in maskfabrication. ion implantation came along and got us away from diffusion furnaces, which were terrible on thestatistical issue that chuck raised. my guess is that there is an engineer or a scientist out there somewhere who hasideas about how we are going to break through these barriers, and this will continue to grow. i do not think weneed a big breakthrough for 15 or 20 years.seitz: if i may add a couple of things. we thought that the scaling limit was somewhat below a quarter ofa micron and would be due to tunneling, statistical fluctuations in threshold voltages at reduced voltages, and a fewother effects. now we know the limit is somewhere below a tenth of a micron. over the next decade, what mattersare not effects such as tunneling, but more mundane interconnection issues. the fabrication processes areimproved in all of the areas where there is the most leverage, such as adding more and more layers of interconnect,which is really what has been limiting for some time.besides bob, if, in your view, we can get a mere factor of 10 or 100 in the next 15 years, maybe the softwarepeople can take up some of the slack.mischa schwartz: i would like to focus on a severe problem that bill raised when he first startedspeaking, and that is the question of the downsizing of basic research in physics and devices by the majorcorporations in the last few years. those of us in the academic world have been very concerned about this. thequestion is, who is going to build the hardware platforms 10 years from now that the software is going to ride on?you have mentioned that maybe universities can pick up the slack. we keep hearing this. this is the reason whywe are looking at increasing the basic research activities, things that are 10 years or more out. if i look at tryingto rebuild bell labs, it is like reassembling humptydumpty to me. i do not think we can put it back togetheragain. bob lucky could give us his view on it. my 15 or 20 years at bell labs convinces me it is gone and itcannot be replaced. national labs, in my view, require a larger cultural change than the universities do. i thinkindustry must undergo a major change. sending someone to work at a university is now considered a positive stepfor your career, rather than a detriment. i think this is the best place for us to turn.i think innovations will come from all sorts of places. we have got to be in a position to capitalize on themwhen they do occur. that is the second major problem we have had in this country. we are not very good at this.we do the first startup. then, when we have a success, we find that east asian countries, or even europeans, nowmanufacture things in high volume better than we do. i do not think there is a simple answer to this, but somebodysaid we badly need a policy in this country that says how we are going to address this issue, not only in the siliconarea, but in all areas of research. i think we no longer have national labs like at&t and ibm, ti, phillips, andfairchild. the national labs are gone, and we have got to find a way to replace them.shukri wakid: if you believe embedded computing is going to be real or very distributed, then intelligentsensors are one way to go. this means you need designs for chips that are a lot simpler, much simpler and moreapplied. by the same token, if federal digital signal processing is going to take offñand people sometimes say itis a barrier to computing because it is very difficult to doñthen it is going to force a need for simpler design versusa more complex design. do you want to say anything about this reverse trend of complexity?seitz: many of the processing media, sensor outputs and so on, of military embedded systems use digitalsignal processing chips. the typical digital signal processing chip dispenses with all the address translationhardware and other facilities required to run an operating system on processors used in desktops. there are at least40 companies in the united states that make digital signal processors, ranging from boutique companies to thelikes of ti and motorola. as long as there is some money to be made in this area, i think it will continue to behealthy.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.36leonard kleinrocki am tempted to suggest to the information technology users of the world, òunite and throw off your chains.óso letõs do that.i am going to talk about òcomputing and communications unchained: the virtual world.ó we could havecalled it the òvirtual universe,ó but we thought we would restrict our focus. after all, academics are narrow(wasnõt that the word used this morning?). i will talk about the virtual world of the nomad, discussing some of theissues and the technology. john major will then talk about the applications in which you begin to see nomadicityhappen.so let us start at the right place, a dungeon. most of us associate our computers, such as they are, with somekind of desktop device, possibly connected to a server, located down in somebodyõs dungeon. you never see it.you are rigidly attached to that architecture.in fact, most of us are nomads. i do not know how many laptops there are in this room, but there are more inyour hotels or automobiles. we travel everywhere. we travel to our office, home, airplane, hotel, automobile,branch office, and bedroom. my wife will not let me use a laptop in bed any more. what bothers her is the noiseof the keyboard: it is forbidden.of course, we also travel to places like this symposium. when i go on the road, i usually travel with a laptopcomputer, pager, cellular telephone, and personal digital assistant (pda). it used to be that shelf space in abookstore or counter space in the supermarket was a precious commodity that all merchandisers would fight for.today, they are fighting for waistline space to hang all this stuff on. you can rent out square inches of your belt thesedays. when i load up and go out into the world, i feel like pancho villa going into battle with the u.s. cavalry.so what is nomadicity? in my mind, it is basically the system support needed to give all kinds of capabilityto nomadsñno matter where they goñin an integrated, transparent, and convenient fashion. you should not haveto suffer because you are moving around. today, you suffer a great deal, so let us talk about what is needed andwhat some of the issues are.why should we care about nomadicity? there are many reasons. it is here right now; the users see it. we allmove around and experience problems with synchronization, updating, access, weight, and more. i think there isa major paradigm shift in computing. it may not be as important as information warfare, but i think this is adominant trend, not just a tangential issue. this is how people are now and how they will be using information5computing and communications unchained:the virtual worldleonard kleinrockjohn majordefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.computing and communications unchained37technology. the technology is available. wireless is here as well as the necessary light devices. batteries areimproving. laptops are becoming more functional, if not lighter, and so on.nomadicity is multidisciplinary. there are no renaissance people around who can deal with all the technologies involved here. you have to start off with nanotechnology and move all the way up to multimedia applications,including everything in between, to deal with this world. a multiinstitutional effort or, at least, a multidepartmentaleffort, is required. almost anything you do will be good because everything right now is bad.you thought interoperability was a problem before. now, however, people move around and suddenly appear5,000 miles from where they last appeared on the net. now what do you do with them? who are they? are theywho they say they are?middleware is an area in which a lot of improvement is needed. many people think of nomadic computing aswireless support alone, but this is just one component. you have all the problems and the fascination of nomadiccomputing without wireless ever entering the picture. most vendors are putting out products that work in littleniche markets and do not interoperate. i think that former computer science and telecommunications board(cstb) member david farber can attest to this. he always has a new device. these things do not interoperate.what is the mean lifetime of a device that you own before you replace it with something else, david? two months.that is a david farber unit, a dfu.since a lot of new research problems have emerged, anything you do will be of immediate, practical use.there are many reasons for getting excited about this field and understanding what is going on. in essence, thenomadic environmentñyour computing, communications, and storage functionalityñshould automatically adjustto everything you do in a way that is basically transparent to you. what components should be transparent? itshould track you, for example, wherever you are. what you do should not depend on where you are located. youshould always see a similar image.what communications device do you have? is it a pcmcia card, a modem, or an asynchronous transfermode (atm) connection? again, you should not have to think too hard at the user level. the system should adjustitself to your circumstances. how much communications bandwidth do you have? this is one of the biggestvariables in the nomadic world. it goes from zero up to gigabits, and this can happen very quickly. it is not smoothin time or in number. the system has to understand this. you might see a difference in performance, but notnecessarily a difference in what you have to do with the keyboard, or in your head, or with wires, plugs,configuration, or rebooting.the ability of the user to be disconnected is one of the main attributes of nomadicity. you have to understandyou are going to be disconnected, but you should not have to do much about it. you should act as if you are stillconnected. this would be the ideal. it requires that the necessary systems support be provided.also, you should be able to perform operations now, behave as if they are happening now, but have themactually happen later. for example, if i update a file, it may not actually happen for 10 minutes or 4 hours, but ihave updated it as far as my operations are concerned. it should be independent of the particular computingplatform that i have and, of course, whether or not i am moving.this is where wireless comes into the picture. if i want to use computing and communications while i ammoving, i need something like wireless. all the other issues come into play and have an impact without the needfor wireless. suppose i am sitting at my desktop computer, and i make a simple move and walk to my conferencetable and sit down at a laptop computer. i have now made a fundamental nomadic move. my platform is different.my communications are different. my screen is different. the keyboard might be different. yet the environmentthat i would like to see should not change that dramatically. i should not have to say, òoh, now i am in thisoperating system, and i have a different thing to do here.ó it should travel with me. notice it is a 10foot movefrom my desk to my conference table. how many elements on charles seitzõs world map would that be? howmany transistors cross over in 10 feet? less than one, and yet it is a nomadic move. i would like that to operatenot only in an office, but worldwide. i should be able to move around and have all the functionality follow me.if you change your view, todayõs systems treat radically changing connectivity and latency as a mistake. thisis a failure of the system, an exception. we design our systems to handle it as an exception. in a nomadicenvironment, this is the usual case, and you have to design your systems from the beginning to handle it. you haveto assume you are going to be disconnected or facing radically changing latency often.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.38defining a decade: envisioning cstbõs second 10 yearswhat are the components of the system design you have to worry about? well, here is a list of the usualcomponents: bandwidth, latency, reliability, error rate, delay, storage, interoperability, user interface, and cost.no surprises here; these are the usual suspects that we always worry about. however, there are some other thingsyou have to worry about when you move to a nomadic environment: physical size, weight and processing power(how many pentiums can i put on my lap?), battery life, communications, interference in the radio world, anddamage. moreover, these technologies are portable, which makes them easy to lose and subject to theft. i am sureyou have read about the latest european scam that has now hit the united states. you are at an airport. you pullyour luggage with the laptop attached. you reach the xray unit at the security gate. one person goes through thexray machine and his cohort is directly in front of you. when the cohort gets there, he fumbles and takes a fewminutes to get through. meanwhile, your laptop is on the way through. the first person gets it and disappears, andyou have lost your laptop. this is a very easy way to lose a machine. do not take your laptop to airports unlessyou watch where it goes.nomadicity exacerbates several concerns: disconnectedness; variable connectivity, either because the worlddoes it for you or because you choose to move and use some other communications medium; latency, which variessignificantly; variable routes in virtual circuits as you move around; variable requirements that you put on thesystemñwhat you expect and need; replication of resources such as files, machines, databases, and applicationsbecause you are moving; and foreign languages.when you go to a new environment, you have to locate the power supply. where is the modem? where is thehighresolution screen or printer? you must become familiar with the environment. conversely, the environmenthas to become aware of you. it must know that you are there. it should know your profile and what you want andshould send the things to you that you expect. as the bandwidth and the platform capabilities change, the systemshould adapt what it sends to you. for example, it should compress some things and not send you highresolutionvideo. perhaps it will send only the name of the movie, if all you have is low bandwidth. this is a major issue.most of all, nomadicity is one of the ultimate problems in distributed systems.what should we do? first, develop the systems architecture and network protocols. we need to interoperatebetween the wireless and the wired worlds to handle the concerns of unpredictable user, network, and computingbehavior and to provide graceful degradation in all of this (simple problems!).second, we should develop a nomadicity reference model. we have heard today about one of the referencemodels that cstb produced (figure 5.1).1 it has various names, the hourglassshaped open data network (odn),with a control level referred to earlier today as the bearer service. the odn could provide a reference model.suppose i wanted to send some email. in one environment, i might connect to a cellular bandwidth source. afew minutes later, i might find that i can use a modem. maybe i will walk into an office and get a 10megabytelocal area network, and maybe i will get a 150megabyte atm (if that ever appears in the office). as i moveamong these choices, what do i have to do? well, right now i have to do a lot of things. i have to plug in a differentpcmcia card, reconfigure, reboot, and put down some ip addresses. i do not ever want to have to do this. i wantsomething to do it automatically for me. this is one of the things that a nomadic system design should be able totake care of.another architectural model might be a more standard oneñnetwork infrastructure, support, and middleware.hopefully, we will achieve integrated nomadic support as opposed to velcro integration, which we have today inmany of our beltladen devices. we happen to have one of these models at the university of california at losangeles. we filled in some of these architectural pieces (figure 5.2). it has happened in a number of locationswhere you provide connectivity management, some file synchronization, and update schemes.what else needs to be done? you have to understand how the system works and develop performance models.there are a lot of choices here. you can make a mathematical model, but the mathematics is not that strong. youcan do numerical evaluation, but then you run into an exponential explosion of computation. there are iterativesolutions, but then does the darn thing converge? simulationñit is hard to search a large space. emulationñit is1computer science and telecommunications board, national research council. 1994. realizing the information future: the internet andbeyond. national academy press, washington, d.c., chapter 2.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.computing and communications unchained39figure 5.1a fourlayer model for the open data network. reprinted from computer scienceand telecommunications board, national research council. 1994. realizing the information future: the internet and beyond. national academy press, washington, d.c., figure 2.1.open bearerservice interfaceelectronic mailvideo serveraudio serverteleconferencingremotelogininformationbrowsingfinancialservicesinteractiveeducationfaximageserverapplicationsmiddleware servicestransport services and representation standards(fax, video, audio, text, and so on)filesystemssecurityprivacyname serversstoragerepositorieselectronicmoneyservicedirectoriesmultisitecoordinationodn bearer servicenetwork technology substratelayer 4layer 3layer 2layer 1lansatmwirelessdirectbroadcastsatellitedialupmodemspointtopointcircuitsframe relaysmdsdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.40defining a decade: envisioning cstbõs second 10 yearssloppy, ugly, and expensive (building the system and measuring how it behaves are certain to bankrupt you). theright answer is probably some hybrid mix to do the part that works best in each environment.consider adaptive agents (figure 5.3). the classic assumption is that there is a big fat network between clientand server. however, it is not clientserver, it is clientnetworkserver. the assumption is that you put a lot of stuffhere because a big network will deliver whatever you need. in the nomadic environment, though, the network maybe thin or zero. as the network skinnies down, you may want to move some functionality around in anticipationof a need for tools and data that you do not have with you. adaptive agents at the middleware level are also calledsurrogates, proxies, helpers, and knowbots. we need a kind of theory or formalism, an architecture, a language foragents. they should help the nomads, the applications, the network, the servers, the communications devices, andthe computing devices. they can sit everywhere. in a peertopeer application, or maybe a clientserver application, agents may help to decide what goes on. we certainly need some of these adaptive agents inside the networkas well to do the thinking for us, the compression, the connectivity management, and so on. this is the challenge.so what do you observe from all of this? nomadicity is here, you cannot escape it, and the needs are real. itmakes every problem you ever thought about much harder. each one gets an order of magnitude more difficult inthis environment. it is a fascinating area (i think). the payoff can be huge. there is a severe lack of anyintegration, and there is chaos and confusion right now. you simply cannot afford to ignore the challenge ofnomadicity.where will the next innovation come from? ask your childrenñthey will tell you, we wonõt.john majori had a professor years ago who taught me that it is key to watch the moments in life when big thingsñnewthingsñhappen, because important things will follow. words matter. when was the first time you heard the wordpicosecond referring to the speed of operation and gigabyte referring to storage capacity, or other words like those?while leonard kleinrock and i were getting acquainted to do this presentation, he pointed out that he had justobtained a new laptop with two 1.2gigabyte hard drives in order to have enough information when he isfigure 5.2architectural model for integrated nomadic supportdeveloped at the university of california at los angeles. reprintedwith permission from leonard kleinrock. copyright 1996 by leonardkleinrock.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.computing and communications unchained41ònomadingó around. this was the first i had heard of two 1.2gigabyte hard drives in a laptop, which is seriousmobile computing. so i started to think that the vision i have of the virtual world will be well received.i told leonard about my concept of teleconferencingñthat people at some point will travel virtually and nolonger physically. in fact, that is the title of this talk, òcreating the virtual world.ó with all sincerity he said,òthat will never work!ó so if i could not convince someone with two 1.2gigabyte drives in his laptop that we aregoing to have a virtual world, perhaps i do not have much chance with the rest of you. the point is that this is adifficult concept. there is a lot to gain, but also a lot needs to be developed to make it possible.does anyone remember the 1987 movie planes, trains, and automobiles, with john candy and steve martin?figure 5.3adaptive agents. reprinted with permission from leonard kleinrock.copyright 1996 by leonard kleinrock.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.42defining a decade: envisioning cstbõs second 10 yearsi travel a lot. i find it impossible to watch this movie all the way through because it hurts too much. everything badthat has happened to me on one trip or anotherñnot having money, the rental car not working, and the airplaneschedules being unreliableñmy whole life is being played out before my eyes in 120 minutes. it is a great movie,but it encapsulates every bad travel experience, and as such, it humorously captures part of the need for a virtualworld.you would think we would be highly motivated to change this. in fact, when i committed to this concept ofdescribing the virtual world, i figured i would just go out, grab a few books, and catch up on the emerging theories.then i would explain it to the audience, leave with loud applause, and feel very comfortable with myself. idiscovered that, today, there is no agenda to create a virtual world. in fact, on some level, most of us believe thatwhat we go through when we travel is important, part of what we do, and necessary to our society. in other words,we feel that there is no other way.yet, we know that travel is inefficient and that it significantly lowers productivity. it also costs a lot of money.the last time i looked, the federal government was spending about $40 billion a year to maintain the highwaysystem. office buildings cost money and isolate workers from their customers. meanwhile, we are competing ina global world. if i believe it is important to be able to walk in to see my motorola team and rally their spirits andconvey information to them, and i do this for the team in schaumburg, what about the team in bangalore? whathave i done about our lab in australia?if i think that being physically there matters, i should never go homeñsimply fly forever. well, you cannotfly forever. with a truly global company, you cannot be in enough places. all of your time would be consumedin airplanes. so there is something wrong if you really want to run a global corporation or have a global team beas productive as it should, unless you are able to create, in some way, this virtual world.how do you do it? how do you eliminate physical travel and commuting? how do you enable virtual traveland commuting? i will offer that it is not going to be via email, fax, teleconferencing, or videoconferencing as weknow them today. these represent a beginning; they are important steps, but they will not solve the problem.they may not even be reflective of the form of the eventual solution. to illustrate this point, imagine if lowcostrobots had been invented by 1940 and were ubiquitous. then someone said, òi am going to invent the dishwasher.ó can you imagine what that dishwasher would look like? it would look like a robot doing the humantasks for washing dishes. as it was, there were no lowcost robots, so someone had to be very innovative. today,as a result, dishes are washed in the dishwasher very efficiently, very conveniently, and completely differentlyfrom the way people wash dishes. this is the kind of fresh thinking that may well be required for virtual travel.we have already seen some examples in communications of how new technology and uses change oldparadigms. today we have email, which is in no way an analogue of what we saw before in the old teletype andtelegram era. it is a completely new embodiment of messagingñtransitional, to be sure, but very different.remember centralized fax? one of my favorite memories is the time i suggested that i should have a fax in myoffice so i could communicate better with my colleagues around the world. i got a stern note from corporate,pointing out that fax was something that we had to be very careful about. there was an office you could go to ifyou wanted to send a fax. it was a couple of hundred yards from where i was. this centralized fax analogue hasnothing to do with how we use fax today. so we have been able to step into new paradigms, but they are alwaysvery difficult to envision. in 1976, jack nilles said, òtelecommuting is the substitution of telecommunications fortravel.ó this was the first time that the term was used. in 20 years, not much has changed, but there has been someprogress.there is some work being done in this area. there is some hope. there is some evolution. bellsouth hasmade a prediction that by the year 2000, 25 percent of corporate employees will, in fact, be working outside theirtraditional offices. by then, we will have evolved to the point at which a substantial amount of the work productwill be separated from the physical office.there are signs of progress. the growing use of email is one. we are, in fact, learning to work as a globalteam with the primitive tools we have today. at motorola, we have 140,000 employees and 120,000 computers.since i manage this activity, it begs another question i am frequently asked by the chief executive officer, how canmotorola have almost one computer per person when well over half of our people work on assembly lines and havenothing to do with computers at all? part of the answer is that we use these computers to run our factories, but thedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.computing and communications unchained43majority of the answer is that we are committed to staying in touch on a global basis, so many people have morethan one computer, and there is a serverintensive network to support such usage.at motorola, we will soon be able to send a message to anyone anywhere in the world for less than a penny.it is a new kind of connectivity and convenience. we also have more than 100 videoconferencing centers.everyone hates them; they are not natural in use or delivered image, but they are used regularly and they allow anearly form of global group connectivity.there are people such as leonard kleinrock coming forward with concepts of the kinds of structures andoperating systems needed in order to have this sort of nomadic use of information technology. this is a nontrivialand very key next step.there have been other relevant inventions. people can now do threedimensional imaging on pcs with arelatively high degree of clarity and in a relatively simple format. people are developing fisheye cameras and themathematical processing that supports them. with these, images can be gathered by fixed imaging equipment andthen reconstructed to your view, not the cameraõs view. the technology also exists to do similar things with imagesbeing reconstructed to give a rolling view from a set of still images. there is a camera product available that, basedon the way you are looking through it, allows you to determine how the camera focuses and frames its image.there is probably a signal here of something very important for the emerging virtual world. people are beginningto see the kind of image manipulation that will be important to a virtual world.we are starting to see more teleworking. the united kingdom claims that about 7 percent of its workers dosome type of teleworking. this figure includes around 2 million britons who earn their living that way. governments around the world are encouraging this trend. in the united states, the environmental protection agency hasa goal to reduce automobile commuting by 20 to 25 percent. the 1990 amendments to the clean air act requiredthat any company in an urban area with more than 100 employees find a solution to commuting to reduce thecommuting load.slowly, but surely, we are starting to see processes that will enable virtual travel and commuting to occur.bellsouth is involved in this in a big way. many of us were in atlanta for the olympics. atlanta has a bigproblem. even without the olympics, with its road system, you cannot get from one place to another today. if youadd several hundred thousand people, it does not get better. so bellsouth took on the challenge of the olympicsand used it to develop a mindset in atlanta that there must be another way to get your work done. they ran hugeads showing atlanta traffic as it is normally, and at the bottom they asked the question, how will your employeesget to work during the olympic games, when they cannot even get to work now? these ads were designed tomotivate people to care about how they were going to get their work done when they could not get to work. thesolution is teleworking, and this vast experiment will advance the cause.at&t has also jumped on the telework bandwagon and begun to do some interesting things. it is pushing theconcept: forget the traditional work environmentñ let us move to a nontraditional work environment. its phraseis òanytime, anywhere.ó at&tõs idea is that a substantial portion of the work force no longer works in thetraditional office, and this number is growing on the order of 20 percent annually.i have an interesting example of this. the man who manages the sale of ibm computers to motorola sells onthe order of $50 million worth of equipment each year. clearly, this is a very successful account executive. still,he does not have an office or a secretary. ibm has embraced teleworking to make its sales teams more effective.he works out of his home with his laptop. if he does not spend time in his office, where does he spend his time?he spends his time in our offices. he is talking to his customers and making himself more effective. thisparadigm could not have existed in 1990, but it exists today and is successful. ibm is pushing the flexibility, lackof boundaries, and portability that will make this model very successful.what are the drivers for change? first, the technology is making it possible. i had the pleasure of being at thegeneva telecom conference when intelõs president and chief executive officer, andrew grove, presented theprocess of global telecommuting and global networkingñchildren talking to children around the world (todayõsversion of pen pals in a global village), doctors in rural hospitals talking to doctors in urban hospitals. it is verypowerful and an early indication of what we might be able to do.corporate restructuring and reengineering are also important factors. at motorola, we do not have the luxuryof providing a secretary for each individual. we can no longer afford to move everyone who is on a team into onedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.44defining a decade: envisioning cstbõs second 10 yearslocation. we find, in some cases, that it is more efficient to have people stay at their normal workplaces. we havesome early data related to the work of our software teams. in fagen method software reviews, we analyze thesoftware for potential errors before we actually write it. we want to find the mistakes before we create them. thisis a fairly standard, emerging process in software; it is done as a team. we found that if we have these meetingson camera, they are more effective (by on camera, we mean that people can even be in adjacent cubicles; the screenis now split so that each of these people can be seen, as opposed to being in a conference room). that is, peoplework better out of their own workplaces, meeting in a virtual meeting room environment. admittedly, whatsatisfies the needs of the software engineer for interpersonal contact and what would satisfy the same needs ofpeople in other lines of work might be very different. this is, however, at least a signal that there may besomething of value here.at motorola, we have shifts in labor pool availability. one of our key software teams works in india. if weflew the team back and forth to the united states for weekly or monthly meetings with other teams, there would bevery little time left to get any work done. clearly, we need a better answer, and teleworking is providing theanswer with email and highbandwidth videoconferencing. additionally, there is the problem of how to retainskilled resources. sometimes the person you want to transfer or hire does not want to live where your business islocated.there are other examples of progress. today, at&t claims to have 40,000 parttime telecommuters, 12,000mobile workers, and 5,000 fulltime athome workers. i, myself, am trying to stay out of the office one day a week.now, how do we get the job done? how do we create the virtual world? first, someone, everyone, has to startbelieving. people have to start getting a sense that all this travel is insane. people need to say things such as: òiam sitting in my fifth traffic jam this week.ó òi have just passed 200,000 frequent traveler miles; united loves me,but my family does not remember me!ó the waste of time and resources and the pollution created can and shouldbe stopped. someone has to start saying, òthis is wrong.ó there must be a sense that there is good reason tochange. it will not happen overnight, but this consensus is building.then we have to start allocating to virtual highways a portion of the $40 billion that is currently going intophysical highways. if 10 percent ($4 billion) were taken off the top and spent at universities, it would be a start.with this resource, the universities could spend time figuring out how to bring together these anecdotal developments to effect change. in addition, we must continue to encourage ògreenó legislation to discourage physicalcommuting.the federal government, particularly the army and air force, is running massive global organizations. itwould be useful to plant the seed in peopleõs minds that these organizations would run more efficiently if they hada new technology. similarly, global companies such as motorola must continue to challenge what the technologycan deliver, as well as begin to develop and market products that can deliver the virtual environments that areneeded. it is time to start convincing people, companies, and government that they would be more efficient if theyhad better tools.these are my thoughts on creating the virtual world.discussionneal laurance: i think in some sense you are trying to swim upstream. in the course of the management development in the past few years, we have seen management practice go from a hierarchical òfind out whatthe boss wants, and then everybody get in line and do it,ó toward a much more collaborative environment, whereyou try to make use of everybodyõs talents and get everybody moving in the right direction. this process of gettingeverybody moving in the right direction takes a lot of human interaction. this has been the real limitation in tryingto use some of the technology. maybe what we need to do is to focus on what substitutes for body language in thisvirtual world. how can we convey this and get the consensus building process really captured in here. what areyour thoughts on this?john major: i absolutely agree with you. we cannot step back. we are not going to give up teaming. thepoint i tried to emphasize is that teams are no longer located within 50 feet of your office. my team is located allaround the world. motorola is in no sense atypical in this. many of our companies have massive investments indefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.computing and communications unchained45countries around the world. so i do not think we should shrink from the magnitude of the problem. we mustunderstand that interaction may be a critical piece of it. there is surprisingly little work at understanding what theissues are.robert bonometti: a few weeks ago, the federal communications commission had what i think isgoing to become a historical notice of proposed rulemaking whereby it proposes to take 350 megahertz ofspectrum and allocate it for a socalled national information infrastructure band or a supernet band or a hyperband,depending on whose buzzword you like. what are your thoughts as far as how this may be a critically importantenabler for nomadic types of computing?leonard kleinrock: well, the problem of getting at the nomad when he is out of harmõs wayñforexample, near fiber or wireñis a major one. certainly the wireless spectrum provides a solution, be it satellite orlocal wireless. i think this is very important. i am anxious to see how the technology buys into it and what thedevices are. this kind of freeing up of spectrum is important, but i do not think it is the most important problem.i think the systems issues are far more difficult and have to be developed first, but this is a component.david farber: a couple of months ago the dean at the university of pennsylvania described me as hisfirst virtual professor. he said it with pride in front of the board of trustees. with respect to johnõs comment aboutthere needing to be some people supporting change, i think you are slowly beginning to see this when a largelyliberal arts institution can absorb the concept without collapsing.let me make a couple of comments and then ask a question. when i travel (i am probably one of the mostgadgetprone people around, courtesy of john and others), i tend to have a lot of gadgets. half of my luggageconsists of power supplies, batteries, everything incompatible with each other. so i would argue that there is a realneed for standardization. it is pretty ridiculous when i have to carry five power sources so i can keep running.i think that you alluded to a serious problem, lenny, but let me see if i can draw you out a little bit. one ofthe big problems is the change of context every time i move from device to device, every time i move from onebandwidth package to another. i am sitting in an office, allegedly in the future, watching you via video link. i walkoutside to my car and get on the road; i am on the way home, and i arrive at home. each time that i pass throughdifferent bandwidth domains, everything seems to want to change. i think it is the discontinuity that i, at least, findvery, very difficult to handle. i have to carry too many models in my head. do you think this is going to change?kleinrock: i think it will improve. it will never be as ideal as you would like. there will be ghostlyimages of me following you one way or another, be it by voice as you are driving or via a still picture of myunshaven face as you are shaving. there will be components like that. i think you are quite right. thediscontinuities in bandwidth should be smoothed out somehow. one can anticipate this. one can, in fact, cachesome of it. predictive caching is a big component here, and it is very hard to do. this is why i say that there area lot of good research problems, but these issues are key.major: i think lenõs concept of the intelligent agent that rounds off some of the edges and, in fact, modifiesthe communications environment automatically in response (where you simply get the headline and then can probefor the rest of the article) is the kind of concept that could be really contributory. to me, it is a fresh thought thatthe middleware community has not touched so far.patrice lyons: i was interested in your reflection on trying to liberate a few billion dollars from localand perhaps national highway programs to support the development of infrastructure. well, if you had a bag ofmoney and were a local government official, what minimal elements would be at the top of your priority list toencourage the kind of things you are thinking about?what comes to mind from some of my legal debates in the last couple of years is the question of what is theproper role for government in a hierarchy of certifying authorities? for example, if you are a lawyer working ina suburb, your main office is downtown, and you are discussing things of an extremely confidential nature, youwould want to make sure that there was some minimal integrity in the communications pathway. perhaps thiswould encourage more people to do so. what would be the top one or two things that you would think about?major: i will suggest that i do not think there is a quick answer here. i think the first pile of money has togo into basic research to understand the nature of the problem and free up the solution set from what would appearto be the direct path to what i describedñto what might be a successful path to solving the real problem.jerry mechling: one of the problems we faced with the london project was about 200 to 300 peopledefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.46defining a decade: envisioning cstbõs second 10 yearsaround the world who were not paid by us. they were like a network of different governments, universities,different entitiesña complete mix. we were going to standardize on some collaborative work. we were going todo lotus notes. we decided not to, because the training requirements became too difficult and you had to fly themall someplace to get trained anyway. if you used video, you had to have all this language capability to translate,which was a problem because you had to explain the translation of the words. it became such a quagmire to figureout whether there was a way to standardize that we ended up with a network that was a complete hodgepodge ofeverything from pcs to fax machines to the most advanced systems.is there anything in the works that is going to address what i think is a problem people are not looking at: howdo you conduct the collaboration when the players are not in the same organization? if it involves just onecompany, you would make a corporate decision, but you cannot do it in an interagency deal.kleinrock: from a technology point of view, i cannot give you any answers to that question. i mentionedforeign language, the one you selected as a problem. sure, translators will help, but they are a long way off. i amnot sure what technology will overcome that problem. i think it is too early to standardize on a lot of this. we arejust probing now; i would be frightened to do it at this point.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.47henry fuchsin this session, we will give you three snapshots of our favorite things in graphicsñthe views of three randomfolks that are not necessarily representative of the field. i will talk about displays, virtual reality (vr), and thingsthat interest me; andries van dam will talk about user interfaces; and donald greenberg will talk about rendering.the first thing i want to talk about is graphics in a nutshell since the 1980s (box 6.1). here is my mantra: 2d(twodimensional graphics) has become ubiquitous. that is, 20 years ago, 2d was the cult; 10 years ago it hit themainstream; and now you cannot get a machine without 2d graphics on it. if you want to get a 24 by 80 textonlyscreen, you have to try really hard. in some sense, what has happened to 3d (threedimensional graphics) is 10years behind 2d. it was still a cult 10 years ago; now, of course, you do not need to be in the cult of 3d in orderto do 3d graphics in your work.some people say that vr, immersive displays, and interactive and immersive environments are also on thisroad, but i am a skeptic.speaking as a person who has been doing vr for 25 years, i think that the best recent development for vr isthe web coming along to take vr off the front pages. until about a year ago, we had to spend a lot of timeñandi suspect a lot of you who are working anywhere near the field spent a lot of timeñjust shooing people out of thelab in order to get anything done. in a nutshell, vr has been around doing useful workñby useful, i mean thatpeople are willing to pay money for it when they do not have any alternativesñsince at least 1970, in terms offlight simulators.in the past 10 years, the military has spent lots of money on training both individual pilots and large groups ofpeople, since it is not practical to train for all situations with real equipment. also, the entertainment area is alwaysgoing to be around, but with a nebulous future. however, i want to give you a couple of examples from medicine,an application area in which you can show incremental improvements that give some idea of what might be morewidely available in the future (box 6.2). two examples can be presented using videos.the first video is from kikinis, jolesz, and lorensen.1 many of you are familiar with their work at brighamand womenõs hospital in boston. the interesting part of this work is that they are using graphics, and the mergingof graphics with the patient, as a standard procedure nowñnot just in the planning of neurosurgery, but as a guide6picture this:the changing world of graphicshenry fuchsdonald p. greenbergandries van dam1videos were shown during the symposium.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.48defining a decade: envisioning cstbõs second 10 yearsor a road map for the surgery itself. this is a planning system for neurosurgery that, of course, starts withcomputed tomography (ct) or magnetic resonance imaging (mri) scans. the novel aspect is showing the resultsregistered on top of the patient. this guides the surgeon in deciding where to cut the hole in the skull and, onceinside, how much tissue to remove.now we have systems that combine graphics with something in the outside world. my theme today is that wecould push that capability forward for the rest of us. the next video provides an example of how to do this forapplications in which the medical imagery is coming to you in real time. for guidance, you could use not only theold data, but realtime data as well. this is similar to the previous application, except now you are using the realtime imagery as part of the display in order to guide what you are doing.in the next 10 years, we will find that the use of 3d graphics is increasingly commonplace. there will be amerging of the live video with the display. you might well ask what we need live video for in our everyday lives.i hesitate to tell you, but i think it is going to be the year2000 version of the picture phone. i say this despite thefact that some of you might laugh, because picture phones have come around about every 10 years and so far havebeen disappointing to just about everybody. i expect that people are not going to be disappointed 10 years fromnow. there will be enough bandwidth, display capability, and computing to make teleconferencing a compellingshared presence, which the current generation of teleconferencing hardware cannot do. my personal view is thatthe crucial aspect of shared presence is going to be the capability to extract a sense of 3d from one place and showit at another place in a way that makes participants feel they are together, even if they are thousands of miles apart.we are going to have to do what you might call desktop vr for modeling and 3d teleconferencing. in 10years, i think we will have the equivalent of, say, a dozen laptops packaged within a single workstation, with abouta dozen times the power and display capability of a laptop (more pixels (perhaps 10 million), higher resolution, anda wider field of view). think about a picturewindow kind of display made up of, for example, 10 or 12 laptopsizescreens and 10 or 12 cameras. these cameras will use the workstationõs computational resources to extract amodel of the small environment you are inña little office cubicle, for instanceñand transmit that model to anotherplace. in this way, together with headtracked stereo, you could share the sense of presence with the person you aretalking to or the two or three people you might be having a teleconference with. i believe the principal problemremaining is how to display these remote environments, because their data representation is probably not going tobe polygonal. this is going to be the most exciting frontier: modeling from imagery and warping the images insuch a way as to make the output image seem more realistic. a lot of people are working on this. the 1996siggraph conference, for example, featured many papers on how to take imagery, warp it, and correct it fromanother perspective, making the output image more like a photograph than a picture of a set of polygons.box 6.1computer graphics since 1980a.2d became ubiquitous¥desktop metaphor¥desktop publishing¥computeraided draftingb.3d: from cult to mainstreamc.hot topic: virtual realityñso widely claimed as to encompass nearly all of interactive 3d graphicsbox 6.2virtual reality in medicine: examples¥neurosurgical planning and guide¥visualization of internal anatomy from preoperative patient imagery¥merged visualization of realtime internal anatomy and external views¥surgical training with simulatorsdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.picture this: the changing world of graphics49these are my conclusions. we are going to have some automatic model extraction from video sequences, butnot model extraction in the intelligent sense. the system will not know what the object is, but it will know wheresurfaces are in your little office cubicle. we are going to have desktop vrlike environments of what you mightcall a 3d window into another world. in addition, and totally separate from this, we will have laptops evolving intowearable computers the size of pagers; we will keep graphic displays with us all the time as we walk around.much larger, higherresolution images in the form of tiled displays will become commonplace. the capability willbe far beyond the same 1,000 by 1,000pixel displays we have had for the past 10 or 15 years.donald greenbergwhat has happened in the computer graphics industry in the first 25 years that i worked in it? the first systemi worked on was a general electric machine, trying to simulate the lunar landing vehicle docking with the apollospacecraft. it was as far as we could go in the 1960s: there was no lighting model and the colors were assigned to3d geometries. this was the extent of the complexity that you could display in real time to try to train theastronauts in a nongravitational environment. about five years later, the graphics pipeline started. without goingthrough the details, you have a model; you have to describe where you are looking at it; you do a perspectivetransformation; you convert that to raster operations; and in the process, you do the lighting, put it into storage, andthen display it. around 1972, researchers at the university of utah developed the fong model. they never couldsee a full picture; there was only enough memory to see 16 scan lines at one time. the model was polygonallybased and able to show diffuse reflections, transparency, and specular highlight. that was the basis of the graphicspipeline then; it is the basis of the graphics pipeline today.the next step occurred around 1979. turner whittick from bell labs introduced ray tracing, which bypassedthe camera perspective in raster operations. that is, you would send the ray through every pixel in the environment, compute where it reflected, pick up the accumulated light contributions, put an image in storage, and displayit. the time required for an image as complex as his first publicly presented image was measured in days on avax 780, a 1mips machine.we have progressed a lot; we can do pretty well today even with complex environments, generating a picturewith, for example, 32 million polygons in it. we are trying to see what happens as we start to get into complexenvironments, because the old algorithms begin to break up. we can generate the 32million polygon image ishowed only on machines that have 768 megabytes of local storage.interest in making pictures for the movie industry led to advances over ray tracing. using thermodynamics,we tried a radiosity approach. instead of doing millions and millions of rays, we took an environment, broke it upinto thousands of polygons, and solved the interaction equations among all of those polygons. this is not a simpleproblem because, in contrast to finite element analysis, where there are nearestneighbor problems, this approachrelies on the global interaction among every part of the environment. there is a fully populated, nondiagonallydominated matrix that must be solved. in complex environments (for example, an occupied meeting room), thesolution can exceed hundreds of thousands of polygons. once solved, if it is for diffuse environments we nolonger have to compute the lighting and we can use the graphics pipeline.so where are we headed? well, part of the problem we have hadñand it is our own faultñis the fact that wehave been too successful in making goodlooking images. we cannot really discern whether the picture we arelooking at is real or synthetic. if we think about what is happening now and what will happen in the future, wehave to look at graphics rendering, at least, as a modeling of the physical world. once we have modeled thephysical world, we have to create the image of the physical world, and then we have to see how the human mindwould evaluate it in the perceptual world.beginning with a model of the physical world, which has not only the geometry but the material properties ona wavelength basis (the light on a goniometric basis with its full distributions), we can do the correct energydistribution throughout the environment and find out what radiance is coming from every surface in the environment. then we want to create the picture from where the camera is. our camera really is our eye, which is madeup of curved lenses, retinas, and so on. we have to create the perceptual image, taking into account the perceptualdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.50defining a decade: envisioning cstbõs second 10 yearsresponse and the physiological change. then there is the interpretation, where the cognitive is not necessarily justwhat you see in the image; this is a tough problem.so how do we start? well, we take off our computer science hats and go back to maxwellõs equations fromthe 1800s. we try to get a simulation of the reflection model, which includes the angle of incidence, thewavelength, and the roughness and distribution of the surface propertiesñhow much it absorbs and how much itreflects. we get an arbitrary distribution in directional diffuse and specular, a very complex fivedimensionalproblem. we have to do this: we want to have the model because we do not even have the measurements of mostmaterials.i want to go back to a comment that edward feigenbaum made. the problem we have in computer graphicsis perhaps a problem in computer science in general; that is, we forget the engineering and experimental part of it.what we really have to do is test these paradigms in terms of their computation complexity. we have to do theexperimentation to make sure that our algorithms are correct. we then want to distribute it, but in fact, we candistribute it right now only for diffuse surfaces because the rest is too complex when using the radiosity algorithms.at cornell, we have set up a $500,000 measurement lab in my laboratory to try to measure bidirectionalreflectance distribution functions. we have come up with an approach using particle tracing, not the correlatetechniques described by physicists years ago. now we will send out billions and billions of photons, determineevery surface they hit at every wavelength from every direction, and then try to accumulate this statistically (wenow have bounds on the density distribution functions and their variance). then we will be able to start to displaythese nonexistent environments by sending out these billions of photons. if we go into our measurement lab, buildthese environments, compare them, and then find they are correct, we will know that the simulations are accurate.we also model the human eyeñthe reflection of the cornea, the lens, and the scattering of light inside thematerial so we can get the glare effects. we model the adaptive perceptions so that we see what something mightlook like in moonlight versus daylight when, in fact, our spatial and color acuities are very different, depending onwhether the rods or the cones are involved. then we build models, simulate them, and assess what is real and whatis false.so what is going to happen in the future? it is clear that as the environments get more complex, we are goingto have more polygons. we are going to have in the neighborhood of 1 million to 2 million pixels, so the ratio ofpolygons to pixels or pixels to polygons is going to approach one. it started off with the pipeline. there were 400pixels per polygon. then it went to 100, to 50, and now down to 10. in the future, the balance point is going tobe pixels to polygons, and i contend the graphics pipeline is going to go away.because we will have to disambiguate complex environments, we need all the perceptual cues, or shadows, ofthe interreflections. we will need global illumination algorithms, which are going to be common. as andy vandam said, we are going to need the progressive algorithms so we can get everything in real time. we will need tohave a transition from wysiwyg to wysiwii. the former refers to òwhat you see is what you get.ó the movieindustry is very happy with this because as long as the image is believable, the industry does not care whether it isreal or not. i claim that it has to be òwhat you see is what it isó (wysiwii) so that we can start to use these thingsto simulate what is going to be.i would like to make three closing comments. first, with increasing processing and bandwidth, the realtimerealistic displaysñthe òholy grailóñwill occur within the next decade. if bandwidth is cheaper than processingand memory, then the clientserver or clusterofcomputing paradigm will hold. if processing and memory arecheaper than bandwidth, then perhaps we will have local computation and display. in either case, however, we aregoing to have realtime, realistic image generation.second, i hope we will reach the stage soon where simulations can be accepted as valid tests. we do autocrashes that way. we do not build chips by building one and testing to see if it works. we test it on a simulatormany, many times, so that on its first computation, it will work. at least in the design of automobile bodies orarchitectural interiors, for example, why canõt we make sure that what we are going to see is what it is going to be?again, the problem is that we have been too convincing in our images. if we start to believe in these tests, wecould also start to use our simulated environments to improve the inverse rendering approaches and some of ourcomputer vision problems.third, i would like to talk about the good newsbad news problem. the goodnews problem is that 90 percentdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.picture this: the changing world of graphics51of the information goes through our visual system. the badnews problem is that seeing is believing, and once wesee the picture, we are liable to believe it. photography will no longer be admissible in court. seeing will not bebelieving, and it is going to be dangerous when we start to mix the real and virtual worlds. a picture is worth 1,024words.andries van dami am going to talk about what i call postwimp user interfaces, wimp meaning windows, icons, menus, andpointing. why user interfaces? because we finally discovered that having raw functionality is not sufficient. ifyou cannot manifest that raw functionality in a usable form to ordinary users, it is not nearly good enough.industries have discovered this and now have usability labs. unfortunately, academia is still lagging a bit. theuser interface does not get the attention that it deserves in computer science curricula, but this is a separate matter.i will talk about what lies beyond the wimp interfaces that we are familiar with today. this was, of course,presaged by michael dertouzosõs plea that we should be much more interested in usability than we are and thatcomputers should be much more userfriendly. this, to me, is an ideal. it was, by the way, considered an idea ofthe lunatic fringe 20 years ago that graphical user interfaces (guis) could make it possible for preschoolersñchildren as young as 2 years of age, i have been told, who can neither read nor writeñto use computers productively. this is revolutionary, but it is just the beginning.what else should we be thinking about? what is good about wimp guis and what are they missing? whatis good about them is ease of learning, ease of transferability from application to application, and ease ofimplementability, thanks to many layers of software. however, not every user fits that paradigm. many people nolonger want to point and clickñthose with repetitive stress injuries, for example, for whom speech is going to befar more important. also, the layers of software support are both a feature and a bug. there is a huge learningcurve in becoming a user interface programmer.i am more concerned about the fundamental builtin limitations of wimp user interfaces. they rest on afinitestate automata (fsa) model where the application sits in a state, waits for an input, and then reacts to it. itis halfduplex at its worst. human conversation, on the other hand, is marked by the fact that it is fullduplex andmultichanneled (that is, you see me gesturing while you are listening to me talking), and user interfaces do notmimic this at all. key presses are discrete, but if we are involved in gesture recognition for handwriting or speechrecognition, all of a sudden input is continuous and has to be disambiguated. the problem becomes far moredifficult as soon as we move outside the boundaries of the user interface.the wimp gui does not appeal to our other senses. it just uses the visual channel for output and the tactilechannel for input. this is not nearly good enough. we would not be able to communicate well if all we had wasvision, so we need to appeal to our other senses. henry fuchs talked a little about immersive virtual reality. youare not about to carry your keyboard and your mouse into an immersive environment where you can walk aroundfreelyñwhere the computer is continuously tracking your body, head, hands, and maybe even your gaze, and iscommunicating with you in a style that is very different from the wimp gui you are accustomed to.so what is a postwimp user interface? clearly, it has to solve the problems of multiple parallel channels andmultiple participants. computing as a solitary vice is largely going to go away and be replaced by teamwork andcollaboration supported by computer tools. a postwimp user interface will require much higher bandwidth thanwe are used to for keyboards and mice. it will be based on probabilistic, continuous input that has to bedisambiguated (consider todayõs experiences with handwriting for personal digital assistants or data gloves forvirtual reality); it will need to have backtracking. the objects, unlike our desktop objects that just sit there andhave no autonomous behavior, will have their own builtin behaviors, and we will interact with them as they areinteracting with each other. twenty years ago, the media lab at the massachusetts institute of technology had awonderful demo (òput that thereó) of somebody who could talk, gesture, and interact with objects on the screenthat did their own thing; this is the kind of future we are moving toward.wimp guis are not going to go away; they will be augmented, suitably and appropriately. i see a spectrumof user interface techniques. on the one hand, there will be direct controlñi, as a user, will be empowered to tellthe application what i want it to do. on the other hand, there will also be indirect control where i can dispatchdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.52defining a decade: envisioning cstbõs second 10 yearsviruses or agents, however you view them, that will operate on my behalf. they will communicate with me inreasonable ways to let me know what they are up to so i can adjust them and make sure that they are not harmful.agents and wizards in primitive forms are already here; you will see them used far more in the future, for betterand for worse, especially on the internet. finally, we are going to have speech and gesture input.a different set of user interface techniques based on 3d graphics holds promise. these involve 3d widgetsand a sketching user interface that employs gesture recognition rather than traditional wimp techniques. the 3dwidget is a specialized tool that lives in the same 3space as the 3d application object. the sketching interfaceprovides a more limited vocabulary of direct manipulation.there are a number of challenges that still have to be met before any of these techniques are used routinely inindustry (box 6.3). one that i can talk about briefly is timecritical computing, which the networking communitycalls quality of service. instead of having algorithms that compute perfectly and take however long they require,we want algorithms that yield a usable result within a given time limit and produce higherquality results if givenmore time. this will enable us to schedule the number of frames that we are able to generate and avoid motionsickness. such timecritical computing requires a new way of looking at algorithms, something the artificialintelligence folks have been doing for a while. object models are another pet peeve of mine. all of the currentobjectoriented programming languages have static, class instance hierarchies. in the 3d graphics multimediaworld, we need dynamic, evolving objects that can change their type and their membership on the fly at run time.this is not supported, nor of course is any kind of input or output, which is all relegated to a library.in conclusion, things are going to change pretty dramatically in the future. change will be driven not so muchby what is happening in our university research laboratories as by the driving forces of the entertainment industry,which is putting enormous resources into making this revolution happen. in the graphics community, we are allvery keen on these new technologies, but we need to conduct usability studies to find out whether they are reallyuseful. this is terra incognita, but it is a very exciting new frontier.discussionwilliam press: i have a question for donald greenberg. an idea whose time comes and goes and neverreally arrives is eye tracking, because people have estimated that the real bit rate into the entire visual cortex isperhaps only a megabyte. does that have any future now?donald greenberg: it definitely has a future. we are making a lot of progress on it. we are alsolooking at the difference in the full view versus peripheral vision, which means that we can reduce the amount ofresolution we need on the slides. henry is conducting the particular tracking experiment.henry fuchs: you characterize it accurately: it is something that comes and goes but has not reallyarrived. i think it is like the picture phone. an optimist will say that when all the necessary technologicalcomponents have arrived, then it will have some advantages. i think it has not arrived because various pieces ofthe puzzle are not in place yet.even when we are able to do eye tracking, for example, we are unable to take advantage of it because of otherlimitations such as the display mechanism itself, the graphic systems, and various other things. my guess is that,in 10 years, there will be a few selected applications, but it will take at least 10 years until we see some part of ithave an impact.box 6.3challenges for 3d user interfaces, virtual reality, and ar¥much better device technology is needed.¥design discipline is necessary.¥a powerful development environment is required.¥òdriving applicationsó are needed that make productive and costeffective use of the technology and drive it.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.53federal research and funding policiesdeborah a. josephi will begin by talking a bit about the breadth i see in computational biology as a discipline, and then identifysome of the challenges i see. discussions at the computer science and telecommunications board (cstb) havemade me realize that some of my more computationally oriented colleagues have a narrower view of biology (andcomputational biology) than i have. some did not take a biology class after high school, while others graduatedfrom college before dna sequencing was something that every undergraduate biology student learns about.computer scientists are well aware that new technology has caused rapid advances in computer science andtelecommunications. however, new technology, both biological and computational, is also rapidly influencingwork in biology.computer scientists often equate computational biology with algorithmic support for genomics and molecularbiology. work in this area is fairly well known by computer scientists, and the computer science conferences oftencontain papers on genome sequencing and mapping. computer scientists are fairly familiar with the problem ofpredicting the secondary and tertiary structure of biological molecules, and applications in drug design. theartificial intelligence community had done some nice work using neural nets and hidden markov models forfinding genetic indicators and genes within genome sequencing data. problems such as predicting gene linkage aresomewhat less well known, but are receiving increasing attention in the algorithms community.however, beyond the areas of genomics and molecular biology, computational biology is much less understood by computer scientists. asked what other topics of research are found within computational biology, mostcomputer scientists would probably think back to its origins within mathematical biology. problems such aspopulation modeling and developmental models (how the zebra get its stripes) would come to mind. some of themost exciting new research in biology that is benefiting from new technology and computational methods isalmost unknown in computer science. i will mention just a few examples.currently, an exciting area of research is the quest for a better understanding of life in extreme environments.this work includes the study of deep sea vents, hot springs, extremely acidic environments such as mine tailings,the ice of the arctic and antarctic, as well as the possibilities of extraterrestrial life. studying the organisms ofthese environments in large part involves studying their dna. a particular gene, the 16s rrna gene, has played7computational biologyand the crossdisciplinary challengesdeborah a. josephedward h. shortliffedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.54defining a decade: envisioning cstbõs second 10 yearsa key role in their classification. computational biology has been central in providing algorithmic techniques forconstructing phylogenetic trees based on molecular sequence data for this gene. these phylogenetic trees can thenbe used to predict the evolutionary relationships between known and newly discovered organisms. using thesetechniques scientists have recently discovered an entire new domain of archaea (bacteria). some of theseorganisms appear to be evolutionarily very old and may offer clues to understanding the origins of life.in other research, computational biologists are building models in areas of biology, ecology, and animalbehavior that have typically been very noncomputational. computational models in some of the areas offerimportant new possibilities, as well as controversy. for example, new models of animal physiology may in partreplace some toxicity and carcinogenicity studies previously done using animals. the acceptable role of computersimulations and models for biological risk assessment is an area of ongoing, and at times heated, discussion.computer modeling has been used for many years in agriculture to predict crop yields. more recently anentire area of information intensive agricultureñprecision agricultureñhas developed. this area brings togethermany tools: variablerate applicators, global positioning systems, databases, plant growth models, and models ofnitrogen and soil characteristics to improve the way we raise crops. through total farm management systems wecan hope to make farming more efficient and to reduce nonpoint source pollution. better models of farms asecological systems will help maintain them in a healthy, efficient, environmentally sensitive way.the last area that i will mention, tools for research and collaboration, is more familiar to computer scientists.many of the database and visualization tools developed outside of biology are now being imported into biologicalresearch tools. consider the work i discussed earlier on the construction of phylogenetic trees from molecularsequence data. as one might suspect there are a variety of algorithms that can be used and they sometimes giveconflicting results. today there are visualization tools being developed that will allow one to morph one phylogenetic tree into another. this gives the researcher a way to dynamically, rather than simply statically, visualize thedifferent trees presented by different algorithms.systems are also being developed to aid in experiment design and hypothesis testing. tool kits for buildingbiological models are helping to eliminate the need to go back and start programming from scratch each time.lastly, large database systems for managing experiment design take the typical laboratory notebook and put it intoelectronic form. they make it possible to take the data from a large experiment (a genome sequencing project, forexample) and make it a flowthrough process.the topics i have mentioned are broad, but are certainly not allinclusive. today computational biologists canbe found working in all areas of the biological sciences. just as broad as the problems pursued are the techniquesemployed. they range from numerical approximation, mathematical programming and numeric solutions ofmathematical equations, to neural networks, discrete algorithms and data structures, databases, and visualization.i have given a very brief overview of some of the exciting work being done today in computational biology.at this point i would like to discuss some of the challenges. although my talk was to focus on funding challenges,i will broaden what i say to include some more general challenges that i think the field faces. as one might expect,these are frequently interrelated and are challenges that many interdisciplinary fields face.federal funding for research in computational biology comes from the national institutes of health, nationalscience foundation, the department of energy, and in more narrowly focused programs from darpa, theenvironmental protection agency, the national institute of environmental health sciences, and the departmentof agriculture. industrial funding is significant in some areas of biotechnology and agriculture. within the federalagencies there are two models for funding. one is a model exemplified by the computational biology activity atthe national science foundation: a specific budget and review process to handle research awards in computationalbiology. the second model makes funds available for research in computational biology through disciplinaryresearch programs in the biological sciences. those programs that benefit from particular tools and algorithms areexpected to support their development. so, for example, research on modeling of biological molecules might behandled through a program in molecular biology. each of these models of funding have advantages and disadvantages.i will discuss some of these as i talk a bit about some of the general challenges that computational biology faces.the first challenge that i will mention is that of disciplinary identity. computational biology is often describedas forming the bridge between biology and computer science. but, is there more to computational biology? doesit form its own discipline, have its own foundation, its own fundamental problems? as scientists, we often judgedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.computational biology55fields by their foundation, and by the difficulty and fundamental nature of their research problems. if as computational biologists we describe our research accomplishments solely in the disciplinary terms of computer scienceand biology, computational biology will fail to develop as an independent discipline. this will have an impact onfunding in computational biology and make it difficult to build a cohort of scientists with the interdisciplinarytraining necessary to push the frontiers of research combining computation and biology.a second problem is closely related. as scientists, we typically credit depth over breadth, and interpret this asbrilliance over diligence. scientists that work in computational biology must learn the background science andlanguage of both biology and computer science. further, scientists that work as part of an interdisciplinaryresearch team pay a price in time as communication across disciplinary boundaries (and the physical boundaries ofwork places) is often slow. scientists that work in a narrowly focused area simply have more time to delve deeplyinto their own science. we reward the time spent on science in a way that we tend not to reward the time that ascientist spends learning to speak a different scientific language and interact with other scientists. this affectsfunding and advancement in computational biology as it comes out in our review process.a third funding challenge is what i will call the òglass ceilingó for computational biology. when there are lotsof resources or there is a special program in computational biology, funding is not a problem. however, whencomputational biology lacks its own funding program or when projects grow and outgrow their special program,computational biology projects must compete in disciplinary programs in biology. many will say that this is as itshould be; the disciplinary programs in biology should support the computational research that they benefit from.nevertheless, when funding becomes short these projects are often the first to go. even in the best of times, it isdifficult to find funding for large computational projects within disciplinary programs. i believe that as computational tools become a more routine part of biological research some of this problem will disappear. nevertheless,cooperation between funding programs can also ease some of the strain.the last issue that i will raise is what i will call it the òhacking problem.ó whether we are mathematiciansproving theorems or computer scientists implementing large software projects, much of our work in science isincremental. we build on our past experiences, our past knowledge and the past knowledge of others. most newresults extend this chain of knowledge; only occasionally are directions radically altered. how we judge this typeof incremental research varies. when results in computer science are extended to produce new results in computerscience, the review of this work may be quite different from that when a computer scientist applies a novel (butknown) date structure in a new way within biology. too often the crossdisciplinary work is viewed in the negativeas òapplicationsó work or òhacking.ó the ambiguity between implementation and novel extension of an idea oftencauses problems in the scientific review process.although these challenges exist, i am optimistic about the future for computational biology. there areexciting problems to be worked on and an excellent group of students interested in the area.finding a home in academiaedward h. shortliffeas someone who works at the intersection of two fields, medicine and computer science i have thought a lotabout bridging issues that affect interdisciplinary fields, of which computational biology is only one example. themore general discipline that deals with all biomedicine plus computing and communications, known as medicalinformatics, is the field in which i have worked personally over the years. in recent years, the role of bioinformaticshas become increasingly important, and i will tell you a little bit about computational biology as it fits within amedical informatics training and research environment at my own institution.the challenge of finding a home in academia is my nominal focus today. however, i am going to broaden mycharge to discuss issues related to how people who are trained in these intersecting fields find professional outletsin the world beyond, whether in academia, industrial research and development, or other settings where leadersincreasingly are recognizing the need for people who have insights and experience that cross disciplines. much ofwhat i am going to say today harkens back to the cstb report academic careers for experimental computerscientists and engineers, which was completed in 1994 (larry snyder from the university of washington chaireddefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.56defining a decade: envisioning cstbõs second 10 yearsthat committee); it addressed the issue of how experimental (as opposed to theoretical) computer scientists best fitin this evolving world of computer science within the academic community.as the field of computer science broadened, we began to hear comments like one that i overheard at a facultymeeting in the computer science department at stanford in about 1988, òi remember the days when i used to go todepartmental seminars and i could understand every talk!ó this remark was made by an eminent computerscientist, reminiscing about the good old days when the computer science department was first formed and hecould go to any colloquium, understand the content, and ask insightful questions. he realized that it was gettingharder to do that. he no longer bothered to go to all of them because some were in areas in which he knew hewould have trouble relating to the topic. as computer science both broadened in scope and developed detailedmethods within narrowly defined subareas of investigation and application, and as seminars focused on subtopicsin areas of specialization, it is hardly surprising that some of these subspecialization areas would involve overlapwith other disciplines in which computing was being applied. biomedicine has been one of these.one might argue that there is a potential scientific discipline at the intersection of computer science withalmost every other field because of the ubiquitous nature of computing and communications as they touch on allaspects of society. therefore, it is not surprising that we are seeing the evolution of expertise in many of theseareas of overlap. the question is, to what extent does interdisciplinary training need to be specifically provided?to what extent can people get good training in computer science and simply choose to apply it in a given area? orto what extent can people get well trained in the areas of application and then try to pick up enough computing sothat they can work effectively at the intersection? given the complexity of computer science and the tendencyeither to reinvent the wheel (if you have not trained in computer science) or to apply computing notions naively (ifyou do not really know the discipline of application), many of us believe that there is an evergrowing need toidentify the disciplines that lie at the intersection and to train people explicitly in these areas.as you might imagine, this was the first question i was asked when, in the early 1980s, i proposed the creationof a new interdisciplinary degree program at stanford in medical informatics. understandably, faculty in thecomputer science department said, òwell, you know we could have a lawyer come in here tomorrow and ask thesame question, and then the nutritionists will come in and say that computers and nutrition is a new field, and theyneed a degree program as well. where do you draw the line?ó such questions do force you to identify what isunique about an interdisciplinary field that is not being addressed adequately during training in one area or theother in order to justify creating an entirely new degree program. we have, accordingly, tried to characterize thespecialized knowledge that lies at the intersection between biomedicine and computer science. figure 7.1 shows oneperspective of the challenge of trying to understand biomedical knowledge and biomedical data, to get them intocomputer programs, and then to use them for purposes of inference and problem solving. these processes define a setof research areas that explain the focus for much of the current training and research in medical informatics.i am going to tell you a little about the stanford informatics degree program to give you a feel for how aneffort to train people explicitly at this intersection between biomedicine and computer science has played out overa 15year period, where people are getting jobs, and some of the lessons that we have learned from this experience.our program offers masterõs and ph.d. degrees, and it is based in the school of medicine. the core facultyin the program all have both m.d.s and a ph.d. in either computer science or medical informatics. we are tryingto train people who will be researchers either in industry or in academic positions.1 the program has close ties to1our program has grown to a steady state of approximately 26 students. about 60 candidates apply each year for four to six positions. wehave about 8 core faculty overseeing the program, although there are some 35 faculty in the university who are involved in student projects orhave volunteered to be preceptors for students who want to work in their areas.since 1984, we have received a national library of medicine (nlm) training grant that supports both pre and postdoctoral degreecandidates. the postdocs are essentially all physicians; they are considered postdocs by the university, but they are all graduate students, too.the resulting anomaly (postdoctoral trainees who are also degree candidates) confuses our university (including its administrative computers)in that we have a group of students who are postdocs in the eyes of the medical school and graduate students in the eyes of the university.some of the universityõs policies about the two groups are conflicting. the predoctoral candidates tend to be either medical students who aregetting joint degrees or students straight out of undergraduate school. the latter group does not necessarily want to get a medical degree oranother health science degree but does want to make biomedical informatics its professional career commitment. trainees who are notsupported by the nlm training grant receive support from regular research grants, and some, especially international students, bring fellowships with them.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.computational biology57the computer science department; we happen to be on a campus where the computer science department and themedical school are close to one another. this greatly facilitates the opportunities for interchange between the twoenvironments, allowing students to take a medical school course, hop on a bicycle, and take a computer sciencecourse in the next segment of the dayõs class schedule.our emphasis is on rigor coupled with methodological innovation. we decline to give degrees simply becausea student builds a clever program, regardless of its size or complexity. instead, we ask that our graduates be ableto describe what the scientific issues are that generalize from the project they worked on to contribute broadly tothe work of others. we try to make people think about what they are doing in this context so they can stand upbefore a computer science colloquium or national meeting and describe their work, playing down the biologyissues or the medical issues and focusing instead on the computer science, and be perceived very much as peerswhen it comes to the nature of the contributions of their work.similarly, the same project often needs to be explained in a medical grand rounds, a pediatric seminar, or amolecular biology research seminar. when the same student goes into that setting, he or she needs to be able toplay down the computer science, which the typical person in the audience will not understand, and focus instead onwhat is exciting and challenging about the biology being done in those projects. this is quite a set of demands tofigure 7.1elements in the process of incorporating biomedical date and knowledge into computer programs for use inproblem solving. source: e.h. shortliffe. 1996. stanford university.fpodefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.58defining a decade: envisioning cstbõs second 10 yearsplace on people. it is one of the reasons our students tend to be older and actually end up with dual training. abouttwothirds of our students are physicians, who then take this kind of degree as their subspecialty training aftergetting a medical degree and often after completing a residency program.since there are many practical issues in biomedical computing, we need to find the right balance betweenteaching students the theory and giving them the skills that allow them, when they graduate, to have an impact inapplied settings. the challenge in building a curriculum for this kind of interdisciplinary field is to think about allthe areas students need to know something about (see figure 7.2). as the bottom of figure 7.2 indicates, webelieve students need to learn something about both clinical medicine and the basic medical sciences. thus,computational biologists in our program take courses on genetic structure and protein unfolding, and they learnabout clinical medicine topics as well. you might ask how much students can learn and retain in any one of theseareas if they are trying to learn something in all of them. we believe the answer is that graduate training requiresa broad background across a set of fields that one is likely to need to know about, with subsequent focus on an areaof specific application or methodology during the period of oneõs m.s. practicum or ph.d. dissertation.in addition to computer science and biomedicine, we add courses in decision sciences, biostatistics, bioengineering, and epidemiology. if people take courses in all of these areas, it is not the same as taking a computerscience degree and then applying it in biomedicine. the curriculum provides an unusual mix that did notpreviously exist in any single degree program. as more schools offer degrees in this area, we are seeing a similarmix of topics being included in their curricula.the program philosophy rests on the following:¥it is oriented toward research training for academic or industrial careers.¥it has close ties with, and gains inspiration from, the computer science department.¥there is emphasis on rigor, methodological innovation, and an ability to generalize from specific results.¥there is emphasis on verbal and written skills, including an ability to present work to three audiences.¥training is balanced between theory and practice.accordingly, we have five major content areas in our curriculum: biomedical informatics, computer science,decision science, biomedicine, and health policy. biomedical informatics courses include the incremental offerings that we had to create for our degree candidates. among these are courses in computational biology, anintroduction to medical applications of computing, medical decision support systems, biomedical imaging, and aproject course in which students build small systems under the guidance of medical informatics faculty. ourinformatics courses are crosslisted in the computer science department, and many computer science and engineerfigure 7.2areas of knowledge required in the field of biomedical informatics. source: e.h. shortliffe.1996. stanford university.epidemiologydefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.computational biology59ing graduate students are coming to the school of medicine to take our offerings. they see biomedicine as aninteresting area of application that they often had not realized could be pursued as part of their academic programuntil they discovered our courses in the computer science portion of the university catalogue.let me summarize some of the lessons learned from this training activity. first, the trainees themselves oftenbecome tremendous bridge builders. they have brought informatics preceptors into contact with faculty throughout the university that we would not otherwise have known because the students take courses all over the campus.they come back with ideas, oftentimes with strong recommendations about people to whom we should be talking.sometimes this has played out in very positive ways, resulting in new collaborations and relationships, oftenoutside the medical school. as i mentioned, we have people from other parts of the university taking our courses,learning about the field, and getting excited about it. some of them end up going into medical informatics or goingoff to medical school. although that may not have been their original plan, they realize that there is an emergingcareer opportunity at the intersection of health care or biology and computer science.the crucial role of an advanced computing environment is probably obvious to anyone in computer science,but this point is not always obvious to deans of medical schools. it is difficult to provide highquality training inmedical informatics unless you have a computing environment that is just as advanced as one expects to find in atypical engineering school department. budgets in medical schools seldom take this into account. thus, we facethe challenge of trying to build an advanced computing environment in a school of medicine, which does not havea tradition of supporting such environments from operating funds. we are very dependent on industrial gifts ofhardware and software as a result. there is also a tension between the academic and service notions that existwithin a school of medicine. some medical school colleagues assume we should be pulling wires through thewalls for them and making sure that their pcs work. similarly, our affiliated hospitals, realizing that we havetalented students who know a lot about computing in the health care setting, often wish we would provide themwith technical support for their applications projects.some service activities of these types would be healthy for our students; many will be involved with preciselysuch issues after they graduate. on the other hand, if an academic unit becomes a service organization within amedical environment, it is difficult to maintain the academic standards that are part of the motivation for havingthe training program in the first place. thus, we try to find the right balance and to avoid formal serviceresponsibilities to the school or hospitals, just as computer science departments do not run their universitycomputing centers.with regard to our current status, we have enrolled about 70 students since we started the program in the early1980s; 33 were physicians when they started, and another 13 were medical students while they studied with us.2much more than half of our graduates end up as m.d.ph.d.s or m.d.masters; the majority of these earn ph.d.s.essentially all of our current students, 24 out of 26, are ph.d. candidates.3 it has become clear that there isdeclining interest in masterõs degrees in this field, at least among our own trainees. one of the reasons there arecurrently more masterõs graduates is that it takes only two years to train a masterõs graduate and four to six yearsto train a ph.d., as in most ph.d. programs.people often ask what our students do when they graduate. they do research; they work in academia; theywork in industry doing research; they work in operational jobs within medical institutions or, increasingly, inbiotech companies that are hiring people who have these kinds of skills. some of them do clinical, administrative,and educational management. we have one or two that actually run hospital computing groups. some are workingon digital libraries, providing expertise in information retrieval and information science. seventeen of ourgraduates are in academia, sixteen in industrial settings, and three in clinical practice, but still doing someinformatics as part of their life as practitioners; one is in a hospital working in a clinical computing environment;two work for the government (one as a military chief information officer and the other as a researcher for thenational aeronautics and space administration doing biomedical computing); and five are completing theirresidency programs.2of the 70 trainees to date, 17 have been women, which reflects about the same ratio as our applicant pool over the years.3there are 24 graduates with masterõs degrees; 20 with ph.d. degrees.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.60defining a decade: envisioning cstbõs second 10 yearsi would like to close by coming back to a point raised by professor joseph. computational biology can beconsidered as providing the interface between molecular biology and computer science (figure 7.3, top), and onecan anticipate the emergence of experts who identify this as their professional field; yet such experts must try tomaintain credibility in all three of the circled domains in figure 7.3, moving as fluidly as they can among threerather different environments. the challenge is that oftentimes the molecular biologists ask whether what thesepeople are doing is true science. can you be doing true science if you are not working with test tubes and gels atthe bench? in the culture of a biomedical research environment, it is common for colleagues to define òtruescientific researchó using such criteria.ironically, from a different perspective, computer scientists sometimes see computational biologists as beingtoo applied. they are viewed as being driven too much by applications from the real world. they may not beworking at the theoretical level that would more easily validate their role as traditional computer scientists in anacademic setting. the same picture applies to those doing medical informatics (figure 7.3, bottom). although therelevant area of application is clinical medicine, medical practice and clinical faculty often view òvalidó researchas focusing on discovering explanations for life processes, with an emphasis on òwet benchó research. suchnarrow definitions and high expectations have forced those who work in a field like medical informatics to definetheir discipline not in terms of their impact on applications but, rather, based on the significant research problemsthat are associated with the development of new methodologies and solutions. perhaps all new fields, drawing onfigure 7.3elements of interdisciplinary fields and the challenge of integratingthem effectively. source: e.h. shortliffe. 1996. stanford university.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.computational biology61elements from existing disciplines while defining a new one, have had similar challenges in defining and explaining both the scientific content and the practical importance of the problems that they address.discussiondavid messerschmitt: let us assume that in some area such as computational nutrition we decide thatit is not appropriate to have a separate program. i can imagine three models. we could have a nutritionist wholearns enough computer science but is in the nutrition department, or somebody who is in computer science whogets in that application area, or someone with a joint appointment. maybe deborah in particular could commenton the relative merits of these three options.deborah joseph: i guess there are two issues. one applies to a faculty member or researcher and theother to a student. one of the things that the university of wisconsin has that i think has worked very well is anational institutes of health training grant in biotechnology. we do not offer a degree in biotechnology, but wenevertheless have students who are trained in biotechnology on this grant. these students have advisers in someaspect of the biological sciences and in some aspect of the mathematical computational or engineering sciences.the program has been extremely successful. students tend to spend a lot of time together, and there tends to be alot of crossfertilization between students in the program. this would fulfill some of the need for a program incomputational nutrition.at the faculty and research level, i think that more of it has to do with how we view our colleagues, more thanthe barriers put in place by universities. the true impediments typically relate to whether you can get throughsome review process, whether your colleagues respect you, and whether you have colleagues who will write lettersof recommendation for you. if we as scientists value crossdisciplinary activities, i do not think that good peoplewho work in computational nutrition will have trouble. if their research is not valued by both their colleagues innutrition and their colleagues in computer science, then they have trouble.messerschmitt: can they survive in either department?joseph: i think at the university of wisconsin they can, but it depends a lot on the university.edward shortliffe: local culture is very important in this regard. i think there is a great differenceamong institutions as to how much they are purists with regard to the implied criteria for appointment andpromotion, as suggested in my last two diagrams. what we are talking about is credibility at promotion time,especially in the tenured line.john riganati: one comment is related to the question you just had, but i wonder if either of you knowswhat the trends are in biology departments around the country about giving credibility to computational biology,as opposed to requiring that the computational aspects be linked directly with wet lab work. i know that a fewyears ago the percentage was very small, and i do not know if there have been any significant changes as a resultof the kinds of things that you describe here. the second question is whether or not biological processes areviewed by either of you as having some basis for practical application in computational devices in the next fewyears, either literally or in an inspirational sense.joseph: i guess i could take the last question. recently dna and biomolecular computing have received alot of press. i think we have been overeager in computer science to believe that this is a technology that is rightaround the corner. it is an interesting idea, but i will reserve judgment on whether we are really going to buildsupercomputers out of dna molecules.shortliffe: to answer the first question, there has been a growing trend in the recruitment of individualswho really know about computational biology to work in traditional molecular biology, genetics, and biologydepartments. frankly, it is impossible now to do modern molecular biology research without local expertise incomputing, especially for dna homology searching and analysis of some of the protein databases and the like.molecular biologists can no longer do effective research unless they have somebody in their midst who canhelp them understand and use the newer technologies. sometimes professors assign graduate students to go off,learn about computational biology, come back, and basically be the òguruó in the lab. increasingly there has beena trend toward bringing in people who provide that expertise professionally inhouse.the problem is that they are not necessarily viewed as peers. i think the way we are beginning to seedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.62defining a decade: envisioning cstbõs second 10 yearscomputational biologists perceived as pure scientists is in the creation of separate academic units within institutions. here they are recruited as computational biologists, they are evaluated as computational biologists, they arepromoted as computational biologists, and then they become collegial collaborators with people in other departments on research projects. the same goes for medical informatics.mischa schwartz: the issues you raise are common to many interdisciplinary areas. there is oneschool of thought that says perhaps the way to do this is to learn one discipline well and become a specialist in itand then go on to the other one, rather than spreading yourself too thin. what are your comments on this?shortliffe: well, it was indeed the question i was asked most when we proposed our program. itdepends on whether you really believe that there is a body of knowledge at the intersection that warrants fulltimefocus during training.there is a difference between learning a lot about medicine and then learning a little about computing, orlearning a lot about computing and then a little about medicine, and focusing your entire graduate training at theintersection itself. i believe there is also a significant difference between formal informatics training and havingsomeone first finish a medical degree and then earn a computer science degree in a conventional computer scienceenvironment; the connections to medicine are not part and parcel of the way computer science is taught, andunderstanding the relationships and relevance is thus inherently left as an exercise to the student. we have foundthat there is a kind of culture in a field that students begin to absorb if they train in an environment whereeverybody else is also working at the intersection of disciplines that interest them.i think time has shown that people who actually train explicitly to work at this intersectionñindividuals whoget all their course work and culture in an environment that allows them to interact regularly with others who havethe same interdisciplinary interestñdevelop a special skill set as bridge people and as productive contributorswhen they get out. such skills are not easily acquired by earning one degree or the other and then sort ofsecondarily adding a pure version of what they missed.william wulf: i personally think that the issues raised here about interdisciplinary work betweencomputer science and biology go across the board. computing intersects absolutely everything, as ted said. sothis is a problem that we computer scientists really have to face up to. i think we can be a model for a lot of otherdisciplines where the same problems occur, but perhaps there is not quite the same urgency to collaborate at themoment.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.63introductory commentsdavid d. clarkin contrast to the previous symposium segments in which people presented set pieces, the visions panel wasdesigned to be entirely interactive.we talk about the future of the field, but it is òfieldsóñpluralñbecause the computer science and telecommunications board (cstb) deals with computer science, computer engineering (this assumes that you believe theyare different things), and telecommunications, especially with the acquisition of the redefined òtó in cstbõs title.some parts of electrical engineering are also relevant. in addition, we have many different kinds of playersñacademia, industry, and governmentñas well as user groups and spokespersons for societal issues. our concernsare multidisciplinary even within individual fields.we have a future that is shaped by a variety of forces. within each of the relevant fields, the question is, whatis going to happen? is it going to converge? is it going to fly apart? we could ask some technological questionsabout the future. are processes going to get faster? are networks going to reach the home? the more interestingquestions, perhaps, are the societal ones that may transform the world in some way. these are actually the hardquestions to answer. another thing we can do, especially with regard to some of the societal issues, is try to boundthe possibilities. what are the boundary conditions of the technologies?panel discussiondavid clark: earlier in the symposium, we heard the phrase òthe reckless pace of innovation in thefield.ó it is a great phrase. i have a feeling that our field has just left behind the debris of halfunderstood ideas inan attempt to plow into the future. one of the questions i wanted to ask the panel is, do you think that we are goingto grow up in the next 10 years? are we going to mature? are we going to slow down? ten years from now, willwe still say that we have been driven by the reckless pace of innovation? or will we, in fact, have been able tobreathe long enough to codify what we have actually understood so far?raj reddy: you make it sound as though we have some control over the future. we have absolutely nocontrol over the pace of innovation. it will happen whether we like it or not. it is just a question of how fast wecan run with it.8visions for the future of the fieldsdavid d. clark, moderatoredward a. feigenbaumjuris hartmanisrobert w. luckyrobert m. metcalferaj reddymary shawdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.64defining a decade: envisioning cstbõs second 10 yearsclark: i was not suggesting that we had any control over the pace of innovation, but are you saying youthink it will continue to be just as fast and just as chaotic?reddy: and most of us will be left behind, actually.robert lucky: we were talking this morning about the purpose of academic research. the problem thatmany of us involved in research have is that, as at bell labs, we used to talk about research in terms of 10 years.now you can hardly see two weeks ahead in our field. the question of what longterm research is all aboutremains unanswered when you cannot see what is out there to do research on.nicholas negroponte was saying recently that, when he started the media lab at the massachusetts instituteof technology, his competition came from places like bell labs, stanford university, and the university ofcalifornia at berkeley. now he says his competition comes from 16yearold kids. i see researchers working ongood academic problems, and then two weeks later some young kids in a small company are out there doing it.you may ask, òwhere do we fit into this anymore?ó in some sense, particularly in this field, i think there must stillbe good academic fields where you can work on longterm problems in the future, but the future is coming at us sofast that i just sort of look in the rearview mirror.mary shaw: i think innovation will keep moving; at least i hope so, because if it were not moving thisfast, we would all be really good ibm 650 programmers by now. i think what will keep it moving is the demandfrom outside. in the past few years, we have just begun to get over the hump where people who are not in thecomputing priesthood, and who have not invested many years in figuring out how to make computers do things,can actually make computers do things. as that becomes easierñit is not easy yetñmore and more people will bedemanding services tuned to their own needs. i believe that they will generate the demand that will keep the fieldgrowing.juris hartmanis: as was stated this morning, i think we can project reasonably well what silicontechnology can yield during the next 20 years; the growth in computing power will follow the established pattern.the fascinating question is, what is the next technology to accelerate this rate and to provide the growth during thenext century? is it quantum computing? could it really add additional orders of magnitude? is it molecular ordna computing? probably not. the key question is, what technologies, if any, will complement and/or replacethe predictable silicon technology?clark: i wonder if growth and demand are the same thing as innovation? mary, you talked about a lot ofdemand from outside. we could turn into a transient decade of interdisciplinary something, but does that actuallymean there is any innovation in our field?shaw: we have had some innovation, but it has not been our own doing. things like spreadsheets and wordprocessors, for example, that have started to open the door to people who are not highly trained computingprofessionals have come at the academic community from the outside, and they had very little credibility for a longtime. i remember when nobody would listen to you if you wanted to talk about text editors in an academic setting.most recently, there has been the upsurge of the world wide web. it is true that mosaic was developed in auniversity, but not exactly in the computer science department. these are genuine innovations, not just nickelanddime things.edward feigenbaum: first, i would like to say a few words about the future, and then i will pick up onthe theme that dave clark started with, the debris, and ask some of my friends in the audience about their debris.there has been a revolution going on that no one really recognizes as a revolution. this is the revolution ofpackaged software, which has created immense amounts of programming at our fingertips. we go to the store; webuy it. this is the single biggest change from, say, 1980. i think the future is best seen not in terms of changinghardware or increased numbers of mips (or gips), but rather in terms of the software revolution. we are nowliving in a softwarefirst world. i think the revolution will be in software building that is now done painstakinglyin a craftlike way by the major companies producing packaged software. they create a òsuiteóña cooperating setof applicationsñ that takes the coordinated effort of a large team.what we need to do now in computer science and engineering is to invent a way for everyone to do this at hisor her desktop; we need to enable people to òglueó packaged software together so that the packages work asintegrated systems. this will be a very significant revolution.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.visions for the future of the fields65 i think the other revolution will be the one alluded to by leonard kleinrock, what he called didactic agentsor intelligent agents. here, the function of the agent is to allow you to express what it is you want to accomplish,providing the agent with enough knowledge about your environment and your context for it to reason exactly howto accomplish it.lastly, i will say something about the debris. i can bring my laptop into this room, take an electric cord out ofthe back (presuming i have the adapter that david farber was talking about before), and plug it into the wall. i getelectricity to power my computer anywhereñin wichita, la jolla, on any air force base in the country, oranywhere else i might be, even at the national academy. yet i cannot take the information wire that comes out ofthe back and plug it into the wall in wichita or la jolla or any air force base i choose because all of a sudden ineed the transmission control protocol (tcp) switcher. i need to have exact contexts for the tcp to operate inthose particular environments. we do not yet have anything like an information utility. yes, i can dial the interneton a modem, but this is a secondrate adaptation to an old world of switched analog telephones. it is not the dream.the architecture of the internetñwonderful as it may seemñhas frustrated the dream of the information utility.robert metcalfe: there are two solutions to your problem. the first relates to the structure of theinternet, and for this i must defer to robert kahn. since he and vint cerf are the fathers of the internet, they mustanswer this question. the second solution to the problem has to do with what gordon moore has recently calledògroveõs law.ó groveõs law states that the bandwidth available doubles every 100 years. it is a description of thesad effects of the structure of the telecommunications industry, which would be in charge of putting these plugswhere you want them. this industry has been underperforming for 40 or 50 years, and now we have to wake it up.lucky: what was the question? we are pushing something we would like to call ip dialtone. i see this asthe future of the infrastructure right now, to have an ip network. there was an interesting interview with marymodahl in last monthõs wired magazine. they asked her if voice on the internet would really take over, and shesaid no. she said that realtime voice is a hobby, like citizenõs band radio, not a permanent application. i actuallythink that in the future, the voice may be a smaller network and the ip infrastructure will really take it over. ipdialtone will be the main thing. i would not rebuild the voice network. i would just leave it there and build thiswhole new network of ip dialtone networks.clark: part of what marks our field is this reckless pace of innovation into the future. another is thepersistence of stubborn, intractable problems that we have no idea of how to solve. an obvious problem that wasraised earlier in various guises is (to look at it abstractly) our ability to understand complexity or (to look at it moreconcretely) our ability to write large software systems that work. when we go to cstbõs 20th anniversarycelebration and look back, do you think we are going to see any new breakthrough? let us pick this stubbornproblem as an example; then we can talk about some others. in software engineering, is something actually goingto change? are we going to see a breakthrough? i am thinking about the point ed feigenbaum made that peopleare going to be able to engineer a software package at their desks. i said, òoh no. it is done by gnomes insidemicrosoft.ó wonõt it be done by gnomes inside microsoft for the next 10 years?shaw: i think this is a very big problem, and ed pointed out a piece of itñthat the parts do not fit together.we have, though, this myth that someday we are going to be able to put software systems together out of parts justlike tinker toys. well, it isnõt like that. it is more like having a bathtub full of tinker toys, erector sets, legoblocks, lincoln logs, and all of the other building kits you ever had as a child; reaching into it; grabbing threepieces at random; and expecting to build something useful.as long as we have parts that are intended to interact with other parts in different ways and we cannot evenrecognize quite how any given part is expected to interact, we will have a problem. we do not even havedistinctions explicit enough to do the analogue of type checkingñto say, òthis one does not fit with that one, whatcan i do about it?ó well, maybe there is nothing we can do, and maybe we can find a piece that will patch it up.i think this is one of the major impediments to being able to put together systems from parts and make them work.i do believe that we will be able to make progress. breakthrough is a pretty big word, but i think we will at leastbe able to make significant progress on articulating these distinctions and helping each other understand when wehave the problem and what, if anything, we can do about it.the other problem that ed feigenbaum raised is the nonmigratory local context. i have the same problem thated does, except mine is at the software level. i put a document on a floppy disk and i take it someplace. well,defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.66defining a decade: envisioning cstbõs second 10 yearsmaybe the text formatter i find when i get there is the same one that the document was created withñhowfortunate. even so, the fonts on the machine are not the same, and the default fonts in the text formatter are notthe same, and it probably takes me half an hour to restore the document to legibility just because the local contextchangedñi see everyone is nodding, so i can quit telling this story. then, of course, there is the rest of the time,when i find a different document formatter entirely. this is another example of having parts that exist independently that we want to move around and put together. once again, i think the big problem is not being able toarticulate the assumptions the parts make about the context they need to have.butler lampson: i say we just had a breakthrough. how many breakthroughs per decade are youentitled to? the breakthrough we just had is the web. you had to cobble together a few million computers, awhole bunch of servers, all kinds of legacy databases and documents, and all kinds of stuff. all you have to do iswrite a few perl scripts and you can patch together huge amounts of stuff and make it accessible to millions ofpeople. what is all this whining and moaning about? furthermore, i would like to point out that if you want yourdocument to be portable, just write it in vanilla ascii and you will not have any problems with portability.shaw: i am really good at ascii, and ascii art too, but we were planning the next decadeõs breakthrough.clark: you used a portable operation object as an example. i actually think that is a lot easier thanintegrating software modules. i thought when butler stood up he was perhaps going to say something about theviability of distributed object linking and embedding (ole). is this the answer to composable software?lampson: give it a decade. microsoft has shortterm things and longterm things. this is one of the longterm things, like windows.metcalfe: at the risk of being nasty, what i just heard is that we need standardization. this is all i heard.i did not hear that all this money we are spending on software research is not resulting in any breakthroughs, orwhatever breakthroughs it is resulting in are not being converted because we just cannot standardize on it. is thisright? is this what i heard?shaw: standardization suggests that there is one size that fits all, and if everyone would òjust do it my way,everything would be just fine.ó that implies that there is one way that suffices for all problems.lucky: isnõt standardization what made the web? we all got together behind one solution; it may not fiteverybody, but we empowered everybody to build on the same thing, and this is what made the whole thinghappen.clark: one statement that was made at the beginning of this decade was that the nineties would be thedecade of standards. there is an old joke: the nice thing about standards is that there are so many to pick from. intruth, i think that one of the things that has happened in the nineties is that a few standardsñnot because they arenecessarily bestñhappened to win some sort of battle.lucky: this is a tragedy and a great triumph at the same time. you can build a better processor than intelor a better operating system than microsoft. it does not matter. it just does not matter.clark: how can you hurtle into the future at a reckless pace and, simultaneously, conclude that it is allover, it does not matter because you cannot do something better, because it is all frozen in standards?metcalfe: there seems to be reckless innovation on almost all fronts except two, software engineeringand the telco monopolies.clark: yet if we look at the web, the fact is that we have a regrettable set of de facto standards in htmland http, both of which any technologist would love to hate. when you try to innovate by saying it would bebetter if urls were different, the answer is, òyes, well there are only 50 million of them outstanding, so go away.ótherefore, i am not sure i believe your statement that there is rapid innovation everywhere, except for these twoareas.metcalfe: i go back to butler lampsonõs comments. just last week there was rapid innovation in theweb.clark: how about windows 95?metcalfe: windows 95 is an endgame.lucky: dave, it is possible that if all the dreams of the java advocates come true, this will permit innovationon top of a standard. it is one way to get at this problem. we do not know how it is going to work out, but at leastthis would be the theory.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.visions for the future of the fields67clark: i actually believe it might be true. i think this is very interesting.a tremendous engine exists down below that is really driving the fieldñthe rate of at least performanceinnovation, if not cost reduction, in the silicon industry. this was the engine that drove us forward. i think that thisis true, but i am not sure itõs the only engine. i wonder if on our 20th anniversary we will say, òwell, yes, siliconis the thing that drove us forwardó; or will there be other things? is the world wide web a creation of siliconinnovation?shaw: no, it is a creation of the frustration of people who did not feel like dealing with ftp and telnetbut still wanted to get to information.clark: i think you just said that silicon and frustration are our drivers.lucky: at the base, silicon has driven the whole thing. it has really made everything possible. this isundeniable, even though we spend most of our time, all of us, working on a different level. this is the engine inthe basement that really is doing it.metcalfe: the old adage: ògrove giveth and gates taketh away.óclark: you know i am an academic researcher. i thought i would ask the panel a question about my future,because i am very concerned about this. we heard all sorts of things earlier in the symposium about the nature ofthe field and the relationships that exist among research activities. i will be somewhat parochial here in order tofocus. for academic research, there is the model of the future of reckless innovation, combined with the alternative model of well, it is all over. somebody said to me that you can build a better operating system, but it wouldnot matter. you can make a better web, but it would not matter. you can create a better computer architecture,but it would not matter. it is all over.in some places, like the silicon industry, i heard that the vector is very clear. they can see all the way out to2010, and they know the problems they have to solve. i cannot repeat their language because i do not speak theirlanguage, but they have to learn how to do bipolar implant polarization. it is advanced technology development. inthat context, howard frank considers what the research community is doing as very narrow.i agree. if, in fact, our agenda has been defined by the boundary conditions of windows 95 and the insistenceof the silicon industry to move forward, then i think it is narrow, and there is no funding. if i had a good idea, i canbring one or two ftes (fulltime equivalents) to bear on it, and industry could bring 100 manyears to bear on it.microsoft cranked out activex in a year, right? how many manyears are in that? so what role can a pooracademic play? i find myself asking, òif all of the academic researchers died, what impact would it have on thefield in 10 years?óreddy: no students.lucky: it is like the nba (national basketball association) draft. students are going to be leaving early,trying to be mark andreessen.clark: this has happened to me. i cannot get them to stay. there is no doubt that it is a serious issue forme. so why does it matter?metcalfe: i think it is true that, right now, industrial advancement in technology is outstripping theuniversities. i see this as a temporary problem that we need to fix. some of us need to stop working on all theseshortterm projects in the universities and somehow leap out ahead of where the industry is now.clark: i once described setting standards on the internet as being chased by the four elephants of theapocalypse. the faster i ran, the faster they chased me because the only thing between them and making a billiondollars was the fact that you had not worked this thing out. you cannot outrun them. if it is a hardware area, youcan hallucinate something so improbable you just cannot build it today. then, of course, you cannot build it in thelab, either. we used to try to have hardware that let us live 10 years in the future. now i am hardpressed to geta pc on my desk. yet in the software area, there really is no such thing as a longterm answer. if you can conceiveit, somebody can reduce it to practice. so i do not know what it means to be long term anymore.hartmanis: i do not believe what was said earlier, that if you invent a better operating system or a betterweb or computer architecture, it does not matter. i think it matters a lot. it is not that industry takes over directlywhat you have done, but the students who move into industry take those ideas with them, and they do show up indevelopment agendas and products. i am convinced that the above assessment is far too pessimistic about theinfluence of academic research.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.68defining a decade: envisioning cstbõs second 10 yearsfeigenbaum: i want to make a couple of comments. on the question of longterm versus shorttermresearch, the universities would say, and i would say, that university researchers attend to longerrange issues. ata defense advanced research projects agency (darpa) software conference last august here in washington,bill joy gave the keynote speech. in the questionandanswer period they talked about this issue of shortrangeand longrange research. in the context of stressing that there is really a place for universities, bill said that at sun,18 months was a long time. he said he would not entertain anything that is more than 24 months out.then i was at another darpa meeting recently where they were talking about advances in parallel computerarchitectures. the project they were focusing on as being very advanced was the work of the stanford computersystem lab on the flash architecture. this project has been going on for more than a decade now. it evolvedwith several different related architectures. this kind of sustained effort is the role of the university.lucky: i want to say, in support of academics, that we are all proud of what the internet and the web havedone. they were created by a partnership between academia and the government. the industry had very little todo with it. the question for all of us is whether this is a model that can be repeated. can government do somethingagain as it did with arpanet that will have the tremendous effects for all of us that this has had two decadeslater?william wulf: i think this longterm versus shortterm language is a red herring. let me remind you ofthe figure that was up on the screen earlier this morning that comes from the brookssutherland report (figure 2.1in this volume). it shows research going on in universities and industry labs, product development going on, andthe point in time at which something becomes a billiondollar industry. the thing that is so wonderful about thatfigure is that it bounces back and forth. it is not a linear translation from farout basic academic research to shortterm grubbing product development. there are interesting, deep academic problems that are spawned by shortterm product development. if any group in the world ought not to be having this discussion, it seems to me it is thisfield because we have experienced that the linear model is just so much nonsense.michael dertouzos: all this negativism, i just do not like it. just a few random reflections: when itry to use my computer, i have to wait for what perceptually is 17 hours of booting. i do not want to wait that long,especially since i have to do it 20 or 30 times a day because it always fails. i would like a machine and a systemthat do not crash every 6 hours.clark: do you use windows?dertouzos: i am using everything under the sun, and they all crash. i would like to have a machine thatis easy to use. when i say a machine, i mean the whole spectrumñsoftware and hardware systems. we bragabout the web, and yes, butler, it is a great thing, and 40 million or 10 million usersñor whatever the number isñis great, but there are 700 million telephones and 7 billion people in the world. there are voiceless millions, andwe are not pinging against the limits. to get there and to have utility from these machines, we will have to be ableto use them easily. to me, this is a longterm project for 30 or 40 years ahead.somebody said that voice was going away. i think speech is the most natural thing. we have to learn how touse it to make our machines understand us and learn from us. i just do not see this bottoming out of the field.maybe we are in a bit of a lull. i agree that it is hard to find specific problems out there, but i think that if you lookat the whole picture, there is a great deal ahead. i would like to ask if the people on the panel could provide theirlist of things that they would like to see.feigenbaum: i would like to say something about a paradox or a dilemma in which university researchersfind themselves. if you go around and look at what individual faculty people do, you find smallish things in aworld that seems to demand more team and system activity. there is not much money around to fund anythingmore than small things, basically to supplement a university professorõs salary and a graduate student or two, andperhaps run them through the summer.partly this is because of a general lack of money. partly it is because we have a population explosion problemand all these mouths to feed. all the agencies that were feeding relatively few mouths 20 years ago are nowfeeding maybe 100 times as many assistant professors and young researchers, so the amounts of money going toeach are very small. this means that, except for the occasional brilliant meteor that comes through once in a while,you have relatively small things being done. when they get turned into anything, it is because the individualfaculty member or studentñas professor hartmanis mentioned, some students take these ideas out into thedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.visions for the future of the fields69worldñconvinces an industry to spend more money on it. subsequently, the world thinks that it came out of theindustry.anita borg: i wanted to talk a little bit about the question of where you get innovation and whereacademics get ideas for problems to work on. this is something that i talk about every time i go, as an industryperson, to talk to a university. it relates to what bill was saying. if we keep training students to look inside theirown heads and become professors, then we lose the path of innovation. if we train our students to look at whatindustry is doing and what customers and people out there using these things cannot doñnot be terrorized by whatthey can do, but look at where they are running into wallsñthen our students start appreciating these as the sourcesof really hard problems. i think that this focus is lacking in academia to some extent and that looking outward atreal problems gives you focus for research.hartmanis: i fully agree. students should be well aware of what industry is and is not doing, and i believethat many of them are well informed. just as michael dertouzos complained about what is happening to hismachine and what he wants to see done, students see problems with software and with the internet. they go outand work summers in industry. they are not in any sense isolated; they know what is going on. limited fundingmay not permit big university projects, but students are quite well informed about industrial activities.shaw: i side more with anita. earlier i mentioned three innovations that came from outside the computerscience communityñspreadsheets, text formatting, and the web. i think they came about because people outsidethe community had something they needed to do and were not getting any help in doing. so we will get more leadsby looking not only at the problems of computer scientists, but also at the problems of people who do not have thetechnical expertise to cope with these problems. i do not think the next innovation is particularly going to be anincrement along the web, or an increment on spreadsheets, or an increment on something else. what anita isasking us to think about is, how are we going to be the originators of the next killer application, rather than waitingfor somebody outside to show it to us?feigenbaum: i have talked to a lot of people abroadñacademics and industry people in japan andeuropeñabout our computer science situation, especially on the software side. we are the envy of the world interms of the connectedness of our professors and our students to realworld problems. talk about isolationñtheythink they are isolated relative to us.i want to make a specific suggestion. there was a topic that came up in joe traubõs talk about informationwarfare. there is, i think, a realworld context about which people ought to be concerned. i am giving you aperspective of 20 months with the air force and seeing the very real side of our academic discussions. we are inwhat i would call a preengineering phase with regard to handling the problems of information warfare that joespoke about. by preengineering i mean crafty and creative tinkering. if you actually go to the places where thiswork is done and watch the people at work, there is some scientific understanding, but not much. indeed, there isno real engineering going on there, although the work is very innovative.i think that the computer science academic world ought to pay attention. len kleinrock was making the caseearlier that computer scientists and engineers should understand the nomadic computing world. he was telling usto understand this from the point of view of the ògood guysó who want to give us functionality and ease of use. iwould say we need to convince computer scientists and engineering researchers to understand the same world fromthe point of view of the òbad guys,ó and understand it at some depth. that is not the kind of thing that weacademics usually pay attention to, but we must have good academic research focused on these issues.stewart personick: i want to add some data here. we have a research program at bellcore; it is notenormous, but it amounts to about $35 million a year funded by our external customers, and we have some fundingfrom the government as well. so we have a fair amount of money. in recent years, we have tried very hard to alignour research to the needs of our customers to keep up the funding. i have advised universities that we funded atmodest levels that i was not going to fund them as much as i used to. however, i indicated that i would bedelighted to subcontract some of our research to them because this would be merely a transaction. i told them thatit was not a gift. bellcore has work to do, and we are prepared to subcontract it to them. we are talking potentiallyabout millions of dollars. i have had no takers. people have been upset or discouraged by the fact that i havereduced the traditional funding; modest as it was, i have reduced it. i have not had anybody come back to me andexpress interest in subcontracting. what i hear people saying is, òwell, you know we donõt do that.ódefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.70defining a decade: envisioning cstbõs second 10 yearsthis goes along with what i think ed was saying. we do not have these enormous teams, but we do haveteams working on big system problems that are very, very tough, and anyone who could solve these problemswould be quite famous, in addition to making money for the customers. we are not seeing the academiccommunity respond by saying it would love to subcontract this work. it seems as if it has not yet bought into thisparadigm that we work together as a team on some problems that have real customers. these are not developmentthings in some grubby sense. they are really, really tough computer science problems and system problems.clark: now it is time to give each of the panelists two or three minutes to tell us the thing about the futurethat matters the most to you.reddy: as bob lucky pointed out, there are different kinds of futures. if you go back 40 years, it was clearthat certain things were going to have an impact on societyñfor example, communications satellites, predicted byarthur clarke; the invention of the computer; and the discovery of the dna structure. at the same time, none ofus had any idea of semiconductor memories or integrated circuits. we did not conceive of the arpanet. all ofthese came to have an impact.so my hypothesis is that there are some things we now know that will have impact. one is digital libraries.the term digital library is a misnomer, the wrong metaphor. it ought to be called digital archive, bookstore, andlibrary. it provides access to information at some price, including no price. in fact, the national sciencefoundation (nsf) and darpa have large projects on digital libraries, but they are mainly technology basedñcreating that technology to access information. nobody is working on the other problem of content.we have a library of congress with 30 million volumes; globally, the estimate is about 100 million volumes.the u.s. government printing office produces 40,000 documents consisting of 6 million pages that are out ofcopyright. creating a movementñbecause it is not going to be done by any one country or any one group, it mustbe done globallyñto get all the content (to use jeffersonõs phrase, all the authored works of mankind) online iscritically important. i think this is one of the futures that will affect every man, woman, and child, and we can doit. at carnegie mellon university (cmu), we are doing two things to help. in collaboration with nationalacademy press, we are beginning to scan, convert, correct, and put in html format all of its outofprint books.there are already about 200 to 300 of them. by the end of the year, we expect to have all of them. the secondthing cmu is doing is offering to put all authored works of cstb members on the network.metcalfe: i would like speak briefly on behalf of efforts aimed at fixing the internet. the internet is oneof our big success stories and we should be proud of it, but it is broken and on the verge of collapse. it is sufferingnumerous brownouts and outages. increasingly, the people i talk to, numbering in the high 90 percent range now,are generally dissatisfied with the performance and reliability of the internet. there is no greater proof of this than the proliferation of intranets, which people tend to build. the goodreason they build them is to serve internal corporate data processing applications, as they always have. the badreason for building intranets is because the internet offers inadequate security, performance, and reliability for itsuses. so we now have a phenomenon in companies. the universities, as i understand it, are currently approachingnsf to build another nsfnet for them. this is really a suggestion not to fix the internet, but to build anothernetwork for us.of course, the internet service providers are also tempted to build their own copies of the internet for specialcustomers and so on. i believe that this is the wrong fix, the wrong approach. we need to be working on fixingthe internet. lest you be in doubt about what this would include, it would mean adding facilities to the internet bywhich it can be managed. i claim that these facilities are not in the internet because universities find managementboring and do not work on it. fixing the internet also would include the addition of mechanisms for finance so thatthe infrastructure can be grown through normal communications between supply and demand in our open markets,and the addition of security; it is not the national security agencyõs fault that we do not have security in theinternet. it occurred because for years and years working on security has been boring, and no one has been doingit; now we finally have started.we need to add money to the internetñnot the finance part i just talked about, but electronic money that willsupport electronic commerce on the internet. we need to introduce the concept of zoning in the internet. thecommunications decency act is an effort, although lame, to bring this about. on the internet, mechanismssupporting freedom of speech have to be matched by mechanisms supporting freedom not to listen.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.visions for the future of the fields71we need progress on the development of residential networking. the telecommunications monopolies havebeen in the way for 30 or 40 years, and we need to break these monopolies and get competition working on ourbehalf.shaw: i think the future is going to be shaped, as the past has been, by changes in the relationship betweenthe people who use computing and the computing that they use. we have talked a lot today about software, and wehave talked a little about the world wide web, which is really a provider of information rather than of computation at this point. i believe we should not think about these two things separately, but rather about their fusion asinformation services, including computation and information, but also the hybrid of active information.on the web, we have lots of information available as a vast undifferentiated sea of bits. we have some searchengines that find us individual points. we need mechanisms that will allow us to search more systematically andto retain the context of the search. in order to fundamentally change the relation between the users and thecomputing, we need to find ways to make computing genuinely widespread and affordable, private and symmetric,and genuinely intellectually accessible by a wider collection of people.i thank bob metcalfe for saying most of what i was going to say about what needs to be done because thenetworks must become places to do real business, rather than places to exchange information among friends. inaddition, we need to spend more time thinking about what you might call naive models, that is, ways for peoplewho are specialists in something other than computing to understand the computing medium and what it will do forthem, and to do this in their own terms so they can take personal control over their computing.lucky: there are two things i know about the future. first, after the turn of the century, one billion peoplewill be using the internet. the second thing i know is that i do not have the foggiest idea what they are going tobe using it for.reddy: digital libraries.lucky: perhaps. i think it is fundamental that we do not know this. we have created something muchbigger than us where biological rules seem more relevant than the future paradigm that we are used to, wheredarwinism and selfadaptive organization may be the more relevant phenomena with which to deal. the questionis, how do we design an infrastructure in the face of this total unknown? there are certain things that seem to bean unalloyed good that we can strive for. one of them is bandwidth. getting bandwidth out all the way to the useris something we can do without loss of generality.on the other side, it is hard to find other unalloyed goods. for example, intelligence is not necessarily a goodthing. recently there was a flurry of email on the internet when one of the router companies announced that itwas going to put an òexon boxó in its router. an exon box would check all packets going by to see if they are adultpackets or not. there was a lot of protest on the internet, not because of first amendment and communicationsdecency act principles, but because people did not want anything put inside the network that exercises control,simply as an architectural paradigm, more than anything else.so it is hard to find these unalloyed goods. bandwidth is good, but anything else you do on the network maylater come back to bite you because of profound uncertainty about what is happening.hartmanis: i would like to talk more about the science part of computer science, namely, theoreticalwork in computer science, its relevance, and identifying some stubborn intellectual problems. for example,security and trust on the internet are of utmost importance; yet all the methods we use for encryption are based onunproven principles. we have no idea how hard it is to factor large integers, but our security systems are largelybased on the assumed difficulty of factoring. there are many more such unresolved problems about the complexity of computations that are of direct relevance to trust, security, and authentication, as well as to the grandchallenge of understanding what is and is not feasibly computable. the notorious p = np problem is probably thebest known problem of this type, but by far not the only one. i consider these among the most important problemsin theoretical computer science and sincerely hope that, during the next 10 years, some of them will be solved. ibelieve that deeper understanding of these problems will have a strong impact on computer science and beyond.because of the universality of the computing paradigm, the quest to understand what is and is not feasiblycomputable is equivalent to understanding the limits of rational reasoningña noble task indeed.feigenbaum: i would like to talk very briefly about artificial intelligence and the near future. if we lookdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.72defining a decade: envisioning cstbõs second 10 yearsback 50 yearsñin fact to the very beginning of computingñturing was around to give us a vision of artificialintelligence and what it would be, beautifully explicated in the play about turingõs life, breaking the code.raj reddy published a paper in the may 1996 communications of the acm, his turing award address, calledòto dream the possible dream.ó i, too, share that possible dream. however, i feel like the character in thewilliam steig cartoon who is tumbling through space saying, òi hope to find out what it is all about before it isout.óthere is a kind of edisonian analogue to this. yes, we have invented the light bulb, and we have given peopleplans to build the generators. we have given them tools for constructing the generators. they have gone out andhandcrafted a few generators. there is one lamppost working here, or lights on one city block are working overthere. a few places are illuminated, but most of the world is still dark. yet the dream is to light up the world!edison, of course, invented an electric company. so the vision is to find out what it is we must doñand i am goingto tell you what i think it isñand then go out and build that electric company.what we learned over the past 25 years is that the driver of the power of intelligent systems is the knowledgethe systems have about their universe of discourse, not the sophistication of the reasoning process the systemsemploy. we have put together tiny amounts of knowledge in very narrow, specialized areas in programs calledexpert systems. these are the individual lampposts or, at most, the city block. what we need built is a large,distributed knowledge base. the way to build it is the way the data space of the world wide web came aboutña large number of individuals contributing their data to the nodes of the web. in the case i am talking about,people will be contributing their knowledge in machineusable form. the knowledge would be presented in aneutral and general wayña way of building knowledge bases so they are reusable and extendibleñso that theknowledge can be used in many different applications. a lot of basic work has been done to enable this kind ofinfrastructure growth. i think we just need the will to go down that road.discussionjerome glenn: as far as being timid about talking about the future, didnõt we all get into computersbecause we wanted to focus global intelligence on the most difficult problems to solve? things that we could notdo alone? are we going to create a global interface between human brains and problems and machines? isnõt thatthe direction? so what is this fear of talking about the future?peter freeman: i see a fair amount of confusion between the development of products or technology andthe development of concepts or understanding. several of you touched on this in your comments about what goeson, or should go on, in universitybased research. i quite agree that many of us in universities are too focused onthe short term, but ultimately, if we are to get to that next generation of products and technology, we have to havesome new concepts.i would point out just one in an area that several of you identified, software engineering, which i agree isalmost devoid of ideas. there are a few peopleñmary shaw is one of themñwho are trying to develop ways toexpress the architecture of software systems. without that kind of architectural representation and description, wewill never be able to do the kinds of things that edward feigenbaum was asking about, for example.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.73i will talk about discontinuities, opportunities, and challenges that we face as we move into the knowledge era,in the context of both the world economy and the business and organizational environment. i would like tohighlight specifically some of the thoughts that i and others have had about the multifaceted role of computing andtelecommunications in the transition from an industrial society to a knowledge society.i want to be very clear that i am neither an academic researcher nor a corporate researcher. i am a corporateexecutive in the area of management. i am a vice chairman of coopers & lybrand, a $6 billion organization withapproximately 70,000 partners and staff, operating in 146 countries at last count. as chief knowledge officer, ihave responsibility for all aspects of technology as it relates to that enterprise and the interface of the enterprise, itssuppliers, and its clients, as well as learning and education and market research and analysis.rather than bringing research results to this setting, i have created a knowledgebased collage. the remainderof my time is going to be a representation of a knowledgebased collage around the subject of computing andtelecommunications in the knowledge economy.there are a great many thoughts about this move from the industrial economy to the knowledge economy(box 9.1). there is little argument about the fact that this transformation is taking place and that human capital andintellectual capital are going to be the principal sources of competitive advantage in this new era. there is littlecontroversy about the shift in balance among labor, capital, land, and knowledge as we have gone from anagricultural to an industrial economy and now as we move toward a knowledge economy.representing the track that we are on into the knowledge era is not particularly controversial. however, itposes one very interesting question. that is, if you look at the relationship of 8,000 years to a century, roughly, ora little better than a century for the second major era, is the knowledge era going to be similarly collapsed withrespect to its prior era? or is it going to last for the next 8,000 years? there is also little controversy about whatthe principal sources of advantage are in these various eras (figure 9.1). where we begin to get into someinteresting dialogue has to do not with the specific elapsed time of a particular era, but rather with the pace of thetransformation or the transition from one era to the other. different people have very different views of the pace(box 9.2). yet some people would point out that a very large part of the worldõs population is still living in theagricultural era. not all of the world economy has even made the transition into the industrial era, even though thattransformation took place in the middle of the last century.there is even some controversy about whether or not we are in the knowledge era yet or whether we areentering the knowledge era, which means that it is somewhere slightly ahead of us. my view is more like the last9unique challenges:computing and telecommunicationsin a knowledge economyellen m. knappdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.74defining a decade: envisioning cstbõs second 10 yearsline of box 9.2, which says that the future has already turned into the present. despite the fact that economistsdisagree on the pace of this transformation there is absolutely no argument about the fact that this transformationwill have profound implications for all aspects of the world economy, the political scene, our individual personaland social behaviors, the environment, and the business environment.there is, in fact, some concrete evidence that the knowledge era is already here. some of that evidence isprovided by both u.s. and global economic statistics. one of my favorites is this quotation from thomas stewart,board of editors of fortune:1991 was the crossover year when capital spending by u.s. companies was greater on telecommunications, copyingand computer equipment than on industrial, construction, mining, and farming equipment.we tend to think of the mid1990s as the transition point, but we actually made this shift, from a macroeconomicperspective, around 1991.computing and telecommunications play very interesting roles with respect to this transition. after 25 yearsin the era of the computer, those roles are causing or catalyzing the transition from the industrial to the knowledgeeconomy. in addition, computing and communications are also fundamental catalysts for the pace at which thistransition is occurring. unlike earlier transformations, the presence of and the advances within computing andtelecommunications are, themselves, radically changing the pace of this transformation.i recently had an opportunity to speak at an massachusetts institute of technology faculty seminar series onthis topic. i represented that one of the most intractableñbut terribly interesting and incredibly importantñproblems we face right now is how to get a better handle on the pace of the transition from the industrial economyto the knowledge economy and of what it might look like. this is important for the following reason.box 9.1the knowledge economyknowledge is replacing matter and energy as the primary generator of wealth.ñthomas stewart, board of editors, fortunetoday, knowledge and skills stand alone as the only source of comparative advantage.ñlester c. thurow, the future of capitalismtomorrowõs economy will revolve around innovatively assembled brain power, not muscle power.ñtom peters, the tom peters seminarfigure 9.1economic eras and sources of advantage in each. source: ron bohlin, vice president, digital equipment corporation, òknowledge networking: the major productivity breakthrough.óeconomic erassources of advantageagricultural eraindustrial eraknowledge eralandnatural resourcessecuritypopulationlaborengineering skillcapitalmarket accessknowledgeskillsflexibilitycontinuous learningdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.unique challenges75many people have made the argument that world economies did not collapse when we went from theagricultural to the industrial era because there was a selfcorrecting mechanism during the transition phase. at thesame time that labor was moving out of agricultural activities, there were new businesses, enterprises, and domainsof human effort created in the industrial age that picked up this labor. as a result, there were no massivediscontinuities in the work force. we do not know whether this will also be true of the next transition. we could,in fact, have some massive discontinuities in the work force in the united states and globally. we will notunderstand what is needed in the way of policy, and what is needed in the way of intervention to deal with thoseissues, unless we have a better understanding of the pace of change we are facing.to understand the implications a little better, i would like to back up a bit and say a few words about theindustrial era in order to encapsulate a few thoughts about the knowledge era. one of the implications of thenumbers and the information in box 9.3 is that during most of the industrial age companies had to be locatedsomewhere. they had a natural home. if you look at the industries that the japanese ministry of internationaltrade and industry(miti) put in its vision for the decade in 1990 (box 9.4), all of them are what lester thurowrefers to in his latest book (the future of capitalism, 1996) as brainpower industries. one of the attributes ofbrainpower industries is that they do not have a natural home. these industries, and the winners in theseindustries, can reside anywhere that someone has the capability to mobilize the brainpower required to be a winnerin them.today, economists (as well as the world bank, which has published some statistics on this subject) estimatethat human capital accounts for more than half of all of the wealth in the united states and other economicallyadvanced nations. however, the difference between today and the future is that during the era of natural resources,box 9.2alternative views of timing of transition between economic eraswe have entered the knowledge economy.ñbrooks manville and nathaniel foote, mckinsey and companyintellectual capital matters as we are leaving the industrial age and entering the information age.ñthomas stewart, fortunefor several decades the worldõs bestknown forecasters of societal change have predicted the emergence of a neweconomy in which brainpower, not machine power, is the critical resource. but the future has already turned into thepresent, and the era of knowledge has arrived.ñòthe learning organization,ó economist intelligence unitbox 9.3aspects of the industrial era in contrastwith those anticipated in the knowledge eraof the 12 largest industrial firms in the u.s. on january 1, 1990 . . .¥10 of the 12 were natural resource companies.¥only 1 of these companiesñgeñis alive today.implications¥those countries with natural resources were rich, and those without were destined to be poor.¥for most of the industrial age, companies had natural homes where they had to be located.source: lester thurow, the future of capitalism.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.76defining a decade: envisioning cstbõs second 10 yearsif you had natural resources you were destined to be rich; if you did not, you were destined to be poor. just becausehuman capital represents this aspect of wealth in our nation today does not necessarily imply that we are going tobe a global winner in this knowledge era without deliberate intent.i have some thoughts for the computer science and telecommunications board (cstb) in terms of challenges. at a macroeconomic level, i think we must better understand and prepare for the profound changes in theworld economy, u.s. economy, business environment, and social structures during òthis period of punctuatedequilibrium,ó as lester thurow calls it. for business organizations, which is the world i live in, i think the mostinteresting representation of what is going on in business is the quote from paul saffo at the institute for the future:it is hardly news that the corporation as we know it is headed for the scrap heap of business history. internalcorporate structures are already mutating beyond recognition. corporate boundaries are dissolving into commercialirrelevance as businesses explore entirely new modes of association and interaction.john major, in his remarks at this symposium, made some very interesting and quite complementary commentsabout what is going on in the world of business and, even so, was referring primarily to the world of motorola insidemotorola. i am going to talk a little about the inside of large multinationals like motorola and my own organization,but equally interesting are the hypertext links being formed between multinationals and other enterprises.again, at the macroeconomic level, computing and telecommunications are playing a multifaceted role in thistransformation of the business world. figure 9.2 comes from business week (december 1993) and encapsulatessome of the more unusual business models that were evolving as a result of the massive influx of computing andtelecommunications into the business sector.in work i did several years ago with peter keen (figure 9.3), we came up with the notion of the relationalbusiness. that was before the web was a major event. now it is called the hypertext organization.1 this is notquite the latest, but perhaps the secondtolatest, notion of what businesses are going to look like. as you caneasily extrapolate, much of our ability to create organizations with this look and feel is empowered, if you will, bycomputing and telecommunications in our environments.equally interesting, the institute for the future recently came out with a publication that contains multiplemodels of types of twentyfirstcentury organizations. the one that clearly and most directly represents thebox 9.4leading industries and implications in the knowledge erañmitiõs viewin 1990, miti published a list of industries it expected to be the most rapidly growing industries in the 1990s and into theearly 21st century (the vision for the decade)¥microelectronics¥biotechnology¥the new material sciences¥telecommunications¥civilian aircraft manufacturing¥machine tools and robotics¥computers (software and hardware)implications¥all of them are manmade brainpower industries.¥all of them could be located anywhere on earthñthey have no natural home.source: lester thurow, the future of capitalism, 1996.1like an actual hypertext document, a hypertext organization is made up of interconnected layers or contexts: the business system, theproject team, and the knowledge base. nonaka, ikujiro and hirotaka takeuchi. 1996. the knowledgecreating company: how japanesecompanies create the dynamics of innovation. oxford university press, oxford.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.unique challenges77industry that i am in, the professional services industry, is the fishnet model (figure 9.4) and, in that veinñfroma business strategy perspectiveñit is the only pure play in the knowledge economy. professional services firmsare knowledgebased, knowledgecentric, knowledgeintense organizations.if anybody was going to have a chief knowledge officer (and i think it is a peculiar title myself) it would bean organization that has concluded that, in the future, both competitive advantage and comparative advantage willrest on its ability to mobilize both internal and external intellectual assets on behalf of its customersñand itsability to do it more efficiently and faster than its competitors. the fishnet organization does not incorporatenotions of traditional secretaries, traditional flying around the world, or traditional buildings.i will share with you a fact (not a particularly competitive one) from a strategy perspective. some years agoñmaybe fiveñi told our board that technology then represented the third largest cost element in our business model.i remind you that this is not a manufacturing or financial services firm, but a professional services firm. even then,technology represented the third largest cost factor in our economic model. i told them that it would be third untilit overtook facilities to become the second largest factor. then it would be the second largest factor until itovertook human capital to become the largest. i said that the trend line was not going to change anytime in theirlifetimes or mine.the men in the boardroom looked at me as though i was crazy. here we are, five years later, and technologyhas overtaken facilities as the second largest factor in our economic model. just by going to a model in whichñin this case, motorola, bbn, dec, and othersñclients are providing the facility space for our people, as opposedto our organization providing it, the firm will substantially reduce costs.brian quinn has also devised an organization model (harvard business review, marchapril 1996) that is aspider web (figure 9.5). in a certain sense, whether it is a hypertext or a spider web or a relational or a fishnetmodel, they are all enabled, specifically and finitely, by the transformation that is taking place. i gave the exampleof coopers & lybrand adding millions of dollars to the bottom line just to show you that this is not all theoretical.another very real example is occurring in the manufacturing sector, not the professional services sector.2if it is true that longterm prosperity and comparative advantage, as well as competitive advantage, are goingfigure 9.2business models for modern organization. source: òthe horizontal corporation,ó business week, december 20, 1993.2ge lighting has closed 26 of 35 warehouses and replaced 25 customer service centers with one new, hightechnology operation. thosebuildings and stockpilesñphysical assetsñhave been replaced by networks and databasesñintellectual assets. thomas stewart, òpursuingthe knowledge advantage,ó fortune.operateenablecreatedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.78defining a decade: envisioning cstbõs second 10 yearsto come in large part from one organizationõs ability to manage its intellectual assets better than another, what isa model? what is a way of thinking about how a person might transform intellectual capital into revenue? figure9.6 is a very simple representation of what intellectual capital isñthe transformation of human capital andstructural capital into customer capital. the purpose, at least in our case because we happen to be a forprofitenterprise, is to create intellectual capital that can be transformed into financial capital. in my personal case, sincei happen to be a partner, i want this transformed into wealth for shareholders.although this model represents a forprofit enterprise, it can also be used for notforprofit enterprises. infact, i have had several conversations about this model with the world bank, and i will provide a short vignette todemonstrate that this is not a professional services phenomenon. it is an acrosstheeconomy phenomenon.the world bank recently decided that it is not a bank, it never really was a bank, and it never should havefigure 9.3relational model for the modern organization. source: peter keen, march 1995.figure 9.4fishnet organization model. source: bob johansen, andrea saveri, and gregory schmid, institute for the future, ò21st century organizations: reconciling control and empowerment.ódefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.unique challenges79thought of itself as a bank. it is really a knowledgecentric professional services organization that, if it had onlythought about this earlier, would have been able to generate (and still might in the future) enormous amounts ofmoneyñrevenue is not quite the right term, but money, financial capitalñthrough knowledgebased products andservices. in fact, the world bank has concluded that, about five years from now, its principal competitor will beandersen consulting. if the bank wanted to look for bestpractice, web organizations as models of what itscomputing and telecommunications infrastructure ought to look like, as well as organizational models, it ought tobe looking at andersen consulting, not citicorp. the bank did not arrive at this notion all on its own, and iabsolutely agree.in this model, computing and telecommunications technology has two fundamentally different òplays.ó oneplay is in the domain of structural capital itself, and this is where we use technology to build organizational assets.we are building intranets, networks, intellectual capital, knowledgebased systems, and so forthñall for thepurpose of building structural capital.a very different role that computing and telecommunications play in this world is in the speed of thetransformation process itself. therefore, the ability to transform intellectual capital into financial capital andwealthñthe speed with which competitors can go through this processñintrinsically is a major issue. at the sametime we are moving into the information age in order to directly convert knowledge into revenue, we are alsofocused on creating and leveraging human capital and doing that with some speed and some facility.the following quote by andrew grove, chief executive officer of intel, that i have been carrying around withme for some years is still true today, and to be perfectly honest, i do not know how old it is.computers will become communications platforms . . . people will use them to tie their work together, to collaborate. . . [in ways] that will revolutionize the way groups of people work.we have moved fundamentally into a different world in terms of the reasons for which we expend financialcapital on computing. lest you conclude that i think that computing and telecommunications will create thiswhole new world all by themselves and propel us into the knowledge economy without attention to other details,i am not so naive. i understand that this will work only if people want it to work and are willing to make it work.our fundamental design challengeñfor organizations at least, if not for individualsñrevolves not aroundbuilding better networks, but rather around building better worknets. if you want something interesting to getexcited about, this is it.figure 9.5the spider web model for the modern organization. source:james brian quinn, philip anderson, and sydney finkelstein. 1996. òmanagingprofessional intellect, ò harvard business review, marchapril.specialistsclientrelationshipmanagersdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.80defining a decade: envisioning cstbõs second 10 yearsthe collective power of computing and telecommunications can also bring us closer to our customers than wehave ever been able to think about. here is the good news and the bad news. consider the delta shuttle card.there are only a few hundred of these in existence; i just happen to be the proud owner of one of them. customersusing the delta shuttle can put a credit card in a machine and get a ticket. then you must go to the gate and geta boarding pass. you must keep your ticket so you do not lose it so that you can turn it in with your expense report.when i go to the delta shuttle, i go up to the gate, stick the delta shuttle card in a slot, and out comes not a ticket,but a boarding card. i take it and get on the plane. i have not spoken to a human being, and i do not need to carryany pieces of paper around with me or keep track of them. this phenomenon is most familiar to us with automaticteller machines. i also get out of my car and stick a card in the gas pump that i used to hand out the window to anattendant. this phenomenon has enormous social consequences, and we need to better understand it and its timing.we face not only an issue of leveraging knowledge; we also have to manage knowledge. the sad fact is that,for the most part, we do not. we are still measuring the one thing that all of our executives learned somethingabout when they were going through school, financial capital (box 9.5). we have to get much better at looking atand understanding how to measure intellectual capital.the conclusion is that if there is anybody in this room who is interested in the science of complexity or theemerging science of complex systems, this is one of the issues: the coevolution of economics, politics, social andenvironmental factors, and technology and what it is going to represent to our world is a very interesting, complexsystems issue. slightly more down to earth, i also have a couple of specific suggestions. we need a much betterunderstanding of (1) the new business models, the organizational models, that are emerging; (2) what knowledgecentric cybercommunities are all about; (3) what we should think about in terms of continuous learning anddelivery of continuous learning to highly dispersed populations; and (4) quality of life. we need a much betterunderstanding of the pace of the massive shift in business process execution from labor to technology; theimplications of this new era for wages, employment, and work location; intellectual capital asset managementissues; and some of the new rules for the game that we are beginning to play.so the bottom line is managing or, at best, figuring out how to adapt to the simultaneity. we know who thewinners were when the issue was natural resources and the parameters of winning, the characteristics of how towin, when financial capital was the issue. we do not know who the winners are going to be in this knowledge era.we barely understand what the parameters of winning might look like.figure 9.6an intellectual capital model. source: cibc leadershipcenter.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.unique challenges81discussiondavid messerschmitt: we have heard much about the significance of the fact that apparently theinvestment in telecommunications and computing has not increased productivity as measured in traditional terms.i get the sense that perhaps you do not believe this is the case. perhaps more importantly, what i think i hear yousaying is that it is missing the point. the increase in productivity in the industrial economy brought on bycomputers and telecommunications is not really the benefit; rather, it is creating a whole new economy. could youcomment on that?ellen knapp: you are right. unfortunately, you hit a very strong point with me. i happen to have beenon the committee that produced the computer science and telecommunication boardõs report information technology in the service society: a twentyfirst century lever, and stephen roach at morgan stanley was also amember. it was really interesting, not the least of which is that he and i both have offices at 1251 avenue of theamericas. we were in the same building and managed not to kill each other all those years until marjoryblumenthal brought us together. to finish with the point that you ended with, i think that most of what stephenhad been measuring for many years was irrelevant. i think he was not asking the right questions about thistransformation into a totally different world, rather than the productivity of a particular individual sitting behind aparticular spreadsheet in the 1980s.raj reddy: this was a fascinating talk. there are a couple of points i wanted to make related to what yousaid. about seven or eight years ago, edward feigenbaum and i were talking about this issue of knowledge. ifknowledge is truly the future wealth of nations, how could we demonstrate it? there were two ideas that came outof this. one is to create a world bank of knowledge. that is, take away all the money and only offer knowledgeto people. the question is, can theyñtaking the knowledge with all of the resources being available only forcostñ create a completely new society out of nowhere? for example, if you built a wall around washington, d.c.,and the only thing that could come in and go out were knowledge, would you survive? it was an interestingquestion. what technology might exist? so the idea of creating a world knowledge bank came out of that. ithought this might be an interesting idea to talk about.the second point you raised, that information technology is going to cause serious dislocations, is veryimportant. dislocations are going to happen much faster than society can handle. one thing i proposed a fewyears ago to an industry forum, which nearly lynched me, was that we should begin taxing the informationindustryñevery computer and every piece of software at a 5 percent rateñand create a dislocation fund just likethe north american free trade agreement fund. information technology is definitely going to cause dislocation.it is going to cause a lot of unhappiness in society. middle managers of various kinds are going to be laid off, anda lot of other people, knowledge workers, are going to be laid off. services industry people are going to be laid off.they need to be taken care of, reeducated, and retrained.box 9.5lack of attention to managing knowledge assetsmost corporations are still managed like old industrial companies . . . weõre still managing our physical and financialassets, rather than our knowledge assets.ñthomas stewart, fortunehow well do we manage assets?cashaevery penny at work all the timereceivablesaðwell controlled, ratios okinventoryb+mostly accuratepeoplecpeople stuck in boxesknowledgedðhuh?? room for improvement?source: charles savage, knowledge era enterprises, inc. òimplementing knowledge era organizations.ódefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.82defining a decade: envisioning cstbõs second 10 yearsknapp: i will focus on the second point first. i believe that this is the first major world economictransformation that we have to address globally. before we think about taxing information industriesñwhich arepredominantly in societies that did well in the industrial ageñand potentially hampering the ability of that portionof the economy to uplift the other portions of the economy, i think we should look at the intricacies of the wholebalancing act and think long and hard about it.edward feigenbaum: what is the situation now with chief knowledge officers? how many companieshave them? how have companies wrestled with the question of valuing knowledge assets for their assets andliability statement?knapp: i have only been a chief knowledge officer since monday. it was announced in the wall streetjournal. i happened to be in europe, so i did not get to read it, but i do not know a lot yet about who my colleaguesare. i know that both mckinsey and hewlettpackard have chief knowledge officers. cocacola has a chieflearning officer, with roughly the same set of responsibilities. there are perhaps a half a dozen companies thathave this title, not all in the services sector.to address your second question, skandia corporation has, for the past three years, produced an intellectualasset balance sheet as a supplement to the annual report of the corporation. by far, skandia and the canadianimperial bank of commerce are in the forefront of this field in terms of quantifying and measuring intellectualcapital on an annual basis, and representing to the shareholder the fact that its valuation is as critical as financialcapital valuation. balance sheets that have to do with intellectual assets are as critical as balance sheets that haveto do with financial capital assets. to my knowledge, skandia is the only corporation today that produces thesemirrorimage balance sheets. i think you will see many, many more in the coming years.mischa schwartz: you mentioned the need for a science of complexity. a previous speaker alsoalluded to some basic knowledge in systems complexity. it seems to me this may be something we cannot reallydo. those of us who have been around a long time remember something called systems science that peopleworked on many years ago. the national science foundation had programs. i worked on something calledòurban system analysis.ó it all fell apart. even a system like the at&t network fails occasionally. at&texecutives have been saying for years, òit is a shame we do not understand that complex system, simple as it is.ónow you start putting people into it, is there really a possibility here?knapp: there is a baby step that i think could be taken and has desperately been needed for many years.this is to get out of professionalacademic, stovepipediscipline problem solving. even if we do not understandcomplexity theory and the science of complexity, we do understand teambased work. i do not know thatuniversities do, but i do know that corporations do. if we could get some interdisciplinary, teambased work underway and energized within the university environment, i think that these teams could do some spectacular thingsthat realworld people really need.david clark: i look at the web, and one of the things the web did was sort of òdisintermediateó accessto a wide variety of information. it may be information you think is valuable. maybe it is not knowledgespecifically. maybe it is joe twiddleõs home page, but at least one of the conclusions is that we have had a hardtime selling this stuff for money. a lot of people are giving web pages away free and hoping they can hide theadvertising in there.i am wondering if there is a possibility, in the long run, that we will succeed in making it possible for humansto have direct access not just to information, but to knowledge. at that point, you will not be able to sell it; themarginal cost of selling it is zero because there are not any humans in the loop. so there is not any value there.one possibility is that the whole market is doomed. another one is that you think the product is the generation ofknowledge and not the resale of it. do you make most of your money creating knowledge or selling it multipletimes? if you are going to sell it multiple times, do you think you can make any money at it, or do you have to sellit at zero marginal cost?knapp: that is a very interesting question. we had an earlier speaker who passed on the question. i havesome thoughts that i would be happy to share with you after the session.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.83we have brought upon ourselves and the world something that encompasses much more than the informationsociety.1 if one asks, òwhat is the value of information?ó it quickly becomes apparent that the traditional theoriesof information are not effective in determining the value of a text editor, the work that an information worker does,a rented video, or an electronic form used in electronic commerce.in the current, rapidly changing environment, people are confused about the nature of information. we hearthat information is not scarce since it can be copied easily and therefore has very little value. maybe it should befree, or maybe it should have a fixed cost. part of the confusion stems from the fact that we view informationmostly as a noun and forget that it is also a verb. as a noun, information consists of text, pictures, movies, andvideos; as a verb, it refers to informationtransforming work carried out either by a person such as a tax accountantor by a computerñfor example, a word processing program.let me start with the assertion that, in economic terms, there is no difference between physical work andinformation work. either one is produced by people who are reimbursed for expending a portion of their lives todo such work. alternatively, the work is produced by computers that, like any other piece of equipment, requirecapital to be purchased. thus, whether office work is produced by a human or a machine, it involves the samefactors of productionñlabor and capitalñas physical work.if i project into the future, i envision a fairly simple model of what the information age is all about. i call it theinformation marketplace. it is a collection of people and their machines engaged in buying, selling, and freelyexchanging information. it is a bit like an old village marketplace, except that what is exchanged is informationrather than physical goods.to address the value of information in this setting, let me divide economic goods and services into informational and physical, as well as intermediate and final. final refers to something that is produced and thenconsumed, such as a loaf of bread. intermediate refers to something used to produce a final or another intermediategood, such as flour. final information encompasses items like books, entertainment, and videosñthings that weconsume for selfactualization, whose purpose ends there. when we add them up, these final goods compriseabout 3 percent of the u.s. gross national product, leading us to the conclusion that the amount of final informationtoday is very small.10ancient humans in the information agemichael l. dertouzos1these brief comments are discussed in greater detail in the authorõs 1997 book, what will be: how the new world of information willchange our lives, harper edge, san francisco.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.84defining a decade: envisioning cstbõs second 10 yearsmost information is intermediate because more than half of the work force is made up of office or knowledgeworkers. these employees and their machines are doing intermediate information work. whereas final information is subject to the rules of supply and demand, intermediate information always goes toward enabling somethingelseñeventually final information or, more often, physical final and intermediate products as well as services.thus, the bulk of information has the important property of pointing to something else, leading to something,making something else possible.the value of intermediate information is derived from what it points to. for example, at general motors, allof the computers, software, and people working in the office represent the intermediate information that goes intomaking cars. the monetary value of all these activities is less than the value of the cars they sell and is derivedfrom it. a huge amount of the u.s. economic basket is filled with physical goods and services. this means thatthere are many things for information to point to and derive value from. employing computers and softwaremakes a country more efficient and increases its wealth. in the united states, we value the hardware and softwarethat point to these goods at 10 percent of the u.s. economy. on the other hand, in a poor country such asbangladesh, the figure is less than 0.1 percent. this disparity illustrates that the rich countries (and people) valueinformation much more than the poor simply because they have more economic goods to which informationpoints. also, since information technology helps those who use it to improve their productivity, we have anunstable situation in which the rich will get richer while the poor will stay behind. left to its own devices, ourtechnology is going to increase the gap between rich and poor. this calls for action and help on our part to ensurethat it does not happen.let me now shift to what i call electronic proximity. proximity and mobility are two sides of the same coin.the more mobile you are, the more people you can get close to. in the village age, we had about a couple ofhundred neighbors whom we visited on foot, so our proximity was several hundred people. in the industrial age,cars increased our proximity by a thousandfold. we could drive a few hours and, potentially, reach hundreds ofthousands of people. we did not have to know them all, but we could reach them. the information age will nowgive us another thousandfold increaseñto hundreds of millions of people who will be within electronic reach.this is because we have 100 million computers connected today and (i forecast) will have some 500 millionmachines in five to seven years. this huge new increase in proximity is worrisome if we consider, by analogy, theproblems that urban areas are facing today. we need to pay a great deal of attention to the problems of theforthcoming increased proximity.when i think of proximity, i also think of telework. if people work from their homes, we are going to have avery interesting situation: a person will be an urban sophisticate by day, living in the worldõs markets; electronically commuting to tokyo, paris, and other major cities; carrying out all sorts of transactions, selling, buying, andfully exchanging knowledge and information. however, when the time comes to turn off the computer, the sameindividual will turn around and go out for pizza at a favorite local restaurant like a villager. she is an urbanite byday and a villager by night. we do not know which part of this split human will win the battle or even if both partswill learn to coexist within us.another point concerns nations and boundaries. nations are located in one landmass because of their naturalresources. france had wine. england and germany had steel. we greeks had grapes, knowledge, and democracy!another reason nations remain physically compact is because they have a history, a set of traditions, and areprotected by tribal unification forces. now, the economic value of local resources has gone out the window, as thejapanese have shown the world. as for culture and history, consider this: in the future, i could dial up my highspeed network from boston to athens. i could sip ouzo while chatting with my friends in the plaka, sing greeksongs, attend services at the athens cathedral, or watch the sunrise on santorini island. i could partake of a lot ofcultural, historic, and tribal òfoodó in a way that is not available to me in boston today. so, all the old factors imentioned that bind a nation within a common landmass seem to be disappearing. perhaps tomorrowõs greecewill not exist as a compact landmass, but as the greek network!let me close with a third consideration, which is psychological. i submit that we are, today, the same ancienthumans that socrates and other more ònormaló ancient people were in the past. we have the same body, mind, andpsychology. yet, our hands have moved from the stone club to the steering wheel, to the jet aircraft stick, and tothe wimp (windows, icons, menus, pointing) interfaceña huge change. how are we coping with it? breadanddefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.ancient humans in the information age85butter items such as text, photographs, videos, tables, and spreadsheets are fully transmitted over the informationinfrastructure. we are tempted to ask which activities and qualities actually pass through the airwaves, satellitelinks, and wires of the information infrastructure and which ones do not. it is clear that emotions are communicated to some degree. we all watch tv and sometimes laugh or cry at what we see and hear. however, emotionsdo not pass through fully, as pen pals can attest. eventually, these virtual friends must consummate theirrelationship with a reallife experience such as meeting each other and shaking hands.are there some things that do not pass through the information infrastructure at all? when we lived in caves,we had a basic fear that animals would come from outside to eat our food or our children. this fear was a powerfulforce of the cave. another force was when we hugged our loved ones and had physical contact with them. justbecause ancient humans left the cave does not mean that these forces have left us. in fact, i suggest that theseprimal forces of the cave not only are still with us but are present for the most important, and even some of themundane, decisions we make. interactions with our siblings and friends; the relationships between doctors andpatients; the trust between business associates or between students and teachersñthese all involve the forces ofthe cave.do these human emotions pass through the wires and wireless links of the edifice we are constructing? i donot think so. for example, you can set up the best virtual reality fullimmersion suit and create a robot designedto frighten meñlike a monster from the caveñeven hit me with its steel fist. i am wearing my bodynet suit withvirtual reality goggles and haptic gloves; i am seeing the monster approach, and it is getting very scary. however,i know that i can flip the switch and the monster will disappear. it is not a force of the cave unless it is real and iknow it and feel it, not only rationally, but instinctively. powerful and instinctive forces like the forces of the cavedo not pass through the systems we are creating.i conclude that the information age that lies ahead will not be a panacea, a paragon of knowledge, or a liberatorof the human spirit as some of the current hype suggests. instead, i believe that it will be a profound and powerfulsocioeconomic movement as big as the industrial revolution but ultimately of the same ilk, providing a new set oftools that will enable ancient humans like us to pursue our ancient goals and aspirations in new ways.discussionaudience participant: on this last point, michael, i can remember in the fifties, back in the age ofradio (which, by the way, was coming over wires or wireless), that this medium had a much greater capability toscare the living daylights out of me than tv or movies or henry fuchsõs virtual reality because it was utilizing myown internalization of imagination and was deliberately limiting what was coming over the airways in that sense.so i am not sure i believe what you say, based on that experience.michael dertouzos: oh, well, you are not scared as much as if a real force of the cave came after you.that would be my quick answer.henry fuchs: i agree. i am not going to scare anybody with virtual reality. i want to comment on yourlast, very wise observation. i want to take a slightly more pessimistic view. i will use the analogy of the telephoneas a tool to let us, as you say, reach after the same forces of the cave. i think these new capabilities will allow usto extend the flexibility that the telephone has allowed usñto be physically in different places, but emotionally tostill be together.dertouzos: one short comment, if i may. in business relationships, if you are trying to set up a seriousmerger or do something with a business partner, you will never do this by telephone unless you already know andhave pressed the flesh of that person. i would say exactly the same thing.fuchs: exactly right. so in this way, i suspect that what we will have is a situation in which you will notdrink ouzo in athens, in a small town, even with virtual reality, but there will be an added sense of sharing withpeople you already have a relationship with.dertouzos: accepted. bob, stump us.robert kahn: mike, you have described this flatearth theory of physical work where, in essence, youconsume the bread, you drop off the cliff, and it is really gone. you have got to do all the work again to create itone more time. you also presented a flatearth theory of information. i wonder if the theory would not be betterdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.86defining a decade: envisioning cstbõs second 10 yearsdescribed as sort of the roundearth theory in the sense you said that, with the final information you get, you cannotfigure out what you are going to do with it after that, so it must be final.dertouzos: not always, bob, but most of the time.kahn: yet, in fact, you may not know what you are going to do with most things, and even if you thoughtyou did, you might not. if final information was just thatñinformation you did not know exactly what you weregoing to do withñthen the whole educational system would be final information. i want to put forth thehypothesis that it is really a roundearth theory and that everything in the information world is really grist forsomebody else.dertouzos: well, you can close the loop back from education over a slower loop to the beginning ofintermediate information. the point here is that you are not describing precise physical or human processes. youare developing a theory that has idealized components. when you have a real situation, it has pieces of them. i amdoing this to try to understand what is going on with the economy.edward feigenbaum: mike, i wanted to take you up on this real life component versus the virtualreality component. i have to mention that mike told me last night that marvin minsky had once said to him thatif you gave humans perfect memory, you would not need more than one sexual experience. so how much real lifedo you think we have to mix in with the virtual life to make a workable modern life?dertouzos: i do not think i need to answer that. you reported precisely. what is good for minsky, isgood for the rest of the world.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.appendixesdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.89appendixaletter from dr. bruce alberts, president,national academy of sciencesdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.90defining a decade: envisioning cstbõs second 10 yearsdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.91appendixbsymposium attendeesmarshall abramsthe mitre corporationraymond albersbell atlantic corporationfrances allenibm t.j. watson research centersaul amarelrutgers universitygary anthescomputerworldruzena bajcsyuniversity of pennsylvaniabarbara blausteinnational science foundationrobert bonomettibell atlantic corporationanita borgdigital equipment corporationgeorge brandontelecomunications reporttimothy brennanuniversity of marylandcharles brownsteincrossindustry working teamaubrey bushnational science foundationstephen caimiciticorp technology officevirginia castoroffice of the secretary of defensejohn cavalliniu.s. department of energyjane cavinesseducomjohn cherniavskynational science foundationmelvyn cimentnational science foundationdavid clarkmassachusetts institute of technologydefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.92defining a decade: envisioning cstbõs second 10 yearsclement colelocus computing corporationeileen collinsnational science foundationgeorge cotternational security agencydorothy denninggeorgetown universitymichael dertouzosmassachusetts institute of technologyrobert direnzothe world bankjeff dozieruniversity of california at santa barbaralarry druffelsouth carolina research associationdavid farberuniversity of pennsylvaniaedward feigenbaumu.s. air forcejeanne ferrischronicle of higher educationkevin finneranissues in science and technologyfrancis dummer fisheruniversity of texas at austinbarbara fossumic2 institute at the university of texas at austinhoward frankdefense advanced research projects agencydarleen fishernational science foundationpeter freemangeorgia institute of technologyhenry fuchsuniversity of north carolinasamuel fullerdigital equipment corporationchristina gabrielnational science foundationcharles geschkeadobe systems incorporatedhelen gigleyoffice of naval researchjerome glennnew earth radionorman glicknational security agencyseymour goodmanuniversity of arizonaian graigglobal policy group, incorporateddonald greenbergcornell universitywilliam griffingte laboratories, incorporatedbarbara groszharvard universityjuris hartmaniscornell universitydonald heaththe internet societyharry hedgesnational science foundationcarol hendersonamerican library associationdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.appendix b93lance hoffmanthe george washington universitylee holcombnational aeronautics and space administrationcharles hollandair force office of scientific researchjohn hopcroftcornell universitymark jacobsair force office of scientific researchanita jonesdepartment of defensedeborah josephuniversity of wisconsincharles judiceeastman kodakrobert kahncorporation for national research initiativessidney karinuniversity of california at san diegojulius katzhills and companystuart katzkenational institute of standards and technologystephen thomas kentbbn communications corporationpradeep khosladefense advanced research projects agencyrichard kieburtzoregon graduate instituteedwin kiesterreaderõs digestsally kiesterreaderõs digestthomas kitchensu.s. department of energyleonard kleinrockuniversity of california at los angelesellen knappcoopers & lybrandwilliam kneislythinking machines corporationbutler lampsonmicrosoft corporationneal lauranceford motor companyalfred leeu.s. department of commerce/ntiadonald lindbergthe national library of medicinebarbara liskovmassachusetts institute of technologyallen lockedepartment of statewilliam lovelessfederal technology reportgloria lubkinphysics todayrobert luckybellcorepatrice lyonsattorney at lawjohn majormotorola, incorporatedpamela mccorduckauthorfrancis mcdonoughgeneral services administration/itsdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.94defining a decade: envisioning cstbõs second 10 yearsdavid messerschmittuniversity of california at berkeleyrobert metcalfeinternational data groupmelvin montemerlonational aeronautics and space administrationlisa mountdepartment of statedavid nelsonu.s. department of energymichael nelsonoffice of science and technology policysusan nunnerymyricom, incorporatedrod oldehoeftu.s. department of energystewart personickbellcorewilliam powlessinside energy/the mcgraw hill companiesarati prabhakarnational institute of standards and technologywilliam pressharvard college  observatoryraj reddycarnegie mellon universityjohn phillip riganatidavid l. sarnoff research centerdamian saccocioamerica onlinepaul schneckmitretek systems, incorporatedmischa schwartzcolumbia universitymary anne scottdepartment of energycharles seitzmyricom, incorporatedmary shawcarnegie mellon universityedward shortliffestanford university school of medicinemargaret simmonsnational coordination office for high performance computing and communicationsmichael simmonsanta fe institutealexander singerfilm directorjudith singerwriterirwin sitkinaetna (retired)paul smithnational coordination office for high performance computing and communicationslawrence snyderuniversity of washingtonrowan snydercoopers & lybrandavi spectorgeorge washington universityhillary (traub) spectorhillaryõs fine jewelrywilliam spencersematechrobert spinradxerox corporationdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.appendix b95gary strongnational science foundationrobert sullivanuniversity of texaslawrence teslerapple computer, incorporatedjoseph traubcolumbia universityleslie vadaszintel corporationandries van dambrown universityandre van tilborgoffice of naval researchshukri wakidnational institute of standards and technologycaroline wardlenational science foundationjane williamsu.s. national commission on libraries & information scienceshmuel winogradibm t.j. watson research centerjoan winstontrusted information systems, incorporatedfred woodnational library of medicinewilliam wulfuniversity of virginiadavid wyefederal communications commissionwu yishanembassy of chinapaul youngnational science foundationdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.96david d. clark, massachusetts institute of technologydavid clark, who became cstbõs chair in 1996, graduated from swarthmore college in 1966, and receivedhis ph.d. from mit in 1973. he has worked since then at the mit laboratory for computer science, where he iscurrently a senior research scientist in charge of the advanced network architecture group. dr. clarkõs researchinterests include networks, network protocols, operating systems, distributed systems, and computer and communications security. after receiving his ph.d., he worked on the early stages of the arpanet and on the development of token ring local area network technology. since the mid1970s, dr. clark has been involved in thedevelopment of the internet. from 19811989, he acted as chief protocol architect in this development, andchaired the internet activities board. his current research area is protocols and architectures for very large andvery highspeed networks. specific activities include extensions to the internet to support realtime traffic,explicit allocation of service, pricing, and new network technologies. in the security area, dr. clark participatedin the early development of the multilevel secure multics operating system. he developed an informationsecurity model that stresses integrity of data rather than disclosure control. dr. clark is a member of the ieee, theacm, and the national academy of engineering. he received the acm sigcomm award and the ieee awardin international communications for his work on the internet. he is a consultant to a number of companies andserves on the boards of two corporations. dr. clark chaired the committee that produced the cstb report,computers at risk: safe computing in the information age. he also served on the committees that produced thecstb reports, toward a national research network; realizing the information future: the internet and beyond;and the unpredictable certainty: information infrastructure through 2000.michael l. dertouzos, massachusetts institute of technologymichael dertouzos, a founding member of cstb, is professor of computer science and electrical engineeringand director of the mit laboratory for computer scienceñthe home base of the world wide web. he is authoror coauthor of six books. one, made in america: regaining the productive edge, is the result of the study by themit commission on industrial productivity, which he chaired. dr. dertouzos is a member of the nationalacademy of engineering and an advisor to the u.s. and european governments. he concentrates his currentefforts on the architecture, uses, and impact of tomorrowõs information infrastructures. he accompanied viceappendixcbiographies of presentersdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.appendix c97president albert gore as a u.s. delegate to the g7 meeting on the global information society in brussels.professor dertouzos is a dual national of the united states and the european union, and is involved on both sidesof the atlantic with the strategic steering of governments and large organizations into the information age. dr.dertouzos was the prime mover behind cstbõs report, the national challenge in computer science and technology.edward a. feigenbaum, u.s. air forceedward feigenbaum, a founding member of cstb, is chief scientist of the u.s. air force, washington, d.c.he serves as chief scientific advisor to the chief of staff and the secretary and provides assessments on a wide rangeof scientific and technical issues affecting the air force mission. dr. feigenbaum has served on the darpainformation science and technology study committee. he has been a faculty member at stanford university for29 years, and is founder and codirector of the knowledge systems laboratory, a leading laboratory for work inknowledge engineering and expert systems. a professor of computer science, dr. feigenbaum is internationallyknown for his work in artificial intelligence and expert systems, and is the coauthor of seven books and monographs as well as some 60 scientific papers. he is a cofounder of three startup firms in applied artificialintelligence and has served on the board of directors of several companies. he has served as a member of theboard of regents of the national library of medicine and as a member of nsfõs computer science advisoryboard. dr. feigenbaum received his b.s. in electrical engineering from the carnegie institute of technology,pennsylvania; and his ph.d. degree from the graduate school of industrial administration, carnegie institute oftechnology, pennsylvania. in 1991, he was elected a member of the american academy of arts and sciences andalso received the career achievement award from the world congress on expert systems (the feigenbaummedal, named in his honor). he was elected a fellow of the american institute of medical and biologicalengineering in 1994, and he is a member of the national academy of engineering.howard frank, defense advanced research projects agencyhoward frank at the time of the symposium was director of darpaõs information technology office, wherehe managed a $350million annual budget aimed at advancing the frontiers of information technology. dr. frankwas responsible for darpaõs research in advanced computing, communications, software, and intelligent systems, with programs ranging from language systems and humancomputer interaction to scalable highperformance computing, networking, security, and microsystems. before becoming director of ito, he was the directorof the computing systems technology office (now part of ito), and earlier, special assistant to the director ofdarpa for information infrastructure technology. while at darpa, dr. frank helped found the darpa/disajoint program office, a joint activity with the defense information systems agency. he then completed a twoyear assignment as its first director. in september 1997 dr. frank became the dean of the college of business andmanagement at the university of maryland.dr. frank was chair of the technology policy working group (tpwg) of the administrationõs informationinfrastructure task force and led the tpwgõs advanced digital video and security process projects. he is alsodarpaõs representative on the white house national science and technology councilõs committee on information and communications. dr. frank has been a member of six editorial boards and a featured speaker at over 100business and professional meetings; he has authored over 190 articles and chapters in books. dr. frank is a fellowof the ieee. he is a senior fellow at the wharton schoolõs sei center for advanced studies in management.before joining darpa, dr. frank was an advisor to large companies in information and corporate strategy,market positioning, and mergers and acquisitions. earlier, he was founder, chairman, and ceo of networkmanagement inc.; president and ceo of contel information systems (a subsidiary of contel); president, ceo,and founder of the network analysis corporation; a visiting consultant within the executive office of thepresident of the united states in charge of its network analysis activities; and an associate professor at theuniversity of california, berkeley. he served on the committee that produced the cstb report, toward anational research network.defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.98defining a decade: envisioning cstbõs second 10 yearshenry fuchs, university of north carolinahenry fuchs, a cstb member at the time of the symposium, is federico gil professor of computer scienceand adjunct professor of radiation oncology at the university of north carolina at chapel hill. he received a b.a.in information and computer science from the university of california at santa cruz in 1970, and a ph.d. incomputer science from the university of utah in 1975. he received the 1992 computer graphics achievementaward from acm/siggraph and the 1992 national computer graphics association academic award. he wasan associate editor of acm transactions on graphics (19831988) and guest editor of its first issue (january1982). he was the technical program chair for the acm/siggraph 1981 conference, chairman of the 1985chapel hill conference on advanced research in vlsi, chairman of the 1986 chapel hill workshop on interactive 3d graphics, codirector of the nato advanced research workshop on 3d imaging in medicine (1990),and cochair of the national science foundation workshop on research directions in virtual environments(1992). he serves on various advisory committees for government and industrial groups.seymour e. goodman, university of arizonaseymour goodman is professor of mis and policy, a member of the center for middle eastern studies at theuniversity of arizona (since 1981), and carnegie science fellow at the center for international security and armscontrol at stanford university (since 1994). he studies the global diffusion and other international developmentsin information technologies and related public policy questions. professor goodman has had various permanentand visiting appointments at the university of virginia (computer science, center for soviet and east europeanstudies), princeton university (mathematics, woodrow wilson school of public and international affairs), andthe university of chicago (economics). he has served on numerous government, academic, and industry studyand advisory committees, and is contributing editor for international perspectives for the communications of theacm. he has visited all seven continents and approximately 70 countries during the past 15 years. professorgoodman was an undergraduate at columbia university and received his ph.d. from the california institute oftechnology. he chaired the committee that produced cstbõs report, global trends in computer technology andtheir impact on export control.donald p. greenberg, cornell universitydonald greenberg is the jacob gould schurman professor of computer graphics at cornell university. hewas the founding director of nsfõs science and technology center for computer graphics and scientific visualization. he is also the director of the program of computer graphics and former director of the computeraideddesign instructional facility at cornell university. since 1965, he has been researching and teaching in the fieldof computer graphics; he is primarily concerned with physicallybased image synthesis and with applying graphictechniques to a variety of disciplines. his specialties include color science, parallel processing, and realistic imagegeneration. he teaches computer graphics, computeraided design, digital photography, and computer informationcourses in the computer science, architecture, art, and business schools, respectively. his application work nowfocuses on medical imaging, architectural design, digital photography, and interactive video. in 1987, he receivedthe acm steven coons award, the highest honor in the field, for his outstanding creative contributions incomputer graphics. he also received the national computer graphics association academic award in 1989. heis a member of the national academy of engineering and a fellow of the international association of medical andbiological engineering and of the acm.juris hartmanis, cornell universityjuris hartmanis at the time of the symposium was a cstb member and the walter r. read professor ofengineering and computer science at cornell university. in 1996 he became nsfõs assistant director for computer and information science and engineering. he was the first chair of cornellõs computer science department,defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.appendix c99founded in 1965, and had been at cornell since then. his research interests include theory of computation andcomputational complexity. he shared the 1993 acm turing award with r.e. stearns for their seminal work oncomputational complexity. in 1995, he received the b. bolzan gold medal from the academy of sciences of theczech republic and the honorary dr.h.c. from the university of dortmund, germany. dr. hartmanis is a memberof the national academy of engineering, american academy of arts and sciences, new york academy ofsciences, fellow of the acm and aaas, and foreign member of the latvian academy of sciences. he hasauthored two books, algebraic structure theory of sequential machines and feasible computations and provablecomplexity properties. in addition, he has authored over 140 research papers. dr. hartmanis graduated with ab.s. in physics from the university of marburg in 1949. he received his m.a. in mathematics from the universityof kansas city and his ph.d. in mathematics from the california institute of technology in 1951 and 1955,respectively. he chaired the committee that produced the cstb report, computing the future: a broader agendafor computer science and engineering.deborah a. joseph, university of wisconsindeborah joseph, a current cstb member, is an associate professor of computer science and mathematics atthe university of wisconsin. she received a b.a. (interdisciplinaryecology, 1976) from hiram college, and anm.s. (computer science, 1978) and a ph.d. (computer science, 1981), both from purdue university. dr. josephheld the national science foundationõs presidential young investigator award for 19851990. her researchinterests include complexity theory, computational problems in molecular biology, computational geometry, andmathematical logicrecursion theory.leonard kleinrock, university of california, los angelesleonard kleinrock, a founding member of cstb, has been a professor of computer science at the universityof california, los angeles, since 1963. he received his b.s. degree in electrical engineering from the citycollege of new york in 1957, and his m.s. and ph.d. degrees in electrical engineering from mit in 1959 and1963, respectively. his research interests focus on performance evaluation of highspeed networks and paralleland distributed systems. he has had over 190 papers published and is the author of five books. he is the principalinvestigator for the darpa advanced networking and distributed systems grant at ucla. he is also founderand ceo of technology transfer institute, a computercommunications seminar and consulting organizationlocated in santa monica, ca.dr. kleinrock is a member of the national academy of engineering, a guggenheim fellow, and an ieeefellow. he has received numerous best paper and teaching awards, including the icc 1978 prize winning paperaward, the 1976 lanchester prize for outstanding work in operations research, and the communications society1975 leonard g. abraham prize paper award. in 1982, he received the townsend harris medal. also in 1982,he was cowinner of the l. m. ericsson prize, presented by his majesty king carl gustaf of sweden, for hisoutstanding contribution in packet switching technology. in july 1986, dr. kleinrock received the 12th marconiinternational fellowship award, presented by his royal highness prince albert, brother of king baudouin ofbelgium, for his pioneering work in the field of computer networks. in the same year, he received the uclaoutstanding teacher award. in 1990, he received the acm sigcomm award recognizing his seminal role indeveloping methods for analyzing packet network technology, and in 1996, he was given the harry goode award.dr. kleinrock chaired the committees that produced the cstb reports, toward a national research network andrealizing the information future: the internet and beyond. he served on the committee that produced the cstbreport, computing the future: a broader agenda for computer science and engineering.ellen m. knapp, coopers & lybrandellen knapp is vice chairman, technology, for coopers & lybrand, l.l.p., and a member of the firmõsmanagement committee and its board of partners. she is responsible for providing technology leadership anddefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.100defining a decade: envisioning cstbõs second 10 yearsembedding technology in all lines of business within the firm. in addition, ms. knapp is chairman of c&lõsinternational technology management group, providing strategy, policy, and standards to c&l member firms,and ensuring consistency of service to c&lõs multinational clients. c&l international, through its member firms,operates in 750 offices in 140 countries with a 1995 revenue of $5.8 billion. ms. knapp is responsible fordeveloping pioneering uses of technology throughout all areas of the firm, including quality and efficiency ofclient services, internal operations, and creation of innovative services to clients. one of the countryõs primaryauthorities on the strategic use of technology, knappõs role is to create competitive advantage and catalyzeorganizational change in c&l through the development and application of leadingedge technology. formerly,ms. knapp was a senior partner in coopers & lybrandõs management consulting services practice and nationaldirector of information technology consulting for the u.s. firm.before joining c&l, ms. knapp was responsible for establishing the advanced technology consulting practiceat boozallen & hamilton. she has provided both technical and management consulting services to a wide rangeof public and private sector clients, including transnational consumer products companies, telecommunicationsfirms, large service sector organizations, and worldwide manufacturing enterprises. ms. knapp has receivedworldwide recognition for her work in advanced technologies. she has published professional papers, contributedto edited texts, and served as a speaker or moderator at numerous symposia, television appearances, and pressconferences in the united states, europe, and asia. she is a featured speaker at the 1996 women shapingtechnology conference and a juror for the 1996 lemelsonmit award for invention and innovation. she has coauthored two books with peter keen, every managerõs guide to business processes (1995) and process payoffs:building value through business process investment (in publication), both published by harvard business schoolpress. she served on the committee that produced the cstb report, information technology in the service society:a twentyfirst century lever.robert w. lucky, bellcorerobert lucky, a founding member of cstb, is corporate vice president of applied research at bellcore. bornin pittsburgh, pa, he attended purdue university, where he received a b.s. degree in electrical engineering in1957, and m.s. and ph.d. degrees in 1959 and 1961. after graduation, he joined at&t bell laboratories inholmdel, nj, where he was initially involved in studying ways of sending digital information over telephone lines.the best known outcome of this work was his invention of the adaptive equalizerña technique for correctingdistortion in telephone signals that is used in all highspeed data transmission today. the textbook on datacommunications that he coauthored became the most cited reference in the communications field over the periodof a decade. at bell labs, dr. lucky became executive director of the communications sciences researchdivision in 1982, where he was responsible for research on the methods and technologies for future communication systems. in 1992, he left bell labs to assume his present position at bellcore. he has served as president ofthe communications society of the ieee, and as vice president and executive vice president of the parent ieee.he has been editor of several technical journals, including the proceedings of the ieee, and, since 1982, he haswritten the bimonthly òreflectionsó column of personalized observations about the engineering profession inspectrum magazine. in 1993, these òreflectionsó columns were collected in the ieee press book, luckystrikes...again.dr. lucky is a fellow of the ieee and a member of the national academy of engineering. he is also aconsulting editor for a series of books on communications through plenum press. he has been on the advisoryboards or committees of many universities and government organizations and was chairman of the scientificadvisory board of the u.s. air force from 19861989. he was the 1987 recipient of the prestigious marconi prizefor his contributions to data communications and has been awarded honorary doctorates from purdue universityand the new jersey institute of technology. he has also been awarded the edison medal of the ieee and theexceptional civilian contributions medal of the u.s. air force. dr. lucky has been an invited lecturer at about100 different universities, and has been the guest on a number of network television shows, including bill moyersõòa world of ideas,ó where he has discussed the impacts of future technological advances. he is the author of thepopular book, silicon dreams, which is a semitechnical and philosophical discussion of the ways in which bothdefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.appendix c101humans and computers deal with information. dr. lucky served on or advised the committees that produced thefollowing cstb reports: keeping the u.s. computer industry competitive: defining the agenda; keeping the u.s.computer industry competitive: systems integration; keeping the u.s. computer and communications industrycompetitive: convergence of computing, communications, and entertainment; realizing the information future:the internet and beyond; and the unpredictable certainty: information infrastructure through 2000.john major, qualcomm, inc.john major, a current cstb member, at the time of the symposium was the senior vice president and assistantchief corporate staff officer for motorola. one of his responsibilities was leading motorolaõs initiative to becomea global leader in software. previously, he managed the worldwide systems group that developed and manufactured private radio systems for voice and data for public safety and business users. mr. major holds a b.s. inmechanical and aerospace engineering from the university of rochester, an m.s. in mechanical engineering fromthe university of illinois, an m.b.a. with distinction from northwestern university, and a j.d. from loyolauniversity. he serves as chairman of the board of directors of the telecommunications industry association(tia), and he serves on the board of directors of the electronics industry association (eia). mr. major also serveson the board of directors of littelfuse and lennox corporation, and is trustee of the allendale school, which helpsdisadvantaged children. he currently chairs the board of health for barrington hills.robert m. metcalfe, international data grouprobert metcalfe, a founding member of cstb, is executive correspondent, infoworld, and vice president/technology, international data group. he was born in 1946 in brooklyn, new york, and grew up on long island.he graduated in 1969 after five years at mit, receiving a bachelorõs degree in electrical engineering and abachelorõs degree from the sloan school of management. in 1970, bob received a masterõs degree in appliedmathematics from harvard university. in 1973, he received a ph.d. from harvard in computer science forresearch done at mitõs project mac on packet switching in the darpa and aloha computer networks. in 1972,dr. metcalfe moved to the computer science laboratory at the xerox palo alto research center (parc) to joinin the early development of personal computing. in 1973, he invented ethernet, the localarea networkingtechnology on which he shares four patents.while at parc, he began eight years of parttime teaching at stanford university, finishing in 1983 as aconsulting associate professor of electrical engineering with a new course on distributed computing. in 1976,metcalfe moved to xeroxõs systems development division to manage microprocessor and communication developments that led, long after he left, to the xerox star workstation. metcalfe left xerox in 1979 to promote personalcomputer localarea networks (pc lans) and, especially, ethernet. also in 1979, metcalfe founded 3comcorporation, the fortune 500 computer networking company where he held various positions, including chairmanof the board of directors, chief executive officer, president, vice president of engineering, vice president of salesand marketing, chief technical officer, and general manager consecutively of the software, workstation, andhardware divisions. metcalfe retired from 3com in 1990 after 11 years.in june 1996, dr. metcalfe was awarded the ieee medal of honor for his exemplary and sustained leadershipin the development, standardization, and commercialization of ethernet. in 1980, he received the grace murrayhopper award from the acm, and, in 1988, the alexander graham bell medal from the ieeeñboth for hisinvention, standardization, and commercialization of localarea networks. metcalfeõs many publications includepacket communication, his groundbreaking harvard ph.d. dissertation published in book form in 1996; the oftencited ethernet: distributed packet switching for local computer networks, with david boggs in the communications of the acm, july 1976; and local networks of personal computers, at the ninth world computercongress in paris in 1983.dr. metcalfe served for a year on the executive office of the presidentõs advisory committee on informationnetworks. for two years he was chairman of the corporation for open systems, promoting worldwide computerand telephone networking standards. in 199192, dr. metcalfe was a visiting fellow at wolfson college in thedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.102defining a decade: envisioning cstbõs second 10 yearscomputer laboratory of the university of cambridge, england. dr. metcalfe was conference chair for acm97:the next 50 years of computing, san jose convention center, march 15, 1997.david b. nelson, department of energydavid nelson is associate director of the office of energy research for computational and technologyresearch in the u. s. department of energy. he manages programs that include technology research, technologytransfer, mathematics, computer and computational science, the energy sciences data network (esnet), andseveral supercomputer centers. he also serves as the office of energy research member of the departmentalstandards committee, which is overseeing significant changes in the departmentõs approach to environment,safety, and health. previously, he served as associate director of energy research, providing general assistance tothe director of energy research. specific responsibilities included: oversight of environment, safety, and health;technology transfer and industrial cooperation; computing, telecommunications, and information; and terminationof the superconducting super collider. he is past chairman of the national science and technology councilsubcommittee on high performance computing, communications, and information technology, a multiagencyplanning and coordinating body of the federal government organized under the presidentõs science advisor. heis the departmentõs alternate representative on the administrationõs information infrastructure task force, organized under the national economic council. dr. nelson moved to the department of energy in 1979 from the oakridge national laboratory (ornl), where he was a research scientist working mainly in theoretical plasmaphysics and its applications to fusion energy, and also in defense and environmental research. he headed themagnetohydrodynamics theory group in the fusion energy division.dr. nelson received his a.b. cum laude from harvard university, majoring in engineering sciences. afterworking as an electrical engineer in new york city, he studied at the courant institute of mathematical sciencesat new york university, where he received the m.s. and ph.d. degrees, both in mathematics. he receivedadditional graduate training in mathematics at the freie universitat, berlin, and in physics at columbia universityand the university of tennessee. in 197576, he returned to the courant institute as visiting member on leave fromornl. he is the author of numerous papers in theoretical plasma physics, computational science, and researchpolicy, and is a member of the american physical society.michael nelson, federal communications commissionmichael nelson at the time of the symposium was special assistant for information technology at the whitehouse office of science and technology policy. he has worked closely with both jack gibbons, the presidentõsscience advisor, and vice president gore on a wide range of issues relating to the national information infrastructure (nii), including telecommunications policy, highperformance computing, encryption, and informationpolicy. he has been part of the information infrastructure task force, which is responsible for coordinating theadministrationõs nii initiative, and has worked closely with the vice president on the administrationõs new globalinformation infrastructure initiative to link together national and international networks in a seamless ònetwork ofnetworks.ó he is now responsible for technology policy at the federal communications commission.prior to moving to the white house, dr. nelson served for five years on the staff of the senate commercecommittee, where he worked closely with thensenator gore, the chairman of the subcommittee on science,technology, and space. among the issues he handled were global warming, highperformance computing research, earthquake issues, antarctica, and biotechnology. he was the lead senate staffer on goreõs highperformance computing act of 1991, which authorized the $1billion hpcc program that is helping to develop thetechnologies needed for the nii. dr. nelson received a b.s. in geology from the california institute of technologyin 1981 and a ph.d. in geophysics from mit in 1988.raj reddy, carnegie mellon universityraj reddy, a founding member of cstb, is dean of the school of computer science at carnegie mellonuniversity and the herbert a. simon university professor of computer science and robotics. dr. reddy joineddefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.appendix c103carnegie mellonõs department of computer science in 1969 and served as director of the robotics institute from1979 to 1992. previously, he was an assistant professor of computer science at stanford university from 1966 to1969, and served as an applied science representative for ibm in australia from 1960 to 1963. his researchinterests include the study of humancomputer interaction and artificial intelligence. his current research projectsinclude speech recognition and understanding systems; multimedia digital libraries; justintime learning technologies; and the automated machine shop project. his professional honors include: fellow of ieee, asa, andaaai; member of the national academy of engineering; president of aaai, 198789; ibm research ralphgomory fellow, 1991; and the turing award, 1994. dr. reddy was presented the legion of honor by presidentmitterrand of france in 1984. he served on the committees that produced the cstb reports, computing thefuture: a broader agenda for computer science and engineering and information technology for manufacturing.charles l. seitz, myricom, inc.charles seitz, vice chair of cstb at the time of the symposium, is the president of myricom, inc., a startupcompany involved in research, development, production, and sales of highspeed computers and localarea networks. during the 16 years prior to founding myricom, he was a professor of computer science at the californiainstitute of technology, where his research and teaching were in the areas of vlsi design, computer architectureand programming, and concurrent computation. he earned s.b. (1965), s.m. (1967) and ph.d. (1971) degreesfrom mit, where he was also an instructor and the recipient of the goodwin medal òfor conspicuously effectiveteaching.ó he was a consultant and member of the technical staff of the evans & sutherland computer corporation during its initial years (196872), an assistant professor of computer science at the university of utah (197072), and a consultant and leader of several research and development projects for burroughs corporation (197178). his research in vlsi and concurrent computing at caltech, including the development of the cosmic cubemulticomputer, was selected by science digest as one of the top 100 innovations in 1985. dr. seitz was elected tothe national academy of engineering in 1992 òfor pioneering contributions to the design of asynchronous andconcurrent computer systems.ó dr. seitz served on the committee that produced the cstb report, computing thefuture: a broader agenda for computer science and engineering.mary shaw, carnegie mellon universitymary shaw, a founding member of cstb, is the alan j. perlis professor of computer science, associate deanfor professional programs, and member of the human computer interaction institute at carnegie mellon university. she has been a member of this faculty since completing the ph.d. degree at carnegie mellon in 1972. from1984 to 1987, she served as chief scientist of cmuõs software engineering institute. she had previously receiveda b.a (cum laude) from rice university and worked in systems programming and research at the researchanalysis corporation and rice university.her research interests in computer science lie primarily in the areas of programming systems and softwareengineering, particularly software architecture, programming languages, specifications, and abstraction techniques.particular areas of interest and projects have included software architectures (vitruvius, unicon), technologytransition (sei), program organization for quality human interfaces (descartes), programming language design(alphard, tartan), abstraction techniques for advanced programming methodologies (abstract data types, genericdefinitions), reliable software development (strong typing and modularity), evaluation techniques for software(performance specification, compiler contraction, software metrics), and analysis of algorithms (polynomial derivative evaluation). she has developed innovative curricula from the introductory to the doctoral level.dr. shaw is an author or editor of seven technical books and more than 100 papers and reports. in 1993, shereceived the warnier prize for contributions to software engineering. she is a fellow of the acm, ieee, andaaas. she is also a member of the society of the sigma xi, the new york academy of sciences, and workinggroup 2.4 (system implementation languages) of the international federation of information processing societies. in addition, she has served on a number of advisory and review panels, conference program committees, anddefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.104defining a decade: envisioning cstbõs second 10 yearseditorial boards. dr. shaw served on the committee that produced the cstb report, scaling up: a researchagenda for software engineering.edward h. shortliffe, stanford universityedward shortliffe, a cstb member at the time of the symposium, is professor of medicine and of computerscience at stanford university. he received an a.b. in applied mathematics from harvard college in 1970, astanford ph.d. in medical information sciences in 1975, and an m.d. at stanford in 1976. during the early 1970s,he was principal developer of the medical expert system known as mycin. after a pause for internal medicinehousestaff training at harvard and stanford between 1976 and 1979, he joined the stanford internal medicinefaculty, where he has directed an active research program in clinical information systems development. hisinterests include the broad range of issues related to integrated decisionsupport systems and their effectiveimplementation. he has spearheaded the formation of a stanford degree program in medical informatics, andcontinues to divide his time between clinical medicine and medicalinformatics research. he is currently associatedean for information technology at stanford university school of medicine.dr. shortliffe is a member of the institute of medicine, the american society for clinical investigation, theassociation of american physicians, and the american clinical and climatological association. he has also beenelected to fellowship in the american college of medical informatics, the american association for artificialintelligence, and the american college of physicians. he sits on the editorial boards of several medical computingand artificial intelligence publications. he has served on the federal networking advisory committee, thebiomedical library review committee, and was recipient of a research career development award from the latteragency. in addition, he received the grace murray hopper award of the association for computing machinery in1976 and has been a henry j. kaiser family foundation faculty scholar in general internal medicine. dr.shortliffe has authored over 150 articles and books in the fields of medical computing and artificial intelligence.volumes include computerbased medical consultations: mycin (elsevier/north holland, 1976), readings inmedical artificial intelligence: the first decade (with w.j. clancey; addisonwesley, 1984), rulebased expertsystems: the mycin experiments of the stanford heuristic programming project (with b.g. buchanan;addisonwesley, 1984), and medical informatics: computer applications in health care (with l.e. perreault, g.wiederhold, and l.m. fagan; addisonwesley, 1990). dr. shortliffe cochaired the cstb planning session onòthe roles of information infrastructure in health and health care.ówilliam j. spencer, sematechsince october 1990, william spencer, a former member of cstb, has been president and chief executiveofficer of sematech in austin, texas. sematech is a research and development consortium jointly fundedby semiconductor industry member companies and the u.s. government, established to solve the technical challenges required to keep the u.s. number one in the global semiconductor industry. before joining sematech,dr. spencer was group vice president and senior technical officer at xerox corporation in stamford, connecticut.he has also served as vice president of xerox palo alto research center, director of systems development atsandia national laboratories in livermore, and director of microelectronics at sandia national laboratories inalbuquerque. he began his career at bell telephone laboratories. dr. spencer received an a.b. degree fromwilliam jewell college in liberty, missouri, followed by an m.s. degree in mathematics and a ph.d. in physicsfrom kansas state university. he was awarded the regents meritorious service medal from the university ofnew mexico in 1981, and an honorary doctorate degree from william jewell college in 1990. he is a member ofthe national academy of engineering, a fellow of ieee, and serves on numerous advisory groups and boards.joseph f. traub, columbia universityjoseph traub, founding chair of cstb, is the edwin howard armstrong professor of computer science atcolumbia university and external professor at the santa fe institute. from 1971 to 1979, he was head of thedefining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.appendix c105computer science department at carnegie mellon university. beginning in 1959, dr. traub pioneered research inwhat is now called informationbased complexity, which studies the computational complexity of problems withpartial or contaminated information. his current work ranges from new fast methods for pricing financial derivatives to investigating what is scientifically knowable. as part of the latter, he directs a center for the study of limitsto scientific knowledge at the santa fe institute, partially funded by the alfred p. sloan foundation.dr. traub is the author or editor of eight books and some 100 journal articles. he is the founding editor of thejournal of complexity. a festschrift in celebration of his 60th birthday was recently published. dr. traub hasreceived numerous honors, including election to the national academy of engineering (1985), and he is a fellowof both the aaas (1972) and the acm (1993). he received the 1991 emanuel r. piore gold medal award ofieee and the 1992 distinguished service award of the computing research association. he has been shermanfairchild distinguished scholar at the california institute of technology and received a distinguished seniorscientist award from the alexander von humboldt foundation. he was selected by the accademia nazionale deilincei in rome to present the 1993 lezioni lincei, a cycle of six lectures that will be published by cambridgeuniversity press. he has served as advisor or consultant to the senior management of numerous organizationsincluding ibm, hewlettpackard, schlumberger, stanford university, inria (paris), the federal judiciary center, darpa, and nsf. dr. traub served on the committee that produced the cstb report, evolving the highperformance computing and communications initiative to support the nationõs information infrastructure.dr. andries van dam, brown universityandries van dam, a founding member of cstb, has been on the faculty of brown university since 1965. hewas one of the founders of the department of computer science, and its first chairman from 1979 to 1985. he isalso director of the nsf/darpa national science and technology center for graphics and visualization, aresearch consortium including brown university, california institute of technology, cornell university, university of north carolina, and utah university. his research has concerned computer graphics, text processing andhypermedia systems, and workstations. he has been working for nearly 30 years on systems for creating andreading òelectronic booksó with interactive illustrations, based on highresolution interactive graphics systems, foruse in teaching and research. most recently, he has been concerned with teaching objectoriented programmingand design to entering students. dr. van dam received the b.s. degree with honors from swarthmore college in1960 and the m.s. and ph.d. degrees from the university of pennsylvania in 1963 and 1966, respectively. amember of sigma xi, ieee computer society, and acm, he helped to found, and from 1971 to 1981 was an editorof, computer graphics and image processing, and was an editor of acmõs transactions on graphics from 1981to 1986. he became a member of the editorial board of the ieeeõs transactions on visualization and computergraphics. in 1967, professor van dam cofounded acmõs siggraph.in 1974, dr. van dam received the society for information displayõs òspecial recognition award,ó and in1984 the ieee centennial medal. in 1988, he received the state of rhode island governorõs science and technology award, and in 1990 he received the national computer graphics associationõs academic award. in july1991, he received siggraphõs steven a. coons award. in may 1992, brown university named him to the l.herbert ballou university professor chair and in march of 1995 to the thomas j. watson, jr., university professorof technology and education chair. in 1994, he received the karl v. karlstrom outstanding educator award, theieee fellow award, and the acm fellow award. in december 1995, he received an honorary ph.d. fromdarmstadt technical university in germany. in june 1996, he received an honorary ph.d. from his alma matter,swarthmore college. he is past chairman of the computing research association, a founder and chief scientist ofelectronic book technologies, a member of the technical advisory boards for object power, inc., the fraunhofercenter for computer graphics, and microsoft. in 1996, he was elected to the national academy of engineering forhis contributions to computer science education and graphics research.paul r. young, university of washingtonpaul young at the time of the symposium was the national science foundationõs assistant director for thedirectorate of computer and information science and engineering (cise). dr. young was responsible for 25defining a decade: envisioning cstb's second 10 yearscopyright national academy of sciences. all rights reserved.106defining a decade: envisioning cstbõs second 10 yearsprograms organized into six divisions representing the areas of computer and computation research; information, robotics and intelligent systems; advanced scientific computing; microelectronic information processingsystems; networking and communications research and infrastructure; and crossdisciplinary activities. before joining the national science foundation, dr. young was professor of computer science and engineering andassociate dean of engineering at the university of washington. he is a graduate of antioch college and receivedhis ph.d. from mit in 1963. he joined the university of washington in 1983, after 17 years at purdue university,where he was one of the first half dozen faculty members in perhaps the first computer science department in theunited states.dr. young has been a brittingham visiting professor in computer science at the university of wisconsin, hastwice taught as a visiting professor in the electrical engineering and computer sciences department at the university of california, berkeley, served briefly as chairman of the computing and information sciences department atthe university of new mexico, and has been a national science foundation postdoctoral fellow at stanforduniversity. his research interests are in theoretical computer science, with an emphasis on questions of computational complexity and on connections with mathematical logic. he is author or coauthor of some 36 researchpapers and more than a half dozen expositories in this area, and is coauthor of a graduate textbook on the generaltheory of algorithms.dr. young served on the board of directors of the computing research association from 1983 to 1991 andwas chairman from 198991. from 19831988, he served as chairman of the computer science department at theuniversity of washington. in 197780, he served on the national science foundationõs advisory subcommitteefor computer science and served as chairman of this subcommittee in 197980. dr. young has served three timeson the program committee for the acmõs symposium on the theory of computing, and he has served on both theexecutive committee and the nominating committee for acmõs special interest group on the theory of computing. he has also been chairman of the program committee for the ieee computer societyõs annual symposium onthe foundations of computer science, and he has served as both vice chairman and chairman of the computersocietyõs technical committee on the mathematical foundations of computing. he has also served on theprogram committee, and later as chair of the program committee, for the ieee structural complexity theoryconference. in 1995, he was elected a fellow of both the ieee and the acm. dr. young served on the committeethat produced the cstb report, computing professionals: changing needs for the 1990s.
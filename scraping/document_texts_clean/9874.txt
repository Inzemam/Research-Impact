detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/9874summary of a workshop on information technology research forfederal statistics68 pages | 6 x 9 | hardbackisbn 9780309381567 | doi 10.17226/9874committee on computing and communications research to enable better use ofinformation technology in government, national research councilsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.committee on computing and communications research to enable betteruse of information technology in governmentcomputer science and telecommunications boardcommission on physical sciences, mathematics, and applicationscommittee on national statisticscommission on behavioral and social sciences and educationnational research councilnational academy presswashington, d.c.summary of a workshop oninformation technology research forfederal statisticssummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved bythe governing board of the national research council, whose membersare drawn from the councils of the national academy of sciences, thenational academy of engineering, and the institute of medicine. themembers of the committee responsible for the report were chosen fortheir special competences and with regard for appropriate balance.support for this project was provided by the national science foundation under grant eia9809120. support for the work of the committeeon national statistics is provided by a consortium of federal agenciesthrough a grant between the national academy of sciences and thenational science foundation (grant number sbr9709489). any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of thesponsor.international standard book number 030907097xadditional copies of this report are available from:national academy press (http://www.nap.edu)2101 constitution ave., nw, box 285washington, d.c. 2005580062462422023343313 (in the washington metropolitan area)copyright 2000 by the national academy of sciences. all rights reserved.printed in the united states of americasummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the generalwelfare. upon the authority of the charter granted to it by the congress in 1863,the academy has a mandate that requires it to advise the federal government onscientific and technical matters. dr. bruce m. alberts is president of the nationalacademy of sciences.the national academy of engineering was established in 1964, under the charterof the national academy of sciences, as a parallel organization of outstandingengineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsorsengineering programs aimed at meeting national needs, encourages educationand research, and recognizes the superior achievements of engineers. dr. williama. wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy ofsciences to secure the services of eminent members of appropriate professions inthe examination of policy matters pertaining to the health of the public. theinstitute acts under the responsibility given to the national academy of sciencesby its congressional charter to be an adviser to the federal government and, uponits own initiative, to identify issues of medical care, research, and education.dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology withthe academyõs purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by theacademy, the council has become the principal operating agency of both thenational academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineeringcommunities. the council is administered jointly by both academies and theinstitute of medicine. dr. bruce m. alberts and dr. william a. wulf are chairmanand vice chairman, respectively, of the national research council.national academy of sciencesnational academy of engineeringinstitute of medicinenational research councilsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.committee on computing and communicationsresearch to enable better use of informationtechnology in governmentwilliam scherlis, carnegie mellon university, chairw. bruce croft, university of massachusetts at amherstdavid dewitt, university of wisconsin at madisonsusan dumais, microsoft researchwilliam eddy, carnegie mellon universityeve gruntfest, university of colorado at colorado springsdavid kehrlein, governorõs office of emergency services,state of californiasallie kellermcnulty, los alamos national laboratorymichael r. nelson, ibm corporationclifford neuman, information sciences institute, university ofsouthern californiastaffjon eisenberg, program officer and study directorrita gaskins, project assistant (through september 1999)daniel d. llata, senior project assistantivsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.computer science and telecommunications boarddavid d. clark, massachusetts institute of technology, chairjames chiddix, time warner cablejohn m. cioffi, stanford universityelaine cohen, university of utahw. bruce croft, university of massachusetts, amhersta.g. fraser, at&t corporationsusan l. graham, university of california at berkeleyjudith hempel, university of california at san franciscojeffrey m. jaffe, ibm corporationanna karlin, university of washingtonbutler w. lampson, microsoft corporationedward d. lazowska, university of washingtondavid liddle, interval researchtom m. mitchell, carnegie mellon universitydonald norman, unext.comraymond ozzie, groove networksdavid a. patterson, university of california at berkeleycharles simonyi, microsoft corporationburton smith, tera computer companyterry smith, university of california at santa barbaralee sproull, new york universitymarjory s. blumenthal, directorherbert s. lin, senior scientistjerry r. sheehan, senior program officeralan s. inouye, program officerjon eisenberg, program officergail pritchard, program officerjanet briscoe, office managerdavid drake, project assistantmargaret marsh, project assistantdavid padgham, project assistantmickelle rodgers rodriguez, senior project assistantsuzanne ossa, senior project assistantdaniel d. llata, senior project assistantvsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.commission on physical sciences,mathematics, and applicationspeter m. banks, veridian erim international, inc., cochairw. carl lineberger, university of colorado, cochairwilliam f. ballhaus, jr., lockheed martin corporationshirley chiang, university of california at davismarshall h. cohen, california institute of technologyronald g. douglas, texas a&m universitysamuel h. fuller, analog devices, inc.jerry p. gollub, haverford collegemichael f. goodchild, university of california at santa barbaramartha p. haynes, cornell universitywesley t. huntress, jr., carnegie institutioncarol m. jantzen, westinghouse savannah river companypaul g. kaminski, technovation, inc.kenneth h. keller, university of minnesotajohn r. kreick, sanders, a lockheed martin company (retired)marsha i. lester, university of pennsylvaniadusa m. mcduff, state university of new york at stony brookjanet l. norwood, former commissioner, u.s. bureau of laborstatisticsm. elisabeth pat…cornell, stanford universitynicholas p. samios, brookhaven national laboratoryrobert j. spinrad, xerox parc (retired)myron f. uman, acting executive directorvisummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.committee on national statisticsjohn e. rolph, university of southern california, chairjoseph g. altonji, northwestern universitylawrence d. brown, university of pennsylvaniajulie davanzo, rand, santa monica, californiawilliam f. eddy, carnegie mellon universityhermann habermann, united nations, new yorkwilliam d. kalsbeek, university of north carolinaroderick j.a. little, university of michiganthomas a. louis, university of minnesotacharles f. manski, northwestern universityedward b. perrin, university of washingtonfrancisco j. samaniego, university of california at davisrichard l. schmalensee, massachusetts institute of technologymatthew d. shapiro, university of michiganandrew a. white, directorviisummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.viiicommission on behavioral and social sciencesand educationneil j. smelser, center for advanced study in the behavioralsciences, stanford, chairalfred blumstein, carnegie mellon universityjacquelynne eccles, university of michiganstephen e. fienberg, carnegie mellon universitybaruch fischhoff, carnegie mellon universityjohn f. geweke, university of iowaeleanor e. maccoby, stanford universitycora b. marrett, university of massachusettsbarbara j. mcneil, harvard medical schoolrobert a. moffitt, johns hopkins universityrichard j. murnane, harvard universityt. paul schultz, yale universitykenneth a. shepsle, harvard universityrichard m. shiffrin, indiana universityburton h. singer, princeton universitycatherine e. snow, harvard universitymarta tienda, princeton universitybarbara torrey, executive directorsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.prefaceas part of its new digital government program, the national sciencefoundation (nsf) requested that the computer science and telecommunications board (cstb) undertake an indepth study of how informationtechnology research and development could more effectively supportadvances in the use of information technology (it) in government. cstbõscommittee on computing and communications research to enable betteruse of information technology in government was established to organize two specific applicationarea workshops and conduct a broaderstudy, drawing in part on those workshops, of how it research can enableimproved and new government services, operations, and interactions withcitizens.the committee was asked to identify ways to foster interaction amongcomputing and communications researchers, federal managers, and professionals in specific domains that could lead to collaborative researchefforts. by establishing research links between these communities andcreating collaborative mechanisms aimed at meeting relevant requirements, nsf hopes to stimulate thinking in the computing and communications research community and throughout government about possibilities for advances in technology that will support a variety of digitalinitiatives by the government.the first phase of the project focused on two illustrative applicationareas that are inherently governmental in natureñcrisis management andfederal statistics. in each of these areas, the study committee convened aworkshop designed to facilitate interaction between stakeholders fromixsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.xprefacethe individual domains and researchers in computing and communications systems and to explore research topics that might be of relevancegovernmentwide. the first workshop in the series explored informationtechnology research for crisis management.1 the second workshop, calledòinformation technology research for federal statisticsó and held onfebruary 9 and 10, 1999, in washington, d.c., is summarized in thisreport.participants in the second workshop, which explored it researchopportunities of relevance to the collection, analysis, and disseminationof federal statistics, were drawn from a number of communities: itresearch, it research management, federal statistics, and academic statistics (see the appendix for the full agenda of the workshop and a list ofparticipants). the workshop provided an opportunity for these communities to interact and to learn how they might collaborate more effectivelyin developing improved systems to support federal statistics. two keynote speeches provided a foundation by describing developments in thestatistics and information technology research communities. the firstpanel presented four case studies. other panels then explored a range ofways in which it is currently used in the federal statistical enterprise andarticulated a set of challenges and opportunities for it research in thecollection, analysis, and dissemination of federal statistics. at the conclusion of the workshop, a set of parallel breakout sessions was held topermit workshop participants to look into opportunities for collaborativeresearch between the it and statistics communities and to identify someimportant research topics. this report is based on those presentationsand discussions.because the development of specific requirements would of course bebeyond the scope of a single workshop, this report cannot presume to be acomprehensive analysis of it requirements in the federal statistical system.nor does the report explore all aspects of the work of the federal statisticalcommunity. for example, the workshop did not specifically address thedecennial census. presentations and discussions focused on individual orhousehold surveys; other surveys depend on data obtained from businessand other organizations where there would, for example, be less emphasison developing better survey interview instruments because the informationis in many cases already being collected through automated systems. because the workshop emphasized survey work in the federal statistical system, the report does not specifically address the full range of statistics applications that arise in the work of the federal government (e.g., biostatistical1computer science and telecommunications board, national research council. 1999.summary of a workshop on information technology research for crisis management. nationalacademy press, washington, d.c.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.prefacexiwork at the national institutes of health). however, by examining a representative range of it applications, and through discussions between it researchers and statistics professionals, the workshop was able to identify keyissues that arise in the application of it to federal statistics work and toexplore possible research opportunities.this report is an overview by the committee of topics covered andissues raised at the workshop. where possible, related issues raised atvarious points during the workshop have been consolidated. in preparing the report, the committee drew on the contributions of speakers,panelists, and participants, who together richly illustrated the role of it infederal statistics, issues surrounding its use, possible research opportunities, and process and implementation issues related to such research. tothese contributions the committee added some contextsetting materialand examples. the report remains, however, primarily an account of thepresentations and discussions at the workshop. synthesis of the workshop experience into a more general, broader set of findings and recommendations for it research in the digital government context was deferredto the second phase of the committeeõs work. this second phase is drawing on information from the two workshops, as well as from additionalbriefings and other work on the topic of digital government, to develop afinal report that will provide recommendations for refining the nsfõsdigital government program and stimulating it innovation more broadlyacross government. support for this project came from nsf, and the committee acknowledges larry brandt of the nsf for his encouragement of this effort. thenational research councilõs committee on national statistics, cnstat,was a cosponsor of this workshop and provided additional resources insupport of the project. this is a reporting of workshop discussions, andthe committee thanks all participants for the insights they contributedthrough their workshop presentations, discussions, breakout sessions, andsubsequent interactions. the committee also wishes to thank the cstbstaff for their assistance with the workshop and the preparation of thereport. in particular, the committee thanks jon eisenberg, cstb programofficer, who made significant contributions to the organization of theworkshop and the assembly of the report, which could not have beenwritten without his help and facilitation. jane bortnick griffith played akey role during her term as interim cstb director in helping conceive andinitiate this project. in addition, the committee thanks daniel llata for hiscontributions in preparing the report for publication. the committee alsothanks andy white from the national research councilõs commission onbehavioral and social sciences and education for his support and assistance with this project. finally, the committee is grateful to the reviewersfor helping to sharpen and improve the report through their comments.responsibility for the report remains with the committee.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.acknowledgment of reviewersthis report was reviewed by individuals chosen for their diverseperspectives and technical expertise, in accordance with the proceduresapproved by the national research councilõs (nrcõs) report reviewcommittee. the purpose of this independent review is to provide candidand critical comments that will assist the authors and the nrc in makingthe published report as sound as possible and to ensure that the reportmeets institutional standards for objectivity, evidence, and responsiveness to the study charge. the contents of the review comments and draftmanuscript remain confidential to protect the integrity of the deliberativeprocess. we wish to thank the following individuals for their participation in the review of this report:larry brown, university of pennsylvania,terrence ireland, consultant,diane lambert, bell laboratories, lucent technologies,judith lessler, research triangle institute,teresa lunt, sri international,janet norwood, former commissioner, u.s. bureau of labor statistics,bruce trumbo, california state university at hayward, andben schneiderman, university of maryland.although the individuals listed above provided many constructivecomments and suggestions, responsibility for the final content of thisreport rests solely with the study committee and the nrc.xiiisummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.contents1introduction and context1overview of federal statistics, 1activities of the federal statistics agencies, 2data collection, 3processing and analysis, 7creation and dissemination of statistical products, 9organization of the federal statistical system, 10information technology innovation in federal statistics, 142research opportunities17humancomputer interaction, 17user focus, 19universal access, 19literacy, visualization, and perception, 20database systems, 23data mining, 25metadata, 29information integration, 30survey instruments, 31limiting disclosure, 34trustworthiness of information systems, 41xvsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.xvicontents3interactions for information technologyinnovation in federal statistical work44appendixworkshop agenda and participants49summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.11introduction and contextoverview of federal statisticsfederal statistics play a key role in a wide range of policy, business,and individual decisions that are made based on statistics produced aboutpopulation characteristics, the economy, health, education, crime, andother factors. the decennial census population countsñalong with related estimates that are produced during the intervening yearsñwill drivethe allocation of roughly $180 billion in federal funding annually to stateand local governments.1 these counts also drive the apportionment oflegislative districts at the local, state, and federal levels. another statistic,the consumer price index, is used to adjust wages, retirement benefits,and other spending, both public and private. federal statistical data alsoprovide insight into the status, wellbeing, and activities of the u.s. population, including its health, the incidence of crime, unemployment andother dimensions of the labor force, and the nature of longdistance travel.the surveys conducted to derive this information (see the next section forexamples) are extensive undertakings that involve the collection of detailed information, often from large numbers of respondents.the federal statistical system involves about 70 government agencies.most executive branch departments are, in one way or another, involved1u.s. census bureau estimate from u.s. census bureau, department of commerce. 1999.united states census 2000: frequently asked questions. u.s. census bureau, washington,d.c. available online at <http://www.census.gov/dmd/www/faqquest.htm>.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.2information technology research for federal statistics2estimate by census bureau director of total costs in dõvera cohn. 2000. òearly signs ofcensus avoidance,ó washington post, april 2, p. a8.3for more details on federal statistical programs, see executive office of the president,office of management and budget (omb). 1998. statistical programs of the united statesgovernment. omb, washington, d.c.in gathering and disseminating statistical information. the two largeststatistical agencies are the bureau of the census (in the department ofcommerce) and the bureau of labor statistics (in the department oflabor). about a dozen agencies have statistics as their principal line ofwork, while others collect statistics in conjunction with other activities,such as administering a program benefit (e.g., the health care financingadministration or the social security administration) or promulgatingregulations in a particular area (e.g., the environmental protectionagency). the budgets for all of these activitiesñexcluding the estimated$6.8 billion cost of the decennial census2ñtotal more than $3 billion peryear.3these federal statistical agencies are characterized not only by theirmission of collecting statistical information but also by their independence and commitment to a set of principles and practices aimed at ensuring the quality and credibility of the statistical information they provide(box 1.1). thus, the agencies aim to live up to citizensõ expectations fortrustworthiness, so that citizens will continue to participate in statisticalsurveys, and to the expectations of decision makers, who rely on theintegrity of the statistical products they use in policy formulation.activities of the federal statistics agenciesmany activities take place in connection with the development offederal statisticsñthe planning and design of surveys (see box 1.2 forexamples of such surveys); data collection, processing, and analysis; andthe dissemination of results in a variety of forms to a range of users. whatfollows is not intended as a comprehensive discussion of the tasks involved in creating statistical products; rather, it is provided as an outlineof the types of tasks that must be performed in the course of a federalstatistical survey. because the report as a whole focuses on informationtechnology (it) research opportunities, this section emphasizes the itrelated aspects of these activities and provides pointers to pertinent discussions of research opportunities in chapter 2.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.introduction and context3box 1.1principles and practices for a federal statistical agencyin response to requests for advice on what constitutes an effective federal statistical agency, the national research councilõs committee on national statisticsissued a white paper that identified the following as principles and best practicesfor federal statistical agencies:principles¥relevance to policy issues¥credibility among data users¥trust among data providers and data subjectspractices¥a clearly defined and wellaccepted mission¥a strong measure of independence¥fair treatment of data providers¥cooperation with data users¥openness about the data provided¥commitment to quality and professional standards¥wide dissemination of data¥an active research program¥professional advancement of staff¥caution in conducting nonstatistical activities¥coordination with other statistical agenciessource: adapted from margaret e. martin and miron l. straf, eds. 1992. principles andpractices for a federal statistical agency. committee on national statistics, national research council. national academy press, washington, d.c.data collectiondata collection starts with the process of selection.4 ensuring thatsurvey samples are representative of the populations they measure is asignificant undertaking. this task entails first defining the population ofinterest (e.g., the u.s. civilian noninstitutionalized population, in the caseof the national health and nutrition examination survey). second, a4this discussion focuses on the process of conducting surveys of individuals. manysurveys gather information from businesses or other organizations. in some instances,similar interview methods are used; in others, especially with larger organizations, the dataare collected through automated processes that employ standardized reporting formats.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.4information technology research for federal statisticsbox 1.2examples of federal statistical surveysto give workshop participants a sense of the range of activities and purposesof federal statistical surveys, representatives of several large surveys sponsoredby federal statistical agencies were invited to present case studies at the workshop. reference is made to several of these examples in the body of this report.national health and nutrition examination surveythe national health and nutrition examination survey (nhanes) is one ofseveral major data collection studies sponsored by the national center for healthstatistics (nchs). under the legislative authority of the public health service,nchs collects statistics on the nature of illness and disability in the population; onenvironmental, nutritional, and other health hazards; and on health resources andutilization of health care. nhanes has been conducted since the early 1960s; itsninth survey is nhanes 1999.1 it is now implemented as a continuous, annualsurvey in which a sample of approximately 5,000 individuals representative of theu.s. population is examined each year. participants in the survey undergo adetailed home interview and a physical examination and health and dietary interviews in mobile examination centers set up for the survey. home examinations,which include a subset of the exam components conducted at the exam center,are offered to persons unable or unwilling to come to the center for the full examination.the main objectives of nhanes are to estimate the prevalence of diseasesand risks factors and monitoring trends for them; to explore emerging public healthissues, such as cardiovascular disease; to correlate findings of health measures inthe survey, such as body measurements and blood characteristics, and to establish a national probability sample of dna materials using nhanescollected bloodsamples. there are a variety of consumers for the nhanes data, including government agencies, state and local communities, private researchers, and companies, including health care providers. findings from nhanes are used as thebasis for such things as the familiar growth charts for children and material onobesity in the united states. for example, the body mass index used in understanding obesity is derived from nhanes data and was developed by the nationalinstitutes of health in collaboration with nchs. other findings, such as the effectsof lead in gasoline and in paint and the effects of removing it, are also based onnhanes data.21earlier incarnations of the nhanes survey were called, first, the health examination surveyand then, the health and nutrition examination survey (hanes). unlike previous surveys,nhanes 1999 is intended to be a continuous survey with ongoing data collection.2this description is adapted in part from documents on the national health and nutritionexamination survey web site. (department of health and human services, centers for disease control, national center for health statistics (nchs). 1999. national health and nutrition examination survey. available online at <http://www.cdc.gov/nchswww/about/major/nhanes/nhanes.htm>.)continuedsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.introduction and context5american travel surveythe american travel survey (ats), sponsored by the department of transportation, tracks passenger travel throughout the united states. the first primaryobjective is to obtain information about longdistance travel3 by persons living inthe united states. the second primary objective is to inform policy makers aboutthe principal characteristics of travel and travelers, such as the frequency andeconomic implications of longdistance travel, which are useful for a variety ofplanning purposes. ats is designed to provide reliable estimates at national andstate levels for all persons and households in the united statesñfrequency,primary destinations, mode of travel (car, plane, bus, train, etc.), and purpose.among the other data collected by the ats is the flow of travel between states andbetween metropolitan areas.the survey samples approximately 80,000 households in the united statesand conducts interviews with about 65,000 of them, making it the second largest(after the decennial census) household survey conducted by federal statisticalagencies. each household is interviewed four times in a calendar year to yield arecord of the entire yearõs worth of longdistance travel; in each interview, a household is asked to recall travel that occurred in the preceding 3 months. informationis collected by computerassisted telephone interviewing (cati) systems as wellas via computerassisted personal interviewing (capi).current population surveythe primary goal of the current population survey (cps), sponsored by thebureau of labor statistics (bls), is to measure the labor force. collecting demographic and labor force information on the u.s. population age 16 and older, thecps is the source of the unemployment numbers reported by bls on the firstfriday of every month. initiated more than 50 years ago, it is the longestrunningcontinuous monthly survey in the united states using a statistical sample. conducted by the census bureau for bls, the cps is the largest of the census bureauõsongoing monthly surveys. it surveys about 50,000 households; the sample isdivided into eight representative subsamples. each subsample group is interviewed for a total of 8 monthsñin the sample for 4 consecutive months, out of thesample during the following 8 months, and then back in the sample for another 4consecutive months. to provide better estimates of change and reduce discontinuities without overly burdening households with a long period of participation, thesurvey is conducted on a rotating basis so that 75 percent of the sample is commonfrom month to month and 50 percent from year to year for the same month.4box 1.2 continued3longdistance is defined in the ats as a trip of 100 miles or more. the nationwide personaltransportation survey (npts) collects data on daily, local passenger travel, covering all typesand modes of trips. for further information, see the bureau of transportationõs web page onthe npts, available online at <http://www.nptsats2000.bts.gov/>.4for more details on the sampling procedure, see, for example the u.s. census bureau.1997. cps basic monthly survey: sampling. u.s. census bureau, washington, d.c. available online at <http://www.bls.census.gov/cps/bsampdes.htm>.continuedsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.6information technology research for federal statisticssince the survey is designed to be representative of the u.s. population, aconsiderable quantity of useful information about the demographics of the u.s.population other than labor force data can be obtained from it, including occupations and the industries in which workers are employed. an important attribute ofthe cps is that, owing to the short time required to gather the basic labor forceinformation, the survey can easily be supplemented with additional questions. forexample, every march, a supplement collects detailed income and work experience data, and every other february information is collected on displaced workers.other supplements are conducted for a variety of agencies, including the department of veterans affairs and the department of education.national crime victimization surveythe national crime victimization survey (ncvs), sponsored by the bureau ofjustice statistics, is a householdbased survey that collects data on the amountand types of crime in the united states. each year, the survey obtains data froma nationally representative sample of approximately 43,000 households (roughly80,000 persons). it measures the incidence of violence against individuals, including rape, robbery, aggravated assault and simple assault, and theft directed atindividuals and households, including burglary, motor vehicle theft, and householdlarceny. other types of crimes, such as murder, kidnapping, drug abuse, prostitution, fraud, commercial burglary, and arson, are outside the scope of the survey.the ncvs, initiated in 1972, is one of two department of justice measures ofcrime in the united states, and it is intended to complement what is known aboutcrime from the federal bureau of investigationõs annual compilation of informationreported to law enforcement agencies (the uniform crime reports). the ncvsserves two broad goals. first, it provides a time series tracing changes in both theincidence of crime and the various factors associated with criminal victimization.second, it provides data that can be used to study particular research questionsrelated to criminal victimization, including the relationship of victims to offendersand the costs of crime. based on the survey, the bureau of justice statisticspublishes annual estimates of the national crime rate.5box 1.2 continued5description adapted in part from u.s. department of justice, bureau of justice statistics(bjs). 1999. crime and victims statistics. bjs, washington, d.c. available online at <http://www.ojp.usdoj.gov/bjs/cvict.htm#ncvs>.listing, or sample frame, is constructed. third, a sample of appropriatesize is selected from the sampling frame. there are many challengesassociated with the construction of a truly representative sample: asample frame of all households may require the identification of all housing units that have been constructed since the last decennial census wassummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.introduction and context75for more on survey methodology and postsurvey editing, see, for example, lars lyberget al. 1997. survey measurement & process quality. john wiley & sons, new york; andbrenda g. cox et al. 1995. business survey methods, john wiley & sons, new york. formore information on computerassisted survey information collection (casic), see mick p.couper et al. 1998. computer assisted survey information collection. john wiley & sons,new york.conducted. also, when a survey is to be representative of a subpopulation (e.g., when the sample must include a certain number of childrenbetween the ages of 12 and 17), field workers may need to interviewhouseholds or individuals to select appropriate participants.once a set of individuals or households has been identified for asurvey, their participation must be tracked and managed, includingassignment of individuals or households to interviewers, scheduling oftelephone interviews, and followup with nonrespondents. a variety oftechniques, generally computerbased, are used to assist field workers inconducting interviews (box 1.3). finally, data from interviews are collected from individual field interviewers and field offices for processingand analysis. data collected from paperandpencil interviews, of course,require data entry (keying) prior to further processing.5processing and analysisbefore they are included in the survey data set, data from respondents are subject to editing. responses are checked for missing items andfor internal consistency; cases that fail these checks can be referred back tothe interviewer or field office for correction. the timely transmission ofdata to a location where such quality control measures can be performedallows rapid feedback to the field and increases the likelihood that corrected data can be obtained. in addition, some responses require codingbefore further processing. for example, in the current population survey, verbal descriptions of industry and occupation are translated into astandardized set of codes. a variety of statistical adjustments, includinga statistical procedure known as weighting, may be applied to the data tocorrect for errors in the sampling process or to impute nonresponses.a wide variety of dataprocessing activities take place before statistical information products can be made available to the public. theseactivities depend on database systems; relevant trends in database technologies and research are discussed in the chapter 2 section òdatabasesystems.ó in addition, the processing and release of statistical data mustbe managed carefully. key statistics, such as unemployment rates, influsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.8information technology research for federal statisticsbox 1.3survey interview methods¥computerassisted personal interviewing (capi). in capi, computer software guides the interviewer through a set of questions. subsequent questionsmay depend on answers to previous questions (e.g., a respondent will be askedfurther questions about children in the household only if he/she indicates the presence of children). questions asked may also depend on the answers given in priorinterviews (e.g., a person who reports being retired will not be repeatedly askedabout employment at the outset of each interview except to verify that he or shehas not resumed employment). such questions, and the resulting data captured,may also be hierarchical in nature. in a household survey, the responses fromeach member of the household would be contained within a household file. thecombination of all of these possibilities can result in a very large number of possible paths through a survey instrument. capi software also may contain featuresto support case management.¥computerassisted telephone interviewing (cati). cati is similar in concept to capi but supports an interviewer working by telephone rather than interviewing in person. cati software may also contain features to support telephonespecific case management tasks, such as call scheduling.1¥computerassisted selfinterviewing (casi). the person being interviewedinteracts directly with a computer device. this technique is used when the directinvolvement of a person conducting the interview might affect answers to sensitivequestions. for instance, audio casi, where the respondent responds to spokenquestions, is used to gather mental health data in the nhanes.2 the techniquecan also be useful for gathering information on sexual activities and illicit drug use.¥paperandpencil interviewing (papi). paper questionnaires, which predate computeraided techniques, continue to be used in some surveys. suchquestionnaires are obviously more limited in their ability to adapt or select questions based on earlier responses than the methods above, and they entail additionalwork (keying in responses prior to analysis). it may still be an appropriate methodin certain cases, particularly where surveys are less complex, and it continues tobe relied on as surveys shift to computeraided methods. papi questionnaireshave a smaller number of paths than computeraided questionnaires; design andtesting are largely a matter of formulating the questions themselves.1the terms òcatió and òcapió have specific, slightly different meanings when used by thecensus bureau. field interviewers using a telephone from their home and a laptop are usuallyreferred to as using capi, and only those using centralized telephone facilities are said to usecati.2the casi technique is a subset of what is frequently referred to as computerized selfadministered questionnaires, a broader category that includes data collection using touchtonephones, mailoutandreturn diskettes, or web forms completed by the interviewee.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.introduction and context9ence business decisions and the financial markets, so it is critical that thecorrect information be released at the designated time and not earlier orlater. tight controls over the processes associated with data release arerequired. these stringent requirements also necessitate such measures asprotection against attack of the database servers used to generate thestatistical reports and the web servers used to disseminate the finalresults. process integrity and information system security research questions are discussed in the chapter 2 section òtrustworthiness of information systems.ócreation and dissemination of statistical productsdata are commonly released in different forms: as key statistics (e.g.,the unemployment rate), as more extensive tables that summarize thesurvey data, and as detailed data sets that users can analyze themselves.historically, most publicly disseminated data were made available in theform of printed tables, whereas today they are increasingly available in avariety of forms, frequently on the internet. tables from a number ofsurveys are made available on web sites, and tools are sometimes provided for making queries and displaying results in tabular or graphicalform. in other cases, data are less accessible to the nonexpert user. forinstance, some data sets are made available as databases or flattext files(either downloadable or on cdrom) that require additional softwareand/or userwritten code to make use of the data.a theme throughout the workshop was how to leverage it to provideappropriate and useful access to a wide range of customers. a key consideration in disseminating statistical data, especially to the general public, is finding ways of improving its usabilityñcreating a system thatallows people, whether high school students, journalists, or market analysts, to access the wealth of statistical information that the governmentcreates in a way that is useful to them. the first difficulty is simplyfinding appropriate datañdetermining which survey contains data ofinterest and which agencies have collected this information. an eventualgoal is for users not to need to know which of the statistical agenciesproduced what data in order to find them; this and other data integrationquestions are discussed in the chapter 2 section òmetadata.ó better toolswould permit people to run their own analyses and tabulations online,including analyses that draw on data from multiple surveys, possiblyfrom different agencies.once an appropriate data set has been located, a host of other issuesarise. there are challenges for both technological and statistical literacyin using and interpreting a data set. several usability considerations arediscussed in the chapter 2 section òhumancomputer interaction.ó userssummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.10information technology research for federal statisticsalso need ways of accessing and understanding what underlies the statistics, including the definitions used (a metadata issue, discussed in thechapter 2 section òmetadataó). more sophisticated users will want to beable to create their own tabulations. for example, household incomeinformation might be available in pretabulated form by zip code, but auser might want to examine it by school district.because they contain information collected from individuals or organizations under a promise of confidentiality, the raw data collected fromsurveys are not publicly released as is or in their entirety; what is releasedis generally limited in type or granularity. because this information ismade available to all, careful attention must be paid to processing thedata sets to reduce the chance that they can be used to infer informationabout individuals. this requirement is discussed in some detail in thechapter 2 section òlimiting disclosure.ó concerns include the loss ofprivacy as a result of the release of confidential information as well asconcerns about the potential for using confidential information to takeadministrative or legal action.6however, microdata sets, which contain detailed records on individuals, may be made available for research use under tightly controlledconditions. the answers to many research questions depend on access tostatistical data at a level finer than that available in publicly released datasets. how can such data be made available without compromising theconfidentiality of the respondents who supplied the data? there areseveral approaches to address this challenge. in one approach, beforethey are released to researchers, data sets can be created in ways that deidentify records yet still permit analyses to be carried out. another approach is to bring researchers in as temporary statistical agency staff,allowing them to access the data under the same tight restrictions thatapply to other federal statistical agency employees. the section òlimitingdisclosureó in chapter 2 takes up this issue in more detail.organization of the federal statistical systemthe decentralized nature of the federal statistical system, with itsmore than 70 constituent agencies, has implications for both the efficiencyof statistical activities and the ease with which users can locate and use6the issue of balancing the needs for confidentiality of individual respondents with thebenefits of accessibility to statistical data has been explored at great length by researchersand the federal statistical agencies. for a comprehensive examination of these issues seenational research council and social science research council. 1993. private lives andpublic policies, george t. duncan, thomas b. jabine, and virginia a. dewolf, eds. nationalacademy press, washington, d.c.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.introduction and context11federal statistical data. most of the work of these agencies goes on without any specific management attention by the office of management andbudget (omb), which is the central coordinating office for the federalstatistical system. ombõs coordinating authority spans a number of areasand provides a number of vehicles for coordination. the highest level ofcoordination is provided by the interagency council on statistical policy.beyond that, a number of committees, task forces, and working groupsaddress common concerns and develop standards to help integrate programs across the system. the coordination activities of omb focus onensuring that priority activities are reflected in the budgets of the respective agencies; approving all requests to collect information from 10 ormore respondents (individuals, households, states, local governments,business);7 and setting standards to ensure that agencies use a commonset of definitions, especially in key areas such as industry and occupational classifications, the definition of u.s. metropolitan areas, and thecollection of data on race and ethnicity.in addition to these highlevel coordination activities, strong collaborative tiesñamong agencies within the government as well as with outside organizationsñunderlie the collection of many official statistics. several agencies, including the census bureau, the bureau of labor statistics,and the national agriculture statistical service, have large field forces tocollect data. sometimes, other agencies leverage their fieldbased resources by contracting to use these resources; state and local governmentsalso perform statistical services under contracts with the federal government. agencies also contract with private organizations such as researchtriangle institute (rti), westat, national opinion research center(norc), and abt associates, to collect data or carry out surveys. (whensurveys are contracted out, the federal agencies retain ultimate responsibility for the release of data from the surveys they conduct, and theircontractors operate under safeguards to protect the confidentiality of thedata collected.)provisions protecting confidentiality are also decentralized; federalstatistical agencies must meet the requirements specified in their ownparticular legislative provisions. while some argue that this decentralized approach leads to inefficiencies, past efforts to centralize the systemhave run up against concerns that establishing a single, centralized statistical office could magnify the threat to privacy and confidentiality. viewing the existence of multiple sets of rules governing confidentiality as a7this approval process, mandated by the paperwork reduction act of 1995 (44 u.s.c.3504), applies to governmentwide informationcollection activities, not just statisticalsurveys.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.12information technology research for federal statisticsbarrier to effective collaboration and data sharing for statistical purposes,the clinton administration has been seeking legislation that, while maintaining the existing distributed system, would establish uniform confidentiality protections and permit limited data sharing among certaindesignated òstatistical data centeró agencies.8 as a first step towardachieving this goal, omb issued the federal statistical confidentialityorder in 1997. the order is aimed at clarifying and harmonizing policyon protecting the confidentiality of persons supplying statistical information, assuring them that the information will be held in confidence andwill not be used against them in any government action.9in an effort to gain the benefits of coordinated activities while maintaining the existing decentralized structures, former omb directorfranklind.raines posed a challenge to the interagency council on statistical policy (icsp) in 1996, calling on it to implement what he termed aòvirtual statistical agency.ó in response to this call, the icsp identifiedthree broad areas in which to focus collaborative endeavors:¥programs. a variety of programs and products have interagencyimplicationsñan example is the gross domestic product, a figure that thebureau of economic analysis issues but that is based on data from agencies in different executive departments. areas for collaboration on statistical programs include establishing standards for the measurement ofincome and poverty and addressing the impacts of welfare and healthcare reforms on statistical programs.¥methodology. the statistical agencies have had a rich history ofcollaboration on methodology; the federal committee on statistical methodology has regularly issued consensus documents on methodologicalissues.10 the icsp identified the following as priorities for collaboration:measurement issues, questionnaire design, survey technology, and analytical issues.¥technology. the icsp emphasized the need for collaboration in thearea of technology. one objective stood out from the others because itwas of interest to all of the agencies: to make the statistical system more8executive office of the president, office of management and budget (omb). 1998.statistical programs of the united states government. omb, washington, d.c., p. 40.9office of management and budget, office of information and regulatory affairs. 1997.òorder providing for the confidentiality of statistical information,ó federal register 62(124,june 27):33043. available online at <http://www.access.gpo.gov/index.html>.10more information on the federal committee on statistical methodology and on accessto documents covering a range of methodological issues is available online from <http://fcsm.fedstats.gov/>.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.introduction and context13consistent and understandable for nonexpert users, so that citizens wouldnot have to understand how the statistical system is organized in order tofind the data they are looking for. the fedstats web site,11 sponsored bythe federal interagency council on statistical policy, is an initiative that isintended to respond to this challenge by providing a single point of accessfor federal statistics. it allows users to access data sets not only by agencyand program but also by subject.a greater emphasis on focusing federal statistics activities and fostering increased collaboration among the statistical agencies is evident in thedevelopment of the presidentõs fy98 budget. the budgeting process forthe executive branch agencies is generally carried out in a hierarchicalfashionñthe national center for education statistics, for example, submits its budget to the department of education, and the department ofeducation submits a version of that to the office of management andbudget. alternatively, it can be developed through a crosscut, whereomb looks at programs not only within the context of their respectivedepartments but also across the government to see how specific activitiesfit together regardless of their home locations. for the first time in twodecades, the omb director called for a statistical agency crosscut as anintegral part of the budget formulation process for fy98.12 in addition tothe omb crosscut, the omb director called for highlighting statisticalactivities in the administrationõs budget documents and, thus, in the presentation of the budgets to the congress.underlying the presentations and discussions at the workshop was adesire to tap it innovations in order to realize a vision for the federalstatistical agencies. a prominent theme in the discussions was how toaddress the decentralized nature of the u.s. national statistical systemthrough virtual mechanisms. the lookup facilities provided by thefedstats web site are a first step toward addressing this challenge. otherrelated challenges cited by workshop participants include finding waysfor users to conduct queries across data sets from multiple surveys, including queries across data developed by more than one agencyña hard problem given that each survey has its own set of objectives and definitionsassociated with the information it provides. the notion of a virtualstatistical agency also applies to the daytoday work of the agencies.although some legislative and policy barriers, discussed above in relation11available online from <http://www.fedstats.gov>.12note, however, that it was customary to have a statisticalagency crosscut in eachbudget year prior to 1980.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.14information technology research for federal statisticsto ombõs legislative proposal for data sharing, limit the extent to whichfederal agencies can share statistical data, there is interest in having morecollaboration between statistical agencies on their surveys.information technology innovation infederal statisticsfederal statistical agencies have long recognized the pivotal role of itin all phases of their activity. in fact, the census bureau was a significantdriver of innovation in information technology for many years:¥punchcardbased tabulation devices, invented by herman hollerithat the census bureau, were used to tabulate the results of the 1890 decennial census;¥the first univac (remingtonrand) computer, univac i, was delivered in 1951 to the census bureau to help tabulate the results of the 1950decennial census;13¥the film optical scanning device for input to computers(fosdic) enabled 1960 census questionnaires to be transferred to microfilm and scanned into computers for processing;¥the census bureau led in the development of computeraidedinterviewing tools; and¥it developed the topologically integrated geographic encodingand referencing (tiger) digital database of geographic features, whichcovers the entire united states.reflecting a long history of it use, the statistical agencies have asubstantial base of legacy computer systems for carrying out surveys.the workshop case study on the it infrastructure supporting the nationalcrime victimization survey illustrates the multiple cycles of modernization that have been undertaken by statistical agencies (box 1.4).today, while they are no longer a primary driver of it innovation, thestatistical agencies continue to leverage it in fulfilling their missions.challenges include finding more effective and efficient means of collecting information, enhancing the data analysis process, increasing the availability of data while protecting confidentiality, and creating more usable,more accessible statistical products. the workshop explored, and thisreport describes, some of the mission activities where partnerships be13see, e.g., j.a.n. lee. 1996. òlooking.back: march in computing history,ó ieee computer 29 (3). available online from <http://computer.org/50/looking/r30006.htm>.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.introduction and context15box 1.4modernization of the information technology usedfor the national crime victimization surveysteven phillips of the census bureau described some key elements in thedevelopment of the system used to conduct the national crime victimization survey (ncvs) for the bureau of justice statistics. he noted that the general trendover the years has been toward more direct communication with the sponsor agency, more direct communication with the subject matter analysts, quicker turnaround, and opportunities to modify the analysis system more rapidly. in the earlydays, the focus was on minimizing the use of central processing unit (cpu) cyclesand storage space, both of which were costly and thus in short supply. becausethe costs of both have continued to drop dramatically, the effort has shifted fromoptimizing the speed at which applications run to improving the end product.at the data collection end, paperandpencil interviewing was originally used.in 1986, minicati, a system that ran on digital equipment corporation minicomputers, was developed, and the benefits of online computerassisted interviewing began to be explored. in 1989, the ncvs switched to a package calledmicrocati, a quicker, more efficient, pcbased cati system, and in 1999 itmoved to a more capable cati system that provides more powerful authoringtools and better capabilities for exporting the survey data and tabulations online tothe sponsor. as of 1999, roughly 30 percent of the ncvs sample was using catiinterviewing.until 1985 a large univac mainframe was used to process the survey data. itemployed variablelength files; each household was structured into one record thatcould expand or contract. all the data in the tables were created by custom code,and the tables themselves were generated by a variety of custom packages. in1986, processing shifted to a fortran environment.in 1989, sas (a software product of the sas institute, inc.) began to be usedfor the ncvs survey. at that time a new and more flexible nested and hierarchicaldata file format was adopted. another big advantage of moving to this softwaresystem has been the ease with which tables can be created. originally, all of thestatistical tables were processed on a customwritten table generator. it produceda large numbers of tables, and the bureau of justice statistics literally cut andpastedñwith scissors and mucilageñto create the final tables for publications. amigration from mainframebased fortran software to a full sas/unix processingenvironment was undertaken in the 1990s; today, all processing is performed on aunix workstation, and a set of sas procedures is used to create the appropriatetables. all that remains to produce the final product is to process these tables,currently done using lotus 123, into a format with appropriate fonts and otherfeatures for publication.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.16information technology research for federal statisticstween the it research community and the statistics community might befostered.it innovation has been taking place throughout government, motivated by a belief that effective deployment of new technology could vastlyenhance citizensõ access to government information and significantlystreamline current government operations. the leveraging of information technology has been a particular focus of efforts to reinvent government. for example, vice president gore launched the national performance review, later renamed the national partnership for reinventinggovernment, with the intent of making government work better and costless. the rapid growth of the internet and the ease of use of the worldwide web have offered an opportunity for extending electronic access togovernment resources, an opportunity that has been identified and exploitedby the federal statistical agencies and others. individual agency effortshave been complemented by crossagency initiatives such as fedstats andaccess america for seniors.14 while government agency web pages havehelped considerably in making information available, much more remainsto be done to make it easy for citizens to locate and retrieve relevant,appropriate information.chapter 2 of this report looks at a number of research topics thatemerged from the discussions at the workshopñtopics that not only address the requirements of federal statistics but also are interesting researchopportunities in their own right. the discussions resulted in anotheroutcome as well: an increased recognition of the potential of interactionsbetween government and the it research community. chapter 3 discusses some issues related to the nature and conduct of such interactions.the development of a comprehensive set of specific requirements or of afull, prioritized research agenda is, of course, beyond the scope of a singleworkshop, and this report does not presume to develop either. nor doesit aim to identify immediate solutions or ways of funding and deployingthem. rather, it examines opportunities for engaging the informationtechnology research and federal statistics communities in research activities of mutual interest.14access america for seniors, a governmentoperated web portal that delivers electronicinformation and services for senior citizens, is available online at <http://www.seniors.gov>.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.172research opportunitiesresearch opportunities explored in the workshopõs panel presentations and smallgroup discussions are described in this chapter, whichillustrates the nature and range of it research issuesñincluding humancomputer interaction, database systems, data mining, metadata, information integration, and information securityñthat arise in the context of thework being conducted by the federal statistical agencies. the chapteralso touches on two other challenges pertinent to the work of the federalstatistical agenciesñsurvey instruments and the need to limit disclosureof confidential information. this discussion represents neither a comprehensive examination of information technology (it) challenges nor aprioritization of research opportunities, and it does not attempt to focuson the more immediate challenges associated with implementation.humancomputer interactionone of the real challenges associated with federal statistical data isthat the people who make use of it have a variety of goals. there are, firstof all, hundreds or thousands of specialists within the statistical systemwho manipulate the data to produce the reports and indices that government agencies and business and industry depend on. then there are thethousands, and potentially millions, of persons in the population at largewho access the data. some users access statistical resources daily, othersonly occasionally, and many others only indirectly, through third parties,but all depend in some fashion on these resources to support importantsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.18information technology research for federal statisticsdecisions. federal statistics resources support an increasingly diverserange of users (e.g., high school students, journalists, local communitygroups, business market analysts, and policy makers) and tasks. thepervasiveness of it, exemplified by the general familiarity with the webinterface, is continually broadening the user base.workshop participants observed, however, that many are likely toremain without ready access to information online, raising a set of socialand policy questions (box 2.1). however, over time, a growing fraction ofpotential users can be expected to gain network access, making it increasingly beneficial to place information resources online, together with capabilities that support their interpretation and enhance the statistical literacy of users. in the meantime, online access is being complemented bypublished sources and by the journalists, community groups, and otherintermediaries who summarize and interpret the data.the responsibility of a data product designer or provider does notend with the initial creation of that product. there are some importanthumancomputer interaction (hci) design challenges in supporting awide range of users. a key hci design principle is òknow thy useró;various approaches to learning about and understanding user abilitiesand needs are discussed below. besides underscoring the need to focuson users, workshop participants pointed to some specific issues: universal access, support for users with limited statistical literacy, improvedvisualization techniques, and new modes of interacting with data. theseare discussed in turn below.box 2.1some policy issues associated with electronic disseminationin her presentation at the workshop, patrice mcdermott, from omb watch,observed that if information suddenly began to be disseminated by electronicmeans alone, some people would no longer be able to access it. even basictelephone service, a precursor for lowcost internet access, is not universal in theunited states. it is not clear that schools and libraries can fill the gap: schools arenot open, for the most part, to people who do not have children attending them,and finding resources to invest in internet access remains a challenge for bothschools and public libraries. mcdermott added that research by omb watch indicates that people see a substantial difference between being directed to a bookthat contains census data and being helped to access and navigate through onlineinformation. another issue is the burden imposed by the shifting of costs: if information is available only in electronic form, users and intermediaries such as libraries end up bearing much of the cost of providing access to it, including, for example, the costs of telecommunications, internet service, and printing.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities19user focusiterative, usercentered design and testing are considered crucial todeveloping usable and useful information products. a better understanding of typical users and the most common tasks they perform, whichcould range from retrieving standard tables to building sophisticatedqueries, would facilitate the design of web sites to meet those usersõneeds. one important approach discussed at the workshop is to involvethe user from the start, through various routine participatory activities, inthe design of sites. the capture of peopleõs routine interactions withonline systems to learn what users are doing, what they are trying to do,what questions they are asking, and what problems they are having allowsimproving the product design. if, for example, a substantial number ofusers are seen to ask the same question, the system should be modified toensure that the answer to this question is easily availableñan approachanalogous to the òfrequently asked questionsó concept. customer ormarket surveys can also be used in conjunction with ongoing log and siteanalyses to better understand the requirements of key user groups. thereare many techniques that do not associate data with individuals and soare sensitive to privacy considerations.1 for example, collecting frequentqueries requires aggregation only at the level of the site, not of the individual. where individuallevel data are useful, they could be madeanonymous.universal accessthe desire to provide access to statistical information for a broadrange of citizens raises concerns about what measures must be taken toensure universal access.2 access to computers, once the province of asmall number of expert programmers, now extends to a wider set ofcomputerliterate users and an even larger segment of the populationsufficiently skilled to use the web to access information. the expandingaudience for federal statistical data represents both an opportunity and achallenge for information providers.1data on user behavior must be collected and analyzed in ways that are sensitive toprivacy concerns and that avoid, in particular, tracking the actions of individuals over time(though this inhibits withinsubject analyses). there are also the matters related to providing appropriate notice and obtaining consent for such monitoring.2this term, similar to the more traditional label òuniversal service,ó also encompasseseconomic and social issues related to the affordability of access services and technology, aswell as the provision of access through communitybased facilities, but these are not thefocus of this discussion.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.20information technology research for federal statisticsuniversality considerations apply as well to the interfaces people useto access information. the web browser provides a common interfaceacross a wide range of applications and extends access to a much largersegment of the population (anyone with a browser). however, the inertiaassociated with such large installed software bases tends to slow theimplementation of new interface technologies. during the workshop,gary marchionini argued that adoption of the web browser interface haslocked in a limited range of interactions and in some sense has set interface design back several years. a key challenge in ensuring universalaccess is finding upgrade trajectories for interfaces that maximize accessacross the broadest possible audience.3providing access to all citizens also requires attention to the diversephysical needs of users. making every web site accessible to everyonerequires more than delivering just a plaintext version of a document,because such a version lacks the richness of interaction offered by todayõsinterfaces. some work is already being done; vendors of operating systems, middleware, and applications provide software hooks that supportalternative modes of access. the world wide web consortium is establishing standards and defining such hooks to increase the accessibility ofweb sites.another dimension of universal access is supporting users whosesystems vary in terms of hardware performance, network connectionspeed, and software. the installed base of networked computers rangesfrom intel 80286 processors using 14.4kbps modems to highperformancecomputers with optical fiber links that are able to support realtime animation. that variability in the installed base presents a challenge indesigning new interfaces that are also compatible with older systems andsoftware.literacy, visualization, and perceptiongiven the relatively low level of numerical and statistical literacy inthe population at large, it becomes especially important to provide userswith interfaces that give them useful, meaningful information. providingdata with a bad interface that does not allow users to interpret data sensibly may be worse than not providing the data at all, because the badinterface frustrates nonexpert users and wastes their time. the goal is toprovide not merely a data set but also tools that allow making sense of thedata. today, most statistical data is provided in tabular formñthe form3see computer science and telecommunications board, national research council. 1997.more than screen deep: toward everycitizen interfaces to the nationõs information infrastructure. national academy press, washington, d.c.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities21of presentation with which the statistical community has the longestexperience. unfortunately, although it is well understood by both statisticians and expert users, this form of presentation has significant limitations. tables can be difficult for unsophisticated users to interpret, andthey do not provide an engaging interface through which to explorestatistical survey data. also, the types of analyses that can be conductedusing summary tables are much more limited than those that can be conducted when access to more detailed data is provided. workshop participants pointed to the challenge of developing more accessible forms ofpresentation as central to expanding the audience for federal statisticaldata.statistics represent complex information that might be thought of asmultimedia. even data tables, when sufficiently large, do not lend themselves to display as simple text. many of the known approaches to multimediañsuch as contentbased indexing and retrievalñmay be applicableto statistical problems as well. visualization techniques, such as usercontrolled graphical displays and animations, enable the user to explore,discover, and explain trends, outliers, gaps, and jumps, allowing a betterunderstanding of important economic or social phenomena and principles.welldesigned twodimensional displays are effective for many tasks, butresearchers are also exploring threedimensional and immersive displays.advanced techniques such as parallel coordinates and novel codingschemes, which complement work being done on threedimensional andimmersive environments, are also worthy of study.both representation (what needs to be shown to describe a given setof data) and control (how the user interacts with a system to determinewhat is displayed) pose challenges. statisticians have been working onthe problem of representation for a very long time. indeed a statistic itselfis a very concise condensation of a very large collection of information.more needs to be done in representing large data sets so that users whoare not sophisticated in statistical matters can obtain, in a fairly compactway, the sense of the information in large collections of data. related tothis is the need to provide users with appropriate indications of the effectsof sampling error.basic human perceptual and cognitive abilities affect the interpretation of statistical products. amos tversky and others have identifiedpervasive cognitive illusions, whereby people try to see patterns in random data.4 in the workshop presentation by diane schiano, evidence4see a. tversky and d.m. kahneman. 1974. òjudgement under uncertainty: heuristicsand biases,ó science 125:11241131. one such heuristic/bias is the perception of patterns inrandom scatter plots. see w.s. cleveland and r. mcgill. 1985. ògraphical perception andgraphical methods for analyzing scientific data,ó science 229 (august 30):828833.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.22information technology research for federal statisticswas offered of pervasive perceptual illusions that occur in even the simplest data displays. people make systematic errors in estimating the angleof a single line in a simple twodimensional graph and in estimating thelength of lines and histograms. these are basic perceptual responses thatare not subject to cognitive overrides to correct the errors. as displaysbecome more complex, the risk of perceptual errors grows accordingly.because of this, threedimensional graphics are often applied when theyshould not be, such as when the data are only twodimensional. moregenerally, because complex presentations and views can suggest incorrect conclusions, simple, consistent displays are generally better.the interpretation of complex data sets is aided by good exploratorytools that can provide both an overview of the data and facilities fornavigating through them and zooming in (or òdrilling downó) on details.to illustrate the navigation challenge, cathryn dippo of the bureau oflabor statistics noted that the current population surveyõs (cpsõs) typical monthly file alone contains roughly 1,000 variables, and the march filecontains an additional 3,000. taking into account various supplements tothe basic survey, the cps has 20,000 to 25,000 variables, a number thatrapidly becomes confusing for a user trying to interpret or even access thedata. that figure is for just one survey; the surveys conducted by thecensus bureau contain some 100,000 variables in all.underscoring the importance of providing users with greater supportfor interaction with data, schiano pointed to her research that found thatdirect manipulation through dynamic controls can help people correctsome perceptual illusions associated with data presentation. once usersare allowed to interact with an information object and to choose differentviews, perception is vastly improved. controls in common use today arelimited largely to scrolling and paging through fairly static screens ofinformation. however, richer modes of control are being explored, suchas interfaces that let the user drag items around, zoom in on details, andaggregate and reorder data. the intent is to allow users to manipulatedata displays directly in a much more interactive fashion.some of the most effective data presentation techniques emergingfrom humancomputer interaction research involve tightly coupled interactions. for example, when the user moves a slider (a control that allowssetting the value of a single variable visually), that action should have animmediate and direct effect on the displayñusers are not satisfied by anunresponsive system. building systems that satisfy these requirements inthe web environment, where network communications latency delaysdata delivery and makes it hard to tightly couple a user action and theresulting display, is an interesting challenge. what, for example, are theoptimal strategies for allocating data and processing between the clientsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities23and the server in a networked environment in order to support this kindof interactivity?two key elements of interactivity are the physical interface and theoverall style of interaction. the trend in physical interfaces has beentoward a greater diversity of devices. for example, a mouse or other twodimensional pointing device supplements keyboard input in desktopcomputing, while a range of threedimensional interaction devices areused in more specialized applications. indeed, various sensors are beingdeveloped that offer enhanced direct manipulation of data. one cananticipate that new ways of interacting will become commonplace in thefuture. how can these diverse and richer input and output devices beused to disseminate statistical information better? the benefits of building more flexible, interactive systems must be balanced against the riskthat the increased complexity can lead unsophisticated users to draw thewrong conclusions (e.g., when they do not understand how the information has been transformed by their interactions with it).also at work today is a trend away from static displays toward whatgary marchionini termed òhyperinteraction,ó which leads users to expectquick action and instant access to large quantities of information by pointing and clicking across the web or by pressing the button on a tv remotecontrol. an evergreater fraction of the population has such expectations,affecting how one thinks about disseminating statistical information.database systemsdatabase systems cover a range of applications, from the largescalerelational database systems widely used commercially, to systems thatprovide sophisticated statistical tools and spreadsheet applications thatprovide simple datamanipulation functionality along with some analysiscapability. much of the work today in the database community is motivated by a commercial interest in combining transactions, analysis, andmining of multiple databases in a distributed environment. for example,data warehouse environmentsñterabyte or multiterabyte systems thatintegrate data from various locationsñreplicate transactions databases tosupport problem solving and decision making. workshop participantsobserved that the problems of other user communities, such as the federalstatistics community, can be addressed in this fashion as well.problems cited by the federal statistics community include legacymigration, information integration across heterogeneous databases, andmining data from multiple sources. these challenges, perhaps more mundane than the splashier web development activities that many it usersare focused on, are nonetheless important. william cody noted in theworkshop that the database community has not focused much on thesesummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.24information technology research for federal statisticshard problems but is now increasingly addressing them in conjunctionwith its application partners. commercial systems are beginning toaddress these needs.todayõs database systems do not build in all of the functionality toperform many types of analysis. there are several approaches to enhancing functionality, each with its advantages and disadvantages. databasesystems can be expanded in an attempt to be all things to all people, orthey can be constructed so that they can be extended using their owninternal programming language. another approach is to give users theability to extract data sets for analysis using other tools and applicationlanguages. researchers are exploring what functions are best incorporated in databases, looking at such factors as the performance tradeoffsbetween the overhead of including a function inside a database and thedelay incurred if a function must be performed outside the database system or in a separate database system.building increased functionality into database systems offers thepotential for increasing overall processing efficiency, cody observed.there are delays inherent in transferring data from one database to another;if database systems have enhanced functionality, processing can be doneon a realtime or nearrealtime basis, allowing much faster access to theinformation. builtin functionality also permits databases to performintegrated tasks on data inside the database system. also, relational databases lend themselves to parallelization, whereas tools external to databases have not been built to take as much advantage of it. operations thatcan be included in the database engine are thus amenable to parallelization,allowing parallel processing computing capabilities to be exploited.cody described the likely evolution over the coming years of an interactive, analytic data engine, which has as its core a database systemenriched with new functions. users would be able to interact with thedata more directly through visualization tools, allowing interactive dataexploration. this concept is simple, but selecting and building the requiredset of basic statistical operations into database systems and creating theintegration tools needed to use a workstation to explore databases interactively are significant challenges that will take time. statisticsrelatedoperations that could be built into database systems include the following:¥datamining operations. by bringing datamining primitives intothe database, mining operations can occur automatically as data are collected in operational systems and transferred into warehousing systemsrather than waiting until later, after special data sets have been constructed for data mining.¥enhanced statistical analysis. today, generalpurpose relationaldatabase systems (as opposed to database systems specifically designedsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities25for statistical analysis) for the most part support only fairly simple statistical operations. a considerable amount of effort is being devoted tofiguring out which additional statistical operators should and could beincluded in evolving database systems. for example, could one performa regression or compute statistical measures such as covariances and correlations directly in the database?¥time series operators. the ability to conduct a timeseries analysiswithin a database system would, for example, allow one to derive a forecast based on the information coming in real time to a database.¥sampling. sampling design is a sophisticated practice. research isaddressing ways to introduce sampling into database systems so that theuser can make queries based on samples and obtain confidence limitsaround these results. while todayõs database systems use sampling during the query optimization process to estimate the result sizes of intermediate tables, sampling operators are not available to the enduserapplication. sql, which is the standard language used to interact withdatabase systems, provides a limited set of operations for aggregatingdata, although this has been augmented with the recent addition of newfunctionality for online analytical processing.additional support for statistical operations and sampling wouldallow, for example, estimating the average value of a variable in a data setcontaining millions of records by requesting that the database itself take asample and calculate its average. the direct result, without any additional software to process the data, would be the estimated mean togetherwith some confidence limit that would depend on the variance and thesample size.before the advent of objectrelational database systems, which addobjectoriented capabilities to relational databases, adding such extensions would generally have required extensive effort by the databasevendor. today, objectrelational systems make it easier for third parties,as well as sophisticated users, to add both new data types and new operations into a database system. since it is probably not reasonable to pushall of the functionality of a statistical analysis product such as sas into ageneralpurpose database system, a key challenge is to identify particularaggregation and sampling techniques and statistical operations thatwould provide the most leverage in terms of increasing both performanceand functionality.data miningdata mining enables the use of historical data to support evidencebased decision makingñoften without the benefit of explicitly statedsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.26information technology research for federal statisticsstatistical hypothesesñto create algorithms that can make associationsthat were not obvious to the database user. ideas for data mining havebeen explored in a wide variety of contexts. in one example, researchersat carnegie mellon university studied a medical database containing several hundred medical features of some 10,000 pregnant women over time.they applied datamining techniques to this collection of historical datato derive rules that better predict the risk of emergency caesarian sectionsfor future patients. one pattern identified in the data predicts that whenthree conditions are metñno previous vaginal delivery, an abnormalsecondtrimester ultrasound reading, and the infant malpresentingñthepatientõs risk of an emergency caesarian section rises from a base rate ofabout 7 percent to approximately 60 percent.5data mining finds use in a number of commercial applications. adatabase containing information on software purchasers (such as age,income, what kind of hardware they own, and what kinds of softwarethey have purchased so far) might be used to forecast who would belikely to purchase a particular software application in the future. banksor credit card companies analyze historical data to identify customers thatare likely to close their accounts and move to another service provider;predictive rules allow them to take preemptive action to retain accounts.in manufacturing, data collected over time from manufacturing processes(e.g., records containing various readings as items move down a production line) can be used by decision makers interested in process improvements in a production facility.both statisticians and computer scientists make use of some of thesame datamining tools and algorithms; researchers in the two fields havesimilar goals but somewhat different approaches to the problem. statisticians, much as they would before beginning any statistical analysis, seekthrough interactions with the data owner to gain an understanding ofhow and why the data were collected, in part to make use of this information in the data mining and in part to better understand the limitations onwhat can be determined by data mining. the computer scientist, on theother hand, is more apt to focus on discovering ways to efficiently manipulate large databases in order to rapidly derive interesting or indicativetrends and associations. establishing the statistical validity of thesemethods and discoveries may be viewed as something that can be done ata later stage. sometimes information on the conditions and circumstancesunder which the data were collected may be vague or even nonexistent,making it difficult to provide strong statistical justification for choosing5this example is described in more detail in tom m. mitchell. 1999. òmachine learningand data mining,ó communications of the acm 47(11).summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities27particular datamining tools or to establish the statistical validity of patterns identified from the mining; the statistician is arguably betterequipped to understand the limitations of employing data mining in suchcircumstances. statisticians seek to separate structure from noise in thedata and to justify the separation based on principles of statistical inference. similarly, statisticians approach issues like subsampling methodology as a statistical problem.research on data mining has been stimulated by the growth in boththe quantity of data that is being collected and in the computing poweravailable for analyzing it. at present, a useful set of firstgenerationalgorithms has been developed for doing exploratory data analysis, including logistic regression, clustering, decisiontree methods, and artificialneuralnet methods. these algorithms have already been used to create anumber of applications; at least 50 companies today market commercialversions of such analysis tools.one key research issue is the scalability of datamining algorithms.mining today frequently relies on approaches such as selecting subsets ofthe data (e.g., by random sampling) and summarizing them, or derivingsmaller data sets by methods other than selecting subsets (e.g., to performa regression relating two variables, one might divide the data into 1,000subgroups and perform the regression on each group, yielding a derivedsubset consisting of 1,000 sets of regression coefficients). for example, tomine a 4terabyte database, one might do the following: sample it downto 200 gigabytes, aggregate it to 80 gigabytes, and then filter the resultdown to 10 gigabytes.a relatively new area for data mining is multimedia data, includingmaps, images, and video. these are much more complex than the numericaldata that have traditionally been mined, but they are also potentially richnew sources of information. while existing algorithms can sometimes bescaled up to handle these new types of data, mining them frequentlyrequires completely new methods. methods to mine multimedia datatogether with more traditional data sources could allow one to learn something that had not been known before. to use the earlier example, whichinvolved determining risk factors in pregnancy, one would analyze notonly the traditional features such as age (a numerical field) and childbearing status (a boolean field) but also more complex multimedia featuressuch as videosonograms and unstructured text notes entered by physicians. another multimedia datamining opportunity suggested at theworkshop was to explore xray images (see box 2.2) and numerical andtext clinical data collected by the nhanes survey. active experimentation is an interesting research area related to datamining. most analysis methods today analyze precollected samples ofdata. with the internet and connectivity allowing researchers to easilysummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.28information technology research for federal statisticsbox 2.2national health and nutrition examination survey xrayimage archivelewis berman of the national center for health statistics presented some possible uses of the nhanes xray image archive. he described nhanes as theonly nationally representative sampling of x rays and indicated that some efforthad been made to make this set of data more widely available. for example, morethan 17,000 xray cervical and lumbar spine images from nhanes ii have beendigitized.1 in collaboration with the national library of medicine, these data arebeing made accessible online under controlled circumstances via web tools, alongwith collateral data such as reported back pain at the time of the x ray. other datasets that could also be useful to researchers include hand and knee films fromnhanes iii, a collection of hip x rays, and a 30year compilation of electrocardiograms. nhanes data could also provide a resource that would allow theinformation technology and medical communities to explore issues ranging frommultimedia data mining to the impact of image compression on the accuracy ofautomated diagnosis.1the images from nhanes ii were scanned at 175 microns on a lumisys scanner. thecervical and lumbar spine images have a resolution of 1,463 1,755  12 bits (5 mb perimage) and 2,048 2,487  12 bits (10 mb per image), respectively. although the images arestored as 2 bytes/pixel, they capture only 12 bits of gray scale.tap multiple databases, there is an opportunity to explore algorithms thatwould, after a firstpass analysis of an initial data set, search data sourceson the internet to collect additional data that might inform, test, or improve conjectures that are formed from the initial data set. in his presentation at the workshop, tom mitchell explored some of these implicationsof the internet for data collection and analysis. an obvious opportunity isto make interview forms available on the web and collect informationfrom useradministered surveys. a more technically challenging opportunity is to make use of web information that is already available. howmight one use that very large, heterogeneous collection of data to augment the more carefully collected but smaller data sets that come fromstatistical surveys? for example, many companies in the united stateshave web sites that provide information on current and new products,the companyõs location, and other information such as recruitingannouncements. mitchell cited work by his research group at carnegiemellon on extracting data from corporate web sites to collect such information as where they are headquartered, where they have facilities, andsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities29what their economic sector is. similarly, most universities have web sitesthat describe their academic departments, degree programs, research activities, and faculty. mitchell described a system that extracts informationfrom the home pages of university faculty. it attempts to locate andidentify faculty member web sites by browsing university web sites, andit extracts particular information on faculty members, such as their homedepartment, the courses they teach, and the students they advise.6metadatathe term òmetadataó is generally used to indicate the descriptionsand definitions that underlie data elements. metadata provides dataabout data. for example, what, precisely, is meant by òhouseholdó oròincomeó or òemployedó? in addition to metadata describing individualdata elements, there is a host of other information associated with a survey, also considered metadata, that may be required to understand andinterpret a data set. these include memos documenting the survey, thealgorithms7 used to derive results from survey responses (e.g., how it isdetermined whether someone is employed), information on how surveysare constructed, information on data quality, and documentation of howthe interviews are actually conducted (not just the questions asked butalso the content of training materials and definitions used by interviewersin gathering the data). workshop participants observed that bettermetadata and metadata tools and systems could have a significant impacton the usability of federal statistics, and they cited several key areas,discussed below.metadata, ranging from definitions of data fields to all other documentation associated with the design and conduct of a statistical survey,can be extensive. martin appel of the census bureau observed thatattempts to manually add metadata have not been able to keep up with6m. craven, d. dipasquo, d. freitag, a. mccallum, t. mitchell, k. nigam, and s. slattery.1998. òlearning to extract symbolic knowledge from the world wide web,ó proceedings ofthe 1998 national conference on artificial intelligence (july). available online at <http://www.cs.cmu.edu/~tom/publications.html>.7simply including computer code as metadata may not be the most satisfactory method;even highlevel language programs may not be useful as metadata. another approachwould be to use specification languages, which make careful statements about what computer code should do. these are more compact and more readable than typical computercode, although some familiarity with the specification language and comfort with its moreformal nature are required. as with computer code itself, a description in a specificationlanguage cannot readily be interpreted by a nonexpert user, but it can be interpreted by atool that can present salient details to nonexpert users. these languages are applicable notonly to representing a particular computer program but also to representing larger systems,such as an entire statistical collection and processing system.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.30information technology research for federal statisticsthe volume of data that are generated. in particular, statistical data madeavailable for analysis are frequently derived from calculations performedon other data, making the task of tying a particular data element to theappropriate metadata more complex. tools for automatically generatingand maintaining metadata as data sets are created, augmented, manipulated, and transformed (also known as selfdocumenting) could help meetthis demand.even if fully satisfactory standards and tools are developed for use infuture surveys, there remain legacy issues because the results of statisticalsurveys conducted in past decades are still of interest. for instance, thenhanes databases contain 30 years of data, during which time spansimilar but not identical questions were asked and evaluated, complicating the study of longterm health trends. much work remains to providea metadata system for these survey data that will permit their integration.another, related challenge is how to build tools that support thesearch and retrieval of metadata. a new user seeking to make sense of acensus data set may well need to know the difference between a òhouseholdó and a òfamilyó or a òblock groupó and a òblockó in order to makesense of that set. more generally, metadata are critical to help users makesense of datañfor instance, what a particular piece of data means, how itwas collected, and how much trust can be placed in it. the developmentof automatic display techniques that allow metadata associated with aparticular data set to be quickly and easily accessed was identified as onearea of need. for example, when a user examines a particular data cell,the associated metadata might be automatically displayed. at a minimum, drilldown facilities, such as the inclusion of a web link in an onlinestatistical report pointing to the relevant metadata, could be provided.such tools should describe not only the raw data but also what sort oftransformations were performed on them. finally, as the next sectiondiscusses, metadata can be particularly important when one wishes toconduct an analysis across data from multiple sources.information integrationgiven the number of different statistical surveys and agencies conducting surveys, òonestop shoppingó for federal statistical data wouldmake statistical data more accessible. doing so depends on capabilitiesthat allow analyzing data from multiple sources. the goal would be tofacilitate both locating the relevant information across multiple surveysand linking it to generate new results. several possible approaches werediscussed at the workshop.metadata standards, including both standardized formats for describing the data as well as sets of commonly agreedon meanings, are one keysummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities31to fully exploiting data sets from multiple sources. without them, forinstance, it is very difficult to ascertain which fields in one data set correspond to which fields in the other set and to what extent the fields arecomparable. while the framework provided by the recently developedxml standard, including the associated datatype definitions (dtds),offers some degree of promise, work is needed to ensure that effectivedtds for federal statistical data sets are defined. xml dtds, becausethey specify only certain structural characteristics of data, are only part ofthe solution; approaches for defining the semantics of statistical data setsalso need to be developed. standards do not, moreover, provide a solution for legacy data sets.another approach to information integration is to leverage the existing metadata, such as the text labels that describe the rows and columnsin a statistical table or descriptions of how the data have been collectedand processed, that accompany the data sets. finding ways of using thesemetadata to represent and relate the contents of tables and databases sothat analyses can be performed is an interesting area for further research.the database community is exploring how to use database systems tointegrate information originating from different systems throughout anorganization (data warehousing). database system developers are building tools that provide an interactive, analytical front end that integratesaccess to information in databases along with tools for visualizing thedata. research is being done on such things as data transformations anddata cleaning and on how to model different data sources in an integratedway.survey instrumentsthe way in which data are collected is critical: without highqualitydata up front, later work will have little value. improved tools for administering surveys, whether they use paper and pencil, are computerassisted, or are interviewee (enduser) administered, would also help.discussions at the workshop suggested that a new generation of tools fordeveloping surveys would offer statistical agencies greater flexibility indeveloping sound, comprehensive surveys. the current generation oftools is hard to use and requires that significant amounts of customizedcode be designed, written, and debugged. the complexity of the surveyssponsored by the federal government exceeds that of most other surveys,so it is unlikely that software to support this complex process will everbecome mainstream. workshop participants suggested that the federalgovernment should for this reason consider consolidating its efforts todevelop (or have others develop) such software. some particular needsare associated with survey tools:summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.32information technology research for federal statistics¥improved survey software tools. it would be useful to easily modifysurveys that have already been developed or deployed; such modification can be difficult when extensive custom coding is required to create asurvey instrument. highlevel language tools (socalled fourthgenerationlanguages), like those developed by the database industry, which demonstrate that families of sophisticated applications can be developed without requiring programmers to write extensive amounts of customizedcomputer code, may also ease the task of developing surveys.¥flexibility in navigation. better software tools would, for example,permit users to easily back up to earlier answers and to correct errors.heather contrino, discussing the american travel survey cati system,observed that if a respondent provides information about several tripsduring the trip section of the survey and then recalls another trip duringthe household section, it would be useful if the interviewer could immediately go back to a point in the survey where the new information shouldbe captured and then proceed with the survey. the new cati systemused for the 1995 american travel survey provides some flexibility, butmore would improve survey work. the issue, from an it research perspective, is developing system designs that ensure internal consistency ofthe survey data acquired from subjects while also promoting more flexible interactions, such as adapting to respondentsõ spontaneous reports.¥improved ease of use. being able to visualize the flow of the questionnaire would be especially helpful. in complex interviews, an interviewercan lose his or her place and become disoriented, especially when following rarely used paths. this difficulty could be ameliorated by showing,for example, the current location in the survey in relation to the overallflow of the interview. builtin training capabilities would also enhancethe utility of future tools. ideally, they should be able to coach the interviewer on how to administer the survey.¥monitoring the survey process. today, survey managers monitor thesurvey process manually. tools for automatically monitoring the surveycould be designed and implemented so that, as survey results are uploaded by the survey takers, status tables could be automatically produced and heuristic and statistical techniques used to detect abnormalconditions. automated data collection would improve the timeliness ofdata collection and enhance monitoring efforts. while the data analyst isgenerally interested only in the final output from a survey instrument, thesurvey designer also wants information on the paths taken through thesurvey, including, for example, any information that was entered andthen later modified. this is similar to the analyses of òclick traceó thattrack user paths through web sites.¥onthefly response checking. it would be useful to build in checks toidentify inappropriate data values or contradictory answers immediately,summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities33as an interview is being conducted, rather than having to wait for postinterview edits and possibly incurring the cost and delay of a followupinterview to correct the data. past attempts to build in such checks arereported to have made the interview instruments run excessively slowly,so the checks were removed.¥improved performance. another dimension to the challenges of conducting surveys is the hardware platform. laptops are the current platform of choice for taking a survey. however, the current generation ofmachines is not physically robust in the field, is too difficult to use, and istoo heavy for many applications (e.g., when an interviewer stands in adoorway, as happens when a household is being screened for possibleinclusion in a survey). predictable advances in computer hardware willaddress size and shape, weight, and battery life problems while advancesin processing speed will enable onthefly checking, as noted above. continued commercial innovation in portable computer devices, building onthe present generation of personal digital assistants, which providesophisticated programmability, appears likely to provide systems suitable for many of these applications. it is, of course, a separate matterwhether procurement processes and budgets can assimilate use of suchproducts quickly.¥new modes of interaction with survey instruments. another set ofissues relates to the limitations of keyboard entry. while a keyboard issuitable for a telephone interview or an interview conducted insidesomeoneõs house, it has some serious limitations in other circumstances,such as when an interviewer is conducting an initial screening interviewat someoneõs doorstep or in a driveway. advances in speechtotext technology might offer advantages for certain types of interviews, as mighthandwriting recognition capability, which is being made available in anumber of computing devices today. limitedvocabulary (e.g., òyesó,òno,ó and numerical digits), speakerindependent speech recognition systems have been used for some time in survey work.8 the technologyenvisioned here would provide speakerindependent capability with aless restricted vocabulary. with this technology it would be possible tocapture answers in a much less intrusive fashion, which could lead toimprovements in overall survey accuracy. speechtotext would also helpreduce human intermediation if it could allow interviewees to interactdirectly with the survey instrument. there are significant research questions regarding the implications of different techniques for administering8the bureau of labor statistics started using this technology for the current employment survey in 1992. see richard l. clayton and debbie l.s. winter. 1992. òspeech dataentry: results of a test of voice recognition for survey data collection,ó journal of officialstatistics 8:377388.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.34information technology research for federal statisticssurvey questionnaires, with some results in the literature suggesting thatchoice of administration technique can affect survey results significantly.9more research on this question, as well as on the impact of human intermediation on data collection, would be valuable.limiting disclosuremaintaining the confidentiality of respondents in data collected under pledges of confidentiality is an intrinsic part of the mission of thefederal statistical agencies. it is this promise of protection against disclosure of confidential informationñprotecting individual privacy or business trade secretsñthat convinces many people and businesses to complywillingly and openly with requests for information about themselves,their activities, and their organizations. hence, there are strong rules inplace governing how agencies may (and may not) share data,10 and datathat divulge information about individual respondents are not released tothe public. disclosure limitation is a research area that spans both statistics and it; researchers in both fields have worked on the issue in the past,and approaches and techniques from both fields have yielded insights.while nontechnical approaches play a role, it tools are frequently employed to help ease the tension between societyõs demands for data andthe agenciesõ ability to collect information and maintain its confidentiality.researchers rely on analysis of data sets from federal statistical surveys, which are viewed as providing the highestquality data on a numberof topics, to explore many economic and social phenomena. while someof their analysis can be conducted using public data sets, some of itdepends on information that could be used to infer information aboutindividual respondents, including microdata, which are the data sets containing records on individual respondents. statistical agencies must strikea balance between the benefits obtained by releasing information forlegitimate research and the potential for unintended disclosures that couldresult from releasing information. the problem is more complicated thansimply whether or not to release microdata. whenever an agency releasesstatistical information, it is inherently disclosing some information about9see, e.g., sara kiesler and lee sproull. 1986. òresponse effects in the electronic survey,ópublic opinion quarterly 50:243253 and wendy l. richman, sara kiesler, suzanneweisband, and fritz drasgow. 1999. òa metaanalytic study of social desirability distortion in computeradministered questionnaires, traditional questionnaires, and interviews,ó journal of applied psychology 84(5, october):754775.10these rules were clarified and stated consistently in office of management and budget, office of information and regulatory affairs. 1997. òorder providing for the confidentiality of statistical information,ó federal register 62(124, june 27):33043. available onlineat <http://www.access.gpo.gov/index.html>.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities35the source of the data from which the statistics are computed and potentially making it easier to infer information about individual respondents.contrary to what is sometimes assumed, protecting data confidentiality is not as simple as merely suppressing names and other obvious identifiers. in some cases, one can reidentify such data using record linkagetechniques. record linkage, simply put, is the process of using identifying information in a given record to identify other records containinginformation on the same individual or entity.11 for example, a set ofattributes such as geographical region, sex, age, race, and so forth may besufficient to identify individuals uniquely. moreover, because multiplesources of data may be drawn on to infer identity, understanding howmuch can be inferred from a particular set of data is difficult. a simpleexample provided by latanya sweeney in her presentation at the workshop illustrates how linking can be used to infer identity (box 2.3).both technical and nontechnical approaches have a role in improvingresearcher access to statistical data. agencies are exploring a variety ofnontechnical solutions to complement their technical solutions. forexample, the national center for education statistics allows researchersaccess to restricteduse data under strict licensing terms, and the nationalcenter for health statistics (nchs) recently opened a research data center that makes data files from many of its surveys available, both onsiteand via remote access, under controlled conditions. the census bureauhas established satellite centers for secured access to research data inpartnership with the national bureau of economic research, carnegiemellon university, and the university of california (at berkeley and atlos angeles), and it intends to open additional centers.12 access to datarequires specific contractual arrangements aimed at safeguarding confidentiality, and deidentified publicuse microdata user files can be accessed through third parties. for example, data from the national crimevictimization survey are made available through the interuniversity consortium for political and social research (icpsr) at the university ofmichigan. members of the research community are, of course, interestedin finding less restrictive ways of giving researchers access to confidentialdata that do not compromise the confidentiality of that data.11for an overview and series of technical papers on record linkage, see committee onapplied and theoretical statistics, national research council and federal committee onstatistical methodology, office of management and budget. 1999. record linkage techniquesñ1997: proceedings of an international workshop and exposition. national academypress, washington, d.c.12see u.s. census bureau, office of the chief economist, 1999. research data centers.u.s. census bureau, washington, d.c., last revised september 28. available online at<http://www.census.gov/cecon/www/rdc.html>.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.36information technology research for federal statisticsbox 2.3using external data to reidentify personal dataremoving names and other unique identification information is not sufficient toprevent reidentifying the individuals associated with a particular data record.latanya sweeney illustrated this point in her presentation at the workshop usingan example of how external data sources can be used to determine the identity ofthe individuals associated with medical records. hospitals and insurers collectinformation on individual patients. because such data are generally believed to beanonymous once names and other unique identifiers have been removed, copiesof these data sets are provided to researchers and sold commercially. sweeneydescribed how she reidentified these seemingly anonymous records using information contained in voter registration records, which are readily purchased formany communities.voter registration lists, which provide information on name, address, and soforth, are likely to have three fields in common with deidentified medical recordsñzip code, birth date, and sex. how unique a link can be established using thisinformation? in one community where sweeney attempted to reidentify personaldata, there are 54,805 voters. the range of possible birth dates (year, month, day)is relatively smallñabout 36,500 dates over 100 yearsñand so potentially can beuseful in identifying individuals. in the community she studies, there is a concentration of people in their 20s and 30s, and birth date alone uniquely identifies about12 percent of the communityõs population. that is, given a personõs birth date andknowledge that the person lived in that community, one could uniquely identify himor her. birth date and gender were unique for 29 percent of the voters, birth dateand zip code, for 69 percent, and birth date and full postal code, for 97 percent.academic work on it approaches to disclosure limitation has so farbeen confined largely to techniques for limiting disclosure resulting fromrelease of a given data set. however, as the example provided by sweeneyillustrates, disclosure limitation must also address the extent to whichreleased information can be combined with other, previously releasedstatistical information, including administrative data and commercial andother publicly available data sets, to make inferences. researchers haverecognized the importance of understanding the impact on confidentiality of these external data sources, but progress has been limited becausethe problem is so complex. the issue is becoming more important for atleast two reasons. first, the quantity of personal information being collected automatically is increasing rapidly (box 2.4) as the web grows anddatabase systems become more sophisticated. second, the statistical agencies, to meet the research needs of their users, are being asked to releaseòanonymizedó microdata to support additional data analyses. as a result,a balancing act must be performed between the benefits obtained fromsummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities37box 2.4growth in the collection of personal dataat the workshop, latanya sweeney described a metric she had developed toprovide a sense of how the amount of personal data is growing. her measureñdisk storage per person, calculated as the amount of storage in the form of harddisks sold per year divided by the adult world populationñis based on the assumption that access to inexpensive computers with very large storage capacities isenabling the collection of an increasing amount of personal data. based on thismetric, the several thousand characters of information that could be printed on an8 1/2 by 11 inch piece of paper would have documented some 2 months of apersonõs life in 1983. the estimate seems reasonable: at that time such information probably would have been limited to that contained in school or employmentrecords, the telephone calls contained on telephone bills, utility bills, and the like.by 1996, that same piece of paper would document 1 hour of a personõs life. thegrowth can be seen in the increased amount of information contained on a massachusetts birth certificate; it once had 15 fields of information but today has morethan 100. similar growth is occurring in educational data records, grocery storepurchase logs, and many other databases, observed sweeney. projections for themetric in 2000, with 20gigabyte drives widely available, are that the informationcontained on a single page would document less than 4 minutes of a personõslifeñinformation that includes image data, web and internet usage data, biometricdata (gathered for health care, authentication, and even webbased clothing purchases), and so on.data release and the potential for unwanted disclosure that comes fromlinking with other databases. what is the disclosure effect, at the margin,of the release of a particular set of data from a statistical agency?the issue of disclosure control has also been addressed in the contextof work on multilevel security in database systems, in which the securityauthorization level of a user affects the results of database queries.13 asimple disclosure control mechanism such as classifying individualrecords is not sufficient because of the possible existence of an inferencechannel whereby information classified at a level higher than that forwhich a user is cleared can be inferred by that user based on informationat lower levels (including external information) that is possessed by that13see national research council and social science research council. 1993. private livesand public policies: confidentiality and accessibility of government statistics. national academy press, washington, d.c., pp. 150151; and d.e. denning et al. 1988. òa multilevelrelational data model,ó proceedings of the 1987 ieee symposium on research security andprivacy. ieee computer society, los alamitos, calif.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.38information technology research for federal statisticsuser. such channels are, in general, hard to detect because they mayinvolve a complex chain of inferences and because of the ability of usersto exploit external data.14various statistical disclosurelimiting techniques have been and arebeing developed to protect different types of data. the degree to whichthese techniques need to be unique to specific data types has not beenresolved. the bulk of the research by statistics researchers on statisticaldisclosure limitation has focused on tabular data, and a number ofdisclosurelimiting techniques have been developed to protect the confidentiality of individual respondents (including people and businesses),including the following:¥cell suppressionñthe blanking of table entries that would provideinformation that could be narrowed down to too small a set of individuals;¥swappingñexchanging pieces of information among similar individuals in a data set; and¥top codingñaggregating all individuals above a certain thresholdinto a single top category. this allows, for example, hiding informationabout an individual whose income was significantly greater than theincomes of the other individuals in a given set that would otherwiseappear in a lone row of a table.however, researchers who want access to the data are not yet satisfied with currently available tabular datadisclosure solutions. in particular, some of these approaches rely on distorting the data in ways thatcan make it less acceptable for certain uses. for example, swapping canalter records in a way that throws off certain kinds of research (e.g., it canlimit researchersõ ability to explore correlations between various attributes).while disclosure issues for tabular data sets have received the mostattention from researchers, many other types of data are also released,both publicly and to more limited groups such as researchers, giving riseto a host of questions about how to limit disclosure. some attention hasbeen given to microdata sets and the creation of publicuse microdata14see t.f. lunt, t.d. garvey, x. qian, and m.e. stickel. 1994. òtype overlap relationsand the inference problem,ó proceedings of the 8th ifip wg 11.3 working conference on database security, august; t.f. lunt, t.d. garvey, x. qian, and m.e. stickel. 1994. òissues indatalevel monitoring of conjunctive inference channels,ó proceedings of the 8th ifip wg11.3 working conference on database security, august; and t.f. lunt, t.d. garvey, x. qian,and m.e. stickel. 1994. òdetection and elimination of inference channels in multilevelrelational database systems,ó proceedings of the ieee symposium on research in security andprivacy, may 1993. for an analysis of the conceptual models underlying multilevel security,see computer science and telecommunications board, national research council. 1999.trust in cyberspace. national academy press, washington, d.c.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities39files. the proliferation of offtheshelf software for data linking and datacombining appears to have raised concerns about releasing microdata.none of the possible solutions to this problem coming from the researchcommunity (e.g., random sampling, masking, or synthetic data generation)seems mature enough today to be adopted as a data release technique.digital geospatial data, including image data, are becoming morewidely available and are of increasing interest to the research community.opportunities for and interest in linking data sets by spatial coordinatescan be expected to grow correspondingly. in many surveys, especiallynatural resources or environmental surveys, the subject matter is inherently spatial. and spatial data are instrumental in research in many areas,including public health and economic development. the confidentialityof released data based on sample surveys is generally protected by minimizing the chance that a respondent can be uniquely identified usingdemographic variables and other characteristics. the situations wheresampling or observational units (e.g., person, household, business, or landplot) are linked with a spatial coordinate (e.g., latitude and longitude) oranother spatial attribute (e.g., census block or hydrologic unit) have beenless well explored. precise spatial coordinates for sampling or observational units in surveys are today generally considered identifying information and are thus excluded from the information that can be releasedwith a public data set. identification can also be achieved through acombination of less precise spatial attributes (e.g., county, census block,hydrologic unit, land use), and care must be taken to ensure that including variables of this sort in a public data set will not allow individualrespondents to be uniquely identified.techniques to limit information disclosure associated with spatialdata have received relatively little attention, and research is needed onapproaches that strike an appropriate balance between two opposingforces: (1) the need to protect the confidentiality of sample and observational units when spatial coordinates or related attributes are integral tothe survey and (2) the benefits of using spatial information to link with abroader suite of information resources. such approaches might drawfrom techniques currently used to protect the confidentiality of alphanumeric human population survey data. for example, random noisemight be added to make the spatial location fuzzier, or classes of spatialattributes might be combined to create a data set with lower resolution. itis possible that the costs and benefits of methods for protecting the confidentiality of spatial data will vary from those where only alphanumericdata are involved. in addition, alternative paradigms making use of newinformation technologies may be more appropriate for problems specificto spatial data. one might, for instance, employ a behindthescenesmechanism for accurately combining spatial information where the linksummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.40information technology research for federal statisticsage, such as the merging of spatial data sets, occurs in a confidentialòspaceó to produce a product such as a map or a data set with summariesthat do not disclose locations. in some cases, this might include a mechanism that implements disclosure safeguards.a third, more general, issue is how to address disclosure limitationwhen multimedia data such as medical images are considered. approachesdeveloped for numerical tabular or microdata do not readily apply toimages, instrument readings, text, or combinations of them. for example,how does one ensure that information gleaned from medical images cannot be used to reidentify records? given the considerable interest of bothcomputer scientists and statisticians in applying datamining techniquesto extract patterns from multimedia data, collaboration with computerscientists on disclosurelimiting techniques for these data is likely to befruitful.few efforts have been made to evaluate the success of data releasestrategies in practice. suppose for example, that a certain database isproposed for release. could one develop an analytical technique to helpdata managers evaluate the potential for unwanted disclosure caused bythe proposed release? the analysis would evaluate the database itself,along with metainformation about other known, released databases, soas to identify characteristics of additional external information that couldcause an unwanted disclosure. it could be used to evaluate not only theparticular database proposed for release but also the impact of that releaseon potential future releases of other databases. several possible approacheswere identified by workshop participants. first, one can further developsystematic approaches for testing the degree to which a particular releasewould identify individuals. given that it is quite difficult to know the fullscope of information available to a wouldbe òattacker,ó it might also beuseful to develop models of the information available to and the behaviorof someone trying to overcome attempts to limit disclosure and to usethese models to test the effectiveness of a particular disclosure limitationapproach.another approach, albeit a less systematic one, is to explore red teaming to learn how a given data set could be exploited (including by combining it with other, previously disclosed or publicly available data sets).red teaming in this context is like red teaming to test information systemsecurity (a team of talented individuals is invited to probe for weaknessesin a system15), and the technique could benefit from collaboration with itresearchers and practitioners.15a recent cstb report examining defense commandandcontrol systems underscoredthe importance of frequent red teaming to assess the security of critical systems. see computer science and telecommunications board, national research council. 1999. realizingthe potential of c4i: fundamental challenges. national academy press, washington, d.c.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities41trustworthiness of information systemsthe challenge of building trustworthy (secure, dependable, and reliable) systems has grown along with the increasing complexity of information systems and their connectedness, ubiquity, and pervasiveness. thisis a burgeoning challenge to the federal statistical community as agenciesmove to greater use of networked systems for data collection, processing,and dissemination. thus, even as solutions are developed, the goal beingpursued often appears to recede.16there have been substantial advances in some areas of security andparticular problems have been solved. for example, if one wishes toprotect information while it is in transit on a network, the technology todo this is generally considered to be available.17 hence experts tend toagree that a credit card transaction over the internet can be conductedwith confidence that credit card numbers cannot be exposed or tamperedwith while they are in transit. on the other hand, there remain manydifficult areas: for example, unlike securing information in transit, theproblem of securing the information on the end systems has, in recentyears, not received the attention that it demands. protecting against disclosure of confidential information and ensuring the integrity of the collection, analysis, and dissemination process are critical issues for federalstatistical agencies.for the research community that depends on federal statistics, a keysecurity issue is how to facilitate access to microdata sets without compromising their confidentiality. as noted above, the principal approachbeing used today is for researchers to relocate themselves temporarily toagency offices or one of a small number of physically secured data centers,such as those set up by the census bureau and the nchs. unfortunately,the associated inconveniences, such as the need for frequent travel, arecited by researchers as a significant impediment to working withmicrodata. another possible approach being explored is the use of various security techniques to permit offsite access to data. nchs is oneagency that has established remote data access services for researchers.this raises several issues. for example, what is the tradeoff between16the recent flap over the proposed federal intrusion detection network (fidnet) indicates that implementing security measures is more complicated in a federal governmentcontext.17for a variety of reasons, including legal and political issues associated with restrictionsthat have been placed on the export of strong cryptography from the united states, thesetechnologies are not as widely deployed as some argue they should be. see, e.g., computerscience and telecommunications board, national research council. 1996. cryptographyõsrole in securing the information society. national academy press, washington, d.c. theserestrictions have recently been relaxed.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.42information technology research for federal statisticspermitting offsite users to replicate databases to their own computers ina secure fashion for local analysis and permitting users to have securedremote access to external analysis software running on computers locatedat a secured center. both approaches require attention to authenticationof users and both require safeguards, technological or procedural, to prevent disclosure as a result of the microdata analysis.18another significant challenge in the federal statistics area is maintaining the integrity of the process by which statistical data are collected,processed, and disseminated. federal statistics carry a great deal ofauthority because of the reputation that the agencies have developedñareputation that demands careful attention to information security. discussing the challenges of maintaining the backend systems that supportthe electronic dissemination of statistics products, michael levi of thebureau of labor statistics cited several demands placed on statistics agencies: systems that possess automated failure detection and recovery capabilities; better configuration management including installation, testing,and reporting tools; and improved tools for intrusion prevention, detection, and analysis.as described above, the federal statistical community is moving awayfrom manual, paperandpencil modes of data collection to more automated modes. this trend started with the use of computerassisted techniques (e.g., capi and cati) to support interviewers and over time canbe expected to move toward more automated modes of data gathering,including embedded sensors for automated collection of data (e.g., imagine if one day the american travel survey were to use global positioningsystem satellite receivers and data recorders instead of surveys). increasing automation increases the need to maintain the traceability of data toits source as the data are transferred from place to place (e.g., uploadedfrom a remote site to a central processing center) and are processed intodifferent forms during analysis (e.g., to ensure that the processed data ina table in fact reflect the original source data). in other words, there is agreater challenge in maintaining process integrityña chain of evidencefrom source to dissemination.there are related challenges associated with avoiding premature datarelease. in some instances, data have been inadvertently released beforethe intended point in time. for example, the bureau of labor statisticsprematurely released part of its october 1998 employment report.18a similar set of technical requirements arise in supporting the geographically dispersedworkers who conduct field interviews and report the data that have been collected. see, forexample, computer science and telecommunications board, national research council.1992. review of the tax systems modernization of the internal revenue service. national academy press, washington, d.c.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.research opportunities43according to press reports citing a statement made by bls commissionerkatharine g. abraham, this happened when information was moved toan internal computer by a bls employee who did not know it wouldthereupon be transferred immediately to the agencyõs world wide website and thus be made available to the public.19 the processes for managing data apparently depended on manual procedures. what kind of automated processsupport tools could be developed to make it much moredifficult to release information prematurely?in the security research literature, problems and solutions areabstracted into a set of technologies or building blocks. the test of thesebuilding blocks is how well researchers and technologists can apply themto understand and address the real needs of customers. while there are anumber of unsolved research questions in information security, solutionscan in many cases be obtained through the application of known securitytechniques. of course the right solution depends on the context; securitydesign is conducted on the basis of knowledge of vulnerabilities andthreats and the level of risk that can be tolerated, and this information isspecific to each individual application or system. solving real problemsalso helps advance more fundamental understanding of security; the constraints of a particular problem environment can force rethinking of thestructure of the world of building blocks.19john m. berry. 1998. òbls glitch blamed on staff error; premature release of job dataon web site boosted stocks,ó washington post, november 7, p. h03.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.443interactions for information technologyinnovation in federal statistical workthe workshop discussed the information technology (it) requirements of the federal statistical agencies and the research questions motivated by those needs. in addition to articulating a sizable list of researchtopics, workshop participants made a number of observations about thenature of the relationship and interactions between the two communities.these observations are offered to illustrate the sorts of issues that arise inconsidering how to foster collaboration and interaction between the federal statistical agencies and the it research community aimed at innovation in the work of the agencies.1one obstacle discussed in the course of the workshop is that despiteinterest in innovation, there are insufficient connections between thosewho operate and develop government information systems or who runagency programs and those who conduct it research. in particular, federal agencies, like most procurers of it systems, tend to rely on what isavailable from commercial technology vendors or system integrators (or,in some cases, what can be developed or built inhouse). a programaimed at bridging this gap, the national science foundationõs (nsfõs)digital government program, was launched in june 1998 to support research aimed at stimulating it innovation in government. the premise of1these observations should not be viewed as necessarily being conclusions of the studycommittee that organized the workshop. the committeeõs conclusions will be presented inthe studyõs final report, to be published later in 2000.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.interactions for information technology innovation45this program is that by promoting interactions between innovators ingovernment and those performing computing and communications research, it may be possible both to accelerate innovation in pertinent technical areas and to hasten the adoption of those innovations into agencyinfrastructure.building connections that address the needs and interests of bothcommunities entails the establishment of appropriate mechanisms forcollaboration between the it research community and government itmanagers. in principle, the right mechanisms can help federal programand it acquisition managers interact with the it research communitywithout exposing operational users to unacceptable levels of risk. also,incorporating new research ideas and technology into the operations ofgovernment agencies frequently requires spanning a gulf between theculture and practices of commercial systems integration and the researchcommunity.also relevant to the issue of innovation and risk in the context ofgovernment in general, and the federal statistical system in particular, isthe value attached to the integrity of the federal statistics community andthe trustworthiness of the results (relevant principles are summarized inbox 1.1). these are attributes that the agencies value highly and wish topreserve and that have led to a strong tradition of careful management.such considerations could constrain efforts that experiment with newtechnologies in these activities.experience suggests that despite these potential constraints andinhibitors, both research and application communities stand to benefitfrom interaction. introduction of new it can enable organizations tooptimize the delivery of existing capabilities. the full benefits of it innovation extend further, as such innovation can enable organizations to dothings in new ways or attain entirely new capabilities. advances in itresearch represent opportunities not only for increased efficiency but alsofor changes in the way government works, including the delivery of newkinds of services and new ways of interacting with citizens. collaboration with government agencies also represents a significant opportunityfor it researchers to test new ideasñgovernment applications are realand have texture, richness, and veracity that are not available in laboratory studies. frequently, these applications are also of a much larger scalethan that found in most research work.while the workshop focused primarily on longterm issues, anotherbenefit was the shedding of light on some shortterm problems. indeed, itis natural for people in an operational setting to focus on problems thatneed to be solved in the next year rather than on longterm possibilities.this suggests that focus on and investment in longterm challenges maybe difficult. but in some respects, the nearterm focus may be approprisummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.46information technology research for federal statisticsate, since some of the information technologies and it practices of thefederal statistical agencies lag behind best industry practices. in an example illustrating the shortterm, mundane challenges that consume considerable time and resources, one workshop presenter described the challenges posed by the need to install new software on several hundredlaptop computers. in later discussions, it was pointed out that this was aproblem that had already been solved in the marketplace; there are wellknown techniques for disk imaging that allow initialization of thousandsof computers. underscoring the potential value of such interactions, informal feedback following the workshop suggested that the exposure tosome cuttingedge computer science thinking stimulated subsequentdiscussion among some statistical agencies about their need for furthermodernization.one factor that may be exacerbating many of the short and longtermitrelated challenges is the decentralized nature of the federal statisticalagencies, which makes it harder to establish a critical mass of expertise,investment, and experimental infrastructure. another difficulty arisesfrom the specialized requirements of federal statistical agencies. themarket is limited for software for authoring and administering surveyinterviews of the complexity found in federal statistical surveys, whichare quite expensive and are conducted only by government and a fewother players. workshop participants discussed how the federal government might consolidate its research and development efforts for this classof software. several it applications in this category were cited, includingsurvey software, easytouse interfaces for displaying complex data sets,and techniques for limiting the disclosure of confidential information indatabases.collaborative research, even within a discipline, is not always easy,and interdisciplinary work is harder still. researchers at the workshopargued that in order for such collaboration to take place, both it andstatistics researchers would need to explore ways of tapping existingresearch programs or establishing new funding mechanisms.2 computerscientists do not typically think of going to one of the statistical agencies,and statisticians do not typically think about teaming with a computerscientist for their fellowship research. both computer scientists and statisticians will find it easier to obtain funding for work in more traditional2workshop participants pointed to two nsf programs that could facilitate such collaborations if they were explicitly targeted toward such interactions. one is a fellows programin the methodology, measurement, and statistics program that sends statisticians to various federal statistical agencies. the second is a computer and information science andengineering (cise) directorate program that provides support for computer scientists totake temporary positions in federal agencies.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.interactions for information technology innovation47research directions. so, given all the additional difficulties associatedwith interdisciplinary work, particularly in academia, it is unlikely tooccur without funding directed at collaborative work.3 this, of course,was part of the impetus for the nsf digital government program.more generally, a number of workshop participants acknowledgedthat involvement in application areas related to federal statistics offerssignificant opportunities for it researchers. each of the areas described inchapter 2 was identified by participants as one where considerable benefits would be obtained from direct collaboration between it and statisticsresearchers. a leading example is the area of information security. whilesome segments of the computer science community may be ambivalentabout doing applicationfocused research, it is difficult to make real progressin information security without a specific application focus. a similarlylarge challenge is building easytouse systems that enable nonexpertusers, who have diverse needs and capabilities, to access, view, and analyze data. both the magnitude of the challenge itself and the opportunityto conduct research on systems used by a large pool of diverse users makethese systems an attractive focus for research. another particularly interesting issue discussed by workshop participants was the development oftechniques to protect the confidentiality of spatial data.3participants in a workshop convened by cstb that explored ways to foster interdisciplinary research on the economic and social impacts of information technology madesimilar observations. see computer science and telecommunications board, nationalresearch council. 1998. fostering research on the economic and social impacts of informationtechnology: report of a workshop. national academy press, washington, d.c.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.49appendixworkshop agenda and participantsagendatuesday, february 9, 19997:30 a.m.registration and continental breakfast8:30welcomewilliam scherlis8:45keynote addressthomas kalil, national economic council9:15panel 1: case studies¥national health and nutrition examination surveys,lewis berman¥american travel study, heather contrino¥current population survey, cathryn dippo¥national crime victimization survey, denise lewissallie kellermcnulty, moderator11:00panel 2: information technology trends and opportunitiesgary marchionini, tom mitchell, ravi s. sandhu, william cody,clifford neuman (moderator)12:30 p.m.lunch1:30panel 3: study design, data collection, and data processingmartin appel, judith lessler, james smith, william eddy(moderator)summary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.50appendix3:00break3:305:00panel 4: creating statistical information productsmichael levi, bruce petrie, diane schiano, susan dumais(moderator)6:007:30reception5:308:00exhibitstiger mapping system, mable/geocorr; u.s. gazetteer;census ferret; cdc wonder; national center for healthstatistics mortality mapping exhibit, display, and demo;westat blaise; consumer price index capi; census capi;fedstatswednesday, february 10, 19997:30 a.m.continental breakfast8:30keynote addresskatherine wallman, office of management and budget9:00panel 5: the consumerõs perspectivevirginia dewolf, latanya sweeney, paul overberg, michael nelson(moderator)10:30break10:45breakout sessions1.data management, survey technique, process, systemsarchitecture, metadata, interoperation2. data mining, inference, privacy, aggregation and sharing,metadata, security3. humancomputer interaction, privacy, dissemination,literacy11:45report back from breakout sessions12:15 p.m.adjournparticipantsrichard allen, u.s. department of agriculture, nationalagricultural statistics servicemartin appel, census bureaudon bay, u.s. department of agriculture, national agriculturalstatistics servicelinda bean, national center for health statisticslewis berman, national center for health statisticstora bickson, rand corporationlarry brandt, national science foundationcavan capps, census bureausummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.appendix51lynda carlson, energy information agencydan carr, george mason universitywilliam cody, ibm almadeneileen collins, national science foundationfrederick conrad, bureau of labor statisticsheather contrino, bureau of transportation statisticsrobert creecy, census bureauw. bruce croft, university of massachusetts at amherstmarshall deberry, bureau of justice statisticsdavid dewitt, university of wisconsin at madisonvirginia dewolf, office of management and budgetcathryn dippo, bureau of labor statisticssusan dumais, microsoft researchwilliam eddy, carnegie mellon universityjean fox, bureau of labor statisticsjohn gawalt, national science foundationjim gentle, george mason universityvalerie gregg, national science foundationjane griffith, congressional research serviceeve gruntfest, university of colorado at colorado springscarol house, u.s. department of agriculture, national agriculturalstatistics servicesally howe, national coordination office for computing,information, and communicationsterrence ireland, consultantthomas kalil, national economic councildavid kehrlein, governorõs office of emergency services,state of californiasallie kellermcnulty, los alamos national laboratorynancy kirkendall, office of management and budgetbill larocque, national center for education statistics,department of educationfrank lee, census bureaujudith lessler, research triangle institutemichael levi, bureau of labor statisticsrobyn levine, congressional research servicedenise lewis, census bureaugary marchionini, university of north carolinapatrice mcdermott, omb watchtom m. mitchell, carnegie mellon universitysally morton, rand corporationkrish namboodiri, national coordination office for computing,information, and communicationssummary of a workshop on information technology research for federal statisticscopyright national academy of sciences. all rights reserved.52appendixmichael r. nelson, ibmclifford neuman, information sciences institute, university ofsouthern californiajanet norwood, former commissioner, u.s. bureau of laborstatisticssarah nussar, iowa state universityleon osterweil, university of massachusetts at amherstpaul overberg, usa todaybruce petrie, statistics canadalinda pickle, national center for health statisticsjoseph rose, department of educationcharlie rothwell, national center for health statisticsalan saalfeld, ohio state universityravi s. sandhu, george mason universitywilliam scherlis, carnegie mellon universitydiane schiano, interval researchpaula schneider, census bureaujames smith, westatkaren sollins, national science foundationedward j. spar, council of professional associations on federalstatisticspeter stegehuis, westatlatanya sweeney, carnegie mellon universityrachel taylor, census bureaunancy van derveer, census bureaukatherine wallman, office of management and budgetlinda washington, national center for health statisticsandy white, national research council
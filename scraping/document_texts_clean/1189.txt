detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/1189improving risk communication352 pages | 6 x 9 | paperbackisbn 9780309039437 | doi 10.17226/1189committee on risk perception and communication, national research councilimproving risk communicationcopyright national academy of sciences. all rights reserved.improving riskcommunicationcommittee on risk perception and communicationcommission on behavioral and social sciences and educationcommission on physical sciences, mathematics, and resourcesnational research councilnational academy presswashington, d.c. 1989iimproving risk communicationcopyright national academy of sciences. all rights reserved.national academy press 2101 constitution avenue, n.w. washington, d.c. 20418notice: the project that is the subject of this report was approved by the governing board of thenational research council, whose members are drawn from the councils of the national academyof sciences, the national academy of engineering, and the institute of medicine. the members ofthe committee responsible for the report were chosen for their special competences and with regardfor appropriate balance.this report has been reviewed by a group other than the authors according to proceduresapproved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.the study reported here was supported by the agency for toxic substances disease registry,alliedsignal, american cyanamid company, american industrial health council, americanpetroleum institute, bristolmyers company, u.s. department of defense, u.s. department ofenergy, dow chemical usa, electric power research institute, u.s. environmental protectionagency, exxon corporation, hercules incorporated, ilsi risk science institute, mobil oil corporation, monsanto company, motor vehicle manufacturers association, and national science foundation. it also has received support from the national research council fund, a pool of private, discretionary, nonfederal funds that is used to support a program of academyinitiated studies of nationalissues in which science and technology figure significantly. the nrc fund consists of contributionsfrom a consortium of private foundations including the carnegie corporation of new york, thecharles e.culpeper foundation, the william and flora hewlett foundation, the john d. and catherine t.macarthur foundation, the andrew w.mellon foundation, the rockefeller foundation, andthe alfred p.sloan foundation; and the academy industry program, which seeks annual contributions from companies that are concerned with the health of u.s. science and technology and withpublic policy issues with technological content.library of congress cataloginginpublication dataimproving risk communication/committee on risk perception and communication, commissionon physical sciences, mathematics, and resources, commission on behavioral and socialsciences and education, national research council.p. cm.bibliography: p.includes index.isbn 0309039460. isbn 0309039436 (pbk.).1. risk communication. i. national research council (u.s.). committee on risk perceptionand communication.t10.68.i47 1989 89œ9464363.1dc20 cipcopyright © 1989 by the national academy of sciencesno part of this book may be reproduced by any mechanical, photographic, or electronic process, orin the form of a phonographic recording, nor may it be stored in a retrieval system, transmitted, orotherwise copied for public or private use, without written permission from the publisher, except forthe purposes of official use by the u.s. government.printed in the united states of americafirst printing, august 1989second printing, january 1990third printing, march 1990forth printing, june 1990fifth printing, july 1994sixth printing, march 1996seventh printing, 1998iiimproving risk communicationcopyright national academy of sciences. all rights reserved.committee on risk perception andcommunicationjohn f.ahearne, vice president, resources for the future, washington, d.c., chairmanernesta ballard, consultant, seattle, washingtonruth faden, professor, department of health policy and management, johns hopkinsuniversity, baltimore, marylandjames a.fay, professor, department of mechanical engineering, massachusetts instituteof technology, cambridgebaruch fischhoff, professor, department of engineering and public policy anddepartment of social and decision sciences, carnegiemellon university,pittsburgh, pennsylvaniathomas p.grumbly, president, clean sites, inc., alexandria, virginiapeter barton hutt, partner, covington & burling, washington, d.c.bruce w.karrh, vice president, safety, health & environmental affairs, e. i. du pont denemours &; co., wilmington, delawared.warner north, principal, decision focus, inc., los altos, california; consultingprofessor, department of engineeringeconomic systems, and associate director, center for risk analysis, stanford university, stanford, californiajoann e.rodgers, deputy director of public affairs and director of media relations, thejohns hopkins medical institutions, baltimore, marylandmilton russell, professor of economics, university of tennessee, knoxville, and senioreconomist, oak ridge national laboratory, oak ridge, tennesseerobert sangeorge, vice president for public affairs, national audubon society, newyorkharvey m.sapolsky, professor of public policy and organization, political sciencedepartment, massachusetts institute of technology, cambridgejurgen schmandt, professor, lbj school of public affairs, university of texas, austin,and director, center for growth studies, houston area research center, thewoodlands, texasmichael schudson, chair, department of communication, and professor, department ofsociology, university of california at san diego, la jollaiiiimproving risk communicationcopyright national academy of sciences. all rights reserved.percy h.tannenbaum, professor of public policy and director, survey research center,university of california at berkeleydetlof von winterfeldt, director, risk communication laboratory, and professor, department of systems science, institute of safety and systems management,university of southern california, los angeleschris whipple, technical manager, electric power research institute, palo alto,californiasusan d.wiltshire, senior associate, jk associates, hamilton, massachusettsstaffrob coppocknancy a.crowelllawrence e.mccraypaul sterndeborah reischmanivimproving risk communicationcopyright national academy of sciences. all rights reserved.commission on behavioral and socialsciences and educationrobert mcc.adams, secretary, smithsonian institution, washington, d.c., chairmanphilip e.converse, institute for social research, university of michigan, ann arborarthur s.goldberger, department of economics, university of wisconsin, madisonbeatrix a.hamburg, mt. sinai school of medicine, new yorkleonid hurwicz, department of economics, university of minnesota, minneapolisjoseph b.kadane, department of statistics, carnegie mellon university, pittsburgh,pennsylvaniaedward o.laumann, department of sociology, university of chicago, chicago, illinoisalvin m.liberman, haskins laboratories, new haven, connecticutstewart macaulay, school of law, university of wisconsin, madisonroger g.noll, department of economics, stanford university, stanford, californiasamuel preston, population studies center, university of pennsylvania, philadelphiafranklin d.raines, lazard frères & co., new yorklauren b.resnick, learning research and development center, university of pittsburgh,pittsburgh, pennsylvaniajohn m.roberts, department of anthropology, university of pittsburgh, pittsburgh,pennsylvaniaeleanor b.sheldon, new yorkjerome e.singer, department of medical psychology, uniformed services university ofthe health sciences, bethesda, marylandmarshall s.smith, school of education, stanford university, stanford, californiajohn a.swets, bolt, beranek & newman laboratories, inc., cambridge, massachusettssidney verba, university library, harvard university, cambridge, massachusettsp.brett hammond, acting executive directorvimproving risk communicationcopyright national academy of sciences. all rights reserved.commission on physical sciences,mathematics, and resourcesnorman hackerman, robert a.welch foundation, houston, texas, chairmangeorge f.carrier, division of applied sciences, harvard university, cambridge,massachusettsherbert d.doan, the dow chemical company (retired), midland, michiganpeter s.eagleson, massachusetts institute of technology, cambridgedean e.eastman, ibm t.j.watson research center, yorktown heights, new yorkmarye anne fox, department of chemistry, university of texas, austingerhart friedlander, brookhaven national laboratory, upton, new yorklawrence w.funkhouser, chevron corporation (retired), atherton, californiaphillip a.griffiths, duke university, durham, north carolinachristopher f.mckee, department of physics, university of california at berkeleyjack e.oliver, cornell university, ithaca, new yorkjeremiah p.ostriker, department of astrophysical science, princeton university,princeton, new jerseyfrank l.parker, department of civil and environmental engineering, vanderbiltuniversity, nashville, tennesseedenis j.prager, macarthur foundation, chicago, illinoisdavid m.raup, department of geophysical sciences, university of chicago, chicago,illinoisrichard j.reed, department of atmospheric sciences, university of washington, seattleroy f.schwitters, eg&g, inc., wellesley, massachusettsrobert e.sievers, department of chemistry, university of colorado, boulderleon t.silver, division of geological and planetary sciences, california institute oftechnology, pasadenalarry l.smarr, department of astronomy and physics, university of illinois, urbanachampaignedward c.stone, jr., division of physics, mathematics, and astronomy, californiainstitute of technology, pasadenaviimproving risk communicationcopyright national academy of sciences. all rights reserved.karl k.turekian, department of geology and geophysics, yale university, new haven,connecticutraphael g.kasper, executive directorlawrence e.mccray, associate executive director (until august 1, 1988)myron uman, associate executive director (as of august 1, 1988)viiimproving risk communicationcopyright national academy of sciences. all rights reserved.ˆ˘˝,#˘/,#˘4ˆ6˘/'˙˙˙viiiimproving risk communicationcopyright national academy of sciences. all rights reserved.prefacein 1983 the national research council completed a study on managing risk,leading to a report risk assessment in the federal government: managing theprocess. this report focused on improving risk assessment and risk decisionswithin the government. however, a major element in risk management in ademocratic society is communication about risk. growing concern that riskcommunication was becoming a major problem led to the chartering of anational research council committee to examine possibilities for improvingsocial and personal choices on technological issues by improving riskcommunication.the national research council initiated the study out of recognition thattechnological issues, in addition to being critically important, are complex,difficult, and laden with political controversy. because the issues are scientificand technical in content, and cut across the concerns of many governmentagencies, scientific disciplines, and sectors of society, the national researchcouncil seemed to provide an ideal forum for the conduct of such a study.moreover, in past work on policy in the areas of risk assessment and riskmanagement (notably, the abovementioned report on risk assessment), thenational research council has helped develop concepts widely used in thinkingabout the policy issues.it became evident in discussions with representatives of some key federalagencies that no single agency was willing to undertakeprefaceiximproving risk communicationcopyright national academy of sciences. all rights reserved.the needed study on its own or even to act as the primary source of support for astudy at the national research council, even though representatives of severalagencies recognized the importance of risk communication to their activities. as aresult, the national research council initiated the study with its own funds,eventually receiving support for about half the cost from a consortium of federaland private sources.to reflect the breadth of issues to be studied, the committee on riskperception and communication was made responsible to two major units of thenational research council, the commission on physical sciences, mathematics,and resources and the commission on behavioral and social sciences andeducation. the committee represents a cross section of many relevant kinds ofexperience and expertise. it includes members with extensive experienceanalyzing, managing, and communicating about diverse risks, including thosefrom radiation, chemicals, drugs, disease, and consumer products. members haveexperience in diverse settings, including federal and local decisionmakingbodies, industry, the mass media, and environmental and citizens' groups. thecommittee also exhibits diverse disciplinary backgrounds, including physical andsocial sciences, law, journalism, public health, and communications research. thenational research council has tried in constituting the committee to achieve abalance of perspectives on all these dimensions.the committee's charge was to offer knowledgebased advice togovernments, private and nonprofit sector organizations, and concerned citizensabout the process of risk communication, about the content of risk messages, andabout ways to improve risk communication in the service of public understandingand betterinformed individual and social choice. this report does not provide aset of prescriptional guidelines, a ﬁhowtoﬂ manual for risk communicators. thecommittee concluded that many participants in the process lack fundamentalunderstanding of the important points that form the basis for successful riskcommunication. therefore this report concentrates on developing those points.the committee believes that without such understanding detailed guidelineswould not be useful. with such understanding, organizations should be able todevelop their own guidelines to fit their own somewhat unique functions.committee members met six times during the period from may 1987 to june1988. the committee sought knowledge from severalprefaceximproving risk communicationcopyright national academy of sciences. all rights reserved.sources: experimental research on processes of perception, cognition, andunderstanding in individuals, including studies of the understanding of riskestimates; laboratory and field research on the conditions affecting theeffectiveness of communications; and the collected experience of individuals andorganizations that have engaged in organized communication about risk. thecommittee discussed a wide range of hazards, including but by no meansrestricted to those posed by toxic and carcinogenic substances and byradioactivity. it considered communication both about social choices, such aswhether or how strictly to regulate hazardous substances or processes, and aboutpersonal choices, such as whether to change eating habits to avoid cancer orsexual habits to avoid aids. and the committee considered addressing advice toseveral audiences, including public agencies at all levels of government;legislatures; firms and industrial associations; environmental, consumer, andcitizens' groups; journalists and mass media organizations; scientists and theorganizations that employ them; and the interested public.this report presents the insights of the committee. the report shouldsignificantly improve the understanding of what the problems are in riskcommunication, particularly the risk communication activities of government andindustry. the committee's recommendations, if followed, would significantlyimprove the risk communication process.john f.ahearne, chairmancommittee on risk perception and communicationprefacexiimproving risk communicationcopyright national academy of sciences. all rights reserved.prefacexiiimproving risk communicationcopyright national academy of sciences. all rights reserved.acknowledgmentsalthough this report represents the work of the committee, it would not havebeen produced without the support of professional staff from the nationalresearch council, who drafted the chapters and refined them on the basis of thecommittee's discussions and conclusions: paul stern (chapters 1 through 4), robcoppock (chapters 5 and 6), and lawrence mccray (chapter 7). their resumesare included with those of the committee because of their intellectualcontributions, which advanced the committee's efforts throughout the study. thereport was greatly improved by the diligent work of its editor, roseanne price. inaddition, invaluable support was provided by deborah reischman for the firsthalf of the committee's tenure and nancy crowell for the second half.the committee acknowledges with appreciation presentations made atcommittee meetings by the following persons:frederick w.allen, associate director, office of policy analysis, u.s.environmental protection agencybetsy anckerjohnson, vice president, general motors corporationgerald l.barkdoll, associate commissioner for planning and evaluation,u.s. department of health and human services, food and drug administrationrichard baxter, senior vice president, the roper organizationacknowledgmentsxiiiimproving risk communicationcopyright national academy of sciences. all rights reserved.don berreth, director, office of public affairs, centers for diseasecontrold.christopher cathcart, associate director for health and safety,chemical manufacturers associationjoan claybrook, president, public citizendevra davis, director, board on environmental studies and toxicology,national research councilann fisher, manager, risk communications program, u.s. environmentalprotection agencylowell harmison, deputy assistant secretary for health, u.s. departmentof health and human servicesnancy holland, executive director, american blood commissionthomas h.isaacs, deputy associate director, office of geologicrepositories, office of civilian radioactive waste management, u.s.environmental protection agencyedward klein, director, tosca assistance office, u.s. environmentalprotection agencyanthony z.roisman, cohen, milstein & hausfeld, washington, d.c.ben c.rusche, director, office of civilian radioactive waste management,u.s. environmental protection agencychristine russell, alicia patterson foundation fellow (on leave from thewashington post) and president, national association of science writerslinda smith, former executive committee member, stop it, warren,massachusettsroger strelow, vice president, corporate environmental programs,general electric companylee m.thomas, administrator, u.s. environmental protection agencyacknowledgmentsxivimproving risk communicationcopyright national academy of sciences. all rights reserved.contents summary 1 a new perspective 1 common misconceptions about risk communication 3 problems of risk communication 4 conclusions and recommendations 81 introduction 14 the new interest in ﬁrisk communicationﬂ 16 a new definition of risk communication 19 risk messages as part of the risk communication process 23 successful risk communication 26 notes 292 understanding hazards and risks 30 toward quantification of hazards 31 knowledge needed for risk decisions 33 gaps and uncertainties in knowledge 38contentsxvimproving risk communicationcopyright national academy of sciences. all rights reserved. scientific judgment and errors in judgment 44 influences of human values on knowledge about risk 47 implications for risk communication 52 notes 533 conflict about hazards and risks 54 is risk increasing or decreasing? 54 changes in the nature of hazards and in knowledge aboutthem 57 changes in u.s. society 62 politicization of the technological debate 64 implications of conflict for communication 68 notes 714 purposes of risk communication and riskmessages 72 settings of risk communication 72 information and influence: the purposes of risk messages 80 use of influence techniques in risk communication 85 notes 935 common misconceptions about risk communication 94 expectations regarding risk communication 95 beliefs about the functioning of the process 100 stereotypes about intermediaries and recipients 102 note 1076 problems of risk communication 108 problems deriving from the institutional and political system 108 problems of risk communicators and recipients 117 summary 142 note 142contentsxviimproving risk communicationcopyright national academy of sciences. all rights reserved.7 recommendations for improving risk communication 143 management of the process 149 the content of risk messages 165 a consumer's guide to risk and risk communication 176 research needs 179 appendixes a background information on committee members and professional staff 185b bibliography 193c risk: a guide to controversybaruch fischhoff 211d availability of working papers 320e key terms and distinctions 321 index 323contentsxviiimproving risk communicationcopyright national academy of sciences. all rights reserved.xviiicontentsimproving risk communicationcopyright national academy of sciences. all rights reserved.improving risk communicationxiximproving risk communicationcopyright national academy of sciences. all rights reserved.xximproving risk communicationcopyright national academy of sciences. all rights reserved.summarya new perspectivehazards of modern life surround us and so, too, does communication aboutthe risks of those hazards. news reports describe such hazards as pollutants in theair and in drinking water, pesticide residues in food, threats from radiation andtoxic chemicals, and aids. government and industry also send out messagesabout hazards and their risks, sometimes directly to the populace but more oftenthrough intermediaries, such as the print and broadcast media.risk messages are difficult to formulate in ways that are accurate, clear, andnot misleading. one reads, for example, that ﬁradon risk can equal or exceed the2% risk of death in an auto accident,– for anyone who lives 20 years at levelsexceeding about 25 picocuries per literﬂ (kerr, 1988). this statement places anunfamiliar risk (radon exposure in homes) in juxtaposition to a more familiar risk(death in an auto accident), which may help people understand the magnitude ofthis unfamiliar risk. but this simple comparison may be misleading because itdoes not specify the respective levels of exposure, leaves out potentially relevantnonlethal consequences, and uses language (picocuries per liter) unfamiliar tomost people. this report addresses these and other problems confronting riskcommunication.summary1improving risk communicationcopyright national academy of sciences. all rights reserved.risk messages can be controversial for many reasons. the hazards theydescribe are often themselves centers of controversy. frequently, there is enoughuncertainty in the underlying knowledge to allow different experts to drawcontradictory conclusions. experts are frequently accused of hiding theirsubjective preferences behind technical jargon and complex, socalled objectiveanalyses. often a message that is precise and accurate must be so complex thatonly an expert can understand it. messages that nonexperts can understandnecessarily present selected information and are thus subject to challenge as beinginaccurate, incomplete, or manipulative.in the past the term risk communication has commonly been thought of asconsisting only of oneway messages from experts to nonexperts. in this reportthe committee on risk perception and communication takes a differentperspective. because much of the controversy seems to center on the content ofspecific messages, it was tempting to proceed along the lines of many previousdiscussions about risk communication and concentrate on message design. wefound a focus on oneway messages too limiting, however. instead, we make acrucial distinction between risk messages and the risk communication process.we see risk communication as an interactive process of exchange of informationand opinion among individuals, groups, and institutions. when riskcommunication is viewed in this way, significant, though perhaps less obvious,underlying problems can be better discerned and treated.we view success in risk communication in a different way also. some takethe position that risk communication is successful when recipients accept theviews or arguments of the communicator. we construe risk communication to besuccessful to the extent that it raises the level of understanding of relevant issuesor actions for those involved and satisfies them that they are adequately informedwithin the limits of available knowledge.finally, it should be noted that one of the most difficult issues we facedconcerned the extent to which public officials in a democratic society shouldattempt to influence individualsšthat is, to go beyond merely informing themšconcerning risks and such riskreducing actions as quitting smoking. governmentofficials must be accountable for their decisions and will likely find their effortsto influence contested if they stray from accepted scientific views or if theychallenge popular consensus. a public official should be aware of the politicalrisks and of the legitimate constraints placed upon government in advocacy.procedural strategies such as independentsummary2improving risk communicationcopyright national academy of sciences. all rights reserved.review processes can be used to determine the appropriateness of the use ofinfluencing techniques. where an unusually strong degree of advocacy seemswarranted, officials should seek legitimization of such actions through thedemocratic process.common misconceptions about riskcommunicationseveral important misconceptions need to be dispelled before the realproblems of risk communication can be addressed. contrary to what some think,there is no single overriding problem and thus no simple way of making riskcommunication easy. risk messages necessarily compress technical information,which can lead to misunderstanding, confusion, and distrust.many peoplešincluding some scientists, decision makers, and members ofthe publicšhave unrealistic expectations about what can be accomplished by riskcommunication. for example, it is mistaken to expect improved riskcommunication to always reduce conflict and smooth risk management. riskmanagement decisions that benefit some citizens can harm others. in addition,people do not all share common interests and values, so better understanding maynot lead to consensus about controversial issues or to uniform personal behavior.but even though good risk communication cannot always be expected to improve asituation, poor risk communication will nearly always make it worse. it is alsomistaken to think, as some do, that if people understood and used riskcomparisons it would be easy for them to make decisions. comparing risks canhelp people comprehend the unfamiliar magnitudes associated with risks, but riskcomparison alone cannot establish levels of acceptable risk or ensure systematicminimization of risk. factors other than the level of riskšsuch as thevoluntariness of exposure to the hazard and the degree of dread associated withthe consequencesšmust be considered in determining the acceptability of riskassociated with a particular activity or phenomenon.some risk communication problems derive from mistaken beliefs aboutscientific research on the nature of how risks are assessed and managed and onrisk communication itself. scientific information, for example, cannot beexpected to resolve all important risk issues. all too often research that wouldanswer the question has not been done or the results are disputed. although agreat deal of research has been done on the dissemination and preparation of riskmessages,summary3improving risk communicationcopyright national academy of sciences. all rights reserved.there has been much less attention devoted to the risk communication process. inaddition, even when valid scientific data are available, experts are unlikely toagree completely about the meaning of these data for risk management decisions.finally, it is unrealistic to expect easy identification and understanding of thevalues, preferences, and information needs of the intended recipients of riskmessages.other misconceptions involve stereotypes about the way intermediaries andrecipients react to risk messages. it is mistaken, for example, to view journalistsand the media always as significant, independent causes of problems in riskcommunication. rather, the problem is often at the interface between science andjournalism. both sides need to better understand the pressures and constraints ofthe other instead of complaining about the sometimes disappointing results.scientists and risk managers should recognize the importance of the partjournalists play in identifying disputes and maintaining the flow of informationduring resolution of conflicts; journalists need to understand how to frame thetechnical and social dimensions of risk issues. it is also important to recognize thedifferences between the broadcast and the print media and between the nationaland the regional or local press corps.finally, even though most people prefer simplicity to complexity, it ismistaken to expect the public to want simple, cutanddried answers as to what todo in every case. the public is not homogeneous. people differ in the degree towhich they exercise control over exposure to hazards or remediation ofundesirable consequences, the importance they attach to various consequences,and their tendency to be risk averse or risk seeking. often at least part of thepublic seeks considerable information about the risks they face.problems of risk communicationwe distinguish two major types of problems in risk communication.problems deriving from institutional and political systems are problems for whichlittle can be donešbeyond trying to understand themšby those involved in riskcommunication. nevertheless, these problems can have a considerable impact onactions and events. problems of risk communicators and recipients can beaddressed more directly and are therefore more amenable to improvement orsolution.summary4improving risk communicationcopyright national academy of sciences. all rights reserved.problems deriving from the institutional and political systemsseveral kinds of legal considerations, including statutory mandates, liability,and informed consent and ﬁrighttoknowﬂ requirements, influence the optionsavailable to risk managers and thus the content of their risk messages. theseconsiderations generally either limit the possible responses to the risk in questionor require that certain actions be taken in given circumstances. for example,sometimes statutes require consideration of certain factors (the federalinsecticide, fungicide, and rodenticide act explicitly includes consideration ofeconomic benefits) or the exclusion of others (the clean water act specifies thatthe best available technology should be used regardless of the financial burdenimposed). although not necessarily problems as such, these considerations oftenconstitute important influences on risk messages and risk communicationprocesses. it is often difficult to understand why risk messages appear as they dowithout consideration of these factors.communicating with citizens about risks can increase their desire toparticipate in or otherwise influence decisions about the control of those risks,thereby making risk management even more cumbersome. the interests ofcitizens and their motivation to participate in the political process can introducedifficult challenges when the implementation of risk control measures isnecessarily decentralized and local preferences (generally to avoid exposure to aparticular risk) preclude solutions in the broader interest. many hazardous wastefacilities operate under these pressures.divided authority, not only among congress, the executive branch, and thecourts at the federal level but also among federal, state, and local or regionaljurisdictions, creates incentives for each actor to gain as much leverage aspossible from the limited portion he or she controls. such fragmentation makescommunicating about risks harder because it makes government regulation andrisk reduction programs more complex and makes it more difficult to determinewho is responsible for the eventual outcomes.government and industry spend large amounts of money on research, andthus their concerns are usually well reflected in the information developed bythat research. individuals and citizens' groups do not usually have the financialresources to fund research and thus do not enjoy this sort of access to informationand influence over its generation. if a group of people that a risk communicator istrying to reach feels that the system for generating information reliedsummary5improving risk communicationcopyright national academy of sciences. all rights reserved.upon by that source does not consider the group's concerns, it may reject theinformation from that source as a basis for decisions about risks. it is reasonableto speculate that this may, in part, explain why it is so difficult to affect youngpeople's attitudes and behavior about drugs and the aids epidemicštheinformation presented is based on facts that they do not consider very importantin the face of their immediate concerns of peer pressure and personal image.there also may be systematic biases in the provision of information. thosemost strongly motivated to communicate about risk are often also those with thestrongest interest in the decision. whenever a personal or social decision affectsinterested groups or organizations, conflicting messages reflecting the interests ofthose groups or organizations may be expected. the u.s. environmentalprotection agency administrator's statement in 1984 that edb (ethylenedibromide) contamination was a longterm health problem being adequatelyhandled by tolerance guidelines, for example, was in the news at about the sametime that public health officials in massachusetts and florida were removinggrain products with edb contamination from grocery store shelves. experts fromthe food industry joined in, downplaying the risks, while scientists fromenvironmental groups criticized the government's inaction. the beliefs,predispositions, and interests of risk communicators and the groups theyrepresent create incentives to slant, or even distort or misrepresent, information.this can skew messages in many different directions on the same issue.problems of risk communicators and recipientsthe problems encountered by the sources and recipients of risk messagescenter on the following topics: establishing and recognizing credibility, makingthe messages understandable, preparing messages in an emergency, capturing andfocusing attention, and getting information.lack of credibility alters the communication process by adding distrust andacrimony. the most important factors affecting the credibility of a source and itsmessages relate to the accuracy of the messages and the legitimacy of the processby which the contents were determined, as perceived by the recipients.recipients' views about the accuracy of a message are adversely affected by (1)real or perceived advocacy by the source of a position in the message that is notconsistent with a careful assessment of the facts; (2)summary6improving risk communicationcopyright national academy of sciences. all rights reserved.a reputation for deceit, misrepresentation, or coercion on the part of the source;(3) previous statements or positions taken by the source that do not support thecurrent message; (4) selfserving framing of information in the message; (5)contradictory messages from other credible sources; and (6) actual or perceivedprofessional incompetence or impropriety on the part of the source. the perceivedlegitimacy of the process by which the contents of a message were determineddepends on (1) the legal standing of the source with respect to the risksaddressed; (2) the justification provided for the communication program; (3) theaccess afforded affected parties to the decisionmaking process; and (4) thedegree to which conflicting claims are given fair and balanced review.ideally, risk information should use language and concepts recipients alreadyunderstand. it is difficult to present scientific and technical information that useseveryday language and magnitudes common in ordinary experience and that issensitive to such psychological needs on the part of recipients as the desire forclear, decisive answers or the fear of the unfamiliar and unknown.sometimes risk communicators must disseminate messages when there arenot enough relevant data to allow them to draw satisfactory conclusions and thereis no time to obtain better information. this usually occurs when an emergencyrequires that action be taken immediately or not at all or when events lead torequests for information prior to the completion of study or analysis.many things compete with risk messages for attention, and it is oftendifficult to get the intended recipients to attend to the issues the riskcommunicator thinks are important. from the risk communicator's standpoint,there are two aspects of this: stimulating the attention of the ultimate recipient andinteracting with the news media and other intermediaries. there are, of course,several different ways that messages can reach the final recipients: facetoface(physician to patient, friend to friend, within the family), in groups (work sites,classrooms), through professional or volunteer organizations (american medicalassociation, red cross), through the mass media (radio, television, magazines,newspapers, direct mail, billboards), and through community service agencies (atlibraries, hospitals, malls, fairs).recipients of risk messages may have difficulty deciding which issues toattend to or what to do because they cannot get information from officials andother message sources that satisfactorily answers their questions. this can happenwhen authorities do not listensummary7improving risk communicationcopyright national academy of sciences. all rights reserved.and therefore do not provide what the recipient considers relevant information orbecause the individual is unable to find a trusted source or interpreter of alreadyavailable information.conclusions and recommendationsin formulating recommendations we focused on the preparation anddissemination of formal risk messages to audiences that include nonexperts andon only two of the many types of riskmanaging organizations: governmentagencies and large private corporations. nevertheless, our recommendations areintended to attack the problems of recipients of risk messages as well. the goalcannot be only to make those who disseminate formal risk messages moreeffective by improving their credibility, understandability, and so on. such anapproach might serve their interests, but it could well degrade the overall qualityof risk communication if it merely meant that they could advance theirviewpoints with greater influence. risk communication can be improved only ifrecipients are also helped to solve their problems at the same time.the risk communication processšusually with many messages from manysourcesšcan be considered successful only to the extent that it, first, improves orincreases the base of accurate information that decision makers use, be theygovernment officials, industry managers, or individual citizens, and, second,satisfies those involved that they are adequately informed within the limits ofavailable knowledge. this does not always result in the responses a particularsource might wish, nor does it always lead to consensus about controversialissues or to uniform personal behavior. people do not all share common interestsand values, and so better understanding will not necessarily lead them all to thesame conclusion.improving risk communication is therefore more than merely craftingﬁbetter messages.ﬂ risk communication procedures as well as risk messagecontent must be improved. because risk communication is so tightly linked to themanagement of risks, solutions to the problems of risk communication oftenentail changes in risk management and risk analysis. once the constraints,limitations, and incentives affecting the preparation and dissemination ofmessagesš as well as how these factors become manifest in what we call the riskcommunication processšare understood, improvements can be implemented.summary8improving risk communicationcopyright national academy of sciences. all rights reserved.this is not to imply, however, that there is a single shortcut to improving thenation's risk communication efforts. the needed improvement can come onlyincrementally and only from careful attention to many details. risk managersneed to consider risk communication as an important and integral aspect of riskmanagement.four sets of recommendations are presented: (1) recommendations thatpertain to the processes that source organizations use to generate decisions,knowledge, and risk messages; (2) recommendations that pertain to the content ofindividual risk messages; (3) a call for a ﬁconsumer's guideﬂ that will enhance theability of other groups or individuals to understand and participate in riskmanagement activities; and (4) a brief summary of research needs.two broad themes run through the process and content recommendations.the first is the recognition that risk communication efforts should be moresystematically oriented to the intended audiences. the most effective riskmessages are those that quite selfconsciously address the audiences' perspectivesand concerns. the second is that openness is the surest policy. a central premiseof democratic governmentšthe existence of an informed electorateš implies afree flow of information. suppression of relevant information is not only wrongbut also, over the longer term, usually ineffective.management of the processwe identified four process objectives that are key elements in improving riskcommunication: (1) goal setting, (2) openness, (3) balance, and (4) competence.setting realistic goalsrisk communication activities ought to be matters of conscious design.practical goals should be established that explicitly accommodate the political/legal mandates and constraints bounding the process and the roles of the potentialrecipients of the organization's risk messages, on the one hand, and clearly showthe contribution to improved understanding of issues and actions on the other.explicit consideration of such factors encourages realistic expectations,clarification of motives and objectives (both within the source organization andamong outside groups and individuals), and evaluation of performance.summary9improving risk communicationcopyright national academy of sciences. all rights reserved.safeguarding opennessrisk communication should be a twoway street. organizations thatcommunicate about risks should ensure effective dialogue with potentiallyaffected outsiders. this twoway process should exhibit (1) a spirit of openexchange in a common undertaking rather than a series of ﬁcannedﬂ briefingsrestricted to technical ﬁnonemotionalﬂ issues and (2) early and sustainedinterchange that includes the media and other message intermediaries. opennessdoes not ordinarily, however, imply empowerment to determine the hostorganization's risk management decisions. to avoid misunderstanding, the limitsof participation should be made clear from the outset.safeguarding balance and accuracy in risk messagesin order to help ensure that risk messages are not distorted and do not appearto be distorted, those who manage the generation of risk assessments and riskmessages should (1) hold the preparers of messages accountable for detecting andreducing distortion; (2) consider review by recognized independent experts of theunderlying assessment and, when feasible, the message; (3) subject draftmessages, if possible, to outside preview to determine if audiences detect anyoverlooked distortions; and (4) prepare and release for comment a ﬁwhite paperﬂon the risk assessment and risk reduction assessment.fostering competencerisk managers need to use procedures that incorporate two distinct types ofexpertise: on the risk subject matter (e.g., carcinogenic risk, occupational safety)and on risk communication. organizations that communicate about risk shouldtake steps to ensure that the preparation of risk messages becomes a deliberate,specialized undertaking, taking care that in the process they do not sacrificescientific quality. such steps include (1) deliberately considering the makeup ofthe intended audience and demonstrating how the choice of media and messagereflects an understanding of the audience and its concerns; (2) attractingappropriate communications specialists and training technical staff incommunications; (3) requiring systematic assurance that substantive risk expertswithin the organization havesummary10improving risk communicationcopyright national academy of sciences. all rights reserved.a voice in producing accurate assessments and the derivative risk message; (4)establishing a thoughtful program of evaluating the past performance of riskcommunication efforts; and (5) ensuring that their organizations improve theirunderstanding of the roles of intermediaries, particularly media reporters andeditors, including an understanding of the factors that make a risk storynewsworthy, of the practical time and space constraints, and of the limitedtechnical background of most media personnel.risk communication in crisis conditionsthe process for risk communication in crisis conditions requires specialcare. risk managers should ensure that (1) where there is a foreseeable potentialfor emergency, advance plans for communication are drafted, and (2) there isprovision for coordinating among the various authorities that might be involvedand, to the extent feasible, a single place where the public and the media canobtain authoritative and current information.content of risk messageswe identified four generic issues that have been the source of difficulty inthe past over a broad range of risk communication efforts: (1) audienceorientation, (2) uncertainty, (3) risk comparisons, and (4) completeness.relating the message to the audiences' perspectivesrisk messages should closely reflect the perspectives, technical capacity,and concerns of the target audiences. a message should (1) emphasizeinformation relevant to any practical actions that individuals can take; (2) becouched in clear and plain language; (3) respect the audience and its concerns;and (4) seek to inform the recipient, unless conditions clearly warrant the use ofinfluencing techniques. one of the most difficult issues in risk communication in ademocratic society is the extent to which public officials should attempt toinfluence individualsšthat is, to go beyond merely informing themšconcerningrisks and such riskreducing actions as quitting smoking.summary11improving risk communicationcopyright national academy of sciences. all rights reserved.handling uncertaintyrisk messages and supporting materials should not minimize the existenceof uncertainty. data gaps and areas of significant disagreement among expertsshould be disclosed. some indication of the level of confidence of estimates andthe significance of scientific uncertainty should be conveyed.comparing risksrisk comparisons can be helpful, but they should be presented with caution.comparison must be seen as only one of several inputs to risk decisions, not asthe primary determinant. there are proven pitfalls when risks of diverse characterare compared, especially when the intent of the comparison can be seen as that ofminimizing a risk (by equating it to a seemingly trivial one). more useful arecomparisons of risks that help convey the magnitude of a particular risk estimate,that occur in the same decision context (e.g., risks from flying and driving to agiven destination), and that have a similar outcome. multiple comparisons mayavoid some of the worst pitfalls. more work needs to be done to developconstructive and helpful forms of risk comparison.ensuring completenessa complete information base would contain five types of information: (1) onthe nature of the risk, (2) on the nature of the benefits that might be changed ifrisk were reduced, (3) on the available alternatives, (4) on uncertainty inknowledge about risks and benefits, and (5) on management issues. there aremajor advantages in putting the information base into written form as an adjunctto the risk message.a consumer's guide to risk and risk communicationmajor government and private organizations that sustain risk communicationefforts should jointly fund the development of a consumer's guide to risk andrisk communication. the purpose of this guide would be to articulate key terms,concepts, and tradeoffs in risk communication and risk management for the layaudience, tosummary12improving risk communicationcopyright national academy of sciences. all rights reserved.help audiences discern misleading and incomplete information, and to facilitatethe needed general participation in risk issues. such a guide should (1) involvesupport from, but not control by, the federal government and other sources of riskmessages; (2) be under the editorial control of a group that is clearly orientedtoward the recipients of risk messages and under administrative management byan organization that is known for its independence and familiarity with layperspectives and that can undertake the needed outreach and public informationeffort; and (3) cover subjects such as the nature of risk communication, conceptsof zero risk and comparative risk, evaluation of risk messages, and othersdesignated by project participants.research needsas a result of our deliberations, we have identified nine research topics forattention: (1) risk comparison, (2) risk characterization, (3) role of messageintermediaries, (4) pertinency and sufficiency of risk information, (5)psychological stress, (6) the ﬁmental modelsﬂ of recipients, (7) risk literacy, (8)retrospective case studies of risk communication, and (9) contemporaneousassessment of risk management and risk communication. two criteria guidedtheir selection: (1) that additional knowledge would lead to materialimprovement in risk communication practices and (2) that creation of suchknowledge is likely given past results and current research methods. we have notassigned priorities among the nine topics.summary13improving risk communicationcopyright national academy of sciences. all rights reserved.1introductionwhen government agencies must decide whether to evacuate people fromareas where toxic substances are leaching from waste dumps, set standards forexposure to suspected carcinogens, or decide whether to license nuclear powerplants despite some low probability of rupture in a future earthquake, democraticsocieties are faced with difficult choices. the usual criteria of consensus or socialacceptability are insufficient to resolve such issues of modern technology. thedecisions also need to be scientifically informed, because some choices set inmotion physical or biological processes whose results, if they could be foreseen,would be considered undesirable by most people.only a few experts possess the best knowledge available to estimateaccurately the extent of the possible harm or the likelihood of its occurrence. butwhile great weight needs to be given to the specialized knowledge of theseexperts, democratic principles require that the decisions be controlled byofficials, generally nonspecialists, who are answerable to the public. as jeffersonrealized long ago, public decisions that require specialized knowledge raisequestions about political power.i know of no safe depository of the ultimate powers of society but the peoplethemselves; and if we think them not enlightened enough to exercise theircontrol with a wholesome discretion, the remedy isintroduction14improving risk communicationcopyright national academy of sciences. all rights reserved.not to take it from them, but to inform their discretion. (thomas jefferson, letterto william charles jarvis, september 28, 1820)to remain democratic, a society must find ways to put specializedknowledge into the service of public choice and keep it from becoming the basisof power for an elite.because technological decisions have implications for public health and forpolitical power, they are often highly contentious and emotional. participants,expert and nonexpert alike, have much at stake and are strongly motivated towork for the outcomes they favor. the ensuing political struggles are oftenfrustrating for the participants. nonexperts are frustrated by the inaccessibility ofthe knowledge they need to inform their opinions and by presentations of neededknowledge that are oversimplified, overly technical, or condescending in tone.technical experts are frustrated when their explanations of available knowledgeare met with apathy, disbelief, or anger. government and corporate officials arefrustrated when their discussions of technological alternatives are met byexpressions of public mistrust and accusations of malevolence. environmentalactivists are frustrated by requirements that they argue positions that are based onhuman and environmental values in the language of science and technology andby lack of sufficient resources to make technical arguments strongly.participants come to see the debates in different ways, depending on theirpositions in them and the frustrations they have experienced (dietz et al., 1989;edwards and von winterfeldt, 1986; lynn, 1986; otway and von winterfeldt,1982). many, especially in the scientific and technical community and ingovernment, have defined the underlying problem in terms of ﬁpublicunderstanding of risk,ﬂ ﬁrisk perception,ﬂ and ﬁrisk communication.ﬂ theybelieve that what is needed is for people to better understand or more accuratelyperceive the potential costs and benefits of certain technological options. toaccomplish this, they argue that scientists, governments, and the mass media needto do a better job of risk communication, by which they mean explaining thechoices and their likely consequences to nonexperts. they argue that increasedefforts of this kind would make conflicts about technological choices easier toresolve and would enable the society to make better choices about protectingpublic health, safety, and environmental quality. for reasons elaboratedthroughout this report, we believe this concept of risk communication anddecision making is incomplete and, in importantintroduction15improving risk communicationcopyright national academy of sciences. all rights reserved.respects, misleading; it supports misconceptions about the risk communicationprocess and raises unrealistic expectations about what risk communication canaccomplish.the new interest in ﬁrisk communicationﬂinterest in ﬁrisk communicationﬂ is quite recent.1 that interest is evident in arecent explosion of conferences, seminars, articles, and books with the term ﬁriskcommunicationﬂ in their titles (bean, 1987; covello et al., 1987b, 1988; davieset al., 1987; fischhoff, 1987; lind, 1988; otway, 1987; plough and krimsky,1987; zimmerman, 1987). it reflects increased attention, especially in someagencies of the federal government but in other organizations as well, to the taskof informing the general public about the nature of the health, safety, andenvironmental risks associated with personal and societal choices. the newconcern with informing the public has several motivating sources, not entirelyconsistent with each other, including (1) a requirement for or desire bygovernment to inform, (2) a desire by government or industry officials toovercome opposition to decisions, (3) a desire to share power betweengovernment and public groups, and (4) a desire to develop effective alternativesto direct regulatory control. moreover, the term risk communication has differentmeanings to different users.requirement for or desire by government to informsometimes government is required to inform the public. a series of federallaws, beginning with the administrative procedures act of 1946 and continuingwith the freedom of information act, the national environmental policy act, andthe ﬁcommunity right to knowﬂ provisions of title iii of the superfundamendments and reauthorization act of 1986, recognizes the public's right to beinformed about certain hazards and risks, even if they have no part in thedecisionmaking process. many of these laws have been reinforced by federalcourt decisions and presidential executive orders. these actions emphasize thegovernment's responsibility to be accountable to the people; they state as nationalpolicy that regulators must explain why one course was chosen rather thananother and that the public has a right to see and challenge the basis for thedecisions. thus agencies are required to send messages to the public about thereasons for their decisions and to solicit messages of comment from citizens. theterm risk communication is sometimes used to describe these messages.introduction16improving risk communicationcopyright national academy of sciences. all rights reserved.some government officials provide information not required by law.regulatory officials sometimes do this because they believe that people wouldbenefit from specific knowledge. for instance, the u.s. environmental protectionagency (epa) over the past few years has made an effort to inform householdersabout the hazard of radiation exposure from indoor radon. and public healthofficials, responding to their general mandate, have long offered information tocitizens about the health risks of dietary and sexual habits, drug and alcohol use,and other personal activities. provision of such information is what some publichealth officials mean by risk communication.desire to overcome opposition to decisionsover the past 30 years public participation in debates on technological issueshas intensified. more groups have become involved, including workerspotentially at risk from hazardous activities, regulatory organizations, citizens andenvironmental groups, the press, and the courts. the proponents of controversialtechnological options or decisions, most frequently in government or industry,often meet intense political opposition. frequently, groups of citizens who opposeparticular technological projects have delayed or stymied those projects withlawsuits, mobilization of congressional opposition, or public demonstrations.when a government or industry official has the benefit of extensive scientificstudy and the opposition seems simply to disregard the technical evidence, theofficial can come to see ﬁthe publicﬂ as irrational. government and industryofficials who see the issues this way are likely to define the conflicts thatsurround them as debates between the informed and the ignorant or, worse,between the rational and the irrational. such officials are tempted to look forways to influence the members of the opposition, either by more activelypresenting a straightforward account of the knowledge they have available or bycarefully packaging or even distorting that knowledge to achieve a persuasiveeffect. the use of information to overcome political opposition makes somenotion of risk communication attractive to many proponents of controversialtechnology; it is, in fact, what they mean by the term.desire to share power between government and publicgroupsgovernment officials have sometimes seen in risk communication a way toreduce conflict with segments of the public by sharing power. in this situation anagency takes the role of technical analyst andintroduction17improving risk communicationcopyright national academy of sciences. all rights reserved.adviser, gathering and summarizing the information relevant to a decision at handand explaining that information to various political actors. the agency's rolemight be to inform a public debate, for instance, in a legislative decision on sitingof a hazardous facility. or, if the agency is legally required to make the decisionitself, it can provide information to the public and use the ensuing debate to helparrive at a decision that it judges to be both defensible within its legal mandateand maximally acceptable to the interested parties involved.such an approach was used, albeit unsuccessfully, by the epa in acontroversial case in 1983. prior to making a regulatory decision about an asarcocorporation smelter that was releasing arsenic into the air, the agency presentedthe people of tacoma, washington, with the best information it had availableabout the risks and benefits of three possible outcomes: continued operation ofthe smelter, operation with pollution controls added, and closing of the smelter(krimsky and plough, 1988). epa intended that the ensuing dialogue would helpthe community arrive at its own preference and inform epa so it could make adefensible decision that would also satisfy local opinion. administrator williamruckelshaus justified his action, which depended critically on the success of theagency's efforts to provide information, with an appeal to jefferson's advice toinform the public's discretion (ruckelshaus, 1983). the incident led to a heatedcontroversy in which epa was accused by some of an evasion of itsresponsibility and by others of attempting to manipulate the public by presentingan incomplete set of options. although the smelter was shut down by asarcobefore the public process ran its course, ruckelshaus's goal of achievingconsensus appears unlikely to have been attained.desire to develop effective alternatives to direct regulatorycontrolgovernment officials sometimes wish to persuade individuals to protecttheir health by personal action rather than to adopt regulatory policies that requirehealthprotective actions. the new interest in risk communication in governmentpartly reflects a search for alternatives to direct regulatory control of healthhazards, which was accelerated in the 1980s by the reagan administration'sphilosophical opposition to regulation. government agencies have sought ways tocontrol hazardous substances or activities short of banning themintroduction18improving risk communicationcopyright national academy of sciences. all rights reserved.(as they have done with highdose vitamin preparations), restricting or taxingthem (as with alcohol), or requiring control measures (as with seat belts). someof the alternatives involve replacing regulatory prohibitions and financialpenalties imposed on those who produce hazardous technologies with reliance oninformed discretion of the users of those technologies. for instance, the early1980s brought a shift in the government's treatment of most motorists'unwillingness to use seat belts. a regulatory requirement that manufacturersequip cars with air bags or other ﬁpassive restraintsﬂ to protect passengers whofail to fasten seat belts was replaced by a campaign to persuade, relying on paidand public service advertising. the government even supported research on betterways to convince people to use seat belts (geller, 1983). now, after many years,there appears to be an increase in the use of seat belts, although in some cases thismay be due to state laws mandating their use. such persuasion programs areadequate as alternatives to regulatory constraint only if two conditions are met: ifpersuasion is accepted as a technique of public policy and if persuasion is aboutas effective as direct regulatory control. to some an important aspect of riskcommunication is the use of messages to induce people to protect themselves.a new definition of risk communicationalthough the motives for and meanings of risk communication describedabove are very different in some ways, each emphasizes a particular kind ofmessage: a message that is developed by technical experts; that describes orcharacterizes hazards, risks, or riskreducing actions; and that is addressed tononexperts. to many who use the term, risk communication means simply thedevelopment and delivery of this kind of oneway message. this widespreadusage is illustrated in the foreword to the published proceedings of the firstnational conference on risk communication, attended by 500 people inwashington, d.c., in 1986. william reilly, the president of the conservationfoundation, observed:[in] the conflict or confusion over risk questions–often the communicationprocess is at fault or, at the least, exacerbates the problem. risk communicatorssimply do not do a good job of getting their message across. (reilly, 1987:vii)this very typical formulation equates risk communication with the deliveryof certain kinds of messagesšoneway messages from government or other riskcommunicators to the general public aboutintroduction19improving risk communicationcopyright national academy of sciences. all rights reserved.the nature of risks. it defines the success of risk communication from the point ofview of the senders of those messages, in terms of ﬁgetting the message across.ﬂthe image is of experts enlightening or persuading an uninformed and passivepublic.we consider this formulation of the problem to be incomplete in criticalways. increased efforts ﬁto get the message acrossﬂ by describing the magnitudeand balance of the attendant costs and benefits or by telling people which optionprovides the greatest net benefit to society will have little effect for severalreasons. first, costs and benefits are not equally distributed across a society.those who bear more than a proportionate share of the costs of one of the optionswant to convince others that the selection of that alternative would be unfair tothem. other political participants want to make similar arguments on their ownbehalf or to consider the arguments of all the interested parties. thus animportant aspect of conflicts about technological issues is that these are oftenconflicts between different interest groups. these conflicts cannot be resolvedsimply by knowledge about the likely effects of each alternative on the society as awhole or on various groups.second, people do not agree about which harms are most worth avoiding orwhich benefits are most worth seeking. they want to argue for the protection ofwhat they value and to consider which values are most worth preserving oradvancing in each decision context. because conflicts about technological issuespit values against each other, it is impossible to calculate net benefit to societyšor even to subgroups of the societyšon any scale that will satisfy all theparticipants. values need to be debated and weighed in a political process.third, citizens of a democracy expect to participate in debate aboutcontroversial political issues and about the institutional mechanisms to whichthey sometimes delegate decisionmaking power. a problem formulation thatappears to substitute technical analysis for political debate, or to disenfranchisepeople who lack technical training, or to treat technical analysis as moreimportant to decision making than the clash of values and interests is bound toelicit resentment from a democratic citizenry. because of such reactions to them,problem formulations that attribute technological conflict to widespread publicignorance only exacerbate the conflict.we do not deny or minimize the importance of scientific and technologicalknowledge to informed public decisions about technology. in fact, we stronglyendorse the proposition that understandingintroduction20improving risk communicationcopyright national academy of sciences. all rights reserved.science in general and the likely consequences of particular technological choicesshould be more widespread. but we emphasize just as strongly the fact thattechnological choices are value laden. nonexperts need to gain technicalknowledge, but technical experts and public officials also need to learn moreabout nonexperts' interests, values, and concerns.in a democracy communication is an essential part of all societal decisions.the participantsšindividuals, groups, and institutionsš express their concernsand viewpoints, present facts and arguments to support them, and listen to whatother participants have to say. at various points in this ongoing process, electedofficials and public servants act in the name of the society, sometimes addingtheir own messages to those already current. the communication continues, withconcerns and viewpoints about government actions and messages as well asabout the original issues being addressed.we see risk communication as a particular instance of this sort of democraticdialogue. thus we have come to use the term risk communication differently fromits common current usage.2 risk communication is an interactive process ofexchange of information and opinion among individuals, groups, andinstitutions. it involves multiple messages about the nature of risk and othermessages, not strictly about risk, that express concerns, opinions, or reactions torisk messages or to legal and institutional arrangements for risk management. aswe will establish in chapter 4, risk communication is successful only to theextent that it raises the level of understanding of relevant issues or actions andsatisfies those involved that they are adequately informed within the limits ofavailable knowledge.risk communication is a component of risk management, which is theselection of risk control options. it is the process that provides the information onwhich government, industry, or individual decision makers base their choices.successful risk communication does not guarantee that risk managementdecisions will maximize general welfare; it only ensures that decision makerswill understand what is known about the implications for welfare of the availableoptions.the above definition of risk communication differs critically from manycommon uses in distinguishing between communication, which is an interactiveprocess, and messages, which flow one way. among people responsible fordesigning messages about risk, there is a temptation to confuse the task ofmessage design and dissemination with the entire communication process and toequate the success of their messages in producing the effect desired with thesuccess of riskintroduction21improving risk communicationcopyright national academy of sciences. all rights reserved.communication. we have chosen a definition that takes a broader perspectivethan that of any single participant in the process in order to emphasize thedifference between the disparate activities and goals of the many participants andthe social purposes of the risk communication process.risk communication includes all messages and interactions that bear on riskdecisions. thus risk communication includes announcements, warnings, andinstructions moving from expert sources to nonexpert audiencesšthe kinds ofmessages reilly refers to. but it also includes other kinds of messagesšaboutrisk information and information sources, about personal beliefs and feelingsconcerning risks and hazards, and about reactions to risk management actions andinstitutions. not all these messages are strictly about risk, but all are material torisk management.our use of the term risk communication also pays explicit attention to thesocial interaction and debate that are essential to democratic political choice andthat often contribute to personal decisions about hazardous activities. riskcommunication includes messages moving in various directionsšnot only fromexperts to nonexperts but also from nonexperts to each other, from nonexperts toexperts, and especially the messages of political participation, from citizens topublic decision makers. decisions in government depend on dialogue between thedecision maker and staff within the responsible agency and between the decisionmaker and various political participants, who influence the decision maker's viewof the risks and the risk management options. messages about nonexperts'perceptions of fairness, legal constraints, feelings of outrage, and the mobilizationof interest group pressure are among the important elements of the riskcommunication process, along with messages about the risks themselves. evenwith personal risk decisions, choice often depends on a dialogue in whichtechnical knowledge may not be the dominant influence. decisions to stopsmoking, for instance, have often been influenced more strongly by the expressedvalue preferences of the smoker's children than by experts' messages about healthconsequences.as with other communication in a democracy, the intent of the participantsin risk communication is sometimes political. that is, messages about risk aresometimes intended to influence the beliefs or actions of those to whom they areaddressed. risk communication, then, must be understood in the context ofdecision making involving hazards and risks, that is, risk management.communication aboutintroduction22improving risk communicationcopyright national academy of sciences. all rights reserved.risk deserves special attention because the highly technical nature of the subjectmatter makes it more difficult than communication about other controversialissues. risk decision makers, including individuals managing personal hazardsand participating in public decisions, need to seek and interpret complextechnical information from scientific disciplines in which they have not beentrained. they must communicate with, and to some extent rely upon, the expertswho generate that information. because the attendant choices are controversial,affecting important economic interests and strongly held values, participants inthe decision process, including experts and their employers, have incentives toappeal to emotions, distort facts, and otherwise use communication to influencethe ultimate choice in the directions they desire. thus there are no participants indebates on technological issues on whom nonexperts and public officials can relyunquestioningly for unbiased information.risk messages as part of the risk communicationprocessrisk messages, because they flow in only one direction, are only part of theinteractive risk communication process. risk messages include verbalstatements, pictures, advertisements, publications, legal briefs, warning signs, orother declaratory activities that describe, characterize, or advocate positions oractions regarding risks, hazardous technologies or activities, or risk controloptions. each risk message has an identifiable source and is addressed to one ormore audiences.risk messages come from a variety of sources: physicians, journalists,regulatory agencies, manufacturers, environmental groups, health officials, andvarious selfappointed advisers. the messages are sometimes merely descriptiveof risks and scientific studies of them; at other times the messages also describethe broad context within which a specific hazard or risk is found, thedevelopments that preceded its occurrence, comparison of it to other hazards orrisks, or the presentation of information about a risk along with informationabout the attendant benefits and the risks and benefits of alternatives. asmentioned above and as discussed in more detail in chapter 4, risk messages maybe constructed to inform their recipients or to influence them.a large theoretical and empirical literature on communication, socialinfluence, and persuasion provides considerable knowledge forintroduction23improving risk communicationcopyright national academy of sciences. all rights reserved.anyone who wants to design effective risk messages. however, this knowledge issufficient only to identify important principles, barriers, pitfalls, opportunities,and so forth. it is not adequate to inform many of the specific choices messagedesigners make about characterizing particular risks for particular audiences.lessons from the communication literature have been applied with some successin a range of areas, some of which involve efforts to induce individuals to reducerisks to themselves from cigarette smoking (mcalister, 1981) and heart disease(maccoby and solomon, 1981).3 the following, necessarily brief, summary givessome idea of the concepts and general conclusions developed in this researchtradition. researchers typically discuss the message content, the source of themessage, the channel by which the message is transmitted, and the audience orrecipients of the message (hovland et al., 1953; mcguire, 1985).of key importance to the effect of a message are the characteristics of theintended audience. the important attributes of the audience include its makeup interms of cultural background, shared interests, concerns and fears, socialattitudes, and its facility with language. a message that has a desired effect onone audience may have little effect on another. messages in scientific languageare likely to mean little to nonscientists, whereas messages about risk in everydaylanguage may be unimpressive to scientists.risk messages can be carried by a variety of media: facetoface interaction,direct mailings, advertising, hot lines, presentations to groups, press conferences,television or radio interviews, newspaper or journal articles, and so on. eachmedium has its advantages and limitationsšfor example, television reaches manypeople but needs visual material and is typically presented in short segments, andnewspapers rely on the written word and can present longer, more complexmessages but are less vivid and immediate in emotional impact. in general, thecharacteristics of each channel affect the type of message that can be effectivelytransmitted.the characteristics of the source of a message often affect the way audiencesrespond to it. among the key factors influencing the way recipients judge amessage are the degree of expertise the recipients believe the source to possessand the degree of trust the recipients have in the source. the term ﬁcredibilityﬂ isused by researchers in this field to refer to an attribute of a source that derivesfrom a combination of expertise and trust, as seen by the audience. it is possiblefor a source to be credible to some recipientsintroduction24improving risk communicationcopyright national academy of sciences. all rights reserved.and not others or on some issues but not others (mcguire, 1985). thus a locallyrespected old farmer may be credible to neighboring farmers as a source ofinformation on pesticide risks but may not be credible to the officials whoconvene a regulatory hearing. similarly, the scientific representative of ahazardous waste disposal company may be credible to a federal regulator but notto the neighbors of a proposed waste site. the officials do not credit the farmerbecause of lack of technical expertise; the neighbors do not credit the company'sscientist because of lack of trust.where there is widespread mistrust of public sources of information, peopleoften rely on wordofmouth or other local sources, even if their informants areless expert than those available through public sources. because of this practice,public agencies can sometimes be more effective in delivering technicalinformation to individual citizens by using trusted sources as intermediaries thanby designing and disseminating messages themselves (stern and aronson, 1984).public officials can also listen to trusted intermediaries to learn if tasks might bedelegated or to save the time and expense of questionnaires or other analyseswhen less detail is sufficient.risk messages are often designed to inform nonspecialists. because suchmessages involve complex and difficult concepts, presenting clear andunderstandable information is a tremendous challenge for message designers. thesource's choice of message content depends on what it believes the audienceneeds to know, on what recipients can be expected to understand, and on theaction or response the source hopes to engender.some risk messages are intended to influence the recipients' beliefs oractions. messages are more effective at producing behavior change when, inaddition to producing understanding, they are specific about any desired responseand proximate in time and place to that response. generally, single messages canbe expected to have little effect on recipients' behavior, but organized programsof messages, in which different messages are aimed at different specificpurposes, can be effective.as discussed above, considerable research has been devoted to the study ofmessages to change individual behavior, and the resulting knowledge can help indesigning more effective risk messages. but less is known about other aspects ofrisk communication. for instance, there has been little systematic study of waysto design more effective messages to express citizens' concerns to governmentintroduction25improving risk communicationcopyright national academy of sciences. all rights reserved.or to influence the actions of organizations such as corporations or governmentagencies.successful risk communicationa focus on risk communication implies a standpoint outside the process. itputs no particular actor or message source at the center. in this respect anemphasis on risk communication is different from one on risk messages. thesource of a risk message is likely to define and assess the success of its messagesaccording to its own criteria. it may choose to consider its messages successfulwhen the recipients understand them, or when they believe them to be accurate,or when they do what the sender wants to influence them to do. obviously,different sources may have conflicting goals for their risk messages. this is onedifference between the success of a single source's messages and the success ofthe risk communication process.it is possible to arrive at a meaningful idea of success for riskcommunication by considering a broad public purpose that successful riskcommunication serves. if a society values democratic decision making and wellinformed, goaldirected individual choice, it makes sense from the societalstandpoint to say that the purpose of risk communication is ﬁto inform thediscretionﬂ of government officials, private organizations, and individuals. fromthat standpoint, risk communication is successful to the extent that it raises thelevel of understanding of relevant issues or actions and satisfies those involvedthat they are adequately informed within the limits of available knowledge.informed discretion for a government official is based on knowledge aboutthe risks and benefits of the choices at hand; about the management situation,including legal and other constraints on choice; about the concerns andpreferences of citizens and other political actors; and about the politicalenvironment. corporate officials need much the same kinds of knowledge,although they need to pay special attention to the preferences of shareholders andcan sometimes afford to pay less attention to the preferences of the public.government and corporate officials usually inform themselves about risks andbenefits with the help of expert employees or consultants who interpret technicalknowledge for them (chapter 2 discusses what is involved in understandingrisks). they inform themselves about citizen concerns and political matters bypaying attention tointroduction26improving risk communicationcopyright national academy of sciences. all rights reserved.elected officials, the mass media, and diverse other sources. according to theabove definition, the more accurate the official's understanding of those matters,the better the risk communication system.citizens are decision makers in their private lives and when they participatein political decisions. thus a successful risk communication process informs theirdiscretion, too. citizens inform themselves by interpreting risk messages fromvarious sources, including experts, intermediaries such as journalists, publicrelations officials of public agencies and corporations, and even friends andneighbors. they evaluate or balance what they know in order to reach a judgmentand to make decisions regarding risks, such as whether to protest, ignore,negotiate, or take protective action. the more accurate the citizens' understandingof the issues at hand, the better the risk communication may be said to be.citizens are well informed with regard to personal choices if they haveenough understanding to identify those courses of action in their personal livesthat provide the greatest protection for what they value at the least cost in termsof those values. citizens are well informed with regard to a public policy issue ifthey have enough understanding to evaluate which options provide the mostprotection at the least cost, both for themselves and for the things they believe thesociety should value. (in chapter 4 we discuss the meaning of successful riskcommunication in more detail.)it is important to make several points about the definition of successful riskcommunication. first, success is defined in terms of the information available tothe decision makers rather than in terms of the quality of the decisions that ensue.successful risk communication does not always lead to better decisions becauserisk communication is only part of risk management. risk managers, includingpublic officials and private citizens, must also take into account their publicresponsibilities or personal values. it is possible to understand fully what isknown about the likely consequences of each available option and yet to make aﬁbadﬂ choice; if this occurs, it is not because of a failure of risk communication.consider, for instance, the head of a federal agency who is constrained by law toban a food additive even though the risk communication process has made itclear that there are no less harmful or costly alternative additives. making thelegally required decision does not mean the communication process failed; in thelong run the process may even provide impetus for changing the law. similarly, ifsomeone understands but disregards information about the dangers of smoking,skydiving, or riding aintroduction27improving risk communicationcopyright national academy of sciences. all rights reserved.motorcycle without a helmet, this may not mean risk communication was at fault.people sometimes put themselves at risk with full knowledge, and observersattribute their acts to overriding values, willfulness, or addiction rather than to afailure of communication. risk communication, even at best, can accomplish onlyso much.second, successful risk communication need not result in consensus aboutcontroversial issues or in uniform personal behavior. although such objectivesoften serve the producers of risk messages as criteria of success for thosemessages, they are not appropriate criteria for the risk communication process in ademocracy. to say that success requires that the recipients do or believe what aparticular message source desires is to assume that that message source is a betterjudge of the recipients' interests than the recipients themselves. because peopledo not all share common interests or values, better understanding will not leadthem all in the same direction. and it will not necessarily make choices easier fordecision makers in government agencies or elsewhere.third, according to the definition of success, the recipient must be able toachieve as complete an understanding of the information as he or she desires. inchapter 4 we develop the reasons underlying this definition of successful riskcommunication. a risk communication process that disseminates accurateinformation is not successful unless the potential recipients achieve a sufficientunderstanding. thus the risk communication process must be judged by the levelof knowledge on which decision makers act rather than by the level of knowledgereflected in particular messages or even in the full set of messages accessible todecision makers. it is common for accurate messages to be ignored,misunderstood, or rejected; it is also possible for several inaccurate messages fromdifferent sources to be compared with each other in such a way as to give therecipient a fairly accurate overall picture.risk communication, then, is more than oneway transmission of expertknowledge to the uninformed. certainly, messages describing expert knowledgeto nonexperts play a critically important role in risk communication. theyprovide essential information that nonexperts cannot get from other sources. theyare also essential because, by revealing expert dissent, they give nonexperts,including many government officials, an important tool for checking againstomissions or excesses in any one expert's analysis. messages about expertknowledge are necessary to the risk communication process; they are notsufficient, however, for the process to be successful. thus, althoughintroduction28improving risk communicationcopyright national academy of sciences. all rights reserved.experts and the organizations that disseminate their knowledge are importantparticipants in risk communication, nonexperts also play an important active partby expressing concerns to experts and by asking or pressuring them to provideanalysis of aspects of risk that they may not yet have examined in detail. theyplay an essential role by participating in the debate about what values ought to beapplied to knowledge about risks and how they ought to be applied. citizens'dialogue with public and industrial risk managers, even when it does not directlyaddress risk, can be critical to risk management decisions. the broad definitionof risk communication is a reminder that public decisions about risk requiredebate about values and interests as much as about risks because risks cannot beweighed against each other without considering values. as we will see in thenext chapter, they cannot even be understood without considering values.notes1. for discussions of the recent interest in the subject of risk communication, see plough and krimsky(1987) and stallen and coppock (1987).2. for a complete listing of the special terminology used in this report, see appendix e.3. extensive reviews of the communication literature, covering well over 1000 sources, can be foundin chapters of the 1985 handbook of social psychology (mcguire, 1985; moscovici, 1985; robertsand maccoby, 1985). a review of much of this literature with a focus on risk communication has beencompleted by covello et al. (1987b).introduction29improving risk communicationcopyright national academy of sciences. all rights reserved.2understanding hazards and risksthroughout recorded history people have engaged in hazardous activities,and governments have taken action to control some of those activities in thepublic interest. but in recent times the hazards of greatest concern, andknowledge about them, have changed in ways that make informed decisionsharder to reach. once the focus was simply on the presence or absence of danger.if a food was ﬁadulterated,ﬂ if water was determined to be ﬁimpure,ﬂ if a bridgeor dam was declared ﬁunsafe,ﬂ or if a workplace was ﬁdangerous,ﬂ action wascalled for. when people called on government to take action, they wantedsimple, clearcut measures: ban sale of the food, supply pure water, condemn thebridge, eliminate the workplace hazard. but with increased understanding of thenature of the choices, it has become harder to maintain a simple view.responsible decision makers need to know more about the alternatives than thatone of them is hazardous.in this chapter we outline the many kinds of knowledge a wellinformeddecision requires and the ways in which this knowledge is often incomplete anduncertain. we show how, under such conditions, the judgments of both expertsand nonexperts can be affected by preexisting biases and cognitive limitations andhow human values and concerns inevitably enter into the analytic process. thesefactors often lead experts to disagree with each other and with nonunderstanding hazards and risks30improving risk communicationcopyright national academy of sciences. all rights reserved.experts about the significance of risks, even when the facts are not in dispute.toward quantification of hazardsone reason decision makers need more knowledge is that it has becomeclear that eliminating one danger can create a new one. to rid the water supply oforganisms that cause typhoid and other infectious diseases, water has beenchlorinated since early in this century. this action resulted in chemical reactionsin the water that produced chloroform and other carcinogenic chlorinatedhydrocarbons. to choose between the dangers, one must answer difficultquestions: which danger is more worth avoiding? how much decreased dangerfrom typhoid is enough to justify a certain amount of increased danger of cancer?experts agree that there will be fewer deaths from chlorinationinduced cancerthan there once were from typhoid, but is that enough information to make adecision? it may be important to consider that typhoid and cancer are verydifferent kinds of dangers. typhoid is an acute disease and cancer is a chronicone; typhoid is much more treatable; and there are alternatives to chlorination forpreventing it, although the alternatives also present hazards, as yet poorlyunderstood.society is faced with many choices that trade one danger for another andthat raise similar questions. for instance, regulated commercial canning of foodreduced the danger of botulism compared with home canning, but the use of leadsolder in ﬁtinﬂ cans introduced a toxin not present in home canning jars. lighterautomobiles use less fuel and generate less air pollution, but in a collision with anolder, heavier vehicle they are more dangerous to their occupants.societal choices also involve the benefits associated with hazards and thecosts of hazard reduction. industries that pollute air and water also provide jobsand profits; before requiring pollution controls, public officials usually want toconsider the probable effects of the available options on those benefits. citiesmay install traffic lights to reduce fatalities and injuries, but officials usuallywant to consider whether this is the best way to spend scarce revenues. thusdecision makers want good estimates of how much each alternative will reducehazards so that they can judge the potential benefits against the potential costs.decision makers need detailed knowledge because it has become clear thatmaking the world safer for most people can make it moreunderstanding hazards and risks31improving risk communicationcopyright national academy of sciences. all rights reserved.dangerous for some. pesticides and herbicides have helped make wholesome foodmore available and have helped improve the diets of lowincome consumers, butthey expose agricultural workers to hazardous chemicals and can be a significantpolluter of water supplies. the total danger to society may have decreasedgreatly, but that knowledge may be of no comfort to farm workers. nuclearpower offers some people the benefit of cleaner air but may expose differentpeople to radioactivity in the event of an accident. how is society to weigh smallbenefits to many against what are sometimes larger dangers for a relative few?decision makers need detailed knowledge for another reason as well: thehazards of greatest concern today are more difficult to observe and evaluate thanthe major hazards of the past. half a century ago most of the major health andsafety hazards were of immediate onset: accidents, bacterial infections,poisonings, and the like. most of the hazards that are now controversial are ofdelayed onset, sometimes not being evident for decades after exposure andsometimes affecting only the offspring of those who were exposed. it can be hardto know what the hazards of a substance or activity are before a generation ofexperience has accumulated.to make informed choices, it helps to look carefully and analytically at thehazards each alternative entails. it is important to develop quantitativeknowledge: how much cancer might be caused by chlorinating water? how muchpesticide are farm workers exposed to? for this kind of analysis, someconceptual distinctions are useful. the most basic of these is between ﬁhazardﬂand ﬁrisk.ﬂ an act or phenomenon is said to pose a hazard when it has thepotential to produce harm or other undesirable consequences to some person orthing. the magnitude of the hazard is the amount of harm that may result,including the number of people or things exposed and the severity ofconsequence. the concept of risk further quantifies hazards by attaching theprobability of being realized to each level of potential harm.1 thus an area thatexperiences a severe hurricane once in 200 years faces the same hazard but onlyonetenth the risk of a similar area that experiences an equally severe hurricaneonce in 20 years. the concept of risk makes clear that hazards of the samemagnitude do not always pose equal risks.risks of the same magnitude do not always pose equal concerns, either.most quantitative measures of risk combine the undesirability of a hazard and itsprobability of occurrence into a single summary measure. use of such summarymeasures can simplify large amountsunderstanding hazards and risks32improving risk communicationcopyright national academy of sciences. all rights reserved.of data but can be unsatisfying to people who want to consider different kinds ofinjuries or deaths separately because, for instance, they believe that certain typesof individuals are worthy of special protection or that certain types of injuries orillnesses are especially to be avoided. some ways of characterizing risk take suchconcerns into account. these involve calculating separate risk estimates for eachhazardous effect, giving heavier weight to qualitative characteristics of risk (e.g.,fischhoff et al., 1984; okrent, 1980) and using explicit measures of values andrisk attitudes (raiffa, 1968).knowledge needed for risk decisionswhat kinds of knowledge must be collected so that the process ofcommunication will be an informed dialogue leading to reasonable choices?understanding the risks is not enough, because organizations and individualsnever choose between risks. rather, they choose between options, each of whichpresents some risks. each also presents benefits, which are as crucial to thechoices as the risks are. understanding risks can be difficult, but understandingthe benefits of a set of decision alternatives can be as difficult. both kinds ofknowledge are needed for an informed choice.this section outlines the many kinds of relevant knowledge. it summarizesfour kinds of knowledge decision makers need: (1) about risks and benefitsassociated with a particular option, (2) about alternative options and their risksand benefits, (3) about the uncertainty of the relevant information, and (4) aboutthe management situation.information about the nature of risks and benefitsﬁrisk assessmentﬂ is the term generally used to refer to the characterizationof the potential adverse effects of exposures to hazards. risk assessment thereforeaddresses the questions listed below. ﬁbenefit assessment,ﬂ a term not commonlyused, addresses many similar questions. some benefit questions are mentionedbelow, in parentheses.1. what are the hazards of concern as a consequence of a substance oractivity? what environments, species, individuals, or organ systems mightbe harmed? how serious is each potential consequence? is it reversible?(what are the benefits associated with a substance or activity? whobenefits and in what ways?)understanding hazards and risks33improving risk communicationcopyright national academy of sciences. all rights reserved.2. what is the probable exposure to each hazard in total number of people orvalued things? how do the exposures cumulate over time? a singleexposure over a short period of time can have effects different from thosedue to exposure to the same amount of a hazard in several episodes orchronically at low levels over a longer period of time. (how many peoplebenefit? how long do the benefits last?)3. what is the probability of each type of harm from a given exposure to eachhazard? how potent is the hazardous substance or activity at the relevantexposures? what is the relation of exposure or ﬁdoseﬂ to response? (whatis the probability that the projected benefits will actually follow from theactivity in question? what events might intervene to prevent those benefitsfrom being received? what are the probabilities of these events?)4. what is the distribution of exposure? in particular, which groups receive adisproportionate share of the exposure? (which groups get adisproportionate share of the benefits?)5. what are the sensitivities of different populations of individuals to eachhazard? what is the appropriate estimate of harm for highly sensitivepopulations that bear a significant proportion of the overall risk? what arethose populations, where are they located, and what proportion of the totalrisk do they bear?6. how do exposures interact with exposures to other hazards? sometimesone exposure can make people more sensitive to another hazardšasynergistic effectšand, occasionally, exposure to one hazard may decreasesensitivity to anotherša blocking effect. what is known about sucheffects?7. what are the qualities of the hazard? for instance, do those exposed havean option to reduce or eliminate their exposure (and at what cost)? wouldharm come to exposed people one at a time or as a mass, in a potentialcatastrophe? is the hazard deadly or not? does the harm take the form ofaccident or illness, acute or chronic disease, damage to the young or theold, to the living or the unborn? if the hazard is an illness, is it treatable? isit a dread illness, such as cancer, or one that creates less of an emotionalreaction? table 2.1 lists qualities of risk that make a difference in mostpeople's judgments. (what are the qualities of the benefits? do they appearas increased income, saved time, physical comfort, improved health, morestable ecosystems, more beautiful surroundings, improved welfare forlowincome people or the elderly, or in other forms?)8. what is the total population risk, taking into account all of the above? toarrive at such an estimate, one must somehow calcuunderstanding hazards and risks34improving risk communicationcopyright national academy of sciences. all rights reserved.table 2.1 qualitative factors affecting risk perception and evaluationfactorconditions associatedwith increased publicconcernconditions associatedwith decreased publicconcerncatastrophic potentialfamiliarityunderstandingcontrollability (personal)voluntariness of exposureeffects on childreneffects manifestationeffects on futuregenerationsvictim identitydreadtrust in institutionsmedia attentionaccident historyequitybenefitsreversibilityoriginfatalities and injuriesgrouped in time and spaceunfamiliarmechanisms or process notunderstooduncontrollableinvoluntarychildren specifically atriskdelayed effectsrisk to future generationsidentifiable victimseffects dreadedlack of trust in responsibleinstitutionsmuch media attentionmajor and sometimesminor accidentsinequitable distribution ofrisks and benefitsunclear benefitseffects irreversiblecaused by human actionsor failuresfatalities and injuriesscattered and randomfamiliarmechanisms or processunderstoodcontrollablevoluntarychildren not specificallyat riskimmediate effectsno risk to futuregenerationsstatistical victimseffects not dreadedtrust in responsibleinstitutionslittle media attentionno major or minoraccidentsequitable distribution ofrisks and benefitsclear benefitseffects reversiblecaused by acts of natureor godnote: in selecting risks to be compared, it is helpful to keep these distinctions in mind. riskcomparisons that ignore these distinctions (e.g., comparing voluntary to involuntary risks) arelikely to backfire unless appropriate qualifications are made.source: covello et al., 1988.late a summation across different types of harm, people of differentsensitivities, and exposures to the hazard in different amounts and incombination with various other hazards. (what is the total benefit?)information on alternativesthe term ﬁrisk control assessmentﬂ may be used to describe the activity ofcharacterizing alternative interventions to reduce orunderstanding hazards and risks35improving risk communicationcopyright national academy of sciences. all rights reserved.eliminate a hazard. more generally, decision makers need responses to questionssuch as the following about all the alternatives to any option under consideration:1. what are the alternatives that would prevent the hazard in question? someinvolve the choice of alternative processes or substances, while othersinvolve action that might prevent or reduce exposure, mitigate theconsequences, or compensate for damage.2. what are the risks of alternative actions and of a decision not to act? howare these risks distributed? since there are an infinite number ofalternatives, it is possible to assess only a few, but a complete analysisshould examine those alternatives being prominently discussed and shouldwork to identify others worthy of consideration. (what benefits does eachalternative promise, other than risk reduction?)3. what is the effectiveness of each alternative? that is, how much does itreduce the risks it is intended to reduce, and how is the risk reductiondistributed across relevant populations? (what benefits does each provide,and how are they distributed?)4. what are the costs of each alternative, and how are these distributed acrossrelevant populations?uncertainties in knowledge about risks and benefitsassessments of the risks and benefits of all available options, to becomplete, should address the following questions about their own reliability:1. what are the weaknesses of the available data? information needed toestimate the risks and benefits of an activity or substance and the effectsand costs of alternatives often does not exist. sometimes experts dispute theaccuracy or reliability of the data that are available. and often not enoughis known to extrapolate confidently from those data to estimates of risks (orbenefits) for a whole population.2. what are the assumptions and models on which the estimates are basedwhen data are missing or uncertain or when methods of estimation are indispute? how much dispute exists among experts about the choice ofassumptions and models?3. how sensitive are the estimates to changes in the assumptions or models?that is, how much would the estimate change if it used different plausibleassumptions about exposures or incidences of harm (or benefits) ordifferent methods for converting available data intounderstanding hazards and risks36improving risk communicationcopyright national academy of sciences. all rights reserved.estimates? what are the boundaries or confidence limits within which thecorrect risk (or benefit) estimate probably falls? what is the basis forconcluding that the correct estimate is not likely to lie outside thosebounds?4. how sensitive is the decision to changes in the estimates? that is, if,because of uncertainty, an estimate of risk or benefit were wrong by afactor of 2, or 10, or 100, would the decision maker's choice be different?5. what other risk and risk control assessments have been made, and why arethey different from those now being offered?information on managementﬁrisk managementﬂ is a term used to describe processes surroundingchoices about risky alternatives. in common usage, assessments of the risks andbenefits of various options are seen as technical activities that yield informationfor decision makers, whose decisions are called risk management decisions(national research council, 1983a). [if one accepts the distinction between riskassessment and risk management (see the list of terms in appendix e),communication about risks that involves nonexperts would generally be part ofrisk management.] in addition to information about risks and benefits, decisionmakers need answers to managerial questions such as these:1. who is responsible for the decision? who is responsible for preventing,mitigating, or compensating for damage? who is responsible for generatingand evaluating data? who has oversight?2. what issues have legal importance? do the applicable laws take benefitsinto consideration? do they allow consideration of the risks of alternatives?do they require the analysis of economic and social impacts of the activityin question or its alternatives?3. what constrains the decision? what technical, physical, biological, orfinancial limits constrain some possible choices? what are the limits ofauthority of the person or organization making the decision? are there timelimits imposed on the decision process? what difference could publicopinion or political intervention make?4. what resources are available for implementing the decision? whatpersonnel and financial resources are available to the decision maker? toothers involved in debating the decision?understanding hazards and risks37improving risk communicationcopyright national academy of sciences. all rights reserved.other relevant knowledgein addition to items on the above lists, other considerations are alsoimportant. technological choices involve risks and benefits not only to the life,health, and safety of individual humans but also to nonhuman organisms,ecological balances, the structures of human communities, political and religiousvalues, and other things that concern decision makers but that are not easilyevaluated by the quantitative approaches implied by the above lists. theassessment of such risks and benefits is not standard practice in the field of riskassessment. such factors are commonly discussed, however, in activities anddocuments described as ﬁimpact assessmentsﬂ or ﬁtechnology assessments.ﬂthese broadly conceived activities and documents often address a wide range ofthe questions just outlined.summaryin sum, a wellinformed choice about activities that present hazards andrisks requires a wide range of knowledge. it depends on understanding of thephysical, chemical, and biological mechanisms by which hazardous substancesand activities cause harm; on knowledge about exposures to hazards or, whereknowledge is incomplete, on analysis and modeling of exposures; on statisticalexpertise; on knowledge of the economic, social, esthetic, ecological, and othercosts and benefits of various options; on understanding of the social valuesreflected in differential reactions to the qualities of risks; on knowledge of theconstraints on and responsibilities of risk managers; and on the ability to integratethese disparate kinds of knowledge, data, and analysis. needless to say, it is oftenimpossible in practice to gather all this knowledge. nevertheless, the morecomplete the knowledge and the more quantitative answers are found, the betterinformed the ultimate decision will be.gaps and uncertainties in knowledgethe above summary of needed knowledge clearly suggests that decisionsabout risky activities and hazardous substances are frequently made withincomplete information. in this section we elaborate on some of the points justraised. we focus on risks, even though there are major gaps and uncertainties inknowledge about benefits as well, and we list several important ways thatinformation aboutunderstanding hazards and risks38improving risk communicationcopyright national academy of sciences. all rights reserved.figure 2.1 source: drawing by richter; ©1987 the new yorkermagazine, inc.the nature and magnitude of risk is often incomplete and uncertain (seefigure 2.1).identification of hazardsit is sometimes difficult even to determine whether a hazard exists. foractivities or substances whose hazards are delayed in onset (such as possiblecauses of cancer or birth defects) and for substances to which people are exposedin very small quantities, it is difficult to connect effects to causes. analysts oftenuse experiments with animals or bacteria to determine whether such activities orsubstances are hazardous under controlled conditions, but not all potentialhazards are studied, even in the laboratory. a national research council panelreviewed the testing that had been done on a random sample of 675 substances(national research council, 1984). within thisunderstanding hazards and risks39improving risk communicationcopyright national academy of sciences. all rights reserved.group, 75 percent of the drugs and inert chemicals in drug formulations had hadsome testing for acute toxicity and 62 percent had had some testing forsubchronic effects. for pesticides and ingredients in pesticide formulations, thesevalues were 59 percent and 51 percent, respectively. testing for chronic,mutagenic, or reproductive and developmental effects was less frequently donethan testing for acute and subchronic effects, and testing of all kinds was lessfrequently done for substances on the toxic substance control act's list ofchemicals in commerce. the panel concluded that toxicity studies had not yetbeen done on the majority of the chemicalsšamounting to tens of thousandsšnow in industrial use in the united states.even when studies have been done with lower organisms, it is uncertainwhether there is a human hazard. substances that cause cancer, mutations, or birthdefects in some species of animals often have no demonstrable effect on otherspecies, and the reasons for these differences are not yet understood. forinstance, a review by the food and drug administration indicated that of 38compounds demonstrated or suspected to cause birth defects in humans, allexcept one tested positive in at least one animal species and more than 80 percentwere positive in more than one species. eightyfive percent of the 38 compoundscaused birth defects in mice, 80 percent in rats, 60 percent in rabbits, 45 percentin hamsters, and 30 percent in primates (national research council, 1986b).thus some substances that do not cause cancer or birth defects in test speciesappear to have these harmful effects on humans. and the reverse may also betrue. scientists may agree that positive results in an animal test on a particularsubstance are strong evidence of a human hazard, but there is always someuncertainty about that judgment.estimation of exposuredata are frequently inadequate on exposures to hazards. many hazardoussubstances are diffused in the air or in surface or underground waterways and inthe process undergo physical or chemical changes that transform them into othersubstances that may be less hazardousšor that may be more so, although moredilute. many hazardous substances are transformed by biological processesbefore they reach humans. and even in the human body, metabolic processes canalter hazardous chemicals before they reach the organs to which they presenthazards, sometimes making them less toxic, but sometimes making them more so(national research council,understanding hazards and risks40improving risk communicationcopyright national academy of sciences. all rights reserved.1986b). thus the hazardous substances released into the environment at thesource may be very different in quantity and even in kind from those to whichpeople are ultimately exposed. the measurement of exposure is therefore mostaccurate at the dispersed sites where people live and work. as a result, it can bevery expensive to collect accurate exposure data. the problems and the expensemultiply when researchers try to address questions about unequal distributions ofexposure and about possibly sensitive populations. many more measurementsmust be made to compare the exposures of a variety of populations. for thesereasons exposures are usually estimated from data on releases of hazardoussubstances. inferring exposures requires numerous assumptions about thetransport, dispersion, and transformations of substances, many of which are basedon incomplete theory and limited evidence (national research council, 1988a).the use of estimates rather than measurements of exposure adds a layer ofuncertainty to risk estimates.further uncertainty is introduced by the fact that many hazards produce theireffects by exposure over time. it is known that exposure to radiation and somehazardous substances in a given amount will have different effects depending onwhether it occurs at once, is spread over several smaller exposures, or iscontinuous at a low rate over a long period of time (national research council,1988b). it is not known, however, how much difference this time dimensionmakes for particular hazards or which rate of exposure carries the greatest risk(national research council, 1984:60).estimation of the probability of harmknowledge about the probability of harm from a given hazard is alsofrequently inadequate or uncertain. the best way to estimate the probability ofharm is to examine the accumulated experience of people exposed to the hazard.only rarely, however, as with automobile travel and other familiar hazards whoseeffects are easy to observe, is there sufficient human experience to calculateaccurate probabilities from observational data. past experience does not exist formany controversial hazards because they involve new technologies. for manyothers, including carcinogens and most air pollutants, past experience is hard tointerpret because it is difficult to tell which illnesses or deaths are attributable tothe hazard rather than to other causes. for yet other hazards the meaning of pastexperience is in dispute because the greatest concern is about very low probabilityunderstanding hazards and risks41improving risk communicationcopyright national academy of sciences. all rights reserved.but potentially disastrous events, such as a nuclear reactor core meltdown or theescape of a virulent organism from a laboratory. the fact that a disaster has nothappened may mean that there is no potential for harm, that the potential is highbut luck has been good, or that the probability of harm is very low. but whenconsidering major disasters, even a very low probability can mean the risk to thepopulation, defined as the probability multiplied by the magnitude of theconsequence, is large.when knowledge from experience is unavailable or unreliable, analystsdevelop methods of estimating the risk. to assess the risk from carcinogens, theycommonly use data from laboratory experiments on nonhuman organisms.adding assumptions about how humans differ from the experimental organismsand about how to extrapolate from the 2year exposures to high doses usuallygiven to laboratory rodents to the longterm low doses characteristic of naturalhuman exposures, they estimate the human risk. an extensive literature debatesthe merits of different methods of making these extrapolations across species,dosages (national research council, 1980), and exposure times (kaufman,1988). risk analysts also use epidemiological studies that correlate evidence ofexposure and evidence of harm, but interpretations of these studies are oftencontroversial because they are open to alternative explanations. for instance,illnesses in exposed groups may be due to some other hazard to which they werealso exposed or to some synergistic interaction of hazards. only very infrequentlydo analysts have access to data on humans whose exposures to the relevanthazards are well known.a different sort of uncertainty arises in assessing the risk of disasters thatresult from the breakdown of complex technological systems, particularly typesof catastrophic accidents that have not previously occurred. risk analystssometimes address this problem with ﬁfaulttreeﬂ analysis, a technique that usesexperience to estimate the probabilities of various events that might contribute to adisaster and then combines the probabilities to estimate the likelihood that enoughcontributing factors will occur at once to trigger the disaster. the analysts thenuse available data and models to estimate potential exposures and theirconsequences. needless to say, these methods of estimation are full of untestedassumptions and uncertainties. in particular, an extensive literature debates theerrors of omission and commission in faulttree analyses of the probability oftechnological disasters, such as in the nuclear power industry (campbell and ott,1979; fischhoff et al, 1981a; mccormick, 1981).understanding hazards and risks42improving risk communicationcopyright national academy of sciences. all rights reserved.figure 2.2 source: drawing by richter; ©1988 the new yorkermagazine, inc.the uncertainties in these methods are legion, so several different and evenconflicting conclusions can often be defended by competent scientists. it isdifficult and sometimes proves impossible to reach a consensual judgment aboutwhat the probabilities are, let alone what to do about the attendant risks (seefigure 2.2).identification of synergistic effectsadditional uncertainty in risk estimates exists because exposure to onehazard can affect a person's sensitivity to another. for instance, asbestos isestimated to be about 10 times as dangerous to smokers as to nonsmokers(breslow et al., 1986). this may occur because chemical reactions between thesubstances yield products of different toxicity or because one substance increasesthe availability to the body of another one that would not have been toxic byitself (national research council, 1988a). in such ways, exposure to onesubstance can potentiate the adverse effects of another or, less commonly,decrease another substance's toxic effect. there is very little knowledge,however, about how frequent or how strong such synergistic or blocking effectsare or about which combinations of substances and activities are likely to exhibitthe effects. the knowledge that such effects exist, however, gives reason toconsider almostunderstanding hazards and risks43improving risk communicationcopyright national academy of sciences. all rights reserved.all estimates of health risk based on studies of single hazardous substances assomewhat uncertain, even when they are based on the most careful analysispossible.summaryin sum, any scientific risk estimate is likely to be based on incompleteknowledge combined with assumptions, each of which is a source of uncertaintythat limits the accuracy that should be ascribed to the estimate. does theexistence of multiple sources of uncertainty mean that the final estimate is thatmuch more uncertain, or can the different uncertainties be expected to cancel eachother out? the problem of how best to interpret multiple uncertainties is one moresource of uncertainty and disagreement about risk estimates.scientific judgment and errors in judgmentwhat do analysts do when confronted with knowledge so full ofuncertainties? scientists' training, which teaches them to accurately representcertain types of uncertainties, comes into conflict with the pressure to givesuccinct, unambiguous answers that can inform the social and personal decisionsnonexperts must make about risks. if the experts remain silent or equivocal,choices will be made without taking into account what they know. once theybegin to convey what they know, however, experts must inevitably makejudgments about the meaning of available information and about the degree towhich uncertainty makes it less reliable. but because experts rely on ordinarycognitive processes to make sense of the wealth of data they have available, theirjudgments about the meaning and conclusiveness of available information cansuffer from some of the same frailties that affect human cognition in general.inappropriate reliance on limited dataeven statistically sophisticated individuals often have poor intuitions abouthow many observations are necessary to support a reliable conclusion about aresearch hypothesis (tversky and kahneman, 1971). in particular, they tend todraw conclusions from small samples that are only justified with much largersamples. thus they may be prone to conclude that a phenomenon such as a toxiceffect does not exist when in fact the data are so sparse that the only appropriateconclusion is that the search for the phenomenon is in itsunderstanding hazards and risks44improving risk communicationcopyright national academy of sciences. all rights reserved.early stages. they may also err in the opposite direction, sounding an alarm onthe basis of extremely limited preliminary data. the tendency for scientists todraw conclusions from ﬁlowpowerﬂ research has been documented in fields frompsychology (cohen, 1962) to toxicology (page, 1981). lowpower research usesmeasurements and methods that are unlikely to reveal small effects without verylarge numbers of measurements. where the tendency to premature conclusionoperates, expert judgment will err by underreporting or overreporting effects,both hazardous and beneficial.tendency to impose order on random eventspeople who are seeking explanations for events, including experts workingin their areas of expertise, have a tendency to see meaning even when the eventsare random (kahneman and tversky, 1972). for instance, stock market analystsdevelop elaborate theories of market fluctuations, but their predictions rarely dobetter than the market average (dreman, 1979), and clinical psychologists seepatterns they expect to find even in randomly generated test data (o'leary et al.,1974). in interpreting statistics relating the incidence of cancer to occupationalexposures to particular chemicals, there is a temptation to interpret a correlationbetween exposure to a particular chemical and the incidence of a particularcancer as evidence of an effect. but some such evidence is to be expected even inrandom data, if large numbers of chemicals and cancers are examined. similarly,occasional ﬁcancer clustersﬂ are likely to be present in large epidemiologicalstudies even by chance. replication on a new sample is the best way to check thereliability of such relationships, but new samples are often hard to find.sometimes, conclusions are reported and publicized as definite before they havebeen adequately checked.such instances, including the interpretation of ﬁunusualﬂ cases, are at heartissues of the proper conduct of scientific analysis. although recent attention onscientific misconduct may attach greater significance to unusual cases than isactually warranted, it is nonetheless important to recognize the natural humantendency to find order even when the evidence is tenuous and to recognize thatwhen analysts are strongly motivated to find particular results they mayoverinterpret the evidence.understanding hazards and risks45improving risk communicationcopyright national academy of sciences. all rights reserved.tendency to fit ambiguous evidence into predispositionswhen faced with ambiguous or uncertain information, people have atendency to interpret it as confirming their preexisting beliefs; with new data theytend to accept information that confirms their beliefs but to question newinformation that conflicts with them (ross and anderson, 1982). because of thehigh degree of ambiguity in the data underlying risk assessments, this cognitivebias may act to perpetuate erroneous early impressions about risks even as newevidence makes them less tenable.tendency to systematically omit components of riskin analyses of complex technological systems, certain features arecommonly omitted, possibly because they are absent from operating theories ofhow the technological systems work. in particular, analysts are prone to overlookthe ways human errors or deliberate human interventions can affect technologicalsystems; the ways different parts of the system interact; the ways humanvigilance may flag when automatic safety measures are introduced; and thepossibility of ﬁcommonmode failures,ﬂ problems that simultaneously affect partsof the technological system that had been assumed to be independent [forelaboration and citations of the evidence, see fischhoff et al. (1981a)]. typically,people who were not involved in performing the analyses are unlikely to noticesuch omissionsšin fact, in a complex technical analysis, observers are likely tooverlook even major omissions in the analysis. although most of these oversightstend to lead to underestimates of overall risk, this need not always be the case.overconfidence in the reliability of analysesweather forecasters are remarkably accurate in judging their own forecasts.when they predict a 70 percent chance of rain, there is measurable precipitationjust about 70 percent of the time. they seem to be so successful because of thefollowing characteristics of their situation: (1) they make numerous forecasts ofthe same kind, (2) extensive statistical data are available on the averageprobability of the events they are estimating, (3) they receive computergeneratedpredictions for specific periods prior to making their forecasts, (4) a readilyverifiable criterion event allows for quick and unambiguous knowledge ofresults, and (5) their profession admitsunderstanding hazards and risks46improving risk communicationcopyright national academy of sciences. all rights reserved.its imprecision and the need for training (fischhoff, 1982; murphy and brown,1983; murphy and winkler, 1984). most of these conditions do not hold forprofessional risk assessors, however, and the predictable result is overconfidenceamong experts. for instance, civil engineers do not normally assess the likelihoodthat a completed dam will fail, even though about 1 in 300 does so when firstfilled with water (u.s. committee on government operations, 1978).2summarythese normal cognitive tendencies can lead expert risk analysts to conveyincorrect impressions of the nature and reliability of scientific knowledge. someof the tendencies predispose to premature judgment that a risk is low or high.several of them bias scientific judgment in the direction of overconfidence aboutthe certainty of whatever currently seems to be known. although the net effect ofthese cognitive tendencies has not been determined, their existence justifies acertain amount of skepticism on the part of decision makers, includingindividuals, about definitive claims made by risk analysts.influences of human values on knowledge aboutriskalthough it is useful conceptually to separate risk assessment and riskcontrol assessment from value judgment, there are many respects in which it isnot possible to accomplish the separation in practice. judgments made byscientists on which types of hazardous consequences to study and by analysts onwhich ones to measure are based in part on technical informationšwhatknowledge already exists, what additional knowledge would be relevant to adecision at hand, what the relative costs are of collecting different kinds of data,and what kinds of information would be most useful for estimating particularrisks. but they are also based on value judgments about which types of hazard aremost serious and therefore most worthy of being reduced. this section discussestwo of the ways that human values enter understanding of risks: through thechoice of numbers to summarize knowledge about the magnitude of risks andthrough the weighting of different attributes of hazards.understanding hazards and risks47improving risk communicationcopyright national academy of sciences. all rights reserved.choices of numerical measures for riskthe need to quantify risks as an aid to decision making creates specialdifficulties because the choice of which numerical measure to use depends onvalues and not only on science. this fact is evident even in a simple problem ofrisk measurementšthe choice of a number to summarize information onfatalities. different risk analysts have used different summary statistics torepresent the risk of death from an activity or technology.3 among the measuresused are the annual number of fatalities, deaths per person exposed or per unit oftime, reduction of life expectancy, and working days lost as a result of reducedlife expectancy. the choice of one measure or another can make a technologylook either more or less risky. for instance, in the period from 1950 to 1970, coalmines became much less risky in terms of deaths from accidents per ton of coal,but they became marginally riskier in terms of deaths from accidents peremployee (crouch and wilson, 1982). this is because with increasingmechanization fewer workers were required to produce the same amount of coal.so although there were fewer deaths per year in the industry, the risk to anindividual miner actually increased during this period. which measure is moreappropriate for decisions depends on one's point of view. as some observers haveargued, ﬁfrom a national point of view, given that a certain amount of coal has tobe obtained, deaths per million tons of coal is the more appropriate measure ofrisk, whereas from a labor leader's point of view, deaths per thousand personsemployed may be more relevantﬂ (crouch and wilson, 1982:13).each way of summarizing deaths embodies its own set of values. forexample, ﬁreduction in life expectancyﬂ treats deaths of young people as moreimportant than deaths of older people, who have less life expectancy to lose.simply counting fatalities treats deaths of the old and young as equivalent; it alsotreats as equivalent deaths that come immediately after mishaps and deaths thatfollow painful and debilitating disease or long periods during which many whowill not suffer disease live in daily fear of that outcome. using ﬁnumber ofdeathsﬂ as the summary indicator of risk implies that it is equally important toprevent deaths of people who engage in an activity by choice and deaths of thosewho bear its effects unwillingly. it also implies that it is equally important toprotect people who have been benefiting from a risky activity or technology andthose who get no benefit from it. one can easily imagine a range of argumentsunderstanding hazards and risks48improving risk communicationcopyright national academy of sciences. all rights reserved.to justify different kinds of unequal weightings for different kinds of deaths, butto arrive at any selection requires a value judgment concerning which deaths oneconsiders most undesirable. to treat the deaths as equal also involves a valuejudgment.there are additional value choices involved in calculations based onfatalities. a particularly controversial choice concerns whether to ﬁdiscountﬂlives, that is, whether to give deaths far into the future less weight than presentdeaths. this approach to valuation is sometimes advocated on the ground thatpeople typically prefer a given amount of any particular good in the present to thesame value in the futurešif they invested the cost of the good, they could expectto have increased purchasing power and thus to be able to purchase more of it inthe future than in the present. although one cannot ﬁinvestﬂ human life in thesame way, society can invest the resources used to save or prolong lives. from anindividual's point of view, one arguably loses less by dying at an old age thanwhen younger, so people may be less willing to work to avoid probable deaths thefarther they are in the future.discounting is controversial partly because it is used to put a monetary valueon human life. some measure, whether based on probable future earnings orconsumption or on willingness to pay to reduce the probability of fatality, isselected to put a price on what for many has intrinsic moral or even religiousvaluešand each of these measures embodies controversial assumptions aboutwhat is worthwhile about life. in addition, choosing a positive discount ratešonethat treats future lives as worth less than present livesš suggests that societycares less about its children's generation than its own, a controversial assumptionto say the least. but deciding not to discount lives also involves a judgment aboutthe future, and so it is also a valueladen choice (zeckhauser and shephard,1981).values also enter into scientists' choices about how to characterize theuncertainty in their information. it is traditional among civil engineers, publichealth professionals, and others to take account of uncertainty by beingﬁconservativeﬂ in stating risk estimates. this means that they leave a margin forerror that will protect the public if the actual risk turns out to be greater than thebest currently available estimate. but it has sometimes been argued that riskanalysts should instead present their best available estimate to decision makers,along with an explicit characterization of its uncertainty, and allow the decisionmakers to decide explicitly how much margin of safety to allow. the dispute ishighly controversial because manyunderstanding hazards and risks49improving risk communicationcopyright national academy of sciences. all rights reserved.figure 2.3 source: national wildlife magazine, augustseptember, 1984.copyright © 1984 mark taylor. reprinted with permission of mark taylor.believe that in practice the latter approach will provide a narrower margin ofsafety. the central point here is that either way of representing uncertaintyembodies a value choice about the best way to protect public health and safety.these few examples show how human values can enter into even apparentlytechnical decisions in risk analysis, such as about the choice of a number tosummarize a body of data. it is easy therefore to see how choices that are justifiedby appeal to data from a risk analysis can sometimes be questioned by appealingto the very same data (see figure 2.3).values and the attributes of hazardswe have noted that decision makers do not choose among risks but amongalternatives, each with many attributes, only some of which concern risk.similarly, each hazardšand, for that matter, each benefitšthat a decisionalternative presents has many attributes. these attributes are important tononexperts for the purpose of making decisions.understanding hazards and risks50improving risk communicationcopyright national academy of sciences. all rights reserved.qualitative aspects of hazards are relevant to decisions in various ways. indifferent decision contexts it may be necessary to consider comparisons andtradeoffs such as the following: is a risk of cancer worse than a risk of heartdisease? is an accidental death of a person at age 30 more to be avoided than adeath by emphysema at age 70? is an industrial hazard more acceptable if it isborne by workers partly compensated by their pay than if it is borne bynonworking neighbors of the industrial plant? are the deaths of 50 passengers inseparate automobile accidents equivalent to the deaths of 50 passengers in oneairplane crash? is a hazard that faces the unborn worse than a similar hazard thatwe face ourselves? is a large hazard with a low probability equally undesirable as asmall hazard with a high probability when the estimated risks are equal? thedifficult questions multiply when hazards other than to human health and safetyare considered. technological choices sometimes involve weighing the value of ariver vista, a smalltown style of living, a holy place, or the survival of anendangered species, in addition to dangers to human health, against probableeconomic benefits. such choices are ultimately matters of values and intereststhat cannot be resolved merely by determining what the risks and benefits are.a growing body of knowledge on what is usually called ﬁrisk perceptionﬂhelps illuminate the values involved in the evaluation of different qualities ofhazards.4 in studies of risk perception individuals are given the names oftechnologies, activities, or substances and asked to consider the risks each onepresents and to rate them, in comparison with either a standard reference or theother items on the list. the responses are then analyzed, taking into accountattributes of the hazards and benefits each technology, activity, or substancepresents (table 2.1 lists several such attributes). analysis consistently shows thatpeople's ratings are a function not only of average annual fatalities according tothe best available estimates, but also of the attributes of the hazards and benefitsassociated with a technology, activity, or substance (fischhoff et al., 1978; gouldet al., 1988; otway and von winterfeldt, 1982; slovic et al., 1979, 1980). inparticular, the studies show that certain attributes of hazards, such as the potentialto harm large numbers of people at once, personal uncontrollability, dreadedeffects, and perceived involuntariness of exposure, among others (see table 2.1),make those hazards more serious to the public than hazards that lack thoseattributes. also, choices that provide different types of benefit, such as money,security, and pleasure, are valued differently from each other (gould etunderstanding hazards and risks51improving risk communicationcopyright national academy of sciences. all rights reserved.al., 1988). the fact that hazards differ dramatically in their qualitative aspectshelps explain why certain technologies or activities, such as nuclear power, evokemuch more serious public opposition than others, such as motorcycle riding, thatcause many more fatalities.an important implication of such findings is that those quantitative riskanalyses that convert all types of human health hazard to a single metric carry animplicit valuebased assumption that all deaths or shortenings of life areequivalent in terms of the importance of avoiding them. the risk perceptionresearch shows not only that the equating of risks with different attributes is valueladen, but also that the values adopted by this practice differ from those held bymost people. for most people, deaths and injuries are not equalš some kinds orcircumstances of harm are more to be avoided than others. one need not concludethat quantitative risk analysis should weight the risks to conform to majorityvalues. but the research does suggest that it is presumptuous for technical expertsto act as if they know, without careful thought and analysis, the proper weights touse to equate one type of hazard with another. when lay and expert values differ,reducing different kinds of hazard to a common metric (such as number offatalities per year) and presenting comparisons only on that metric have greatpotential to produce misunderstanding and conflict and to engender mistrust ofexpertise.implications for risk communicationwe have shown in this chapter that different experts are likely to seetechnological choices in different, sometimes contradictory, ways even when theinformation is not at issue. incomplete and uncertain knowledge leavesconsiderable room for scientific disagreement. judgments about the sameevidence can vary, and both judgments and the underlying analyses can beinfluenced by the values held by researchers. since scientists and the people whoconvert scientific information into risk messages do not all share common values,it is reasonable to expect risk messages to conflict with each other. even in thebest of circumstances for communication, conflicting risk messages would createconfusion in the minds of nonexperts who must rely on them to inform theirchoices. but as the next chapter shows, the circumstances are not the best. thesocial conflict that surrounds modern technological choices is characterized byanxiety and mistrust and by clashes of vested interests and values, conditionsunderstanding hazards and risks52improving risk communicationcopyright national academy of sciences. all rights reserved.that create formidable tasks for those who would improve decision makingthrough risk communication.notes1. one technical definition of risk is that risk is the product of a measure of the size of the hazard andits probability of occurrence. regardless of how numerical estimates are made, the essence of thedistinction between hazard and risk is that ﬁriskﬂ takes probability explicitly into account.2. this discussion is drawn from fischhoff et al. (1981 a). more extensive discussions of expertoverconfidence with additional examples can be found there and in lichtenstein et al. (1982).3. this discussion is drawn from fischhoff et al. (1984:125œ126), where further citations can befound.4. the term ﬁrisk perceptionﬂ is put in quotation marks because, as the discussion shows, this body ofresearch is more accurately described as the study of human values regarding attributes of hazards(and benefits).understanding hazards and risks53improving risk communicationcopyright national academy of sciences. all rights reserved.3conflict about hazards and risksconflict within our society about technological choices, focusing on hazardsand risks, is an essential part of the environment in which those choices aredebated and made (e.g., dickson, 1984; lawless, 1977; mazur, 1981; nelkin,1979a).1 that is, conflict is an essential part of the environment of riskcommunication. this chapter discusses the reasons communication about hazardsand risks in the u.s. political system has become so contentious over the last twodecades. it identifies the major sources of this increasing conflict and brieflyexplores the nature of that conflict. risk communication is profoundly affected bythe conflictual atmosphere in which it occurs.is risk increasing or decreasing?for many observers the central dispute about technology and risk concernswhether risk is increasing or decreasing (e.g., national research council, 1982).in some accounts people are concerned about the risks of technology becausethere is an increasing threat of technological disaster; in other accounts, publicconcern flies in the face of a demonstrable decrease in net risk to human healthand survival. although we do not believe this debate to be productive for riskcommunication, a brief and simplified account of it will serve to introduce thediscussion that follows, concerning the sources of increasing conflict abouttechnological choices.conflict about hazards and risks54improving risk communicationcopyright national academy of sciences. all rights reserved.table 3.1 life expectancies in the united states, 1900œ1984white maleblack maleawhite femaleblack femalealife expectancy at birth1900œ190248.232.551.135.01949œ195166.358.972.062.7198471.865.678.773.7remaining life expectancy at age 251900œ190238.532.240.133.91949œ195144.939.549.842.4198448.743.155.050.7alife expectancy figures for 1949œ1951 are for nonwhites.source: metropolitan life insurance company statistical bulletin, 1987.it is the safest of timesproponents of the view that this is the safest of times2 point out that the bestoverall measure of health and safety risk is average life expectancy. they notethat during this century there have been dramatic increases in life expectancyeven as the society has increased its use of the chemicals and other hazardoussubstances that are the subject of intense debate about risk. the increases havebeen marked for women and men and for blacks and whites (see table 3.1).while much of the increased longevity is due to declining infant mortality and isprobably unrelated to environmental and occupational health hazards,improvements in life expectancy of young adults have also been striking. thusmedical science, improved nutrition, water purification, and other advances havecombined to give each person a good chance at living a full life span. the dataoffer no indication that epidemics of chemicalinduced cancer or othertechnologically borne scourges are increasing the risk of fatality.proponents of the view that risk is decreasing point out that many of thehazardous substances now in the environment decrease overall risk by replacingmore dangerous substances. for instance, chlorinated hydrocarbon solvents,which cause cancer in animals and possibly humans as well, have replacedflammable ones, which caused death by fire. many other hazardous substancesdecrease risk by reducing more serious preexisting hazards. pesticides andherbicidesconflict about hazards and risks55improving risk communicationcopyright national academy of sciences. all rights reserved.may cause cancer, but, in some parts of the world at least, they have helpedprevent famine. water chlorination increases exposure to carcinogens butdecreases exposure to typhoidcausing bacteria and other infectious agents.proponents of the view that technology improves safety conclude that manypeople are becoming more and more concerned about smaller and smaller risks.they see the gains from past technological change as outweighing the new risksby a large margin, and they see no reason the trend will not continue.it is the riskiest of timesproponents of the view that this is the riskiest of times see moderntechnology as generating new threats to society and the earth's lifesupportsystems and as doing so at an accelerating pace. they argue that because of thetechnological advances that have increased life spans, population growththreatens more devastating famines than the world has ever seen. they also notethat the longterm biological and ecological effects of rapid increases in the useof chemicals are still unknown. to illustrate the reason for concern, they notethat serious hazards continue to be discoveredša recent example is the hazard tothe earth's ozone layer from manufactured chlorofluorocarbons. they point outthat the synergistic effects of technological hazards remain almost entirelyunstudied even though people are rarely exposed to one hazard in isolation fromothers. they point to a range of global environmental threats whose ultimateimplications for humanity are unknown but potentially catastrophic: the rapid rateof extinction of species and the destruction of their habitats; deforestation anddecreases in biological diversity in the tropics; the possibility of major climaticchange due to human activity; and, of course, the possibility of nuclearholocaust.proponents of the view that technology is increasing risks do not seeadvances in life expectancy as a convincing counterargument. they point out thatmany of the new risks are unlikely to be reflected in current life expectancy databecause they are so far only evident in indicators of ecosystems and thegeosphere. they note that the new lowprobability catastrophic risks that theyconsider important cannot appear in life expectancy tables because thecatastrophes have not yet occurred. and they suggest that progress in raising lifeexpectancy, which has slowed since 1950, might have been greater if it had notbeen for the new risks. thus those who see risk asconflict about hazards and risks56improving risk communicationcopyright national academy of sciences. all rights reserved.increasing call for tighter control over technology, introduction of moreenvironmentally benign technology, and abandonment of some technologiesconsidered particularly risky.understanding the conflictalthough each of these views has some valid and convincing evidence on itsside, the dispute cannot be resolved by available evidence. in fact, it may notultimately be about evidence. at a deeper level it is about what kinds of riskspeople want most to avoid, what kinds of lives they want to lead, what theybelieve the future will bring, and what the proper relationship is betweenhumanity and nature. reviewing the evidence will not resolve the disputeš infact, debates over technology framed in this way seem only to increase anger andfrustration. but understanding the conflict may be a necessary first step towardimproving dialogue, that is, toward making better risk communication possible.to understand the conflict, it helps to begin by asking what has changed inthe relation of technology and society and what has not. as we noted in chapter 2,the existence of technological hazards is nothing new. whether such hazardspresent an increased net risk is, of course, a matter of dispute. there is littledoubt, however, that the extent and intensity of conflict about technologicalhazards have increased substantially over the past 30 years. this can be seen inthe pressures that culminated in a flurry of environmental legislation in the late1960s and the 1970s, in evidence of increasing public opposition to nuclearpower since the early 1970s (ahearne, 1987; freudenburg and rosa, 1984;hively, 1988), and in the continuing strong public support for environmentalregulation during the reagan years in the face of the administration'scommitment to deregulation (dunlap, 1987).3 the following sections elaborate onthe major factors contributing to intense conflict over technology and on thenature of that conflict.changes in the nature of hazards and inknowledge about themthe hazards recognized in modern living have changed in kind, regardlessof whether any particular type of risk has increased or decreased. in addition, newknowledge about hazards and risks has led people to think about them in newways. the important changesconflict about hazards and risks57improving risk communicationcopyright national academy of sciences. all rights reserved.described below give reason for a continuing high level of public concern(dunlap, 1987; mitchell, 1980).increased understanding of human influence on hazardsadvances in science and technology have made clear that humanity hasmuch more to do with its own health and longevity than was once believed. manyillnesses and deaths that were once seen as inevitable, random, or divinely causedare now known to have human origins. modern science can detect anthropogenictoxic substances at increasingly low concentrations and can trace their biologicaleffects with animal experiments and epidemiological studies. modern techniquesof detection and analysis can connect events over great distances and throughcomplex pathways, revealing the human causes of hazards.people are also increasingly aware that human action can avoid or reducerisks. individuals have learned that they can increase their life expectancies bywearing seat belts, avoiding tobacco use, and controlling their diets. governmentsand firms can reduce human health risks with pollution controls and improvedsafety measures in industrial processes and consumer products. and, of course,medical science continues to develop ways to prolong life. it is an irony ofprogress that each success in prolonging and enhancing human life bringsincreasing awareness that human actionšor inactionšcan also be responsiblefor death.awareness of the human influence over life and death makes technologicalchoices into moral issues. in most modern societies harm to a person readilybecomes a moral issue if a responsible party can be identified. thus people feelmorally obligated to donate blood or bone marrow when they are made tounderstand that their particular type is needed to prolong life (schwartz, 1977).similarly, people who believe industrial firms are responsible for some cancerstend to see them as morally obligated to ameliorate the harm (stern et al., 1986).from such moral feelings comes the widespread sentiment for usingextraordinary, risky, and expensive measures to prolong lives when nothing elseis likely to work. by the same reasoning, reports that the burning of coal in ohiois killing fish in new york and may be threatening human health can lead peopleto see the pollution of air as immoral.in the u.s. and other legal systems, awareness of human influence calls intoaction fundamental norms about responsibility, rights, andconflict about hazards and risks58improving risk communicationcopyright national academy of sciences. all rights reserved.due process. when people who are perceived to be innocent are put in jeopardy,discussions about intent, justice, blame, and punishment are almost inevitable.what is at issue is no longer only whether an activity makes people better orworse off but whether the changes are fair and whether the responsible agent hasthe right to affect other people's wellbeing.worsening worst casesmodern technology, by making it possible for humans to alter naturalprocesses at the level of the geosphere, has made possible disasters that could noteven be fantasized a few generations ago. already, deforestation is disruptinghuge ecosystems, and there is evidence that it, combined with the burning ofunprecedented quantities of fossil fuel, is altering the earth's temperature andthreatening to raise the level of the oceans and disrupt the patterns of temperatureand precipitation on which world agriculture depends. although deforestationleading to climatic disruption is not newšit is responsible for the present aridityin much of the middle east and chinašhuman alteration of climate has neverbefore been possible on a global scale. there is dispute over the probability of aclimatic catastrophe, but little dispute that global climatic changes of historicproportions are now possible as a result of human activity (jaeger, 1988).similarly, the threat to the earth's ozone layer suggests the possibility of humangenerated environmental damage on an unprecedented scale. and, of course, thepossibility of devastation of whole nations by nuclear weapons is unprecedented.most of the unprecedented catastrophes scientists have described have a verylow probability of occurrence, but because the outcomes are so undesirable therisks are worth considering carefully. however, the low probability makes themhard to analyze. an example is major disasters from nuclear power plantoperation. the industry is too young for the probability to be estimated accuratelyfrom experience; yet indirect methods of estimation are highly uncertain. thuspeople are left with huge disasters to contemplate but no reliable guidance abouthow seriously to take them.with worsening worst cases, it makes sense to pay attention to smaller andsmaller probabilities and to smaller differences between probability estimates.but most people have difficulty understanding very low probabilities (see, e.g.,fischhoff et al., 1981b). they tend to think in the categories of language (such asﬁnever,ﬂ ﬁrarely,ﬂconflict about hazards and risks59improving risk communicationcopyright national academy of sciences. all rights reserved.ﬁoccasionally,ﬂ ﬁoften,ﬂ and so forth) rather than along the continuousdimensions of mathematics (cf. starr and whipple, 1980). for very lowprobability events, nonexperts tend to use two categories, ﬁpossibleﬂ andﬁeffectively impossible.ﬂ thus the changes that have made nightmares intopossibilities may drastically alter many people's thinking by making a qualitativechangešby making them aware of a hazard where they had perceived none.people may pay more attention to the size of the consequences and ignore boththe magnitude and the uncertainty of very low probability estimates. the resultwould be a muchincreased concern about catastrophic risks and a correspondingincrease in opposition to technologies that pose them.unintended side effectstechnological activity has probably always had effects on people who werenot directly involved in it, but knowledge of the extent of such effects hasincreased dramatically in this century. technological changes are accelerating, asare the materials and energy transformations that can disturb preexisting physicaland biological systems and affect human wellbeing. although people havealways been exposed to the side effects of other people's activity, they are nowaware of being exposed to much more and at greater distances. there isincreasing evidence that technological activities can now affect people around theearth by altering air quality, exposing them to ultraviolet radiation, or changingclimate.when side effects spread more widely and when that change is recognized,collective action often follows. the risk bearers tend to take up common interestagainst the risk givers. and when the effects extend across the boundaries ofcommunities and then of nations, the conflicts of interest often enter formalpolitical and diplomatic arenas or, if those are not available, find informal waysof gaining wide attention. thus increasing technological conflict is due in part tothe widening range of technology's effects and the greater social awareness of thechange.changing portfolio of hazardsthe hazards society confronts today are different from those of the past. asnoted in chapter 2, the principal threats to health, especially among the moreeducated and politically active segmentsconflict about hazards and risks60improving risk communicationcopyright national academy of sciences. all rights reserved.of the public, are now from chronic diseases rather than acute illnesses and fromillnesses now known to have long latency periods. sometimes decades passbetween exposure and effect; sometimes the effect manifests itself only in latergenerations. whereas infectious diseases can be convincingly linked tomicroorganisms in the body, cancer and many other chronic diseases cannot, ingeneral, be conclusively linked to causative agents.4 people are often unsure whatcaused such illnesses. moreover, if they are exposed to a hazard, they cannotknow whether they will become ill. people spend more of their lives under acloud: whenever they are exposed to a ﬁprobable carcinogenﬂ or other hazardwith delayed potential effects, they may worry about whether it will eventuallyharm them. if they become ill, they can consider a range of hypotheses abouthuman actions that might have been to blame: past occupational exposure, dietarypractice, air pollution, and so forth. some people agonize over whether they areguilty of causing their own illness; others conclude that they are innocent victimsof greed or negligence. the former conclusion produces anxiety; the latter,whether correct or not in any particular instance, motivates lawsuits and otherforms of social conflict.hazards have also changed in that there is more knowledgešand morewidespread awarenessšof hazards to which people are exposed but over whichthey have no control as individuals. individuals on their own are helpless toreduce the risks of nuclear war, depletion of the ozone layer, and global climaticchange. media accounts make people acutely aware of other hazards that strikemore or less at random, such as airplane hijackings and releases of toxicsubstances such as at bhopal or radioactivity such as at chernobyl. people havelearned that some industrial chemicals are toxic but that for many chemicals nowwidely used in commerce in the united states little is known about whether theythreaten human health (national research council, 1984). the anxiety thatcomes from awareness of apparently uncontrollable risks derives in large partfrom a sense of uncertainty. people may get the sense that past experiencešincluding longevity tablesšmay not provide a reliable estimate of the risks theyface.for highly uncertain risks it is difficult to refute extreme estimates of theirmagnitude. concerns may persist precisely because of the uncertainty. anexample is the concern that aids may be transmitted by mosquitoes. whiletechnical experts agree that mosquito transmission is too improbable to worryabout, a skeptic can maintain that it has not been proven impossible.additionally, highlyconflict about hazards and risks61improving risk communicationcopyright national academy of sciences. all rights reserved.uncertain risks generate special conflicts about their management, with decisionmakers disagreeing widely about how large a margin of safety should be allowedto protect against the occurrence of disastrous consequences that they agree areunlikely.changes in u.s. societytechnological decisions have become more controversial in part becauseu.s. society has changed in several ways in the era since world war ii.increasing affluencefor most of those who participate actively in american politics, economicsecurity has allowed certain basic human concerns to recede from awareness andto be replaced by other more indirect threats to personal wellbeing, includingconcerns about technology and risk. more and more people have attained a levelof economic security that allows them to take up concerns beyond those offeeding and housing themselves and their families, securing basic health care, andproviding for these security needs for their old age. and, regardless ofsocioeconomic level, people whose chief personal values extend beyond personalsecurity are more likely to be concerned with environmental problems than theaverage citizen (dunlap et al., 1983; inglehart, 1977). thus it is not surprisingthat affluence has brought increasing concern about the risks of technology.increasing dependence of the economy on technologythe u.s. and world economies have come to depend increasingly onadvanced technology for the production of food (petrochemicals); health care(drugs and other medical technologies), communication (computers andinformation transmission technology), transportation (jet aircraft), manufacturedgoods (automation and electric power technologies), and, of course, militarysecurity. such technologies have increasingly been controlled by large, politicallyand economically powerful organizations with vested interests in discovering,developing, and implementing them. they are also supported by individuals whobenefit from them economically or in other ways. the new technologies offergreat benefits to their sponsors in money or political power and potential benefitsand risks to society that may also be largešbut poorly understood. thesponsoring organizationsconflict about hazards and risks62improving risk communicationcopyright national academy of sciences. all rights reserved.need public acquiescence to achieve their technological aims, but for the reasonsdiscussed below that acquiescence has become more difficult to achieve. at thesame time proposals to restrict technologies typically meet intense oppositionfrom powerful proponents.distrust of institutionspublic opinion polling data indicate that there has been a ﬁsharp decline ofpublic faith in government, business, and labor since the mid1960sﬂ (lipset andschneider, 1987:40). the decline was especially rapid between 1964 and 1975.other polls have shown similar results, but the decline has been partially reversedmore recently (lipset and schneider, 1987). the decline in trust in majorinstitutions was in sharp contrast to the especially low level of criticism, distrust,and rebellion in the 1950s (schudson, 1978). it was, no doubt, influenced by aseries of formative political events of the 1960s and early 1970s. the civil rightsmovement, the war in vietnam and the protest against it, the assassinations ofthree major national leaders, and, finally, the watergate scandal all forcedattentive people to look at the dark side of our national character and nationalinstitutions.5 a climate developed in which major decisions by government andindustry, including decisions about technology, were increasingly open toquestion.the environmental movementa social movement concerned with environmental protection developed inthe 1960s in the united states and has since become a regular participant intechnological debates. influenced by new scientific knowledge conveyed inworks like silent spring (carson, 1962), large numbers of ordinary people sawfor the first time that their personal interests or values were affected by the waysociety used and regulated technology. they expressed their concerns throughenvironmental and related organizations and by direct pressure on governmentfor action. although environmental organizations were not new on the americanscene, those that had existed before the 1960s, such as the audubon society, thenature conservancy, and the sierra club, had focused mainly on the conservationof wildlife and wilderness. the new organizations, and to some extent the oldones through changes in their political agendas, advanced a new brand ofenvironmentalism concerned with threats to ecosystems andconflict about hazards and risks63improving risk communicationcopyright national academy of sciences. all rights reserved.global and regional lifesupport systems and with the protection of people fromtechnologically based threats to health and wellbeing (hays, 1987). the newenvironmental organizations and their political allies gained widespread publicsupport and raised funds to lobby, to conduct independent scientific analyses oftechnological issues, to participate in regulatory decision processes on matters ofconcern to their supporters, and to challenge government and corporate decisionsin court. they have became an institutional presence in opposition to a range ofefforts by industry and government to implement controversial new technologiesand to further spread existing ones.6new public institutionsduring the 1960s and 1970s national institutions were being restructured topay more attention to social goals, including improved management of societallyshared risks. beginning with passage of the national environmental protectionact in 1969, several new government bodies, such as the u.s. environmentalprotection agency (1970), the occupational safety and health administration(1970), the consumer product safety commission (1972), the nuclearregulatory commission (1975), the office of technology assessment (1972),and the office of disease prevention and health promotion (1984), were createdto promote and protect public safety and health in specific areas of risk. courtsbegan to require that medical professionals provide patients with betterinformation to guide their decisions about their treatment, and formal proceduresfor ﬁinformed consentﬂ came into being (applebaum et al., 1987; faden andbeauchamp, 1986). federal agencies, for their part, began to make moreinformation about risk available to the public, for instance by requiringrecordkeeping of the life histories of toxic substances. these changes created newpublic institutions whose purpose was to make technological decisions in thepublic arena and that resulted in new settings for conflict.politicization of the technological debatethe above changes in risks, knowledge, and society have contributed to theincreasing conflict about technology in recent decades. the benefits oftechnology have increased, but many people believe the risks have as well. thehazards confront more people than everconflict about hazards and risks64improving risk communicationcopyright national academy of sciences. all rights reserved.before (even if the risks may be less), and they have gained the attention of awider range of political actors. the attendant choices have huge potential effectson the distribution of wealth, health, and even political power in society. it is nowonder, then, that technological choices have come to concern more people andthat the nature of those choices has come to be seen in a different light. astraditional political issues such as public health, social equity, and due processbecame more prominent in technological decision making, decisions that hadbeen treated as essentially technical and economic, to be decided by executives offirms and government agencies with the advice of experts, came to be seen as alsobeing essentially political (dietz et al., 1989). the trend toward publicinvolvement can be seen in a recent expansion of ﬁrighttoknowﬂ legislation, theeffect of which is to disseminate information that citizens can use to heightentheir political involvement. the redefinition of environmental problems aspolitical is evident in a number of changes in the political system, as describedbelow.concepts of regulationchanges in federal law in the mid1960s transformed the judicial concept ofpublic interest as used in administrative law in regard to regulatory agencies.regulatory proceedings were opened to more than just the parties who sufferdirect legal injury from government action (office of communication of theunited church of christ v. federal communications commission, 1966; scenichudson preservation conference v. federal power commission, 1965). the newdeal notion of a regulatory agency as the embodiment of the public interest gaveway to a concept of the regulatory agency as a political, quasilegislative forumfor the meeting of competing interests (ackerman and hassler, 1977). it is nowonder, then, that the epa faced a rapid rise in the number of civil lawsuitschallenging its regulations, from under 20 in 1973 to nearly 500 in 1978 (o'brienand marchand, 1982:80).tort lawtort law has changed, broadening the ability of different kinds of people andgroups to bring legal action and creating new ways for plaintiffs to suesuccessfully even when there are formidable difficulties involved in determiningwho is responsible for an injury to theconflict about hazards and risks65improving risk communicationcopyright national academy of sciences. all rights reserved.plaintiff. in the past 30 years privatelaw adjudication has moved away fromcaveat emptor and related rules to permit greater access to the judicial arena andto apply more flexible doctrines regarding compensation for environmentallycaused damages to health and safety (o'brien and marchand, 1982). in thecalifornia supreme court decision in the case of sindell v. abbott laboratories,for instance (a decision the u.s. supreme court let stand in 1980), the courtallowed mothers whose children had suffered injury because of the mother's useof diethylstilbestrol (des) to recover damages without being able to identify aparticular manufacturer as responsible for the injury. the plaintiffs were allowedto recover by suing those manufacturers who collectively represented a majorshare of the market for the product that caused the injury (o'brien and marchand,1982).regulatory proceduresregulatory rule making over the past two decades has evolved a set ofprocedures that guarantees a variety of interested parties the opportunity tocomment on proposed rules and that makes it increasingly likely that regulatorswill have to address those comments as they justify their decisions (schmandt,1984). federal agencies are required by the courts to prepare detailed scientificanalyses in support of regulatory actions. these changes occurred in response toincreasing conflict about risk and created a channel for the expression ofopposition to government agencies' positions. they imposed some limits on whatopponents could legitimately raise as objections, but at the same time the newprocedures gave the opponents predictable access to the decision process and newopportunities to challenge decisions in court.politically potent symbolic eventsa number of incidents have received widespread attention and have becomecognitive markers of danger for many people. just as ﬁwatergateﬂ is synonymousfor many with governmental malfeasance, so ﬁthree mile islandﬂ has come torepresent the dangers of high technology. ﬁbhopal,ﬂ ﬁchernobyl,ﬂ and ﬁlovecanalﬂ are other such symbols. these reach out beyond the immediate mediacoverage they receive to become part of the cultural consciousness of manypeople, even those who know little of or paid little attention toconflict about hazards and risks66improving risk communicationcopyright national academy of sciences. all rights reserved.the original incidents (slovic, 1987). as a result, the mere mention of theseincidents can be a trigger for argument.increased focus on science in technological debatesthe laws and procedures that control governmental decisions abouttechnology in the united states have come increasingly to demand scientific andtechnical knowledge. some regulations require government to determine whethera particular risk exists and to act accordingly; others require a determination ofthe ﬁbest available technologyﬂ; and others explicitly require a weighing of costsand benefits. the national environmental policy act requires the preparation ofcareful assessments of the environmental and socioeconomic impacts of majortechnological choices. all these developments put science and scientificdisagreements at the center of technological debates. because of the difficulty, asdiscussed in chapter 2, of gathering and interpreting all the scientific knowledgerelevant to modern technological decisions, there is considerable room forscientists to disagree. when a decision that may have major political effects byaltering the distribution of money, power, and wellbeing in society is madethrough procedures that emphasize scientific judgment, scientific disagreementstend to become proxies for political disagreements, and political adversaries oftenexpress their positions in the language of science (dickson, 1984; mazur, 1981;nelkin, 1979a). in this way the inherent difficulty of understanding technologicalchoices combines with the political importance of their effects to multiply theintensity of conflict.institutionalization of scientific conflictpartly because regulatory decisions now rely so heavily on the evaluation ofscientific knowledge, divisions in the scientific community have becomeincreasingly public. conflicts that might once have been contained withinprofessional societies now appear occasionally as frontpage news. someenvironmental organizations and groups of scientists, such as the federation ofamerican scientists, whose members share common concerns aboutcontroversial technologies, have built scientific resources that allow them toadvocate political choices in the technical language of risk and benefit analysisthat statutes and regulatory procedures often require. not to be outdone,industrybased groups have increased their capability to doconflict about hazards and risks67improving risk communicationcopyright national academy of sciences. all rights reserved.ﬁregulatory scienceﬂ in support of their positions on the same issues. thusdisagreements between scientists have gained an institutional place in thepolitical debate, with scientists whose analyses support particular positionspresenting their judgments on behalf of groups advocating those positions(schmandt, 1984).implications of conflict for communicationthe above discussion makes clear that many factors have contributed toincreasing social conflict over hazards and risks. the conflict itself is amultifaceted one. a review of the environmental policy literature has identifiedfour distinct aspects of risk conflicts, as described below. according to a recentsurvey of scientists, lawyers, and others whose careers are largely devoted tothinking, researching, and debating about technological choices, each of these is amajor source of controversy about environmental risk (dietz and rycroft, 1987;dietz et al., 1988).7 this section distinguishes these four aspects of technologicalconflict and discusses the implications of each for risk communication.differential knowledgeone source of conflict about risk is that experts and nonexperts knowdifferent things about the risks and benefits of technology. in particular, technicalexperts have specialized knowledge about the nature of both the hazards and theirbenefits that nonexperts, lacking this knowledge, may dispute. conversely,nonexperts sometimes have local knowledge about exposures or the practicaloperation of a hazardous activity that technical experts do not share. whenconflict arises mainly from differential knowledge, risk messages focused oninformation, which promote the sharing of knowledge, can improve the riskcommunication process. this realization underlies proposals to design messagesthat would explain to nonexperts in a clear and simple format what scientists andtechnologists know about particular risks. it also provides justification for the flowof informational messages from nonexperts to experts. in conflicts that arise fromdifferential knowledge, better sharing of knowledge may also help reduce theconflict. however, when a conflict is in large part based on other factors, sharingof knowledge may not resolve it. it may even adversely affect the riskcommunication process if it is perceived as a diversion from the real issues.conflict about hazards and risks68improving risk communicationcopyright national academy of sciences. all rights reserved.a second aspect of differential knowledge and conflict is the differences inthe degree of understanding in various groups typically involved in risk issues.information simply made available to the public through the mass media andother channels is typically taken up more readily by those with high, rather thanlow, socioeconomic status because the former usually have a higher level ofeducation, enabling them to understand technical material more easily. this leadsto what is called a knowledge gap. but the presence of a conflict can change thissituation. in certain circumstances the presence of conflict might be seen aspositive because it effectively increases the number of people who becomeinformed about the issues involved.vested intereststhose who bear the risks of a technology are not always the same peoplewho gain the benefits, and, when the risks and benefits are distributed in unequalproportion, those holding different interests come into conflict. this kind ofconflict is most clearly evident in decisions about the siting of locally unwantedfacilities such as hazardous waste sites, power lines, and radioactive wasterepositories, but it is characteristic of other conflicts about risk as well. when aconflict is based in large part on vested interest, risk messages can be helpful ifthey clarify what different groups' interests are and describe how the availableoptions would affect each of those interests. such messages improve riskcommunication by providing information relevant to the choices at hand. but theyoften do not resolve conflict. even messages that simply describe scientificinformation can exacerbate conflict if the information helps clarify who stands towin or lose.value differencesdifferences in values also underlie conflict about risk. for instance, somepeople may believe that a potential catastrophe should be avoided by not adopting atechnology that might produce it, while others may believe that potentialproblems could be solved after the technology is implemented but before theproblems become too serious. in tradeoffs between economic growth and threatsto health and to esthetic, ecological, or community values, political participantswho expect the same outcome may still disagree with each other because whatthey may gain or lose does not have theconflict about hazards and risks69improving risk communicationcopyright national academy of sciences. all rights reserved.same value to each of them. the source of such disputes may lie in people'srelative preferences for values (e.g., money versus beauty), their beliefs insociety's ability to control technologies once introduced, or their predispositionsabout how much risk to take under conditions of uncertainty. when a conflict isbased in large part on differences in values, the following types of messages canmake risk communication more successful: statements identifying the values atstake, arguments about which values deserve the most weight, and analyses ofhow each available option would affect different values. as with conflicts basedon different interests, messages that improve knowledge relevant to the choicesat hand and that therefore raise the quality of risk communication can at the sametime make the conflict more intense. even messages describing scientific analysiscan have this effect, by clarifying which values an alternative would advance orimpede.mistrust of expert knowledge as interest servingpublic mistrust of information from government and industry sources alsounderlies conflict about technology. many people are aware that experts can befound who will support nearly any position in a technological debate. theyrealize that industry groups tend to produce only those scientific arguments thatadvance their goals and that environmental groups do the same. they know thateven the federal government has been subject to strong accusations that itsscientific analyses have been influenced by political pressure from variousinterest groups (e.g., nelkin and brown, 1984; smith, 1983). thus the statementsof scientific experts in risk debates are seen by the skeptical parts of the public asreflecting political positions rather than unbiased assessments. particular types ofmessages cannot by themselves alleviate mistrust, although altered proceduresfor the design of risk messages may help (see chapters 6 and 7). rather, theeffect of mistrust is to make communication more difficult in all contexts.note for risk message designersin most risk debates some participants are concerned with narrower issues ofrisk analysis, some with interests, some with value questions, and some withissues of trust. for this reason, different participants want to send and receivedifferent kinds of risk messages, and the risk communication process includes thefull range ofconflict about hazards and risks70improving risk communicationcopyright national academy of sciences. all rights reserved.types of messages mentioned herešscientific analyses, expressions of interestand value, and arguments about which values to favor. the designers of riskmessages need to be aware that a program of messages that addresses one sourceof conflict may fail to address other sources. thus someone who designs amessage to eliminate differential knowledge may find an audience concernedwith interests or values or one that mistrusts the message sourcešand themessage may not have the desired effect. such a message may even intensifyconflict because the audience sees it as irrelevant or as a diversion from what itconsiders to be the main issue.risk communication is difficult in part because risk messages often seem tooperate at crosspurposes. the next chapter distinguishes the major settings ofrisk communication and the major purposes for risk messages. it explores theissue of what techniques are appropriate for risk messages, particularly when thepurpose is to influence the recipients' beliefs or actions.notes1. conflict also occurs about the benefits of technological choices. this chapter discusses the risksbecause they have usually been the focus of the most intense conflict.2. the headings ﬁit is the safest of timesﬂ and ﬁit is the riskiest of timesﬂ are quoted from dentonmorrison's paper, ﬁa tale of two toxicitiesﬂ (1987).3. although public support for increased environmental regulation is strong, as evidenced by directquestions on opinion surveys, environmental problems are not usually mentioned with great frequencyin response to openended questions such as, ﬁwhat are the three most important problems facing thenation?ﬂ4. some types of cancer are clearly linked to chemical exposures: mesothelioma and asbestos, vaginalcancer and diethylstilbestrol (des), bladder cancer and benzidine dyes. in these situations theinference about possible causal agents involves assessment of statistical evidence (e.g.,epidemiological studies) and biological evidence on the plausibility of the linkage between agent anddisease [e.g., gasoline vapors cause kidney tumors in male rats, but the mechanism is not believedapplicable to human kidney cancer (epa science advisory board, 1988)].5. research on the ways social movements mobilize citizens' attention and participation has recentlybeen reviewed by cohen (1985) and jenkins (1983).6. recent studies on the growth of the environmental movement include those by hays (1987),milbrath (1984), and touraine et al. (1983).7. that is, each of these four aspects of conflict was rated as a major source of controversy aboutenvironmental risk by a majority of the ﬁrisk professionalsﬂ in the survey sample.conflict about hazards and risks71improving risk communicationcopyright national academy of sciences. all rights reserved.4purposes of risk communication and riskmessagesin this chapter we distinguish two types of settingsšpublic debate andpersonal actionšin which risk decisions and risk communication occur, and weshow how the risk communication process and its participants vary in thesesettings. we then discuss two distinct purposes of risk messagesšinforming andinfluencingšthat coexist in risk communication, sometimes even in a single riskmessage. finally, we address the thorny ethical problem of the appropriateness ofinfluencing as a purpose of risk messages, particularly messages that publicagencies distribute to citizens.settings of risk communicationpublic debatein a setting of public debatešsuch as congressional hearings, congressionaldebates, formal regulatory adjudication, and noticeandcomment rule makingšdemocratic risk communication includes a wide range of messages, sources, andaudiences. interested groups raise questions for the experts, who respond; expertsfrom different perspectives dispute with each other; and citizens and theirrepresentatives dispute using, among other things, the experts' findings andcriticisms of each other's results. messages describing and summarizing scientificknowledge about risks and benefits are important, as arepurposes of risk communication and risk messages72improving risk communicationcopyright national academy of sciences. all rights reserved.critiques of those messages and that knowledge. in the united states, regulatorydecisions must generally be based on the best available scientific knowledge to bedefensible against legal challenges. as a result, much risk communication in theregulatory context deals with the adequacy and proper interpretation of scientificevidence. but risk communication also includes expressions of opinion, concern,frustration, and the like by all participants, directed at whomever will hear andmight act. such decision making tends to be adversarial, with political actorsmaking the strongest possible case for their positions, overtly expressing theirinterests and values or citing expert judgment and analysis depending on whicharguments seem most effective. recipients of risk messages understand that thosemessages are guided by interests and political positions and so do not expect anysingle source to offer an unbiased assessment of available scientific knowledge.public policy about tobacco smoking illustrates the range of risk messagesthat come out of public debate. the policy options for risk management involvedecisions to be made in different bodies, each using different rules of debate andassigning different roles to the general public within those rules. for instance, thefederal government has considered increasing excise taxes on cigarettes, placingwarning labels on cigarette packages, funding antismoking advertisingcampaigns, distributing informational pamphlets on the health hazards ofsmoking, and banning smoking in various public places. other options that mightbe considered for cigarettes, and that have been used for other health hazards,include outright prohibition on manufacture or sale and restriction to use byprescription only. in state and local governments, debates have also proceeded onoptions such as banning cigarette advertisements in some public places, raisingthe minimum age for purchasing tobacco products, banning smoking inmunicipal buildings, and requiring nosmoking sections in restaurants.risk communication varies from one of these decisionmaking arenas toanother. citizens participate in legislative settings by attempting to influencetheir representatives directly or by affecting the general climate of opinion andthus achieving indirect influence. in federal regulatory decision making, there isalso wide latitude for participation, although the administrative procedures actand agencies' practices constrain the time and type of participation and the kindsof arguments that can be introduced (greenwood, 1984). agency proceduresdiffer, particularly in terms of how much twowaypurposes of risk communication and risk messages73improving risk communicationcopyright national academy of sciences. all rights reserved.communication they allow and how much they do to provide expert knowledge tothe citizenry at large. nevertheless, public debate in the regulatory or legislativecontext allows for risk messages and other related messages from a large numberof sources.we consider risk communication in a setting of public debate successful tothe extent that it raises the level of understanding of relevant issues or actionsamong the affected and interested parties and those involved are satisfied thatthey are adequately informed within the limits of available knowledge. as notedin chapter 1, successful risk communication does not imply optimal riskdecisions; it only ensures that the decisions are informed by the best availableknowledge. also as noted in chapter 1, raising the level of understandingrequires more than making accurate information accessible to the interestedparties. success requires increased understanding of the issues to the extent thatthe parties involved desire to understand. although individual risk messages maycontribute to increased understanding, the net effect of risk communication onunderstanding depends on all the messages individuals receive and theirinterpretation of them. therefore, the designers of risk messages who wish toincrease the recipients' understanding need to take into account the recipients'willingness and ability to receive and understand the messages as well as theeffects of other, sometimes conflicting, messages that they may also receive.success for risk communication does not require that every citizen beinformed about the risks presented in every regulatory decision, but people needto be confident that some person or group that shares their interests and values iswell informed and is representing those positions competently in the politicalsystem. public debate, in a traditional view in the united states, implies apluralism of constituencies, with ﬁconsent of the governedﬂ consisting of trustthat the relevant views are represented, that the procedures do not disadvantageimportant constituencies, and that the people are able to hold public officialsaccountable for their actions.the requirement that interested parties believe they are adequately informedis worth explanation. it stems from recognition that in several arenas of publicdebate risk decisions are intensely controversial and many message sources arewidely mistrusted. this situation imposes requirements, particularly on thosemessage sources and in those policy arenas, that may seem unfair to officials whopurposes of risk communication and risk messages74improving risk communicationcopyright national academy of sciences. all rights reserved.believe their responsibility to the public extends only to making wise decisionsand providing complete, accurate information. but if a message source is widelymistrusted, its messages will be rejected by many regardless of completeness oraccuracy. if accurate information is rejected by recipients, it does nothing toincrease their knowledge basešhence the requirement that recipients ofinformation for public debate be satisfied that they are adequately informed.both of the abovementioned requirements for successful riskcommunication were factors in the public debate that resulted in the successfulsiting of the ecoflo hazardous waste facility in greensboro, north carolina.this siting case also illustrates an instance in which understandable and sensitivemessages from an individual risk communicator (ecoflo) contributed to thesuccess of the overall risk communication process involving the guilford countyhazardous waste task force, environmentalists, and other concerned citizens(see accompanying story, pages 76œ77). nevertheless, it should be emphasizedthat open and free communication will not necessarily ease conflicts in allsituations.with respect to a designated decision maker, such as the head of aregulatory agency, risk communication is successful only if it adequately informsthe decision maker. a decision maker is adequately informed within the limits ofavailable knowledge if provision of all remaining available information wouldadd nothing to justify a modification of his or her choice. decision makers needto be informed about the managerial and political aspects of the choice at hand aswell as about the state of technical knowledge. and, as already noted, therelevant knowledge should be understood by the decision maker, not merelymade accessible.it is important to emphasize that a successful risk communication process isdifferent from a risk message that is successful from the standpoint of its source.in a public debate (like that in the ecoflo case), participants produce riskmessages aimed at changing minds and influencing political outcomes. fromtheir perspective a risk message is successful to the extent that it contributes tothe outcomes its sponsor desires. sometimes a risk communicator will make falseor deceptive statements or will withhold pertinent information to achieve apolitical effect. such activities, if they are not revealed, may achieve the ends ofthe message source but not the social goal of an adequately informed debate.purposes of risk communication and risk messages75improving risk communicationcopyright national academy of sciences. all rights reserved.ecoflo hazardous waste facility sitinggreensboro, north carolinathe successful siting of the ecoflo hazardous waste facility ingreensboro, north carolina, in 1985 is an example of good riskcommunication and effective risk messages. although representing asituation somewhat less problematic than those encountered elsewherešthe company proposed a treatment facility to reduce the overall amount oftoxic material in that localešit does illustrate the role of communicationefforts in the siting of a hazardous waste facility. the siting of such plants isnotoriously difficult. as a result of ecoflo's efforts, however, the finalpublic hearing to site the facility lasted only 15 minutes and led to thepermitting of the plant with the blessing of local government officials andenvironmentalists (lynn, 1987).ecoflo began operation in greensboro in september 1983 with alicense as a waste transporter. it worked mainly with small companies thatproduced about 20 drums of waste a month. although ecoflo was a newcompany, its owners had previously worked for other hazardous wastecompanies. in july 1984, ecoflo submitted its plans for a hazardouswaste treatment facility to the state of north carolina. the plant wasdesigned to serve primarily local and intrastate markets and would nothandle pcbs, dioxins, cyanide, radioactives, biological wastes, orexplosives. the treatment processes to be used were neutralization andcentrifugation. wastes that had to be burned would be transportedelsewhere (lynn, 1987).a year and a half prior to ecoflo's application, another company hadtried to site a hazardous waste facility in greensboro and failed. localcitizens, unable to receive information or to have their concerns addressed,had successfully organized opposition to that facility.the greensboro area had a group of citizens well versed in hazardouswaste issues. as a result of an epa grant to the north carolina league ofwomen voters in 1979, the guilford county hazardous waste task forcewas formed. the task force sponsored short courses on toxic materials andworkshops and displays to educate and organize the community. by 1985the task force and its chair, carolyn allen, had good working relationshipswith the local government staff and elected officials.purposes of risk communication and risk messages76improving risk communicationcopyright national academy of sciences. all rights reserved.when ecoflo decided to site in greensboro, the task force invitedneighborhood leaders from the part of the city where the facility might belocated to a series of education meetings on hazardous waste. theseworkshops included the chemistry of hazardous waste, disposal processes,and a session with tom barbee, ecoflo's vice president and thegreensboro plant manager (lynn, 1987).this was not barbee's first contact with the task force. he had beenattending task force meetings since 1979, as a professional waste managerwith another firm. he was also a native of north carolina and a longtimegreensboro resident. he did not see the environmentalists as the enemy. ina local tv interview he said that ecoflo ﬁhonestly wants to be a service tothe community–. we want to help local companies handle their waste asresponsibly as possible–. we are on the side of theenvironmentalistsﬂ (quoted in lynn, 1987).from the time ecoflo decided to site a facility in greensboro, barbeehad been contacting relevant groups and individuals. he went to the localpolice and fire departments to ask what they thought he needed to do toensure a safe site. he talked with ministers, neighbors, the planning andzoning department, and county commissioners. he gave candid anddetailed answers to questions by citizens. he and his staff took the press,state and local officials, and neighbors on plant tours. he even sponsoredhis own public meeting before the state held its public hearing. barbee'smeeting was cohosted by bruce banks, a local chemistry professor andaudubon society member; carolyn allen, chair of the task force; and jimrayburn, chair of the guilford county advisory board on environmentalaffairs (lynn, 1987). at this meeting barbee detailed how he had madechanges in his original proposal based on feedback from the firedepartment, the planning commission, and the task force, among others. heinvited public participation and took the public's concerns and suggestionsinto consideration in ecoflo's revised plan.this willingness on the part of ecoflo to involve the community, toshare information, and to implement changes based on community inputproved effective. the ecoflo waste treatment facility was approved andthe citizens were satisfied it could be operated safely (lynn, 1987).purposes of risk communication and risk messages77improving risk communicationcopyright national academy of sciences. all rights reserved.personal actionrisk communication regarding personal action is quite different from riskcommunication regarding public decisions. at minimum the setting is morelimited because most risk messages are addressed to individuals rather than to aspectrum of participants in public debate. sending messages to an individual is inone respect like sending them to the head of a regulatory agency: both have theultimate authority to act. but the two situations are also different in importantrespects: few individuals have staffs of experts paid to answer their questions, andindividuals seldom want the amount of detail that is justified when a federalregulator is about to make a decision for the whole population (see figure 4.1).much of risk communication in this setting takes the form of messages directedat the public offering information, advice, warnings, or recommendationsregarding risky individual actions. both public agencies and private organizationssometimes design such risk messages. but personal action is also influenced by avariety of risk messages, usually informal, from other individuals. people want toknow how hard it was to stop smoking, or whether lowfat meals can be made totaste good, or in what ways other people feel better after losing weight. suchriskrelated messages, regardless of whether they accurately represent the likelyoutcomes of alternative actions, can be critical in individual decisions (nisbettand ross, 1980).tobacco smoking also illustrates the kinds of risk communication issuesthat arise in the context of personal choice. despite the restrictions created byrecent policies, people still choose whether, how much, when, and where, withinlimits, to smoke. but congress has decided that it is in the public interest toinfluence smoking behavior in various ways short of directly restricting tobaccouse. cigarette taxes and advertising restrictions are two policies that constrainindividuals and the tobacco trade. other policies, such as the requirement ofwarning labels and widespread dissemination of the surgeon general's findings onthe risks of smoking, rely on risk messages as an alternative to direct control ofthe substance. such policies create a risk communication setting much differentfrom that of public decision making, particularly because they call for specializedrisk messages. congress has sanctioned efforts by government officials, includingthe surgeon general and other medical experts, to design and disseminatemessages aimed at changing individual behavior.we consider risk communication in the setting of personal choice purposes of risk communication and risk messages78improving risk communicationcopyright national academy of sciences. all rights reserved.figure 4.1 for personal action to reduce risks, a simple warning sign (e.g.,ﬁhills and curves next 10 milesﬂ) may be sufficient; a report of a formal riskanalysis could be counterproductive. source: courtesy of paul stern.successful only if it adequately informs the individual for making a choiceamong alternatives. adequate information, to reiterate, must be understandablefor risk communication to succeed; it is not sufficient that it be available. part ofthe debate is about going further, so that the recipients are somehow brought tounderstand the material. but we have not gone so far as to include this as acriterion for success.getting recipients' attention and comprehension poses significant barriers torisk communication, especially in the arena of personal action, where manyrecipients customarily act without carefully considering risks and benefits. itshould be noted that from the standpoint of the designers of risk messages, thegoal may or may not be to inform choice. often a message is intended toinfluence choice, a very different matter, even if experts believe that the choicethey desire to elicit is in the audience member's interest. thus some riskpurposes of risk communication and risk messages79improving risk communicationcopyright national academy of sciences. all rights reserved.messages from government agencies are designed to inform choice (e.g.,nutritional information on food packages), but at other times, occasionally afteropen debate in a legislative setting, an explicit decision is made to influencebeliefs or behavior in a particular direction (e.g., antidrunkdriving campaigns).although risk messages are sometimes judged against a criterion of behaviorchange, this is not an appropriate test of whether an individual has made aninformed choice. it is possible for an individual, fully informed of the risks, tochoose to engage in hazardous behaviors such as smoking, skydiving, or leavingseat belts unbuckled.sometimes risk messages are intended to inform or explain rather than to beused as direct input to a choice. this can be the case when the risk manager is inthe position of explaining a decision that has already been made. it can also occurin situations when individuals or groups are unavoidably exposed to particularhazards. it may be necessary to explain why a decision has been made that isinjurious to the recipients of the message or that has other undesirableconsequences.information and influence: the purposes of riskmessageswe have noted that successful risk communication, such as that described inthe ecoflo case, makes for betterinformed decision makers, both individualsand public or private officials. a ﬁsuccessfulﬂ risk message, in contrast, is notalways one that increases the understanding of decision makers. for riskmessages success is commonly interpreted in relation to the goals or purposes ofthe message source. the sources of risk messages sometimes aim to inform therecipients, but sometimes they aim to influence their beliefs or actions. a riskmessage designed to influence may be judged successful even if it does nothing toadd to the audience's understanding. an antidrug campaign that relies onexhortations from prominent sports figures is successful if it keeps someteenagers from addiction, even if they learn nothing new about the health effectsof heroin or cocaine.we recognize that efforts to influence through risk messages do not alwayshave such noble purposes. the sources of risk messages may set their owncriteria of success but attaining them does not always advance a public good.sometimes ﬁeffectiveﬂ risk messages are inconsistent with promoting substantivepublic good, as when they mislead people about what is in their interest. at suchtimespurposes of risk communication and risk messages80improving risk communicationcopyright national academy of sciences. all rights reserved.they are in conflict with the public goal of successful risk communication.(sometimes, however, audience members gain understanding even from biasedrisk messages. for instance, judges, elected officials, and interested citizens oftengain understanding on matters of public controversy by comparing messages fromvarious sources that they realize are trying to influence them. they informthemselves, despite the efforts of message sources to influence rather thaninform.)serious confusion can arise because any given risk message may be intendedto inform or to influence. it can be difficult for a recipient to tell which aim aparticular message has; message sources, aware of this difficulty, sometimesattempt to persuade in the guise of informing. that tactic is likely to be mosteffective when it goes undetected,1 but it can backfire seriously if revealed,undermining the credibility of the message source and creating resentment andmistrust. the problem of dual purposes is compounded by the fact that thedesigners of risk messages are often called on to both inform and influence thesame audience with the same message. regulatory agency employees, forinstance, are routinely asked to prepare a document to support a decision at theend of a formal rulemaking process that both summarizes the evidence on whichthe decision was based (thus informing the audience) and justifies that decision(thus endeavoring to influence the audience to believe the right choice has beenmade).the dual purposes of risk messages complicate defining responsiblebehavior for the designers of the messages. in order to arrive at some criteria forthe acceptability of attempts to influence, we begin by describing a dimensionalong which one can array techniques for the construction of risk messages. atone end of the dimension is an ideal, pure information, free of techniques ofinfluence; at the other end is deception. although the purpose of informing isconsistent with the goal of successful risk communicationšto raise decisionmakers' level of understandingšthe use of techniques that aim to persuade,deceive, or otherwise influence decision makers implies that a different goal isbeing pursued.informationto inform someone about an issue or choice is to assist that person toapprehend the relevant propositions or statements that describe the issue orchoice. ideally, the result is that the person or persons informed gain a full orcomplete understanding of the issuepurposes of risk communication and risk messages81improving risk communicationcopyright national academy of sciences. all rights reserved.or choice. this appears to have happened in the ecoflo case. in practice,however, full understanding does not exist for most important choices about risk(see chapter 2), so it cannot be conveyed. a practical goal for information is forthe recipient to gain understanding, within the limits of available knowledge, thatis adequate to make appropriate choices given his or her values. adequateunderstanding does not require knowing everything that is known about an issue,only enough to be able to make choices in one's own best interest. if more preciseinformation would enable members of the audience to make choices that betterapproximate their desires, it should be provided; if it would not aid in decisionmaking, more precision is unnecessary.influencea spectrum of techniques is available for designing risk messages that gobeyond pure information and that can be used to influence an audience. the mostextreme techniques involve outright deception: strategies such as ﬁlying,withholding of information, true assertion that omits a vital qualification, andmisleading exaggeration to cause persons to believe what is falseﬂ (faden andbeauchamp, 1986:363). but many influence techniques do not do such violenceto the truth. in order to consider the appropriateness of different techniques, it isuseful to identify them. the following paragraphs describe different techniques,beginning with some that stay close to the facts and moving to some that do notdepend much on factual information. some of these techniques can be used eitherto inform or to influence. it is this possibility that makes it difficult for recipientsof risk messages to determine their intent and therefore to interpret their content.highlighting factsrisk messages cannot include all the details known to science and still beread and understood by most nonexperts. therefore the designers of messagesomit some information and highlight other information. for instance, messagedesigners choose whether to summarize knowledge about both possible deathsand illnesses arising from a risk or only about deaths, about both direct andsynergistic effects or only direct effects, about effects on subpopulationsincluding sensitive groups or just on whole populations, and so forth. havingpurposes of risk communication and risk messages82improving risk communicationcopyright national academy of sciences. all rights reserved.chosen what to present, message designers must also make choices about whatparts of the message to emphasize with visual aids, vocal emphasis, underlining,color, and other techniques. although highlighting may be employed only toemphasize the essentials of what is known, decisions to highlightšwhich areunavoidablešinvolve judgments about what is essential. a large psychologicalliterature demonstrates that highlighting information, or making it moreﬁavailable,ﬂ affects the understanding and the decisions of those who receive themessages (fiske and taylor, 1984; kahneman et al., 1982; tversky andkahneman, 1973). thus highlighting can influence the audience's beliefs aboutwhat aspects of a risk decision are important in the direction desired by themessage designer.ﬁframingﬂ information and decisionsdifferent ways of presenting the same facts can create different impressions.when a risk estimate is uncertain, it can be described by a point or ﬁmaximumlikelihoodﬂ estimate or by a range of possibilities around the point estimate. butestimates that include a wide range of uncertainties can imply that a disastrousconsequence is ﬁpossible,ﬂ even when expert opinion is unanimous that thelikelihood of disaster is extremely small. the amount of uncertainty to present is ajudgment that can potentially influence a recipient's judgment.another example of ﬁframingﬂ involves the choice between alternative waysof presenting the same numerical information. one study, for example, foundthat a hypothetical vaccine that reduces the probability of contracting a diseasefrom 0.20 to 0.10 is less attractive if it is described as effective in half the casesthan if it is presented as fully effective against one of two virus strains that strikewith equal probability and that produce the same disease. this finding suggeststhat people favor full protection against an identified risk over equivalent butprobabilistic protection (tversky and kahneman, 1981). similar differences inpresentation have been identified with respect to whether outcomes are presentedin terms of ﬁsure lossﬂ or an ﬁinsurance premiumﬂ (fischhoff et al., 1980) orﬁlives lostﬂ as opposed to ﬁlives savedﬂ (tversky and kahneman, 1981). it haseven been demonstrated that when two versions are presented sequentially peopleoften reverse their preference from the first presentation to the second (hersheyand shoemaker, 1980).purposes of risk communication and risk messages83improving risk communicationcopyright national academy of sciences. all rights reserved.risk comparisonsan important instance of framing is the use of risk comparisons. comparingone risk that is not well understood to another that the audience comprehends maybe a useful way to convey information about the former risk. it is often difficult,however, to find risks that are similar on enough attributes to carry thecomparison. but risk comparisons can also be used to influence or even mislead,because a risk comparison may improperly carry the implication that if a personis willing to take the larger of two risks he or she should accept the smaller aswell (covello et al., 1988; fischhoff et al., 1981a). the uses of risk comparisonsare discussed in more detail in chapter 5.persuasive use of factsrisk messages often involve a selection of the facts to make a point.messages aimed at convincing recipients of a point of view can use techniques ofhighlighting and framing but can also employ other rhetorical techniques:selective presentation of evidence, creation and destruction of ﬁstrawmanﬂarguments, judicious placement of the various arguments within a message formaximum effect, listing of supporting arguments by number to make theargument look stronger, and so forth. such techniques can enhance the persuasiveeffect of messages, sometimes without any alteration of the content (cialdini,1984; eagly and chaiken, 1985; mcguire, 1985), and they can be quite difficultfor a recipient to detect.appeals to authoritynonexperts often want to know who has taken what position on a difficultchoice before them. when they do not know enough to make an informed choicethemselves, or believe it too expensive or time consuming to become fullyinformed, they may choose to adopt the position of a person or organization theyconsider expert and trustworthy. thus risk messages can be influential bysupplying information about who has taken positions on an issue. they may bebalanced in their references to authority or they may not: a message may quotesome scientists in support of a position but omit quotations from similar scientistswho disagree. they may quotepurposes of risk communication and risk messages84improving risk communicationcopyright national academy of sciences. all rights reserved.relevant authorities who have specialized knowledge or they may refer to sourceswidely trusted on other issues but ill informed on the issue at hand. and they maybe accurate or inaccurate in representing the views of the authorities. clearly,appeals to authority can fall at many different points along the dimension frompure information to deception.appeals to emotionrisk messages sometimes appeal to fear, pride, guilt, community spirit,parental concerns, or other emotions to spur people to action. sometimesemotional appeals are made in the context of a presentation of information. thus,saying that cigarette smoking causes emphysema conveys the same informationwith or without an accompanying film of an endstage emphysema patient, butwith the film the message will have a different effect. appeals to emotion are notalways more effective in inducing behavior change than less emotional appeals:the psychological research shows that the effect depends on other aspects of themessage as well (petty et al., 1988). nevertheless, appeals to emotion can beeffective influence techniques under some conditions. sometimes the use ofemotional appeals is widely accepted, but often it is considered manipulative andirresponsible. the conditions under which emotional appeals are consideredacceptable are not well understood.use of influence techniques in riskcommunicationachieving balancerisk messages often employ some of the above influence techniques;indeed, it is difficult to imagine a risk message that could attract the attention ofnonexperts without making use of at least highlighting or framing. a paradoxarises for risk communication: how can messages be made to improve therecipients' base of information if, in order to be effective, they must usetechniques of influence? the paradox disappears when one realizes that there arestrategies for controlling the use of influence techniques consistent with the goalof successful risk communication. substantive guidelines should be establishedfor the content of risk messages that responsible message designers, includinggovernment officials, canpurposes of risk communication and risk messages85improving risk communicationcopyright national academy of sciences. all rights reserved.to keep influence techniques under control so as not to bias recipients'understanding. because available knowledge is inadequate to provide highlydetailed substantive guidelines, procedural approaches that keep messagedesigners in bounds are also critical to achieving successful risk communication.the strategy of substantive guidelines is highly demanding. as alreadynoted, the language of risk messages and even the measures used in risk analysisoften embody value judgments or otherwise tend to lead the recipients ofmessages toward particular conclusions. we have noted several examples, butnot enough is known to identify all the ways a risk message might bias arecipient's understanding. thus it is not now possible to devise a complete guideto sources of potential bias that would allow risk messages to be evaluated forbalance. moreover, research on communication strongly suggests that the mosteffective message design for any particular purpose varies with the subject matterat hand, the decision alternatives, the intended audience, and other factors. butvery little is known about the key situational variables that alter the effects of riskmessages. thus at present any guidelines for balanced risk messages would lacksituational specificity. existing knowledge can help message designers byidentifying some potential pitfalls, but it cannot yield highly specific guidance.responsible message designers need to interpret available advice, keeping inmind that knowledge is incomplete and that general principles may not apply tocertain specific situations. since there is no clear best way to make suchjudgments, substantive guidelines are not enough to ensure balance in riskmessages, even when the sources are doing their best to achieve it.the procedural strategy, which relies on a system of checks and balances tocontrol the possible biases in risk messages, is applicable without regard to thestate of knowledge about the effects of risk messages. the strategy assumes thatavailable guidelines will never be perfectly correct or clearcut and that vestedinterests or strongly held values will often induce ingenious message designers tofind ways around guidelines. it therefore relies on systems of scrutiny andcriticism, and the discipline of competing messages, to keep message designerswithin bounds.examples of procedural strategies applied to individual messages are theprocedures of the national center for toxicological research (nctr) consensusworkshops and those of the national research council (nrc) for review of itsreports. the nctr consensus workshop series involves scientists fromacademia, government, inpurposes of risk communication and risk messages86improving risk communicationcopyright national academy of sciences. all rights reserved.dustry, and public interest groups gathered to resolve toxicological issues, usuallyconcerning the hazard posed by particular substances (gough et al., 1984).consensus is sought, not by formal voting, but through the chairman's guidingdiscussion toward agreement. careful procedures ensure that all panelists have anopportunity to submit statements and to evaluate and comment on reports. theseprocedures ensure that reports focus on those areas where consensus is reachedand present the major factors in reaching agreement. the nrc, many of whosereports are detailed messages about risk, does not rely on guidelines for the use oflanguage, graphics, and so forth. rather it relies on a balanced choice ofcommittee members and an independent review process. the nrc presumes thata dialogue of wellinformed individuals with varying perspectives will yield afirst approximation of a balanced assessment. the outcome of this process isdoublechecked by submitting it to an independent review process involvingexperts who also represent a range of perspectives. in these two procedures it isnot substantive guidelines but the process of dialogue and criticism that is used toensure a balanced message.achieving influenceeven more difficult than the problem of achieving balance in riskcommunication is the problem of deciding whether balance is the wrongobjective. advocates whose clear purpose is to influence their audiences mayexperience no problem, but the issue can be particularly acute for public officialswho sit in a relation of public trust to the recipients of their messages. whenshould messages aim at merely informing the public, or government decisionmakers, and when should the goal be to influence the recipients?government officials are commonly expected to follow a more restrictedstandard of behavior in the area of risk communication than are advocacy groups,private citizens, or corporations. similarly, citizens apply a stricter standard tomessages paid for with public funds than to privately funded messages. we judgethat such standards are justified because government officials hold a public trust.but the specifics of such standards are not easily defined.after considerable debate focusing on the appropriate use of risk messagesby public officials, we concluded that no explicit guidelines can be drawndefining which techniques are appropriate or inappropriate in particular situationsor for particular message sources. wepurposes of risk communication and risk messages87improving risk communicationcopyright national academy of sciences. all rights reserved.agreed that informing is always an appropriate goal in the design of riskmessages and that deception is never appropriate. but we recognize thatmessages that employ influence techniques or that have influence as an objectiveare often considered acceptable, even coming from public officials. we believethat more extensive public debate is needed to arrive at standards for responsiblebehavior by public officials in the design of risk messages. as a contribution tothat debate, we offer the following observations about the conditions under whichinfluence techniques seem most likely to be considered appropriate by variousaudiences.first, the acceptability of influence as a purpose of risk messages seems todepend in part on which beliefs or actions are being influenced. consider therange of actions and opinions that government agencies have tried or might try toinfluence with risk messages. here are some examples: inoculating children against diphtheria, polio, pertussis, or swine influenza; using condoms to prevent aids, gonorrhea, or pregnancy; avoiding or reducing consumption of heroin, alcohol by drivers, tobaccoproducts, alcohol by pregnant women, aspirin by children, or animal fat; using seat belts, motorcycle helmets, or masks for painting or working withfiberglass; supporting drug enforcement activities, aids research, epa enforcementactivities, or the repeal (or passage) of particular pieces of legislation.depending on the action or opinion in question, the likely response togovernmentsponsored influence attempts may vary from general acceptance toextreme controversy. within each of the categories just listed, we believe thatefforts to influence the action or opinion mentioned first would be relativelyuncontroversial compared with similar efforts to influence the actions given laterin each category. it is important to recognize, however, that observers, includingmembers of our study committee, differ on the appropriateness of influencetechniques in certain of the contexts listed. some variation in judgments concernsscientific knowledge: the more clearly it has been established that an activity isdangerous or that it may harm persons generally considered to deserve societalprotection (e.g., children), the more acceptable influence attempts seem tobecome.2 butpurposes of risk communication and risk messages88improving risk communicationcopyright national academy of sciences. all rights reserved.because of scientific uncertainty, informed observers sometimes disagree abouthow well established the relevant knowledge is. another central issue seems to bethe compatibility of governmental influence with individual autonomy and relatedvalues (faden, 1987; faden and beauchamp, 1986). when a class of personalaction (such as drunk driving) affects a large portion of the populace or threatensto inflict substantial monetary and other costs on society or on individuals who donot engage in that action, people are more willing to accept, and even to demand,that government agencies be proactive and try to influence beliefs and actions.under such conditions, people are more willing to compromise the autonomy,privacy, or freedom of some individuals for the good of others.second, the acceptability of influence seems to depend on the techniquesemployed. generally, the farther an influence technique lies along the dimensionfrom information to deception, the harder the message becomes to justify and theclearer and more explicit must be the legitimate public purpose being served. toinfluence people to use condoms to prevent aids, government might appeal toauthorities (the surgeon general recommending use of condoms to avoid aids)or respected or admired individuals (film and popular music stars hosting a tvspecial encouraging use of condoms in aids prevention), post warning signs (inlavatories of establishments frequented by homosexual males), present selectedrisk and risk reduction information (ﬁuse of condoms can reduce the transmissionof aids by 95 percentﬂ), or appeal to emotion (photographically depict the latestages of aids or state that ﬁyou sleep with your partner's whole sexualhistoryﬂ). observers differ on the appropriateness of such techniques for aparticular purpose, even when all agree that the purpose justifies some form ofgovernmental influence.we conclude that public values about the importance of particular publicpurposes and the acceptability of particular influence techniques are not wellunderstood. generally, the more an influence attempt would compromiseimportant values such as personal autonomy or constitutional guarantees such asfreedom of speech or association, and the more closely the influence techniqueapproaches deception, the more it needs to be legitimated in order to beacceptable. legitimacy is what makes people consider a particular influenceattempt either responsible or irresponsible and either appropriate or inappropriatefor government officials.but there are no clear a priori guidelines that can tell a government officialor other designer of a risk message when the message'spurposes of risk communication and risk messages89improving risk communicationcopyright national academy of sciences. all rights reserved.purposes are sufficiently legitimate to justify a particular technique that goesbeyond informing. government officials will likely find their efforts to influencecontested if they stray from accepted scientific views or if they challenge popularconsensus. it is for this reason that decisions about governmental use of influencetechniques in risk messages are often debated in overtly political arenas ratherthan being left to unelected officials' unscrutinized discretion. we believe thatpolitical arenas are the proper place for deciding the appropriateness ofgovernmental efforts to influence citizens. governmental attempts to influencecitizens' beliefs and actions can be justified only to the extent that some legitimatepublic process has culminated in a decision that using risk messages to influencebehavior serves an important public purpose.influence and personal actionthe clearest example of politically established legitimacy for risk messagesoccurred in the congressional debate on persuading people to stop smoking. acongressional act codified languageša set of risk messagesšthat now appearson cigarette packages. the process of debate and approval by elected officialsgranted legitimacy to the messages.such explicit public debate rarely occurs to give clear prior justification forgovernmental attempts to influence personal behavior. nevertheless, an agencyor official can sometimes act legitimately on general authority. for example,public health officials have fairly general support in the mandates of theiragencies for influencing people to take action to prevent the spread of infectiousdiseases. as a result, the surgeon general's 1988 mass mailing of a risk messageabout avoiding aids was met with wide public acceptance and even gratitude.sometimes executive branch officials justify influence attempts within the spiritof their legislative mandates. the u.s. environmental protection agency's effortsto inform the public about the health risks of indoor radon, and to convincepeople to have their homes tested and sometimes modified at considerableexpense, are not justified by anything stronger than the epa's general mandatefor environmental protection. yet this attempt to influence behavior in the settingof personal action was widely welcomed.purposes of risk communication and risk messages90improving risk communicationcopyright national academy of sciences. all rights reserved.influence and public debatesometimes executive branch officials rely on their general mandate toinfluence beliefs in the setting of public debate. such efforts tend to be moreacceptable after a risk management decision than before (e.g., when regulatorsare expected to justify their decisions to the public). but even before a decision ismade, there are situations in which some kinds of efforts to influence publicdebate are appropriate. regulatory officials sometimes argue that they have anobligation to evaluate new risks and, when public action is needed, to persuadeelected officials of that fact. it is not enough, they say, merely to inform thepublic of the latest knowledge. thus some public officials, on receiving evidenceon the risk to the earth's ozone layer from chlorofluorocarbons, attempted toinfluence the highest levels of government to support an international treaty tocut production of that class of chemicals.but it is easy for a public official to overstep the bounds of acceptability.this happens most readily when the subject matter of the influence attempt isalready politically controversial or when government can be seen as trying toinfluence free political expression. when the san francisco office of the energyresearch and development administration distributed 78,000 pamphletsdefending the safety of the nuclear power industry during a 1976 californiareferendum campaign on the future of nuclear power, the result was a criticalreport from the general accounting office and strong expressions ofcongressional outrage (burnham, 1976). not only was the message unacceptable,but its dissemination and the agency's evasive response to criticism harmed theagency's credibility. with many influence attempts it takes fairly explicit debateand agreement to make them legitimate: vague appeals to an agency's mandateare not sufficient.the judgment of whether public officials have or have not exceeded theirproper role in a particular attempt to influence public debate is difficult to make.but it is a matter of judgment. clearly, the freedom of public servants toinfluence decision makers must be kept within bounds. we considered andrejected the position that advocacy is always inappropriate for executive branchofficials in the setting of public debate. there are situations in which suchofficials are in the best position to alert the public to a hazard that may deservegovernmental action. but it is difficult to define the properpurposes of risk communication and risk messages91improving risk communicationcopyright national academy of sciences. all rights reserved.limit. scientific analysis is indispensable to successful risk communication. it canshow what is known about risks and the attendant choices and can identify thelimits and uncertainties of that knowledge; it can therefore indicate what can besaid. science can also advise on when and how best to say it in order to improvean audience's understanding or to influence beliefs and actions. a decision toengage in advocacy, however, involves judgments about which risk managementoption is appropriate and about how much to influence audiences with other thaninformationšjudgments that must be based on values as well as knowledge. weconcluded that natural and social sciences cannot provide guidelines for when toengage in advocacy in risk communication. although empirical research candetermine which beliefs americans consider acceptable for influence bygovernment and which influence techniques they consider most extreme andtherefore most in need of legitimation, there is no practical way to tell in advancewhether enough legitimation exists in the political system to justify a particularattempt to use risk messages to influence recipients. advocacy messages fromexecutive branch officials must therefore be judged against the legitimate role ofthe officials in question, as set forth in the relevant legislation and judicialinterpretations and as argued by elected officials. the decision of what arelegitimate bounds for governmental risk messages is and ought to be madethrough the political process.we recognize that the boundaries for advocacy in the political process oftenare clear only after a public official has overstepped them, leaving public officialsin an unpleasant position. however, such boundaries usually can be discerned inadvance by careful analysis. in any case, when officials judge that the publicwelfare depends on a specific change in policy or individual behavior, they mustalso judge how far they can go before overstepping legitimate constraints.advocacy can be politically risky for public officials. it may be widely applaudedor widely condemned, and types of messages that may be widely accepted on onesubject matter or from one government source may be criticized when the topicor source changes. a public official should be aware of the political risks and ofthe legitimate constraints placed upon government in advocacy and, where anunusually strong degree of advocacy seems warranted, seek political approval ofsuch action.risk communication may be difficult because the purposes of messages arenot clear or because they have multiple, perhaps conflicting, purposes. the nextchapter describes several misconceptionspurposes of risk communication and risk messages92improving risk communicationcopyright national academy of sciences. all rights reserved.about risk communication that may also contribute to confusion and frustration onthe part of risk communicators and recipients.notes1. generally, persuasive messages are less effective when recipients have the opportunity to ﬁanchorﬂtheir preexisting beliefs against persuasion in the following ways: by defending them against a priorpersuasive message, by considering their other beliefs or values that are supported by the beliefsubject to persuasive communication, or by training in the ability to question or argue againstpersuasive messages or to be suspicious of the source (the evidence is reviewed by mcguire,1985:292œ294). persuasion that does not appear to be persuasion might not evoke such defenses.2. for instance, public support for persuasive messages about aids prevention was minimal when thedisease seemed to threaten only homosexual males, haitians, and intravenous drug users but increasedrapidly when children, hemophiliacs, adult heterosexuals, and hospital patients receiving bloodtransfusions were seen to be at risk. shilts (1987) gives an extensive account of how public concernabout aids has related to the identity of the groups believed to be at risk.purposes of risk communication and risk messages93improving risk communicationcopyright national academy of sciences. all rights reserved.5common misconceptions about riskcommunicationsome of the more important misconceptions about risk communication,including unrealistic expectations about what it can accomplish, are discussed inthis chapter. once these misconceptions are dispelled, the real problems of riskcommunication can be addressed.we have taken care to distinguish between risk communication and riskmanagement and between risk communication and risk messages. the primarygoal of risk communication is to inform the participants in decisions about risks.neither successful communication nor successful execution of the politicalprocess guarantees that risk management decisions will maximize welfare interms of reducing exposure to hazards. yet many people judge riskcommunication by the quality of the relevant risk management decisions.we take political constraints as given and attempt to find ways within themto inform debates about risk. a wellinformed decision process is likely to yieldbetter decisions than an uninformed process. if all participants are adequatelyinformed, the ultimate decision is more likely to improve conditions for allinvolved than a decision made by experts alone.it is important, however, to realize that because risk communication usuallyinvolves multiple messages from many sources, and because these messagescontain difficult and complex ideas, there is no simple way of making riskcommunication easy.risk messages necessarily compress technical information, whichcommon misconceptions about risk communication94improving risk communicationcopyright national academy of sciences. all rights reserved.can lead to misunderstanding, confusion, and distrust. preparing risk messagescan involve choosing between a message that is so extensive and complex thatonly experts can understand it and a message that is more easily understood bynonexperts but that is selective and thus subject to challenge as being inaccurateor manipulative.since it is a reasonable precaution to assume that the compression in riskmessages may introduce intentional or unintentional bias, it is natural to treat riskmessages as reflecting political as well as scientific elements. because peopleview risk messages as incorporating both scientific and political elements,appeals to scientific quality and veracity alone on the part of the riskcommunicator may not always sway the skeptic.expectations regarding risk communicationmany peoplešincluding some scientists, decision makers, and members ofthe publicšhave unrealistic expectations about what can be accomplished by riskcommunication. it is mistaken to expect improved risk communication to alwaysreduce conflict and smooth risk management. in addition, risk comparisons alonecannot establish levels of acceptable risk or ensure systematic minimization ofrisk, although they can help people comprehend unfamiliar magnitudes.communication, conflict, and managementmany people, especially decision makers, seem to think that wellcraftedmessages or communication campaigns can eliminate or reduce conflicts in riskissues. these individuals believe that the conflicts are based on lack ofinformation, that if all the parties were made aware of the facts, they wouldagree. this overlooks the possibility that conflicts are based on factors such asdistribution of risks and benefits (e.g., do both fall equally on the same people?),different values (e.g., are the participants risk averse as opposed to riskseeking?), and different goals (e.g., is it better to avoid food additives or toenhance preservation and length of storage for food stuffs?).communication may reduce conflict about risks in some instances.however, when the underlying knowledge is uncertain, when people disagreeabout the meaning of existing data, when there is disagreement about theacceptable level of riskšin other words, in most cases of conflict about riskšinformative risk messages might make the issues, and thus the conflict, clearerand more obvious.common misconceptions about risk communication95improving risk communicationcopyright national academy of sciences. all rights reserved.in the introduction we discussed the desire to develop effective alternativesto regulatory control as one of the reasons for interest in risk communication. butnot all people see this as a positive development. the possibility of divertingattention from the risks and their control with careful information campaigns issufficient to make some observers chary of risk communication. ellen silbergeld,senior scientist with the environmental defense fund, expressed ambivalenceabout the large attendance (approximately 500) at the national conference onrisk communication in 1986. she viewed increased interest in the topic as aresult of the destruction of consensus on environmental and other risk areas anddescribed risk communication as a ﬁshield for inactionﬂ (silbergeld, 1987a).comparing risksanother mistaken expectation is that risk comparisons can be used todetermine acceptable levels of risk and help minimize overall exposures.comparing different risks can help people comprehend the uncommonmagnitudes involved and understand the level, or magnitude, of risk associatedwith a particular hazard. but comparison with other risks cannot itself establishthe acceptability of the risk in question. to realize, for example, that the chanceof death from a previously unknown risk is about the same as that from a knownrisk does not necessarily imply that the two risks are equally acceptable.generally, comparing risks along a single dimension is not helpful when the risksare widely perceived as qualitatively different.risk messages commonly convey quantitative information that is unfamiliarand difficult to comprehend. these magnitudes and risk estimates are not easilyunderstood without benchmarks or points of reference, and providing carefulcomparisons can help people understand this information. risk magnitudes aredifficult enough to understand when referring to a single consequence, such asdeath. but comparison of different consequences, such as injury, disability, orchronic disease, is even more difficult.an interesting approach is the use of risk ladders, for which a range ofprobabilities is presented for a single class of risks. although this technique canhelp people understand the magnitudes, it is not without problems. figure 5.1shows two examples of risk ladders. we consider the first weaker because of theseveral deficiencies listed. the second is considered stronger because it involvesfewer deficiencies. the two risk ladders illustrate both the potential of theapproach and the difficulty of using comparison. (note: not all attributes ofcommon misconceptions about risk communication96improving risk communicationcopyright national academy of sciences. all rights reserved.the two risk ladders have been empirically tested, so it is not possible to state withcertainty how people will react to them. the principal weaknesses listed are basedon the existing literature. each practical use of risk comparison should becarefully pretested if possible.)use of multiple comparisons helps counteract the possibility that people mayseverely misestimate a particular risk, even though it is familiar to them. it alsoreduces the danger of arousing the scientific disputes that can often arise whenonly two risk estimates are compared, one or both of which are subject toscientific debate.one difficulty in risk comparison is that it is often difficult to find risks thatare sufficiently similar to make the comparison meaningful. the easiest way toavoid comparing apples and oranges is to compare the risk associated with thesame hazard at different times or risks associated with different options forachieving the same purpose. these comparisons are the least problematic becausethey address the same hazards and consequences with variation in themechanisms for controlling or reducing the risk in question.when such direct comparisons are not possible, it is important to recognizethat various risks have different qualitative characteristics and that these canaffect the way comparisons are viewed (fischhoff et al., 1981a; slovic, 1987;slovic et al., 1980). two that have been shown to have considerable impact arecomposite indices derived from factor analysis. the first, labeled ﬁdread,ﬂ isassociated with perceived lack of control, dread, catastrophic potential, and fatalconsequences. the second, called ﬁunknown,ﬂ is associated with the degree towhich the risk is perceived to be unobserved, unknown, new, and with delayedmanifestations of harm (slovic, 1987). hazards whose quantitative risks areestimated to be the same or similar may result in quite different responses if theirqualitative characteristics are sufficiently different. care must be taken that therisks compared exhibit qualitative characteristics that are reasonably similar.another pitfall of risk comparison is the appearance of selecting risks forcomparison that minimize or otherwise trivialize the risk in question (covello etal., 1988). compendiums of risks, or risk ladders placing various risks along aspectrum from lower to higher, may give this appearance when the risk inquestion is much lower than other risks and when there are few risks presentedwith comparable levels. if, however, the comparison presents risks that clearlyrelate to the risk in question and relate or position its level or magnitude, theappearance of trivialization can probably be avoided.common misconceptions about risk communication97improving risk communicationcopyright national academy of sciences. all rights reserved.figure 5.1a a poor risk comparison. source: schultz et al., 1986, as cited incovello et al., 1988. reprinted with permission of the chemical manufacturersassociation.it is sometimes assumed that once they are told about risks people willsystematically minimize their exposures and disregard truly small risks when theyunderstand how little they are. this encourages comparing the risk in question toother risks that are familiar to most people with the intent of claiming that thelevel of the risk under examination is acceptable. the logic of using riskcomparison to determine acceptable risk usually runs as follows: since youcommon misconceptions about risk communication98improving risk communicationcopyright national academy of sciences. all rights reserved.accept the risk of driving an automobile, which is about 240 annual fatalities permillion persons (total population), you also ought to accept the risk of exposure tox (whatever hazard the communicator supports), which is, say, 10 annualfatalities per million. this logic is faulty (fischhoff et al., 1981a). a homeowner,for example, should not neglect the potential fire hazard of electrical appliancesor gas stoves and furnaces just because the risk of annual fatality due tofigure 5.1b a better risk comparison. source: smith et al., 1987, as cited incovello et al., 1988. reprinted with permission of the chemical manufacturersassociation.common misconceptions about risk communication99improving risk communicationcopyright national academy of sciences. all rights reserved.fire is about one tenth as large as that due to driving an automobile. rather,reasonable precautions should be considered with regard to risks deriving fromall the hazards over which one has control. the level of risk is only one amongseveral factors that determine acceptability (fischhoff et al., 1981a; gould et al.,1988; slovic, 1987; slovic et al., 1980), and the information requirements for aninformed decision by private individuals or public officials will generally includemore than the level of risk alone.beliefs about the functioning of the processmany problems for risk communication derive from mistaken beliefs aboutthe nature of the risk assessment, risk management, and risk communicationprocesses.1 it is mistaken to expect scientific information to resolve all importantrisk issues. in addition, even when valid scientific data are available, experts areunlikely to agree completely about the meaning of the data for risk managementdecisions. finally, it is unrealistic to expect easy identification and understandingof the values, preferences, and information needs of the intended recipients of riskmessages.adequacy of the scientific information baseas is clear from the discussion in chapter 2, it is unrealistic to expectcomplete information about all the various aspects of a hazard and the risk ofexposure to it. but even if the scientific risk information were perfect, it mightnot resolve all the issues involved. the best technical analysis cannot reveal whatought to be done. analysis can only estimate the consequences and, in somesituations, the way those expected outcomes compare to other related outcomes.the adequacy of the information base is an important consideration not onlybecause some statutes as well as current interpretation of the administrativeprocedures act require regulatory decisions to be based on reasonedconsideration of the evidence, but also because risk management decisions shouldbe based on the best available information rather than arbitrary or unfoundedbeliefs and assumptions. it could thus be argued that the information base for arisk management decision would be inadequate if additional scientific data couldprovide at reasonable cost a more detailed or more complete understanding of thephenomena giving rise to the risk in question. of course, more scientific dataalways would be of positivecommon misconceptions about risk communication100improving risk communicationcopyright national academy of sciences. all rights reserved.value under such a criterion, and the difficulty lies in determining how manyresources should be allocated to this particular problem and how long the decisionshould be delayed in order to obtain more data.agreement as to the meaning of existing informationthere is seldom definitive scientific data about important risk issues.science continually develops new, more sophisticated testing methodologies.even with the most recent additions it is doubtful that any substance or producthas been, or can be, so thoroughly tested as to preclude further scientificquestion. the numbers usually can only give an estimation of the consequencesand, in some situations, the way those expected outcomes compare to otherrelated outcomes. very often regulatory and other risk control decisions must betaken before all the scientific questions are fully resolved. in these cases thedecision maker will be faced with choosing from among conflicting, sometimescontradictory interpretations of the data.these issues are important because they can strongly affect thedetermination of risk concerning a particular substance or activity. whether alinear or multistage model is used for extrapolation, or whether a restricted orgeneralized model is used to compute doses, estimation of the no observed effectlevel (noel), or the safety factors used to allow for various kinds of uncertainty,can have significant impact on the characterization of risk. such issues can be atthe center of a controversy and can dominate debate about them and the relatedrisk messages.interpretation of public attitudes and information needsbecause of the public's ability to make itself heard on risk issues, publicopinion does influence the introduction and application of modern technology.but it is usually a relatively small part of the general public that makes its viewsknown about a particular issue. it is therefore useful to distinguish between thepassive public (largely unaware of the issue), the attentive public (aware of theissue and its ramifications), and the active public (seeking to make its viewsknown or to affect decisions in other more direct ways). depending on the natureof the issue, the source of a risk message may need to understand the attitudes andinformation needs of each of thesecommon misconceptions about risk communication101improving risk communicationcopyright national academy of sciences. all rights reserved.different types of potential recipients of risk messages. both the differencesamong these types of potential recipients and the respective ease or difficulty ofestablishing contact with them and determining their views and informationneeds contribute to the complexity of the task.a few years ago several of the large groups of government and industry riskmanagers began seeking out the advice of social scientists because of oppositionto their programs in the public (fischhoff, 1985a). risk managers typically madeconfident statements about public opinion on the basis of anecdotal observation,in contrast to practicing social scientists, who usually venture carefully qualifiedstatements only after extensive investigation. risk managers also made confidentstatements about the information the public wants and uses in particularsituations. for the most part both types of statements were based on a view ofﬁthe publicﬂ that did not differentiate among the general public, the attentivepublic, and the active public or among people with different personal values,levels of exposure, or sensitivities to the hazards in question.not only does the level of interest in specific topics vary among differentpeople, but so also does the way they think about the issues involved. during thelast decade researchers have examined the opinions people express when asked,in a variety of ways, to evaluate hazardous activities, substances, andtechnologies (slovic, 1987). psychological research suggests that people'sperceptions and attitudes are not determined solely by the sort of unidimensionalstatistics used to describe the magnitude of risks. to many people, statementssuch as, ﬁthe annual risk from living near a nuclear power plant is equivalent tothe risk of riding an extra 3 miles in an automobile,ﬂ give inadequateconsideration to important differences in the nature of the risks from these twotechnologies. as noted in chapter 3, risk is only one facet of these conflicts (seealso douglas and wildavsky, 1982; short, 1984). risk concerns may provide arationale for actions taken on other grounds or they may be a surrogate for othersocial or ideological concerns. when this is the case, communication about risk isoff the mark.stereotypes about intermediaries and recipientssome risk communication problems derive from misconceptions about theway intermediaries and recipients react to risk messages. itcommon misconceptions about risk communication102improving risk communicationcopyright national academy of sciences. all rights reserved.is mistaken to view journalists and the media always as significant, independentcauses of problems in risk communication. it is mistaken, as well, to expectpeople only to want simple, cutanddried answers in every case.journalists and the news mediamany people who are disgruntled with the slowness and apparentincoherence of decision making about risk control or with the outcomes attributemany of these problems to the news media and journalists. some claim, forexample, that public concern is ﬁdriven by media coverage rather than by rationalscientific analysisﬂ or that ﬁthe media has driven the public insaneﬂ (cohen,1987). these critics claim, for example, that the news media are basically in theentertainment business and that the only thing that matters is the ability of a storyto attract attention because this sells newspapers and attracts viewers.it is true that newspapers, radio and television stations, and the networks arebusinesses. and it is true that they must pay attention to income and profits. butthe direct effect on subscriptions or advertising income is not likely to be in theminds of reporters as they prepare stories nor in the minds of editors or producersas they make story assignments, edit copy, or determine the placement of variousstories in that day's newspaper or newscast.in selecting and preparing stories, the reporter is much more likely to bemotivated by events, by what other reporters are paying attention to, byinformation provided on a regular basis by sources he or she has cultivated, bydeadlines, and by what interests him or her as a citizen. the editor or producerwill be concerned about the appeal and impact of the issue or program as awhole. both, for their different reasons, will be concerned about the importanceof the stories, their impact, and their drama. the attractiveness of stories withsuch appeal will be strong whenever censorship is absent and there is free andopen access to information sources. it is mistaken to attribute the way the mediadefines ﬁnewsworthinessﬂ in practice to crass economic motives alone.because of their involvement in selecting and preparing stories, journalistsmay have a better perception of the audience and its interests than do editors orproducers. but that perception is probably also based on the ﬁconveniencesampleﬂ with which that journalist happens to have contact. journalists and themedia play importantcommon misconceptions about risk communication103improving risk communicationcopyright national academy of sciences. all rights reserved.roles in revealing conflicts and sometimes in their resolution. the media maynurture the development of controversy by serving as a channel for debate amongthe major actors in a conflict, and they can play crucial roles in providinginformation to citizens during conflicts (tichenor et al., 1980). this latter can beespecially important since significant portions of the public may never attend torisk information unless such a conflict attracts their attention.for the most part, what can be called the national press (the new yorktimes, wall street journal, and comparable news organizations) treats risk issueswith considerable care and understanding. but this is not always true, especiallyat the regional or local level. the performance of the press in the reporting of riskissues is not always up to the standards found in other topic areas. most newsorganizations would not tolerate sports or business reporting by reporters who donot understand the subject and are unable to correctly frame those topics. thesame is not always true of the reporting of the technical and social dimensions ofrisk messages.some criticism of the news media emerges from a failure to examine thestructure of the media industry or how journalists work. it would be more fruitfulfor risk communicators to try to understand the pressures and constraints on newsgathering than to curse the sometimes disappointing results. the structure of theindustry and the incentives and influences that affect the way it works are part ofour social and political system. what is needed are ways to improve riskcommunication by helping scientists and decision makers understand how andwhy journalists do their work and by helping journalists understand howscientists and decision makers think and interact.there are, for example, differences between the structure and incentivesaffecting the broadcast media and those affecting the print media. material withvisual impact will be especially appealing for television. there also will bedifferences within segments of the different media. the focus and approach ofscience magazines, for example, differ from those of straight news magazines.national newspapers differ from regional or local newspapers. despite thesedifferences, however, the overall impact of the incentives and influences onreporters, editors, producers, and so on is more similar in the various media thandifferent.another characteristic of the press worth understanding is that mostreporters deal with news, not education (sandman, 1986). it is usually the eventsthat make something newsworthy, not the issuescommon misconceptions about risk communication104improving risk communicationcopyright national academy of sciences. all rights reserved.or principles involved. news reporters seldom want to know the insandouts ofrisk assessment, how sure the experts are, or how they found out. they want toknow about the number of people affected; the gravity of the consequences; andthe cost of damage, repairs, or remedies. they pay attention to the vividness withwhich these can be presented.feature stories, such as those found in sunday editions or specialbroadcasts, can go into much greater depth and offer more complex treatments.specialist reporters also pay attention to newsworthiness, imagery, and so forth,but they tend to go into greater depth in laying out the background and some ofthe underlying factors that bear upon the events.most journalists care about accuracy and objectivity. often the onlyoperational definition of objectivity for journalists is balance (sandman, 1986).they are seldom experts in the topics they cover. they cannot, as a result,determine for themselves what is true. they can only try to present the conflictingclaims fairly. and because their job generally is reporting events rather thanissues, they get most of their information from people who are directly involvedin the event and only occasionally seek out uninvolved experts for advice. somejournalists, especially at the regional and local levels, also emphasize thereactions of ordinary people. they present the events of concern, theconsequences and their importance, any conflict about outcomes orresponsibilities, and the response of ﬁthe man on the street.ﬂ this helps peopleinterpret the news in terms of themselves, their families, and their neighbors.journalists may seek out those with conflicting claims about the events inthe news. in striving for a balanced coverage, they often attempt to identifyextreme positions about the events or issues. not being able to assess whichpositions have been given greater credence among the community of experts, theyattempt to discover the range of views. although they may not present the mostextreme positionsšthe ones and sevens on a range from one to sevenšthey willtypically look for individuals expressing welldefined positions that bracket themiddle of the range of relevant viewsšthe twos and threes and the fives andsixes. positions that clearly differ in this way are attractive to the journalistbecause they define the range and because their juxtaposition sharpens the dramaand heightens interest.to be sure, there have been instances in which media coverage has favoredone extreme, such as the television network that showedcommon misconceptions about risk communication105improving risk communicationcopyright national academy of sciences. all rights reserved.a skull and crossed bones in the background whenever a reporter spoke aboutethylene dibromide (edb) (sharlin, 1987). but there is also some evidence thateven in events with massive attention and media coverage, the news media seekbalance. an extensive content analysis of media coverage of the nuclear industryaccident at three mile island found the balance between supportive and negativestatements to be, if anything, more reassuring than alarming (report of thepublic's right to information task force, 1979; stephens and edison, 1982).the attraction of decisive answersthe public often appears to want decisive, clearcut determinations of riskand descriptions of the appropriate control measures, especially when the choicesthey face appear to be simple dichotomiesša product can be used or not used, anincinerator built or not built. this response is based on fundamentalpsychological mechanisms. most people prefer simplicity to complexity inmatters outside their own field of expertise. in addition, most people are too busyto spend much time on any particular topic, and some find it hard to understandwhy information about risk cannot be put in concise, decisive terms.unfortunately, one seldom knows how often or in what mixes these varioussituations obtain.sometimes, however, people prefer to have the options laid out for them andto be given the choice of selecting the one they prefer. this is most commonwhen the risk control measures require action by the individual. examplesinclude using seat belts, choosing among medical treatments, and changingsexual practices to curb the spread of contagious diseases such as hepatitis oraids.several things may influence people's preference for decisive or ambiguousinformation: the degree to which they as individuals exercise control overexposure or remediation, the importance they attach to the issue, and theirtendency to be risk averse or risk seeking. that different segments of thepopulation may prefer decisive or equivocal information about a particular riskcan make the job of the risk communicator more difficult. it may even be thatindividuals prefer different types of information at different times during thecourse of discovery, analysis, and control of a hazard.this chapter has discussed some of the more important misconceptionsabout risk communication. the next chapter addressescommon misconceptions about risk communication106improving risk communicationcopyright national academy of sciences. all rights reserved.directly what we believe to be the most important problems confronting thepractice of risk communication.note1. these and other relevant terms are defined in a list given in appendix e.common misconceptions about risk communication107improving risk communicationcopyright national academy of sciences. all rights reserved.6problems of risk communicationin this chapter we address what we consider to be the principal problems ofrisk communication. first we describe problems deriving from the structure of thepolitical and administrative system. these are problems for which little can bedone by those involved in risk communication beyond understanding them. theymust be confronted and accommodated, since they cannot be done away with.next we describe problems of risk communicators and recipients. theseproblems, in contrast, are much more amenable to improvement or solution. theproblems of these two groups are presented together because many things areproblems for risk communicators because they are problems for the recipients ofrisk messages. for example, the risk communicator needs to pay attention to theunderstandability of risk messages because most recipients have difficultycomprehending the technical terms typically found in risk assessments and othertechnical analyses.problems deriving from the institutional andpolitical systemas we have seen, scientific and technical information is of centralimportance to decisions about how to respond to risks and thus is an importantelement in risk messages. but risk management decisions also take place as partof a democratic process, and risk analysisproblems of risk communication108improving risk communicationcopyright national academy of sciences. all rights reserved.is only one of several sources of relevant information. furthermore, politics can,and often does, assert control over decisions otherwise delegated to experts. theintrusion of politics can result in considerable frustration to risk managers as wellas to others involved in the process. it is thus important to consider problemsposed by the institutional and political system for risk communication.for the most part the problems of the institutional and political system arepart of the context within which risk managers and risk communicators operate.even though these problems are largely beyond the ability of the principals in riskcommunication to affect, they nevertheless can have considerable impact onactions and events.legal considerationsrisk communicators may be constrained because legal considerationsinfluence the options available to risk managers and therefore the content of riskmessages. several kinds of legal provisions may provide such constraints,including (1) statutory mandates, (2) liability, and (3) informed consent andﬁrighttoknowﬂ requirements.statutory prescriptions and proscriptionsstatutory language may, in effect, force the risk manager to take certainkinds of actions, some of which have important consequences for the content ofrisk messages or their dissemination. this is perhaps most obvious with respect tounits of the public health service. the major goal of the centers for diseasecontrol (cdc), for example, is to lead public health efforts to preventunnecessary disease, disability, and death. the cdc pursues this goal throughprograms aimed at prevention and control of infectious and chronic disease andof disease, disability, and health associated with environmental and workplacehazards (department of health and human services, 1986). these programsinclude not only regular publications such as the morbidity and mortality weeklyreport (mmwr) but also emergency advisories. cdc officials are often quotedin the news, and their statements can have considerable impact. cdc's concernabout longterm contact with soil in the times beach, missouri, area was animportant factor in the government's decision to purchase homes and permanentlyrelocate the residents. cdc's mandate to lead public health efforts thus goes along way toward establishing the tone and approach of risk messages emanatingfrom the cdc.problems of risk communication109improving risk communicationcopyright national academy of sciences. all rights reserved.such prescriptions and constraints and their impact on action may in their turnhave a strong impact on whether the public views the agency as an advocate andon the credibility of the organization with respect to public health issues.liabilityone principal reason that legal constraints constitute a problem for riskcommunication is that these considerations may make difficult or impossible thecrafting and presenting of messages that effectively address the issues that maybe most relevant to the intended recipients of the message. for example,following the 1985 release of aldicarb oxime from its plant in institute, westvirginia, union carbide had to decide about what information to make publicabout the accident (coppock, 1987). communications and community relationsexperts usually advise making available everything that is known about anaccident as quickly as possible, in terms that laypeople can readily understand.legal advice is almost always exactly the opposite: give out as little informationas possible so as to avoid providing ammunition for use in court. given theprevalence of large court awards in product liability and toxic exposure cases,concern with liability is in the minds of many business people. the final messageprobably involves a compromise between these perspectives.informed consent and righttoknowissues of informed consent have changed the way the health professioninteracts with patients. attempts are made, for example, to hold physicians tofairly stringent standards in obtaining consent prior to initiating experimentaltherapy. but ﬁrighttoknowﬂ issues are having equally important impact in manyother areas. employees are to be informed about the hazards of the materials theyhandle under occupational safety and health administration rules, communitiesare to be informed about inventories and emissions of hazardous substancesunder the community righttoknow provisions in title iii of the superfundamendments and reauthorization act of 1986, and california's proposition 65provides for provision of information about any product containing carcinogensor teratogens. the overall effect of such developments is that there are manymore legal requirements that result in the preparation and dissemination of riskmessages than in the past.problems of risk communication110improving risk communicationcopyright national academy of sciences. all rights reserved.sharing of powercommunicating with citizens about risk issues can increase their desire toparticipate in or otherwise influence decisions about the control of those risks.these demands can change the dynamics of the situation for the risk manager, therisk communicator, and the citizen. the motivation for citizen involvementbecomes even stronger when a decision process appears to result in an outcomewith which the individual disagrees. the interests of citizens and their motivationto participate can be especially problematic when the implementation of riskcontrol measures is necessarily decentralized and local preferences precludesolutions in the broader interest.the sharing of power is a central facet of representative democracy. citizenstransfer decisionmaking power to elected officials. however, citizens have theresponsibility to hold both the legislative and the executive branches ofgovernment accountable. to exercise this responsibility and judge the delegationof authority, citizens need information. demonstration of this accountability isone of the important functions of risk communication.holding government accountable means, in a basic sense, ensuring thatgovernment policies and actions correspond to public preferences. the difficulty,of course, is aggregating across the preferences of the many people involved.most people believe they have a right not to be subjected by others tounreasonable risks. some people believe not only that risk to life and healthshould be minimized but also that three kinds of unfairness should disqualify,say, siting a hazardous waste facility: (1) imposing costs on those who have notvoluntarily agreed to bear them, (2) imposing costs on those who opposeavailability of and avoid use of the products and services generating the hazard,and (3) imposing disproportionately large burdens on those who benefit least(simmons, 1984). when people believe that any of these three hold, they mayfeel imposition of a hazardous waste site to be unfair regardless of the processesused to derive that particular site. such conflicts are at the core of many instancesof ﬁlocally unwanted land usesﬂ (lulus). local preferences often run counter tosolutions that would otherwise seem to be in the broader public interest. here riskis only one part of the problem, and thus risk communication per se cannot beexpected to resolve all the issues.communication research suggests that risk messages will be more easilyunderstood when the risk communicator not only incorporates language familiarto the recipients but also genuinely respects andproblems of risk communication111improving risk communicationcopyright national academy of sciences. all rights reserved.incorporates their views (covello et al., 1987b). this perspective suggests thateffective risk communication should begin before important decisions have beenmade. if all the important choices have already been determined, it will bedifficult to reflect the views of the recipients. risk messages will become littlemore than attempts to ﬁsellﬂ a predetermined conclusion, which may createconsiderable alienation among the intended recipients. but as suggested in theabove discussion, people may reject the attempt no matter when it occurs if theyare unwilling to compromise their position. the risk manager may thus face adifficult task in seeking advice from people but excluding them from the resultingdecision.people naturally want to see their views affect the outcome, and they mayhave difficulty differentiating the risk communication process from the riskmanagement process. the american political culture puts a premium onprocedures that offer a wide variety of interest groups and citizens the opportunityto participate in decision making (melnick, 1988).fragmentationrisk control decisions can be made or influenced by several differentpolitical actors. at the federal level, congress, the executive branch, and thecourts all shape health and environmental regulations. despite the dominant roleof the federal government, state and local governments also remain important.this fragmentation may make communicating about risks more difficult becauseof dispersion of responsibility, incentives for each actor to gain as much leverageas possible from the limited portion he or she controls, and difficulty indetermining who is responsible for the eventual outcomes.dispersion of responsibilityfragmentation of risk control decisions derives from a central feature of thestructure of american political institutions: dispersion of power. a basic tenet ofthe american political system is separation of powers, but power is also dispersedto a remarkable degree. for example, one source claims that edmund muskie,though only chairman of a senate subcommittee, had at least as much influenceon environmental policy from 1969 to 1979 as richard nixon, jimmy carter,william ruckelshaus, or douglas costle (melnick, 1988). and severalenvironmental groups, most notably the natural resources defense council andthe environmental defense fund, haveproblems of risk communication112improving risk communicationcopyright national academy of sciences. all rights reserved.used their success in litigation to become major players in national policymaking(melnick, 1988).in the united states each level and branch of government provides access to avariety of groups. thus corporations, trade associations, labor unions,professional associations, intergovernmental lobbies, and environmental groupsall influence regulation and its implementation. to the extent that individuals andorganizations participate, they can also be held responsible for the overalloutcome.dispersion of responsibility can lead various executive agencies to takedifferent positions with respect to the same issue. the u.s. department ofenergy, for example, views the hazards associated with radioactive contaminationof groundwater differently than does the epa. officials of the two agencies sayquite different things about the risks involved in specific instances ofcontamination in idaho. differing positions can also be found within differentparts of the same government organization. this derives in part from theorganization of large bureaucracies into separate divisions but also in part fromthe belief that separation of power yields greater benefit than cost.when fragmentation leads various parts of government to different positionsor approaches with respect to the same risk, it can lead to problems for riskcommunicators.incentives to gain leveragethe extensive dispersion of responsibility among parts of government meansthat there are often jurisdictional conflicts and overlapping responsibilities amongdifferent governmental organizations. the existence of these overlaps can providethe opportunity for particular organizations to apply leverage beyond theirorganizational boundaries. the ﬁcrisisﬂ involving groundwater and contaminationof foodstuff with ethylene dibromide (edb) is one example. at the federal levelthe epa, the food and drug administration, the department of agriculture, andthe occupational safety and health administration all had responsibility forsome aspect of exposure to edb. they had more or less reached agreement as tothe handling of the pesticide. but action by state government agencies in floridaand massachusetts brought edb to the public attention and forced changes in theresponse from the federal government. the massachusetts edb team leader, dr.havas, summed up the stateproblems of risk communication113improving risk communicationcopyright national academy of sciences. all rights reserved.department of public health experience like this: ﬁit was a success –particularlyhow quickly we got edb out of the massachusetts food supply. what we diddrove edb out of the food supply for the entire nation–not just formassachusetts–everybody got the benefitﬂ (quoted in krimsky and plough,1988).difficulty in determining responsibility for outcomesdispersion of responsibility and the actions of various individuals andorganizations to obtain as much leverage as possible can mean that the recipientof risk messages has a difficult time knowing exactly which organizations havejurisdiction over the hazard in question. various organizations may havecompeting aims or goals with respect to the hazard in question and the control ofthe associated risks. the resulting confusion can constitute a problem for the riskcommunicator because he or she needs to clarify the organizationalresponsibilities as well as the risk involved. the fragmentation of risk controldecision making thus contributes to the difficulty of communicating about risks.imbalanced access to informationif the group of people that a risk communicator is trying to reach thinks thatthe system for generating information relied upon by that source does notconsider its concerns, it may reject the information from that source as a basisfor decisions about risks. rejection of its information can be a considerableproblem for a risk communicator. organizations disseminating risk messagesneed to be aware of the effects of uneven access to information by those affectedby or requesting the organization's action.information is not free. it is expensive to develop empirical data, and thereare not enough research funds to examine all questions that might be relevant toparticular issues. thus the amounts of information about all considerationsrelevant to such decisions are unequal and may therefore introduce imbalance intothe information base for risk decisions.government and industry spend large amounts of money on research. thisnot only encourages their concerns to be reflected in research projects but alsoestablishes patterns of information flow and interactions that reinforce this effect.environmental groups or trade unions do not have equal amounts of money tofund researchproblems of risk communication114improving risk communicationcopyright national academy of sciences. all rights reserved.and may be at a disadvantage in justifying their positions in conflicts aboutregulatory decisions or other risk management strategies. however, they canoften serve the valuable function of criticizing the information developed byother organizations.science tends to be conducted in institutional settings with strongincentivesšthe amateur scientist was, for the most part, a character of the lastcentury. researchers at universities and other independent research facilities aresubject to powerful influences, both through budgetary constraints and the need topublish their results in peerreviewed journals. there never will be enoughresearch funds to pursue all questions relevant to particular hazards. funds that doexist may be inappropriately allocated. issues that are popular in particulardisciplines may thus introduce imbalance into the information base for riskdecisions.even when information has been created, it may not be equally accessible toeveryone. the research community can be reached more easily by those withresources to support, follow, and interpret its activities.local citizens' groups are likely to have even less contact with relevantresearch communities. they will probably be unfamiliar with the language ofscience and may not formulate their questions in ways that scientists can use.this may detract from the usefulness of public hearings and other settings whereexchange might take place between the providers of information and concernedcitizens.systematic interests and biasesthose most strongly motivated to communicate about risk are often alsothose with the strongest interest in the decisions. so whenever a personal or asocial decision may affect interested groups, conflicting messages that reflect theconflicting interests may be expected. the beliefs of risk communicators, andtheir interests, create incentives to slant or even distort or misrepresentinformation. this can skew messages in many different directions on the sameissue.the american cancer society and the tobacco institute offer conflictingmessages about the health effects of smoking, the national agriculturalchemicals association and the national farmworkers union are in conflict aboutthe health risks of pesticides, and the sierra club and the edison electric institutetake different positions about the dangers of acid rain. the reasons for thesedifferences may be complicated, but smokers contemplating quitting, farmersproblems of risk communication115improving risk communicationcopyright national academy of sciences. all rights reserved.considering the adoption of integrated pest management, and citizens takingpositions on the regulation of air pollution are confronted with making judgmentsabout the risks by weighing messages from obviously interested sources andmessages from other sources whose biases are not so obvious.consider the experience with messages about aids. fearing the response tothe epidemic of a traditionally homophobic society, various groups representingthe gay community have at different times underplayed and exaggerated the riskof aids (shilts, 1987). initially, the gay community denied that there werespecial risks associated with homosexual practices and sought to protectbathhouses and other gathering places from interference by public healthofficials. as the toll has increased, the tendency has been to claim rapid spread ofthe disease among heterosexuals. gay community groups tended to describeaids as a societal affliction not concentrated in an isolatable and stigmatizedgroup. when everyone is a potential victim, both compassion and resources arelikely to become more plentiful.for their part, blood banking organizations have consistently sought tounderplay the risks of aids contracted through transfusions. a prime motivatingfactor has been their need to maintain an adequate supply of blood for the nation.if blood is linked to a new and highly dangerous disease, the public might, as hashappened, curtail donations in a mistaken belief that there is a risk to donors.until 1982 the blood banking community rejected epidemiological evidence thataids could be transmitted through banked blood and told the public the bloodsupply was ﬁsafe,ﬂ when all that was known was that the risk of aids had notbeen convincingly demonstrated. the overriding concern was a desire to reassurethe donating public (holland, 1987). the blood banking community continues toclaim that the blood supply is ﬁas safe as it possibly can be for aids,ﬂ althoughsome recommend that additional screening procedures be used (holland, 1987).the point is that on matters of public controversy risk messages tend to beflavored by the positions taken by the sources of the various messages.moreover, these biases are not necessarily obvious to those who receive themessages and use them to make personal decisions or to inform their politicalpositions.problems of risk communication116improving risk communicationcopyright national academy of sciences. all rights reserved.problems of risk communicators and recipientsexaminations of risk communication have tended to focus on thepreparation, presentation, and transmission of messages about the nature of risksand risk reduction measures and on their receipt and interpretation by theintended recipients (covello et al., 1987b; davies et al., 1987). most of thisattention has been directed at the problems of the individual or office preparingand disseminating risk messages. here we describe many important problems therisk communicator will face in these tasks as well as the special problems of therecipients of such messages. we also examine aspects of the interactions betweenthe risk communicator and other groups: other people within his or herorganization, other groups or organizations, and the intended recipients.one of the central aspects of risk communication is that risk messages arenot created and transmitted in a vacuum. the policy, administrative, or politicalarena within which the communication process occurs is an important influenceon what eventually happens. we describe problems that derive from within therisk communicator's organization or group as well as those that characterize thebroader setting of interactions with other individuals, groups, and organizations.debates between risk managers and experts, or between experts andmembers of the informed and involved public, are often poorly understood by thegeneral public. although such debates are not particularly well attended to, theyare also not ignored. risk debates often are interpreted by the general public intwo ways: the world is a dangerous place, and risk managers either do not knowwhat they are doing or do not understand what they are supposed to be doing. inother words, risk debates often generate fear, which is unpleasant and generallynot helpful for making decisions. neither heightening of public fear norheightening of public distrust of risk management can be considered constructiveas such. but even though risk communication may engender at least some fears inthe public regardless of content and procedural safeguards, we feel that it is anecessary and important part of risk management in a democracy.the risk communicator attempts to present information in such a way thatthe intended recipients will receive and attend to its message. usually, the riskcommunicator presents this information in the hope of influencing the recipients'attitudes or actions. but the recipient may not particularly care about the issuesraised by aproblems of risk communication117improving risk communicationcopyright national academy of sciences. all rights reserved.message. many messages are likely clamoring for his or her attention, and thoseabout the same issues are likely to be contradictory. the recipient is faced withthe difficult task of making sense out of a confusing mess of information frommany different sources.to some extent, the problems of the recipient of a risk message are themirror image of those of the risk communicator. this is the reason we addressthem together. the communicator worries about credibility because the recipientjudges messages on the basis of the reputation of the source as well as the contentof the message. risk messages not only need to be, but must also appear to be,accurate and responsible representations of the issues because the skepticalrecipient will be on the lookout for incompetence, inaccuracy, misrepresentation,and deceit. similarly, the communicator tries to be clear and easilyunderstandable because most recipients have difficulty with complex technicalmaterial.occasionally recipients of risk messages become risk communicators. whenan individual becomes motivated to join or create a group with the aim ofinfluencing decisions about risks, he or she generally disseminates oral or writtenmessages to others. in these situations that individual will experience not only theproblems of interpreting risk messages from other sources but also many of theproblems of risk communicators. the risk communication process then becomesinteractive in its most fundamental sense.in examining the problems of risk communicators and risk recipients, wedescribe several things that make easily understandable risk messages difficult toachieve. we present general conclusions about mistakes to be avoided. theattributes of risk communicators and risk messages we identify and their impacton the risk communication process are derived from this general research baseand our collective judgment. the research is much weaker, however, in givingguidance about what will work in specific situations. the only way to be sure isto pretest communications materials with representatives of the intendedaudience.establishing and recognizing credibilitylack of credibility alters the communication process by adding distrust andacrimony. the most important factors detracting from the credibility of a riskmessage relate to the accuracy of the message and the legitimacy of the processby which its contents were determined, as perceived by the recipients.problems of risk communication118improving risk communicationcopyright national academy of sciences. all rights reserved.the perceived accuracy of a message is hampered by the following: real orperceived advocacy of a position not consistent with a careful assessment of thefacts; reputation for deceit, misrepresentation, or coercion; previous statementsor positions that do not support the current message; selfserving framing ofmessages; contradictory messages from other sources; and actual or perceivedprofessional incompetence and impropriety.the perceived legitimacy of the process by which the contents of themessage were determined depends on the following: legal standing, justificationof the communication program, access of affected parties to the decisionmakingprocess, and fair review of conflicting claims.real or perceived advocacy of unjustified positionsperhaps the most critical element of credibility for a source is the degree towhich intermediaries and the ultimate recipients of the risk message believe thatsource to be justified in the position reflected in the message. as we have alreadypointed out, it is extremely unlikely that the recipients will be in the position tojudge the accuracy, balance, and fairness of a risk message from the content ofthat message alone. one result is that recipients tend to judge the messenger aswell as the message. the reputation of the source, in terms of past record withregard to accuracy of content and legitimacy of the processes by which it isdeveloped, will be an important influence on the way recipients view particularmessages.it is important that an organization ensure that its positions are technicallycompetent. an unfortunate example of the failure to do so involves the epa andits decision to conduct a chromosome damage study of the residents of lovecanal in new york (levine, 1982). at the outset the decision was made,apparently by epa lawyers, to restrict the number of people studied because ofthe high cost of studying chromosome damage (davis, 1987). individuals weretherefore selected to maximize the likelihood of damage, following the reasoningthat comparison of what should be a highrisk group to a group from anuncontaminated neighborhood should make it easier to determine whether alarger study would be justified (levine, 1982). however, funds were furthercurtailed, so that a total of only 36 cases could be included and the control groupfrom the uncontaminated neighborhood was eliminated. this ensured that theresults would be difficult to interpret, which was recognized by the scientistconducting the study. given the extreme emotionsproblems of risk communication119improving risk communicationcopyright national academy of sciences. all rights reserved.surrounding the events at love canal, the result was considerable controversy.five independent reviews of the chromosome study were submitted to the epa,two requested by federal agencies and three by the scientist conducting the study.all emphasized the limited inferences that could be drawn due to the lack of acontrol group. the reviews commissioned by federal agencies criticized theinterpretation of the data on chromosome damage in the study, while thoserequested by the scientist conducting the study were more favorable concerningthe data interpretation. although this example is extreme, scientific studies aresubject to strict examination of their methods of data collection andinterpretation. this examination is usually severe when the studies are used tosupport controversial public policy decisions.reputation for deceit, misrepresentation, or coercionperhaps the most difficult problem for credibility is a past record of deceit,misrepresentation, or coercion. for example, as was acknowledged to us byofficials from the u.s. department of energy (doe), one of the biggest problemsconfronting the civilian radioactive waste program at doe is the legacy of theatomic energy commission and even earlier government programs (isaacs,1987). the attribution during the 1950s of fallout in st. louis to russian sourceswhen in fact it was known to come from tests in nevada was a blatant abuse ofpublic trust, the repercussions of which the doe must live with today. when theresponsible government organizations have been proven to lie, it is not surprisingthat people want independent verification. one year of being honest with thepeople is not enough. given the knowledge today of the cavalier treatment offacts concerning its activities in the past and the tremendous opportunity foruncertainty to enter its analyses and for its analyses to be skewed, the doe facestremendous credibility difficulties. even the slightest indication of less thancomplete candor and honesty will probably lead many people to reject whateverposition the agency takes. given the highly politicized issues that doe's programaddresses, this legacy adds to an exceedingly difficult challenge.the situation is somewhat different for nongovernment organizations.private corporations, advocacy groups, and private citizens are commonlyexpected to interpret the facts of the situation in ways that support their aims andgoals. this is part of the reason corporations and their messages are distrusted.despite the difficultyproblems of risk communication120improving risk communicationcopyright national academy of sciences. all rights reserved.that recipients of risk messages have in recognizing misleading or deceitfulmessages, a reputation for consistently bending the facts to fit one's purposes willundermine one's credibility to many recipients, although one's direct constituencymay become more supportive.contradiction of previous positionsestablishing and defending credibility is difficult when the messagerepresents a departure from previous positions. in large part credibility derivesfrom the demonstration over time of consistent competence and fairness. bothscientific incertitude and changes in policy can serve to undermine credibility tothe lay public. the necessity of correcting mistaken statements or positions canundermine credibility with the public. care must be taken to demonstrate why theinterpretation of scientific or policy conclusions has changed.the rapidity with which new scientific findings about the aids virus thatcounter or revise previous positions are being presented undermines thecredibility of the experts. at least one response to the rapidfire changes inestimates and contradictory conclusions about heterosexual transmission of thehiv virus is to conclude that the experts really do not understand what ishappening and that every imaginable precaution is thus justified.inconsistencies can also result from changes in administration, as was clearlyillustrated by the treatment of formaldehyde by the occupational safety andhealth administration (osha). in december 1980, osha, acting in conjunctionwith the national institute for occupational safety and health, released a currentintelligence bulletin recommending that formaldehyde be considered a potentialcarcinogen and that appropriate controls be implemented to reduce employeeexposure to the chemical. in march 1981 the new assistant secretary of laborfor occupational safety and health rescinded osha's sponsorship of thebulletin. in january 1982, osha denied a petition by labor unions for anemergency temporary standard to reduce formaldehyde levels in the workplace(ashford et al., 1983). such reversals of position based on the same evidence canonly reinforce the appearance of inconsistency and undermine the credibility ofthe source.selfserving framing of messagesthe leeway that exists in the collection of data, its interpretation, and thefinal crafting of messages provides ample opportunityproblems of risk communication121improving risk communicationcopyright national academy of sciences. all rights reserved.to present information in ways that support the position of the organization morestrongly than the evidence itself might justify. such framing of messages neednot involve direct deception or lying. for example, on august 27, 1986, thewashington post reported on the soviet government analysis of the potentialhealth effects of the chernobyl disaster under the heading, ﬁchernobyl reportsurprisingly detailed but avoids painful truths, experts sayﬂ (smith, 1986). theﬁpainful truthﬂ was that the disaster may result in 35,000 to 45,000 cancer deathsin the soviet union and that ﬁas many as 90,000 people could be affected by therecent explosion.ﬂ but the soviet report said that fatalities would be ﬁless than0.05 percent in relation to the death rate due to spontaneously arising cancer.ﬂsince this percentage works out to be 35,000 to 45,000 premature deaths over thelifetimes of the people exposed, both reports are equally accurate. but the twomessages stimulate very different responses in most people.the incentives to slant the presentation of information to support an issueone believes to be important can be strong. but in order to strengthen theircredibility, public service organizations, and especially those in government,must resist this temptation.contradictory messages from other sourcesthe adversarial nature of the american regulatory system is often cited asone of its strengths. but it also can help undermine the credibility of sources ofrisk messages. parties with potential gain or loss are motivated to develop thebest evidence and strongest arguments for their respective positions. when this isaccomplished according to strict rules of evidence and scientific review, it can beexpected to produce a reasonably complete picture of the issues in question.however, it also encourages competing interpretations of the evidence wheneverthere is uncertainty in the data, in applicable methods, or in the models forinterpreting empirical results. sometimes science is claimed to support all sidesof a conflict about risk.conflicting messages also can derive from sources that are usually notassociated with different ﬁsidesﬂ in an adversarial situation. for example, stateofficials in florida and massachusetts sent clear signals that ethylene dibromide(edb) should be of serious concern at exactly the same time that the epaadministrator was attempting to reassure the public (krimsky and plough, 1988).problems of risk communication122improving risk communicationcopyright national academy of sciences. all rights reserved.in most situations the risk communicator will have to deal with conflictingmessages from other sources. contradictory messages can be a central part of theissue as seen by the public or may be a relatively minor side point. if thecommunicator does not deal with these effectively, it can undermine his or hercredibility.the extremes the recipient needs to watch out for are intentionalmanipulation and outright dishonesty. although it is generally difficult for thelayperson to determine from the message itself when it is manipulative ordishonest, some are so poorly crafted as to be blatantly miscreant. less extremeinstances are exceedingly difficult to identify. one strategy is to see how otherindividuals and organizations with a stake in the issue respond to the positionsand statements of that source. the other parties in an issue have incentives to sortout misrepresentations and unsupported findings of their opponents and makethem known.the difficulty of determining the degree to which a risk message reflectsadvocacy of a particular position can be illustrated by reference to the experiencewith edb in the early 1980s (krimsky and plough, 1988). environmentalistscritical of the basic federal regulatory treatment of pesticides found the debate onedb to be an excellent opportunity to press their argument. they were joined bysome state officials, who used the issue as a lever to force the federal agencies toact. industry and trade associations thought the issue illustrated the need to weigheconomic considerations in the regulation of hazardous chemicals. the epa hadto both defend its previous decisions with regard to edb and quickly evaluatenew data on exposure levels. the news media attempted to package all this in away that would be newsworthy. and the recipient had to weigh all these positionsagainst each other.another problem for the recipient of risk messages is to differentiateconflicts based on scientific disagreement about the facts or their interpretationfrom conflicts based on advocacy of policy aims. sometimes this requiresdetermination of the extent to which the facts reflect advocacy. if the source hasidentified its aims and purposes, this task can be made easier. unfortunately,many sources are much more concerned with the outcome of the issue underquestion than with helping the recipient understand all its insandouts. in fact,some sources deliberately confuse such questions and obfuscate rather thanclarify because such actions contribute to the possibility that their position mayhold sway. the recipient of risk messagesproblems of risk communication123improving risk communicationcopyright national academy of sciences. all rights reserved.may thus have considerable difficulty separating scientific conflicts from policyconflicts.one of the effects of ﬁdueling expertsﬂ may be to reduce the importance ofexpertise in the minds of the recipients of risk messages. if scientific evidence isinconclusive, then why not use whatever factors seem to suggest a solution? oneresult of such an approach would probably be to shift the nature of the problemdefinition away from the aspects of risk that have been quantified to other factorsthat may or may not be measurable. this has probably happened with respect tonuclear power. the technical questions are no doubt less critical to most peoplethan their beliefs about the overall impact of increasing or decreasing reliance onnuclear reactors.professional incompetence and improprietya major element helping determine the credibility of risk messages is theperceived competence of the individual or organization concerning the subjectunder question. an individual with special training about the phenomena involvedis often accorded greater credibility than someone whose training is less relevant.the statements of a physician, for example, might be given greater credence withrespect to a public health question than those of a dentist or health economist,even though each is a health professional. similarly, organizations enhance thecredibility of their messages about technical issues when they have professionalstaff with training in the areas covered. two strong criticisms of governmentagencies are that they do not have sufficient staff with the necessary professionalcompetencies to fully and completely understand the phenomena they regulateand that the understanding that the professional staff does have is not shared bythe nontechnical personnel at the apex of the organization.private organizations, especially business corporations, are more oftensuspected on account of propriety than competence. many people believe thatcorporations have, or hire, the best expertise available but that these expertspresent only information that is in the interest of the corporation. since thedecisions of private organizations are not subject to the constraints of due processas are those of government, there is a strong presumption by many that themessages of private organizations emphasize only the information they believe tobest serve their interests. similar descriptions apply to many public interest orpublic advocacy groups.problems of risk communication124improving risk communicationcopyright national academy of sciences. all rights reserved.there are thus strong incentives for an organization disseminating riskmessages to be as open and clear as possible about the way it gathers andinterprets information. this is especially true if it incorporates scientific peerreview or other technical review procedures. demonstrating the professionalcompetence of the organization and the propriety of its procedures will likelyenhance the way its messages are received.legal standingan important influence on perceived credibility is legal standing andinvolvement in the issue under question. being lawful or corresponding to thedictates of the law is a fundamental component of legitimacy.as we have indicated above, a special sense of responsibility attaches togovernment. government officials and agencies are expected to act in the publicinterest. at the most fundamental level the legitimacy of a federal agency tospeak to an issue derives from its statutory mandate and from the exercise of dueprocess under the administrative procedures act. determining this is relativelystraightforward, although there may be questions of jurisdiction among differentagencies. for example, there may be disagreement about whether the epa or theosha should regulate airborne toxic contaminants in particular situations.on occasion, however, government agencies are expected to act and, ifappropriate, interact with the public even when the topic is not strictly withintheir statutory responsibilities. for example, the epa feels obliged, correctlymany would agree, to deal with radon exposures in homes even though radon isnot strictly within its statutory responsibilities. the obligation of public officialsand organizations to act in the public interest even when their charge is less thanperfectly clear can make the risk communicator's job more difficult. when theepa publicizes the standards it sets, the reasons for them, and the penalties forviolation for topics within its statutory responsibilities, there is little objection.but some people question the agency's justification for disseminating messageswhere due process has not given the agency that responsibility.establishing legal standing and involvement for nongovernmentalindividuals or organizations is more complicated. whether a nongovernmentalentity is ﬁlawfulﬂ in the sense of corresponding toproblems of risk communication125improving risk communicationcopyright national academy of sciences. all rights reserved.statutory requirements is less important than, for example, the concept ofdetermination of ﬁstandingﬂ in court, that is, of who has the right to bring suit.this concept includes notions of being materially affected by the outcome orbeing a representative of those who at least potentially are so affected. beingmaterially affected usually ensures the right to have a say in issues of publicchoice. in addition, personal involvement in or knowledge of the activity orevents under question generally increases interest in what that individual has tosay about that question. such justification is used with respect to environmentalinterest groups. finally, there is the expectation that any citizen should have theright to speak to any public issue.of course, being party to the creation of a hazard automatically grantsstanding in determining how to control that hazard. there was no question, forexample, that union carbide should be heard from following the release ofmethyl isocyanate from its plant in bhopal, india. needless to say, however,most people expect that an organization that contributes to the creation of ahazard will make statements they believe will be in that organization's own bestinterest.justification of communication campaignspeople may view what it means to be a responsible communicatorsomewhat differently for government and nongovernmental organizations. manypeople feel that government agencies should never ﬁadvocateﬂ in the ordinarysense, that the job of public officials is to determine the factual situation, identifythe impacts on affected parties, and lay out the options. the selection of what is tobe done should be left to elected bodies or to the relevant individuals andorganizations. in chapter 4, however, we discussed the acceptability of influencein risk messages. we observed there that there are no clear lines distinguishingtechniques that are appropriate from those that are not and that it is important tobe able to demonstrate that the effort derives from a social decision supportingthe communications program.most people would agree that an important part of the activities of federalagencies is communication about the standards they set, the reasons for them, andthe penalties for violations. however, it is not always obvious when governmentought to undertake programs of risk communication and how far it should go inpersuading theproblems of risk communication126improving risk communicationcopyright national academy of sciences. all rights reserved.populace to undertake particular actions. for at least some programs there areserious questions about the conditions under which persuasive riskcommunication should be permitted.the central issue here is the compatibility of governmentsponsoredprograms with individual autonomy and related values (faden, 1987; faden andbeauchamp, 1986). in the first place the basis for the government promotingcertain lifestyles over others is not clear. with major problems that affect a largeportion of the populace and inflict substantial monetary and social costs, mostpeople are prepared for government agencies to be proactive and persuasive. butexactly what makes an issue a major problem justifying advocacy on the part ofgovernment is not clear.this, of course, is not to claim that government should never conduct riskcommunication programs. some problems addressed by collective programs canbe most efficiently dealt with by the affected individuals. but some question howthe epa can judge the risk of household radon exposure to be sufficient towarrant an extensive campaign of communication to homeowners, while that riskis not deemed sufficient to warrant establishment of exposure standards. oneproblem that may confront a government source is to justify conducting a riskcommunication effort rather than devoting its effort directly to reducing the riskin question. however, this can usually be justified in terms of improvedefficiency in implementation of risk reduction programs.people are generally more tolerant about communication from privateenterprises or interest groups than from government agencies. theseorganizations are not expected to exhibit the same impartiality as government,and their attempts to present persuasive information are not viewed with the samesuspicion.access of affected parties to the decisionmaking processalienation of citizens due to the difficulty of getting government officials tolisten to them or due to the judged inappropriateness of the officials' response hasbeen repeatedly described as a major motivation of individuals who have becomeactive in controversial issues. the homemaker/activist/media spokespersonappears frequently in community disputes over environmental issues (levine,1982; mazur, 1987; spain, 1984).problems of risk communication127improving risk communicationcopyright national academy of sciences. all rights reserved.an important part of the legitimization of government activities that hasbeen codified in the administrative procedures act and elsewhere concernsguaranteeing the affected parties access to regulatory decisions. government is,however, sometimes criticized for being too passive in this respect, for notactively seeking out those affected and informing them about proposed actions.this has become more obvious with respect to siting decisions and cleanup ofhazardous waste facilities, where local hearings and extensive communityinvolvement are increasingly necessary.nongovernmental organizations are not under the same obligation to grantthe affected parties access to their decisionmaking processes. nonetheless, itoften appears to be advantageous to involve the public in appropriate ways. thechemical manufacturers association, for example, organized a program calledcommunity awareness and emergency response (caer), which helpschemical plant managers provide information to their communities on a regularbasis and involve the community in emergency response planning, including thechemical plant emergency response plans. the caer program, which has beenimplemented at more than 1500 facilities, does not suggest that citizens beinvolved in the actual operating decisions but rather that plant managers regularlyinteract with them. it emphasizes the importance of treating citizen concerns asimportant and providing careful and accurate responses to the public.fair review of conflicting claimsexpectations of fair and impartial treatment by government organizations areof central importance. nothing undermines the legitimacy of governmentpositions more quickly than the demonstration that dispensations have beenunfairly granted. it is important that all claims are genuinely listened to andtreated fairly. some criticized the epa under anne burford because it allegedlyattempted to rid the scientific advisory board of scientists holding views ofwhich it did not approve (marshall, 1983b). epa officials were also accused ofinappropriately using portions of an industry publication in a ﬁcutandpasteﬂreview of toxicology data (marshall, 1983a:1200). the charge that epa officialswere not treating the issues fairly, but serving industry interests better thanenvironmental interests, became a serious challenge to the agency's credibility(rushefsky, 1984; sosenko, 1983). this tarnished image caused problems for theagency for several years.problems of risk communication128improving risk communicationcopyright national academy of sciences. all rights reserved.again, there is not the same expectation that nongovernmental organizationswill treat conflicts fairly. many believe, on the basis of past experience, thatindustry and citizen action groups will present information that supports theirinterests in the most effective way. but treating the positions taken by othersrespectfully and thoughtfully, and carefully and clearly laying out the premisesand assumptions of one's position, are likely to enhance the reputation ofnongovernmental organizations as well.making messages understandablethe risk communicator needs to present information in language andconcepts that recipients already understand, that use magnitudes common inordinary experience, and that are sensitive to the psychological needs of therecipients.unfamiliar languagein chapter 2 we described the scientific information needed for riskdecisions and the difficulty of presenting that information in simple terms that donot overwhelm the recipient. here we will point out some of the ways theterminology of science, and of risk assessment in particular, interferes withunderstanding by laypeople.for those who are not familiar with it, the technical terminology of riskassessment is very difficult to understand. research has shown, for example, thateven for seemingly familiar terms such as probabilistic precipitation forecaststhere is a high degree of misunderstanding in lay interpretations. people wereequally likely to interpret a ﬁ70% chance of rainﬂ as ﬁrain 70% of the time,ﬂ ﬁrainover 70% of the area,ﬂ and ﬁ70% chance of some measurable rainﬂ (the officialdefinition) (murphy et al., 1980).in this case people apparently had difficulty understanding not theprobabilities being used but the events to which they were applied. in other casesrisk assessments confuse people because they use concepts of probability theorythat are not intuitive. for example, a committed communicator can usuallyconvey the meaning of a simple probability of an event occurring (as long as it isnot too small). but the notion of conditional probability is much more difficult toget across, as is that of the probability of the conjunction of several events. in theepa's experience with edb, people were confused by the notion of the aggregaterisk of this pesticide to the exposedproblems of risk communication129improving risk communicationcopyright national academy of sciences. all rights reserved.population. they did not know how to interpret this information to answer thequestion, ﬁshould i eat the bread?ﬂ (sharlin, 1987).much has been written about the intuitive properties of probabilistic eventsand how they differ from probability theory (see the summary in fischhoff et al.,1981a). these include the ﬁgambler's fallacyﬂ (after a series of heads in flips of afair coin, most people expect a tail) and the tendency to impose order on theresults of random processes. the skeptical recipient of risk messages will be onthe lookout for such influences in the material he or she receives.recipients of risk messages also need to be wary of ﬁframing effectsﬂšdifferences that can result from the way information is presented. for example,one guide for chemical plant managers points out the following ways that theannual fatalities resulting from emission of an air toxic might be presented(covello et al., 1988): deaths per million people in the population, deaths per million people within miles of the facility, deaths per unit of concentration, deaths per facility, deaths per ton of the airborne toxic substance released, deaths per ton of the airborne toxic substance absorbed by people, deaths per ton of chemical produced, and deaths per million dollars of product produced.the authors point out that depending on the circumstances differentexpressions will strike the recipients as more or less appropriate, more or lessfrightening, or more or less credible. the recipient needs to be aware that simplychanging the way a piece of information is presented can alter its effect on manypeople and be aware of their pattern of response. each of these ways of presentinghas framing effects, which suggests that using more than one might be useful insome circumstances.misunderstanding can also result from inconsistency in the use of the sameterm among different disciplines. for example, the term ﬁriskﬂ has been used with avariety of somewhat different meanings. uses include the total number of deaths,deaths per person exposed or per hour of exposure, loss of life expectancy due toexposure, and loss of the ability to work (fischhoff et al., 1986).the recipient also needs to look for the sources of uncertainty in theanalysis. as we pointed out in chapter 2, at least four typesproblems of risk communication130improving risk communicationcopyright national academy of sciences. all rights reserved.of uncertainty may be found in risk messages: (1) weaknesses of the availabledata, (2) assumptions and models on which estimates are based when data aremissing or uncertain, (3) sensitivity of the estimates to changes in theassumptions or models, and (4) sensitivity of the decision to changes in theestimates.unfamiliar magnitudesrisk communicators are generally well aware that most people havedifficulty comprehending magnitudes that are exceedingly small or exceedinglylarge. often they utilize analogies to convey such magnitudes. for example, a riskof 0.05 may not mean much to most people; the statement that about 5 people inan auditorium of 100 would be affected is much easier to comprehend. a cancerrisk of 4.7×10!6 is difficult for most people to relate to. but it may be moreunderstandable to imagine 10 cities of 100,000 people each, all exposed to thehazard. in 5 of the cities there would be no effect, and in 5 cities there would be 1additional cancer as a result of the exposure (covello et al., 1988). such mentalaids must be used with extreme care, however. for instance, in this example therecould be 5 additional cancers in a single city instead of spread across the cities,and there could be an overall total of 10 or of 0. recipients need to look for themagnitudes in risk messages and how they are presented. these numbers aresubject to the same kinds of presentation effects as the concepts described in theprevious section.recipients need to be especially wary of misrepresentation that can beintroduced in comparing risks. in particular, magnitudes do not always representthe level of hazard. for example, characterizing the magnitude of ash emitted asfilling an olympicsized swimming pool or covering a football field to a depth of 6inches omits any reference to the potency of the material. because of differencesin potency, a fairly small amount of one substance may present the same risk as amuch larger amount of another.insensitivity to psychological needs of the recipientfactors influencing understandability intermingle with certain psychologicalcomponents of decision making. motivational factors can be involved. forexample, it may be difficult to determine whether the intended recipients havenot understood the message or whether they have understood but decided forwhatever reasonproblems of risk communication131improving risk communicationcopyright national academy of sciences. all rights reserved.not to heed its contents. people may want simple yesorno answers, and theymay want to know what they as individuals should do. when expectinginformation in such a format, they are likely to have trouble understandinginformation presented in some other format. the first image people receive about aproblem also tends to be the strongest and longest lasting. if they make up theirminds on the basis of that image, it will be hard to get them to change. but therisk communicator can use such psychological attributes to his or her advantage,as well. if the risk communicator is timely and presents a vivid image, he or shecan have considerable impact.with certain issues and certain parts of the population, communication maybe especially difficult. there may be, for example, a climate of mistrust in partsof the population about anything that can be labeled toxic. these people mayautomatically reject a message and oppose the production, use, or disposal ofproducts labeled toxic regardless of the risk estimates of experts. for them it maybe that risk messages would elicit little differentiation of response regardless oftheir format, message content, or the organization from which they emanate.people are unlikely to be interested in risk information that they cannot use.a risk communicator wishing to change the recipient's thinking (even if only tomake him or her better informed) thus needs to try to understand how that personreceives, processes, and acts on information. elsewhere we discussed thepsychology of risk perception and social factors that influence perceptions ofrisk. here we review only the more important of those psychological and socialinfluences.people differ. their interests, lifestyles, and living conditions vary. whatthey do in their private lives and how they interact with others in their publiclives will strongly influence how they are likely to use risk information. the riskcommunicator will be most effective if these attributes of recipients can bereflected in the risk message.for issues that affect large numbers of people, it will nearly always be amistake to assume that the people involved are a homogeneous group. it istherefore generally necessary to segment the population into groups with similarneeds. it is often useful to craft separate messages that are appropriate for eachsegment. preparing messages appropriate for different segments of the populationrequires determining what the recipients already know or think they know, whatis necessary for a full and sufficient understanding of the risk and risk reductionmeasures, and how they would be able to useproblems of risk communication132improving risk communicationcopyright national academy of sciences. all rights reserved.new information. depending on the numbers involved, this can be expensive andtime consuming.the purpose the message serves can dominate people's information needsand therefore the content of an effective risk message. for example, informationneeds will probably be quite different when the situation calls for providingemergency instructions, for alerting people to a previously unrecognized risk, orfor providing information that a risk is actually less serious than was previouslythought.a common mistake is to expect quantitative risk assessments to includeeverything people are concerned about. affective states (those involving orappealing to emotion) are equally or more important than physical conditions tomany people. since risk assessments are usually limited to physical events andconsequences, they can be expected to speak to only part of what concerns mostpeople. these other aspects of risks that concern people are sometimes calledqualitative risk factors.thus, in order to present information that is relevant to the intendedrecipients, it may be necessary to expend some effort to find out what is botheringpeople. to be effective, a risk message needs to refer both to information aboutrisk and risk reduction and to the psychological or affective factors that influencethe intended recipients. unfortunately, it can be difficult and expensive to developempirical information about recipients, especially if they are geographically orculturally dispersed.it is important not to expect too much from risk communication efforts.advertising campaigns are considered successful if they result in shifts of a fewpercentage points in the market for a product. it took decades of multiplemessages from many different sources to create major shifts in public attitudesabout smoking. it is hard enough to make risk messages understandable tolaypeople. it is harder still to know whether risk messages have an impact ontheir thinking.preparing messages with few data and no timesometimes the risk communicator must disseminate messages when thereare not enough relevant data to draw satisfactory conclusions and there is notime to obtain better information. this usually occurs in one of the followingsituations: (1) an emergency requires immediate action or (2) events lead torequests for information prior to the completion of study or analysis.problems of risk communication133improving risk communicationcopyright national academy of sciences. all rights reserved.responding in an emergencyemergencies occur when external events take control and require action byan individual or organization. they often, but not always, require immediateissuance of warnings, instructions, or advice about what to do. examples of suchemergencies include three mile island, the tylenol poisonings, and emergencyreleases from chemical or other industrial facilities.the problem is most extreme in a true emergency when no preparation hasbeen made in advance of the event. for example, the nuclear regulatorycommission was almost totally unprepared for an accident at the time of threemile island (ahearne, personal communication with national research councilstaff, 1988). there was no effective management structure to support emergencydecision making, and time was lost figuring out who should do what. the lack ofpreparedness permitted the involvement of too many who lacked the technicalcompetence to grapple with the emergency, thereby slowing the rate at whichnecessary information could be generated and interpreted. no one with thetechnical background to explain what was happening had been assigned the roleof spokesperson, and it was a couple of days before a credible source ofinformation emerged. nor did the agency, much less the electric power companyinvolved, appreciate the importance of timely and accurate news releases.finally, the agency had no notion of how to deal with the electric company or thenews media in such an emergency.emergency situations are likely to expose the risk communicator toconflicting motivations. for example, a company dealing with an emergencyrelease of toxic substances into the air, such as that from the union carbide plantat institute, west virginia, in august 1985, will probably balance severalcompeting factors in deciding what messages to give out (coppock, 1987). afterthe initial emergency response, when the overriding concern is what to do tocontain and stop the release, almost every business person immediately wonderswho will bring suit. the common view among legal advisors is almost always togive out as little information as possible so as to avoid providing ammunition foruse in court. this is in almost direct conflict with what communications andcommunity relations experts advise, which is to say everything that is known, asquickly as possible, in terms the layperson can easily understand. advice ofcompany scientists and engineers usually falls somewhere between these twoviews. they caution against attributing cause and effectproblems of risk communication134improving risk communicationcopyright national academy of sciences. all rights reserved.before being reasonably certain about what happened. the message that is finallysent out probably involves compromises between these three points of view.situations involving emergency response are often governed by specialconsiderations not shared with other kinds of risk communication. we havechosen to focus primarily on the other, more prevalent situations of riskcommunication and therefore do not discuss emergency response in detail.communicating on the basis of incomplete informationit is often difficult to estimate risks, consequences, and possible riskreduction measures with any precision. one result is that the risk communicatormay be left with very little information that can be presented with confidence. asone scientist at the epa put it, ﬁone of the nice things about the environmentalstandard setting business is that you are always setting the standard at a levelwhere the data is lousyﬂ (quoted by melnick, 1983:244).the poor quality of relevant information is also often involved in pressingissues. when the concern about edb shifted from groundwater contamination in afew isolated wells to residues in food products, epa administrator williamd.ruckelshaus sent a letter to the governors of the 50 states requesting data onresidue levels in food products. he had to answer queries by admitting that hisagency did not have the answers. ﬁif they [the public] want absolute information,we can't give it to them.ﬂ for a period of nearly a month, the best he could do wassay, ﬁi don't want to unduly alarm the public, nor do i want them not to knowabout itﬂ (sharlin, 1987:192). the risk communicator may often feel as if theworld wants to know definitive answers to questions about which he or she has noadequate information.external demands can also force an organization to make statements on thebasis of limited data. examples include love canal and transmission of the aidsvirus. another form in which this problem can be found is the decision aboutwhether to release preliminary information or tentative results. in 1986 the epabegan cooperating with the new york state energy research and developmentauthority (nyserda) on a program monitoring radon levels in geographicregions thought to have radon problems (smith et al., 1987). three monitors wereplaced in each home, one in the basement for 2 to 3problems of risk communication135improving risk communicationcopyright national academy of sciences. all rights reserved.months, a second in the basement for 12 months, and a third in the living area for12 months. originally, the plan was to give the homeowners the readings from allthree monitors at the end of the study. but in the spring of 1986, when radonbecame very much a public issue, nyserda became concerned that they wouldbe accused of withholding public health information if they kept the shorttermreading until the end of the study. it was decided to provide the initial basementreading to the homeowners, even though the full research design called forconfirmation of annual exposure levels and livingarea exposures with the othermonitors (fisher, 1987).capturing and focusing attentionmany other things compete with risk messages for attention, and the riskcommunicator often has difficulty getting intended recipients to attend to theissues. there are two separate aspects to this problem: (1) stimulating theattention of recipients and (2) interacting with the news media and otherintermediaries.stimulating recipient interestit is not always easy to capture the attention of people who receive riskmessages. most information campaigns share the following attribute: the peoplemost likely to receive messages and to attend to them are those who alreadypossess some information about the issues under question; those who may becharacterized as relatively uninformed are less likely to receive and pay attentionto messages. the people who need information most seem to be the least likely topay attention. one contemporary example might be the very low likelihood thatintravenous drug users will attend to messages about aids transmission via dirtyhypodermic needles.involvement in community affairs has been characterized as a pyramid. atthe bottom is the broad base of most people who are uninvolved in any personalsense and basically are uninterested. a somewhat smaller number of people areaware of issues but do not go to much effort to obtain additional information. astill smaller group actively seek information on particular issues. the number ofpeople actually participating in organized efforts is smaller still. finally, someindividuals seem to participate in, and often lead, every activity in a community(verba and nie, 1972).problems of risk communication136improving risk communicationcopyright national academy of sciences. all rights reserved.one of the consequences of this differential interest and involvement amongvarious parts of the public is that information readily made available willgenerally be taken up much more readily by some people than by others.educated and involved people usually absorb information much more quickly.but conflict can motivate otherwise uninterested people to gain moreinformation. in some situations, when the principal aim is to stimulateunderstanding in a broader sector of the public, it might be useful to stimulateconflict. very often journalists and the media seek out such conflicts and serve asinformation channels as conflicts play out.the majority of people, however, will probably not be interested in theissues addressed in a particular risk message. when a significant number ofpeople are similarly affected, a champion for that group is likely to emerge,especially when the impact is undesirable. such people can be engaged in riskcommunication activities as described elsewhere in this report. when trying toaffect the behavior of uninterested, uninvolved people, however, the riskcommunicator will need to find ways of attracting the attention of intendedrecipients and making the message meaningful to those people. this willprobably be easier if the risk is one that is perceived to directly impinge uponpeople and for which there are clear control measures that do not substantiallyinterrupt their private lives. for example, people have tended not to heedmessages about seat belt use, maintenance of automotive emission controldevices, and radon contamination of homes. however, it is difficult to determinewhether they simply paid no attention or whether they received the information,understood it, and decided not to act in accordance with the proffered advice.different people rely on different information channels. they read differentnewspapers and magazines and listen to different radio and television stations.they may turn to different information channels for different purposes. youngpeople, for example, may rely on mass media sources to learn about the aidsepidemic and its spread. but they may turn to their friends in determiningwhether to be worried and alter their behavior. risk communicators need to knowwhat channels their intended audience uses for what aspects of risk information.one example of this is the use of music television spots by the national cancerinstitute to convey the message to teens that it is not ﬁsexyﬂ to smoke, rather thanproviding information about the undesirable health effects of cigarette smoke.1problems of risk communication137improving risk communicationcopyright national academy of sciences. all rights reserved.interacting with the news media and other intermediariesthe mass media are widely perceived as playing a powerful role inconstructing laypeople's understanding of and attitudes about risk. journalists andthe media help identify conflicts about risk and are important channels ofinformation during the resolution of those conflicts. there are both critics anddefenders of the effects of the news media. in any case the risk communicatormust deal with the fact that some journalists tend to treat risk issues differentlyfrom the way technical and scientific people do.some conflict between risk communicators and journalists and otherintermediaries is probably inevitable. but this conflict can be reduced, and thereare approaches the risk communicator can use toward achieving this aim. animportant part of this is to recognize the typical differences in the way riskcommunicators, as sources of information, and journalists approach informationgathering and dissemination.organizations involved in risk issues typically seek to centralize and restrictthe flow of information, hoping to prevent the publication of damaginginformation. but reporters expect access not only to public information specialistsbut also to experts and managers and what they know (sandman et al., 1987a).this is especially true in emergency situations. the price of not providing thataccess may include suspicion, anger, and sometimes damaging coverage. despitethe legal and technical constraints, it is important to consider meeting the needsof the news media.many journalists are proud of their ability to flesh out a story with the viewsof uninvolved experts, dissident insiders, and others whose perspective on anevent is likely to be different from the official one (sandman et al., 1987a).specialized reporters are proud of their contacts and investigative reporters oftheir skill at finding those who know and of persuading them to talk. trying tostop reporters from talking to people within an organization is sure to encouragethem to investigate further.differences of opinion as to what should ideally be presented are likely toexist between risk communicators and journalists. sources sacrifice all credibilityin the eyes of reporters when they lie or mislead, and they lose much of it whenthey err, omit, or delay (sandman et al., 1987a). different sources are commonlyheld to different standards of credibility. industry spokespeople, for example, areoften discounted as opinionated even when they are providingproblems of risk communication138improving risk communicationcopyright national academy of sciences. all rights reserved.facts, while academics and public interest groups often are accorded thecredibility of neutral sources even when they are offering opinions.journalists, too, have a problem with credibility (sandman et al., 1987a). abotched story not only misleads the reader or viewer but it also diminishes thesource's willingness to cooperate with that reporter next time and perhaps withother reporters as well. the two most important complaints about reporters'treatments are misquotation and inaccuracy. technical stories have greaterchance of misquotation, simply because they involve terms and concepts lessfamiliar to the reporter than nontechnical stories. nevertheless, incompletenessand misemphasisšquoting out of contextšare more frequent than directmisquotation. complaints about inaccuracy are also generally about beingincomplete or misleading. sometimes the complaint is that too much credence isgiven to other sources who, in that source's judgment, are wrong or intentionallymisleading the journalist. these questions are commonly sources of conflictbetween risk communicators and journalists, especially because the journalistdoes not see his job as discovering the truth, but rather as reporting accuratelywhat others with some claim to attention consider to be true.risk messages are often routed to their intended recipients through healthprofessionals or other intermediaries. in addition, the views of influentialmembers of the community, such as county or local public health officers,prominent physicians, fire chiefs, and politicians, often provide valued guidanceto citizens as they form their opinions about controversial issues. sometimesexecutive officers of professional or volunteer organizations serve asﬁgatekeepers,ﬂ controlling the distribution of information, and their approval ordisapproval can be a critical factor in the dissemination of some risk messages. insome circumstances the intermediaries are even more important than the newsmedia in reaching the intended recipients.interacting with nonnewsmedia intermediaries can also involve problems.health departments, public libraries, professional associations, and voluntaryorganizations all have their own aims and purposes. they may or may not offerrelevant messages to the intended recipients that are appropriate in terms offormat and style. it may be quite time consuming and costly to establish workingrelationships with such intermediaries, however, and there is the danger of losingcontrol over the content of the messages. nevertheless, establishing links withsuch institutions and organizations can shortcut the development of routes ofinfluence with the target recipients.problems of risk communication139improving risk communicationcopyright national academy of sciences. all rights reserved.in this context it is important to realize that there are several different waysthat messages can reach the final recipients: face to face (physician to patient,friend to friend, within the family), in groups (work sites, classrooms), withinorganizations (professional or volunteer), through the mass media (radio,television, magazines, newspapers, direct mail, billboards, and transit cards), andwithin the community (libraries, malls, fairs, and local government). each ofthese channels offers advantages and disadvantages in specific situations.interpersonal channels like physicians or pharmacists are likely to be trustedand influential. but messages relying on interpersonal channels require theintermediaries to be thoroughly familiar with the message and may thus requireexpensive and slow longterm contact.community channels such as libraries and community organizations canreinforce and expand upon media messages. establishing links with communityorganizations can require less time than reliance on interpersonal channels.using celebrities can be effective if they are directly associated with themessage (e.g., they have been a cancer patient, are pregnant, or successfullyaltered a hazardous habit). but they speak for themselves, and it is important tohave firm agreement about what they willšand will notšsay. the appearance ofa celebrity may compete with the content of the message for attention, and somerecipients may not react favorably to some celebrities. finally, celebrities live inthe public eye and a change in their popularity or personal life style could affecttheir impact.working with intermediaries is essential in many situations. intermediariescan help by providing special access to the intended recipients, credibilitybecause the recipients consider them to be a trusted source of information, andadditional tangible or intangible resources. working with these individuals andorganizations, however, can also have drawbacks. it can be time consuming tolocate them, convince them to participate, gain their approval, and develop andagree on their role. it can require adjustment in order to match the priorities andprograms of intermediary organizations. it can result in loss of control of the riskmessage because they may change the time schedule, functions, or even thecontent of messages and take credit for part or all of the effort.problems of risk communication140improving risk communicationcopyright national academy of sciences. all rights reserved.getting informationrecipients of risk messages may have difficulty deciding what to do becausethey cannot get information that satisfactorily answers their questions. this canresult from one or both of the following: authorities who do not listen or whorespond inappropriately and difficulty in finding trusted local sources ofinformation.authorities who do not listen or respondthe story of the concerned citizen motivated to organize protest groupsbecause of the cold or indifferent response of public officials is common in theliterature of environmental and citizens' organizations (fitchen et al., 1987;institute for environmental negotiation, 1984; krimsky and plough, 1988;mazur, 1987). a citizen who had spent several years as an activist opposing theconstruction of a hazardous waste facility in her community told us of thefrustration her group experienced in trying to get the authorities to take theirconcerns seriously and in attempting to obtain materials they could use to informthemselves and their neighbors (smith, 1987). she spoke of the anger generatedby the lack of respect given her group's questions by government officials. at apublic hearing the company proposing to construct the facility was allowed tospeak freely. but questions from the public had to be submitted in writing. norwas the citizens' group able to find support from the traditional nationalenvironmental organizations. finally, they turned to other citizens' groups whowere opposing the same company in other locations. this may be a commonexperience for citizens' groups focusing on locally unwanted land uses. thenumber of superfund sites around the country and the pressing necessity forfinding ways of dealing with hazardous wastes will make this kind of difficultylikely to reappear many times.difficulties in finding trusted sources of informationother developments will result in citizens or citizens' groups seekingadditional information. for example, title iii of the superfund amendments andreauthorization act of 1986, also called the community righttoknow act,includes provisions for creating emergency response plans and for reporting dataabout hazardous substances stored and regular emissions to the epa. the epamustproblems of risk communication141improving risk communicationcopyright national academy of sciences. all rights reserved.make these data available to the public. the community righttoknow act willmake a tremendous amount of information about potential hazardous situationsavailable to citizens who wish to obtain it. but this information is likely to be inhighly technical form, most of which would require considerable interpretation toappreciate. a citizen wishing to make sense of this information about a facility inhis or her community will need to interpret data from material safety data sheetsdeveloped for occupational exposures and estimate peak or periodic exposuresfrom annual emission totals. he or she may wish to seek additionalinterpretations to those provided by facility personnel, and finding trusted andqualified people to interpret this information will be an important part of theprocess.summarywe distinguish two major types of problems in risk communication. thoseinvolved in risk communication can do little about problems deriving from theinstitutional and political system beyond understanding them and their influence.these problems can have considerable impact on events, and if they are ignoredit may be quite difficult to understand why things happen the way they do.problems of risk communicators and recipients can be addressed more directlyand are more amenable to improvement or solution. in most instances theproblems of risk communicators and the recipients of risk messages are mirrorimages of each other. in the next chapter we describe conclusions andrecommendations that are intended to improve risk communication in ways thatwill address the problems of risk communicators and of the recipients of riskmessages.note1. this is unlikely to meet our criteria of informing or of accuracy of the message.problems of risk communication142improving risk communicationcopyright national academy of sciences. all rights reserved.7recommendations for improving riskcommunicationdrawing lessons from the available understanding about the nature andproblems of risk communication, we present four sets of recommendations in thischapter: (1) recommendations that pertain to the processes that sourceorganizations use to generate decisions, knowledge, and risk messages; (2)recommendations that pertain to the content of individual risk messages; (3) acall for a ﬁconsumer's guideﬂ that will enhance the ability of other groups orindividuals to understand and participate in risk management activities; and (4) abrief summary of particular areas for which additional knowledge is needed toresolve current problems of risk communication.we have attempted a focused search. the committee faced a centraldilemma about how detailed we could expect to be in meeting our charge todiscern practical lessons for practitioners. given the breadth and diversity of thegeneral topic of risk communication, any attempt to look for lessons that apply toall forms of risk communication would constrain us to a discussion so generalthat any particular reader would gain little insight. on the other hand, a detailedﬁcookbookﬂ for particular situations would fail to advance the broad nationaldiscussion that is now needed. we have accordingly sought a middle ground,electing to narrow our scope in two ways.first, we have elected to focus on certain forms of risk communication. theterm ﬁrisk communicationﬂ can cover a vast range ofrecommendations for improving risk communication143improving risk communicationcopyright national academy of sciences. all rights reserved.actions, from casual telephone calls between two experts to booklength reportsmeant for the general public. our main subject in this chapter is formal riskmessages intended for audiences that include nonexperts. included, for example,are press releases, material prepared for an open meeting in a community or aformal meeting with representatives of interested outside groups (e.g., a localpublic meeting about siting a facility), a government agency's public explanationof a decision it has made, a brochure for citizens concernsome aspect of publichealth (e.g., an aids pamphlet), package inserts for prescription drugs, and risksummaries prepared by experts within an organization for the use of their (lessexpert) superiors. we recognize that some of our recommendations may have lessrelevance for other very important, but less formal, varieties of riskcommunication.second, we have directed our recommendations to just two of the manytypes of riskmanaging organizations that are discussed in other parts of ourreport: namely, government agencies and large private corporations. again, thischoice of emphasis is not intended to imply that other communicatingorganizations and individualsš small firms, citizen/consumer advocacy groups,and so onšare not important. in fact, many of the points we raise doubtless applyto them. we chose this narrower range of organizations because they are mostdirectly involved in many of the best known and most controversial cases, thecommittee members have greater knowledge of their experiences, and we areconvinced that improvements by these organizations would both contributesubstantially to easing the national problem and provide models for otherorganizations.our objective, then, is to improve risk communication, particularly aspracticed by government and large corporations. what do we mean byﬁimproveﬂ? we mean that solutionsšsometimes admittedly only partialsolutionsšare put in place for the range of problems identified in the previouschapter. we emphasize in particular that we have tried to fashionrecommendations that, while addressed to government and large corporations,will attack the problems of recipients as well. our goal is not then to make thosewho disseminate formal risk messages simply more effective by improving theircredibility, understandability, and so onšsuch an approach might serve theirinterests but could well degrade the overall quality of risk communication if itmeant that they would merely advance their viewpoints with more influence.ﬁimprovementﬂ can only occur if recipients are also enabled to solve theirproblems at therecommendations for improving risk communication144improving risk communicationcopyright national academy of sciences. all rights reserved.same time. generally, this means obtaining relevant information for betterinformed decisions.we have also focused our recommendations on measures that will help thosegroups meet the criteria we have set out above for successful riskcommunication. in reality, of course, many organizations have other criteria forsuccess, such as whether messages convince recipients to act in a manner that therisk communicator desires. we have not chosen to recommend actions to helporganizations meet those other goals.in recommending steps to be taken by government entities, we havenecessarily focused on the respective roles of citizens, private groups, andgovernment in a democratic society. controversies about risk communicationoften turn out to be basic debates about the limits of governmentalaccountability, legitimacy, and authority. the goal of our recommendations is notto alter american democratic institutions but to make them work moreeffectively. two points need to be emphasized about accountability. first, oursociety has elaborate and politically responsive procedures for assigningresponsibilities for making government risk management decisions. once agovernment agency has received that responsibility, it must retain it. this placesinherent limits on what agencies can do in discussing risk issues with citizens,because they cannot share responsibility with outside groups; they must remainpublicly accountable. second, accountability increasingly implies an affirmativeduty to interact with interested and affected outside parties in reaching andexplaining individual policy decisions. although citizensšand the groups thatundertake to represent their interestsšare not required to participate in suchinteractions, solving problems of risk communication becomes much easier ifthey do, and government needs to ensure that the opportunity to participatebecomes routine.implementation of many of our recommendations requires organizationalresources of several kinds. we are aware that such resources will not be adequatein many instances. one resource in particularštimešis crucially lacking forsome of the most difficult risk communication efforts, as when emergencyconditions leave no possibility of consulting with outside organizations orassembling complete factual information. other recommendations require staffresources and the capacity to conduct specialized analyses, both of which may bein short supply in some organizations. when resources are so constrained, ourrecommendations may well best serve as a reminder of the full set of factors thatshould be accommodated,recommendations for improving risk communication145improving risk communicationcopyright national academy of sciences. all rights reserved.although the form of accommodation may fall short of what we recommend.our recommendations are based on our understanding of the growingliterature of studies of risk communication and risk messages and on committeemembers' diverse experience with specific instances of risk communication.before we list our recommendations, we would like to draw attention tothree general conclusions that we have made:conclusion 1. even great improvement in risk communication will notresolve risk management problems and end controversy (although poor riskcommunication can create them). because risk communication is so tightly linkedto the management of risks, solutions to the problems of risk communication oftenentail changes in risk management and risk analysis. there is, unfortunately, noready shortcut to improving the nation's risk communication efforts. the neededimprovement in performance can only come incrementally and only fromassiduous attention to many details.while it is important to improve risk communication practices, no oneshould expect such improvements to end public controversy over riskmanagement. risk managers should understand and accept that, even when theyhave done all they can to ensure the integrity of their risk messages, publicskepticism of their motives and their honesty will likely persist. they shouldappreciate that, particularly in recent years, distrust has been institutionalized inour country. while it is important for most risk managersšespecially those in thegovernmentšto avoid distortions in their messages, they should expect that manyaudiences will continue to assume that bias is present.we have discovered no sweeping broadspectrum remedies for the problemsof risk communication described in chapter 6. many will be solved only over thelong term and only by sustained effort. many of the institutional problems weidentified in the previous chapteršfragmentation of authority, legal constraints,and so onš reflect social decisions about how risk management should beconducted. such decisions are inherently, and appropriately, political in nature.risk communication might well be improved if certain contextual constraintswere changed or removed. however, such reforms would also create otheradvantages and disadvantages that are well beyond our capacity to evaluate inthis study. thus we are left with a more modest, and necessarily incremental, setof available remedies.recommendations for improving risk communication146improving risk communicationcopyright national academy of sciences. all rights reserved.the source organization's problem of achieving credibility provides a goodexample. an organization's credibility can be quickly lost, as illustrated in thecase of the epa in the early 1980s, when many observers came to believe thatone of epa's leaders' highest goals was to dismantle regulatory programs. incontrast, credibility is gained (or regained) only through a sustained effort to beresponsive to audience concerns and to be accurate, open, and honest indisclosing essential information. thus we are led to recommend concurrentattention to several factors in managing the risk communication process and informulating particular risk messages. no one of these measures, alone, is enough.an underlying reason for this is that the problems of risk communication arerooted in risk management practices and procedures. because of this, several ofthe measures we recommend call for adjustments in the source organization'sprocedures for risk management and for analyzing risk issues. for example, wecall for more interaction with audiences and intermediaries while the sourceorganization considers risk management alternatives, and we suggest how formalrisk assessments should be scoped, reviewed, and presented. we have explicitlyaddressed many of our recommendations to risk managers precisely because theyare the individuals within an organization who can provide the neededcoordination of risk communication, risk management, and the assessment of riskand risk control.conclusion 2. solving the problems of risk communication is as much aboutimproving procedures as improving content. risk managers need to consider riskcommunication as an important and integral aspect of risk management. in someinstances, risk communication will, in fact, change the risk management processitself.it would be a mistake to believe that better risk communication is mainly amatter of crafting better messages. to enhance credibility, to ensure accuracy, tounderstand recipients and their concerns, and to gain the necessary insight intohow messages are actually apprehended, one must ultimately seek proceduralsolutions. thus we devote much of this chapter to matters of process. there maybe many cases in which problems of credibility, potential controversy over valuejudgments, and diverse audiences reduce the risk communication task to asimpler matter of making messages clearer, in themselves. we do not believe thatthe national frustration over risk communication practices derives from failures insuch ﬁsimplerﬂ cases and therefore have not addressed simpler cases in anydetail.recommendations for improving risk communication147improving risk communicationcopyright national academy of sciences. all rights reserved.risk managers cannot afford to treat risk communication as an afterthought.one of the root problems in risk communication is that, perhaps due toorganizational imperatives and tradition, risk management has too often beentreated as a sequential process: (1) the organization's technical experts assess arisk and explore options, (2) a risk management decision is made, (3) a messageis internally prepared, and (4) the message is sent to outsiders. riskcommunication is thus regarded as a subsidiary activity.the importance of risk communication has only recently become apparent,and even the most progressive risk managers are only now beginning to adjust tothe realization. improvement of risk communication requires that theorganizations that disseminate risk messages become simply more deliberate intheir communication efforts.at their best, risk communication efforts can be expected to affect the riskmanagement process itself. considerations of risk communication might, forexample, determine what kinds of analyses of risks and benefits are performed,how risk assessments are summarized, what options are explored, and whatpeople are consulted in exploring possible courses of action.risk communication requires its own specialized expertise and deliberateplanning and evaluation. senior managers need to devote attention and time tomanaging risk communication efforts per se. it is a mistake to simply considerrisk communication to be an addon activity for either scientific or public affairsstaffs; both elements should be involved. there are clear dangers if risk messagesare formulated ad hoc by public relations personnel in isolation from availabletechnical expertise; neither can they be prepared by risk analysts as a casualextension of their analytic duties.conclusion 3. two broad themes are apparent in the extended list ofrecommendations: that communication efforts should be more systematicallyoriented to specified audiences and that openness is the surest policy.both the management of the process of formulating risk messages and thecontent of risk messages should be systematically oriented to the intendedaudience. the most effective risk messages are those that quite selfconsciouslyaddress the audience's perspectives and concerns. similarly, the best proceduresfor formulating risk messages have been those that involved interactions withrecipients and that elicited recipients' perceptions and needs.recommendations for improving risk communication148improving risk communicationcopyright national academy of sciences. all rights reserved.a central premise of democratic governmentšthe existence of an informedelectoratešimplies a free flow of information. suppression of relevantinformation is not only wrong but is usually, over the longer term, alsoineffective. risk messages should be explicit about current knowledge of thesubject risk but also about the limits of that knowledge and the existence ofdisagreement among the experts or others. the longterm improvement ofcredibility, in particular, depends on openness. several of our procedural andcontent recommendations are intended to foster openness and to promoteopenmindedness about outside viewpoints.management of the processmuch recent concern about risk communication has centered on questions ofmessage content. failures have frequently been attributed to the inability of theaudience to comprehend complex technical issues and to the tendency of riskmessages to be badly written. this view would lead one to seek solutions in thedesign of better risk messages themselves. our assessment has led us to believethat longerterm solutions are equally likely to involve attention to and changes inthe process by which risk management decisions are made and explained.there are two basic reasons for our emphasis on process. first, when lessonsabout message content are identified, the operational question becomes one ofensuring that those lessons are systematically followed. procedural safeguardsprovide the best assurance of routine compliance. second, and more important, itis increasingly clear that content and process are not easily separated, particularlyon the crucial matter of appearing credible. if recipients believe the process isflawedšfor example, if the communicating organization is known to ignore orreject certain facts, viewpoints, or optionsšthey are likely to doubt the message,even if it is, in fact, technically competent.this section is addressed to risk managersšthose senior officials who havethe overall responsibility of determining their organization's action. these riskmanagers also oversee the preparation of risk assessments and risk messagesassociated with the action to be taken.we identify four process objectives that are key elements in improving riskcommunication: goal setting, openness, balance, and competence. we note thatthese objectives are general in nature.recommendations for improving risk communication149improving risk communicationcopyright national academy of sciences. all rights reserved.different management styles may work best for different managers in particularsituations, in pursuit of these four objectives.setting realistic goalssome past deficiencies in risk communication efforts have arisen becauserisk managers have not appreciated that risk communication needs to receivedeliberate management attention. until now, risk communication efforts have alltoo often been pursued with implicit or impractical objectives within the sourceorganization.risk communication activities ought to be matters of conscious design.practical goals should be established that explicitly accommodate thepolitical/legal mandates and constraints bounding the process and the rolesof the potential recipients of the organization's risk messages. explicitconsideration of such factors encourages realistic expectations, clarificationof motives and objectives (both within the source organization and amongoutside groups and individuals), and evaluation of performance.consideration of these issues of practical goals and impediments to theirachievement may be the only way for managers to reach realistic expectations.otherwise, source organizations may set themselves up for frustration and, ifnaive or insensitive programs result, for disrespect among recipients that can onlyaggravate any preexisting tensions about how the risk should be managed.effective program management is enhanced by setting explicit objectives.this is especially important with respect to risk communication because of thedifficulty of assessing the effect of messages. a cornerstone of systematic riskcommunication goals is a realistic review of the political and legal context of thecommunication effort and the risk management decisions to which it relates.what is one empowered to do? can messages properly attempt to inducerecipients to take certain actions or can they only transmit neutral information?who must receive the information? what level of understanding (if any) must beassured? how active a part can interested and affected parties be allowed to playin the risk management process? analysis contributing to goal setting provides away to articulate the basic premises for action and a basis for evaluation ofperformance.such analysis sets the general context for a risk communication effort. itneeds then to be translated into operational objectives. for example, how manypeople should receive the message? whatrecommendations for improving risk communication150improving risk communicationcopyright national academy of sciences. all rights reserved.changes (if any) should be observed in recipients' beliefs or actions regarding therisk? will recipients be motivated to listen? will they rely on other, possiblycontradictory, sources? realistic assessment of factors affecting messagepreparation, transmission, and receipt can be an important contribution to anorganization's effective participation in the risk communication process.safeguarding opennessin many cases risk communication efforts have foundered because publictrust and credibility were damaged because risk management was conductedbehind closed doors or because of a patronizing attitude toward interested outsidegroups.risk communication should be a twoway street. organizations thatcommunicate risks should ensure effective dialogue with potentially affectedoutsiders. this twoway process should exhibit: a spirit of open exchange in a common undertaking, not a series ofﬁcannedﬂ briefingsšdiscussion should not be restricted to technicalﬁnonemotionalﬂ issuesšand early and sustained interchange, including the media and othermessage intermediaries.openness does not ordinarily, however, imply empowerment todetermine the host organization's risk management decisions. to avoidmisunderstanding the limits of participation should be made clear from theoutset.risk managers should resist the temptation to close their processes to outsidescrutiny and participation unless, as is rarely the case, extreme conditions warrantsecrecy. as a practical matter, problems of risk communication for many pastcases seem most pronounced when risk communicators have not appeared tovalue openness. in addition, many of the cases that were resolved relativelyeffectively were marked by openness.openness thus has practical benefits both for the organization that managesrisk and for outside participants, but there are deeper reasons for it. openness ishighly valued in a democratic society like ours because public accountability is acentral element of our political culture. this is particularly true for organizationsthat are responsible to an electorate or that are charged with a public purpose, butprivate organizations are hardly immune in contemporary america. the fact thatours is a democratic culture means that there are strong negative sanctions inpublic opinion for evidence of secrecy. whenrecommendations for improving risk communication151improving risk communicationcopyright national academy of sciences. all rights reserved.governments or corporations can be found guilty of withholding information, theycommonly find themselves severely condemned, and their credibility is damagedfor some time, regardless of the content of their risk message. thus openness maybe seen both as a matter of principle and a matter of practical wisdom foroperating in a culture where many others take openness to be a matter ofprinciple.openness may take diverse forms in diverse risk management settings.when a government agency considers issuing a regulation, it can involverepresentatives of interested and affected groups in discussions of the rationalefor action, quantitative and qualitative indications of the subject risk, availablealternatives, and other factors affecting its choice. if an organization undertakes toadvise the general public of a risk associated with personal behavior (e.g., diet,sex), it can involve representatives of the intended audiences in discussions of theneed for risk messages and the best ways to compose them. if a corporationdecides to locate a new facility in a community, it can draw community groupsinto discussions of the nature of risks presented by the facility and take steps tocontrol such risks. risk messages will prove much more difficult to convey whenrecipients believe they were excluded from risk management decisions that affectthem.openness also provides an opportunity for risk managers to receiveimportant information from outside the organization relevant to their riskmanagement decisions, as is amplified in the later discussion of competence.effective dialoguethe most productive interactions are those that treat outside parties as fullylegitimate participants, so that twoway exchange occurs. if the host organizationconveys the impression that it is meeting with groups simply to diffuse outsideconcerns, or to edify ﬁuninformedﬂ lay risk perceptions, this goal cannot be met.if mutual trust is established, the host organization will benefit from fresh ideas,will understand better how its formal risk messages will be perceived, and will beable to incorporate needed adjustments to messages earlier than if oppositionforms in response to a message. participating organizations will have a chance tounderstand the basis for action and to determine for themselves the degree towhich the risk decision and the associated risk message are based on full andopenminded consideration of available knowledge and the full range ofalternative actions.recommendations for improving risk communication152improving risk communicationcopyright national academy of sciences. all rights reserved.eliciting participation is not simply the passive provision of access to theprocess of forming risk messages. many outside groups have had frustratingexperiences in which their views have been elicited but not listened to. anexample is the holding of pro forma public hearings, which frustrated participantslater feel should have been labeled as ﬁtalkings,ﬂ not hearings, from the hostorganization's apparent lack of attention to points raised. active effort should beapplied to identifying the full set of interested and affected groups and ensuringthat the full range of potentially contending viewpoints is apprehended. the riskmanager should ensure that those in the organization have come to understand: what the participants know, believe, and do not believe about the subjectrisk and ways to control it; what quantitative and qualitative information participants need to know tomake critical decisions; and how they think about and conceptualize the risk.to accomplish this, those within the organization who interact directly withoutside participants should be good listeners. they should not make facileprejudgments about what people think and know and which options they willprefer. they should be prepared for skepticism, antagonism, and hostility. theyshould respect the legitimacy of subjective, as distinct from coldly analytic,responses. they should not be surprised if people are more interested in mattersof trust, credibility, and fairness than in the technical details of risk estimates andrisk reduction options. they should not expect outside participants to know, or tonecessarily accept, the legal or other practical boundaries that constrain the riskdecision.risk managers should expect, and not resent (or appear to resent), skepticismabout their motives in establishing more open procedures. they shouldunderstand that the fear of cooptation may impede trust, at least initially.the job of interacting with outside participants should not be delegated tolowerlevel staff. those with the power to make the decisions under discussionneed to be directly involved in facetoface dialogue, at least for the major issues,for this provides convincing evidence of the organization's sensitivity to theviewpoints of interested and affected groups.in some cases it may be advisable to formalize the participation, forexample, by forming a citizen advisory group. such a move would signal anorganizational commitment to continue to listen andrecommendations for improving risk communication153improving risk communicationcopyright national academy of sciences. all rights reserved.to heed. representative sample surveys also can help identify what people knowand how they feel, what they think their choices are, and their responses to newinformation. such surveys could constitute a valuable contribution to theopenness of the overall process.early and sustained interactionthe best form of interaction is that which begins at an early stage andcontinues from then on. if outside groups are brought in very late, they are likelyto be frustrated if the decisions the organization has already made are effectivelyoff limits for discussion. participating organizations have scarce resources andwill resent being drawn into what they see as empty proceedings.open procedures are most successful when the host organization leavesitself ample room to adapt as discussions mature. for example, participants maywant to see the underlying risk assessment done in a different way, so as toilluminate issues of particular salience (e.g., the risks or costs imposed onparticular groups, alternative units of measurement). they may call for furtherdata collection to address uncertainties that trouble them most. when participantsare asked to contribute to the development of a risk message itself, they maywant to explore different strategies for dissemination and additional targetaudiences. where time and legal considerations permit, participants mayproductively help the risk manager to develop new or refine extant riskmanagement options.once participation has begun, it is important to sustain it. regular updates,newsletters, and briefings can reinforce the belief that the organization isresponsive to input from participating groups. there may be strong disincentivesto early efforts at openness. for example, at early stages the organization's riskassessment may be unfinished; openness at this point could result in inconsistentinformation emanating from different sources within the host organization, whichitself could undermine trust and credibility. we do not wish to deny that suchcomplications exist; however, such considerations should not be permitted toautomatically preclude early participation unless they clearly outweigh itsconsiderable advantages.the empowerment problemopenness is not the same thing as empowerment. risk managers shouldanticipate some confusion concerning the objectives ofrecommendations for improving risk communication154improving risk communicationcopyright national academy of sciences. all rights reserved.participation. in the past some host organizations have seen participation as ameans to a narrow endšthe development of better risk messagesšwhile outsideparticipants may believe that they have been given a full vote in making the riskmanagement decision (e.g., the choice among regulatory options, the decision toissue a public health announcement, the decision to locate a new corporatefacility) or in changing the decision process itself. some participants may feelthat discussing risk messages without addressing the risk management decisionitself is beside the point. there is admittedly a fine line between being responsiveto outside concerns and relinquishing responsibility to make risk managementdecisions. it is the risk manager's responsibility to be as clear as possible at theoutset about where the line is drawn for a particular case. (this does not mean,however, that the risk manager should expect assent on this point, and ambiguityis likely to remain, but it is a matter that is better explicitly discussed than leftbelow the surface.)outside participants need to understand that, because of statutes andelectoral responsibility, an organization cannot, and should not, share itsresponsibility for risk management decisions. federal agencies, for example, arenot commonly able to delegate authority and still remain within their legalauthority and thus accountable to the electorate (through executive or legislativeoversight) for their regulatory actions. in the past vested interests have beensuspected (often by groups that are absent because they cannot afford the costs ofparticipation) of abusing open procedures to thwart or delay decisions, rather thanto improve them. the host organization, for its part, should not expectparticipating groups to relinquish their right to raise objections later on, usinglitigation or other means, simply because they have been consulted in advance.safeguarding balance and accuracy in risk messagesšpreventing real and perceived distortionfor many risk messages, credibility depends on the audience's belief that themessage is reasonably objective; there is broad skepticism about organizationsshading the truth to suit their ends.because bias, like beauty, is often in the eye of the beholder, it may be verydifficult for those who oversee the preparation of risk messages to ascertain, byexamining the messages themselves, whether they will mislead audiences or beperceived as distorted. procedural safeguards may be much more effective.recommendations for improving risk communication155improving risk communicationcopyright national academy of sciences. all rights reserved.to help ensure that risk messages are not distorted and do not appearas distorted, those who manage the generation of risk assessments and riskmessages should: hold the preparers of messages accountable for detecting and reducingdistortion; consider review by recognized independent experts of the underlyingassessment and, when feasible, the message; when feasible, subject draft messages to outside preview to determine ifaudiences detect any overlooked distortions; and prepare and release a ﬁwhite paperﬂ on the risk assessment and riskreduction assessment for comment.accountabilitydistortion can enter at two stages: in the preparation of the expert analysesthat form the basis of a risk message and in the composition of the message itself.risk managers should actively encourage those who prepare messages and theexpert analysts within the organization to supply materials that are as free ofdistortion as possible. risk managers should sensitize employees to particulartypes of biases and perceived biases that it is particularly concerned about and seethat the experts are aware of subtle causes of perceived bias.experts commonly must synthesize risk information that is fraught withuncertainty, for which many choices among competing quantitative andqualitative assumptions and methodologies must be exercised. to cite but one ofthe many assumptions that will be found in a particular case, for example, theassessment may be based on ﬁworst caseﬂ or on ﬁbest estimateﬂ calculations.there is a constant danger that such choices will be unduly influenced by threetypes of bias: (1) the expert's personal value judgments about what the riskmanagement outcome should be; (2) the expert's belief of where theorganization's selfinterest lies; and (3) ﬁexpert bias,ﬂ which sometimes leadsexperts to exaggerate the certainty and precision of their assessments.unfortunately, one cannot assume that experts are significantly more selfconscious about the subtle distinction between value judgment and scientificconsensus in complex analyses than nonexperts are; this means that the riskmanager needs to be actively involved in preventing distortion in the way riskassessments and risk reduction assessments are performed and presented.recommendations for improving risk communication156improving risk communicationcopyright national academy of sciences. all rights reserved.some of the assumptions inherent in a risk assessment have to do withuncertainties in the underlying science: for example, choosing among availablescientifically supportable theories about extrapolating to humans from animaldata on the carcinogenicity of various doses of tested substances. theseassumptions unavoidably interject a subjective element into the risk assessment,by reflecting the assessor's judgment about which extrapolation method is mostlikely to be confirmed by future research.other assumptions will reflect the values of the scientists performing theassessment. they may have chosen to use ﬁconservativeﬂ estimates in variousportions of their work. they may have summarized risks to different groups ofexposed people in a way that ignores who those people are (rather than, say,giving extra weight to risks to children).risk analysts and risk managers also may make special assumptions abouthow to weight diverse risks. for example, risks that involve horrific outcomes(e.g., cancer deaths as compared to death by cardiovascular disease) andcatastrophic outcomes (e.g., release of lethal chemicals in populated areas, asoccurred in bhopal, india) are sometimes given extra weight in making riskmanagement decisions.such assumptions may be widely accepted value choices. they may be justwhat the public prefers experts to do when confronting uncertainty. however,they need routinely to be made explicit if audiences are to interpret the resultingrisk messages appropriately. moreover, because they reflect the interjection ofvalues into assessment, they need to be cited as a matter of openness and publicaccountability.more commonly feared by skeptical recipients than expert bias is thepossibility of (intentional or unintentional) distortion to fit ideologicalprecepts. .government organizations are particularly susceptible to suspicions ofdistortion born of ideological bias. congress, the press, and advocacy groupsfrequently charge that agency positions subordinate science to the currentadministration's ideology (e.g., a preservationist tendency in the carteradministration and a laissezfaire one in the reagan administration). one shouldnot conclude that such influences are flatly inappropriate in public riskmanagement decisions; we elect presidents and legislators based on theirexpressed values and platforms and then hold them politically accountable for thevalue judgments they make when they are in office. thus, for example, differentadministrations may properly seek different balances between health risks andeconomic benefits.recommendations for improving risk communication157improving risk communicationcopyright national academy of sciences. all rights reserved.similarly, how agency decision makers value usually unmeasurable quantities(e.g., nonmarket goods) in reaching regulatory decisions appropriately dependson philosophy or ideology. however, when the ideology overrides science orblinds the decision maker to established facts, the result is distortion.risk managers usually rely on information provided to them by staffscientists, engineers, and analysts. frequently, this information is generatedseveral levels below the manager and must pass through a series of intermediatemanagerial and policy reviewers. these reviews can filter out information orpositions that are seen to contradict current policies, presenting a danger that therisk manager receives, perhaps unknowingly, distorted, incorrect, or inadequateinformation. risk managers should establish an environment in which staffmembers believe themselves obligated to be honest and to come forward withtheir best information and analysis, even if it is not entirely welcome. riskmanagers should not permit anyone to be penalized for arguing within theorganization against the organization's or the administration's position, when thefacts point elsewhere. they should remain constantly aware that failure to elicitthe best technical information from within the organization can be extremelycounterproductive to their credibility. establishing this environment may beabetted by a formal procedure, such as that established by the nuclear regulatorycommission in 1980, known as differing professional opinions (u.s. nuclearregulatory commission, nrc manual chapter nrc4125, september 1980,amended july 1985), and by periodic attention to ensure effectiveness(nureg1290, ﬁdiffering professional opinions: 1987 special review panel,ﬂu.s. nuclear regulatory commission, november 1987).independent reviewto help ensure that choices made in performing the risk assessment do notintroduce errors or analytic assumptions that conflict with areas of currentscientific consensus, organizations should routinely subject the underlyingassessments, and when feasible the ensuing risk messages themselves, toindependent peer review. this review can help managers satisfy themselves thatuncertainties are adequately characterized and that scientific disagreements areunderstood.peer review should be as independent of the communicating organization aspossible and should be conducted by a group whoserecommendations for improving risk communication158improving risk communicationcopyright national academy of sciences. all rights reserved.collective expertise blankets the scientific areas that are germane to the riskmessage. the science advisory board of the u.s. environmental protectionagency (epa) and various assessment panels convened by the national researchcouncil are effective examples.message previewwhen possible, drafts of risk messages and the information on which theyare based should be made available to selected outside individuals for theirpreview and comment.previews by partisans is a proven method of identifying intentional andunintentional slants in risk messages: if value judgments have inappropriatelyintruded to produce distortion, groups that hold contrary values are certain toproclaim the misstep. in cases where early participation has been possible, theparticipants themselves can perform the preview.outside previews will also help reveal where agreement exists amongdiverse groups. such coordination can help reduce the incidence of needlesslycompeting or conflicting messages from groups that are in basic accord.there are many instances in which partisan preview is not advisable,particularly when it would appear that the organization is unfairly giving advanceinformation on major policy changes to some groups and not to others. (note:previews of messages by the general public have also proven effective. ﬁfocusgroupsﬂ have increasingly been used, less to detect bias and inaccuracy than tojudge whether the intended message is actually understood. although moreexpensive and time consuming, representative sample surveys can be used toprovide a more accurate picture of the likely response of the intended audience.)written documentthe assessment of potential bias, as well as the search for technical errors, isgreatly enhanced when written supporting documents are available. when timeand resources permit, the communicating organization should synthesize thescientific information base into a formal ﬁwhite paperﬂ that can be generallyreleased. this document should summarize relevant quantitative and qualitativescientific information, the attendant uncertainty about the risk and about riskreduction alternatives, and the assumptions employed. federal agencies couldrelease such a document asšor in conjunction withštherecommendations for improving risk communication159improving risk communicationcopyright national academy of sciences. all rights reserved.preamble to a formal notice of proposed rule making, as has been done for majorregulations by the food and drug administration and the epa. such a documentcan facilitate the elicitation of reactions from review by independent experts,partisan groups, and even the lay public. it can also foster understanding of therisk issue in different parts of the organization itself.fostering competencešmaking risk communicationsmarterrisk communication has only recently come into focus as a concept, and inmany organizations it is still subsumed under other functions, such as riskassessment or public affairs. more attention should be paid to risk communicationas a distinct undertaking. successful efforts in risk communication require a blendof technical and communications proficiency in the risk organization. excludingtechnical experts can lead to false or incomplete messages or the appearance orreality of the willful manipulation of facts. excluding those with public affairsfunctions provides a danger of insensitivity to the capacities, interests, and needsof the audience.risk managers need to use procedures that attain a balance between twodistinct types of expertise: the risk subject matter (e.g., carcinogenic risk,occupational safety) and risk communication. organizations thatcommunicate about risk should take steps to ensure that the preparation ofrisk messages becomes a deliberate, specialized undertaking, taking carethat in the process they do not sacrifice scientific quality. such steps include: deliberately considering the makeup of the intended audience anddemonstrating how the choice of media and message reflects anunderstanding of the audience and its concerns; attracting appropriate communications specialists and trainingtechnical staff in communications; requiring systematic assurance that substantive risk experts within theorganization have a voice in producing accurate assessments and thederivative risk messages; establishing a thoughtful program of evaluating the past performanceof risk communication efforts; and ensuring that their organizations improve their understanding of theroles of intermediaries, particularly media reporters and editors,including an understanding of the factors that make a risk storynewsworthy, of the practical time and space constraints, and of thelimited technical background of most media personnel.recommendations for improving risk communication160improving risk communicationcopyright national academy of sciences. all rights reserved.assessment of audienceas noted above, a source organization should, before it initiates riskcommunication, set realistic goals; it should make a deliberate effort to formulateits communication objectives, identify intended audiences, consider alternativecommunication strategies, and assess the likely usefulness of a message to theaudience.risk communication cannot be considered an informal addon to thetechnical assessment effort. effective risk communication involves specializedknowledge of, and when feasible interaction with, the intended target audience,an understanding of media practices, and an appreciation of the role of otherintermediaries in relaying and translating messages. those who assess risks andrisk control options within an organization are not usually experienced in theseareas.as soon as the organization's risk communication objective is established,analysis of and interaction with the target community, or its representatives,should commence; deliberate audience research is important. ideally, an audienceprofile should be compiled that describes the nature of the members of theaudience and gives some idea of whom they trust, what they believe, and whatconcerns and worries motivate their actions; focus groups, surveys usingrepresentative sample techniques, and demographic studies may be helpful incompiling the profile. available time and resources do not always permit thecompilation of a detailed profile, but the risk manager should realize that riskcommunication will suffer to the extent that the audience is mischaracterized. theresults of such audience research should be made public in a timely manner.failure to do so may undermine the apparent openness of the organization.specific knowledge of the intended target groups permits intelligentsegmentation of the audience, another key to effective communication. a uniformmessage will have varying effects on different individuals. audiencesegmentation is useful both to customize the message and in the choice of themost effective communication channels. risk communicators need to be awarethat individuals may prefer to use different channels for different aspects of theirdecisionmaking process. some channels, for example, may be best for conveyinggeneral knowledge but less reliable for affecting whether individuals believe arisk is or is not something to be worried about.risk managers should expect those who prepare risk messages to construct acommunication plan that clearly links the choice of channel and customizedmessages to an understanding of audiencerecommendations for improving risk communication161improving risk communicationcopyright national academy of sciences. all rights reserved.segments and that links the definition of audience segments to an objectiveunderstanding of the audience population.two caveats should be noted. first, while an explicit analysis of theaudience is important, it should not be expected to supplant all otherconsiderations in planning for the risk message. other important organizationalgoals (legal constraints, consistency of current and past policy, support of currentenforcement efforts) must be factored in. the risk manager's difficult job is toattain a reasonable balance among these competing organizational objectives.second, but no less important, the risk manager must be concerned with theoutside appearance of the explicit communication planning effort. observers mayrightly or wrongly perceive a deliberate effort to understand, segment, and reachthe audience as inherently manipulative and invasive.specialized talentrisk communication requires specialized knowledge and talent. it may bedifficult to adequately reeducate technical or other existing staff to coordinate themessage preparation effort. preparing and helping implement the explicit riskcommunication plan described in the previous section require special expertise.specialized knowledge in such subjects as demographic techniques, thepsychology of risk perception, and how the media work, combined with the rareknack for writing clearly about complicated technical issues, is needed.recruiting staff with such capabilitiesšor retraining existing staffšamounts to putting the task of risk communication on a professional level in theorganization in order to achieve betterinformed risk communication decisions.however, skeptics inside and outside the organization may see it as importingdubious strategies and techniques from marketing and advertising into theheretofore scientific domain of risk assessment. vigilance must be applied (openprocedures can be of great value here) to ensure that such techniques do notbecome manipulative or deceptive in fact or appearance.scientific/technical accuracy and completenessupgrading the staff that coordinates the preparation of risk messages to aprofessional level does not mean that substantive experts within thecommunicating organization can be shunted away from the process. they mustremain involved in order to ensure that factual errors are not introduced.recommendations for improving risk communication162improving risk communicationcopyright national academy of sciences. all rights reserved.technical inaccuracy and incompleteness in message content can easily beused by knowledgeable advocates of alternative positions. for example, nationaladvocacy groups, including consumer and environmentalist groups, usecompetent scientific/technical professionals in presenting positions and incountering government and corporate press releases. once a message has beenshown to be inaccurate or misleading, organizational credibility is lost for thatmessage and for succeeding messages on quite different topics.a technically flawed risk message may reflect poor risk communicationwithin the communicating organization. to prevent this, risk managers shouldrequire that senior technical staff have an opportunity to evaluate the quantitativeand qualitative accuracy of risk messages and that any exceptions are clearlyreported.evaluation and feedbackeven when communications professionals help design and guide the riskcommunication effort, doubt will remain about whether and how the intendedaudience will apprehend the message.source organizations should routinely conduct retrospective evaluations oftheir communication efforts and of particular messages. at this stage thereappears to have been remarkably little formal evaluation by organizations thatcommunicate about risks. evaluation, if coupled to a feedback mechanism, is anecessary step in ensuring improvement in the competence of an organization'srisk communication program. organizations that disseminate risk messagesshould institute formal programs that assess their experience. evaluations shouldaddress both questions of content and questions of process, as described in thisreport. that is, the effectiveness of messages should be examinedšalong with asense of how different channels and intermediaries affected transmissionšbutattention also needs to be devoted to the organization's performance with itsprocedures for setting realistic goals, involving interested and affected parties,attaining balance, and creating internal expertise in risk communication.role of intermediariesmost risk messages will pass through one or more organizations orindividuals before reaching the final recipients. sometimes the only way ofensuring that a message reaches the people for whom itrecommendations for improving risk communication163improving risk communicationcopyright national academy of sciences. all rights reserved.is intended is to rely on intermediaries. in any case an organization initiating riskcommunication should identify the intermediaries who will handle its messages,assess their needs and constraints, and adjust to those conditions if possible.journalists look first for clear statements about events and issues at conflict.they operate under strict deadlines and compete for allocations of space or time.providing journalists with written copy will reduce, but not eliminate, the chancesof being misquoted. regular contact with journalists, including after stories haveappeared, will generally improve the basis for later exchanges.community organizations and prominent individuals can be effectiveintermediaries for risk messages. but health departments, public libraries,professional associations, voluntary organizations, and similar groups all havetheir own aims and purposes. discovering which organizations or individualswould be appropriate in a particular situation and developing the workingrelationship that is necessary for constructive interaction with such intermediariescan require considerable time and effort. it can, however, make the difference inreaching the intended recipients.some notes on handling risk communication in crisisconditionsmany risk situations require that risk messages be delivered immediately:examples include emergency conditions, challenges to an organization's positionsbefore the organization is prepared to respond, and intense and contentious publiccontroversy. in that atmosphere the deliberate procedures recommended above(e.g., outside reviews and analysis of the audience) may well be impractical.the process for risk communication in crisis conditions requires specialcare. risk managers should ensure that: where there is a foreseeable potential for emergency, advance plans forcommunication are drafted. these plans should be drafted jointly with theintended audiences (e.g., local communities near a chemical plant,paramedics, and fire departments). such plans should be prepared in thecontext of concrete events and scenarios, should provide specificinformation that is relevant to people's riskaverting actions, and shouldspecify actions that may be taken in case of a disaster or emergency; and there is provision for coordinating information among the variousauthorities that might be involved and, to the extent feasible,recommendations for improving risk communication164improving risk communicationcopyright national academy of sciences. all rights reserved.a single place where the public and the media can obtain authoritative andcurrent information.the content of risk messagesthe preceding section is addressed to risk managers, who have overallresponsibility within their organization for assessing risks, making riskmanagement decisions, and managing risk communications. this section isaddressed to those within the communicating organization who are responsiblefor preparing formal risk messages.in general, we find that practical advice on the content of risk messagesdepends heavily on the particular situation; for example, a public health advisorymessage on aids and an epa announcement on the regulation of the use of apesticide for certain crops may have quite different purposes, audiences, urgency,and visibility. we concentrate here on four generic mattersšaudienceawareness, uncertainty, comparative risk, and completenessšthat have been thesource of difficulty in the past over a broad range of risk communication efforts.relating the message to the audiences' perspectivesrisk messages are often based on the information in special analysesprepared for internal organizational purposes (e.g., to assess whether a particularrisk exists or what risk management option to choose). that information oftenreflects the prior knowledge, perspectives, and language of risk experts and riskmanagers. it may not be sufficient for effective risk messages.risk messages should closely reflect the perspective, technical capacity,and concerns of the target audiences. a message should: emphasize information relevant to any practical actions thatindividuals can take; be couched in clear and plain language; respect the audience and its concerns; and seek strictly to inform the recipient, unless conditions clearly warrantthe use of influencing techniques.personal relevanceconsideration of the specific decisions that recipients face provides thesurest basis for determining what risk information to emphasize in a riskcommunication. such decisions might be whetherrecommendations for improving risk communication165improving risk communicationcopyright national academy of sciences. all rights reserved.and how to change personal behavior to respond to a reported health risk,whether to use or avoid a product that is being regulated, how to vote on a localsiting issue, and whether to follow a particular risk issue further.much of the information available to those who prepare formal riskmessages has been assembled in risk assessments prepared in a context of riskmanagement decision making. the basic question in such assessments is, ﬁwhatshould the organization do (if anything) to reduce risk to the population?ﬂestimates of total exposures, total risk reduction costs, and other aggregate datašoften written by experts whose immediate objective, understandably, is tomake them scientifically defensible in the eyes of other expertsšoftenpredominate. the central question answered in a risk message should be ﬁwhatshould the recipient know to improve the choice among personal options(including the consequences of doing nothing)?ﬂ data and analyses that riskexperts have not emphasized may be needed. in the terms of decision theory, arisk message should contain information to which those decisions areﬁsensitiveﬂšthe facts that are most central to the choice at hand. this criterionshould determine the kinds of information included and the detail and precisionwith which it is presented. for some decisions the critical information is themagnitude of the risk involved; for others it is the processes by which risks arecreated and controlled.risk information should be expressed in terms of risk to a representativeindividual, not only as a general population estimate. if there are highly exposedor particularly sensitive subgroups, such groups should be identified in a way thatindividuals can understand if they have reason for concern. practical advice onsuch matters as danger signals of exposure, available remedies, sources of help,and so on should be included.selecting information relevant to individual choice is particularly importantfor risk messagesšhealth warnings are prime examplesš that are intended for anaudience that is not already motivated to listen. the existence of such risks maymean little if it is not made clear what practical measures an exposed individualmight use to avoid or reduce them.claritythe risk message should be understandable to the target audience. whenthere is doubt about the ability of the audience torecommendations for improving risk communication166improving risk communicationcopyright national academy of sciences. all rights reserved.absorb technical material, little is lost by assuming that the audience has littletechnical training. carefully chosen, vivid, concrete images and the use ofpersonalized examples can help a lay audience to understand and can often evenensure understanding among those who are more familiar with the subject risk.message designers should try to avoid using images or terms (ﬁmorbidityﬂ is oneexample) that, while seemingly familiar to laypersons, have different or moreprecise technical meanings for experts.special care is needed in depicting statistical concepts and probabilities. fewpeople can meaningfully distinguish among small probabilities and may have noway of determining if such an assessment as ﬁ1in10,000 lifetime riskﬂ is worthworrying about.in a long message with extensive technical detail or quantitative complexity,the key portionsšconclusions, summary, and recommended actionsšshould bewritten in lay terms.the pursuit of clarity is likely to be enhanced by experimentation, post hocevaluation, and the pretesting of messages with laypersons, all of which havebeen discussed above.in our view clarity is a necessary but not a sufficient condition for improvedrisk messages. this is because while such features as plain language and vividexamples can enhance understanding, they can also, if misused, potently enhancemisunderstanding. one pitfall is that of equating clarity with brevity. themessage preparer's goal should not be to gloss over the complexity anduncertainty of a risk but to reflect those qualities in plain language. those whoprepare risk messages should expect that their attempts at brevity will provokeprotest among those who fear it will lead to greater misunderstanding. where thenature of the chosen communication channel requires a short message, as withmass media announcements, this of course poses an unavoidably difficultdilemma and one in which the procedural measures recommended above (e.g.,openness and message pretesting) may be vital.respect for the audience and its concernsif a message appears insensitive to an audience's actual concerns, there is areal chance that the audience will be alienated. the message should not disparagepeople's subjective reactions as inferior to expert assessments. if members of theaudience hold beliefs that the source organization sees as false, it is better for themessage to address these beliefs than to omit them as irrelevant. if peoplerecommendations for improving risk communication167improving risk communicationcopyright national academy of sciences. all rights reserved.are advocating specific options for reducing risk, the message should addressthem, even if the source organization's analysis shows the options to be infeasibletechnically or legally. the impact of a message is, of course, determined as muchby style and general demeanor as literal message content.in all cases, but particularly in facetoface delivery of messages, care shouldbe taken to show compassion and to avoid distant, antiseptically statisticaltreatment of illnesses, injuries, and death. for example, if a person is gravelyconcerned about a particular hazard, a message that dismisses the risk as triviallysmall will surely come across as coldly patronizing.the best way to summarize this general point is to observe that legitimacy isinherently reciprocal in nature; only if a source acknowledges the legitimacy ofthe audience's felt concerns will it have a chance to be seen as legitimate itself.use of ﬁinfluence strategiesﬂthose who prepare risk messages, and particularly those in governmentorganizations, need to be circumspect about using ﬁinfluence strategiesﬂ in theirrisk messages to influence recipients' beliefs or actions, and they should expecttheir audiences to suspect attempts to influence even when the intent is simply toinform.americans are usually most comfortable with risk messages that, injefferson's words, ﬁinform their discretion,ﬂ but that do not attempt to advise themhow to act in response. some would draw a line for risk messages at the functionof describing the risks and other outcomes (e.g., costs) associated with alternativerisk management options, claiming that to go further involves the application ofvalue judgments that are beyond the proper reach of the message source. otherswould point out that governments make such judgments commonly, as when theyby law or regulation establish sanctions against certain private actions (e.g.,polluting, littering, and not wearing seat belts). in addition, americans want theirpublic servants to be strongly committed to the pursuit of their agency's nationalmission, and such individuals understandably form strong views on what they seeas the correct ways to respond to problems. they will want to express thosestrong views. not understanding many of the concepts presented in this report,these public servants may see a dilemma with respect to their role as riskcommunicators.recommendations for improving risk communication168improving risk communicationcopyright national academy of sciences. all rights reserved.in chapter 4 we introduced a distinction between two risk messagestrategies, informing and influencing. influence strategies comprise a range oftechniques ranging from, at one end, messages that attempt to ﬁpersuadeﬂ throughthe selective use of factual information to outright deception at the other extreme.the user's intent is to convince a recipient to accept the source's opinions orprescribed actions. the audience response to a message that is seen as usinginfluence strategies may be to count it as illegitimate. the audience may disregardthe entire risk message as slanted toward a predetermined outcome. credibility is acasualty.when is an influence strategy appropriate? as we noted in the discussion inchapter 4, americans accept influence strategies in some settings. dietarywarnings by public health officials are an example of the generally accepted useof influence. there is some indication that reigning traditions vary according tothe culture of the professional field of the risk assessorsštraditions in damsafety, for example, may differ from those in toxicology with respect topractitioners' efforts to prescribe policy or personal choices. risk managers ingovernment agencies would be well served to know when the use of influencestrategies is safely legitimate and when it is not.in general, we would urge great care in the use of influence strategies bygovernment agencies. we have identified three particular situations in which useof influence strategies by government may arouse resentment that could affect thecredibility of a message and/or source:1. when there is unresolved public controversy over the issue, particularly ifthere has been no public forum at which relevant voices have had their say.whenever government attempts to influence citizens' beliefs and actions, itshould be able to point to some legitimate public processšone that hasgiven interested and affected parties a chance to express themselvesšwhich concluded that using risk messages to influence behavior serves animportant public purpose.2. when the form of influence strategy is toward the more severe end of thespectrum of influence techniques (i.e., near deception).3. when there is no evident threat of externalized effectsšthat is, when therisk is confined largely to the persons who themselves undertake the riskybehavior, without endangering others.when influence strategies are used, risk messages should attempt todistinguish the analytic function of describing risk from therecommendations for improving risk communication169improving risk communicationcopyright national academy of sciences. all rights reserved.prescriptive function of advising recipients about what to do. it is an error toimply that the technical analysis led irrefutably to the prescription. theorganization that disseminates the message should make clear that, in balancingrisks and other factors to arrive at its recommended action, it has made a policyjudgment. it is, of course, politically accountable for such judgments.handling uncertaintyuncertainty is a central fact in the assessment of many contemporary risks.it is usually present both in risk assessment and in the assessment of riskmanagement options. the way that risk messages treat this uncertainty can have amajor influence on the effectiveness and credibility of a communication effort. amajor difficulty is avoiding unnecessary confusion between scientific uncertaintyon one hand and policy disagreement on the appropriate risk managementapproach on the other hand.risk messages and supporting materials should not minimize theexistence of uncertainty. data gaps and areas of significant disagreementamong experts should be disclosed. some indication of the level of confidenceof estimates and the significance of scientific uncertainty should beconveyed.there are dangers if existing uncertainty is widely perceived as eitherunderplayed or exaggerated. any attempt to minimize uncertainty may make itappear that the caveats expressed by experts are being ignored. exaggeratinguncertainties can have the effect of obscuring the scientific basis of a riskmanagement decision (e.g., whether to regulate, whether to issue a healthadvisory to the public), leaving the audience with the impression that the decisionhas been arbitrary in nature.one reason the effectiveness of risk messages is so sensitive to theirtreatment of uncertainty is that the handling of uncertainty is a central issue inmany of today's risk controversies. often one side in a controversy willemphasize the need to base important risk management decisions on soundscience, rather than on mere conjecture. as often, the other view will emphasizethat ordinary prudencešﬁbetter safe than sorryﬂšdictates that action can betaken before conclusive scientific proof comes in. a central dispute thusbecomes, ﬁhow much proof is needed?ﬂ and the degree of extant scientific proofitself becomes a matter of close partisan scrutiny.recommendations for improving risk communication170improving risk communicationcopyright national academy of sciences. all rights reserved.those who prepare risk messages commonly must choose betweenpresenting the full range of available estimates, presenting a restricted set, oroffering a single estimate based on consensus among consulted experts. choosingany of these methods has its dangers, and more complicated presentations of therange of uncertainty are often needed; what is appropriate depends on the dataavailable for a particular case.it is usually dangerous for messages to characterize the overall level ofuncertainty quantitatively, as might be done by describing statistical confidenceintervals. in most situations expert assessments have multiple sources ofuncertainty, and statistical measures do not adequately represent the complexityof the analysis.for many messages an extensive description of uncertainty obviouslycannot be included in the text itself. however, it remains useful to have preparedan explicit account, even if for practical reasons it must be consigned tosupplementary documents made available to recipients upon request.in general, those preparing risk messages are best served if they haveavailable to them a statement of the scientific conclusions of the assessment of aprofessional quality that might be used for materials intended for expert peerreview, such as papers submitted to professional journals; this will help ensurethat uncertainties and necessary qualifications are adequately conveyed from theexperts to those who prepare messages.a form of sensitivity analysis can be helpful. to gauge the significance ofuncertainty and of differences among experts, it is frequently helpful to vary thedifferent sets of expert estimates systematically and then to gauge the effects onthe overall risk estimate. the estimate will be more sensitive to some choices ofassumptions than to others. if the risk message uses one of the competingassumptions, the risk message should say so, disclose why it was chosen overothers, and indicate what difference it makes to the assessed risk. it should beobserved that this procedure is one for which the needs of risk communicationmay dictate how a risk assessment itself is done.the general goal of this recommendation is to help audiences distinguishareas of scientific agreement amid what may appear as vast areas of policydisagreement. the advantage of careful delineation of existing scientificuncertainty is that it gives audiences a sense of the degree of scientific consensusand allows them to distinguish minor from major uncertainties.recommendations for improving risk communication171improving risk communicationcopyright national academy of sciences. all rights reserved.comparing risksone factor that inhibits public understanding of risk messages is that peopleoften cannot easily relate the lowšsay, 1 in 10,000š risk probabilities presentedto their everyday experience. they are thus often deprived of a sense of thepersonal meaning of the risk in question and so cannot arrive at a comfortabledecision about whether to take actions to deal with the risk or whether to beconcerned at all about the hazard. in theory, at least, this difficulty can beovercome by quantitative comparisons between risks between familiar and lessfamiliar risks).risk comparisons can be helpful, but they should be presented withcaution. risk comparisons must be seen as one of several inputs to riskdecisions, not as determinants of decisions. there are proven pitfalls whenrisks of diverse character are compared, especially when the intent of thecomparison can be seen as that of minimizing a risk (by equating it to aseemingly trivial risk). more useful are comparisons of risks that help conveythe magnitude of a particular risk estimate, that occur in the same decisioncontext (e.g., risks from flying and driving to a given destination), and thathave a similar outcome. multiple comparisons may avoid some of the worstpitfalls. more work needs to be done to develop constructive and helpfulforms of risk comparison.in theory, at least, comparative information should be an attractive elementof risk messages. we have advised that the best risk messages are those thatinform the recipient's actual choices, and increasingly those choices are betweencourses of action (or inaction) that represent different risks. risk comparisonsideally might help individuals steer a prudent course between risks of varioussizes.however, actual attempts to compare risks have engendered considerablecontroversy and distrust. one reason for this is the fear that comparisons will beused to influence and even mislead the lay public. individuals are known, forexample, to subjectively underestimate actual incidence rates for some fatal risks(e.g., those resulting from asthma and strokes) and to overestimate others (e.g.,risks that are especially feared, like those resulting from tornadoes and botulism).thus, comparing a risk to the likelihood of death by asthma would probablyinduce most people to similarly underestimate it.another difficulty is that alternatives often have more than one riskattribute, and different people emphasize different facets. for a particular choice,for example, one group might concentrate on the relative number of deathsassociated with each alternative, and arecommendations for improving risk communication172improving risk communicationcopyright national academy of sciences. all rights reserved.second group may emphasize the way risks (or costs) are distributed amongvarious groups within society. the choice of any single metric for comparisonwill thus ignore facts that some observers may value highly.some who have used comparative risk information seem to have done so onthe assumption that recipients would, upon seeing that a particular risk is small,elect to stop worrying about it. implicit was the notion of an ﬁaction threshold,ﬂor perhaps a ﬁworry threshold,ﬂ that would render subthreshold risks unfit forserious consideration. the notion has a debilitating flaw, and it is not surprisingthat risk comparisons that seemed to be used to trivialize certain risks met withobjections. personal and organizational risk management decisions are based onmany factors, of which a risk estimate is only one. for example, even a trivial riskmay be worth eliminating if the costs of elimination are negligible; to suggestthat people should decide based on one factoršfor example, expected mortalityšalone is somewhat analogous to saying people should make purchases basedsolely on comparative pricing without considering the value of the product tothem. in practice, risk comparison data can rarely be closely linked to specificdecisions in the absence of other critical information about decision options.in general, comparisons of ﬁunlikeﬂ risks should be avoided, as they haveoften either confused message recipients or irritated them because they were seenas unfair or manipulative. directly comparing voluntary (e.g., skiing) andinvoluntary (e.g., air pollutants) risks, or natural (e.g., earthquakes) andtechnological (e.g., food additives) risks, for example, is rarely a good idea. moregenerally, those who prepare risk messages should appreciate the weakness ofrisk comparisons as a means of placating people about risks that are calculated tobe small.when can comparisons be used in a risk message? three situations suggestthemselves:1. to help message recipients comprehend probabilities. in isolation a termlike ﬁone chance in a million per yearﬂ may convey little. an analogy tolengths (1 inch to 16 miles) or volumes (1 drop to 16 gallons) may helpsome people; reference to other known onetoamillion risks of the typeunder discussion (for lung cancer, that of smoking a certain number ofcigarettes; for private transportation mortality, that of traveling 300 milesby car) may help others, if they have a grasp of the reference risk.recommendations for improving risk communication173improving risk communicationcopyright national academy of sciences. all rights reserved.2. to directly compare alternative options. personal and organizationaldecisions can be better informed if the risks of alternative actions are laidout in comparable terms. comparing the risks of coffee and teaconsumption, or the risks of air and automobile travel between two points,may improve one's ability to make informed choices (again, however, onewould not expect a risk comparison to necessarily dominate in suchchoices). for a regulatory agency the health risk of a pesticide may bedirectly compared to that of its substitute if it were removed fromcommerce.3. to gauge the relative importance of different causes of the same hazard.discussions of public and private actions with respect to indoor radon maybe improved, for example, by a comparison of radon with smoking andother known causes of lung cancer.one interesting approach is the use of risk ladders, for which a range ofprobabilities is presented for a single class of risks. the discussion of figure 5.1shows the limitations of past use. if one is careful, however, the use of multiplecomparisons helps counteract the possibility that people may severelymisestimate a particular risk, even though it is familiar to them. it also reducesthe danger of arousing the scientific disputes that can often arise when only tworisk estimates are compared, one or both of which are subject to scientific debate.ensuring completenessif the information in a risk message is incomplete, the recipients may beunable to make wellinformed decisions.a complete information base contains five types of qualitative and/orquantitative information: (1) the nature of the risk, (2) the nature of thebenefits that might be affected if risk were reduced, (3) the availablealternatives, (4) uncertainty in knowledge about risks and benefits, and (5)management issues. there are major advantages in putting the informationbase into written form as an adjunct to the risk message.those who prepare risk messages should ensure that the messages arecomplete. a suggested risk information checklist of relevant topics for the designof a complete message, drawn from the description in chapter 2, is summarizedin figure 7.1.two points are worth emphasis. first, a complete risk message, as we havedefined it, includes information other than a risk assessment; it covers thecharacterization of current or possible efforts torecommendations for improving risk communication174improving risk communicationcopyright national academy of sciences. all rights reserved.reduce risk. some topics include the cost of control, who pays, how effective theapproach is, and whether the control implies additional risks of its own.uncertainty in the analysis of risk control measures should be included. themessage should also contain pertinent information about how any riskmanagement decision has been or will be made.figure 7.1 risk message checklist.recommendations for improving risk communication175improving risk communicationcopyright national academy of sciences. all rights reserved.second, the checklist used for preparing a complete risk message should beused to ensure that the underlying analysis itself is complete; that is, concern forrisk communication should influence the conduct of risk assessment and riskcontrol assessment. if the information base developed in the analytic process isincomplete, the risk message will be deficient.there are advantages to compiling and keeping the information base inwritten form. in at least some cases, for example, it will prove useful to compile aﬁwhite paperﬂ of factual information on the subject risk. as described in thesection above on management of the process, a written record provides a usefulmanagement tool for risk communication; if the underlying information is inwritten form, it can be examined (and perhaps improved) by others inside andoutside the organization, helping to prevent surprises when the risk message isdisseminated. such a document also can provide a useful single source for diversemessages, enhancing consistency and accuracy. when feasible, this documentshould be made available as an adjunct to the formal risk message.whether or not the information base is compiled in written form, riskcommunicators should treat itšand be seen as treating itšas work in progressthat is continually subject to improvement. discussions and debates that surrounda risk message often raise new questions, and new data can arise from researchand other sources.a consumer's guide to risk and riskcommunicationa major theme of this report is that risk communication should beunderstood to be a twoway interchange between source organizations and those,including the public and its representatives, who are the intended recipients ofrisk messages. in the previous pages we have directed many recommendationsabout the process and content of risk communications efforts to sourceorganizations, specifically government agencies and large corporations.if risk communication is a twoway enterprise, both sides have rights andresponsibilities that must be understood if the process is to work well. thefollowing recommendation is directed at improving the recipient's ability toparticipate meaningfully in risk management and risk communication. it is basedon the conclusion that, at this stage, nonexpert participants have differentunderstandings of the nature of risk and how it is managed. it is also based on therecommendations for improving risk communication176improving risk communicationcopyright national academy of sciences. all rights reserved.conclusion that the risk communication process would benefit if the interestedpublic were better able to ask intelligent, probing questions of those ingovernment, industry, and elsewhere who prepare risk messages for theirconsumption. as source organizations become more accomplished at riskcommunication, we expect that there will be more opportunities for twowayinteractions. we believe there needs to be a national locus for improving thepublic's ability to participate.major government and private organizations, including environmentaland consumer groups, that sustain risk communication efforts should jointlyfund the development of a consumer's guide to risk and riskcommunication. the purposes of this guide would be to articulate keyterms, concepts, and tradeoffs in risk communication and risk managementfor the lay audience, to make audiences better able to discern misleading andincomplete information, and to facilitate the needed general participation inrisk issues.such a guide should: involve support from, but not control by, the federal government andother sources of risk messages; be under the editorial control of a group that is clearly oriented towardthe recipients of risk messages, and under administrative managementby an organization that is known for its independence and familiaritywith lay perspectives, and that can undertake the needed outreach andpublic information effort; and cover subjects such as those suggested belowše.g., the nature of riskcommunication, the concepts of zero risk and comparative risk, andevaluating risk messagesšand others designated by projectparticipants.we believe that the development of such a guide would have severaladvantages. it would help orient the interested publicšand the leaders oforganized groupsšand prevent some of the misunderstanding that has occurredin the past. it would provide nonexpert participants with tools and concepts toenhance their participation, including sections about how to identify incomplete,imbalanced, or misleading messages. the process of writing it would advancenational discussion about areas of current controversy among players in an oftenadversarial process of making risk management decisions. the guide would alsoarticulate the basis for public skepticism that sometimes causes consternationamong those responsible for risk management and the design of risk messages.recommendations for improving risk communication177improving risk communicationcopyright national academy of sciences. all rights reserved.project supportit is important that major risk communicatorsšfederal agencies, largecorporationsšsupport the project. we would expect that the project wouldrequire about 1 year to complete and that it would require a fulltime staff of twoor three persons. allowance should be made for wide distribution of gratis copiesof the final document. provision should be made to update the guide 3 to 5 yearsafter it is published; updating will help ensure that there is a national focal pointfor continuing interactions among the groups that, together, can bring aboutlongterm improvement in risk communication.project managementeditorial control of the guide should be exerted by a steering group in whichthe views and concerns of the lay recipients of risk messages are paramount. itshould not be difficult to identify individuals who reflect an appropriately broadrange of lay perspectives. the steering group should also include a minority ofother relevant perspectives (e.g., risk managers, scientists and other experts,media representatives, and advocacy groups).the project requires a stable but independent administrative home. forpractical reasons it would be most suitably placed under the aegis of an existingorganization in order to permit an efficient startup and a reliable dissemination/outreach phase. the administrative home should be one that is credible to allsides involved in risk management issues and one that has demonstrable relevantexperience. the league of women voters and the national safety council aretwo of several organizations that meet these criteria.an integral part of the project should be the design of a dissemination effortthat, among other possibilities, makes use of compatible existing efforts at publicoutreach involving aspects of risk by professional (e.g., american barassociation, american medical association, american chemical society) andother groups.content of the guidewe offer a brief topic list as representative of subjects to be covered in aconsumer's guide (see figure 7.2). in addition to coverage of these pointsšandother subjects raised during the guide project itselfšthe guide might contain adirectory of information resources on risk topics for the lay public and groupsthat represent it.recommendations for improving risk communication178improving risk communicationcopyright national academy of sciences. all rights reserved.figure 7.2 a consumer's guide to risk and risk communication.research needsas a result of our deliberations, we recommend the nine specific researchtopics listed below. some stem directly from the problems identified in chapter 6.others are based on our review of available information and the substantialpractical experience of committee members. two criteria guided our selection oftopics: (1) additionalrecommendations for improving risk communication179improving risk communicationcopyright national academy of sciences. all rights reserved.knowledge would lead to material improvement in risk communication practicesand (2) creation of such knowledge is likely, given past results and currentresearch methods. we have not set priorities among the topics.risk comparisonif performed thoughtfully, risk comparison holds promise of making riskcommunication more relevant and meaningful to recipients. however, threeissues need to be explored to prevent past shortcomings of the technique. comparability. when are two risks ﬁsimilarﬂ enough in nature to becompared without misleading, confusing, or angering recipients? what arethe crucial dimensions across which risks should not be compared? apprehension of risk magnitudes. how do people apprehend themagnitudes of risks; in particular, how do they interpret very smallprobabilities, which often seem beyond most people's intuitiveunderstanding? how do different ways of presenting risk magnitudes affectpeople's feeling for the size of risks? validation. the use of risk comparisons is undermined if there is doubtabout the validity of the data that are compared. risk estimates used in riskcomparisons must be validated in two ways: (1) as to the current scientificaccuracy and the associated uncertainty or qualifications and (2) as towhether nonexperts are known to systematically underestimate oroverestimate such estimates subjectively (which would make theminappropriate as ﬁanchorsﬂ in risk comparisons).risk characterizationwe need better ways of presenting complex information about risk clearlyand accurately and better understanding of the limitations of techniques forsimplifying complex material. how do people respond to alternative ways ofcharacterizing risks, including alternative treatments of uncertainty?role of message intermediarieswe need a better empirical base for understanding the role of intermediariesin carrying and translating risk messages. what channels (mass media,specialized media, advocacy groups, communityrecommendations for improving risk communication180improving risk communicationcopyright national academy of sciences. all rights reserved.organizations, local professionals and other opinion leaders, casualacquaintances) do people actually use? how do people validate and integratemessages from multiple sources in deciding what to do, or what to believe, about aparticular risk? case examinations and a review of research in allied fields (e.g.,medical education) can help elucidate the direct and indirect flow of informationfrom source to recipient.pertinency and sufficiency of risk informationrisk communicators need to focus on the information that is most pertinentto recipients' needs; they are in danger of wasting the limited access they have totheir audience if they are viewed as preoccupied with marginal issues. what typesof information do people actually find pertinent in reaching personal decisionsabout risk? how does this compare with what the risk manager or decisionanalyst thinks should be pertinent? how and when do people determine that theydo not need additional information in order to decide what they will do about arisk? what information appears necessary to trigger active personal concernabout a risk?psychological stressgiven the number and variety of known risks in modern life, whatconditions are necessary to induce stress about a particular risk in persons andcommunities? which of the messages that appeal to fear, or that advert toimminent danger, actually cause stress? if people are stressed about a particularrisk, how is their apprehension of risk information affected?recipients' ﬁmental modelsﬂthe information in risk messages is useful only if recipients can incorporateit into their prior thinking about the risk and its management. only by betterknowing how recipients conceptualize risks and their risk decisions can peoplecreate more effective messages. in particular: how do people think about the risk decisions that confront them? forexample, what alternatives do they consider, and what consequences arethey aware of?recommendations for improving risk communication181improving risk communicationcopyright national academy of sciences. all rights reserved. how do people think about the causal processes that create risks? forexample, do they misconceive exposure processes, and how effective docontrol efforts seem, intuitively? how do people perceive the social/governmental processes involved inmanaging risks? for example, what do they believe regulatory agencies areempowered to do, and when are public interest advocates seen as credible?risk literacyhow do people learn the ﬁanalyticﬂ concepts and language they need tounderstand risk statements? do they lack important concepts? what kinds ofmaterials, including special curricular materials in science and mathematicseducation, might be effective?retrospective casesthere is a dearth of case studies that focus directly on risk communication.in particular, retrospective case materials should be prepared that: examine risk communication processes, including such topics as the role ofexperts and others in message preparation, whether and how outside groupswere involved in risk management and risk communication decisions, andthe role of intermediaries in message transmission. analyze the responses of recipients and how the responses corresponded tothe expectations of the source.contemporaneous assessments of risk casestoo seldom are there attempts to learn from ongoing cases of riskmanagement. this is partly due to an understandable desire to concentrateresources on solving a risk problem, rather than calibrating it; nonetheless, realtime assessments can provide valuable knowledge for making generalimprovements in risk communication. this contemporaneous research shouldaddress such matters as how people react to different types of messages andchannels; what their actual concerns, frustrations, and data needs are; and howeffective alternative communication and message strategies are.recommendations for improving risk communication182improving risk communicationcopyright national academy of sciences. all rights reserved.appendixes183improving risk communicationcopyright national academy of sciences. all rights reserved.184improving risk communicationcopyright national academy of sciences. all rights reserved.appendix abackground information on committeemembers and professional staffcommittee membersjohn f.ahearne, chairman, is vice president of resources for thefuture, washington, d.c. a physicist specializing in systems and policy analysisin defense, energy, and resources, dr. ahearne served as a deputy assistantsecretary of defense for systems analysis, deputy assistant secretary of defensefor program analysis and evaluation, and principal deputy assistant secretary ofdefense for manpower and reserve affairs. dr. ahearne also served as systemsanalyst for the white house energy office (1977) and deputy assistant secretaryof energy for resource applications (1978). he was a member of the nuclearregulatory commission from 1978 to 1983 and was chairman from 1979 to1981. currently, he is chairman of the department of energy's advisorycommittee on nuclear facility safety.ernesta ballard is a private consultant on toxic substancemanagement in seattle, washington. as a regional administrator for theenvironmental protection agency from 1983 to 1986, she was responsible forimplementation and enforcement of environmental programs in alaska, idaho,oregon, and washington. ms. ballard served as director of public services forseattle (1976œ1978) and budget director of the university of washington(1974œ1976). she is chairman of the board of trustees of university hospital,university of washington; a member of the advisory board of albers school ofappendix a185improving risk communicationcopyright national academy of sciences. all rights reserved.business, seattle university; and a member of the board of trustees of thenature conservancy.ruth faden is professor of health policy and management, the johnshopkins university, baltimore, maryland, where she directs the program in law,ethics, and health, and is senior research scholar, kennedy institute of ethics,georgetown university. dr. faden has done extensive research and writing inethics and health policy and is coauthor of the book a history and theory ofinformed consent. she has served as a consultant to, among others, the nationalinstitute on alcohol abuse and alcoholism, office of technology assessment,president's commission for the study of ethical problems in medicine andbiomedical and behavioral research, and centers for disease control.james a.fay is professor of mechanical engineering, massachusettsinstitute of technology. dr. fay's areas of expertise include air pollution andenergy. he has been a member of maine's natural resources council and themassachusetts energy facility siting council, as well as chairman of the bostonair pollution control commission. dr. fay served on the national researchcouncil's environmental studies board and the committee on radioactive wastemanagement. he is a fellow of the american institute of aeronautics andastronautics, the american academy of arts and sciences, and the americanassociation for the advancement of science; a member of the american societyof mechanical engineers; and director of the union of concerned scientists.baruch fischhoff is professor in the department of engineering andpublic policy and the department of social and decision sciences at carnegiemellon university, pittsburgh, pennsylvania. earlier, dr. fischhoff spent 11years with decision research and eugene research institute, eugene, oregon,working in the areas of judgment and decision making, human factors, and riskmanagement. he has numerous publications in these fields, including the bookacceptable risk. dr. fischhoff is on the editorial boards of policy sciences,cognitive psychology, journal of personality and social psychology, accidentanalysis and prevention, social behavior organizational behavior and humandecision processes, and international journal of forecasting. he has served onthe national research council's committee on priority mechanisms for thenational toxicology program, panel on survey measurement of subjectivephenomena, committee on human factors, and committeeappendix a186improving risk communicationcopyright national academy of sciences. all rights reserved.on pilot performance modeling for a computer aided design and engineeringfacility.thomas p.grumbly is president of clean sites, inc., alexandria,virginia. mr. grumbly was executive assistant to the commissioner of the u.s.food and drug administration (1977œ1979); deputy administrator, food safetyand inspection service, u.s. department of agriculture (1979œ1981); and staffdirector, subcommittee on investigations and oversight, house committee onscience and technology, u.s. house of representatives (1981œ1982). he spentthree years as executive director of the health effects institute. mr. grumbly hasalso served as a consultant to the u.s. environmental protection agency in thearea of risk assessment and has served on the national research council's panelon reform of the federal meat and poultry system.peter barton hutt is a partner with the law firm of covington &;burling, washington, d.c. his expertise is in administrative and regulatory law.he served as chief counsel to the food and drug administration (1971œ1975). heis a member of the institute of medicine and serves on the advisory boards of theinstitute for health policy analysis, georgetown university; the scripps clinicand research foundation, la jolla, california; and the center for study of drugdevelopment, tufts university. mr. hutt has served on a number of nationalinstitutes of health, institute of medicine, and national research councilcommittees and on the advisory panels on technology innovation and health,safety and environmental regulation, animal testing, biotechnology, and medicaldevices for congress's office of technology assessment. he coauthored the bookfood and drug law: cases and materials and has written a number of bookchapters and articles. he has also worked and written extensively in the area ofdrug and alcohol abuse. mr. hutt serves on the editorial boards of regulatorytoxicology and pharmacology, food, drug and cosmetic law journal, andbiotechnology law report.bruce karrh is vice president, safety, health and environmentalaffairs, e.i. du pont de nemours & co., wilmington, delaware. he is a fellow ofthe american academy of occupational medicine, the american college ofpreventive medicine, and the american occupational medical association. dr.karrh is chairman of the board of directors of the chemical industry institute oftoxicology and of the american industrial health council. he is also a memberof the board of directors of thomas jefferson university and itsappendix a187improving risk communicationcopyright national academy of sciences. all rights reserved.clinical affairs committee. he is a diplomate of the american board of preventivemedicine, certified in occupational medicine.d.warner north is a principal with decision focus, inc., los altos,california, specializing in decision analysis; he is also a consulting professor,department of engineeringeconomic systems at stanford university, andassociate director of stanford's center for risk analysis, stanford, california.over the last 20 years dr. north has carried out applications of decision analysisand risk assessment to a variety of public policy issues. he has participated in sixprevious national research council studies on air quality and toxic chemicals,including the 1983 national research council's committee on the institutionalmeans for assessment of risks to public health. his recent work includesdevelopment of decision frameworks for risk management of coal combustionbyproducts and acid deposition. dr. north is a member of the scientificadvisory panel to the governor of california for the safe drinking water andtoxic enforcement act of 1986 (proposition 65). he has served on committeesof the science advisory board of the u.s. environmental protection agencysince 1979.joann e.rodgers is deputy director of public affairs and director ofmedia relations, the johns hopkins medical institutions, baltimore, maryland.ms. rodgers worked as a newspaper journalist specializing in science writing for20 years and continues as a freelance writer of books and magazine articles onscience and medicine. she is a past president of the national association ofscience writers and a vice president of the council for the advancement ofscience writing. she is the recipient of a number of science writing awards,including a lasker award, two american heart association awards, and theama medical journalism award. she is the author or coauthor of books on drugsand childrearing and hundreds of magazine articles. she teaches and lecturesfrequently on science communication.milton russell is professor of economics and senior fellow, wastemanagement research and education institute, university of tennessee,knoxville, and senior economist, oak ridge national laboratory, oak ridge,tennessee. he served as assistant administrator for policy, planning andevaluation at the u.s. environmental protection agency (1983œ1987). dr.russell was senior fellow and director of the center for energy policy researchat resources for the future and spent 2 years as a senior staff economist withpresident ford's council of economic advisors. he taught in iowa andappendix a188improving risk communicationcopyright national academy of sciences. all rights reserved.texas before joining the economics department at southern illinois university,which he subsequently led as chairman. dr. russell has coauthored some halfdozen books focusing on energy and resource economics, lectured widely, andauthored over 70 articles and chapters in journals and texts.robert sangeorge is vice president for public affairs of thenational audubon society, headquartered in new york city. he was a workingjournalist for 12 years (1972œ1984), including 3 years as the nationalenvironment and energy correspondent for united press international, based inwashington, d.c. he held several other assignments during 9 years of servicewith upi, including supreme court correspondent and bureau chief in cleveland,ohio. he also worked as a reporter/producer for 3 years in public broadcasting.prior to his present position with the national audubon society, he was theassistant to the president for public accountability of clean sites, inc.,alexandria, virginia. mr. sangeorge held a kiplinger foundation fellowship atohio state university in 1975œ1976.harvey m.sapolsky is professor of public policy and organization inthe political science department, massachusetts institute of technology. dr.sapolsky specializes in bureaucratic politics and science and public policy. hehas studied and written articles on risk, specifically concerning cigarettesmoking, the fluoridation of water, and aids and the blood supply, and hasrecently edited the book consuming fears: the politics of product risks. dr.sapolsky is a fellow of the american association for the advancement ofscience and a member of the american political science association and is onthe editorial boards of the journal of health politics, policy and law andinquiry.jurgen schmandt is professor, lbj school of public affairs,university of texas, and director, center for growth studies, houston arearesearch center. he has published books on nutrition policy, the acid raindispute between canada and the united states, and environmental and resourcepolicies. he recently served on the texas science and technology council.while serving as a senior environmental fellow at the u.s. environmentalprotection agency, he worked on the development of a strategy for the control oftoxic substances in the environment. from 1965 to 1970, dr. schmandt wasassociate director of harvard university's program on technology and society.at the organization for economic cooperation andappendix a189improving risk communicationcopyright national academy of sciences. all rights reserved.development in paris he directed the review series on science policy in membercountries.michael schudson is professor, department of communication anddepartment of sociology, and chair, department of communication, universityof california, san diego. dr. schudson's areas of expertise are the media andadvertising. he is the author of the books discovering the news: a social historyof american newspapers, advertising, the uneasy persuasion, and reading thenews, as well as many articles on the media. dr. schudson also serves ascorresponding editor for theory and society and is a member of the editorialboard of critical studies in mass communication.percy h.tannenbaum is professor of public policy and director ofthe survey research center, university of california, berkeley. his researchspecialties include communication behavior, attitude change and measurement,mass media functions and effects, telecommunications policy, and social researchmethodology. his recent books include tunedon tv/turnedoff votes: policyoptions for election projections and flies in the policy ointment: perspectives inthe california medfly crisis. dr. tannenbaum is a fellow of the americanassociation for the advancement of science and the american psychologicalassociation and was a resident fellow at the center for advanced study in thebehavioral sciences (stanford) and the institute for advanced study, berlin.detlof von winterfeldt is director, risk communicationlaboratory, and professor, department of systems science, institute of safetyand systems management, university of southern california. his specialties aredecision theory and risk analysis. dr. von winterfeldt coauthored riskcommunication: a review of the literature and decision analysis andbehavioral research. he is also a member of the institute of managementscience and the society for risk analysis, an associate member of operationsresearch society of america, an associate editor for operations research, and amember of the editorial board of risk analysis and risk abstracts.chris whipple is technical manager, risk and health sciencedepartment, environment division, at the electric power research institute, paloalto, california. dr. whipple's expertise is in the areas of analysis andmanagement of technological risks. he serves on the national research council'sboard on radioactive waste management and has served on national researchcouncil committeesappendix a190improving risk communicationcopyright national academy of sciences. all rights reserved.on the health and ecological impacts of synfuel industries and on nuclear safetyresearch. he also chaired the international atomic energy agency's coordinatedresearch program on risk criteria for the nuclear fuel cycle. he served on theadvisory committee to the national science foundation project on riskassessment and has been a contractor with the u.s. environmental protectionagency's office of air quality planning and standards. dr. whipple is a memberand past president of the society for risk analysis and a member of theamerican association for the advancement of science.susan wiltshire is senior associate at jk associates, hamilton,massachusetts, a consulting firm specializing in public policy formulation andcitizen involvement in technical decisions. ms. wiltshire is particularly involvedin issues of radioactive waste management. she is a member of the nationalresearch council's board on radioactive waste management and served on theboard's panel on uranium mill tailings. she was a member of the programreview committee for the national lowlevel waste management program andthe environmental/institutional review group for the office of crystallinerepository development. ms. wiltshire is former president of the league ofwomen voters of massachusetts, served as a member of the national league ofwomen voters nuclear energy education project advisory committee, andcoauthored the 1985 revision of the league of women voters' a nuclear wasteprimer. she is currently chairman of the elected board of selectmen of the townof hamilton and vicechairman of northeast health systems, inc., and of beverlyhospital, beverly, massachusetts.professional staffrob coppock is senior program officer with the commission onphysical sciences, mathematics, and resources of the national researchcouncil. dr. coppock was a staff scientist at the science center berlin in westgermany before joining the commission. he has conducted research in the areaof risk and regulation for several years and is the author of regulating chemicalsin japan, west germany, france, the united kingdom, and the europeancommunity: a comparative examination and social constraints ontechnological progress. he edited, with others, technological risk: itsperception and handling in the european community. from 1981 to 1987 hewas on the editorial board of the journal risk analysis.appendix a191improving risk communicationcopyright national academy of sciences. all rights reserved.lawrence e.mccray is executive director of the committee onscience, engineering, and public policy of the academies and the institute ofmedicine. he was associate executive director of the commission on physicalsciences, mathematics, and resources of the national research council untilaugust 1, 1988. dr. mccray has held positions with the u.s. environmentalprotection agency, the u.s. regulatory council, and the office of managementand budget. he was project director for the 1983 national research councilstudy on risk assessment in the federal government and a 1985 nationalresearch council study on the atmospheric effects of nuclear explosions. dr.mccray won the schattschneider award of the american political scienceassociation for the best dissertation in american government and politics in1973.paul c.stern is senior staff officer with the commission on behavioraland social sciences and education of the national research council. he also isstudy director of the national research council's committee on contributions ofbehavioral and social science to the prevention of nuclear war. he previouslyserved as study director of the committee on behavioral and social aspects ofenergy consumption and production at the national research council and asresearch associate at yale university's institution for social and policy studies.dr. stern's current research is on the formation of social attitudes aboutenvironmental policy. he is coeditor of energy use: the human dimension andcoauthor of the chapter ﬁmanaging scarce environmental resourcesﬂ in thehandbook of environmental psychology. he also chairs the environmentalproblems committee of the division of population and environmentalpsychology, american psychological association.appendix a192improving risk communicationcopyright national academy of sciences. all rights reserved.appendix bbibliographyabelson, r., and a.levi. 1985. decision making and decision theory. in handbook ofsocial psychology, 3d ed., g.lindzey and e.aronson, eds. new york: randomhouse.ackerman, b.a., and w.t.hassler. 1977. clean coal, dirty air. new haven, conn. : yaleuniversity press.ahearne, j. 1987. nuclear power after chernobyl. science 236(4802):673œ679.allen, f.w. 1987. towards a holistic appreciation of risk: the challenge forcommunicators and policymakers. science, technology, and human values 12(3&4):138œ143.ames, b.n., l.s.gold, and r.magaw. 1987a. letter. science 237(4821):1399œ 1400.ames, b.n., r.magaw, and l.s.gold. 1987b. ranking possible carcinogenic hazards.science 236(4799):271œ280.appelbaum, p.s., c.w.lidz, and a.meisel. 1987. informed consent: legal theory andclinical practice. new york: oxford university press.aries, p. 1974. western attitudes toward death. baltimore, md.: johns hopkinsuniversity press.arrow, k.j. 1982. risk perception in psychology and economics. economic inquiry 20(1):1œ9.ashford, n.a., c.w.ryan, and c.c.caldart. 1983. a hard look at federal regulation offormaldehyde: a departure from reasoned decisionmaking. harvard environmentallaw review 7:297œ370.atkinson, s.e., t.d.crocker, and r.g.murdock. 1985. have priors in aggregate airpollution epidemiology dictated posteriors? journal of urban economics 17:319œ334.baird, b.n.r. 1986. tolerance for environmental health risks: the influence ofknowledge, benefits, voluntariness, and environmental attitudes. risk analysis 6(4):425œ435.appendix b193improving risk communicationcopyright national academy of sciences. all rights reserved.bandura, a. 1978. the self system in reciprocal determinism. american psychologist(april):344œ358.barkdoll, g.l. 1983. involving constituents in agency priority setting: a case study.evaluation and program planning 6:31œ37.barles, b., and j.kotas. 1987. pesticides and the nation's ground water. epa journal 13(4):42œ43.bartlett, j. 1980. familiar quotations, 15th ed. boston: little, brown.bean, m.c. 1987. tools for environmental professionals involved in risk communicationat hazardous waste facilities undergoing siting, permitting, or remediation. paperpresented at the 80th annual meeting of the association dedicated to air pollutioncontrol and hazardous waste management, new york, june 21œ26, 1987.bean, m.c. 1988. speaking of risk. civil engineering (february):59œ61.bean, m.c., and m.k.null. 1988. a workshop for citizens on risk assessment. epapilot project. reston, va.: ch2m hill.benson, h., l.gordon, c.mitchell, and v.place. 1977. patient education and intrauterinecontraception: a study of two package inserts. american journal of public health 67(5):446œ449.berreth, d. 1987. presentation to national research council committee on riskperception and communication meeting, washington, d.c., november 17, 1987.beythmarom, r. 1982. how probable is probable? journal of forecasting 1:257œ269.breslow, l., s.brown, and j.van ryzin. 1986. letter. science 234(4779):923.brickman, r., s.jasanoff, and t.ilgen. 1985. controlling chemicals: the politics ofregulation in europe and the united states. ithaca, n.y.: cornell university press.broome, t.h. 1986. the slippery ethics of engineering. the washington post, december28:d3a.brown, g. 1987. the outlook for a new pesticides law. epa journal 13(4) :35œ 36.budescu, d.v., and t.s.wallsten. 1987. subjective estimation of precise and vagueuncertainties. in judgmental forecasting, g.wright and p.ayton, eds. new york:john wiley & sons.burnham, d. 1976. energy agency data termed misleading. new york times,september 30:a13.campbell, g., and k.o.ott. 1979. statistical evaluation of major human errors during thedevelopment of new technological systems. nuclear science and engineering71:267œ279.campt, d. 1987. daminozide: a case study of a pesticide controversy. epa journal 13(4):32œ34.carpenter, s.l., and w.j. d.kennedy. 1988. managing public disputes. san francisco,calif.: josseybass.carson, r. 1962. silent spring. boston, mass.: houghton mifflin.chaiken, s. 1980. heuristic versus systematic information processing and the use ofsource versus message cues in persuasion. journal of personality and socialpsychology 39(5):752œ766.chemical and engineering news. 1985. union carbide: new accidents revive safetyissue. (august 19):4.appendix b194improving risk communicationcopyright national academy of sciences. all rights reserved.chemical education for public understanding project. 1986. risk module teachers guide(draft). university of california at berkeley, november 18, 1986.chemical manufacturers association. 1986. caer progress report. washington, d.c.:chemical manufacturers association.cialdini, r.b. 1984. influence: how and why people agree to things. new york:morrow.clark, w.c. 1980. witches, floods, and wonder drugs: historical perspectives on riskmanagement . in societal risk assessment: how safe is safe enough?, r.c.schwingand w.a.albers, jr., eds. new york: plenum press.cohen, b.l. 1987. reducing the hazards of nuclear power: insanity in action. physics andsociety 16(3):2œ4.cohen, j. 1962. the statistical power of abnormalsocial psychological research: areview. journal of abnormal and social psychology 65(3):145œ153.cohen, j.l. 1985. strategy or identity: new theoretical paradigms and contemporarysocial movements. social research 52:663œ716.coppock, r. 1987. risk perception and communication. working paper for the nationalresearch council committee on risk perception and communication meeting,washington, d.c., may 26œ27, 1987.council on environmental quality. 1985. report of an expert meeting on research needsand opportunities at federallysupervised hazardous waste site cleanups, councilon environmental quality, washington, d.c.: october 28œ30, 1985.covello, v.t. 1984. actual and perceived risk: a review of the literature. intechnological risk assessment, p.f.ricci, l.a.sagan, and c.g. whipple, eds. thehague: martinus nijhoff.covello, v.t., and m.abernathy. 1984. risk analysis and technological hazards: apolicyrelated bibliography. in technological risk assessment, p.f.ricci,l.a.sagan, and c.g.whipple, eds. the hague: martinus nijhoff.covello, v.t., and f.allen. 1988. seven cardinal rules of risk communication.washington, d.c.: u.s. environmental protection agency, office of policyanalysis.covello, v.t., d.von winterfeldt, and p.slovic. 1986. risk communication: a review ofthe literature. risk abstracts 3(4):171œ182.covello, v.t., l.b.lave, a.moghissi, and v.r.r.uppuluri, eds. 1987a. uncertainty inrisk assessment, risk management, and decision making. new york: plenumpress.covello, v.t., p.slovic, and d.von winterfeldt. 1987b. risk communication: a reviewof the literature. washington, d.c.: national science foundation.covello, v.t., p.m.sandman, and p.slovic. 1988. risk communication, risk statistics,and risk comparisons: a manual for plant managers. washington, d.c.: chemicalmanufacturers association.crouch, e.a.c., and r.wilson. 1982. risk/benefit analysis. cambridge, mass.:ballinger.cvetkovich, g., c.vlek, and t.c.earle. in press. designing public hazard communicationprograms about largescale technologies. in social decision methodologies fortechnological projects, c.vlek and g.cvetkovich, eds. amsterdam: northholland.appendix b195improving risk communicationcopyright national academy of sciences. all rights reserved.davies, j.c., v.t.covello, and f.w.allen, eds. 1987. risk communication. washington,d.c.: the conservation foundation.davis, d. 1987. presentation to national research council committee on risk perceptionand communication meeting, washington, d.c., november 17, 1987.davis, d.l., a.d.lilienfeld, a.gittelsohn, and m.e.scheckenbach. 1986. increasingtrends in some cancers in older americans: fact or artifact? toxicology andindustrial health 2(1):127œ144.deisler, p.f., jr. 1988. the risk managementrisk assessment interface. environment,science, and technology 22(1):15œ19.department of health and human services. 1986. determining risks to health: federalpolicy and practice. dover, mass.: auburn house publishing.derbaix, c. 1983. perceived risk and risk relieversšan empirical investigation. journal ofeconomic psychology 3:19œ38.dezern, j.n. 1988. risk assessment and astm: reasons for and against astm'sinvolvement. astm standardization news 88(february):52œ55.diamond, s. 1985. carbide blames a faulty design for toxic leak; effects can be serious,company memo says. new york times, august 13:a1, b8.dickson, d. 1984. the new politics of science. new york: pantheon.dickson, r.b. 1987. risk assessment and the law: evolving criteria by whichcarcinogenicity risk assessments are evaluated in the legal community. in uncertaintyin risk assessment, risk management, and decision making, v.t.covello,l.b.lave, a.moghissi, and v.r.r.uppuluri, eds. new york: plenum press.dietz, t.m., and r.w.rycroft. 1987. the risk professionals. new york: russell sagefoundation.dietz, t., p.c.stern, and r.w.rycroft. 1989. definitions of conflict and the legitimationof resources: the case of environmental risk. sociological forum 4(1):47œ70.dirkin, g.r. 1983. cognitive tunneling: use of visual information under stress. perceptualand motor skills 56:191œ198.douglas, m. 1985. risk acceptability according to the social sciences. social researchperspectives. new york: russell sage foundation.douglas, m., and a.wildavsky. 1982. risk and culture. berkeley: university ofcalifornia press.dreman, d. 1979. contrarian investment strategy. new york: random house.dunlap, r.e. 1987. public opinion and the environment in the reagan era. environment29(july/august):6œ11, 32œ37.dunlap, r.e., j.k.grieneeks, and m.rokeack. 1983. human values and proenvironmental behavior. in energy and material resources: attitudes, values, andpublic policy, w.d.conn, ed. aaas selected symposium 75. boulder, colo.:westview.dunwoody, s., m.friestad, and m.a.shapiro. 1987. conveying risk information in themass media. paper presented to the mass communication division of theinternational communication association, may 1987.eagly, a.h., and s.chaiken. 1985. psychological theories of persuasion. in advances inexperimental social psychology, l.berkowitz, ed. new york: academic press.economist, the. 1987. making company disasters less disastrous (january 31):55œ56.appendix b196improving risk communicationcopyright national academy of sciences. all rights reserved.edwards, w., and d.von winterfeldt. 1986. public disputes about risky technologies:stakeholders and arenas. in risk evaluation and management, v.t.covello,j.menkes, and j.mumpower, eds. new york: plenum press.elliott, m.l.p. 1987. the effect of differing assessments of risk in hazardous waste facilitysiting negotiations. paper presented at the workshop on negotiating hazardouswaste facility siting and permitting agreements, the conservation foundation,washington, d.c., june 15, 1987.environmental protection agency. 1987a. answering questions about pesticides: aninterview with john a.moore. epa journal 13(4):4œ8.environmental protection agency. 1987b. a consumer's guide to safer pesticide use. epajournal 13(4):9œ31.environmental protection agency, science advisory board. 1988. letter of march 9.faden, r. 1987. ethical issues in government sponsored public health campaigns. healtheducation quarterly 14(1):27œ37.faden, r.r., and t.l.beauchamp. 1986. a history and theory of informed consent. newyork: oxford university press.ferguson, e.s. 1987. risk and the american engineering profession: the asme boilercode and american industrial safety standards. in the social and culturalconstruction of risk, b.b.johnson and v.t.covello, eds. dordrecht, holland:d.reidel.file, s.e., and a.jew. 1973. syntax and the recall of instructions in a realistic situation.british journal of psychology 64:65œ70.fischhoff, b. 1982. debiasing. in judgment under uncertainty: heuristics and biases,d.kahneman, p.slovic, and a.tversky, eds. new york: cambridge university press.fischhoff, b. 1983. ﬁacceptable riskﬂ: the case of nuclear power. journal of policyanalysis and management 2(4):559œ575.fischhoff, b. 1984. setting standards: a systematic approach to managing public healthand safety risks. management science 30(7):823œ843.fischhoff, b. 1985a. managing risk perceptions. issues in science and technology 2(1):83œ96.fischhoff, b. 1985b. protocols for environmental reporting: what to ask the experts. thejournalist (winter):11œ15.fischhoff, b. 1985c. risk analysis demystified. ncap news (winter):30œ33.fischhoff, b. 1987. treating the public with risk communications: a public healthperspective. science, technology, and human values 12(3&4):13œ 19.fischhoff, b. 1988. judgment and decision making. in the psychology of humanthought, r.j.sternberg and e.e.smith, eds. new york: cambridge university press.fischhoff, b., and l.a.cox, jr. 1985. conceptual framework for regulatory benefitsassessment. in benefits assessment: the state of the art, j.d. bentkover,v.t.covello, and j.mumpower, eds. dordrecht, holland: d. reidel.fischhoff, b., and d.macgregor. 1983. judged lethality: how much people seem to knowdepends upon how they are asked. risk analysis 3:229œ236.fischhoff, b., and o.svenson. 1987. perceived risks of radionuclides: understandingpublic understanding. in radionuclides in the food chain, g. schmidt, ed. newyork: praeger.appendix b197improving risk communicationcopyright national academy of sciences. all rights reserved.fischhoff, b., p.slovic, and s.lichtenstein. 1977. knowing with certainty: theappropriateness of extreme confidence. journal of experimental psychology: humanperception and performance 20:159œ183.fischhoff, b., p.slovic, s.lichtenstein, s.read, and b.combs. 1978. how safe is safeenough? a psychometric study of attitudes towards technological risks and benefits.policy sciences 9:127œ152.fischhoff, b., p.slovic, and s.lichtenstein. 1980. knowing what you want: measuringlabile values. in cognitive processes in choice and decision behavior, t.wallsten,ed. hillsdale, n.j.: erlbaum.fischhoff, b., s.lichtenstein, p.slovic, s.l.derby, and r.l.keeney. 1981a. acceptablerisk. new york: cambridge university press.fischhoff, b., p.slovic, and s.lichtenstein. 1981b. lay foibles and expert fables injudgments about risk. in progress in resource management and environmentalplanning, t.o'riordan and r.k.turner, eds. new york: john wiley & sons.fischhoff, b., p.slovic, and s.lichtenstein. 1983. ﬁthe publicﬂ vs. ﬁthe expertsﬂ:perceived vs. actual disagreements about risks of nuclear power. in analysis ofactual vs. perceived risks, v.covello, g.flamm, j. rodericks, and r.tardiff, eds.new york: plenum press.fischhoff, b., s.r.watson, and c.hope. 1984. defining risk. policy sciences 17:123œ129.fischhoff, b., o.svenson, and p.slovic. 1986. active responses to environmental hazards:perceptions and decision making. in handbook of environmental psychology,d.stokols and i.altman, eds. new york: john wiley & sons.fischhoff, b., l.furby, and r.gregory. 1987. evaluating voluntary risks of injury.accident analysis and prevention 19(1):51œ62.fishburn, p.c. 1982. foundations of risk measurement: ii. effects of gains on risk. journalof mathematical psychology 25:226œ242.fisher, a. 1987. radon projects at epa. presented to national research councilcommittee on risk perception and communication meeting, washington, d.c., july23, 1987.fiske, s., and s.taylor. 1984. social cognition. reading, mass.: addisonwesley.fitchen, j.m., j.s.heath, and j.fessendenraden. 1987. risk perception in communitycontext: a case study. in the social and cultural construction of risk, b.b.johnsonand v.t.covello, eds. dordrecht, holland; d. reidel.folkman, s. 1984. personal control and stress and coping processes: a theoreticalanalysis. journal of personality and social psychology 46(4):839œ852.freudenburg, w., and e.rosa, eds. 1984. public reaction to nuclear power: are therecritical masses? boulder, colo.: westview.friedman, b., d.lockwood, l.snowden, and d.zeidler. 1986. mass media and disaster:annotated bibliography. miscellaneous report no. 36. newark: university ofdelaware, disaster research center.friedman, s., c.m.gorney, and b.p.egolf. 1987. reporting on radiation: a contentanalysis of chernobyl coverage. journal of communication 37(3):58œ79.gale, r.p. 1987. calculating risk: radiation and chernobyl. journal of communication37(3):68œ79.appendix b198improving risk communicationcopyright national academy of sciences. all rights reserved.geller, e.s. 1983. development of industrybased strategies for motivating seat beltusage. final report for contract no. dtrs5681c0032. washington, d.c.: u.s.department of transportation.gibbs, l. 1982. love canal: my story. new york: grove press.gough, m., r.hart, b.w.karrh, a.koestner, r.neal, d.parkinson, f.perera, k.e.powell,and h.s.rosenkranz. 1984. report on the consensus workshop on formaldehyde .environmental health perspectives 58:323œ381.gould, l.c., g.t.gardner, d.r.deluca, a.r.tiemann, l.w.doob, and j.a.j.stolwijk.1988. perceptions of technological risks and benefits. new york: russell sagefoundation.gray, j. 1981. three case studies of organized responses to chemical disasters.miscellaneous report no. 29. newark: university of delaware, disaster researchcenter.greenberg, d.s., and m.taylor (illustrations). 1984. what is an acceptable risk? nationalwildlife (august/september) :29œ32.greenwald, a.g., and c.leavitt. 1984. audience involvement in advertising: four levels.journal of consumer research 11:581œ592.greenwood, t. 1984. knowledge and discretion in government regulation. new york:praeger.hadden, s.g., ed. 1984. risk analysis, institutions, and public policy. port washington,n.y.: associated faculty press.hadden, s.g. 1986. read the label: providing information to reduce health and safetyrisks. boulder, colo.: westview press for the american association for theadvancement of science.hance, b.j., c.chess, and p.m.sandman. 1988. improving dialogue with communities: arisk communication manual for government. trenton: division of science andresearch risk communication unit, new jersey department of environmentalprotection.harness, r.l. 1987. managing pesticides: an industry view. epa journal 13(4):40œ41.hasher, l., and r.t.zachs. 1984. automatic and effortful processes in memory. journal ofexperimental psychology: general 108:356œ388.hays, s.p. 1987. beauty, health, and permanence: environmental politics in the unitedstates, 1955œ1985. new york: cambridge university press.henrion, m., and b.fischhoff. 1986. assessing uncertainty in physical constants.american journal of physics 54(9):791œ798.hershey, j.c., and p.j.h.shoemaker. 1980. risk taking and problem context in the domainof losses: an expected utility analysis. journal of risk and insurance 47:111œ132.hively, w. 1988. nuclear power at risk. american scientist 76(julyaugust): 341œ343.hogarth, r.m. 1981. beyond discrete biases: functional and dysfunctional aspects ofjudgmental heuristics. psychological bulletin 90(2):197œ217.hohenemser, c., r.w.kates, and p.slovic. 1983. the nature of technological hazard.science 220(4595):378œ384.holland, n. 1987. presentation to national research council committee on riskperception and communication meeting, washington, d.c., november 17, 1987.appendix b199improving risk communicationcopyright national academy of sciences. all rights reserved.hovland, c.i., i.l.janis, and h.h.kelley. 1953. communication and persuasion:psychological studies of opinion change. new haven, conn.: yale universitypress.humber, j.m., and r.f.almeder, eds. 1987. quantitative risk assessment: biomedicalethics reviews1986. clifton, n.j.: humana press.hutt, p.b. 1974. a regulator's viewpoint. in how safe is safe? the design of policy ondrugs and food additives. washington, d.c.: national academy press.hutt, p.b. 1982. food and drug law: a strong and continuing tradition. food drugcosmetic law journal 37:123œ137.hutt, p.b., and p.b.hutt ii. 1984. a history of government regulation of adulteration andmisbranding of food. food drug cosmetic law journal 39:2œ73.ikeda, s. 1986. managing technological and environmental risks in japan. risk analysis6(4):389œ401.inglehart, r. 1977. values, objective needs, and subjective satisfaction among westernpublics. comparative political studies 4:428œ458.inhaber, h. 1981. the risk of producing energy. proceedings of the royal society oflondon a376:121œ131.inside epa. 1982. congressmen will press epa on national dioxin policy. (november19):1, 4.institute for environmental negotiation. 1984. not in my backyard! community reactionto locally unwanted land use. charlottesville: university of virginia.isaacs, t. 1987. presentation to national research council committee on risk perceptionand communication meeting, washington, d.c., july 23, 1987.jaeger, j. 1988. developing policies for responding to climatic change. world climateprogramme impact studies (wmo/tdno. 225). world meteorological associationand united nations environmental programme. april.jasanoff, s. 1986. risk management and political culture. new york: russell sagefoundation.jasanoff, s. 1987. epa's regulation of daminozide: unscrambling the messages of risk.science, technology, and human values 12(3&4):116œ124.jenkins, c. 1983. resource mobilization theory and the study of social movements.annual review of sociology 9:527œ553.jerome, f. 1986. check it out: journalists communicating about risk. technology insociety 8:287œ290.johnson, b.b., and v.t.covello, eds. 1987. the social and cultural construction of risk:essays on risk selection and perception. dordrecht, holland: d.reidel.kahneman, d., and a.tversky. 1972. subjective probability: a judgment ofrepresentativeness. cognitive psychology 3:430œ454.kahneman, d., and a.tversky. 1979. prospect theory: an analysis of decision under risk.econometrica 47(2):263œ291.kahneman, d., p.slovic, and a.tversky, eds. 1982. judgments under uncertainty:heuristics and biases. new york: cambridge university press.kasperson, r. 1986. six propositions on public participation and their relevance for riskcommunication. risk analysis 6(3):275œ281.appendix b200improving risk communicationcopyright national academy of sciences. all rights reserved.kaufman, d.g. 1988. assessment of carcinogenicity: generic issues and their applicationto diesel exhaust. in air pollution, the automobile, and public health, a.y.watson,r.r.bates, and d.kennedy, eds. washington, d.c.: national academy press.keeney, r., and d.von winterfeldt. 1986. improving risk communication. risk analysis6(4):417œ424.kerr, r.a. 1988. indoor radon: the deadliest pollutant. science 240(4852):606œ 608.kong, a., g.o.barnett, f.mosteller, and c.youtz. 1986. how medical professionalsevaluate expressions of probability. new england journal of medicine 315(12):740œ744.koshland, d.e. 1987. immortality and risk assessment. science 236(4799):241.krimsky, s., and a.plough. 1988. environmental hazards: communicating risks as asocial process. dover, mass.: auburn house.lave, l.b. 1987. health and safety risk analyses: information for better decisions. science236(4799):291œ295.lawless, e.w. 1977. technology and social shock. new brunswick, n.j.: rutgersuniversity press.levine, a.g. 1982. love canal: science, politics, and people. lexington, mass.:lexington books.lewis, h.w. 1980. the safety of fission reactors. scientific american 342(3):33œ 45.lichtenstein, s., and b.fischhoff. 1980. training for calibration. organizational behaviorand human performance 26:149œ171.lichtenstein, s., p.slovic, b.fischhoff, m.layman, and b.combs. 1978. judged frequencyof lethal events. journal of experimental psychology: human learning and memory4:551œ578.lichtenstein, s., b.fischhoff, and l.d.phillips. 1982. calibration of probabilities: thestate of the art. in judgments under uncertainty: heuristics and biases,d.kahneman, p.slovic, and a.tversky, eds. new york: cambridge university press.lind, n., ed. 1988. risk communication: a symposium. waterloo: university ofwaterloo.lipset, s.m., and w.schneider. 1987. the confidence gap: business, labor, andgovernment in the public mind (revised edition). baltimore, md.: johns hopkinsuniversity press.lowrance, w. 1976. of acceptable risk. san francisco: freeman.lynn, f.m. 1986. the interplay of science and values in assessing and regulatingenvironmental risks. science, technology, and human values 11(2):40œ50.lynn, f.m. 1987. citizen involvement in hazardous waste sites: two north carolinasuccess stories. environmental impact assessment review 7:347œ 361.maccoby, n., and d.s.solomon. 1981. heart disease prevention: community studies. inpublic communication campaigns, r.e.rice and w.j. paisley, eds. beverly hills,calif.: sage.manning, w.w. 1986. concern justified about frostban. salinas californian. january 22,1986.marshall, e. 1983a. house reviews epa's record on pesticides. science 219(4589):1200.marshall, e. 1983b. hit list at epa? science 219(4590):1303.appendix b201improving risk communicationcopyright national academy of sciences. all rights reserved.marshall, e. 1983c. epa's troubles reach a crescendo. science 219(4591):1402œ 1404.mazis, m.b., r.staelin, h.beales, and s.salop. 1981. a framework for evaluatingconsumer information regulation. journal of marketing 45:11œ 21.mazur, a. 1981. the dynamics of technical controversy. washington, d.c.:communications press.mazur, a. 1987. putting radon on the public's risk agenda. science, technology, andhuman values 12(3&4):86œ93.mazur, a. 1988. mass media effects on public opinion about nuclear power plants.unpublished manuscript. syracuse university, syracuse, new york.mcalister, a. 1981. antismoking campaigns: progress in developing effectivecommunications. in public communication campaigns, r.e.rice and w.j.paisley,eds. beverly hills, calif.: sage.mcarthur, l.z., d.q.crocker, and e.folino. 1981. individual differences in cue utilizationon spatial tasks. perceptual and motor skills 52:923œ929.mccormick, n.j. 1981. reliability and risk analysis. new york: academic press.mcguire, w.j. 1985. attitudes and attitude change. in handbook of social psychology, 3ded., vol. 2., g.lindzey and e.aronson, eds. new york: random house.mckean, k. 1985. decisions, decisions. discover (june):22œ31.melnick, r.s. 1983. regulation and the courts: the case of the clean air act.washington, d.c.: brookings institution.melnick, r.s. 1988. the politics of costbenefit analysis. paper prepared for nationalacademy of sciences conference on valuing health risks, costs, and benefits inenvironmental decisions, washington, d.c., june 23œ24, 1987.metropolitan life insurance company. 1987. new high expectation of life. statisticalbulletin 68(3):8œ14.milbrath, l. 1984. environmentalists: vanguard for a new society. albany: stateuniversity of new york press.mitchell, r.c. 1980. public opinion on environmental issues. in environmental quality:the eleventh annual report of the council on environmental quality. washington,d.c.: u.s. government printing office.morrison, d. 1987. a tale of two toxicities. presentation to the annual meeting of theamerican sociological association, chicago, ill., august 1987.moscovici, s. 1985. social influence and conformity. in handbook of social psychology,3d ed., vol. 2., g.lindzey and e.aronson, eds. new york: random house.mott, l. 1987. managing pesticides: an environmentalist view. epa journal 13(4):37œ39.murphy, a.h., and b.g.brown. 1983. forecast terminology: composition andinterpretation of public weather forecasts. bulletin of the american meteorologicalsociety 64:13œ22.murphy, a.h., and r.l.winkler. 1984. probability of precipitation forecasts. journal ofthe american statistical association 79:391œ400.appendix b202improving risk communicationcopyright national academy of sciences. all rights reserved.murphy, a.h., s.lichtenstein, b.fischhoff, and r.l.winkler. 1980. misinterpretations ofprecipitation probability forecasts. bulletin of the american meteorological society61:695œ701.mydans, s. 1987. specter of chernobyl looms over bangladesh. the new york times,june 5:i9.naber, t. 1988. nanograms are not the answer to ‚will i be hurt?' questions. waste age(march) :44œ48.national research council. 1979. disasters in the mass media: proceedings of thecommittee on disasters and the mass media workshop. washington, d.c.: nationalacademy press.national research council. 1980. the effect on populations of exposure to low levelsof ionizing radiation. washington, d.c.: national academy press.national research council. 1982. risk and decisionmaking: perspectives and research.washington, d.c.: national academy press.national research council. 1983a. risk assessment in the federal government:managing the process. washington, d.c.: national academy press.national research council. 1983b. risk assessment in the federal government:managing the process. working papers for the committee on the institutional meansfor assessment of risks to public health. washington, d.c.: national academypress.national research council. 1984. toxicity testing: strategies to determine needs andpriorities. washington, d.c.: national academy press.national research council. 1986a. confronting aids: directions for public health,health care, and research. washington, d.c.: national academy press.national research council. 1986b. drinking water and health, vol. 6. washington, d.c.:national academy press.national research council. 1988a. complex mixtures: methods for in vivo toxicitytesting. washington, d.c.: national academy press.national research council, committee on the biological effects of ionizing radiations(beir iv). 1988b. appendix ii: cellular radiobiology. in health risks of radon andother internally deposited alphaemitters, beir iv. washington, d.c.: nationalacademy press.nelkin, d. 1979a. science, technology, and political conflict: analyzing the issues. incontroversy: politics of technical decisions, d.nelkin, ed. beverly hills, calif.:sage.nelkin, d., ed. 1979b. controversy: politics of technical decisions. beverly hills, calif.:sage.nelkin, d. 1987. selling science: how the press covers science and technology. newyork: w.h.freeman.nelkin, d., and m.s.brown. 1984. worker at risk: voices from the workplace. chicago,ill.: university of chicago.neutra, r.r. 1985. epidemiology for and with a distrustful community. environmentalhealth perspectives 62:393œ397.nisbett, r.e., and l.ross. 1980. human inference: strategies and shortcomings of socialjudgment. englewood cliffs, n.j.: prenticehall.nuclear regulatory commission. 1985. nrc manual chapter nrc4125 (september1980, amended july 1985). washington, d.c.: nuclear regulatory commission.appendix b203improving risk communicationcopyright national academy of sciences. all rights reserved.nuclear regulatory commission. 1987. differing professional opinions: 1987 specialreview panel. nureg1290. washington, d.c.: nuclear regulatory commission.o'brien, d.m., and d.a.marchand. 1982. the politics of technology assessment.lexington, mass.: lexington books.office, of communication of the united church of christ v. federal communications commission, u.s. court of appeals district of columbia circuit, 359 f.2d 994(1966).office of technology assessment. 1981. assessment of technologies of determiningcancer risks from the environment. washington, d.c.: u.s. government printingoffice.okrent, d. 1980. an approach to quantitative safety goals for nuclear power plants.nureg0739. washington, d.c.: nuclear regulatory commission.okrent, d. 1981. industrial risks. proceedings of the royal society of london a376:133œ149.okrent, d. 1987. the safety goals of the u.s. nuclear regulatory commission. science236(4799):296œ300.o'leary, m.k., w.d.coplin, h.b.shapiro, and d.dean. 1974. the quest for relevance.international studies quarterly 18:211œ237.otway, h. 1987. experts, risk communication, and democracy. risk analysis 7(2):125œ129.otway, h.j., and d.von winterfeldt. 1982. beyond acceptable risk: on the socialacceptability of technologies. policy sciences 14:247œ256.page, t. 1981. a framework for unreasonable risk in the toxic substances control act. incarcinogenic risk assessment, r.nicholson, ed. new york: new york academy ofsciences.paté, m.e. 1983. acceptable decision processes and acceptable risks in public sectorregulations. ieee transactions on systems, man, and cybernetics smc13(2):113œ124.patterson, j.t. 1987. the dread disease. cambridge, mass.: harvard university press.peterson, c.r., and l.r.beach. 1969. man as an intuitive statistician. psychologicalbulletin 69:29œ46.petty, r.e., j.t.cacioppo, c.sedikides, and a.j.strathman. 1988. affect and persuasion.american behavioral scientist 31(3):355œ371.plough, a., and s.krimsky. 1987. the emergence of risk communication studies: socialand political context. science, technology, and human values 12(3&4):4œ10.pochin, e.e. 1980. the need to estimate risks. physics in medicine and biology 25(1):1œ12.pochin, e.e. 1981. quantification of risk in medical procedures. proceedings of the royalsociety of london a376:87œ101.pochin, e.e. 1982. risk and medical ethics. journal of medical ethics 8:180œ184.pollatsek, a., and a.tversky. 1970. a theory of risk. journal of mathematical psychology7:540œ553.poulton, e.c. 1968. the new psychophysics: six models for magnitude estimation.psychological bulletin 69:1œ19.poulton, e.c. 1982. biases in quantitative judgments. applied ergonomics 13:31œ42.appendix b204improving risk communicationcopyright national academy of sciences. all rights reserved.quarantelli, e.l., d.c.hutchinson, and b.d.phillips. 1983. evacuation behavior: casestudy of the taft, louisiana chemical tank explosion incident. miscellaneousreport no. 34. newark: university of delaware, disaster research center.raiffa, h. 1968. decision analysis. reading, mass.: addisonwesley. rayner, s. 1984.disagreeing about risk: the institutional cultures of risk management and planningfor future generations. in risk analysis, institutions, and public policy, s.g.hadden,ed. port washington, n.y.: associated faculty press.rayner, s. 1986. management of radiation hazards in hospitals: plural rationalities in asingle institution. social studies of science 16:573œ591.rayner, s. 1987a. learning from the blind men and the elephant, or seeing things whole inrisk management. in uncertainty in risk assessment, risk management, anddecision making, v.t.covello, l.b.lave, a. moghissi, and v.r.r.uppuluri, eds.new york: plenum press.rayner, s. 1987b. risk and relativism in science for policy. in the social and culturalconstruction of risk, b.b.johnson and v.t.covello, eds. dordrecht, holland:d.reidel.rayner, s., and r.cantor. 1987. how fair is safe enough?: the cultural approach tosocietal technology choice. risk analysis 7(1):3œ9.regens, j.l., and j.a.donnan. 1986. uncertainty and information integration in acidicdeposition policymaking. the environmental professional 8(4):342œ350.reich, r.b. 1985. public administration and public deliberation: an interpretive essay.the yale law journal 94:1617œ1641.reilly, w.k. 1987. foreword. in risk communication, j.c.davies, v.t. covello, andf.w.allen, eds. washington, d.c.: the conservation foundation.report of the public's right to information task force. 1979. washington, d.c.: u.s.government printing office.reporting from the russell sage foundation. 1987. living with risk. report no. 10, may1987:1, 8, 11.ricci, p.f., l.a.sagan, and c.g.whipple, eds. 1984. technological risk assessment. thehague: martinus nijhoff.rice, r.e., and w.j.paisley. 1985. public communication campaigns. beverly hills,calif.: sage.roberts, d.f., and n.maccoby. 1985. effects of mass communication. in handbook ofsocial psychology, 3d ed., vol. 2, g.lindzey and e.aronson, eds. new york:random house.rodricks, j.v., s.m.brett, and g.c.wrenn. undated. significant risk decisions in federalregulatory agencies. washington, d.c.: environ corporation.roe, e.m. 1988. a case study of the 1980/82 medfly controversy in california. workingpaper prepared for the national research council's committee on risk perceptionand communication.rosenblatt, r.a. 1976. gao calls u.s. apower booklet ‚propaganda.' los angelestimes, september 30, 1976, i30.ross, l., and c.a.anderson. 1982. shortcomings in the attribution process: on the originsand maintenance of erroneous social assessments. in judgment under uncertainty:heuristics and biases, d.kahneman, p.slovic, and a.tversky, eds. new york:cambridge university press.appendix b205improving risk communicationcopyright national academy of sciences. all rights reserved.ruckelshaus, w.d. 1983. science, risk and public policy. science 221:1026œ 1028.ruckelshaus, w.d. 1984. risk in a free society. risk analysis 4(3):157œ162.ruckelshaus, w.d. 1985. risk, science, and democracy. issues in science and technology1(3):19œ38.ruckelshaus, w.d. 1987. communicating about risk. in risk communication,j.c.davies, v.t.covello, and f.w.allen, eds. washington, d.c.: the conservationfoundation.rushefsky, m.e. 1984. the misuse of science in governmental decisionmaking. science,technology, and human values 9(3):47œ59.russell, m., and m.gruber. 1987. risk assessment in environmental policymaking.science 236(4799):286œ290.sagan, l.a. 1987. beyond risk assessment. risk analysis 7(1):1œ2.sandman, p.m. 1986. explaining environmental risk: some notes on environmental riskcommunication. washington, d.c.: u.s. environmental protection agency.sandman, p.m., d.b.sachsman, and m.r.greenberg. 1987a. the environmental newssource: informing the media during an environmental crisis. new brunswick, n.j.:rutgers university.sandman, p.m., n.d.weinstein, and m.l.klotz. 1987b. public response to the risk fromgeological radon . journal of communication 37(3):93œ108.sapolsky, h.m. 1968. science, voters, and the fluoridation controversy. science 162(october 25):427œ433.sapolsky, h.m., ed. 1986. consuming fears: the politics of product risks. new york:basic books.sapolsky, h.m. 1987. is honesty the best policy? in aids public policy dimensions,j.griggs, ed. new york: united hospital fund of new york.scenic hudson preservation conference v. federal power commission, 354 f. 2d 608 (2dcir. 1965).schmandt, j. 1984. regulation and science. science, technology, and human values 9(1):23œ38.schneiderman, m.a. 1983. cancer: scientific policy, public policy, and the prevention ofdisease. carolina environmental essay series. chapel hill, n.c.: institute forenvironmental studies.schneiderman, m.a. 1987a. risk assessmentšwhere do we want it to go? in quantitativerisk assessment: biomedical ethics reviews, j.m.humber and r.f.almeder, eds.clifton, n.j.: humana press.schneiderman, m.a. 1987b. expectation and limitation of human studies and riskassessment. in health effects from hazardous waste sites, j.b. andelman andd.w.underhill, eds. chelsea, mich.: lewis publishers.schudson, m. 1978. discovering the news. new york: basic books. schultz, w.,g.mcclelland, b.hurd, and j.smith. 1986. improving accuracy and reducing costsof environmental benefits assessment. vol. iv. boulder: university of colorado,center for economic analysis.schuman, h., and j.scott. 1987. problems in the use of survey questions to measure publicopinion. science 236(4804):957œ959.schwartz, s.h. 1977. normative influences on altruism. in advances in experimentalsocial psychology, vol. 10, l.berkowitz, ed. new york: academic press.appendix b206improving risk communicationcopyright national academy of sciences. all rights reserved.sharlin, h.i. 1987. macrorisks, microrisks, and the media: the edb case. in the socialand cultural construction of risk, b.b.johnson and v.t. covello, eds. dordrecht,holland: d.reidel.sherry, s. 1985. high tech and toxics: a guide for local communities. washington,d.c.: national center for policy alternatives.sheth, j.n., and g.l.frazier. 1982. a model of strategy mix choice or planned socialchange. journal of marketing 46:15œ26.shilts, r. 1987. and the band played on: politics, people, and the aids epidemic. newyork: st. martin's press.short, j.f., jr. 1984. the social fabric at risk: toward the social transformation of riskanalysis. american sociological review 49:711.siegel, b. 1987. ﬁmanagingﬂ risks: sense and science. los angeles times, july 5:i1.silbergeld, e. 1987a. responsibilities of risk communicators. in risk communication,j.c.davies, v.t.covello, and f.w.allen, eds. washington, d.c.: the conservationfoundation.silbergeld, e. 1987b. letter. science 237(4821):1399.simmons, j. 1984. rights and wrongs in hazardous waste disposal. in not in mybackyard! community reaction to locally unwanted land use. charlottesville:university of virginia.sindell v. abbott laboratories, 26 cal. 3d 588 (1980).singer, s., and p.endreny. 1987. reporting hazards: their benefits and costs. journal ofcommunication 37(3):10œ26.slovic, p. 1986. informing and educating the public about risk. risk analysis 6(4):403œ415.slovic, p. 1987. perception of risk. science 236(4799):280œ285.slovic, p., b.fischhoff, and s.lichtenstein. 1979. rating the risks. environment 21(3):14œ20, 36œ39.slovic, p., b.fischhoff, and s.lichtenstein. 1980. perceived risk. in societal riskassessment: how safe is safe enough?, r.schwing and w.a.albers, eds. newyork: plenum press.slovic, p., s.lichtenstein, and b.fischhoff. 1988. decision making. in stevens' handbookof experimental psychology. new york: john wiley & sons.smith, h.l. 1974. myocardial infarctionšcase studies of ethics in the consent situation.social science and medicine 8:399œ404.smith, l. 1987. stop i.t.: citizens' response to hazardous waste treatment facility.presented to national research council committee on risk perception andcommunication meeting, washington, d.c., july 23, 1987.smith, r.j. 1983. covering the epa: or, wake me up if anything happens. columbiajournalism review 22(septemberoctober):29œ34.smith, r.j. 1986. chernobyl report surprisingly detailed but avoids painful truths, expertssay. washington post, august 27:a25.smith, v.k., w.h.desvousges, a.fisher, and f.r.johnson. 1987. communicating radonrisk effectively: a midcourse evaluation. epa cooperative agreement no.cr811075. washington, d.c.: u.s. environmental protection agency, office ofpolicy analysis.sood, r., g.stockdale, and e.m.rogers. 1987. how the news media operate in naturaldisasters. journal of communications 37(3):27œ41.appendix b207improving risk communicationcopyright national academy of sciences. all rights reserved.sosenko, a. 1983. after burford, what? the time has come for a new epa beginning .inside epa 4(11):3.spain, d. 1984. women's role in opposing locally unwanted land uses. in not in mybackyard! community reaction to locally unwanted land use. charlottesville:university of virginia.stallen, p.j., and r.coppock. 1987. about risk communication and risky communication.risk analysis 7(4):413œ414.starr, c., and c.whipple. 1980. risks of risk decisions. science 208(4448):1114œ 1119.starr, p. 1982. the social transformation of american medicine. new york: basicbooks.stephens, m., and n.g.edison. 1982. news media coverage of issues during the accidentat three mile island. journalism quarterly 59:199œ204.stern, p.c., and e.aronson. 1984. energy use: the human dimension. new york:w.h.freeman.stern, p.c., t.dietz, and j.s.black. 1986. support for environmental protection: the roleof moral norms. population and environment 8:204œ 222.suplee, c. 1987. semiotics: in search of more perfect persuasion. the washington post,january 18:c3.svenson, o., and b.fischhoff. 1985. levels of environmental decisions. journal ofenvironmental psychology 5:55œ67.tarr, j.a., and c.jacobson. 1987. environmental risk in historical perspective. in thesocial and cultural construction of risk, b.b.johnson and v.t. covello, eds.dordrecht, holland: d.reidel.thaler, r. 1985. mental accounting and consumer choice. marketing science 4(3):199œ214.thomas, l. 1987. making and communicating pesticide decisions. epa journal 13(4):3.tichenor, p., g.donohue, and c.olien. 1980. community conflict and the press. beverlyhills, calif.: sage.tiemann, a.r. 1987. comment: risk, technology, and society. risk analysistierney, j. 1988. not to worry. hippocrates (january/february):29œ38.touraine, a., et al. 1983. antinuclear protest: the opposition to nuclear energy infrance. new york: cambridge university press.towle, m. 1986. carbide neighbors view leak as catalyst for change. charleston dailymail, august 8:1a.travis, c.c., and r.k.white. 1988. interspecific scaling of toxicity data. risk analysis 8(1):119œ125.tversky, a., and d.kahneman. 1971. the belief in the ﬁlaw of small numbers.ﬂpsychological bulletin 76:105œ110.tversky, a., and d.kahneman. 1973. availability: a heuristic for judging frequency andprobability. cognitive psychology 5:207œ232.tversky, a., and d.kahneman. 1981. the framing of decisions and the rationality ofchoice. science 211(4481):453œ458.urquhart, j., and k.heilmann. 1984. riskwatch: the odds of life. new york: facts onfile.u.s. bureau of the census. 1980. social indicators iii. washington, d.c.: u.s. bureau ofthe census.appendix b208improving risk communicationcopyright national academy of sciences. all rights reserved.u.s. committee on government operations. 1978. teton dam disaster. washington,d.c.: u.s. government printing office.u.s. congress, senate. 1987. s.638, a bill to require the secretary of health and humanservices to develop standards governing the notification of individuals who have beenexposed to hazardous substances or physical agents, and for other purposes. 100thcong., 1st sess.verba, s., and n.h.nie. 1972. participation in america: political democracy and socialequality. new york: harper & row.viscusi, w.k. 1985. a bayesian perspective on biases on risk perception. economicsletters 17:59œ62.viscusi, w.k., and c.j.o'connor. 1984. adaptive responses to chemical labeling: areworkers bayesian decision makers? the american economic review 74(5):942œ956.viscusi, w.k., w.a.magat, and j.huber. 1986. informational regulation of consumerhealth risks: an empirical evaluation of hazard warnings. rand journal of economics17(3):351œ365.viscusi, w.k., w.a.magat, and j.huber. 1987. an investigation of the rationality ofconsumer valuations of multiple health risks. rand journal of economics 18(4):465œ479.von winterfeldt, d., and w.edwards. 1984. patterns of conflict about risky technologies.risk analysis 4(1):55œ68.von winterfeldt, d., and w.edwards. 1986. decision analysis and behavioral research.new york: cambridge university press.von winterfeldt, d., r.s.john, and k.borcherding. 1981. cognitive components of riskratings. risk analysis 1(4):277œ287.wallsten, t., and d.budescu. 1983. encoding subjective probabilities: a psychologicaland psychometric review. management science 29:135œ140.weinstein, n.d., p.m.sandman, and m.l.klotz. 1987. public response to the risk fromradon, 1986. research contract c29543. trenton: division of environmentalquality, new jersey department of environmental protection.whittemore, a.s. 1983. facts and values in risk analysis for environmental toxicants. riskanalysis 3(1):23œ33.wildavsky, a. 1988. searching for safety. new brunswick, n.j.: transaction books.wilkie, w.l., and d.m.gardner. 1974. the role of marketing research in public policydecision making. journal of marketing 38:38œ47.wilkins, l., and p.patterson. 1987. risk analysis and the construction of news. journal ofcommunication 37(3):80œ92.wilkinson, c.f.undated. communicating with the public and the media. washington,d.c.: institute for health policy analysis, georgetown university medical center.wilson, r., and e.a.c.crouch. 1987a. risk assessment and comparisons: anintroduction. science 236(4799):267œ270.wilson, r., and e.a.c.crouch. 1987b. letter. science 237(4821):1400.wilson, r., s.d.colome, j.d.spengler, and d.g.wilson. 1980. health effects of fossilfuel burning: assessment and mitigation . cambridge, mass.: ballinger.wise, j. 1987. risk management in a climate of public fear. paper presented at peninsulaindustrial and business association meeting, june 12, 1987.appendix b209improving risk communicationcopyright national academy of sciences. all rights reserved.wolfe, a.k. 1986. confidence in technologies: interactions between publics andindustries. paper presented at the society for risk analysis annual meeting, boston,mass., november 11.zeckhauser, r., and d.s.shephard. 1981. principles for saving and valuing lives. in thebenefits of health and safety regulation, a.ferguson and e.p.leveen, eds.cambridge, mass.: ballinger.zimmerman, r. 1987. a process framework for risk communication. science,technology, and human values 12(3&4):131œ137.appendix b210improving risk communicationcopyright national academy of sciences. all rights reserved.appendix crisk: a guide to controversybaruch fischhoffforeword by the committeethis appendix was written by baruch fischhoff to assist in the deliberationsof the national research council's committee on risk perception andcommunication. it describes in some detail the complications involved incontroversies over managing risks in which risk perception and riskcommunication play significant roles. it addresses these issues from theperspective of many years of research in psychology and other disciplines. thetext of the committee's report addresses many of the same issues, and, notsurprisingly, many of the same themes, although the focus of the report is moregeneral. the committee did not debate all points made in the guide. even thoughthis appendix represents the views of only one member, the committee decided toinclude it because we believe the guide to be a valuable introduction to anextremely complicated literature.prefacethis guide is intended to be used as a practical aid in applying generalprinciples to understanding specific risk management controversies and theirassociated communications. it might be thought of as a user's guide to risk. itsform is that of a ﬁdiagnostic guide,ﬂ showing participants and observers how tocharacterize risk controversiesappendix c211improving risk communicationcopyright national academy of sciences. all rights reserved.along five essential dimensions, such as ﬁwhat are the (psychological) obstaclesto laypeople's understanding of risks?ﬂ and ﬁwhat are the limits to scientificestimates of riskiness?ﬂ its style is intended to be nontechnical, thereby makingthe scientific literature on risk accessible to a general audience. it is hoped thatthe guide will help make risk controversies more comprehensible and helpcitizens and professional risk managers play more effective roles in them.the guide was written for the committee by one of its members. itssubstantive contents were considered by the committee in the course of its work,either in the form of published articles and books circulated to other committeemembers or in the form of issues deliberated at its meetings. as a document, theguide complements the conclusions of the committee's report.contentsi introduction 214 usage 215 some cautions 216ii the science 217 what are the bounds of the problem? 217 what is the hard science related to the problem? 224 adherence to essential rules of science 236 how does judgment affect the risk estimation process? 238 summary 253iii science and policy 254 separating facts and values 254 measuring risk 257 measuring benefits 262 summary 268iv the nature of the controversy 269 the distinction between ﬁactualﬂ and ﬁperceivedﬂ risksis misconceived 270 laypeople and experts are speaking different languages 272 laypeople and experts are solving different problems 273appendix c212improving risk communicationcopyright national academy of sciences. all rights reserved. debates over substance may disguise battles over form,and vice versa 275 laypeople and experts disagree about what is feasible 277 laypeople and experts see the facts differently 278 summary 280v strategies for risk communication 282 concepts of risk communication 282 some simple strategies 283 conceptualizing communication programs 286 evaluating communication programs 291 summary 298vi psychological principles in communicationdesign 299 people simplify 299 once people's minds are made up, it is difficult tochange them 300 people remember what they see 301 people cannot readily detect omissions in the evidencethey receive 301 people may disagree more about what risk is thanabout how large it is 302 people have difficulty detecting inconsistencies in riskdisputes 303 summary 304vii conclusion 305 individual learning 305 societal learning 307 bibliography 309appendix c213improving risk communicationcopyright national academy of sciences. all rights reserved.i introductionrisk management is a complex business. so are the controversies that itspawns. and so are the roles that risk communication must perform. in the faceof such complexity, it is tempting to look for simplifying assumptions. madeexplicit, these assumptions might be expressed as broad statements of the form,ﬁwhat people really want is–ﬂ; ﬁall that laypeople can understand is–ﬂ; orﬁindustry's communicators fail whenever they–.ﬂ like other simplifications inlife, such assumptions provide some shortterm relief at the price of creatinglongterm complications. overlooking complexities eventually leads toinexplicable events and ineffective actions.on one level this guide might be used like a baseball scorecard detailing theplayers' identities and performance statistics (perhaps along with any uniquefeatures of the stadium, season, and rivalry). like a ballgame, a risk controversyshould be less confusing to spectators who know something about the playersand their likely behavior under various circumstances. thus, experts mightrespect the public more if they were better able to predict its behavior, even ifthey would prefer that the public behave otherwise. similarly, understanding thebasics of risk analysis might make disputes among technical experts seem lesscapricious to the lay public.more ambitiously, such a guide might be used to facilitate effective actionby the parties in risk controversies, like the baseball abstract (james, 1988) inthe hands of a skilled manager. for example, the guide discusses how todetermine what the public needs to know in particular risky situations. being ableto identify those needs may allow better focused risk communication, therebyusing the public's limited time wisely and letting it know that the communicatorsreally care about the problems that the public faces. similarly, understanding theethical values embedded in the definitions of ostensibly technical terms (e.g.,risk, benefit, voluntary) can allow members of the public to ask more penetratingquestions about whose interests a risk analysis serves. realizing that differentactors use a term like ﬁriskﬂ differently should allow communicators to removethat barrier to mutual understanding.appendix c214improving risk communicationcopyright national academy of sciences. all rights reserved.usagethe guide's audience includes all participants and observers of riskmanagement episodes involving communications. its intent is to helpgovernment officials preparing to address citizens' groups, industryrepresentatives hoping to site a hazardous facility without undue controversy,local activists trying to decide what information they need and whether existingcommunications meet those needs, and academics wondering how central theirexpertise is to a particular episode.the premise of the guide is that risk communication cannot be understood inisolation. rather, it is one component of complex social processes involvingcomplex individuals. as a result, this fuller context needs to be understood beforerisk communication can be effectively transmitted or received. that contextincludes the following elements and questions: the science. what is the scientific basis of the controversy? what kinds ofrisks and benefits are at stake? how well are they understood? howcontroversial is the underlying science? where does judgment enter the riskestimation process? how well is it to be trusted? science and policy. in what ways does the nature of the science preempt thepolicy making process (e.g., in the definition of key terms, like ﬁriskﬂ andﬁbenefitﬂ; in the norms of designing and reporting studies)? to what extentcan issues of fact and of value be separated? the nature of the controversy. why is there a perceived need for riskcommunication? does the controversy reflect just a disagreement about themagnitude of risks? is controversy over risk a surrogate for controversyover other issues? strategies for risk communication. what are the goals of riskcommunication? how can communications be evaluated? what burden ofresponsibility do communicators bear for evaluating their communications,both before and after dissemination? what are the alternatives for designingrisk communication programs? what are the strengths and weaknesses ofdifferent approaches? how can complementary approaches be combined?what nonscientific information is essential (e.g., the mandates of regulatoryagencies, the reward schemes of scientists)? psychological principles in communication design. what are thebehavioral obstacles to effective risk communication? what kindsappendix c215improving risk communicationcopyright national academy of sciences. all rights reserved.of scientific results do laypeople have difficulty understanding? how doesemotion affect their interpretation of reported results? what presentationsexacerbate (and ameliorate) these problems? how does personal experiencewith risks affect people's understanding?some cautionsa diagnostic guide attempts to help users characterize a situation. to do so,it must define a range of possible situations, only one of which can beexperienced at a particular time. as a result, the attempt to make one guide fit alarge universe of risk management situations means that readers will initiallyhave to read about many potential situations in order to locate the real situationthat interests them. with practice, users should gain fluency with a diagnosticapproach, making it easier to characterize specific situations. it is hoped that thefull guide will be interesting enough to make the full picture seem worthknowing.at no time, however, will diagnosis be simple or human behavior becompletely predictable. all that this, or any other, diagnostic guide can hope to dois ensure that significant elements of a socialpoliticalpsychological process arenot overlooked. for a more detailed treatment, one must look to the underlyingresearch literature for methods and results. to that end, the guide providesnumerous references to that literature, as well as some discussion of its strengthsand limitations.to the extent that a guide is useful for designing and interpreting acommunication process, it may also be useful for manipulating that process. inthis regard, the material it presents is no different than any other scientificknowledge. this possibility imposes a responsibility to make research equallyavailable to all parties. therefore, even though this guide may suggest ways tobias the process, it should also make it easier to detect and defuse such attempts.appendix c216improving risk communicationcopyright national academy of sciences. all rights reserved.ii the scienceby definition, all risk controversies concern the risks associated with somehazard. however, as argued in the text of the report and in this diagnostic guide,few controversies are only about the size of those risks. indeed, in many cases,the risks prove to be a side issue, upon which are hung disagreements about thesize and distribution of benefits or about the allocation of political power in asociety. in all cases, though, some understanding of the science of risk is needed,if only to establish that a rough understanding of the magnitude of the risk is allthat one needs for effective participation in the risk debate. following the text, theterm ﬁhazardﬂ is used to describe any activity or technology that produces a risk.this usage should not obscure the fact that hazards often produce benefits as wellas risks.understanding the science associated with a hazard requires a series ofessential steps. the first is identifying the scope of the problem underconsideration, in the sense of identifying the set of factors that determine themagnitude of the risks and benefits produced by an activity or technology. thesecond step is identifying the set of widely accepted scientific ﬁfactsﬂ that can beapplied to the problem; even when laypeople cannot understand the scienceunderlying these facts, they may at least be able to ensure that such acceptedwisdom is not contradicted or ignored in the debate over a risk. the third step inunderstanding the science of risk is knowing how it depends on the educatedintuitions of scientists, rather than on accepted hard facts; although these may bethe judgments of trained experts, they still need to be recognized as matters ofconjecture that are both more likely to be overturned than published (andreplicated) results and more vulnerable to the vagaries of psychologicalprocesses.what are the bounds of the problem?the science learned in school offers relatively tidy problems. the typicalexercise in, say, physics gives all the facts needed for its solution and nothing butthose facts. the difficulty of such problems for students comes in assemblingthose facts in a way that provides the right answer. (in more advanced classes,one may have to bring some general facts to bear as well.)appendix c217improving risk communicationcopyright national academy of sciences. all rights reserved.the same assembly problem arises when analyzing the risks and benefits of ahazard. scientists must discover how its pieces fit together. they must also figureout what the pieces are. for example, what factors can influence the reliability of anuclear power plant? or, whose interests must be considered when assessing thebenefits of its operation? or, which alternative ways of generating electricity arerealistic possibilities?the scientists responsible for any piece of a risk problem must face a set ofsuch issues before beginning their work. laypeople trying to follow a risk debatemust understand how various groups of scientists have defined their pieces of theproblem. and, as mentioned in the report, even the most accomplished ofscientists are laypeople when it comes to any aspects of a risk debate outside therange of their trained expertise.the difficulties of determining the scope of a risk debate emerge quiteclearly when one considers the situation of a reporter assigned to cover a riskstory. the difficult part of getting most environmental stories is that no oneperson has the entire story to give. such stories typically involve diverse kinds ofexpertise so that a thorough journalist might have to interview specialists intoxicology, epidemiology, economics, groundwater movement, meteorology, andemergency evacuation, not to mention a variety of local, state, and federalofficials concerned with public health, civil defense, education, andtransportation.even if a reporter consults with all the relevant experts, there is no assuranceof complete coverage. for some aspects of some hazards, no one may beresponsible.for example, no evacuation plans may exist for residential areas that arepacked ﬁhopelesslyﬂ close to an industrial facility. no one may be capable ofresolving the jurisdictional conflicts when a train with military cargo derails near areservoir just outside a major population center. there may be no scientificexpertise anywhere for measuring the longterm neurological risks of a newchemical.even when there is a central address for questions, those occupying it maynot be empowered to take firm action (e.g., banning or exonerating a chemical)or to provide clearcut answers to personal questions (e.g., ﬁwhat should i do?ﬂor ﬁwhat should i tell my children?ﬂ). often those who have the relevantinformation refuse to divulge it because it might reveal proprietary secrets or turnpublic opinion against their cause.appendix c218improving risk communicationcopyright national academy of sciences. all rights reserved.having to piece together a story from multiple sources, even recalcitrantones, is hardly new to journalists. what is new about many environmental storiesis that no one knows what all of the pieces are or realizes the limits of their ownunderstanding.experts tend to exaggerate the centrality of their roles. toxicologists mayassume that everyone needs to know what they found when feeding rats apotential carcinogen or when testing groundwater near a landfill, even thoughadditional information is always needed to make use of those results (e.g.,physiological differences among species, routes of human exposure,compensating benefits of the exposure).another source of confusion is the failure of experts to remind laypeople ofthe acknowledged limits of the experts' craft. for example, costbenefit analystsseldom remind readers that the calculations consider only total costs and benefitsand, hence, ignore questions of who pays the costs and who pays the benefits(bentkover et al., 1985; smith and desvousges, 1986).finally, environmental management is an evolving field that is onlybeginning to establish comprehensive training programs and methods, making ithard for anyone to know what the full picture is and how their work fits into it.an enterprising journalist with a modicum of technical knowledge should beable to get specialists to tell their stories in fairly plain english and to cope withmoderate evasiveness or manipulation. however, what is the journalist to dowhen the experts do not know what they do not know? one obvious solution is totalk to several experts with maximally diverse backgrounds. yet, sometimes sucha perfect mix is hard to find. available experts can all have common limitationsof perspective.another solution is to use a checklist of issues that need to be covered in anycomprehensive environmental story. scientists themselves use such lists to ensurethat their own work is properly performed, documented, and reported. such aprotocol does not create knowledge for the expert any more than it would providean education to the journalist. it does, however, help users exploit all they knowšand acknowledge what they leave out.some protocols that can be used in looking at risk analyses are the causalmodel, the fault tree, a materials and energy flow diagram, and a risk analysischecklist.appendix c219improving risk communicationcopyright national academy of sciences. all rights reserved.figure ii.1 the causal chain of hazard evolution. the top line indicates sevenstages of hazard development, from the earliest (left) to the final stage (right).these stages are expressed generically in the top of each box and in terms of asample motor vehicle accident in the bottom. the stages are linked by causalpathways denoted by triangles. six control stages are linked to pathwaysbetween hazard states by vertical arrows. each is described generically as wellas by specific control actions. thus control stage 2 would read: ﬁyou can modifytechnology choice by substituting public transit for automobile use and thusblock the further evolution of the motor vehicle accident sequence arising out ofautomobile use.ﬂ the time dimension refers to the ordering of a specific hazardsequence; it does not necessarily indicate the time scale of managerial action.thus, from a managerial point of view, the occurrence of certain hazardconsequences may trigger control actions that affect events earlier in the hazardsequence. source: figurešbick et al., 1979; captionšfischhoff,lichtenstein, et al., 1981.the causal modelthe causal model of hazard creation is a way to organize the full set offactors leading to and from an environmental mishap, both when getting the storyand when telling it. the example in figure ii.1 is an automobile accident, tracedfrom the need for transportation to the secondary consequence of the collision.between each stage, there is some opportunity for an intervention to reduce therisk of an accident. by organizing information about the hazard in achronological sequence, this scheme helps ensure that nothing is left out, such asthe deepseated causes of the mishap (to the left) and its longrange consequences(to the right).applied to an ﬁirregular eventﬂ at a nuclear power station, for example, thisprotocol would work to remind a reporter of such (lefthanded) causes as the needfor energy and the need to protect the large capital investment in that industry andsuch (righthanded) consequences as the costs of retooling other plants designedlike theappendix c220improving risk communicationcopyright national academy of sciences. all rights reserved.affected plant or the need to burn more fossil fuels if the plant is taken off line(without compensating reductions in energy consumption).the fault treea variant on this procedure is the fault tree (figure ii.2), which lays out thesequence of events that must occur for a particular accident to happen (green andbourne, 1972; u.s. nuclear regulatory commission, 1983). actual fault trees,which can be vastly more involved than this example, are commonly used toorganize the thinking and to coordinate the work of those designing complextechnologies such as nuclear power facilities and chemical plants. at times, theyare also used to estimate the overall riskiness of such facilities. however, thenumbers produced are typically quite imprecise (u.s. nuclear regulatorycommission, 1978).in effect, fault trees break open the righthanded parts of a causal model fordetailed treatment. they can help a reporter tofigure ii.2 fault tree indicating the possible ways that radioactivity could bereleased from deposited wastes after the closure of a repository. source:slovic and fischhoff, 1983.appendix c221improving risk communicationcopyright national academy of sciences. all rights reserved.order the pieces of an accident story collected from different sources, see wherean evolving incident (e.g., three mile island or a leaking waste dump) isheading, and find out what safety measures were or were not taken.materials and energy flow diagramsthe next model (figure ii.3) is adapted from the engineering notion of amaterials or energy flow diagram. if something is neither created nor destroyed in aprocess, then one should be able to account schematically for every bit of it. inenvironmental affairs, one wants to account for all toxic materials. it is importantto know where each toxic agent comes from and where each goes.keeping track of a substance can help anticipate where problems willappear, recur, and disappear. it can reveal when a problem has actually beentreated and when it has merely been shifted to another time, place, or jurisdiction.with a story like edb (ethylene dibromide, a fungicide used on grain) (sharlin,1987), such a chart would have encouraged questions such as, does it decay withstorage or does it become something even worse when cooked and digested?applying this approach led harriss and hohenemser (1978) to conclude thatpollution controls had not reduced the total amount of mercury released into theenvironment, but only the distribution of releases (replacing a few big polluterswith many smaller ones). in creating such figures, it is important to distinguishbetween where a substance is supposed to go and where it actually goes.a comparable figure might be drawn to keep track of where the moneygoes, identifying the beneficiaries and losers resulting from different regulatoryactions. with the edb story, such a chart would have encouraged questionsabout who would eventually pay for the grain lost to pests if that chemical werenot used. that is, would reducing the risk of edb reduce producers' profits orincrease consumers' prices? in the former case, failure to ban edb looks muchmore callous than in the latter.a risk analysis checklistthe fourth aid (figure ii.4) is a list of questions that can be asked in a riskanalysis (or of a risk analyst) in order to clarify what problem has been addressedand how well it has been solved.this list was compiled for a citizens' group concerned with pesticides. itsmembers had mastered many substantive details of theappendix c222improving risk communicationcopyright national academy of sciences. all rights reserved.discipline, such as toxicology and biochemistry, involved in pesticidemanagement, when suddenly they were confronted with a new procedurešriskanalysis. in principle, risk analysis does no more than organize information fromsubstantive disciplines in a way that allows overall estimates of risk to becomputed. it can facilitate citizen access by forcing all the facts out on the table.figure ii.3 materials and energy flow diagram: current options for thenuclear fuel cycle. source: gotchy, 1983.appendix c223improving risk communicationcopyright national academy of sciences. all rights reserved.figure ii.4 risk analysis checklist. source: northwest coalition foralternatives to pesticides, 1985.however, unless one can penetrate all its formalisms, risk analysis canmystify and obscure the facts rather than reveal them. such a checklist can clarifywhat an analysis has done in terms approximating plain english.what is the hard science related to the problem?with most ﬁinterestingﬂ hazards, the data run out long before enough isknown to estimate their risks and benefits as precisely as one would want. muchof risk management involves going beyond the available data either to guess atwhat the facts might be or to figure out how to live with uncertainty. obviously,one wants to reduce this uncertainty by making the best of the hard dataavailable.unfortunately, there is no shortcut to providing observers with ways to readcritically all of the kinds of science that could be invoked in the course ofcharacterizing a risk. there are too many sciences to consider and too manynuances in each type of science to knowappendix c224improving risk communicationcopyright national academy of sciences. all rights reserved.about in assessing the validity of studies conducted in any one field. even thesocial sciences, which seem relatively accessible (compared with the physicalsciences) and the results of which can be rendered into common english,routinely foil the efforts of amateur scientists.these failures can be seen most clearly in the attempts by nonsocialscientists to make factual statements about the behavior of laypeople, solely onthe basis of their untrained anecdotal observations. such speculations can misleadmore than inform if they are made without realizing that they lack the disciplineof science.the complexities of science arise in the details of creating, analyzing, andinterpreting specific sets of data. to give a feeling for these strengths and limitsof scientific research, several examples drawn from social science research intorisk perception and communication are presented here. each science has its ownnuances. featuring this science also provides background for interpreting thesocial science results described below.like speculations about chemical reactions, speculations about humanbehavior must be disciplined by fact. such speculations make importantstatements about people and their capabilities, and failure to validate them maymean arrogating to oneself considerable political power. such happens, forexample, when one says that people are so poorly informed (and ineducable) theyrequire paternalistic institutions to defend them, and, furthermore, they might bebetter off surrendering some political rights to technical experts. it also happens,at the other extreme, when one claims that people are so well informed (andoffered such freedom of choice) one need not ask them anything at all about theirdesires; to know what they want, one need only observe their behavior in themarketplace. it also happens when we assume that people are consummatehedonists, rational to the extreme in their consumer behavior but totallyuncomprehending of broader economic issues, so we can impose effective fiscalpolicies on them without being secondguessed.one reason for the survival of such simplistic and contradictory positions ispolitical convenience. some people want the lay public to participate actively inhazard management decisions, and need to be able to describe the public ascompetent; others need an incompetent public to legitimate an expert elite. asecond reason is theoretical convenience. it is hard to build models of people whoare sometimes wise and sometimes foolish, sometimes risk seeking andsometimes risk averse. a third reason is that one can effortlessly speculate abouthuman nature and even produce a bit of supporting anecdotalappendix c225improving risk communicationcopyright national academy of sciences. all rights reserved.information. indeed, good social theory may be so rare because poor social theoryis so easy.judgments of riskat first sight, assessing the public's risk perceptions would seem to be verystraightforward. just ask questions like, ﬁwhat is the probability of a nuclear coremeltdown?ﬂ or ﬁhow many people die annually from asbestosrelated diseases?ﬂor ﬁhow does wearing a seat belt affect your probability of living through theyear?ﬂ once the results are in, they can be compared with the best availabletechnical estimates, with deviations interpreted as evidence of respondents'ignorance.unfortunately, how one asks the question may in large part determine thecontent (and apparent wisdom) of the response. lichtenstein and her colleagues(lichtenstein et al., 1978) asked two groups of educated laypeople to estimate thefrequency of death in the united states from each of 40 different causes. thegroups differed only in the information that was given to them about one cause ofdeath in order to help scale their responses. one group was told about 50,000people die annually in motor vehicle accidents, and the other was told about 1,000annual deaths result from electrocution. both reports were accurate, but receivinga larger number increased the estimates of most frequencies for respondents inthe motor vehicle accident group. this is a special case of a generalpsychological phenomenon called ﬁanchoring,ﬂ whereby people's responses arepulled toward readily available numbers in cases in which they do not knowexactly what to say (poulton, 1968, 1977; tversky and kahneman, 1974). suchanchoring on the original number changed the smallest estimates by roughly afactor of 5.fischhoff and macgregor (1983) asked people to judge the lethality ofvarious potential causes of death using one of four formally equivalent formats(e.g., ﬁfor each afflicted person who dies, how many survive?ﬂ or ﬁfor each100,000 people afflicted, how many will die?ﬂ). table ii.1 expresses theirjudgments in a common format and reveals even more dramatic effects ofquestion phrasing on expressed risk perceptions. for example, when peopleestimated the lethality rate for influenza directly (column 1), their mean responsewas 393 deaths per 100,000 cases. when told that 80 million people catchinfluenza in a normal year and asked to estimate theappendix c226improving risk communicationcopyright national academy of sciences. all rights reserved.table ii.1 lethality judgments with four different response modes (geometricmean)death rate per 100,000 afflictedconditionestimatedlethalityrateestimatednumberwho dieestimatedsurvivalrateestimatednumberwhosurviveactuallethalityrateinfluenza3936265111mumps4411419412asthma155121459933venerealdisease9163811150high bloodpressure535891753876bronchitis1621943211185pregnancy672413787250diabetes487101525666800tuberculosis852178318885201535automobileaccidents619532723168132500strokes11,011464818124,75811,765heart attacks13,011366613127,47716,250cancer10,88910,47516021,74937,500note: the four experimental groups were given the following instructions:(a) estimate lethality rate: for each 100,000 people afflicted, how many die?(b) estimate number who die: x people were afflicted, how many died?(c) estimate survival rate: for each person who died, how many were afflictedbut survived?(d) estimate number who survive: y people died, how many were afflicted butdid not die?responses to (b), (c), and (d) were converted to deaths per 100,000 to facilitate comparisons.source: fischhoff and macgregor, 1983.number who die (column 2), their mean response was 4800, representing adeath rate of only 6 per 100,000 cases. this slight change in the question changedthe estimated rate by a factor of more than 60. similar discrepancies occurredwith other questions and other hazards. one consequence for risk communicatorsis that whether lay people intuitively overestimate or underestimate risks (orperceive them accurately) depends on what question they are asked.in a recent study at an ivy league college (linville et al., 1988), studentswere asked to give estimates of the probability that the aids virus could betransmitted from a man to a woman in a single case of unprotected sex. themedian estimate was about 10 percent, considerably above current scientificestimates (fineberg,appendix c227improving risk communicationcopyright national academy of sciences. all rights reserved.1988). however, when asked to give estimates for the probability of transmissionin 100 cases of unprotected sex, the median answer was about 25 percent. thisrisk estimate is considerably more in line with scientific thinkingšso that aninvestigator asking this question would have a considerably more optimisticassessment of the state of public understanding. unfortunately, it is alsocompletely inconsistent with the singlecase estimates produced by the sameindividuals. if one believes in a singlecase probability of 10 percent, thentransmission should be a virtual certainty with 100 exposures. such failure to seehow small risks mount up over repeated exposures has been observed in suchdiverse settings as the risks from playing simple gambles (barhillel, 1973),driving (slovic et al., 1978), and relying on various contraceptive devices(shaklee et al., 1988).such effects are hardly new; indeed, some have been recognized for close to100 years. early psychologists discovered that different numerical judgments maybe attached to the same physical stimulus (e.g., the loudness of a tone) as afunction of whether the set of alternatives is homogeneous or diverse, andwhether the respondent makes one or many judgments. even when the samepresentation is used, different judgments might be obtained with a numerical or acomparative (ordinal) response mode, with instructions stressing speed oraccuracy, with a bounded or an unbounded response set, and with verbal ornumerical response labels.the range of these effects may suggest that the study of judgment is not justdifficult, but actually impossible. closer inspection, however, revealsconsiderable orderliness underlying this apparent chaos (atkinson et al., 1988;carterette and friedman, 1974; woodworth and schlosberg, 1954).judgments of valuesonce the facts of an issue have been estimated and communicated, it isusually held that laypeople should (in a democracy) be asked about their values.what do they wantšafter the experts have told them what they can(conceivably) have? here, too, the straightforward strategy of ﬁjust ask themﬂruns into trouble.the problem of poorly (or even misleadingly) worded questions in attitudesurveys is well known, although not necessarily well resolved (bradburn andsudman, 1979; national research council, 1982; payne, 1952; zeisel, 1980). forexample, a major trade pubappendix c228improving risk communicationcopyright national academy of sciences. all rights reserved.lication (ventner, 1979) presented the results of a survey of public attitudestoward the chemical industry containing the following question:some people say that the prime responsibility for reducing exposure of workersto dangerous substances rests with the workers themselves, and that allsubstances in the workplace should be clearly labeled as to their levels of dangerand workers then encouraged or forced to be careful with these substances. doyou agree or disagree?it is hard to know what one is endorsing when one says ﬁyes,ﬂ ﬁno,ﬂ or ﬁidon't knowﬂ to such a complex and unclear question.although annoying, ambiguous wording is, in principle, a relatively easyproblem to deal with because there are accepted ways to ﬁdo it right.ﬂ much morecomplicated are cases in which seemingly arbitrary aspects of how a question isposed affect the values. parducci (1974) has found that judged satisfaction withone's state in life may depend on the range of possible states mentioned in thequestion put to people. in an attempt to establish a dollar value for aestheticdegradation of the environment, brookshire et al. (1976) asked visitors to lakepowell how much they would be willing to pay in increased users' fees in ordernot to have an ugly (coalfired) power plant looming on the opposite shore. theyasked ﬁwould you pay $1, $2, $3?ﬂ and so on, until the respondent answeredﬁnoﬂ and then they retreated in decrements of a quarter (e.g., ﬁwould you pay$5.75, $5.50,–?ﬂ). rather different numerical values might have been obtainedhad the bidding procedure begun at $100 and decreased by steps of $10 or withother plausible variants. any respondents who were not sure what they wanted indollars and cents might naturally and necessarily look to the range of optionspresented, the difference between first and second options, and so on, for cues asto what are reasonable and plausible responses (cummings et al., 1986; smithand desvousges, 1986).at first glance, it might seem as though questions of value are the lastredoubt of unaided intuition. who knows better than an individual what he or sheprefers? when people are considering simple, familiar events with which theyhave direct experience, it may be reasonable to assume that they have wellarticulated opinions. regarding the novel, global consequences potentiallyassociated with co2induced climatic change, nuclear meltdowns, or geneticengineering, that may not be the case. our values may be incoherent, not thoughtthrough. in thinking about what are acceptable levels of risk, for example, wemay be unfamiliar with the terms in whichappendix c229improving risk communicationcopyright national academy of sciences. all rights reserved.issues are formulated (e.g., social discount rates, minuscule probabilities, ormegadeaths). we may have contradictory values (e.g., a strong aversion tocatastrophic losses of life and a realization that we are no more moved by a planecrash with 500 fatalities than by one with 300). we may occupy different roles inlife (parents, workers, children) that produce clearcut but inconsistent values. wemay vacillate between incompatible, but strongly held, positions (e.g., freedom ofspeech is inviolate, but should be denied to authoritarian movements). we maynot even know how to begin thinking about some issues (e.g., the appropriatetradeoff between the opportunity to dye one's hair and a vague, minute increasein the probability of cancer 20 years from now). our views may undergo changesover time (say, as we near the hour of decision or of experiencing theconsequence) and we may not know which view should form the basis of ourdecision.an extreme, but not uncommon, situation is having no opinion and notrealizing it. in that state, we may respond with the first thing that comes to mindonce a question is asked and then commit ourselves to maintaining that firstexpression and to mustering support for it, while suppressing other views anduncertainties. as a result, we may be stuck with stereotypical or associativeresponses, generated without serious contemplation.once an issue has been evoked, it must be given a label. in a world with fewhard evaluative standards, such symbolic interpretations may be very important.while the facts of abortion remain constant, individuals may vacillate in theirattitude as they attach and detach the label of murder. figure ii.5 shows twoversions of the same gamble, differing only in whether one consequence islabeled a ﬁsure lossﬂ or an ﬁinsurance premium.ﬂ most people dislike the formerand like the latter. when these two versions are presented sequentially, peopleoften reverse their preferences for the two options (hershey and shoemaker,1980). figure ii.6 shows a labeling effect that produced a reversal of preferencewith practicing physicians; most preferred treatment a over treatment b, andtreatment d over treatment c, despite the formal equivalence of a and c and of band d. saving lives and losing lives afforded very different perspectives on thesame problem.people solve problems, including the determination of their own values, withwhat comes to mind. the more detailed, exacting, and creative their inferentialprocess, the more likely they are to think of all they know about a question. thebriefer that process becomes,appendix c230improving risk communicationcopyright national academy of sciences. all rights reserved.the more they will be controlled by the relative accessibility of variousconsiderations. accessibility may be related to importance, but it is also related tothe associations that are evoked, the order in which questions are posed,imaginability, concreteness, and other factors only loosely related to importance.as one example of how an elicitor may (perhaps inadvertently) controlrespondents' perspective, turner (1980) observed a large difference in responsesto a simple question such as ﬁare you happy?ﬂ on two simultaneous surveys ofthe same population (figure ii.7). the apparent source of the difference was thatone (norc) preceded the happiness question with a set of questions aboutmarried life. in the united states, married people are generally happier thanunmarried people. reminding them of that aspect of their life apparently changedthe information that they brought to the happiness question.figure ii.5 two formulations of a choice problem: insurance versus certainloss. source: fischhoff et al., 1980.it would be comforting to be able to say which way of phrasing thesequestions is most appropriate. however, there is no general answer. one needs toknow why the question is being asked (fischhoffappendix c231improving risk communicationcopyright national academy of sciences. all rights reserved.figure ii.6 two formulations of a choice problem: lives saved versus liveslost. source: tversky and kahneman, 1981. copyright © 1981 by theamerican association for the advancement of science.and furby, 1988). if one wants to predict the quality of casual encounters,then a superficial measure of happiness may suffice. however, an appraisal ofnational malaise or suicide potential may require a questioning procedure thatevokes an appreciation of all components of respondents' lives. it has been knownfor some time that white interviewers evoke more moderate responses fromblacks on racerelated questions than do black interviewers. the usual responsehas been to match the races of interviewer and interviewee (martin, 1980). thissolution may be appropriate for predicting voting behavior or conversation insamerace bars, but not for predicting behavior of blacks in whitedominatedworkplaces.the fact that one has a question is no guarantee that respondents haveanswers, or even that they have devoted any prior thought to the matter. whenone must have an answer (say, because public input is statutorily required), theremay be no substitute for an elicitation procedure that educates respondents abouthow they might look at the question. the possibilities for manipulation in suchinterviews are obvious. however, one cannot claim to be serving respondents'best interests (letting them speak their minds) by asking a questionappendix c232improving risk communicationcopyright national academy of sciences. all rights reserved.that only touches one facet of a complex and incompletely formulated set ofviews.refining common sensesocial scientists often find themselves in a nowin situation. if they describetheir work in technical jargon, no one wants to listen. if they use plain language,no one feels a need to listen. listeners feel that they ﬁknew it all alongﬂ and thatthe social scientist was just ﬁaffirming the obviousﬂ or ﬁvalidating commonsense.ﬂ one possible antidote to this feeling is to point out the evidence showingthat, in hindsight, people exaggerate how much they could have known inforesight, leading them to discount the informativeness of scientificfigure ii.7 trends in selfreported happiness derived from sample surveys ofthe noninstitutionalized population of the continental united states aged 18 andover. error bars demark ±1 standard error around sample estimate.source: turner, 1980.appendix c233improving risk communicationcopyright national academy of sciences. all rights reserved.reports (slovic and fischhoff, 1977). a second antidote is to note that commonsense often makes contradictory predictions (e.g., two heads are better than oneversus too many cooks spoil the broth; absence makes the heart grow fonderversus out of sight, out of mind). research is needed to determine which versionof common sense is correct or what their respective ranges of validity are. a thirdstrategy, adopted immediately below, is to present empirical results thatcontradict conventional wisdom (lazarsfeld, 1949).informing people about risksit is often claimed that people do not want to know very much about thehealth risks they face, since such information makes them anxious. moreover,they cannot use that information very productively, even if it is given. if true,these claims would make it legitimate for someone else (e.g., physicians,manufacturers, government) to decide what health (and therapeutic) risks areacceptable, and not to invest too much effort on information programs. a numberof investigators, however, have replaced anecdotal evidence with systematicobservation and have found that, by and large, people want to be told aboutpotential risks (alfidi, 1971; weinstein, 1980a). in clinical settings, this desirehas been observed with such risky practices as psychotropic medication(schwarz, 1978), endoscopy (roling et al., 1977), and oral contraceptives(applied management sciences, 1978; joubert and lasagna, 1975). figure ii.8shows respondents' strong opinions about the appropriate use of a pamphletdesigned to explain the risks faced by temporary workers in a nuclear powerplant. ninety percent of these individuals gave the most affirmative answerpossible to the question, ﬁif you had taken such a job without being shown thispamphlet, would you feel that you had been deprived of necessaryinformation?ﬂ (fischhoff, 1981).risktaking propensitywe all know that some people are risk takers and others are risk avoiders;some are cautious, whereas others are rash. indeed, attitude toward risk might beone of the first attributes that comes to mind when one is asked to describesomeone else's personality. in 1962, slovic compared the scores of 82 individualson nine different measures of risk taking. he found no consistency at all inpeople's propensity for taking risks in the settings created by the various tests(slovic, 1962). correlations ranged fromš.35 to .34, with a mean ofappendix c234improving risk communicationcopyright national academy of sciences. all rights reserved.figure ii.8 opinions about the appropriate use of a pamphlet describing therisks associated with temporary work in a facility handling nuclear materials.respondents were drawn from the readers of a student newspaper and fromunemployed individuals at a state labor exchange. the ﬁxﬂ on each linerepresents the mean response to a question by the 173 individuals. source:fischhoff, 1981..006. that is, people who are daring in one context may be timid in another, aresult that has been replicated in numerous other studies (davidshofer, 1976).the surprising nature of these results may tell us something about ourselvesas well as about the people we observe. one of the most robust psychologicaldiscoveries of the past 20 years has been identification of the fundamentalattribution error, the tendency to view ourselves as highly sensitive to thedemands of varying situations, but to see others as driven to consistent behaviorby dominating personality traits (nisbett and ross, 1980). this misperceptionmay be attributable to the fact that we typically see most others in only one role,as workers or spouses or parents or tennis players or drivers or whatever, in whichthe situational pressures are quite consistent. thus, we may observe accuratelythe evidence available to us, but fail to understand the universe from which thesedata are drawn.protective behaviorfor years, the united states has been building flood control projects. despitethese great expenditures, flood losses today (inappendix c235improving risk communicationcopyright national academy of sciences. all rights reserved.constant dollars) are greater than they were before this enterprise began.apparently, the behavioral models of the dam and levee builders failed toaccount for the extent to which eliminating the recurrence of smalltomoderatefloods reduced residents' (and particularly newcomers') sensitivity to flooddangers, which in turn led to overbuilding the flood plain. as a result, when thebig floods come (about once every 100 years), exceeding the containmentcapacity of the protective structures, much more lies in their path (white, 1974).the official response to this situation has been the national flood insuranceprogram (kunreuther et al., 1978), designed according to economic models ofhuman behavior, which assumes that flood plain residents are allknowing, allcaring, and entirely ﬁrationalﬂ (as defined by economics). initially, premiumswere greatly subsidized by the federal government to make the insurance highlyattractive; these subsidies were to be withdrawn gradually once the insurancebuying habit was established. unfortunately for the program, few people boughtthe insurance. the typical explanation for this failure was that residents expectedthe government to bail them out in the event of flood. however, a field surveyfound this speculation, too, to be in error. flood plain residents reported that theyexpected no help, feeling that they were willingly bearing an acceptable risk.when residents thought about insurance at all, they seemed to rely on a melangeof ad hoc principles like, ﬁi can't worry about everythingﬂ and ﬁthe chances ofgetting a return (reimbursement) on my investment (premium) are too small,ﬂrather than on the concepts and procedures of economics (kunreuther et al.,1978; slovic et al., 1977).adherence to essential rules of sciencelooking hard at other sciences would reveal them to be similarlycomplicated, and similarly surprising. sciences may not reveal their intricaciesreadily, but committed citizen activists have often proven themselves capable ofmastering enough of the relevant science to be able to ask hard questions aboutrisk issues that interest them (figure ii.4, for example, was created as a steptoward this end). many, of course, do not, and none could learn the hardquestions about all of the sciences impinging on complex risk issues. this is,however, an option for those who care enough.short of such intense involvement, it is possible to ask someappendix c236improving risk communicationcopyright national academy of sciences. all rights reserved.generic questions about almost any science. these are ways of asking ﬁhow goodcould it be?ﬂ, given the conditions of its production.perhaps the most basic question that one can ask about any bit of scienceintroduced into an environmental dispute, whether it be a single rodent bioassayor a fullblown risk analysis, is whether it actually represents a bit of science. inapplied settings, one often finds evidence that fails to adhere to such essentialrules of science as: (1) subjecting the study to critical peer review; (2) making alldata available to other investigators; (3) evaluating the statistical reliability ofresults; (4) considering alternative explanations of the results; (5) relating newresults to those already in the literature; and (6) pointing out critical assumptionsthat have not been empirically verified. studies that fail to follow suchprocedures may be attempting to assume the rights, but not the responsibilities ofscience. conversely, good science can come even from partisan sources (e.g.,industry labs, environmental activists), if the rules are followed.the definitiveness of science is bounded not only by the process by which itis conducted, but also by the object of its study. some topics are simply easierthan others, allowing for results clouded by relatively little uncertainty.unfortunately for the rapid understanding and resolution of problems, riskmanagement often demands understanding of inherently difficult topics.this difficulty for risk managers can be seen as a byproduct of onefortunate feature of the natural environment, namely, that the most fearsomeevents are quite infrequent. major floods, disastrous plagues, and catastrophictremors are all the exception rather than the rule. social institutions attempt toconstrain hazards of human origin so that the probability of their leading todisaster is low. however great their promised benefit, projects that mightfrequently kill large numbers of people are unlikely to be developed. thedifficult cases are those in which the probability of a disaster is known to be low,but we do not know just how low. unfortunately, quantitative assessment of verysmall probabilities is often very difficult (fairley, 1977).at times, one can identify a historical record that provides frequencyestimates for an event related to the calamity in question. the u.s. geologicalsurvey has perhaps 75 years of reliable data on which to base assessments of thelikelihood of large earthquakes (burton et al., 1978). iceland's copiousobservations of icepack movements over the last millennium provide a clue tothe probability of an extremely cold year in the future (ingram et al., 1978). theappendix c237improving risk communicationcopyright national academy of sciences. all rights reserved.absence of a fullscale meltdown in 500 to 1000 reactoryears of nuclear powerplant operation sets some bounds on the probability of future meltdowns(weinberg, 1979). of course, extrapolation from any of these historical records isa matter of judgment. the great depth and volume of artificial reservoirs mayenhance the probability of earthquakes in some areas. increased carbon dioxideconcentrations in the atmosphere may change the earth's climate in ways thatamplify or moderate yearly temperature fluctuations. changes in design, staffing,and regulation may render the next 1000 reactoryears appreciably different fromtheir predecessors. indeed, any attempt to learn from experience and make atechnology safer renders that experience less relevant for predicting futureperformance.even when experts agree on the interpretation of records, a sample of 1000reactoryears or calendaryears may be insufficient. if one believes the worstcasescenarios of some opponents of nuclear power, a 0.0001 chance of a meltdown(per reactoryear) might seem unconscionable. however, we will be into the nextcentury before we will have enough online experience to know with greatconfidence whether the historical probability is really that low.how does judgment affect the risk estimationprocess?to the extent that historical records (or records of related systems) areunavailable, one must rely on conjecture. the more sophisticated conjectures arebased on models such as the faulttree and eventtree analyses of a lossofcoolant accident upon which the reactor safety study was based (u.s. nuclearregulatory commission, 1975). as noted in figure ii.2, a fault tree consists of alogical structuring of what would have to happen for an accident (e.g., ameltdown) to occur. if sufficiently detailed, it will reach a level of specificity forwhich one has direct experience (e.g., the operation of individual valves). theoverall probability of system failure is determined by combining the probabilitiesof the necessary component failures.the trustworthiness of such an analysis hinges on the experts' ability toenumerate all major pathways to disaster and on the assumptions that underlie themodeling effort. unfortunately, a modicum of systematic data and manyanecdotal reports suggest that experts may be prone to certain kinds of errors andomissions. table ii.2appendix c238improving risk communicationcopyright national academy of sciences. all rights reserved.table ii.2 some problems in structuring risk assessmentsfailure to consider the ways in which human errors can affect technological systems.example: owing to inadequate training and control room design, operators at threemile island repeatedly misdiagnosed the problems of the reactor and tookinappropriate actions (sheridan, 1980; u.s. government, 1979).overconfidence in current scientific knowledge.example: ddt came into widespread and uncontrolled use before scientists had evenconsidered the possibility of the side effects that today make it look like a mixed, andirreversible, blessing (dunlap, 1978).failure to appreciate how technological systems function as a whole.example: the dc10 failed in several early flights because its designers had notrealized that decompression of the cargo compartment would destroy vital controlsystems (hohenemser, 1975).slowness in detecting chronic, cumulative effects.example: although accidents to coal miners have long been recognized as one cost ofoperating fossilfueled plants, the effects of acid rain on ecosystems were slow to bediscovered (rosencranz and wetstone, 1980).failure to anticipate human response to safety measures.example: the partial protection afforded by dams and levees gives people a falsesense of security and promotes development of the flood plain. thus, although floodsare rarer, damage per flood is so much greater that the average yearly loss in dollars islarger than before the dams were built (burton et al., 1978).failure to anticipate commonmode failures, which simultaneously afflict systems thatare designed to be independent.example: because electrical cables controlling the multiple safety systems of thereactor at browns ferry, alabama, were not spatially separated, all five emergencycorecooling systems were damaged by a single fire (jennergren and keeney, 1982;u.s. government, 1975).source: fischhoff, lichtenstein, et al., 1981a.suggests some problems that might underlie the confident veneer of a formalmodel.when the logical structure of a system cannot be described to allowcomputation of its failure probabilities (e.g., when there are large numbers ofinteracting systems), physical or computerized simulation models may be used. ifone believes the inputs and the programmed interconnections, one should trustthe results. what happens, however, when the results of a simulation arecounterintuitive or politically awkward? there may be a strong temptation toappendix c239improving risk communicationcopyright national academy of sciences. all rights reserved.try it again, adjusting the parameters or assumptions a bit, given that many ofthese are not known with certainty in the first place. susceptibility to thistemptation could lead to a systematic and subtle bias in modeling. at theextreme, models would be accepted only if they confirmed expectations.acknowledging the role of judgmentalthough the substance of sciences differs greatly, sciences do have incommon the fact that they are produced by the minds of mortals. those mindsmay contain quite different facts, depending on the disciplines in which they weretrained. however, it is reasonable to suppose that they operate according tosimilar principles when they are pressed to make speculationsštaking thembeyond the limits of hard datašin order to produce the sorts of assessmentsneeded to guide risk managers.indeed, the need for judgment is a defining characteristic of risk assessment(federal register 49(100):21594œ21661). some judgment is, of course, a part ofall science. however, the policy questions that hinge on the results of riskassessments typically demand greater scope and precision than can be providedby the ﬁhardﬂ knowledge that any scientific discipline currently possesses. as aresult, risk assessors must fill the gaps as best they can. the judgmentsincorporated in risk assessments are typically those of esteemed technicalexperts, but they are judgments nonetheless, taking one beyond the realm ofestablished fact and into the realm of educated opinions that cannot immediatelybe validated.judgment arises whenever materials scientists estimate the failure rates forvalves subjected to novel conditions (joksimovich, 1984; ostberg et al., 1977),whenever accident analysts attempt to recreate operators' perceptions of theirsituation prior to fatal mishaps (kadlec, 1984; pew et al., 1982), whentoxicologists choose and weight extrapolation models (rodricks and tardiff,1984; tockman and lilienfeld, 1984), when epidemiologists assess the reasonsfor nonresponse in a survey (joksimovich, 1984; national research council,1982), when pharmacokineticists consider how consumers alter the chemicalcomposition of foods (e.g., by cooking and storage practices) before theyconsume them (national research council, 1983a; o'flaherty, 1984), whenphysiologists assess the selection bias in the individuals who volunteer for theirexperiments (hackney andappendix c240improving risk communicationcopyright national academy of sciences. all rights reserved.linn, 1984; rosenthal and rosnow, 1969), when geologists consider how theconstruction of underground storage facilities might change the structure of therock media and the flow of fluids through them (sioshansi, 1983; travis, 1984),and when psychologists wonder how the dynamics of a particular group ofinteracting experts affect the distribution of their responses (brown, 1965; davis,1969; hirokawa and poole, 1986).the process by which judgments are produced may be as varied as the topicsthey treat. individual scientists may probe their own experience for clues to themissing facts. reviewers may be sponsored to derive the best conclusions that theliterature can provide. panels of specialists may be convened to produce acollective best guess. trained interviewers may use structured elicitationtechniques to extract knowledge from others. the experts producing thesejudgments may be substantive experts in almost any area of science andengineering, risk assessment generalists who take it upon themselves toextrapolate from others' work, or laypeople who happen to know more thananyone else about particular facts (e.g., workers assessing how respirators arereally used, civil defense officials predicting how evacuation plans will work).few experts would deny that they do not know all the answers. however,detailed treatments of the judgments they make in the absence of firm evidenceare seldom forthcoming (federal register 49(100):21594œ21661). there appearto be several possible causes for this neglect. knowing which is at work in aparticular risk assessment establishes what effect, if any, the informal treatmentof judgment has had.one common reason for treating the role of judgment lightly is the feelingthat everyone knows that it is there, hence there is no point in repeating theobvious. although this feeling is often justified, acting on it can have twodeleterious consequences. one is that all consumers of an assessment may notshare the same feeling. some of these consumers may not realize that judgment isinvolved, whereas others may suspect that the judgments are being hidden forsome ulterior purpose. the second problem is that failure to take this stepprecludes taking the subsequent steps of characterizing, improving, andevaluating the judgments involved.a second, complementary reason for doing little about judgment is thebelief that nothing much can be done, beyond a goodfaith effort to think as hardas one can. considering the cursory treatment of judgmental issues in mostmethodological primers for riskappendix c241improving risk communicationcopyright national academy of sciences. all rights reserved.analysts, this perception is understandable. considering the importance of doingsomething and the extensive research regarding what can be done, it is, however,not justifiable. although the research is unfamiliar to most practicing analysts, thestudy and cultivation of judgment have proven tractable. the vulnerability ofanalyses to judgmental difficulties means that those who ignore judgment for thisreason may miss a significant opportunity to perform at the state of the art.a third reason for ignoring judgment is being rewarded for doing so. attimes, analysts discern some strategic advantage to exaggerating thedefinitiveness of their work. at times, analysts feel that they must make abegrudging concession to the demands of political processes that attend only tothose who speak with (unjustifiable) authority. at times, the neglect of judgmentis (almost) a condition of employment, as when employers, hearings officials, orcontracting agencies require statements of fact, not opinion.diagnosing the role of judgmentthe first step in dealing with the judgmental aspects of risk assessments isidentifying them. all risk assessment, and most contemporary science, can beconstrued as the construction of models. these include both procedures used toassess discrete hazards (e.g., accidents), such as probabilistic risk analysis, andprocedures used to assess continuous hazards (e.g., toxicity), such as doseresponse curves or structuralactivity relationships. although these models takemany forms, all require a similar set of judgmental skills, which can be used as aframework for diagnosing where judgment enters into analyses (and,subsequently, how good it is and what can be done about it). these skills are:1. identifying the active elements of the hazardous system being studied. thesemay be the physical components of a nuclear power plant (e.g., the valves,controls, and piping) (u.s. nuclear regulatory commission, 1983), theenvironmental factors affecting the dispersal of toxins from a wastedisposal site (e.g., geologic structure, rainfall patterns, adjacentconstruction) (pinder, 1984), or the potential predictors of cancer in anepidemiological study (tockman and lilienfeld, 1984).2. characterizing the interrelationships among these elements. not everythingis connected to everything else. reducing the set of interconnectionsrenders the model more tractable, its resultsappendix c242improving risk communicationcopyright national academy of sciences. all rights reserved.more comprehensible, and its data demands more manageable. theprobabilistic risk analyst must judge which malfunctions in system x needto be considered when studying the performance of system y. theepidemiologist needs to judge which interaction terms to include inregression models.3. assessing the value of model parameters. the amount of this kind ofjudgment varies greatly both across and within analyses. some values havea sound statistical base (e.g., the number of chemical workers, as revealedby a decennial census), whereas others must be created from whole cloth(e.g., the sabotage rate at an asyetunconstructed plant 10 years in thefuture). yet even the firmest statistics require some interpretation, forexample, to correct for sampling and reporting biases or to adjust forsubsequent changes in conditions.4. evaluating the quality of the analysis. every analysis requires somesummary statement of how good it is, whether for communicating itsresults to policymakers or for deciding whether to work on it more. suchevaluation requires consideration of both the substance and the purpose ofthe analysis. in both basic and applied sciences, the answer to ﬁis theassessment good enough?ﬂ presupposes an answer to ﬁgood enough forwhat?ﬂ5. adopting appropriate judgmental techniques. just as each stage in riskassessment requires different judgmental skills, it also requires differentelicitation procedures. the reason for this is that each kind of information isorganized in people's minds in a different way, and needs, therefore, to beextracted in a different way. for example, listing all possible mistakes thatoperators of a processcontrol industry might make is different thanestimating how frequently each mistake will be made. the former requiresheavy reliance on memory for instances of past errors, whereas the latterrequires aggregation across diverse experiences and their extrapolation tofuture situations. different experts (e.g., veteran operators, human factorstheorists) may be more accustomed to thinking about the topic in one wayrather than the other. although transfer of information between thesemodes of thinking is possible, it may be far from trivial (lachman et al.,1979; tulving, 1972).as noted earlier, studies with laypeople have found that seemingly subtlevariations in how judgments are elicited can have large effects on the beliefs thatare apparently revealed. these effects are most pronounced when people are leastcertain about how to respond, either because they do not know the answers orbecause theyappendix c243improving risk communicationcopyright national academy of sciences. all rights reserved.are unaccustomed to expressing themselves in the required terms. thus, inextrapolating these results one must ask how expert the respondents are both inthe topic requiring judgment and in using that response mode.assessing the quality of the judgmentif analysts have addressed the preceding steps conscientiously and left anaudit trail of their work, all that remains is to review the protocol of the analysisto determine how heavily its conclusions depend on judgment and how adequatethose judgments are likely to be. that evaluation should consider both theelicitation methods used and the judgmental capabilities of the experts. ideally,the methods would have been empirically tested to show that they are: (1)compatible with the experts' mental representation of the problem, and (2) able tohelp the experts use their minds more effectively by overcoming commonjudgmental difficulties. ideally, the experts would not only be knowledgeableabout the topic, but also capable of translating that knowledge into the requiredjudgments. the surest guarantees of that capability are having been trained injudgment or having provided judgments in conditions conducive to skillacquisition (e.g., prompt feedback).how good are expert judgments?as one might expect, considerably more is known about the judgmentalprocesses of laypeople than about the judgmental processes of experts performingtasks in their areas of expertise. it is simply much easier to gain access tolaypeople and create tasks about everyday events. nonetheless, there are somestudies of experts per se. in addition, there is some basis in psychological theoryfor extrapolating from the behavior of laypeople to that of experts. what followsis a selection of the kinds of problems that any of us may encounter when goingbeyond the available data, and which must be considered when weighing theusefulness of analyses estimating risks and benefits.sensitivity to sample sizetversky and kahneman (1971) found that even statistically sophisticatedindividuals have poor intuitions about the size of sampleappendix c244improving risk communicationcopyright national academy of sciences. all rights reserved.needed to test research hypotheses adequately. in particular, they expect smallsamples to represent the populations from which they were drawn to a degreethat can only be assumed with much larger samples. this tendency leads them togamble their research hypotheses on underpowered small samples, to place undueconfidence in early data trends, and to underestimate the role of samplingvariability in causing results to deviate from expectations (preferring instead tooffer causal explanations for discrepancies). for example, in a survey of standardhematology texts, berkson et al. (1939œ1940) found that the maximum allowabledifference between two successive blood counts was so small that it wouldnormally be exceeded by chance 66 to 85 percent of the time. they mused aboutwhy instructors often reported that their best students had the most troubleattaining the desired standard.small samples mean low statistical power, that is, a small chance ofdetecting phenomena that really exist. cohen (1962) surveyed published articlesin a respected psychological journal and found very low power. even under thecharitable assumption that all underlying effects were large, a quarter of thestudies had less than three chances in four of showing statistically significantresults. he goes on to speculate that the one way to get a lowpower studypublished is to keep doing it again and again (perhaps making subtle variationsdesigned to ﬁget it right next timeﬂ) until a significant result occurs.consequently, published studies may be unrepresentative of the set of conductedstudies in a way that inflates the rate of spuriously significant results (beyond thatimplied by the officially reported ﬁsignificance levelﬂ). page (1981) has similarlyshown the low power of representative toxicological studies. in designing suchstudies, one inevitably must make a tradeoff between avoiding false alarms(e.g., erroneously calling a chemical a carcinogen) and misses (e.g., erroneouslycalling a chemical a noncarcinogen). low power increases the miss rate anddecreases the false alarm rate. hence, wayward intuitions may lead toexperimental designs that represent, perhaps inadvertently, a social policy thatprotects chemicals more than people.hindsightexperimental work has shown that in hindsight people consistentlyexaggerate what could have been anticipated in foresight.appendix c245improving risk communicationcopyright national academy of sciences. all rights reserved.they tend not only to view what has happened as having been relativelyinevitable, but also to view it as having appeared relatively inevitable before ithappened. people believe that others should have been able to anticipate eventsmuch better than was actually the case. they even misremember their ownpredictions so as to exaggerate in hindsight what they knew in foresight(fischhoff, 1980).the revisionist history of strategic surprises (e.g., lanir, 1982; wohlstetter,1962) argues that such misperceptions have vitiated the efforts of scholars andﬁscalpersﬂ attempting to understand questions like, ﬁwho goofed at pearlharbor?ﬂ these expert scrutinizers were not able to disregard the knowledge thatthey had only as a result of knowing how things turned out. although it isflattering to believe that we personally would not have been surprised, failing torealize the difficulty of the task that faced the individuals about whom we arespeculating may leave us very exposed to future surprises.methodological treatises for professional historians contain numerouswarnings about related tendencies. one such tendency is telescoping the rate ofhistorical processes, exaggerating the speed with which ﬁinevitableﬂ changes areconsummated (fischer, 1970). mass immunization against poliomyelitis seemslike such a natural idea that careful research is needed to show that its adoptionmet substantial snags, taking almost a decade to complete (lawless, 1977). asecond variant of hindsight bias may be seen in barraclough's (1972) critique ofthe historiography of the ideological roots of nazism; looking back from thethird reich, one can trace its roots to the writings of many authors from whosewritings one could not have projected nazism. a third form of hindsight bias,also called ﬁpresentism,ﬂ is to imagine that the participants in a historical situationwere fully aware of its eventual importance [ﬁdear diary, the hundred years'war started todayﬂ (fischer, 1970)].more directly relevant to the resolution of scientific disputes, lakatos(1970) has argued that the ﬁcritical experiment,ﬂ unequivocally resolving theconflict between two theories or establishing the validity of one, is typically anartifact of inappropriate reconstruction. in fact, ﬁthe crucial experiment is seen ascrucial only decades later. theories don't just give up, a few anomalies are alwaysallowed. indeed, it is very difficult to defeat a research programme supported bytalented and imaginative scientistsﬂ (lakatos, 1970:157œ158).future generations may be puzzled by the persistence of the antinuclearmovement after the 1973 arab oil embargo guaranteed the future of nuclearpower, or the persistence of nuclear advocatesappendix c246improving risk communicationcopyright national academy of sciences. all rights reserved.after three mile island sealed the industry's fatešdepending on how things turnout. perhaps the best way to protect ourselves from the surprises and reprobationof the future in managing hazards is to ﬁaccept the fact of uncertainty and learn tolive with it. since no magic will provide certainty, our plans must work withoutitﬂ (wohlstetter, 1962:401).judging probabilistic processesafter seeing four successive heads in flips of a fair coin, most people expect atails. once diagnosed, this tendency is readily interpreted as a judgmental error.commonly labeled the ﬁgambler's fallacyﬂ (lindman and edwards, 1961), it isone reflection of a strong psychological tendency to impose order on the resultsof random processes, making them appear interpretable and predictable(kahneman and tversky, 1972). such illusions need not disappear with higherstakes or greater attention to detail. feller (1968) offers one example in riskmonitoring: londoners during the blitz devoted considerable effort to interpretingthe pattern of german bombing, developing elaborate theories of where thegermans were aiming (and when to take cover). however, a careful statisticalanalysis revealed that the frequency distribution of bombhits in different sectionsof london was almost a perfect approximation of the poisson (random)distribution. dreman (1979) argues that the technical analysis of stock prices bymarket experts represents little more than opportunistic explication of chancefluctuations. although such predictions generate an aura of knowing, they fail tooutperform market averages.gilovich et al. (1985) found that, appearances to the contrary, basketballplayers have no more shooting streaks than one might expect from a randomprocess generated by their overall shooting percentage. this result runs stronglycounter to the conventional wisdom that players periodically have a ﬁhot hand,ﬂattributable to specific causes like a halftime talk or dedication to an injuredteammate. one of the few basketball experts to accept this result claimed that hecould not act on it anyway. fans would not forgive him if, in the closing minutesof a game, he had an inbound pass directed to a higher percentage shooter, ratherthan to a player with an apparent ﬁhot handﬂ (even knowing that opposing playerswould cluster on that player, expecting the pass).at times, even scientific enterprises seem to represent little moreappendix c247improving risk communicationcopyright national academy of sciences. all rights reserved.than sophisticated capitalization on chance. chapman and chapman (1969) foundthat clinical psychologists see patterns that they expect to find even in randomlygenerated data. o'leary et al. (1974) observed that the theories of foreign affairsanalysts are so complicated that any imaginable set of data can be interpreted asbeing consistent with them. short of this extreme, it is generally true that, given aset of events (e.g., environmental calamities) and a sufficiently large set ofpossible explanatory variables (antecedent conditions), one can always devise atheory for retrospectively predicting the events to any desired level ofproficiency. the price one pays for such overfitting is shrinkage, failure of thetheory to work on a new sample of cases. the frequency and vehemence ofwarnings against such correlational overkill suggest that this bias is quiteresistant to even extended professional training (armstrong, 1975; campbell,1975; crask and parreault, 1977; kunce et al., 1975).even when one is alert to such problems, it may be difficult to assess thedegree to which one has capitalized on chance. for example, as a toxicologist,you are ﬁcertainﬂ that exposure to chemical x is bad for one's health, so youcompare workers who do and do not work with it in a particular plant for bladdercancer, but obtain no effect. so you try intestinal cancer, emphysema, dizziness,and so on, until you finally get a significant difference in skin cancer. is thatdifference meaningful? of course, the way to test these explanations or theories isby replication on new samples. that step, unfortunately, is seldom taken and isoften not possible for technical or ethical reasons (tukey, 1977).a further unintuitive property of probabilistic events is regression to themean, the tendency for extreme observations to be followed by less extremeones. one depressing failure by experts to appreciate this fact is seen incampbell and erlebacher's (1970) article, ﬁhow regression artifacts in quasiexperimental evaluations can mistakenly make compensatory education lookharmfulﬂ (because upon retest, the performance of the better students seems tohave deteriorated). similarly unfair tests may be created when one asks only ifenvironmental management programs have, say, weakened strong industries orreduced productivity in the healthiest sectors of the economy.appendix c248improving risk communicationcopyright national academy of sciences. all rights reserved.judging the quality of evidencesince cognitive and evidential limits prevent scientists from providing all theanswers, it is important to have an appraisal of how much they do know. it is notenough to claim that ﬁthese are the ranking experts in the field,ﬂ for there aresome fields in which the most knowledgeable individuals understand a relativelysmall portion of all there is to be known.weather forecasters offer some reason for encouragement (murphy andbrown, 1983; murphy and winkler, 1984). there is at least some measurableprecipitation on about 70 percent of the occasions for which they say there is a 70percent chance of rain. the conditions under which forecasters work and trainsuggest the following prerequisites for good performance in probabilisticjudgment: great amounts of practice; the availability of statistical data offering historical precipitation base rates(indeed, forecasters might be fairly well calibrated if they ignored themurmurings of their intuitions and always responded with the base rate); computergenerated predictions for each situation; a readily verifiable criterion event (measurable precipitation), offering clearfeedback; and explicit admission of the imprecision of the trade and the need for training.in experimental work, it has been found that large amounts of clearlycharacterized, accurate, and personalized feedback can improve the probabilityassessments of laypeople (e.g., lichtenstein and fischhoff, 1980).training professionals to assess and express their uncertainty is, however, ararity. indeed, the role of judgment is often acknowledged only obliquely. forexample, civil engineers do not routinely assess the probability of failure forcompleted dams, even though approximately one dam in 300 collapses when firstfilled (u.s. committee on government operations, 1978). the ﬁrasmussenﬂreactor safety study (u.s. nuclear regulatory commission, 1975) was animportant step toward formalizing the role of risk in technological systems,although a subsequent review was needed to clarify the extent to which theseestimates were but the product of fallible, educated judgment (u.s. nuclearregulatory commission, 1978).ultimately, the quality of experts' assessments is a matter ofappendix c249improving risk communicationcopyright national academy of sciences. all rights reserved.judgment. since expertise is so narrowly distributed, assessors are typically calledupon to judge the quality of their own judgments. unfortunately, an extensivebody of research suggests that people are overconfident when making suchassessments (lichtenstein et al., 1982). a major source of such overconfidenceseems to be failure to appreciate the nature and tenuousness of the assumptions onwhich judgments are based. to illustrate with a trivial example, when asked ﬁtowhich country are potatoes native? (a) ireland (b) peru?ﬂ, many people are veryconfident that answer (a) is true. the irish potato and potato blight are familiar tomost people; however, that is no guarantee of origin. indeed, the fact thatpotatoes were not indigenous to ireland may have increased their susceptibility toblight there.experts may be as prone to overconfidence as laypeople (in cases in whichthey, too, are pressed to evaluate judgments made regarding topics about whichtheir knowledge is limited). for example, when several internationally knowngeotechnical experts were asked to predict the height of fill at which anembankment would fail and to give confidence intervals for their estimates,without exception, the true values fell outside the confidence intervals (hynes andvanmarcke, 1976), a result akin to that observed with other tasks and respondentpopulations (lichtenstein et al., 1982). one of the intellectual challenges facingengineering is to systematize the role of judgment, both to improve its quality andto inform those who must rely on it in their decision making.this basic pattern of results has proved so robust that it is hard to acquiremuch insight into the psychological processes producing it (lichtenstein et al.,1982). one of the few effective manipulations is to force subjects to explain whytheir chosen answers might be wrong (koriat et al., 1980). that simple instructionseems to prompt recall of contrary reasons that would not normally come to mindgiven people's natural thought processes, which seem to focus on retrievingreasons that support chosen answers. a second seemingly effective manipulation,mentioned earlier, is to train people intensively with personalized feedback thatshows them how well they are calibrated.figures ii.9 and ii.10 show one sign of the limits that exist on the capacityof expertise and experience to improve judgmentšin the absence of theconditions for learning enjoyed, say, by weather forecasters. particle physicists'estimates of the value of several physical constants are bracketed by what mightbe called confidence intervals, showing the range of likely values within whichthe trueappendix c250improving risk communicationcopyright national academy of sciences. all rights reserved.value should fall, once it is known. narrower intervals indicate greaterconfidence. these intervals have shrunk over time, as physicists' knowledge hasincreased. however, at most points, they seem to have been too narrow.otherwise, the new best estimates would not have fallen so frequently outside therange of what previously seemed plausible. in an absolute sense, the level ofknowledge represented here is extremely high and the successive best estimateslie extremely close to one another. however, the confidence intervals define whatconstitute surprises in terms of current physical theory. unless thefigure ii.9 calibration of confidence in estimates of physical constants.source: henrion and fischhoff, 1986. copyright © 1986 by the americanassociation of physics teachers.appendix c251improving risk communicationcopyright national academy of sciences. all rights reserved.figure ii.10 recommended values for fundamental constants, 1952 through1973. source: henrion and fischhoff, 1986. copyright © 1986 by theamerican association of physics teachers.appendix c252improving risk communicationcopyright national academy of sciences. all rights reserved.possibility of overconfident judgment is considered, values falling outsidethe intervals suggest a weakness in theory.summarythe science of risk provides a critical anchor for risk controversies. there isno substitute for that science. however, it is typically an imperfect guide. it canmislead if one violates any of a wide variety of intricate methodologicalrequirementsšincluding the need to use judgment judiciously (and to understandits limitations). the general nature of these assumptions was illustrated withexamples drawn from the science of understanding human behavior. sections ivthrough vi deal with the human anchors for risk controversies: the nature of theirpolitical tensions, the strategies that risk communicators can take in them, andpsychological barriers to risk communication. the next section (iii) deals withthe interface between science and behavior, specifically ways in which scienceshapes and is shaped by the political process.appendix c253improving risk communicationcopyright national academy of sciences. all rights reserved.iii science and policyseparating facts and valuesthe first recommendation of the national research council's committee onthe institutional means for assessment of risks to public health (nationalresearch council, 1983b:7) was that:regulatory agencies take steps to establish and maintain a clear conceptualdistinction between assessment of risks and considerations of risk managementalternatives; that is, the scientific findings and policy judgments embodied inrisk assessments should be explicitly distinguished from the political, economic,and technical considerations that influence the design and choice of regulatorystrategies.the principle of separating science and politics seems to be a cornerstone ofprofessional risk management. many of the antagonisms surrounding riskmanagement seem due to the blurring of this distinction, resulting in situations inwhich science is rejected because it is seen as tainted by politics. as hammondand adelman (1976), mazur et al. (1979), and others have argued, this distinctioncan help clear the air in debates about risk, which might otherwise fill up withhalftruths, loaded language, and character assassinations. even technical expertsmay fall prey to partisanship as they advance views on political topics beyondtheir fields of expertise, downplay facts they believe will worry the public, ormake statements that cannot be verified.although a careful delineation between values and facts can help preventvalues from hiding in facts' clothing, it cannot assure that a complete separationwill ever be possible (bazelon, 1979; callen, 1976). the ﬁfactsﬂ of a matter areonly those deemed relevant to a particular problem, whose definition foreclosessome action options and effectively prejudges others. deciding what the problemis goes a long way to determining what the answer will be. hence, theﬁobjectivityﬂ of the facts is always conditioned on the assumption that they areaddressing the ﬁrightﬂ problem, where ﬁrightﬂ is defined in terms of society's bestinterest, not the interest of a particular party. the remainder of this sectionexamines how our values determine what facts we produce and use, and how ourfacts shape our values.appendix c254improving risk communicationcopyright national academy of sciences. all rights reserved.values shape factswithout information, it may be hard to arouse concern about an issue, toallay fears, or to justify an action. but information is usually created only ifsomeone has a use for it. that use may be pecuniary, scientific, or political.thus, we may know something only if someone in a position to decide feels thatit is worth knowing. doern (1978) proposed that lack of interest in the fate ofworkers was responsible for the lack of research on the risks of uranium mining;neyman (1979) wondered whether the special concern with radiation hazards hadrestricted the study of chemical carcinogens; commoner (1979) accused oilinterests of preventing the research that could establish solar power as an energyoption. in some situations, knowledge is so specialized that all relevant expertsmay be in the employ of a technology's promoters, leaving no one competent todiscover troublesome facts (gamble, 1978). conversely, if one looks hard enoughfor, say, adverse effects of a chemical, chance alone will produce an occasionalpositive finding. although such spurious results are likely to vanish when studiesare replicated, replications are the exception rather than the rule in many areas.moreover, the concern raised by a faulty study may not be as readily erased frompeople's consciousness as from the scientific literature (holden, 1980; kolata,1980; peto, 1980). a shadow of doubt is hard to remove.legal requirements are an expression of society's values that may stronglyaffect its view of reality. highwaysafety legislation affects accident reports inways that are independent of its effects on accident rates (v.l.wilson, 1980).crimeprevention programs may have similar effects, inflating the perceivedproblem by encouraging victims to report crimes (national research council,1976). although it is not always exploited for research purposes, an enormouslegacy of medical tests has been created by the defensive medicine engendered byfear of malpractice. legal concerns may also lead to the suppression ofinformation, as doctors destroy ﬁoldﬂ records that implicate them in theadministration of diethylstilbestrol (des) to pregnant women in the 1950s,employers fail to keep ﬁunnecessaryﬂ records on occupational hazards, orinnovators protect proprietary information (lave, 1978; pearce, 1979;schneiderman, 1980).whereas individual scientists create data, it is the community of scientistsand other interpreters who create facts by integrating data (levine, 1974).survival in this adversarial context is determined in part by what is right (i.e.,truth) and in part by the staying power of those who collect particular data orwant to believe in them. scrutinyappendix c255improving risk communicationcopyright national academy of sciences. all rights reserved.from both sides in a dispute is a valuable safeguard, likely to improve the qualityof the analysis. each side tries to eliminate erroneous material prejudicial to itsposition. if only one side scrutinizes, the resulting analyses will be unbalanced.because staying with a problem requires resources, the winners in themarketplace of ideas may tend to be the winners in the political and economicmarketplace.facts shape valuesvalues are acquired by rote (e.g., in sunday school), by imitation, and byexperience (rokeach, 1973). the world we observe tells us what issues are worthworrying about, what desires are capable of fruition, and who we are in relation toour fellows. insofar as that world is revealed to us through the prism of science,the facts it creates help shape our world outlook (r.p.applebaum, 1977;henshel, 1975; markovic, 1970; shroyer, 1970). the content of science's factscan make us feel like hedonistic consumers wrestling with our fellows, likepassive servants of society's institutions, like beings at war with or at one withnature. the quantity of science's facts (and the coherence of their explication)may lower our selfesteem and enhance that of technical elites. the topics ofscience's inquiries may tell us that the important issues of life concern the masteryof others and of nature, or the building of humane relationships. some argue thatscience can ﬁanaesthetize moral feelingﬂ (tribe, 1972) by enticing us to thinkabout the unthinkable. for example, setting an explicit value on human life inorder to guide policy decisions may erode our social contract, even though we setsuch values implicitly by whatever decisions we make.even flawed science may shape our values. according to wortman (1975),westinghouse's poor evaluation of the head start program in the mid1960s had amajor corrosive effect on faith in social programs and liberal ideals. weaver(1979) argued that whatever technical problems may be found with inhaber's(1979) comparison of the risks of different energy sources, he succeeded increating a new perspective that was deleterious to the opponents of nuclearpower. as mentioned earlier, incorrect intuitions regarding the statistical powerof statistical designs can lead to research that implicitly values chemicals morethan people (page, 1978, 1981). in designing such studies, one must make atradeoff between avoiding either false alarms (e.g., erroneously calling achemical a carcinogen) or misses (e.g., not identifying a carcinogen as such). thedecision to studyappendix c256improving risk communicationcopyright national academy of sciences. all rights reserved.many chemicals with relatively small samples both increases the miss rate anddecreases the falsealarm rate. the value bias of such studies is compoundedwhen scientific caution also becomes regulatory caution.where science concerns realworld objects, then the selection andcharacterization of those objects inevitably express attitudes toward them. thoseattitudes may come from the risk managers who commission scientific studies, orthey may come from the scientists who conduct them. in either case, the deepestlink between science and politics may be in basic issues of definition. the nextsection discusses some of the subtle ways in which science can preempt or becaptured by the policymaking process in its treatment of two basic concepts ofrisk management: risk and benefit.measuring riskwhich hazards are being considered?the decision to decide whether a technology's risks are acceptable impliesthat, in the opinion of someone who matters, it may be too dangerous. such issueidentification is itself an action with potentially important consequences. putting atechnology on the decisionmaking agenda can materially change its fate byattracting attention to it and encouraging the neglect of other hazards. forexample, concern about carbondioxideinduced climatic change (schneider andmesirow, 1976) changes the status of fossil fuels visàvis nuclear power.after an issue has been identified, the hazard in question must still bedefined. breadth of definition is particularly important. are military andnonmilitary nuclear wastes to be lumped together in one broad category, or dothey constitute separate hazards? did the collision of two jumbo jets at tenerifein the canary islands represent a unique miscommunication or a large class ofpilotcontroller impediments? do all uses of asbestos make up a single industryor are brake linings, insulation, and so forth to be treated separately? dohazardous wastes include residential sewage or only industrial solids (chemicaland engineering news, 1980)? grouping may convert a set of minor hazards into amajor societal problem, or vice versa. lead in the environment may seem worthworrying about, but lead solder in tuna fish cans may not. in recent years, isolatedcases of child abuse have been aggregated in such a way that a persistentappendix c257improving risk communicationcopyright national academy of sciences. all rights reserved.problem with a relatively stable rate of occurrence now appears as an epidemicdemanding action.often the breadth of a hazard category becomes apparent only after thedecision has been made and its implications experienced in practice. somecategories are broadened, for example, when precedentsetting decisions areapplied to previously unrelated hazards. other categories are narrowed over timeas vested interests gain exceptions to the rules applying to the category in whichtheir technology once belonged (barber, 1979). in either case, different decisionsmight have been made had the hazard category been better defined in advance.definition of riskmanaging technological risks has become a major topic in scientific,industrial, and public policy. it has spurred the development of some industriesand prompted the demise of others. it has expanded the powers of some agenciesand overwhelmed the capacity of others. it has enhanced the growth of somedisciplines and changed the paths of others. it has generated political campaignsand countercampaigns. the focal ingredient in all this has been concern overrisk. yet, the meaning of ﬁriskﬂ has always been fraught with confusion andcontroversy. some of this conflict has been overt, as when a professional bodyargues about the proper measure of pollution or reliability for incorporation in ahealth or safety standard. more often, though, the controversy is unrecognized;the term risk is used in a particular way without extensive deliberations regardingthe implications of alternative uses. typically, that particular way follows customin the scientific discipline initially concerned with the risk.however, the definition of risk, like that of any other key term in policyissues, is inherently controversial. the choice of definition can affect the outcomeof policy debates, the allocation of resources among safety measures, and thedistribution of political power in society.dimensionality of riskthe risks of a technology are seldom its only consequences. no one wouldproduce it if it did not generate some benefits for someone. no one could produceit without incurring some costs. the difference between these benefits andnonrisk costs could be calledappendix c258improving risk communicationcopyright national academy of sciences. all rights reserved.the technology's net benefit. in addition, risk itself is seldom just a singleconsequence. a technology may be capable of causing fatalities in several ways(e.g., by explosions and chronic toxicity), as well as inducing various forms ofmorbidity. it can affect plants and animals as well as humans. an analysis of riskneeds to specify which of these dimensions will be included. in general,definitions based on a single dimension will favor technologies that do their harmin a variety of ways (as opposed to those that create a lot of one kind ofproblem). although it represents particular values (and leads to decisionsconsonant with those values), the specification of dimensionality (like any otherspecification) is often the inadvertent product of convention or other forces, suchas jurisdictional boundaries (fischhoff, 1984).summary statisticsfor each dimension selected as relevant, some quantitative summary isneeded for expressing how much of that kind of risk is created by a technology.the controversial aspects of that choice can be seen by comparing the practicesof different scientists. for some, the unit of choice is the annual death toll (e.g.,zentner, 1979); for others, deaths per person exposed or per hour of exposure(e.g., starr, 1969); for others, it is the loss of life expectancy (e.g., cohen andlee, 1979; reissland and harries, 1979); for still others, lost working days (e.g.,inhaber, 1979). crouch and wilson (1982) have shown how the choice of unit canaffect the relative riskiness of technologies. for example, today's coal mines aremuch less risky than those of 30 years ago in terms of accidental deaths per tonof coal, but marginally riskier in terms of accidental deaths per employee. thedifference between measures is explained by increased productivity. the choiceamong measures is a policy question, with crouch and wilson suggesting that:from a national point of view, given that a certain amount of coal has to beobtained, deaths per million tons of coal is the more appropriate measure of risk,whereas from a labor leader's point of view, deaths per thousand personsemployed may be more relevant (1982:13).other value questions may be seen in the units themselves. for example,loss of life expectancy places a premium on early deaths that is absent frommeasures treating all deaths equally; using it means ascribing particular worth tothe lives of young people. justappendix c259improving risk communicationcopyright national academy of sciences. all rights reserved.counting fatalities expresses indifference to whether they come immediately aftermishaps or following a substantial latency period (during which it may not beclear who will die). whatever types of individuals are included in a category, theyare treated as equals; the categories may include beneficiaries andnonbeneficiaries of a technology (reflecting an attitude toward that kind ofequity), workers and members of the general public (reflecting an attitude towardthat kind of voluntariness), or participants and nonparticipants in setting policyfor the technology (reflecting an attitude toward that kind of voluntariness). usingthe average of past casualties or the expectation of future fatalities meansignoring the distribution of risk over time; it treats technologies taking a steadyannual toll in the same way as those that are typically benign, except for the rarecatastrophic accident. when averages are inadequate, a case might be made forusing one of the higher moments of the distribution of casualties over time or forincorporating a measure of the uncertainty surrounding estimates (fischhoff,1984).bounding the technologywillingness to count delayed fatalities means that a technology's effects arenot being bounded in time (as they are, for example, in some legal proceedingsthat consider the time that passes between cause, effect, discovery, andreporting). other bounds need to be set also, either implicitly or explicitly. one isthe proportion of the fuel and materials cycles to be considered: to what extentshould the risks be restricted to those people who enjoy the direct benefits of atechnology or extended to cover those involved in the full range of activitiesnecessary if those benefits are to be obtained? crouch and wilson (1982) offer aninsightful discussion of some of these issues in the context of imported steel; theu.s. nuclear regulatory commission (1983) has adopted a restrictive definitionin setting safety goals for nuclear power (fischhoff, 1983); much of the acrimonyin the debates over the risks of competing energy technologies concernedtreatment of the risks of backup energy sources (herbert et al., 1979; inhaber,1979). a second recurrent bounding problem is how far to go in consideringhigherorder consequences (i.e., when coping with one risk exposes people toanother). as shown in figure ii.1, hazards begin with the human need thetechnology is designed to satisfy, and develop over time. one can look at thewhole process or only at its conclusion. the more narrowly a hazard's moment intime isappendix c260improving risk communicationcopyright national academy of sciences. all rights reserved.defined, the fewer the options that can be considered for managing its risks. athird issue of limits is how to treat a technology's partial contribution toconsequences, for example, when it renders people susceptible to other problemsor when it accentuates other effects through synergistic processes.concernevents that threaten people's health and safety exact a toll even if they neverhappen. concerns over accidents, illness, and unemployment occupy people evenwhen they and their loved ones experience long, robust, and salaried lives.although associated with risks, these consequences are virtual certainties. allthose who know about them will respond to them in some way. in some cases,that response benefits the respondent, even if its source is an aversive event. forexample, financial worries may prompt people to expand their personal skills orcreate socially useful innovations. nonetheless, their resources have been divertedfrom other, perhaps preferred pursuits. moreover, the accompanying stress cancontribute to a variety of negative health effects, particularly when it is hard tocontrol the threat (elliot and eisdorfer, 1982). stress not only precipitatesproblems of its own, but can complicate other problems and divert thepsychological resources needed to cope with them. thus, concern about a riskmay hasten the end of a marriage by giving the couple one more thing to fightabout and that much less energy to look for solutions.hazardous technologies can evoke such concern even when they arefunctioning perfectly. some of the response may be focused and purposeful, suchas attempts to reduce the risk through personal and collective action. however,even that effort should be considered a cost of the technology because that timeand energy might have been invested in something else (e.g., leisure, financialplanning, improving professional skills) were it not for the technology. whenmany people are exposed to the risk (or are concerned about the exposure of theirfellows), then the costs may be extensive. concern may have even greater impactthan the actual health and safety effects of the technology. ironically, because thesigns of stress are diffuse (e.g., a few more divorces, somewhat aggravatedcardiovascular problems), it is quite possible for the size of the effects to be bothintolerably large (considering the benefits) and undetectable (by currenttechniques).including concern among the consequences of a risky technologyappendix c261improving risk communicationcopyright national academy of sciences. all rights reserved.immediately raises two additional controversial issues. one centers on whatconstitutes an appropriate level of concern. it could be argued that concern shouldbe proportionate to physical risk. there are, however, a variety of reasons whycitizens might reasonably be concerned most about hazards that they themselvesacknowledge to be relatively small (e.g., they feel that an important precedent isbeing set, that things will get worse if not checked, or that the chances foreffective action are great) (see section iv). the second issue is whether to hold atechnology responsible for the concern evoked by people's perceptions of its risksor for the concern that would be evoked were people to share the best availabletechnical knowledge. it is the former that determines actual concern; however,using it would mean penalizing some technologies for evoking unjustifiedconcerns and rewarding others for having escaped the public eye.measuring benefitsalthough the term risk management is commonly used for dealing withpotentially hazardous technologies, few risk policies are concerned entirely withrisk. technologies would not be tolerated if they did not bring some benefit.residual risk would not be tolerated if the benefits of additional reduction did notseem unduly expensive (to whoever is making the decision). as a result, someassessment of benefits is a part of all risk decisions, whether undertaken byinstitutions or by individuals. faith in quantification makes formal costbenefitanalysis a part of many governmental decisions in the united states (bentkoveret al., 1985). however, a variety of procedures are possible, each with its ownbehavioral and ethical assumptions.definition of benefitbenefit assessment begins with a series of decisions that bound the analysisand specify its key terms. together, these decisions provide an operationaldefinition of what ﬁbenefitﬂ means. although they may seem technical and areoften treated in passing, these decisions are the heart of an analysis. they express asocial philosophy, elaborating what society holds to be important in a particularcontext. the ensuing analysis is ﬁmerelyﬂ an exercise in determining how welldifferent policy options realize this philosophy. if the philosophy has not beeninterpreted, stated, and implemented appropriately, then the analysis becomes anexercise in futility.appendix c262improving risk communicationcopyright national academy of sciences. all rights reserved.the details of this definitional process in some ways parallel that fordefining risk. policymakers commission benefit assessments to help them makedecisions; that is, to help them choose among alternative courses of action(including, typically, inaction). to make those decisions, they must (1) identifythe policy alternatives (or options) that could be adopted; (2) circumscribe the setof policyrelevant consequences that these alternatives could create; (3) estimatethe magnitude of each alternative's consequences were it adopted; (4) evaluate thebenefits (and costs) that affected individuals would derive from theseconsequences; and (5) aggregate benefits across individuals. defining thepolicymaking question is a precondition for commissioning any benefitassessment meant to serve it. for example, one cannot calculate the consequencesof one particular policy without knowing the alternative policies that might comein its stead were it not adopted (and whose benefits would be foregone if it was).one cannot begin to assess and tally benefits without knowing whichconsequences and individuals fall within the agency's jurisdiction. figure iii.1provides a summary of these definitional issues. fischhoff and cox (1985)discuss them in greater detail.once it has been determined what evaluations to seek, a method must befound for doing the seeking. there are two natural places to look for guidanceregarding the evaluation of benefits: what people say and what people do.methods relying on the former consider expressed preferences; methods relyingon the latter consider revealed preferences. each makes certain ethical andempirical assumptions regarding the nature of individual and societal behavior,the validity of which determines their applicability to particular situations (driveret al., 1988).expressed preferencesthe most straightforward way to find out what people value, regardingsafety or anything else, is to ask them. the asking can be done at the level ofoverall assessments (e.g., ﬁdo you favor –?ﬂ), statements of principle (e.g.,ﬁshould our society be risk averse regarding–?ﬂ), or detailed tradeoffs (e.g.,ﬁhow much of a monetary sacrifice would you make in order to ensure–?ﬂ). thevehicle for collecting these values could be public opinion polls (conn, 1983),comments solicited at public hearings (mazur, 1973; nelkin, 1984), or detailedinterviews conducted by decision analysts or counselors (janis, 1982; keeney,1980). the advantages of theseappendix c263improving risk communicationcopyright national academy of sciences. all rights reserved.figure iii.1 steps in problem definition. source: fischhoff and cox, 1985.procedures are that they are current (in the sense of capturing today'svalues), sensitive (in the sense of theoretically allowing people to say whateverthey want), specifiable (in the sense of allowing one to ask the precise questionsthat interest policymakers), direct (in the sense of looking at the preferencesthemselves and not how they reveal themselves in application to some specificdecision problem), superficially simple (in the sense that you just ask peoplequestions), politically appealing (in the sense that they let ﬁthe peopleﬂ speak),and instructive (in the sense that they force people to think in a focused mannerabout topics that they might otherwise ignore).appendix c264improving risk communicationcopyright national academy of sciences. all rights reserved.as discussed in section ii, however, a number of difficult conditions mustbe met if expressed preference procedures are to fulfill their promise. one is thatthe question asked must be the precise one needed for policymaking (e.g., ﬁhowmuch should you be paid in order to incur a 10 percent increase in your annualprobability of an injury sufficiently severe to require at least one day ofhospitalization, but not involving permanent disability?ﬂ), rather than an illdefined one, such as ﬁdo you favor better roads?ﬂ or ﬁis your job too risky?ﬂ (inresponse, a thoughtful interviewee might ask, ﬁwhat alternatives should i beconsidering? am i allowed to consider who pays for improvements?ﬂ) oneresponse to the threat of ambiguity is to lay out all details of the evaluationquestion to respondents (fischhoff and furby, 1988). a threat to this solution isthat the full specification will be so complex and unfamiliar as to pose anoverwhelming inferential task. to avoid the incompletely considered, andpotentially labile, responses that might arise, one must either adjust the questionsto the respondents or the respondents to the questions. the former requires anempirically grounded understanding of what issues people have considered andhow they have thought about them. this understanding allows one to focus theinterview on the areas in which people have articulated beliefs, to provide neededelaborations, and to avoid repeating details that correspond to respondents'default assumptions (and could, therefore, go without saying).if the gap between policymakers' questions and respondents' answers is toogreat to be bridged in a standard interviewing session, then it may be necessaryeither to simplify the questions or to complicate the session. a structured form ofsimplification is offered by techniques, such as multiattribute utility theory,which decompose complex questions into more manageable components, each ofwhich considers a subsidiary evaluation issue (keeney and raiffa, 1976). thestructuring of these questions allows their recomposition into overall evaluations,which are interpreted as representing the summary judgments that respondentswould have produced if they had unlimited mental computational capacity. theprice paid for this potential simplification is the need to answer large numbers ofsimple, formal, and precise questions.where it becomes impossible to bring the question ﬁdownﬂ to the level ofthe respondent, there still may be some opportunity to bring the respondent ﬁupﬂto the level of the question. ways of enabling respondents to realize their latentcapability for thinking meaningfully about questions include talking with themabout theappendix c265improving risk communicationcopyright national academy of sciences. all rights reserved.issues, including them in focused group discussions, suggesting alternativeperspectives (for their consideration), and giving them time to ruminate over theiranswers.revealed preferencesthe alternative to words is action. this collection of techniques assumesthat people's overt actions can be interpreted to reveal the preferences thatmotivated them. the great attraction of such procedures is that they are based onreal acts, whose consequences are presumably weightier than those of even themost intelligently conducted interview. they focus on possibilities, rather thanjust desires.by concentrating on current, real decisions, these procedures are alsostrongly anchored in the status quo. it is today's work, with today's constraints,that conditions the behavior observed. if today's society inhibits people's ability toact in ways that express their fundamental values, then revealed preferenceprocedures lose their credibility (whereas expressed preferences, at least inprinciple, allow people to raise themselves above today's reality). thus, if onefeels that advertising, or regulation, or monopoly pressures have distortedcontemporary evaluations of some products or consequences, then revealingthose values does not yield a guide to true worth. relying on those values forpolicymaking would mean enshrining today's imperfections (and inequities) intomorrow's world.the commitment to observing actual behavior also makes these proceduresparticularly vulnerable to deviations from optimality. a much smaller set ofinferences separates people's true values from their expressed preferences thanfrom their overt behavior. on the one hand, this means that people must completean even more complex series of inferences in order to do what they want than tosay what they want. on the other hand, investigators must make even moreassumptions in order to infer underlying values from what they observe. thus,for example, it is difficult enough to determine how much compensation onewould demand to accept an additional injury risk of magnitude x in one's job.implementing that policy in an actual decision also requires that suitable optionsbe available and that their consequences be accurately perceived. if thoseconditions of informed consent are not met, then the interpretation of paydangerrelationships may be quite tenuous. workers may be coercing their employer intocompensating them for imagined risks;appendix c266improving risk communicationcopyright national academy of sciences. all rights reserved.or, they may be coerced into accepting minimal compensation by an employercognizant of a depressed job market.the most common kind of revealed preference analysis is also the mostcommon kind of economic analysis: interpreting marketplace prices as indicatingthe true value of goods. if the goods whose values are of interest (e.g., healthrisks) are not traded directly, then a value may be inferred by conceptualizing thegoods that are traded (e.g., jobs) as representing a bundle of consequences (e.g.,risks, wages, status). analytic techniques may then be used to discern the pricethat markets assign to each consequence individually, by looking at its role indetermining the price paid for various goods that include it.these regressionbased procedures rest on a welldeveloped theoreticalfoundation describing why (under conditions of a free market, optimal decisionmaking, and informed consent) prices should reveal the values that people ascribeto things (bentkover et al., 1985). the same general thought has been appliedheuristically in various schemes designed to discern the values revealed indecisions (ostensibly) taken by society as a whole or by individuals under lessconstrained conditions. these analyses include attempts to see what benefitssociety demands for tolerating the risks of different technologies (starr, 1969),what risks people seem to accept in their everyday lives (b.cohen and lee, 1979;r.wilson, 1979), and what levels of technological risk escape further regulation(fischhoff, 1983; u.s. nuclear regulatory commission, 1982). these attemptsare typically quite ad hoc, with no detailed methodology specifying how theyshould be conducted. the implicit underlying theory assumes, in effect, thatwhatever is, is right and that present arrangements are an appropriate basis forfuture policies. thus, these procedures can guide future decisions only if onebelieves that society as a whole currently gets what it wants, even with regard toregulated industries, unregulated semimonopolies, and poorly understood newtechnologies. extracting useful information from them requires a very detailedassessment of the procedures that they use, the existing reality that they endorse,and the kinds of behavior that they study.ascertaining the validity of the theory underlying approaches to measuringﬁbenefitﬂ that assume optimality has often proven difficult, for what can best bedescribed as philosophical reasons. some investigators find it implausible thatpeople do anything other than optimize their own best interest when makingdecisions, maintainingappendix c267improving risk communicationcopyright national academy of sciences. all rights reserved.that society would not be functioning so well were it not for this ability. theseinvestigators see their role as discerning what people are trying to optimize (i.e.,what values they ascribe to various consequences).the contrary position argues that this belief in optimality is tautological, inthat one can always find something that people could be construed as trying tooptimize. looking at how decisions are actually made shows that they arethreatened by all the problems that can afflict expressed preferences. thus, forexample, consumers may make suboptimal choices because a good is marketed ina way that evokes only a portion of their values, or because they unwittinglyexaggerate their ability to control its risks (svenson, 1981; weinstein, 1980a).because of the philosophical differences between these positions, relativelylittle is known about the general sensitivity of conclusions drawn from analysesthat assume optimality to deviations from optimality. the consumer of suchanalyses is left to discern how far conditions deviate from optimal decisionmaking by informed individuals in an unconstrained marketplace and, then, howfar those deviations threaten the conclusions of the analyses.summaryscience is a product of society; as such, it reflects the values of its creators.that reflection may be deliberate, as when young people decide how to dedicatetheir lives and research institutes decide how to stay solvent. or, it may beunconscious, as scientists routinely apply valueladen procedures and definitionsjust because that was what they learned to do in school. conversely, society ispartly a product of science. that influence may be direct, as when science shapesthe conditions under which people live (e.g., how prosperous they are, whatindustries confront them). or it may be indirect, as when science defines ourrelationship with nature or raises specific fears. understanding theseinterdependencies is essential to, on the one hand, discerning the objectivecontent versus inherently subjective science and, on the other hand, directingscience to serve socially desired ends. an understanding of these relationships isalso necessary to appropriately interpret the conflicts between lay and expertopinions that constitute the visible core of many risk controversies. thediagnoses of these conflicts are discussed in section iv.appendix c268improving risk communicationcopyright national academy of sciences. all rights reserved.iv the nature of the controversya public opinion survey (harris, 1980) reported the following three results:1. among four ﬁleadership groupsﬂ (top corporate executives, investors andlenders, congressional representatives, and federal regulators), 94 to 98percent of all respondents agreed with the statement ﬁeven in areas in whichthe actual level of risk may have decreased in the past 20 years, our societyis significantly more aware of risk.ﬂ2. between 87 and 91 percent of those four leadership groups felt that ﬁthemood of the country regarding riskﬂ will have a substantial or moderateimpact ﬁon investment decisionsšthat is, the allocation of capital in oursociety in the decade ahead.ﬂ (the remainder believed that it would have aminimal impact, no impact at all, or were not sure.)3. no such consensus was found, however, when these groups were askedabout the appropriateness of this concern about risk. a majority of the topcorporate executives and a plurality of lenders believed that ﬁamericansociety is overly sensitive to risk,ﬂ whereas a large majority ofcongressional representatives and federal regulators believed that ﬁwe arebecoming more aware of risk and taking realistic precautions.ﬂ a sample ofthe public endorsed the latter statement over the former by 78 to 15percent.in summary, there is great agreement that risk decisions will have a majorrole in shaping our society's future and that those decisions will, in turn, beshaped by public perceptions of risk. there is, however, much disagreementabout the appropriateness of those perceptions. some believe the public to bewise; others do not. these contrary beliefs imply rather different roles for publicinvolvement in risk management. as a result, the way in which this disagreementis resolved will affect not only the fate of particular technologies, but also the fateof our society and its social organization.to that end, various investigators have been studying how and how wellpeople think about risks. although the results of that research are not definitive asyet, they do clearly indicate that a careful diagnosis is needed whenever thepublic and the experts appear to disagree. it is seldom adequate to attribute allsuch discrepancies toappendix c269improving risk communicationcopyright national academy of sciences. all rights reserved.public misperceptions of the science involved. from a factual perspective, thatassumption is often wrong; from a societal perspective, it is generally corrosiveby encouraging disrespect among the parties involved. when the availableresearch data do not allow one to make a confident alternative diagnosis, asounder assumption is that there is some method in the other party's apparentmadness. this section offers some ways to find that method. specifically, itoffers six reasons why disagreements between the public and the experts neednot be interpreted merely as clashes between actual and perceived risks.the distinction between ﬁactualﬂ and ﬁperceivedﬂ risksis misconceivedalthough there are actual risks, nobody knows what they are. all thatanyone does know about risks can be classified as perceptions. those assertionsthat are typically called actual risks (or facts or objective information) inevitablycontain some element of judgment on the part of the scientists who producethem. in this light, what is commonly called the conflict between actual andperceived risk is better thought of as the conflict between two sets of riskperceptions: those of ranking scientists performing within their field of expertiseand those of anybody else. the element of judgment is most minimal when all theexperts do is to assess the competence of a particular study conducted within anestablished paradigm. it grows with the degree to which experts must integrateresults from diverse studies or extrapolate from a domain in which results arereadily obtainable to another in which they are really needed (e.g., from animalstudies to human effects). judgment becomes all when there are no (credible)available data, yet a policy decision requires some assessment of a particular fact.section ii discusses at length the trustworthiness of such judgments.the expert opinions that make up the scientific literature aspire to beobjective in two senses, neither of which can ever be achieved absolutely andneither of which is the exclusive province of technical experts. one meaning ofobjectivity is reproducibility: one expert should be able to repeat another's study,review another's protocol, reanalyze another's data, or recap another's literaturesummary and reach the same conclusions about the size of an effect. clearly, asthe role of judgment increases in any of these operations, the results becomeincreasingly subjective. typically, reproducibility should decrease (andsubjectivity increase) to the extent that a problemappendix c270improving risk communicationcopyright national academy of sciences. all rights reserved.attracts scientists with diverse training or falls into a field that has yet to reachconsensus on basic issues of methodology.the second sense of objectivity means immune to the influence by valueconsiderations. one's interpretations of data should not be biased by one'spolitical views or pecuniary interests. applied sciences naturally have developedgreat sensitivity to such problems and are able to invoke some penalties fordetected violations. there is, however, little possibility of regulating the ways inwhich values influence other acts, such as one's choice of topics to study orignore. some of these choices might be socially sanctioned, in the sense that one'svalues are widely shared (e.g., deciding to study cancer because it is an importantproblem); other choices might be more personal (e.g., not studying an issuebecause one's employer does not wish to have troublesome data created on thattopic). although a commitment to separating issues of fact from issues of value is afundamental aspect of intellectual hygiene, a complete separation is neverpossible (see section iii).at times, this separation is not even desiredšas when experts offer theirviews on how risks should be managed. because they mix questions of fact andvalue, such views might be better thought of as the opinions of experts rather thanas expert opinions, a term that should be reserved for expressions of substantiveexpertise. it would seem as though members of the public are the experts when itcomes to striking the appropriate tradeoffs between costs, risks, and benefits.that expertise is best tapped by surveys, hearings, and political campaigns.of course, there is no allpurpose public any more than there are allpurposeexperts. the ideal expert on a matter of fact has studied that particular issue andis capable of rendering a properly qualified opinion in a form useful to decisionmakers. using the same criteria for selecting value experts might lead one tophilosophers, politicians, psychologists, sociologists, clergy, intervenors, pundits,shareholders, or wellselected bystanders. thus, one might ask, ﬁin what sense,ﬂwhenever someone says ﬁexpertﬂ or ﬁpublicﬂ (schnaiburg, 1980; thompson,1980). this appendix uses ﬁexpertﬂ in the restrictive sense and ﬁpublicﬂ orﬁlaypeopleﬂ to refer to everyone else, including scientists in their private lives.appendix c271improving risk communicationcopyright national academy of sciences. all rights reserved.laypeople and experts are speaking differentlanguagesexplicit risk analyses are a fairly new addition to the repertoire ofintellectual enterprises. as a result, risk experts are only beginning to reachconsensus on basic issues of terminology and methodology, such as how to definerisk (see section iii). their communications to the public reflect this instability.they are only beginning to express a sufficiently coherent perspective to help thepublic sort out the variety of meanings that ﬁriskﬂ could have. under thesecircumstances some miscommunication may be inevitable. studies (slovic et al.,1979, 1980) have found that when expert risk assessors are asked to assess therisk of a technology on an undefined scale, they tend to respond with numbersthat approximate the number of recorded or estimated fatalities in a typical year.when asked to estimate average year fatalities, laypeople produce fairly similarnumbers. when asked to assess risk, however, laypeople produce quite differentresponses. these estimates seem to be an amalgam of their averageyear fatalityjudgments, along with their appraisal of other features, such as a technology'scatastrophic potential or how equitably its risks are distributed. thesecatastrophic potential judgments match those of the experts in some cases, butdiffer in others (e.g., nuclear power).on semantic grounds, words can mean whatever a population group wantsthem to mean, as long as that usage is consistent and does not obscure importantsubstantive differences. on policy grounds, the choice of a definition is apolitical question regarding what a society should be concerned about whendealing with risk. whether we attach special importance to potential catastrophiclosses of life or convert such losses to expected annual fatalities (i.e., multiply thepotential loss by its annual probability of occurrence) and add them to the routinetoll is a value questionšas would be a decision to weight those routine lossesequally rather than giving added weight to losses among the young (or among thenonbeneficiaries of a technology).for other concepts that recur in risk discussions, the question of what theydo or should mean is considerably murkier. it is often argued, for example, thatdifferent standards of stringency should apply to voluntarily and involuntarilyincurred risks (e.g., starr, 1969). hence, for example, skiing could (or should)legitimately be a more hazardous enterprise than living below a major dam.although thereappendix c272improving risk communicationcopyright national academy of sciences. all rights reserved.is general agreement among experts and laypeople about the voluntariness of foodpreservatives and skiing, other technologies are more problematic (fischhoff etal., 1978b; slovic et al., 1980). there is considerable disagreement within expertand lay groups in their ratings of the voluntariness of technologies such asprescription antibiotics, commercial aviation, handguns, and home appliances.these disagreements may reflect differences in the exposures considered; forexample, use of commercial aviation may be voluntary for vacationers, butinvoluntary for certain business people (and scientists). or, they may reflectdisagreements about the nature of society or the meaning of the term. forexample, each decision to ride in a car may be voluntarily undertaken and may, inprinciple, be foregone (i.e., by not traveling or by using an alternative mode oftransportation); but in a modern industrial society, these alternatives may besomewhat fictitious. indeed, in some social sets, skiing may be somewhatinvoluntary. even if one makes a clearly volitional decision, some of the risksthat one assumes may be indirectly and involuntarily imposed on one's family orthe society that must pick up the pieces (e.g., pay for hospitalization due to skiingaccidents).such definitional problems are not restricted to ﬁsocialﬂ terms such asﬁvoluntary.ﬂ even a technical term such as ﬁexposureﬂ may be consensuallydefined for some hazards (e.g., medical x rays), but not for others (e.g.,handguns). in such cases, the disagreements within expert and lay groups may beas large as those between them. for orderly debate to be possible, one needs somegenerally accepted definition for each important termšor at least a goodtranslating dictionary. for debate to be useful, one needs an explicit analysis ofwhether each concept, so defined, makes a sensible basis for policy. once theyhave been repeated often enough, ideas such as the importance of voluntarinessor catastrophic potential tend to assume a life of their own. it does not go withoutsaying that society should set a double standard on the basis of voluntariness orcatastrophic potential, however they are defined.laypeople and experts are solving different problemsmany debates turn on whether the risk associated with a particularconfiguration of a technology is acceptable. although these disagreements maybe interpreted as reflecting conflicting social values or confused individualvalues, closer examination suggests thatappendix c273improving risk communicationcopyright national academy of sciences. all rights reserved.the acceptablerisk question itself may be poorly formulated (otway and vonwinterfeldt, 1982).to be precise, one does not accept risksšone accepts options that entailsome level of risk among their consequences. whenever the decisionmakingprocess has considered benefits or other (nonrisk) costs, the most acceptableoption need not be the one with the least risk. indeed, one might choose (oraccept) the option with the highest risk if it had enough compensating benefits.the attractiveness of an option depends on its full set of relevant positive andnegative consequences (fischhoff, lichtenstein, et al., 1981).in this light, the term ﬁacceptable riskﬂ is ill defined unless the options andconsequences to be considered are specified. once the options and consequencesare specified, ﬁacceptable riskﬂ might be used to denote the risk associated withthe most acceptable alternative. when using that designation, it is important toremember its context dependence. that is, people may disagree about theacceptability of risks not only because they disagree about what thoseconsequences are (i.e., they have different risk estimates) or because theydisagree about how to evaluate the consequences (i.e., they have differentvalues), but also because they disagree about what consequences and optionsshould be considered.some familiar policy debates might be speculatively attributed, at least inpart, to differing conceptions of what the set of possible options is. for example,saccharin (with its risks) may look unacceptable when compared with lifewithout artificial sweeteners (one possible alternative option). artificialsweeteners may, however, seem more palatable when the only alternative optionconsidered is another sweetener that appears to be more costly and more risky.or, nuclear power may seem acceptable when compared with alternative sourcesof generating electricity (with their risks and costs), but not so acceptable whenaggressive conservation is added to the option set. technical people from thenuclear industry seem to prefer the narrower problem definition, perhaps becausethey prefer to concentrate on the kinds of solutions most within their domain ofexpertise. citizens involved in energy debates may feel themselves less narrowlybound; they may also be more comfortable with solutions, such as conservation,that require their kind of expertise (bickerstaffe and peace, 1980).people who agree about the facts and share common values may stilldisagree about the acceptability of a technology because they have differentnotions about which of those values are relevant to aappendix c274improving risk communicationcopyright national academy of sciences. all rights reserved.particular decision. for example, all parties may think that equity is a good thingin general, without agreeing also that energy policy is the proper arena forresolving inequities. for example, some may feel that both those new inequitiescaused by a technology and those old ones endemic to a society are best handledseparately (e.g., through the courts or with income policies).thus, when laypeople and experts disagree about the acceptability of a risk,one must always consider the possibility that they are addressing differentproblems, with different sets of alternatives or different sets of relevantconsequences. assuming that each group has a full understanding of theimplications of its favored problem definition, the choice among definitions is apolitical question. unless a forum is provided for debating problem definitions,these concerns may emerge in more indirect ways (stallen, 1980).debates over substance may disguise battles overform, and vice versain most political arenas, the conclusion of one battle often sets some of theinitial conditions for its successor. insofar as risk management decisions areshaping the economic and political future of a country, they are too important tobe left to risk managers (wynne, 1980). when people from outside the riskcommunity enter risk battles, they may try to master the technical details or theymay concentrate on monitoring and shaping the risk management process itself.the latter strategy may exploit their political expertise and keep them from beingoutclassed on technical issues. as a result, their concern about the magnitude of arisk may emerge in the form of carping about how it has been studied. they maybe quick to criticize any risk assessment that does not have such features as eagerpeer review, ready acknowledgment of uncertainty, or easily accessibledocumentation. even if they admit that these features are consonant with goodresearch, scientists may resent being told by laypeople how to conduct theirbusiness even more than they resent being told by novices what various risksreally are.lay activists' critiques of the risk assessment process may be no lessirritating, but somewhat less readily ignored, when they focus on the way inwhich scientists' agendas are set. as veteran protagonists in hazard managementstruggles know, without scientific information it may be hard to arouse andsustain concern about an issue, to allay inappropriate fears, or to achieve enoughcertainty to justify action.appendix c275improving risk communicationcopyright national academy of sciences. all rights reserved.however, information is, by and large, created only if someone has a(professional, political, or economic) use for it. whether the cause is fads orfinances, failure to study particular topics can thwart particular parties and maylead them to impugn the scientific process.at the other extreme, debates about political processes may underliedisputes that are ostensibly about scientific facts. as mentioned earlier, thedefinition of an acceptablerisk problem circumscribes the set of relevant facts,consequences, and options. this agenda setting is often so powerful that adecision has effectively been made once the definition is set. indeed, the officialdefinition of a problem may preclude advancing one's point of view in a balancedfashion. consider, for example, an individual who is opposed to increased energyconsumption but is asked only about which energy source to adopt. the answersto these narrower questions provide a de facto answer to the broader question ofgrowth. such an individual may have little choice but to fight dirty, engaging inunconstructive criticism, poking holes in analyses supporting other positions, orridiculing opponents who adhere to the more narrow definition. this apparentlyirrational behavior can be attributed to the rational pursuit of officiallyunreasonable objectives.another source of deliberately unreasonable behavior arises whenparticipants in technology debates are in it for the fight. many approaches todetermining acceptablerisk levels (e.g., costbenefit analyses) make thepoliticalideological assumption that our society is sufficiently cohesive andcommongoaled that its problems can be resolved by reason and withoutstruggle. although such a ﬁget on with businessﬂ orientation will be pleasing tomany, it will not satisfy all. for those who do not believe that society is in afinetuning stage, a technique that fails to mobilize public consciousness andinvolvement has little to recommend it. their strategy may involve a calculatedattack on what they interpret as narrowly defined rationality (campen, 1985).a variant on this theme occurs when participants will accept any process aslong as it does not lead to a decision. delay, per se, may be the goal of those whowish to preserve some status quo. these may be environmentalists who do notwant a project to be begun or industrialists who do not want to be regulated. aneffective way of thwarting practical decisions is to insist on the highest standardsof scientific rigor.appendix c276improving risk communicationcopyright national academy of sciences. all rights reserved.laypeople and experts disagree about what is feasiblelaypeople are often berated for misdirecting their efforts when they chooserisk issues on which to focus their energies. however, a more careful diagnosiscan often suggest several defensible strategies for setting priorities. for example,zentner (1979) criticizes the public because its rate of concern about cancer (asmeasured by newspaper coverage) is increasing faster than the cancer rate. onereasonable explanation for this pattern is that people may believe that too littleconcern has been given to cancer in the past (e.g., our concern for acute hazardslike traffic safety and infectious disease allowed cancer to creep up on us). asecond is that people may realize that some forms of cancer are among the onlymajor causes of death that experience increasing rates.systematic observation and questioning are, of course, needed to tellwhether these speculations are accurate (and whether the assumption ofrationality holds in this particular case). false positives in divining people'sunderlying rationality can be as deleterious as false negatives. erroneouslyassuming that laypeople understand an issue may deny them a needed education;erroneously assuming that they do not understand may deny them a neededhearing. pending systematic studies, these error rates are likely to be determinedlargely by the rationalist or emotionalist cast of one's view of human nature.without solid evidence to the contrary, perhaps the most reasonable generalassumption is that people's investment in problems depends on their feelings ofpersonal efficacy. that is, they are unlikely to get involved unless they feel thatthey can make a difference, personally or collectively. in this light, theirdecisionmaking process depends on a concern that is known to influence otherpsychological processes: perceived feelings of control (seligman, 1975). as aresult, people will deliberately ignore major problems if they see no possibility ofeffective action. here are some reasons why they might reject a charge ofﬁmisplaced prioritiesﬂ when they neglect a hazard that poses a large risk: the hazard is needed and has no substitutes; the hazard is needed and has only riskier substitutes; no feasible scientific study can yield a sufficiently clear andincontrovertible signal to legitimate action;appendix c277improving risk communicationcopyright national academy of sciences. all rights reserved. the hazard is distributed naturally, and hence cannot be controlled; no one else is worried about the risk in question, and thus no one will heedmessages of danger or be relieved by evidence of safety; and no one is empowered to or able to act on the basis of evidence about therisk.thus, the problems that actively concern people need not be those whoseresolution they feel should rank highest on society's priorities. for example, onemay acknowledge that the expected deaths from automobile accidents over thenext century are far greater than those expected from nuclear power, and yet stillbe active only in fighting nuclear power out of the conviction, ﬁhere, i can make adifference. this industry is on the ropes now. it's important to move in for the killbefore it becomes as indispensable to american society as automobiletransportation.ﬂthus, differing priorities between experts and laypeople may not reflectdisagreements about the size of risks, but differing opinions on what can be doneabout them. at times, the technical knowledge or cando perspective of theexperts may lead them to see a broader range of feasible actions. at other times,laypeople may feel that they can exercise the political clout needed to make someoptions happen, whereas the experts feel constrained to doing what they are paidfor. in still other cases, both groups may be silent about very large problemsbecause they see no options.laypeople and experts see the facts differentlythere are, of course, situations in which disputes between laypeople andexperts cannot be traced to disagreements about objectivity, terminology, problemdefinitions, process, or feasibility. having eliminated those possibilities, one mayassume the two groups really do see the facts of the matter differently. here, itmay be useful to distinguish between two types of situations: those in whichlaypeople have no source of information other than the experts, and those inwhich they do. the reasonableness of disagreements and the attendant policyimplications look quite different in each case.how might laypeople have no source of information other than the experts,and yet come to see the facts differently? one way is for the experts' messagesnot to get through intact, perhaps because: (1)appendix c278improving risk communicationcopyright national academy of sciences. all rights reserved.the experts are unconcerned about disseminating their knowledge or hesitant todo so because of its tentative nature; (2) only a biased portion of the experts'information gets out, particularly when the selection has been influenced by thoseinterested in creating a particular impression; (3) the message gets garbled intransmission, perhaps due to illinformed or sensationalist journalists; or (4) themessage gets garbled upon reception, either because it was poorly explicated orbecause recipients lacked the technical knowledge needed to understand themessage (friedman, 1981; hanley, 1980; nelkin, 1977). for example, lordrothschild (1978) has noted that the bbc does not like to trouble its listenerswith the confidence intervals surrounding technical estimates.a second way of going astray is to misinterpret not the substance, but theprocess of the science. for example, unless an observer has reason to believeotherwise, it might seem sensible to assume that the amount of scientific attentionpaid to a risk is a good measure of its importance. science can, however, be morecomplicated than that, with researchers going where the contracts, limelight,blueribbon panels, or juicy controversies are. in that light (and in hindsight),science may have done a disservice to public understanding by the excessiveattention it paid to saccharin (ﬁscientists wouldn't be so involved if this were not amajor threatﬂ).a second aspect of the scientific process that may cause confusion is itsfrequent disputatiousness. it may be all too easy for observers to feel that ﬁif theexperts can't agree, my guess may be as good as theirsﬂ (handler, 1980). or, theymay feel justified in picking the expert of their choice, perhaps on spuriousgrounds, such as assertiveness, eloquence, or political views. indeed, it mayseldom be the case that the distribution of lay opinions on an issue does notoverlap some of the distribution of expert opinions. at the other extreme,laypeople may be baffled by the veil of qualifications that scientists often castover their work. all too often, audiences may be swayed more by twofisteddebaters (eager to make definitive statements) than by twohanded scientists(saying ﬁon the one hand x, on the other hand y,ﬂ in an effort to achievebalance).in each of these cases, the misunderstanding is excusable, in the sense that itneed not reflect poorly on the public's intelligence or on its ability to governitself. it would, however, seem hard to justify using the public's view of the factsinstead of or in addition to the experts' view. a more reasonable strategy wouldseem to be attempts at education. these attempts would be distinguished fromappendix c279improving risk communicationcopyright national academy of sciences. all rights reserved.attempts at propaganda by allowing for twoway communication, that is, by beingopen to the possibility that even when laypeople appear misinformed, they maystill have defensible reasons for seeing things differently than do the experts.for laypeople to disagree reasonably, they would have to have someindependent source of knowledge. what might that be? one possibility is thatthey have a better overview on scientific debates than do the active participants.laypeople may see the full range of expert opinions and hesitations, immune tothe temptations or pressures that actual debaters might feel to fall into one campand to discredit skeptics' opinions. in addition, laypeople may not feel bound bythe generally accepted assumptions about the nature of the world and the validityof methodologies that every discipline adopts in order to go about its business.they may have been around long enough to note that many of the confidentscientific beliefs of yesterday are confidently rejected today (frankel, 1974). suchlay skepticism would suggest expanding the confidence intervals around theexperts' best guess at the size of the risks.finally, there are situations in which the public, as a result of its lifeexperiences, is privy to information that has escaped the experts (brokensha etal., 1980). to take three examples: (1) the mackenzie valley pipeline (orberger) inquiry discovered that natives of the far north knew things about therisks created by icepack movement and seabed scouring that were unknown tothe pipeline's planners (gamble, 1978); (2) postaccident analyses often revealthat the operators of machines were aware of problems that the designers of thosemachines had missed (sheridan, 1980); and (3) scientists may shy away fromstudying behavioral or psychological effects (e.g., dizziness, tension) that are hardto measure, and yet still are quite apparent to the individuals who suffer fromthem. in such cases, lay perceptions of risk should influence the experts' riskestimates (cotgrove, 1982; wynne, 1983).summaryit is tempting to view others in simplistic terms. cognitively, one can savemental effort by relying on uncomplicated labels like ﬁthe hysterical publicﬂ orﬁthe callous experts.ﬂ motivationally, properly chosen labels can affirm one's ownlegitimacy. by the same token, such interpretations can both obstruct theunderstanding of conflicts (by blurring significant distinctions) and hamper theirresolutionappendix c280improving risk communicationcopyright national academy of sciences. all rights reserved.(by bolstering selfserving characterizations). the following section begins byexplaining the consequences of such stereotyping for risk communication bydiscussing the sort of communication strategies that can follow from simplisticinterpretations of the controversy. it continues to outline principles for morecomplex strategies. these can inform both those designing communicationsprograms and those receiving them.appendix c281improving risk communicationcopyright national academy of sciences. all rights reserved.v strategies for risk communicationconcepts of risk communicationrisk communication is a collective noun for a variety of proceduresexpressing quite different attitudes toward the relationship between a society'slaypeople and its technicalmanagerial elite (covello et al., 1986). at one extremelies the image of an inactive public docilely waiting for the transmission of vitalinformation from those who know better. within this perspective, thecommunication process involves a source, a channel, and a receiver (to use oneset of technical terms common among social scientists). although conceptuallysimple, this characterization still forces one to consider myriad details about eachcomponent. for example (hovland et al., 1953): how well trusted is the source?is it a corporate entity, capable of speaking with a single voice, or does itsometimes contradict itself? how much experience and language does the sourceshare with the receivers? how much time does it have to prepare its messages?what are the legal restrictions on how much it can say?at the other extreme lie highly interactive images of the communicationprocess, in which the public shares responsibility for the social management ofrisks. such processes, which require exchanges of information, could, inprinciple, be viewed as special cases of the sourcechannelreceiver model.however, using that model (and the research associated with it) requires bearingin mind the notion that these ﬁreceiversﬂ are actively shaping the messages thatthey receive and perhaps even the research conducted in order to create thesubstance of those messages (kasperson, 1986).one way of diagnosing the nature of specific risk communication processesis in terms of the philosophies that guide those who design them. the followingdiscussion describes some generic strategies in terms of their strengths andlimitations. the discussion after that considers some more integrative designprinciples. together, they are intended to create a framework for responsiblyusing the more technical material on communication design presented in the finalsection. that material assumes an understanding of the role of information in therisk management (including communication) process (johnson and covello,1987; rayner and cantor, 1987).appendix c282improving risk communicationcopyright national academy of sciences. all rights reserved.some simple strategiesthe technical and policy issues involved in making risk managementdecisions are complex enough in themselves. dealing with public perceptions ofrisks creates an additional level of complexity for risk managers. one possibleresponse to this complexity is to look for some ﬁquick fixﬂ that will deal with thepublic's needs. unfortunately for the risk manager, these strategies are both hardto execute well by themselves and unlikely to be sufficient even if they are wellexecuted. at times, these simple solutions seem to reflect a deepmisunderstanding of the public's role in risk management, reflecting perhaps abelief that the human element in risk management can be engineered in the sameway as mechanical and electronic elements. undertaken in isolation and withthese unrealistic expectations, such strategies can produce mutually frustratingcommunication programs. the following are some of the more common of thesesimple strategies for dealing with risk controversies, presented in caricature formto highlight their underlying motivations and inherent limitations.give the public the factsthe assumption underlying this strategy is that if laypeople only knew asmuch as the experts, they would respond to hazards in the same way. undertakeninsensitively, this strategy can result in an incomprehensible deluge of technicaldetails, telling the public more than it needs to know about specific risk researchresults, and much less than it needs to know about the quality of the research (andabout how to make the decisions that weigh most heavily on its mind).concentrating communications on the transmission of information also ignoresthe possibility that there are legitimate differences between the public and theexperts regarding either the goals or the facts of risk management.sell the public the factsthe premise here is that the public needs persuasion, rather than education.it often follows the failure of an information campaign to win public acceptancefor a technology. undertaken heavyhandedly, this approach may amount to littlemore than repeating more loudly (or fancily) messages that the public has alreadyrejected. here, as elsewhere, obvious attempts at manipulation can breedresentment.appendix c283improving risk communicationcopyright national academy of sciences. all rights reserved.give the public more of what it has gotten in the pastthe underlying assumption here is that the public will accept in the futurethe kinds of risks that it has accepted in the past. if true, then what the publicwants (and will accept) can be determined simply by examining statistics showingthe riskbenefit tradeoffs involved in existing technologies. this ﬁrevealedpreferenceﬂ philosophy ignores the fact, consistently revealed by opinion pollsshowing great public support for environmental regulations, that people areunhappy with how risks have been managed in the past. the risks that peoplehave tolerated are not necessarily acceptable to them. as a result, giving themmore of the same means enshrining past inequities in future decisions. inprinciple, this approach attaches no importance to educating the public, tocreating a constituency for risk policies, or to involving the public in the politicalprocess. it seems to respect the public's wishes, while keeping the public itself atarm's length.give the public clearcut, noncontroversial statements of regulatoryphilosophythe assumption underlying this family of approaches is that people do notwant facts, but instead the assurance that they are being protected. that is,whatever the risks may be, they are in line with government policy. examples inthe united states include the delaney clause, prohibiting carcinogenic additivesin foods, and the nuclear regulatory commission's ﬁsafety goals for nuclearpower,ﬂ describing how risky it will allow the technology to be. each policy isstated in terms of levels of acceptable risk, as though laypeople are toounsophisticated to understand, in the context of technology management, the sortof riskbenefit tradeoffs that they routinely make in everyday life, such as whenthey undergo medical treatments or pursue hazardous occupations. moreover,such simple statements provide little guidance for many real situationsšbydenying the complexity of the (riskbenefit) decisions that needed to be made. ifperceived as hollow, then they will do little to reassure the public.let the marketplace decideanother hope for risk communication is that risks will be understood whencommunicated in the context of specific consumerappendix c284improving risk communicationcopyright national academy of sciences. all rights reserved.decisions. one variant on this approach is the claim that reducing governmentregulation will allow people to decide independently what risks they are willingto accept, with the courts addressing any excesses. a second variant is providingquantitative risk information along with goods and drugs. it makes optimisticassumptions regarding laypeople's ability to know enough to fend for themselveswith all life's risks. the assumption of personal responsibility and the motivationto get it right are meant to prompt efficient acquisition and understanding. itassumes that people will recognize the limits to their risk perceptions and graspthe risk information presented to them. a threat to any approach emphasizingselfreliance is that people might not want to defend their own welfare when itcomes to health and safety, especially where risks have long latencies and it isimpossible to prove the source of a health risk (and obtain redress).put risk managers on the firing linethe assumption underlying this strategy is that what the public needs inorder to understand risk issues is a coherent story from a single credible source.examples might include the nuclear regulatory commission's reliance on asingle spokesperson as the three mile island incident wore on and theassumption of center stage by the president of union carbide after the chemicalgas leak in bhopal, india. this strategy can reduce the confusion created byincomplete conflicting messages, although only if the manager has goodcommunication skills or is sensitive to listeners' information needs; that is, theremust be both substance and style. oversimplifications, misrepresentations, andunacceptable policies are just that, even if they come from a nice guy. thisapproach can also create a bottleneck for understanding the public's concerns tothe extent that the single source of information must also be the single recipient.involve local communities in resolving their own risk managementproblemsthis approach assumes that people will be flexible and realistic about tradeoffs when they seešand have responsibility foršthe big picture. such anapproach can founder when the community lacks real decisionmaking authorityor the technical ability to understand its alternatives. it may also founder whenthose alternatives accept perceived past inequities (e.g., reduce chronic poverty byacceptingappendix c285improving risk communicationcopyright national academy of sciences. all rights reserved.a hazardous waste dump) or are of the jobsversushealth variety that peopleexpect government to help them resolve. ensuring the informed consent of thegoverned for the risks to which they are exposed is a laudable goal. however, itsachievement requires that people have tolerable choices, adequate information,and the ability to identify which course of action is in their own best interests.conceptualizing communication programsdespite their flaws, these simple strategies all have some merit. it isimportant to give people the facts and to be persuasive when the facts do notspeak for themselves or when existing prejudices must be overcome. it is alsoimportant to maintain some consistency with past risk management decisions, toexpound clear policies, to exploit the wisdom of the marketplace, to encouragedirect communication between risk managers and the public, and to givecommunities meaningful control over their own destinies. the problem is thateach strategy oversimplifies the nature of risk issues and the public's involvementwith them. when risk managers pin unrealistic hopes on such strategies, then theopportunity to address the public's needs more comprehensively is lost. whenthese hopes are not met, the frustration that follows is often directed at the public.it is both unfair and corrosive for the social fabric to criticize laypeople forresponding inappropriately to risk situations for which they were not adequatelyprepared. it is tragic and dangerous when members of our technical elite feel thatthey have devoted their lives to creating a useful technology (e.g., nuclear power)only to have it rejected by a foolish and unsophisticated public. likewise, it ispainful and unfortunate when the public labels those elites as evil and arrogant.risk management requires allocating resources and making tradeoffsbetween costs and benefits. thus, it inherently involves conflicts. both thesubstance and the legitimacy of these conflicts are obscured, however, when theparticipants come to view them as struggles between the forces of good and evil,or of wisdom and stupidity. effective solutions will have to be respectfulsolutions, recognizing both the legitimacy and complexity of the public'sperspective, giving it no more and no less credit for reasonableness than itdeserves.how can the preceding observations about risk perceptions (and the researchliterature from which they were drawn) be used to design better procedures fordealing with risk controversies?appendix c286improving risk communicationcopyright national academy of sciences. all rights reserved.one necessary starting point is a detailed consideration of the nature of therisk that the public must understand. that consideration must cover not only thebest available technical estimates for the magnitude of the risk, but also the bestavailable psychological evidence on how people respond to that kind of risk.research has shown, for example, that people have special demands for safetyšand reassurancešwhen risks are perceived to have delayed effects orcatastrophic potential, and when risks appear to be poorly understood or out ofpeople's personal control (slovic, 1986; vlek and stallen, 1980, 1981; vonwinterfeldt et al, 1981). such risks are likely to grab people's attention and createunrest until they can be put in some acceptable perspective. they demand greatercommunication resources, with particular attention devoted to creating anatmosphere of trust. perhaps paradoxically, people may need to be treated withthe greatest respect in those situations in which they may seem most emotional(or most human) (eiser, 1982; weinstein, 1987).a second necessary starting point is a detailed description of howinformation about risk can reach people (johnson and covello, 1987; rubin andsachs, 1973; schudson, 1978). such information may be the result of accidents atvarious distances away and attributed to various causes (e.g., malfunctions,human error, sabotage) or of mere ﬁincidents,ﬂ such as newspaper exposes, sitingcontroversies, false alarms, or government inquiries. proactively, this analysiswill show the opportunities for reaching people. for example, is there a chance toeducate at least some of the public in advance, or can one only prepare materialsfor times of crisis? reactively, this analysis should help one anticipate whatpeople will already know (or believe) when the time comes for systematiccommunication. it may show that people are buffeted by confusing,contradictory, and erroneous messagesšor that they have some basicunderstanding within which they can integrate new information. in any case,communication must build on people's current mental representation of thetechnologyševen if its first step is to challenge inappropriate beliefs and enhancepeople's ability to examine future information more critically.knowing what people do know allows a systematic analysis of what theyneed to knowšthe next point of departure in communicating with the public. insome cases, crude estimates of a technology's risks and benefits may be enough;in other cases, it may be importantappendix c287improving risk communicationcopyright national academy of sciences. all rights reserved.figure v.1 the radiation hazard in homes from the residents' perspective.source: svenson and fischhoff, 1985.to know how a technology operates. the needs depend on the problems thatthe public is trying to solve: what to do in an emergency; how to react in a sitingcontroversy; whether to eat vegetables, or whether to let their children do so; andso on. perhaps the most efficient description would be in the terms of decisiontheory, such as the simple decision tree in figure v.1, depicting the situationfaced by the head of a household deciding whether to test for domestic radonaccumulations. such descriptions allow one to determine how sensitive thesedecisions are to different kinds of information, so that communication can focuson the things that people really need to know.producing comparable descriptions for the different actors in a riskmanagement episode will help clarify sources of disagreement among them.often the risk managers' decision problem (e.g., whether to ban edb) will bequite different from the public's decision problem (e.g., whether to use blueberrymuffin mix). for example, figure v.2 shows the key decision problem that mightface risk managers concerned about radon: what standard to set as expressing atolerable level of exposure. the critical outcomes of this decision are quitedifferent from those associated with the residents' focal decision of whether totest their homes for radon (figure v.1). failureappendix c288improving risk communicationcopyright national academy of sciences. all rights reserved.figure v.2 the radiation hazard in homes from the authorities' perspective.source: svenson and fischhoff, 1985.to address the public's information needs is likely to leave them frustratedand hostile. failure to address the managers' own problems is likely to leave theireventual actions inscrutable. for telling their own story, the managers need aprotocol that will ensure that all of the relevant parts get out, including whatoptions they are legally allowed to consider, how they see the facts, and what theyconsider to be the public interest. such comprehensive accounts are often absentfrom the managers' public pronouncements, preventing the public fromresponding responsibly and suggesting that the managers failed to consider theissues fully. the procedures offered in section ii as ways for the public (or themedia) to discover what risk issues are all about might also be used proactively asways to tell the public (or the media) directly about those risks.after determining what needs to be said, risk managers can start worryingabout how to say it. a common worry is that the public will not be able tounderstand the technical details of how a technology operates. where thosedetails are really pertinent, the services of good science writers and educators maybe needed. perhaps a more common problem is making the basic concepts of riskmanagement clear. just what is a oneinamillion chance? what does it mean toprotect wastes for a hundred generations? must we inevitably set a value onhuman life when resources are allocated for risk reduction?appendix c289improving risk communicationcopyright national academy of sciences. all rights reserved.the psychological research described above has shown the difficulty of theseconcepts; it is beginning to show ways to communicate them meaningfully. theresearch base for addressing these obstacles to understanding is described in thenext section.adopting such a deliberative approach to characterizing people's needswould help avoid the inadvertent insensitivity found in the institute of medicine's(1986) report, confronting aids. the report noted, somewhat despairingly, thatonly 41 percent of the general public knew that aids was caused by a virus. yet,although this fact is elemental knowledge for medical researchers, it has relativelylittle practical importance for laypeoplešin the sense that one would be hardpressed to think of any real decision whose resolution hinged on knowing thataids was a virus. laypeople interested in a deep understanding of the aidsproblem ought to know this fact. however, it is irrelevant to laypeople satisfiedjust to make reasonable decisions regarding aids. such insensitivity is sociallydamaging insofar as it demeans the public in the eyes of the experts and promptsthe provision of seemingly irrelevant communications.another example of this insensitivity to the needs of message recipients canbe found in the advice literature about sexual assault (morgan, 1986). much ofthe research is performed and communicated without consideration for women'sdecisionmaking needs (furby and fischhoff, in press). most studies concentrateon significance levels, whereas what women need is reliable information oneffect size. that is, women need to know not only whether a strategy makes adifference, but how much of a difference. a second form of insensitivity towomen's decisionmaking needs is that few studies collect data on the temporalorder of strategies and consequences. as a result, although if greater physicalresistance by women were associated with greater violence by men, one wouldnot know which causes which. a third form of insensitivity can be found inrecommendations telling women how to respond to different kinds of assailants,without considering whether women can even make such diagnoses under reallife conditions or without reporting the overall prevalence (or ﬁbase ratesﬂ) of thedifferent assailant types, an essential piece of information for making anydiagnosis. finally, some studies actually made the ﬁbaserate fallacyﬂ (barhillel,1980; kahneman and tversky, 1972), concluding, say, that screaming is moreeffective than fighting because, among women who escape, 80 percent do theformer and only 20 percent do the latter.taking the details of risk perceptions seriously means reconcilingappendix c290improving risk communicationcopyright national academy of sciences. all rights reserved.ourselves to a messy process. in managing risks, society as a whole is slowly andpainfully learning how to make deliberative decisions about very difficult issues.avoiding frustration with the failures and with the public that seems responsiblefor them will help us keep the mental health and mutual respect needed to getthrough it all.evaluating communication programstesting risky treatmentsif they were creating risks rather than explaining them, risk communicatorswould be subject to various political, legal, and social constraints. if thetreatment involved a medical intervention, then there would be a comparabletangle of restrictions. what analogous responsibilities are incumbent on thosewho treat others with information?a minimal requirement might be that a communication have positiveexpected value. that is, its anticipated net effect should be for the good,considering the magnitude and likelihood of possible consequences. releasing acommunication program that flunked this test would be like authorizing a drugwith uncompensated side effects.a minimal standard of proof for passing this minimal test is expertjudgment. thus, a communication technique could be approved if it wereﬁgenerally regarded as safeﬂ and seemed likely to be at least somewhat effective.such reliance on experts' intuitions creates the same discomfort as comparableproposals for grandparenting existing drugs or additives because they are familiarand appear to be safe. how do we know they work? might negative effectssimply have escaped notice or measure? just what do these experts know? canthey be trusted?more convincing would be empirical evidence from a basic science of riskcommunication providing some a priori basis for predicting the effects ofparticular communications. that evidence could be positive, showing that acommunication draws on a demonstrated cognitive ability [e.g., people canunderstand quantitative probabilities, as long as they are not too small (beythmarom, 1982)]. or, it could be negative, showing that a communication demands akind of understanding that is not widely distributed [e.g., people have troublerealizing how the probability of failure accumulates from repeated events, such asusing a contraceptive device or being exposed to a disease (barhillel, 1973)].appendix c291improving risk communicationcopyright national academy of sciences. all rights reserved.more convincing still is evidence from a test of the communication itself,performed with individuals like its ultimate recipients and in a setting like that inwhich it will ultimately be administered. if that setting must be simulated, thenthe simulation should capture both those features of the actual communicationcontext that interfere with understanding (e.g., talking to friends during thetransmission) and those features that can enhance comprehension (e.g., discussingthe transmission with friends) (turner and martin, 1985).evaluative criteriaperforming an evaluation requires a clear, operable definition of theconsequences to be desired and avoided. with medical treatments, identifying theconsequences is usually a straightforward processšthey are various possiblehealth effects, some good and some bad. what might be more complicated ismeasuring some of the effects (e.g., those involving delayed consequences) anddetermining their relative importance. although medical personnel and theirclients are likely to agree about which outcomes are good and which are bad, theyneed not agree about how good and how bad the outcomes are. for example, theymight feel differently about tradeoffs between short and longterm effects orbetween changes in quality of life and in expected longevity (mcneil et al.,1978). as a result, even after a definitive evaluation, there may be no universalrecommendation. a wellunderstood treatment might be right for some people,but wrong for others.in evaluating communication programs, similar issues arise, although with afew additional wrinkles. potential consequences must still be identified.however, the set seems less clearly defined. there are the good and bad healtheffects, but they may be hard to observe. if a communication causes undueconcern, then there may be stressrelated effects, but they tend to be quite diffuse(e.g., a few more cases of child abuse, depression, divorce, and so on, scatteredthrough the treated population) (elliot and eisdorfer, 1982). on the other side ofthe ledger, if people do engage in healthenhancing behavior, then the influenceof the focal communication must be isolated from that of other informationsources (including, perhaps, continued rumination about an issue).difficulties in observing the effects of ultimate interest may divert attentionto more observable effects closer to the treatment.appendix c292improving risk communicationcopyright national academy of sciences. all rights reserved.one possibility that arises with communication programs (unlike conventionalmedical treatments) is assessing comprehension of the message. if people havenot understood the message, then an appropriate response seems unlikely. thesimplest test of comprehension might be remembering the facts of a message.those recipients who pass it would, however, still have to be tested for whetherthey are able to use those remembered facts in their decision making. those whofail the test would still have to be tested for whether they have heard themessage, but chose to reject it. rejection might mean distrusting the source'scompetence or its motives. that is, the communicators may not seem to knowwhat they are talking about or they may seem inadequately concerned about therecipients' welfare.setting objectives for communication programsit is accepted wisdom that program planning of any sort ought to begin withan explicit statement of objectives, in the light of which a program's elements canbe selected and its effects evaluated. figure v.3 offers one conceptualization ofrisk communication programs, categorized according to their primary objective.according to covello et al. (1986:172œ173):in the real world, these four types of risk communication tasks overlapsubstantially, but they still can be conceptually differentiated. the task ofinforming and educating the public can be considered primarily a nondirective,although purposeful, activity aimed at providing the lay public with useful andenlightening information. in contrast, both the task of encouraging behaviorchange and personal protective action and that of providing disaster warningsand emergency information can be considered primarily directive activitiesaimed at motivating people to take specific types of action. these three tasks, inturn, differ from the task of involving individuals and groups in joint problemsolving and conflict resolution, in which officials and citizens exchangeinformation and work together to solve health and environmental problems.as can be seen from figure v.3, much risk communication is initiated withthe communicators' benefit foremost in mind. for example, the sponsors of atechnology may wish to reassure a recalcitrant and alarmed public about itssafety. if the public's worry is really unwarranted, then everyone comes outahead: the technology will get a fairer shake and the public will be relieved of anunnecessary worry. the crucial question is what constitutes ﬁunwarrantedﬂconcern. one possible definition is exaggerating the magnitude of the risk (orunderestimating the magnitude of accompanying benefits).appendix c293improving risk communicationcopyright national academy of sciences. all rights reserved.figure v.3 a typology of risk communication objectives. source: covelloet al., 1986.in such cases, straight information messages might help. however, they needto be designed with an eye to implicit as well as explicit content. for example, ifthey are perceived as insistently repeating that ﬁthe risk is only xﬂ (or that ﬁthebenefit is really yﬂ), then recipients may read between the lines, ﬁand that oughtto be good enough for you.ﬂ communicators may convince themselves about therectitude of such implicit messages, feeling that expert knowledge about the sizeof risks generalizes to expert knowledge about their acceptability.certainly, people should be better off with better information. however,even wellinformed people may dislike a technology if they feel that its benefits(to them) are not commensurate with its risks (to them), or that those benefits aresubstantially lower than the benefits enjoyed by a technology's sponsor. honestcommunications should help people reach such determinations. as a result,neither the senders nor the recipients of messages should be faulted if moreinformation leads to more opposition.appendix c294improving risk communicationcopyright national academy of sciences. all rights reserved.an alternative definition of ﬁunwarranted concernﬂ is ﬁlarger than theconcern associated with hazards having equivalent risk.ﬂ in more sophisticatedversions, the comparison might be with concern over hazards having anequivalent relationship between risks and benefits. a popular contribution to therisk literature a decade ago was lists of disparate risks, chosen so that most were,arguably, accepted by most people (cohen and lee, 1979; crouch and wilson,1982). the lists would also contain some favored technology (e.g., nuclearpower) that should seemingly be accepted, by whatever criterion led to theacceptance of the other risks in the list. such lists might, if thoughtfullyassembled, help to educate readers' intuitions about the relative magnitude ofdifferent risks and the nature of very small risks (e.g., 10!6), such as often appearin such lists. however, even recipients who accept the general idea of consistencythat underlies such claims need not accept the particular form of consistencyimplied by the list (covello et al., 1988). they may not endorse the particulardefinition of risk used in the list; they may not feel that all currently accepted (ortolerated or endured) risks are actually acceptable (in the sense that they haveagreed voluntarily to the hazards bearing those risks and would not want lesserrisks if those were available at a reasonable price). nor need people accept eventhe weaker consistency claim that they should not worry more about any hazardthan they worry about hazards that they believe to have greater risks. section iiidiscusses some of people's reasons for ignoring admittedly large hazards.comprehension of risk messages is seldom the consequence that isultimately of interest. rather, it is a potentially observable surrogate for actualimprovements in wellbeing. a step closer to that consequence would beevidence that recipients of a message had connected their perception of itscontents with the course(s) of action in their own best interests (i.e., what adecision theorist would prescribe, given recipients' definition of the situation).for achieving this goal, recipients could be left to their own devices, or theymight be provided some help in connecting their beliefs and values with possibleactions.assuming that it can be done in a neutral (noncoercive) way, providing suchhelp changes the nature of the relationship. rather than one party administeringan informational treatment to another, the treater becomes more of an aide andservant. one particular expression of the change emerges in situations in which acommunicator wishes to claim that people have given ﬁinformed consentﬂappendix c295improving risk communicationcopyright national academy of sciences. all rights reserved.to the risks described in a communication (p.s.appelbaum et al., 1987). thatclaim should interest people exposed to the risks only if it changes theirbargaining position visàvis the creator of the risks (e.g., ﬁwhat's it worth to youfor me to sign this release?ﬂ or ﬁdoes that mean that i can force you to give memore information about potential adverse health effects?ﬂ). what people shouldcare about is identifying the best choice of action. a communication serves thatend if it provides people with the information that they need in a form that theycan use. in this light, informed consent may be claimed when people have chosenthe best possible course of action for themselves.these criteria for evaluating risk communication, like those typicallyinvoked for evaluating medical treatments, are focused on direct effects of simpleinterventions. however, any treatment is but one in a series (at least for those whosurvive). for example, treatment with an antibiotic might cause no immediateadverse side effects, but might still create an allergic condition that reduces theset of possible treatments for future maladies. good communication can enhancerecipients' actual and perceived ability to understand a risky world and deal withit effectively. poor communication can do the opposite, reducing recipients'confidence in their own competence to manage the risks in their lives. just asemotional involvement can impair understanding of the content of messages, socan misunderstanding messages produce unproductive emotions.institutional controlsif risk communications were viewed as treatments, then they might alsoﬁenjoyﬂ an institutional context like that created for medical treatments. onecomponent might be review panels to scrutinize the protocols for testing orrunning communication programs. such panels might both ensure that programsuse suitable evaluation criteria (e.g., reflecting both senders' and recipients'needs) and examine messages for attempts to coerce or misinform. review panelsmight also provide guidance on ethical issues. for example, if there is acommonly accepted ﬁbestﬂ way to convey a certain kind of information, can onelegitimately substitute new, experimental methods? how would that decisionchange as a function of the kind of testing that the accepted method hadundergone? or, what should be done with messages telling people that they arepowerless to affect their fate (e.g., they have been exposed to a carcinogen withirreversibleappendix c296improving risk communicationcopyright national academy of sciences. all rights reserved.effects, such as asbestos)? recipients' natural concern over the risk could beaggravated by the feeling of helplessness, especially if the risk is perceived ashaving been imposed by someone else without providing proper consent orcompensation. do senders have a responsibility to provide counseling for thoseupset by their messages? might they even restrict dissemination? how would thedecision about the communication process change if the information would helprecipients (or others) to mobilize their resources in responding to other hazards?if there are only limited resources for communication, who should receive them(e.g., those at greatest risk, those most responsive to available communicationtechniques, or those most accessible)?the institutional context for medical treatments attempts not only to ensurethat they are delivered properly, but also to address possible failures. lists ofcounterindications accompany many treatments. physicians are always on standby, ready to ameliorate the side effects of their treatments. various mechanismsexist for collecting and disseminating (good and bad) experiences, for bothveteran and experimental treatments. when the rate of side effects isunacceptable, either for a treatment or for a treater, government and professionalbodies may stop the exposure. in the background of all these efforts to managerisks lurks the threat of legal proceedings to rectify unmanaged problems (e.g.,malpractice and product liability suits). people are more likely to behave wellwhen there are strong social norms for doing so and significant penalties forfailure. the desire to be fair to all parties prompts a sharpening of standards.it took many years to evolve these institutions and standards (manycenturies, if one reaches back to hippocrates). judging by the variouscontemporary crises (e.g., malpractice, cost containment), they are still far fromperfect. however, those imperfections pale before those of treatments with nosuch infrastructure. in cases in which an institutional context is created anew for aparticular cause, it may be hard to get this degree of balance. for example, righttoknow laws have recently been enacted to ensure that workers receiveinformation about occupational hazards. the laws are intended to help workersprotect themselves on the job and to help employers protect themselves in court(by strengthening their claim that workers have given informed consent to bearingthe risks). the criteria for evaluating these efforts seem to concentrate more onwhat is said than on what is understood, raising the threat of overloaded andoverly technical messages filling the letter but not the intent of theappendix c297improving risk communicationcopyright national academy of sciences. all rights reserved.law. the existence of such threats suggests a tenuous state of affairs for even themore developed areas of risk communication.summaryrisk information is an important part of many human activities. yet it is atmost but a part. understanding its role is essential to giving risk communicationprograms their basic shape, with appropriate objectives and realisticexpectations. such an analysis can help communicators avoid simplisticstrategies that leave recipients, at best, unsatisfied and, at worst, offended by thefailure to address their perceived needs. in some cases, these will be for betterinformation; in other cases, they will be for better protection. only aftercommunication programs are recipient centered in this respect can theyproductively begin to be recipient centered in the sense of the following section,considering laypeople's strengths and weaknesses in understanding riskinformation.appendix c298improving risk communicationcopyright national academy of sciences. all rights reserved.vi psychological principles incommunication designwhenever they read a brochure, talk to their neighbors, or observe ominousactivities at a local plant in order to understand the risks of a technology, peoplemust rely on the same basic cognitive processes that they use to understand otherevents in their lives. as mentioned in section ii, the study of such processes is aninvolved pursuit, with many methodological nuances (like most sciences). toprovide some access to the substantive results of such research, here are anumber of relatively simple and generally supported statements about behavior.the difficulty in applying them to the prediction of reallife behavior is that life'ssituations are complex, meaning that various simple behaviors interact in waysthat require a subtle analysis to understand.people simplifymost substantive decisions require people to deal with more nuances anddetails then they can readily handle at any one time. people have to juggle amultitude of facts and values when deciding, for example, whether to changejobs, trust merchants, or protest a toxic landfill. to cope with this informationoverload, people simplify. rather than attempting to think their way through tocomprehensive, analytical solutions to decisionmaking problems, people try torely on habit, tradition, the advice of neighbors (or the media), and on generalrules of thumb (e.g., nothing ventured, nothing gained). rather than consider theextent to which human behavior varies from situation to situation, peopledescribe other people in terms of allencompassing personality traits, such asbeing honest, happy, or risk seeking (nisbett and ross, 1980). rather than thinkprecisely about the probabilities of future events, people rely on vaguequantifiers, such as ﬁlikelyﬂ or ﬁnot worth worrying aboutﬂšterms that are alsoused differently by different people and by the same individual in differentcontexts (beythmarom, 1982).the same desire for simplicity can be observed when people press riskmanagers to categorize technologies, foods, or drugs as ﬁsafeﬂ or ﬁunsafe,ﬂ ratherthan treating safety as a continuous variable. it can be seen when people demandconvincing proof from scientists who can provide only tentative findings. it canbe seen when peopleappendix c299improving risk communicationcopyright national academy of sciences. all rights reserved.attempt to divide the participants in risk disputes into good guys and bad guys,rather than viewing them as people who, like themselves, have complex andinteracting motives. although such simplifications help people cope with life'scomplexities, they can also obscure the fact that most risk decisions involvegambling with people's health, safety, and economic wellbeing in arenas withdiverse actors and shifting alliances.once people's minds are made up, it is difficult tochange thempeople are extraordinarily adept at maintaining faith in their current beliefsunless confronted with concentrated and overwhelming evidence to the contrary.although it is tempting to attribute this steadfastness to pure stubbornness,psychological research suggests that some more complex and benign processesare at work (nisbett and ross, 1980).one psychological process that helps people maintain their current beliefs isfeeling little need to look actively for contrary evidence. why look, if one doesnot expect that evidence to be very substantial or persuasive? for example, howmany environmentalists read forbes and how many industrialists read the sierraclub's bulletin in order to learn something about risks (as opposed to readingthese publications to anticipate the tactics of an opposing side)? a secondcontributing thought process is the tendency to exploit the uncertaintysurrounding apparently contradictory information in order to interpret it as beingconsistent with existing beliefs. in risk debates, a stylized expression of thisproficiency is finding just enough problems with contrary evidence to reject it asinconclusive.a third thought process that contributes to maintaining current beliefs can befound in people's reluctance to recognize when information is ambiguous. forexample, the incident at three mile island would have strengthened the resolveof any antinuclear activist who asked only, ﬁhow likely is such an accident, given afundamentally unsafe technology?ﬂ, just as it would have strengthened theresolve of any pronuclear activist who asked only, ﬁhow likely is the containmentof such an incident, given a fundamentally safe technology?ﬂ although a verysignificant event, three mile island may not have revealed very much about theriskiness of nuclear technology as a whole. nonetheless, it helped the opposingsides polarize their views. similar polarization has followed the accident atchernobyl,appendix c300improving risk communicationcopyright national academy of sciences. all rights reserved.with opponents pointing to the ﬁconsequences of a nuclear accidentﬂ (which comewith any commitment to nuclear power) and proponents pointing to the uniquefeatures of that particular accident (which are unlikely to be repeated elsewhere,especially considering the precautions instituted in its wake) (krohn andweingart, 1987).people remember what they seefortunately, given their need to simplify, people are quite good at observingthose events that come to their attention (and that they are motivated tounderstand) (hasher and zacks, 1984; peterson and beach, 1967). as a result, ifthe appropriate facts reach people in a responsible and comprehensible formbefore their minds are made up, there is a decent chance that their first impressionwill be the correct one. for example, most people's primary sources ofinformation about risks are what they see in the news media and observe in theireveryday lives. consequently, people's estimates of the principal causes of deathare strongly related to the number of people they know who have suffered thosemisfortunes and the amount of media coverage devoted to them (lichtenstein etal., 1978).unfortunately for their risk perceptions (although fortunately for their wellbeing), most people have little firsthand knowledge of hazardous technologies.rather, what laypeople see most directly are the outward manifestations of therisk management process, such as hearings before regulatory bodies or statementsmade by scientists to the news media. in many cases, these outward signs are notvery reassuring. often, they reveal acrimonious disputes between supposedlyreputable experts, accusations that scientific findings have been distorted to suittheir sponsors, and confident assertions that are disproven by subsequent research(dietz and rycroft, 1987; maclean, 1987; rothman and lichter, 1987).people cannot readily detect omissions in theevidence they receivenot all problems with information about risk are as readily observable asblatant lies or unreasonable scientific hubris. often, the information that reachesthe public is true, but only part of the truth. detecting such systematic omissionsproves to be quite difficult (tversky and kahneman, 1973). for example, mostyoung people know relatively few people suffering from the diseases of oldappendix c301improving risk communicationcopyright national academy of sciences. all rights reserved.age; nor are they likely to see those maladies cited as the cause of death innewspaper obituaries. as a result, young people tend to underestimate thefrequency of these causes of death, while overestimating the frequency of vividlyreported causes, such as murder, accidents, and tornadoes (lichtenstein et al.,1978).laypeople are even more vulnerable when they have no way of knowingabout information because it has not been disseminated. in principle, forexample, patients could always ask their physicians whether they have neglectedto mention any side effects of the drugs they prescribe. likewise, people couldalways ask merchants whether there are any special precautions for using a newpower tool, or ask proponents of a hazardous facility if their risk assessmentshave considered operator error and sabotage. in practice, however, thesequestions about omissions are rarely asked. it takes an unusual turn of mind torecognize one's own ignorance and insist that it be addressed.as a result of this insensitivity to omissions, people's risk perceptions can bemanipulated in the short run by selective presentation. not only will people notknow what they have not been told, but they will not even notice how much hasbeen left out (fischhoff et al., 1978a). what happens in the long run depends onwhether the unmentioned risks are revealed by experience or by other sources ofinformation. when deliberate omissions are detected, the responsible party islikely to lose all credibility. once a shadow of doubt has been cast, it is hard toerase.people may disagree more about what risk is thanabout how large it isgiven this mixture of strengths and weaknesses in the psychologicalprocesses that generate people's risk perceptions, there is no simple answer to thequestion ﬁhow much do people know and understand?ﬂ the answer depends onthe risks and on the opportunities that people have to learn about them.one obstacle to determining what people know about specific risks isdisagreement about the definition of risk. (see sections ii and iii for morecomplete discussions of different possible definitions of risk and other terms.) iflaypeople and risk managers use the term risk differently, then they can agree onthe facts about a specific technology but still disagree about its degree ofriskiness. several years ago, the idea circulated in the nuclear power industry thattheappendix c302improving risk communicationcopyright national academy of sciences. all rights reserved.public cared much more about multiple deaths from large accidents than aboutequivalent numbers of casualties resulting from a series of small accidents. if thisassumption were valid, then the industry would be strongly motivated to removethe threat of such large accidents. if removing the threat proved impossible, thenthe industry could argue that a death is a death and that in formulating socialpolicy it is totals that matter, not whether deaths occur singly or collectively.there were never any empirical studies to determine whether this was reallyhow the public defined risk. subsequent studies, though, have suggested thatwhat bothers people about catastrophic accidents is the perception that atechnology capable of producing such accidents cannot be very well understoodor controlled (slovic et al., 1984). from an ethical point of view, worrying aboutthe uncertainties surrounding a new and complex technology such as nuclearpower is quite a different matter than caring about whether a fixed number oflives are lost in one large accident rather than in many small accidents.people have difficulty detecting inconsistencies inrisk disputesdespite their frequent intensity, risk debates are typically conducted at adistance (hance et al., 1988; mazur, 1973). the disputing parties operate withinselfcontained communities and talk principally to themselves. opponents areseen primarily through their writing or their posturing at public events. thus,there is little opportunity for the sort of subtle probing needed to discover basicdifferences in how the protagonists think about important issues, such as themeaning of key terms or the credibility of expert testimony. as a result, it is easyto misdiagnose one another's beliefs and concerns.the opportunities for misunderstanding increase when the circumstances ofdebate restrict candor. for example, some critics of nuclear power actuallybelieve that the technology can be operated with reasonable safety. however, theyoppose it because they believe that its costs and benefits are distributedinequitably. although they might like to discuss these issues, critics find thatpublic hearings about risk and safety often provide them with their only forumfor venting their concern. if they oppose the technology, then they areappendix c303improving risk communicationcopyright national academy of sciences. all rights reserved.forced to do so on safety grounds, even if this means misrepresenting theirperceptions of the actual risk.individuals also have difficulty detecting inconsistencies in their own beliefsor realizing how simple reformulations would change their perspective on issues.for example, most people would prefer a gamble with a 25 percent chance oflosing $200 (and a 75 percent chance of losing nothing) to a gamble with a sureloss of $50. most of the same people would also buy a $50 insurance policy toprotect against such a loss. what they will do depends on whether the $50 isdescribed as a sure loss or as an insurance premium. as a result, one cannotpredict how people will respond to an issue without knowing how they willperceive it, which depends, in turn, on how it will be presented to them bymerchandisers, politicians, or the media.thus, people's insensitivity to the importance of how risk issues arepresented exposes them to manipulation. for example, a risk might seem muchworse when described in relative terms than in absolute terms (e.g., doublingtheir risk versus increasing that risk from 1 in a million to 1 in a half million).although both representations of the risk might be honest, their impacts would bequite different. perhaps the only fair approach is to present the risk from bothperspectives, letting recipients determine which one (or which hybrid) bestrepresents their world view.summarythese statements (and others like them cited elsewhere in this appendix)reduce both complex people and intricate research literatures to necessarilyoversimplified summaries. neither the people nor the literature can be readwithout their appropriate context. much of section ii discussed the intricacies ofthe literature and the sort of conclusions than might be extracted from it. much ofthis whole appendix concerns the context for risk perception. ideally, one wouldhave polished studies of how specific people respond to specific risks, either inmessages or in the flesh (or the metal). those should be the standards fordesigning and evaluating risk communication programs. in lieu of such studies,such principles are all that we have to go on. they are the stuff of everydayexplanations of behavior. they can be enriched, refined, and (sometimes)disqualified by behavioral research.appendix c304improving risk communicationcopyright national academy of sciences. all rights reserved.vii conclusionindividual learningmaking decisions about risks is often complex, whether done individually oras part of a larger socialpolitical process. so is dealing with many of life's otherdecisions, even without obvious risks to health and safety (e.g., choosing acareer, a partner, an anniversary present). all these decisions have sets of optionsto consider, bodies of fact to master, and competing objectives to weigh. addingto the complexity of these individual decisions is the fact that each of usconfronts so many of themšeach with its own details and nuances.individually and collectively, these decisions present a daunting challenge toidentify those courses of action that are in our own best interests. it should not besurprising if people sometimes feel overwhelmed by the panoply of risks thrownat them, sometimes seem to respond suboptimally, and sometimes get angry atthose who force them to deal with yet another riskševen if it is associated with atechnology bringing considerable benefit.however, although the substance of these decisions may vary enormously,their common elements mean that there is an opportunity for learning somegeneral lessons from this experience with diverse risks. so, even though fewpeople receive formal training in decisionmaking methods, life itself can providean education. people could not make it through life if they had not learnedsomething about the relative riskiness of different activities (e.g., driving at nightversus driving during the day, getting polio from vaccine versus getting it whileunvaccinated, storing household chemicals under the sink versus storing them outof the reach of children). people would be perennially dissatisfied if they had notacquired some ability to understand and predict their own tastes. a representativedemocracy could not function if people did not have some ability to evaluate thecandor and competence of political candidates and governmental officials. therewould not be significant declines in smoking and fat consumption if people werenot able to extract personally relevant implications from risk communications.some of these accomplishments are documented in the references cited inthe preceding sections. most are also common knowledge (although perhaps notas precisely delineated as they can be in systematic research). most are alsoincomplete. both anecdotalappendix c305improving risk communicationcopyright national academy of sciences. all rights reserved.and systematic observations can point to places where people misestimate risks,mistake their own needs, misjudge public figures, or misinterpret the message ofrisk communications. in some cases, this is because life is not structured forlearning. it may not provide people with prompt, immediate feedback on howwell they are doing. it may discourage them from admitting the need to learn(without which even the sharpest feedback may have little value).under these circumstances, a guide like this can facilitate learning in severalways. one is to provide a structure for thinking about risk controversies, so as tofacilitate identifying common elements and extracting general lessons. a secondis to summarize the lessons found in the research literature and in the pooledexperience of risk communicators (and communicants). in some cases, theselessons will confirm readers' expectations; in others, they will suggest alternativeinterpretations; in still others, they will raise issues that have not beenconsidered. a third way is to provide annotated references to the researchliterature that could be consulted for more detailed treatment of specific riskissues. making this research generally available in nontechnical terms can help tolevel the playing field, by granting equal access to it for all parties to riskcontroversies (and not just for those parties with staffs paid to follow the researchliterature).finally, such a guide can provide some insight into the psychologicalprocesses of the parties involved in risk controversies. that insight can be useddirectively, by those who must design risk communications and interpret theresponses of the public to them. it can also be used reflectively, by those whowish to clarify the psychological limits to their own participation in riskmanagement. these groups include nontechnical people concerned aboutinterpreting the nature of risks, as well as technical people concerned aboutmaking themselves understood to others.such understanding has both a ﬁcognitiveﬂ and a ﬁmotivationalﬂ component(to use psychological jargon for a moment). that is, it involves both how peoplethink and how people feel. deciphering scientific communications can becomplicated both by difficulty interpreting strange terms or unfamiliar units (e.g.,very small probabilities) and by difficulty coping with one's anger with the riskcommunicators (e.g., for their perceived insensitivity or vested interests).designing such communications can be complicated both by difficultyinterpreting complex social processes and by difficultyappendix c306improving risk communicationcopyright national academy of sciences. all rights reserved.managing one's frustration at being mistrusted and disbelieved. better riskcommunication is typically thought of as a largely cognitive enterprise, focusedon conveying factual material more comprehensibly. accomplishing that goalrequires an understanding of what aspects of risk conflicts really hinge onscientific facts. if it can be accomplished, then risk conflicts can be focused onareas of legitimate disagreement, without the confusion and frustration generatedby the receipt of incomprehensible messages. such messages both blur the issuesand create the feeling that communicators care so littleš or live in such adifferent worldšthat they cannot communicate in ways that address recipients'needs.societal learningsweeping statements about people and society are easy to make, but hard tosubstantiate. if i were to chance a summary of personal observations from 15years of working on this topic, it would be that there is increasing sophisticationon the part of all concerned. we have better risk science than we had in the pastand a better understanding of its limits. we have increasing understanding amongrisk managers of the need to take public concerns seriously when designing riskpolicies and among members of the public when deciding which risks to worryabout and how to worry about them. we have increasing professionalism inreporting about risk issues and increasing ability to read or view risk stories with adiscerning eye.we also have, however, a long way to go in each of these respects.moreover, the learning to date has come at a price that creates an obstacle tofuture progress. people remember their own past mistakes (at least the moreobvious ones), which makes them hesitant about future actions. they alsoremember others' mistakes (at least those from which they think they havesuffered), which makes them leery of those others' future actions. it is hard toerase a shadow of doubt or undo the undue impact of first impressions.as in a social relationship, by the time those involved learn how to get alongwith a significant other, they may have hurt one another enough that they cannotapply these lessons in that relationship. unfortunately, industry cannot break offits relationship with its current public (or its current government or currentmedia) and start up with a new, more enlightened one. so, some personal woundsneed to heal at the same time as we are collectively addressing new problems.appendix c307improving risk communicationcopyright national academy of sciences. all rights reserved.in addition, old problems continue to aggravate these wounds and toundermine the parties' faith in one another. for example, the question of whetherto complete or operate many nuclear reactors is a lingering source of mutualfrustration among all involved. the public commitments made by the variousparties concerned are such that the conflicts have a life of their own. they maydefy reasoned resolution and be almost refractory to the addition of scientificevidence. the strategizing and posturing of the parties may make great sensewhen viewed as part of a political struggle. yet when viewed as part of adisciplined debate over risks and benefits, they can strengthen perceptions of acallous industry and hysterical public.a guide such as this cannot dispel such complex conflicts and emotions.they are natural and legitimate parts of life. it can, however, help to put them inperspective, leaving the conflicts that remain better focused and more productive.appendix c308improving risk communicationcopyright national academy of sciences. all rights reserved.bibliographyalfidi, j. 1971. informed consent: a study of patient reaction. journal of the americanmedical association 216:1325œ1329.appelbaum, p.s., c.w.lidz, and a.meisel. 1987. informed consent: legal theory andclinical practice. new york: oxford university press.appelbaum, r.p. 1977. the future is made, not predicted: technocratic planners vs. publicinterests. society (may/june):49œ53.applied management sciences. 1978. survey of consumer perceptions of patient packageinserts for oral contraceptives. ntis no. pb248œ740. washington, d.c.: appliedmanagement sciences.armstrong, j.s. 1975. tom swift and his electric regression analysis machine 1973.psychological reports 36:806.atkinson, r.c., r.j.herrnstein, g.lindzey, and r.d.luce. 1988. stevens' handbook ofexperimental psychology. new york: wiley interscience.barhillel, m. 1973. on the subjective probability of compound events. organizationalbehavior and human performance 9:396œ406.barhillel, m. 1980. the base rate fallacy in probability judgment. acta psychologica44:211œ233.barber, w.c. 1979. controversy plagues setting of environmental standards. chemical andengineering news 57(17):34œ37.barraclough, g. 1972. mandarins and nazis. new york review of books 19(6):37œ42.bazelon, d.l. 1979. risk and responsibility. science 205(4403):277œ280.bentkover, j.d., v.t.covello, and j.mumpower, eds. 1985. benefits assessment: thestate of the art. dordrecht, holland: d.reidel.berkson, j., t.b.magath, and m.hurn. 1939œ1940. the error of estimate of the blood cellcount as made with the hemocytometer. american journal of physiology 128:309œ323.beythmarom, r. 1982. how probable is probable? journal of forecasting 1:257œ269.bick, t., c.hohenemser, and r.w.kates. 1979. target: highway risks. environment 21(2):7œ15, 29œ38.bickerstaffe, j., and d.peace. 1980. can there be a consensus on nuclear power? socialstudies of science 10:309œ344.bradburn, n.m., and s.sudman. 1979. improving interview method and questionnairedesign. san francisco: josseybass.brokensha, d.w., d.m.warren, and o.werner. 1980. indigenous knowledge: systemsand development. lanham, md.: university press of america.brookshire, d.s., b.c.ives, and w.d.schulze. 1976. the valuation of aestheticpreferences. journal of environmental economics and management 3:325œ346.brown, r. 1965. social psychology. glencoe, ill.: free press.burton, i., r.w.kates, and g.f.white. 1978. the environment as hazard. new york:oxford university press.callen, e. 1976. the science court. science 193:950œ951.campbell, d.t. 1975. degrees of freedom and the case study. comparative politicalstudies 8:178œ193.appendix c309improving risk communicationcopyright national academy of sciences. all rights reserved.campbell, d.t., and a.erlebacher. 1970. how regression artifacts in quasiexperimentalevaluations can mistakenly make compensatory education look harmful. incompensatory education: a national debate, vol. 3, disadvantaged child,j.hellmuth, ed. new york: brunner/mazel.campen, j. 1985. benefitcost and beyond. cambridge, mass.: ballinger.carterette, e.c., and m.p.friedman. 1974. handbook of perception, vol. 2. new york:academic press.chapman, l.j., and j.p.chapman. 1969. illusory correlation as an obstacle to the use ofvalid psychodiagnostic signs. journal of abnormal psychology 74:271œ280.chemical and engineering news. 1980. a look at human error. 58(18):82.cohen, b., and i.lee. 1979. a catalog of risks. health physics 36:707œ722.cohen, j. 1962. the statistical power of abnormalsocial psychological research: areview. journal of abnormal and social psychology 65(3):145œ153.commoner, b. 1979. the politics of energy. new york: knopf.conn, w.d., ed. 1983. energy and material resources. boulder, colo.: westview.cotgrove, a. 1982. catastrophe or cornucopia? the environment, politics and the future.new york: john wiley & sons.covello, v.t., p.m.sandman, and p.slovic. 1988. risk communication, risk statistics,and risk comparisons: a manual for plant managers. washington, d.c.: chemicalmanufacturers association.covello, v., d.von winterfeldt, and p.slovic. 1986. risk communication: a review of theliterature. risk abstracts 3(4):171œ182.crask, m.r., and w.d.parreault, jr. 1977. validation of discriminant analysis in marketingresearch. journal of marketing research 14:60œ68.crouch, e.a.c., and r.wilson. 1982. risk/benefit analysis. cambridge, mass.:ballinger.cummings, r.g., d.s.brookshire, and w.d.schulze, eds. 1986. valuing environmentalgoods: an assessment of the contingent valuation method. totowa, n.j.: rowman& allanheld.davidshofer, i.o. 1976. risktaking and vocational choice: reevaluation. journal ofcounseling psychology 23:151œ154.davis, j. 1969. group performance. reading, mass.: addisonwesley.dietz, t.m., and r.w.rycroft. 1987. the risk professionals. washington, d.c.: russellsage foundation.doern, g.b. 1978. science and technology in the nuclear regulatory process: the case ofcanadian uranium miners. canadian public administration 21:51œ82.dreman, d. 1979. contrarian investment strategy. new york: random house.driver, b., g.peterson, and r.gregory, eds. 1988. evaluative amenity resources. newyork: venture.dunlap, t.r. 1978. science as a guide in regulating technology: the case of ddt in theunited states. social studies of science 8:265œ285.eiser, j.r., ed. 1982. social psychology and behavioral medicine. new york: john wiley& sons.elliot, g.r., and c.eisdorfer. 1982. stress and human health. new york: springerverlag.appendix c310improving risk communicationcopyright national academy of sciences. all rights reserved.fairley, w.b. 1977. evaluating the ﬁsmallﬂ probability of a catastrophic accident from themarine transportation of liquefied natural gas. in statistics and public policy,w.b.fairley and f.mosteller, eds. reading, mass.: addisonwesley.feller, w. 1968. an introduction to probability theory and its applications, 3d ed., vol.1. new york: john wiley & sons.fineberg, h.v. 1988. education to prevent aids: prospects and obstacles. science 239(4840):592œ596.fischer, d.h. 1970. historians' fallacies. new york: harper & row.fischhoff, b. 1980. for those condemned to study the past: reflections on historicaljudgment . in new directions for methodology of behavior science: falliblejudgment in behavioral research, r.a.shweder and d. w.fiske, eds. san francisco:josseybass.fischhoff, b. 1981. informed consent for transient nuclear workers. in equity issues innuclear waste management, r.kasperson and r.w.kates, eds. cambridge, mass.:oelgeschlager, gunn and hain.fischhoff, b. 1983. ﬁacceptable riskﬂ: the case of nuclear power. journal of policyanalysis and management 2(4):559œ575.fischhoff, b. 1984. setting standards: a systematic approach to managing public healthand safety risks. management science 30:823œ843.fischhoff, b. 1985a. managing risk perceptions. issues in science and technology 2(1):83œ96.fischhoff, b. 1985b. protocols for environmental reporting: what to ask the experts. thejournalist (winter):11œ15.fischhoff, b. 1985c. risk analysis demystified. ncap news (winter):30œ33.fischhoff, b. 1987. treating the public with risk communications: a public healthperspective . science, technology, and human values 12:3œ19.fischhoff, b. 1988. judgment and decision making. in the psychology of humanthought, r.j.sternberg and e.e.smith, eds. new york: cambridge university press.fischhoff, b., and l.a.cox, jr. 1985. conceptual framework for regulatory benefitsassessment. in benefits assessment: the state of the art, j.d. bentkover,v.t.covello, and j.mumpower, eds. dordrecht, holland: d. reidel.fischhoff, b., and l.furby. 1988. measuring values: a conceptual framework forinterpretive transactions with special reference to contingent valuation of visibility.journal of risk and uncertainty 1:147œ184.fischhoff, b., and d.macgregor. 1983. judged lethality: how much people seem to knowdepends upon how they are asked. risk analysis 3:229œ236.fischhoff, b., and o.svenson. 1987. perceived risks of radionuclides: understandingpublic understanding. in radionuclides in the food chain, g. schmidt, ed. newyork: praeger.fischhoff, b., l.furby, and r.gregory. 1987. evaluating voluntary risks of injury.accident analysis and prevention 19(1):51œ62.fischhoff, b.s., lichtenstein, p.slovic, s.l.derby, and r.l.keeney. 1981. acceptablerisk. new york: cambridge university press.fischhoff, b., p.slovic, and s.lichtenstein. 1978. fault trees: sensitivity of assessedfailure probabilities to problem representation. journal of experimental psychology:human perception and performance 4:330œ344.appendix c311improving risk communicationcopyright national academy of sciences. all rights reserved.fischhoff, b., p.slovic, and s.lichtenstein. 1980. knowing what you want: measuringlabile values. in cognitive processes in choice and decision behavior, t.wallsten,ed. hillsdale, n.j.: erlbaum.fischhoff, b., p.slovic, and s.lichtenstein. 1981. lay foibles and expert fables injudgments about risk. in progress in resource management and environmentalplanning, t.o'riordan and r.k.turner, eds. new york: john wiley & sons.fischhoff, b., p.slovic, s.lichtenstein, s.read, and b.combs. 1978. how safe is safeenough? a psychometric study of attitudes towards technological risks and benefits.policy sciences 9:127œ152.fischhoff, b., s.r.watson, and c.hope. 1984. defining risk. policy sciences 17:123œ129.fiske, s., and s.taylor. 1984. social cognition. reading, mass.: addisonwesley.frankel, c. 1974. the rights of nature. in when values conflict, c.schelling, j.voss, andl.tribe, eds. cambridge, mass.: ballinger.friedman, s.m. 1981. blueprint for breakdown: three mile island and the media beforethe accident. journal of communication 31:116œ129.furby, l., and b.fischhoff. in press. rape selfdefense strategies: a review of theireffectiveness. victimology.gamble, d.j. 1978. the berger inquiry: an impact assessment process. science 199(4332):946œ951.gilovich, t., r.vallone, and a.tversky. 1985. the hot hand in basketball: on themisperception of random sequences. cognitive psychology 17:295œ314.gotchy, r.l. 1983. health risks from the nuclear fuel cycle. in health risks of energytechnologies, c.c.travis and e.l.etnier, eds. boulder, colo.: westview.green, a.e., and a.j.bourne. 1972. reliability technology. new york: wileyinterscience.hackney, j.d., and w.s.linn. 1984. human toxicology and risk assessment. in handbookon risk assessment. washington, d.c.: national science foundation.hammond, k.r., and l.adelman. 1976. science, values and human judgment. science194:389œ396.hance, b.j., c.chess, and p.m.sandman. 1988. improving dialogue with communities: arisk communication manual for government. trenton: division of science andresearch risk communication unit, new jersey department of environmentalprotection.handler, p. 1980. public doubts about science. science 208(4448):1093.hanley, j. 1980. the silence of scientists. chemical and engineering news 58(12):5.harris, l. 1980. risk in a complex society. public opinion survey conducted for marsh andmclennan companies, inc.harriss, r., and c.hohenemser. 1978. mercury: measuring and managing risk.environment 20(9).hasher, l., and r.t.zacks. 1984. automatic and effortful processes in memory. journal ofexperimental psychology: general 108:356œ388.henrion, m., and b.fischhoff. 1986. assessing uncertainty in physical constants.american journal of physics 54(9):791œ798.henshel, r.l. 1975. effects of disciplinary prestige on predictive accuracy: distortionsfrom feedback loops. futures 7:92œ196.appendix c312improving risk communicationcopyright national academy of sciences. all rights reserved.herbert, j.h., l.swanson, and p.reddy. 1979. a risky business. environment 21(6):28œ33.hershey, j.c., and p.j.h.schoemaker. 1980. risk taking and problem context in thedomain of losses: an expected utility analysis. journal of risk and insurance47:111œ132.hirokawa, r.y., and m.s.poole. 1986. communication and group decison making.beverly hills, calif.: sage.hohenemser, k.h. 1975. the failsafe risk. environment 17(1):6œ10.holden, c. 1980. love canal residents under stress. science 208:1242œ1244.hovland, c.i., i.l.janis, and h.h.kelley. 1953. communication and persuasion:psychological studies of opinion change. new haven, conn.: yale universitypress.hynes, m., and e.vanmarcke. 1976. reliability of embankment performance prediction. inproceedings of the asce engineering mechanics division specialty conference.waterloo, ontario, canada: university of waterloo press.ingram, m.j., d.j.underhill, and t.m.l.wigley. 1978. historical climatology. nature276:329œ334.inhaber, h. 1979. risk with energy from conventional and nonconventional sources.science 203(4382):718œ723.institute of medicine. 1986. confronting aids: directions for public health, health care,and research. washington, d.c.: national academy press.james, w. 1988. baseball abstract. new york: ballantine.janis, i.l., ed. 1982. counseling on personal decisions. new haven, conn.: yaleuniversity press.jennergren, l.p., and r.l.keeney. 1982. risk assessment. in handbook of appliedsystems analysis. laxenburg, austria: international institute of applied systemsanalysis.johnson, b.b., and v.t.covello, eds. 1987. the social and cultural construction of risk:essays on risk selection and perception. dordrecht, holland: d.reidel.joksimovich, v. 1984. models in risk assessment for hazard characterization. in handbookof risk assessment. washington, d.c.: national science foundation.joubert, p., and l.lasagna. 1975. commentary: patient package inserts. clinicalpharmacology and therapeutics 18(5):507œ513.kadlec, r. 1984. field and laboratory event investigation for hazard characterization. inhandbook of risk assessment. washington, d.c.: national science foundation.kahneman, d., and a.tversky. 1972. subjective probability: a judgment ofrepresentativeness. cognitive psychology 3:430œ454.kasperson, r. 1986. six propositions on public participation and their relevance for riskcommunication. risk analysis 6(3):275œ281.keeney, r.l. 1980. siting energy facilities. new york: academic press.keeney, r.l., and h.raiffa. 1976. decisions with multiple objectives: preferences andvalue tradeoffs. new york: john wiley & sons.kolata, g.b. 1980. love canal: false alarm caused by botched study. science 208(4449):1239œ1242.koriat, a., s.lichtenstein, and b.fischhoff. 1980. reasons for confidence. journal ofexperimental psychology: human learning and memory 6:107œ 118.appendix c313improving risk communicationcopyright national academy of sciences. all rights reserved.krohn, w., and p.weingart. 1987. commentary: nuclear power as a social experimentšeuropean political ﬁfalloutﬂ from the chernobyl meltdown. science, technology,and human values 12(2):52œ58.kunce, j.t., d.w.cook, and d.e.miller. 1975. random variables and correlationaloverkill. educational and psychological measurement 35:529œ 534.kunreuther, h., r.ginsberg, l.miller, p.sagi, p.slovic, b.borkan, and n.katz. 1978. disaster insurance protection. new york: john wiley & sons.lachman, r., j.t.lachman, and e.c.butterfield. 1979. cognitive psychology andinformation processing. hillsdale, n.j.: erlbaum.lakatos, i. 1970. falsification and scientific research programmes. in criticism and thegrowth of scientific knowledge, i.lakatos and a.musgrave, eds. new york:cambridge university press.lanir, z. 1982. strategic surprises. tel aviv, israel: hakibbutz hameuchad.lave, l.b. 1978. ambiguity and inconsistency in attitudes toward risk: a simple model.pp. 108œ114 in proceedings of the society for general systems research annualmeeting. louisville, ky.: society for general systems research.lawless, e.w. 1977. technology and social shock. new brunswick, n.j.: rutgersuniversity press.lazarsfeld, p. 1949. the american soldieršan expository review. public opinionquarterly 13:377œ404.levine, m. 1974. scientific method and the adversary model: some preliminary thoughts.american psychologist 29:661œ716.lichtenstein, s., and b.fischhoff. 1980. training for calibration. organizational behaviorand human performance 26:149œ171.lichtenstein, s., b.fischhoff, and l.d.phillips. 1982. calibration of probabilities: thestate of the art. in judgment under uncertainty: heuristics and biases, p.slovic anda.tversky, eds. new york: cambridge university press.lichtenstein, s., p.slovic, b.fischhoff, m.layman, and b.combs. 1978. judged frequencyof lethal events. journal of experimental psychology: human learning and memory4:551œ578.lindman, h.g., and w.edwards. 1961. supplementary report: unlearning the gambler'sfallacy. journal of experimental psychology 62:630.linville, p., b.fischhoff, and g.fischer. 1988. judgments of aids risks. pittsburgh, pa.:carnegiemellon university, department of social and decision sciences.maclean, d. 1987. understanding the nuclear power controversy. in scientificcontroversies: case studies in the resolution and closure of disputes in science andtechnology, h.t.engelhardt, jr., and a.l.caplan, eds. new york: cambridgeuniversity press.markovic, m. 1970. social determinism and freedom. in mind, science and history,h.e.keifer and m.k.munitz, eds. albany: state university of new york press.martin, e. 1980. surveys as social indicators: problems in monitoring trends. chapelhill: institute for research in social science, university of north carolina.mazur, a. 1973. disputes between experts. minerva 11:243œ262.appendix c314improving risk communicationcopyright national academy of sciences. all rights reserved.mazur, a. 1981. the dynamics of technical controversy. washington, d.c.:communications press.mazur, a., a.a.marino, and r.o.becker. 1979. separating factual disputes from valuedisputes in controversies over technology. technology in society 1:229œ237.mcgrath, p.e. 1974. radioactive waste management: potentials and hazards from a riskpoint of view. report eur fnr1204 (kfk 1992). karlsruhe, west germany: useuratom fast reactor program.mcneil, b.j., r.weichselbaum, and s.g.pauker. 1978. the fallacy of the 5year survivalrate in lung cancer. new england journal of medicine 299:1397œ1401.morgan, m. 1986. conflict and confusion: what rape prevention experts are tellingwomen. sexual coercion and assault 1(5):160œ168.murphy, a.h., and b.g.brown. 1983. forecast terminology: composition andinterpretation of public weather forecasts. bulletin of the american meteorologicalsociety 64:13œ22.murphy, a.h., and r.l.winkler. 1984. probability of precipitation forecasts. journal ofthe american statistical association 79:391œ400.national research council. 1976. surveying crime. washington, d.c.: national academypress.national research council. 1982. survey measure of subjective phenomena.washington, d.c.: national academy press.national research council. 1983a. priority mechanisms for toxic chemicals.washington, d.c.: national academy press.national research council. 1983b. risk assessment in the federal government:managing the process. washington, d.c.: national academy press.nelkin, d. 1977. technological decisions and democracy. beverly hills, calif.: sage.nelkin, d., ed. 1984. controversy: politics of technical decisions. beverly hills, calif.:sage.neyman, j. 1979. probability models in medicine and biology: avenues for theirvalidation for humans in real life. berkeley: university of california, statisticallaboratory.nisbett, r.e., and l.ross. 1980. human inference: strategies and shortcomings of socialjudgment. englewood cliffs, n.j.: prenticehall.northwest coalition for alternatives to pesticides. 1985. position documentš riskanalysis . ncap news (winter):33.office of science and technology policy. 1984. chemical carcinogens: review of thescience and its associated principles. federal register 49(100):21594œ 21661.o'flaherty, e.j. 1984. pharmacokinetic methods in risk assessment. in handbook of riskassessment. washington, d.c.: national science foundation.o'leary, m.k., w.d.coplin, h.b.shapiro, and d.dean. 1974. the quest for relevance.international studies quarterly 18:211œ237.östberg, g., h.hoffstedt, g.holm, b.klingernstierna, b.rydnert, v.samsonowitz, andl.sjöberg. 1977. inconceivable events in handling material in heavy mechanicalengineering industry. stockholm, sweden: national defense research institute.otway, h.j., and d.von winterfeldt. 1982. beyond acceptable risk: on the socialacceptability of technologies. policy sciences 14:247œ256.appendix c315improving risk communicationcopyright national academy of sciences. all rights reserved.page, t. 1978. a generic view of toxic chemicals and similar risks. ecology lawquarterly 7:207œ243.page, t. 1981. a framework for unreasonable risk in the toxic substances control act. incarcinogenic risk assessment, r.nicholson, ed. new york: new york academy ofsciences.parducci, a. 1974. contextual effects: a rangefrequency analysis. in handbook ofperception, vol. 2, e.c.carterette and m.p.friedman, eds. new york: academicpress.payne, s.l. 1952. the art of asking questions. princeton, n.j.: princeton universitypress.pearce, d.w. 1979. social costbenefit analysis and nuclear futures. in energy riskmanagement, g.t.goodman and w.d.rowe, eds. new york: academic press.peterson, c.r., and l.r.beach. 1967. man as an intuitive statistician. psychologicalbulletin 69(1):29œ46.peto, r. 1980. distorting the epidemiology of cancer. nature 284:297œ300.pew, r.d., c.miller, and c.e.feeher. 1982. evaluation of proposed control roomimprovements through analysis of critical operator decisions. palo alto, calif.:electric power research institute.pinder, g.f. 1984. groundwater contaminant transport modeling. environmental scienceand technology 18(4):108a114a.poulton, e.c. 1968. the new psychophysics: six models of magnitude estimation.psychological bulletin 69:1œ19.poulton, e.c. 1977. quantitative subjective assessments are almost always biased,sometimes completely misleading. british journal of psychology 68:409œ421.president's commission on the accident at three mile island. 1979. report of thepresident's commission on the accident at three mile island. washington, d.c.:u.s. government printing office.rayner, s., and r.cantor. 1987. how fair is safe enough?: the cultural approach tosocietal technology choice. risk analysis 7(1):3œ9.reissland, j., and v.harries. 1979. a scale for measuring risks. new scientist 83:809œ811.rodricks, j.v., and r.g.tardiff. 1984. animal research methods for doseresponseassessment. in handbook of risk assessment. washington, d.c.: national sciencefoundation.rokeach, m. 1973. the nature of human values. new york: the free press.roling, g.t., l.w.pressgrove, e.b.keefe, and s.b.raffin. 1977. an appraisal of patients'reactions to ﬁinformed consentﬂ for peroral endoscopy. gastrointestinal endoscopy24(2):69œ70.rosencranz, a., and g.s.wetstone. 1980. acid precipitation: national and internationalresponses. environment 22(5):6œ20, 40œ41.rosenthal, r., and r.l.rosnow. 1969. artifact in behavioral research. new york:academic press.rothman, s., and s.r.lichter. 1987. elite ideology and risk perception in nuclear energypolicy. american political science review 81(2):383œ404.rothschild, n.m. 1978. rothschild: an antidote to panic. nature 276:555.rubin, d., and d.sachs, eds. 1973. mass media and the public. new york: praeger.schnaiburg, a. 1980. the environment: from surplus to scarcity. new york: oxforduniversity press.appendix c316improving risk communicationcopyright national academy of sciences. all rights reserved.schneider, s.h., and l.e.mesirow. 1976. the genesis strategy. new york: plenum.schneiderman, m.a. 1980. the uncertain risks we run: hazardous material. in societalrisk assessment: how safe is safe enough?, r.c.schwing and w.a.albers, jr., eds.new york: plenum.schudson, m. 1978. discovering the news. new york: basic books.schwarz, e.d. 1978. the use of a checklist in obtaining informed consent for treatmentwith medicare. hospital and community psychiatry 29:97œ100.seligman, m.e.p. 1975. helplessness. san francisco: freeman, cooper.shaklee, h., b.fischhoff, and l.furby. 1988. the psychology of contraceptive surprises:cumulative risk and contraceptive failure. eugene, oreg.: eugene research institute.sharlin, h.i. 1987. macrorisks, microrisks, and the media: the edb case. in the socialand cultural construction of risk, b.b.johnson and v.t. covello, eds. dordrecht,holland: d.reidel.sheridan, t.b. 1980. human error in nuclear power plants. technology review 82(4):23œ33.shroyer, t. 1970. toward a critical theory for advanced industrial society. in recentsociology, vol. 2, patterns of communicative behavior, h.p. drietzel, ed. london:macmillan.sioshansi, f.p. 1983. subjective evaluation using expert judgment: an application. ieeetransactions on systems, man and cybernetics 13(3):391œ397.sjöberg, l. 1979. strength of belief and risk. policy sciences 11:539œ573.slovic, p. 1962. convergent validation of risktaking measures. journal of abnormal andsocial psychology 65:68œ71.slovic, p. 1986. informing and educating the public about risk. risk analysis 6(4):403œ415.slovic, p., and b.fischhoff. 1977. on the psychology of experimental surprises. journal ofexperimental psychology: human perception and performance 3:544œ551.slovic, p., and b.fischhoff. 1983. how safe is safe enough? determinants of perceivedand acceptable risk. in too hot to handle? social and policy issues in themanagement of radioactive wastes, c.walker, l.gould, and e.woodhouse, eds.new haven, conn.: yale university press.slovic, p., b.fischhoff, and s.lichtenstein. 1978. accident probabilities and seatbeltusage: a psychological perspective. accident analysis and prevention 17:10œ19.slovic, p., b.fischhoff, and s.lichtenstein. 1979. rating the risks. environment 21:14œ20, 30, 36œ39.slovic, p., b.fischhoff, and s.lichtenstein. 1980. facts vs. fears: understanding perceivedrisk. in societal risk assessment: how safe is safe enough?, r.schwing andw.a.albers, jr., eds. new york: plenum.slovic, p., b.fischhoff, and s.lichtenstein. 1984. modeling the societal impact of fatalaccidents . management science 30:464œ474.slovic, p., b.fischhoff, s.lichtenstein, b.corrigan, and b.combs. 1977. preference forinsuring against probable small losses: implications for the theory and practice ofinsurance. journal of risk and insurance 44:237œ258.smith, v.k., and w.h.desvousges. 1986. measuring water quality benefits. boston:kluwer.stallen, p.j. 1980. risk of science or science of risk? in society, technology and riskassessment, j.conrad, ed. london: academic press.appendix c317improving risk communicationcopyright national academy of sciences. all rights reserved.starr, c. 1969. social benefit versus technological risk. science 165:1232œ1238.svenson, o. 1981. are we all less risky and more skillful than our fellow drivers? actapsychologica 47:143œ148.svenson, o., and b.fischhoff. 1985. levels of environmental decisions. journal ofenvironmental psychology 5:55œ67.thompson, m. 1980. aesthetics of risk: culture or context. in societal risk assessment,r.c.schwing and w.a.albers, jr., eds. new york: plenum.tockman, m.s., and a.m.lilienfeld. 1984. epidemiological methods in risk assessment. inhandbook of risk assessment. washington, b.c.: national science foundation.travis, c.c. 1984. modeling methods for exposure assessment. in handbook of riskassessment. washington, d.c.: national science foundation.tribe, l.h. 1972. policy science: analysis or ideology? philosophy and public affairs2:66œ110.tukey, j.w. 1977. some thoughts on clinical trials, especially problems of multiplicity.science 198:679œ690.tulving, e. 1972. episodic and semantic memory. in organization of memory, e.tulvingand w.donaldson, eds. new york: academic press.turner, c.f. 1980. surveys of subjective phenomena. in the measurement of subjectivephenomena, d.johnston, ed. washington, d.c.: u.s. government printing office .turner, c.f., and e.martin, eds. 1985. surveying subjective phenomena, vols. 1 and 2.new york: russell sage foundation.tversky, a., and d.kahneman. 1971. the belief in the ﬁlaw of small numbers.ﬂpsychological bulletin 76:105œ110.tversky, a., and d.kahneman. 1973. availability: a heuristic for judging frequency andprobability. cognitive psychology 5:207œ232.tversky, a., and d.kahneman. 1974. judgment under uncertainty: heuristics and biases.science 185:1124œ1131.tversky, a., and d.kahneman. 1981. the framing of decisions and the psychology ofchoice. science 211(4481):453œ458.u.s. committee on government operations. 1978. teton dam disaster. washington,d.c.: government printing office.u.s. government. 1975. hearings, 94th cong., 1st sess. browns ferry nuclear plant fire,september 16, 1975. washington, d.c.: u.s. government printing office.u.s. nuclear regulatory commission. 1975. reactor safety study: an assessment ofaccident risks in u.s. commercial nuclear power plants. wash 1400(nureg75/014). washington, d.c.: u.s. nuclear regulatory commission.u.s. nuclear regulatory commission. 1978. risk assessment review group to the u.s.nuclear regulatory commission. nureg/cr0400. washington, d.c.: u.s.nuclear regulatory commission.u.s. nuclear regulatory commission. 1982. safety goals for nuclear power plants: adiscussion paper. nureg0880. washington, d.c.: u.s. nuclear regulatorycommission.u.s. nuclear regulatory commission. 1983. pra procedures guide. nureg/ cr2300.washington, d.c.: u.s. nuclear regulatory commission.vlek, c.a.j., and p.j.stallen. 1980. rational and personal aspects of risk. actapsychologica 45:273œ300.appendix c318improving risk communicationcopyright national academy of sciences. all rights reserved.vlek, c.a.j., and p.j.stallen. 1981. judging risks and benefits in the small and in thelarge. organizational behavior and human performance 28:235œ271.von winterfeldt, d., r.s.john, and k.borcherding. 1981. cognitive components of riskratings. risk analysis 1(4):277œ287.weaver, s. 1979. the passionate risk debate. the oregon journal, april 24.weinberg, a.m. 1979. salvaging the atomic age. the wilson quarterly (summer):88œ112.weinstein, n.d. 1980a. seeking reassuring or threatening information aboutenvironmental cancer. journal of behavioral medicine 2:125œ139.weinstein, n.d. 1980b. unrealistic optimism about future life events. journal ofpersonality and social psychology 93:806œ820.weinstein, n.d., ed. 1987. taking care. new york: cambridge university press.white, g., ed. 1974. natural hazards: local, national and global. new york: oxforduniversity press.wilson, r. 1979. analyzing the daily risks of life. technology review 81(4):40œ 46.wilson, v.l. 1980. estimating changes in accident statistics due to reporting requirementchanges. journal of safety research 12(1):36œ42.wohlstetter, r. 1962. pearl harbor: warning and decision. stanford, calif.: stanforduniversity press.wood worth, r.s., and h.schlosberg. 1954. experimental psychology. new york: henryholt.wortman, p.m. 1975. evaluation research: a psychological perspective. americanpsychologist 30:562œ575.wynne, b. 1980. technology, risk and participation. in society, technology and riskassessment, j.conrad, ed. london: academic press.wynne, b. 1983. institutional mythologies and dual societies in the management of risk. inthe risk analysis controversy, h.c.kunreuther and e.v. ley, eds. new york:springerverlag.zeisel, h. 1980. lawmaking and public opinion research: the president and patrickcaddell. american bar foundation research journal 1:133œ139.zentner, r.d. 1979. hazards in the chemical industry. chemical and engineering news57(45):25œ27, 30œ34.appendix c319improving risk communicationcopyright national academy of sciences. all rights reserved.appendix davailability of working papersphotocopies of the working papers of the committee on risk perception andcommunication are available from the national academy press, 2101constitution avenue, n.w., washington, dc 20418. case study: ﬁthe 1980/82 medfly controversy in california,ﬂ by emorym.roe. case study: ﬁcommunicating corporate disaster: the aldicarb oximerelease at the union carbide plant at institute, west virginia, on august11, 1985,ﬂ by rob coppock.appendix d320improving risk communicationcopyright national academy of sciences. all rights reserved.appendix ekey terms and distinctionsrisk communication practitioners and researchers and the general publicoften confuse key distinctions such as that between hazard and risk and thatbetween risk communication and risk message. we have therefore categorizedterms in order to emphasize such distinctions.hazard an act or phenomenon posing potential harm to some person(s) or thing(s); themagnitude of the hazard is the amount of harm that might result, including theseriousness and the number of people exposed.risk adds to the hazard and its magnitude the probability that the potential harm orundesirable consequence will be realized.* * *risk assessment the characterization of potential adverse effects of exposures tohazards; includes estimates of risk and of uncertainties in measurements, analyticaltechniques, and interpretive models; quantitative risk assessment characterizes therisk in numerical representations.appendix e321improving risk communicationcopyright national academy of sciences. all rights reserved.risk control assessment characterization of alternative interventions to reduceor eliminate the hazard and/or unwanted consequences; considers technologicalfeasibility, costs and benefits, and legal requirements or restrictions.risk management the evaluation of alternative risk control actions, selectionamong them (including doing nothing), and their implementation; the responsibleindividual or office (risk manager) sometimes oversees preparation of riskassessments, risk control assessments, and risk messages. risk management may ormay not be open to outside individuals or organizations.* * *risk communication an interactive process of exchange of information andopinion among individuals, groups, and institutions; often involves multiple messagesabout the nature of risk or expressing concerns, opinions, or reactions to riskmessages or to legal and institutional arrangements for risk management.risk message a written, verbal, or visual statement containing information aboutrisk; may or may not include advice about risk reduction behavior; a formal riskmessage is a structured written, audio, or visual package developed with the expresspurpose of presenting information about risk.* * *risk communicator/message source the individual or office sending arisk message or interacting with other individuals, groups, or organizations in a riskcommunication process; may also be the risk manager, risk message preparer, riskanalyst, or other expert.audience/recipients the recipient(s) of a risk message; almost never ahomogeneous group; can include the recipients intended by the preparer of themessage as well as others who receive it even though addressed elsewhere.appendix e322improving risk communicationcopyright national academy of sciences. all rights reserved.indexaabortion attitudes, 230acceptable risk, 274, 284; see also 5471, 8590accessto decisionmaking process, 7, 12728,28586to scientific information, 5, 78, 11415,14142, 27880accident reports, 255accountability, 10, 15658acid rain, 115acquired immune deficiency syndrome(aids)public's knowledge of, 22728, 290risk communication issues, 6, 89, 90,116, 13537, 165uncertainty of information on, 61, 121action threshold, 173active public, 101, 102administrative procedures act of 1946,16, 73, 100, 125, 128advocacy, see influence techniquesagricultural workers, 32agriculture, 59agriculture department, 113air bags, 19airline accidents, 257air pollution, 58, 116alcohol information, 17alcohol taxation, 19aldicarb oxime, 110ambiguously worded questions, 22833,265american bar association, 178american cancer society, 115american chemical society, 178american medical association, 7, 178anchoring, 226animal experiments, 39, 40, 58appeals to authority, 8485appeals to emotion, 85arsenic contamination, 18artificial sweeteners, 274asarco corp., 18asbestos hazard, 43, 257assassinations, 63atomic energy commission, 120attentive public, 101, 102attitude surveys, 22833, 26366audience/recipients, 322audience profiles, 10, 24, 16162ﬁaudience/recipientsﬂ defined, 322characteristics of, 1012concept defined, 271effect on message formulation, 282proposed consumer's guide, 1213,17679psychological principles, 299304relating messages to, 11, 13, 16570,18182risk literacy, 13, 182index323improving risk communicationcopyright national academy of sciences. all rights reserved.strategies for dealing with, 28386audubon society, 63automobile accidents, 220, 278automobile industry, 31, 137seat belts, 19, 58, 137bbaserate fallacy, 290bbc (british broadcasting corp.), 279behavioral principles, 299304benefit assessmentexpressed preferences, 26366policy concerns, 26263questions addressed in, 3335revealed preferences, 26668, 284reliability of, 3637bhopal, india, 61, 66, 126, 157, 285blood banks, 116bombhits analysis, 247botulism, 31ccalifornia, 91, 110cancer, 31, 32, 45, 61, 277canned food, 31, 257carter administration, 157case analyses, 13, 182catastrophic events, 42, 56, 23738, 303causal model, 22021celebrities' endorsements, 140centers for disease control (cdc), 109chemical industry, 229chemical manufacturers association, 128chemical plant management, 128, 130chernobyl disaster, 61, 66, 122, 300child abuse, 25758china, 59chlorinated hydrocarbons, 31, 55chlorination of water, 31, 32, 56chlorofluorocarbons, 56, 91choice problems, 23032, 304chromosome damage study, 11920chronic diseases, 61citizens' groups, 141, 153, 222civil engineers, 47, 49, 249civil rights movement, 63clean water act, 5climatic changes, 56, 59, 237, 238, 257clinical psychologists, 45, 248coal mining, 48, 259cognitive processes, 299304common sense, 23336communications technology, 62community awareness and emergencyresponse program, 128community channels, 7, 140, 164community involvement, 1718, 12728,28586community righttoknow act, 16, 110,14142competence development, 1011, 16064confidence intervals, 25053, 279, 280conflict, see social conflictsconservation attitudes, 274consumer product safetycommission, 64consumer's guide to risk and riskcommunication, 1213, 17679content of messages, see risk messagescostbenefit analysis, 219, 262;see also 3338credibility concerns, 11819, 146, 280accuracy of message, 67, 10, 11819,15560advocacy of unjustified positions, 11920contradiction of previous positions, 121contradictory messages from othersources, 12224credibility of source, 2425, 7475,11829, 282, 285, 293distrust of institutions, 63, 147fair review of conflicting claims,12829, 149justification offered for program, 12627legal standing of source, 12526legitimacy of process, 7, 119, 169news media interactions, 13839index324improving risk communicationcopyright national academy of sciences. all rights reserved.professional incompetence or impropriety, 12425public interaction and involvement, 10,12728, 15155reputation for deceit, 12021selfserving framing of information, 70,12122crimeprevention programs, 255crises, see emergenciesﬁcritical experimentﬂ artifact, 246ddeception, 81, 82, 88, 89, 162;see also 12021decision trees, 288deforestation, 59delaney clause, 284dietary information, 17, 58diethylstilbestrol (des), 66, 255differential knowledge, 6869, 71differing professional opinions, nuclearregulatory commission, 158ﬁdiscountingﬂ valuation measure, 49disease prevention and health promotion,office of, 64drug information, 17drug production, 62drug testing, 40drug users, 136eearthquake likelihood, 237, 238ecoflo hazardous waste facility siting,7577edison electric institute, 115emergenciesadvisories, 109response planning, 128, 141, 218responses, 7, 11, 13435, 138, 145,16465emission control devices, 137energy department, 113, 120energy policy/sources, 255, 256, 260,275, 276energy research and developmentadministration, 91environmental defense fund, 112environmental degradation, 229environmental impact statements, 67environmental movement, 6364environmental protection agency (epa),113, 165credibility concerns, 11920, 128, 147,159, 160edb contamination, 6, 113, 122, 123,129information availability, 14142mandate, 64, 65, 125, 127radon monitoring, 17, 90, 125, 127,13536report requirements, 141risk management strategies, 18environmental regulation, 57environmental threats, 56epidemiological studies, 42, 45, 58ethylene dibromide (edb), 6, 106,11314, 122, 123, 129, 135, 222evaluation of communication programs,11, 163, 29198eventtree analysis, 238expectations and misconceptions, seemisconceptions about risk communicationexpert judgment, 4447, 271;see also risk assessmentexpert knowledge, see scientific and technical knowledgeexposure estimates, 4041ffaulttree analysis, 42, 22122, 238federal insecticide, fungicide, and rodenticide act, 5federation of american scientists, 67feedback, 163, 249, 250financial resources, 5, 11415flood concerns, 23536florida, 6, 113, 122focus groups, 159food additives, 284food and drug administration, 113, 160food contamination, 6, 11314food production, 32, 62foreign affairs analysis, 248formaldehyde exposure, 121fragmented authority, 5, 11214index325improving risk communicationcopyright national academy of sciences. all rights reserved.framing information and decisions, 7, 83,12122, 130freedom of information act, 16ﬁfrequency of deathﬂ measurement, 226fundamental attribution error, 235fungicides, 222ggambler's fallacy, 130, 247general accounting office, 91geological survey, u.s., 237goal setting, 9, 15051, 29396government communication, 1618, 144grain contamination, 6, 222greensboro, n.c., 7577guilford county hazardous waste taskforce, 75, 76hhappiness questions, 23132hazardous substancesreport requirements, 141see also specific substanceshazardous waste facility siting, 5, 7577,111, 128, 141hazardsidentification, 3940, 25758increased social awareness, 5462qualification, 5052, 97, 13233quantification, 1, 3133term defined, 32, 321health care, 62health information, 17health professionals, 139heart disease, 24herbicides, 32, 5556highlighting facts, 8283highwaysafety legislation, 255hindsight, 233, 24547, 279historical records, 23738, 24647iiceland, 237idaho, 113impact assessments, 38immunization campaigns, 246incompetence and impropriety, 7, 12425industry communication, 17, 144influence techniquesambiguously worded questions, 22833appeals to authority, 84appeals to emotion, 85audience segmentation, 162deception, 81, 82, 88, 89, 162effects on credibility, 6, 11920framing information and decisions, 7,83, 12122, 130highlighting facts, 8283legitimacy of purpose, 23, 11, 7881,8793, 12627, 16870persuasion, 17, 84, 283procedural strategies to achieve balance,23, 8587risk comparisons, 84informationlaypersons' specialized knowledge,2425, 68, 280misconceptions about public's wants, 4,1012, 106, 234public's right to be informed, 5, 1617,65, 110, 14142, 297see also scientific and technical knowledgeinformed consent, 5, 64, 110, 266,28586, 295, 297informing function, 8182institute, w.va., 110, 134institutional constraints, see problems ofrisk communicationintegrated pest management, 116interest group conflict, see social conflictsinterest stimulation, 7, 13637intermediariescredibility, 25interacting with, 7, 13940, 16364role analysis, 13, 18081see also media for risk communicationinterpersonal channels, 7, 140, 164involuntarily incurred risks, 35, 27273index326improving risk communicationcopyright national academy of sciences. all rights reserved.jjefferson, thomas, 1415, 168journalists, see media for risk communicationjudgment, see risk assessmentkknowledge, see information; scientific and technical knowledgeknowledge gap, 69llanguage and conceptsclarity needed in, 1, 7, 11, 111, 12931,16667experts/laypeople, differing perceptions,27280ﬁlaypeople,ﬂ concept defined, 271lead solder, 31, 257league of women voters, 178legal constraints, 5, 10910, 255legal standing, 7, 12526lethality judgments, 22627liability, 5, 110life expectancy, 55, 56, 58, 259locally unwanted land uses (lulus),111, 141love canal, 66, 11920, 135lowpower research, 45, 24445mmalpractice concerns, 255, 297manipulation, see influence techniquesmanufacturing technology, 62massachusetts, 6, 11314, 122materials and energy flow diagrams, 222media for risk communicationcontradictory messages, 123credibility of journalists, 139credibility of sources, 13839interacting with, 11, 138, 160, 164media identified, 1, 7, 23, 24, 137misconceptions about role, 4, 1026role and responsibilities, 4, 137medical testing, 255medical treatments, evaluation criteria,292, 297mercury contamination, 222messages, 22;see also risk messagesmethyl isocyanate, 126middle east, 59military security, 62misconceptions about risk communication, 94107adequacy and meaning of information,100101communication improvement/conflictreduction, 9596interpretation of public attitudes, 4,1012, 106, 234news media role, 1036overview, 34, 9495value of risk comparisons, 96100missouri, 109modeling, 23940, 24243morbidity and mortality weekly report, 109multiattribute utility theory, 265nnational agricultural chemicals association, 115national cancer institute, 137national center for toxicologicalresearch (nctr) consensus workshops, 8687national conference on risk communication, 96national environmental policy act, 16,64, 67national farmworkers union, 115national flood insurance program, 236national institute for occupational safetyand health, 121national research council, 86, 87, 159national safety council, 178natural resources defense council, 112nature conservancy, 63nevada testing, 120new deal, 65news media, see media for risk communicationindex327improving risk communicationcopyright national academy of sciences. all rights reserved.new york state energy research anddevelopment authority(nyserda), 13536new york times, 104nonlethal consequences, 1no observed effect level (noel), 101north carolina, 7577ﬁnumber of deathsﬂ measurement, 48nuclear poweragency influence techniques, 91benefit/risk comparisons, 32, 256, 257chernobyl disaster, 61, 66, 122, 300nrc safety goals, 260, 284public attitudes and knowledge, 57, 124,234, 24647, 278, 300, 3023risk analysis, 42, 59, 218, 22021, 238,272social conflict, 308three mile island, 66, 106, 134, 222,247, 285, 300nuclear regulatory commission (nrc),64, 134, 158, 260, 284, 285nuclear war, 59nuclear wastes, 257numerical judgments, 22628oobjectivity of judgments, 27071occupational safety and health administration (osha), 64, 110, 113, 121,125ocean levels, 59office of communications of the unitedchurch of christ v. federal communications commission, 65oil embargo, 246oil industry, 255, 280openness, 10, 13435, 138, 15155overconfidence, 4647, 250253ozone layer, 56, 59, 91ppartisan preview, 10, 159passive public, 101peer review, 10, 15859, 237, 275personal action, 7880, 90personal responsibility, 285persuasion, 17, 84, 283pesticides, 32, 40, 5556, 115, 123, 129,165petrochemicals, 62poisson distribution, 247policy issues, see regulatory policypoliomyelitis immunization, 246political constraints, see problems of riskcommunicationpolitical opposition, 17power fragmentation, 5, 11214power sharing, 5, 10, 1718, 11112,15455probability theory, 12930, 24748problems of risk communication, 10842deriving from institutional and politicalsystems, 56, 9, 10816, 145, 150, 155focusing attention, 7, 13640fragmented authority, 5, 11214imbalanced access to authority, 56,11415incomplete information, 7, 13336legal considerations, 5, 10910obtaining information, 78, 14142overview, 4, 108, 142of risk communicators, andrecipients, 68, 11742sharing of power, 5, 10, 1718, 11112,15455systematic biases, 6, 11516understandable language and concepts,1, 7, 12933see also credibility concerns;social conflictsprocess management, 14965, 28298conceptualizing communication programs, 28691crisis situations, 11, 16465evaluating communication programs,11, 163, 29198fostering competence, 1011, 16064objectives summarized, 9, 14950safeguarding balance and accuracy inrisk messages, 10, 15560safeguarding openness, 10, 15155setting realistic goals, 9, 15051simple strategies, 28386index328improving risk communicationcopyright national academy of sciences. all rights reserved.professional incompetence and impropriety, 7, 12425protective behavior, 23536protocols, 21924psychological principles, 299304public, see audience/recipientspublic debate, 7277, 9193public health professionals, 49public health service, 109public interest, 65public mistrust, see credibility concernspublic opinion, 1012;see also 22833, 26366, 269qquantification of hazards, 3133rradiation hazards, 255radioactive waste program, 120radon hazard, 17, 90, 125, 127, 135, 137,288reactor safety study, 238, 249reagan administration, 157recipients, see audience/recipientsrecommended improvementsconsumer's guide, 1213, 17679content of messages, see risk messagesmanagement of process, see processmanagementresearch needs, 13, 17982summary, 89, 14349red cross, 7ﬁreduction in life expectancyﬂ measurement, 48regression analysis, 248, 267regulatory agencies, 64, 65regulatory policyalternatives to regulatory control, 1819desire or requirement to inform public,1617measuring benefits, 26268measuring risk, 25762public participation, 65, 66separating science and policy, 25457,268relevancy of risk message, 11, 13,16566, 181reliability analysis, 3637, 4647replication/reproducibility of results, 255,27071reporting requirements, 141reputation concerns, see credibility concernsresearch needs, 13, 17982resource limitations, 145rhetorical techniques, 84righttoknow legislation, 5, 16, 65, 110,14142, 297ﬁrisk,ﬂ concept defined, 32, 130, 25859,321risk assessment, 3053, 21753adherence to rules of science, 23638benefit assessment, 3338, 26268checklist, 175, 22224definition, 321errors in scientific judgment, 4447,13031expert judgment, quality assessed,24453, 27071expert judgment role, 23844, 270identification of problem, 21724improvement recommendations, 147information needed for, 3338, 25762knowledge gaps and uncertainties, 34,3844overview of problems, 2, 3031, 5253public's risk judgments, 22628public's value judgments, 22833quantification of hazards, 3133refining common sense, 23336relevant science identified, 22426separating facts and values, 25457,268, 271steps comprising, 217value judgments, 20, 4752, 25960risk characterization, 13, 3338, 180risk communicationconsumer's guide, 1213, 17679contemporaneous case assessment, 13,182definition and concept of, 2, 1923, 282,322index329improving risk communicationcopyright national academy of sciences. all rights reserved.improvement need, 1416motives, 1619recommended improvements summarized, 89, 14349success criterion, 2, 8, 21, 2629, 7475,7879, 94, 14445risk communication process, see processmanagementrisk communication problems, see problems of risk communicationrisk communication settings, 7280risk communicator, 6, 11742, 322;see also 162risk comparison, 96100improvement recommendations, 17274as influence tool, 84research needs, 13, 180usefulness/inadequacies, 3, 12, 96100risk conflict, see social conflictsrisk control assessment, 3536, 322risk debates, 7277, 117, 300, 303risk estimates, 4144, 83risk ladders, 9697, 174risk literacy, 13, 182risk magnitudes, 96risk management, 22definition, 21, 322improvement recommendations, 14748questions addressed in, 37see also process managementrisk management controversy, see socialconflictsrisk messagesaudience shaping of, 282comparing risks, 12, 96100, 17274credibility, see credibility concernsdefinition, 322design procedures, 7071, 86, 95differential knowledge, 6869draft preview, 10, 159ensuring completeness (checklist), 12,17476examples of, 144expert knowledge, 2829focus concerns, 25, 69formulation difficulties, 12, 28791handling uncertainty, 12, 17071influence purpose, see influence techniquesinformation purpose, 8082relating to audience perspectives, 11,16570role in communication process, 21, 2326sources and media, 1, 7, 23, 24, 13840,287, 322success criterion, 80systematic biases, 6, 11516understandable language and concepts,7, 12933values identification, 6970risk monitoring, 247risk perception, 5152, 132risk quantification, 4850, 22628,25960, 272ssaccharin, 274, 279st. louis, mo., 120scenic hudson preservation conferencev. federal power commission, 65science of risk, see risk assessmentscientific advisory board, epa, 128, 159scientific and technical knowledgeaccess problems, 5, 78, 11415, 14142,27880conflict within scientific community,6768, 12324, 279, 301errors in judgment, 4447essential rules, 23638incomplete or uncertain information, 7,12, 13336, 17071misconceptions and unrealistic expectations concerning, 34, 100101public mistrust, 70, 280role in technological debates, 2829, 67,68, 73, 92separating facts and values, 25457,268, 27071specialized talent requirements, 1011,16263understandable language and concepts,12933index330improving risk communicationcopyright national academy of sciences. all rights reserved.seat belts, 19, 58, 137selfreliance, 285sensitivity analysis, 171sex information, 17sexual assault, 290shrinkage, 248side effects, 60sierra club, 63, 115silent spring (carson), 63simulation models, 23940sindell v. abbott laboratories, 66skepticism, see credibility concernsskiing, 27273social conflicts, 5471, 26981conflicts identified, 5457, 254, 26971,28081diagnostic guide, 21112, 21416, 3058hazards and awareness changes, 5762implications for risk communication, 3,20, 6871, 9596, 286linguistic and conceptual differences,27280politicization of technological debate,6468societal changes, 6264, 146social sciencecognitive processes summarized,299304justification for, 23336social theory development, 22526socioeconomic changes, 62solar power, 255source credibility, see credibility concernssource of message, 1, 7, 23, 24, 13940,287, 322;see also media for risk communicationstanding, 7, 12526statutory mandates, 5, 10910steel industry, 260stock market analysis, 45, 247strawman arguments, 84stress, 13, 181, 261superfund amendments and reauthorization act of 1986, 16, 110, 14142surveys, 154, 22833, 26366, 269symbolic events, 6667synergistic effects, 42, 43, 56systematic biases, 6, 11516ttacoma, wash., 18technical knowledge, see scientific andtechnical knowledgetechnology, dependence on, 6263technology assessment, office of, 64technology assessments, 38television, see media for risk communicationthree mile island, 66, 106, 134, 222, 247,285, 300times beach, mo., 109tobacco institute, 115tobacco smoking, 22, 58, 133public policy, 73, 78, 90risk messages, 24, 85, 115, 137tort law, 6566toxic exposure cases, 110toxicity studies, 3940, 219toxic substance control act, 40training need, 47, 249transportation technology, 62trust, see credibility concernstylenol poisonings, 134typhoid, 31, 56uunion carbide co., 110, 126, 134, 285unwarranted concern, 29395uranium mining, 255vvalue judgments, 47, 86, 157conflict generated by, 20, 6970expressed preferences, 26366hazard qualification, 5052message sensitivity to, 13233revealed preferences, 26668, 284risk quantification, 4850, 25960, 3023separating facts and values, 25457,268, 271survey questions formulation, 22833vested interests, 69index331improving risk communicationcopyright national academy of sciences. all rights reserved.vietnam war, 63vitamin restrictions, 19voluntarily incurred risks, 27273wwall street journal, 104washington, 18washington post, 122water contamination, 3032, 56, 113watergate scandal, 63, 66weather forecasts, 4647, 249west virginia, 110, 134white papers, 10, 15960, 176worry threshold, 173index332
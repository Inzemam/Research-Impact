detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/10785the role of scientific and technical data and information inthe public domain: proceedings of a symposium238 pages | 8.5 x 11 | hardbackisbn 9780309385589 | doi 10.17226/10785steering committee on the role of scientific and technical data and informationin the public domain; office of international scientific and technicalinformation programs; board on international scientific organizations; policyand global affairs; national research councilthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.prefaceithe role ofscientific and technical data andinformation in the public domainproceedings of a symposiumjulie m. esanu and paul f. uhlir, editorssteering committee on the role of scientific and technicaldata and information in the public domainoffice of international scientific and technical information programsboard on international scientific organizationspolicy and global affairs divisionthe national academies presswashington, d.c.www.nap.eduthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.iiassessment of mars science and mission prioritiesthe national academies press 500 fifth street, n.w. washington, dc 20001notice: the project that is the subject of this report was approved by the governing board of the national researchcouncil, whose members are drawn from the councils of the national academy of sciences, the national academy ofengineering, and the institute of medicine. the members of the committee responsible for the report were chosen for theirspecial competences and with regard for appropriate balance.support for this project was provided by the center for public domain (under an unnumbered grant), the james d.and catherine t. macarthur foundation (under grant no. 0273708gen), the national library of medicine (underpurchase order no. 467mz200750), the national oceanic and atmospheric administration/national weatherservice (under an unnumbered purchase order), and the national science foundation (under grant no. geo0223581). any opinions, findings, conclusions, or recommendations expressed in this publication are those of theauthor(s) and do not necessarily reflect the views of the organizations or agencies that provided support for theproject.international standard book number 030908850x (book)international standard book number 0309525454 (pdf)copies of this report are available from board on international scientific organizations, 500 fifth street, nw,washington, dc 20001; 2023342807; internet, http://www7.nationalacademies.org/biso/.additional copies of this report are available from the national academies press, 500 fifth street, n.w., lockbox285, washington, dc 20055; (800) 6246242 or (202) 3343313 (in the washington metropolitan area); internet,http://www.nap.edu.copyright 2003 by the national academy of sciences. all rights reserved.printed in the united states of americathe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.prefaceiiithe national academy of sciencesis a private, nonprofit, selfperpetuating society of distinguished scholars engaged inscientific and engineering research, dedicated to the furtherance of science and technology and to their use for the generalwelfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it toadvise the federal government on scientific and technical matters. dr. bruce m. alberts is president of the national academyof sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as aparallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members,sharing with the national academy of sciences the responsibility for advising the federal government. the national academyof engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, andrecognizes the superior achievements of engineers. dr. wm. a. wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminentmembers of appropriate professions in the examination of policy matters pertaining to the health of the public. the instituteacts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to thefederal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v.fineberg is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broadcommunity of science and technology with the academyõs purposes of furthering knowledge and advising the federalgovernment. functioning in accordance with general policies determined by the academy, the council has become theprincipal operating agency of both the national academy of sciences and the national academy of engineering inproviding services to the government, the public, and the scientific and engineering communities. the council isadministered jointly by both academies and the institute of medicine. dr. bruce m. alberts and dr. wm. a. wulf arechair and vice chair, respectively, of the national research council.www.nationalacademies.orgthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.ivassessment of mars science and mission prioritiesthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.prefacevprefacethe body of scientific and technical data and information (sti)1 in the public domain in the united states ismassive and has contributed broadly to the economic, social, and intellectual vibrancy of our nation. the òpublicdomainó can be defined in legal terms as sources and types of data and information whose uses are not restrictedby statutory intellectual property laws or by other legal regimes, and that are accordingly available to the public foruse without authorization. in recent years, however, there have been growing legal, economic, and technologicalpressures that restrict the creation and availability of publicdomain informationñscientific and otherwise. it istherefore important to review the role, value, and limits on publicdomain sti.new and revised laws have broadened, deepened, and lengthened the scope of intellectual property and neighboring rights in data and information, while at the same time redefining and limiting the availability of specific typesof information in the public domain. national security concerns also constrain the scope of information that can bemade publicly available. economic pressures on both government and university producers of scientific datasimilarly have narrowed the scope of such information placed in the public domain, thus introducing access and userestrictions on resources that were previously openly available to researchers, educators, and others. finally, advances in digital rights management technologies for enforcing proprietary rights in various information productspose some of the greatest potential restrictions on the types of sti that should be accessible in the public domain.nevertheless, various wellestablished mechanisms for preserving publicdomain access to stiñsuch aspublic archives, libraries, data centers, and everincreasing numbers of open web sitesñexist in the government,university, and notforprofit sectors. in addition, innovative institutional and legal models for making availabledigital information resources in the public domain or through òopen access initiativesó are now being developed bydifferent groups in the scientific, educational, library, and legal communities.in light of these rapid and farreaching developments, the office of international scientific and technicalprograms organized the symposium on the role of scientific and technical data and information in the publicdomain. the symposium was held on september 56, 2002, at the national academies in washington, d.c. themeeting brought together leading experts and managers from the public and private sectors who are involved in thecreation, dissemination, and use of sti to discuss (1) the role, value, and limits of making sti available in thepublic domain for research and education; (2) the various legal, economic, and technological pressures on the1 sti has been used for several decades to refer to scientific and technical information, generally limited to scientific and technical (s&t)literature. for purposes of this symposium, it was used in the broader sense to refer also to scientific data so as to be comprehensive.vthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.viassessment of mars science and mission prioritiesproducers of publicdomain sti and the potential effects of these pressures on research and education; (3) theexisting and proposed approaches for preserving the sti in the public domain or for providing òopen accessó in theunited states; and (4) other important issues in this area that may benefit from further analysis.the main question that was addressed by the symposium participants was, what can the s&t communityitself do to address these issues within the context of managing its own data and information activities? enlightened new approaches to managing publicdomain sti may be found to be desirable or necessary, and these needto be thoroughly discussed and evaluated. the primary goal of this symposium, therefore, was to contribute to thatdiscussion, which will be certain to continue in many other fora and contexts.the symposium was organized into four sessions, each introduced by an initial framework discussion and thenfollowed by several invited presentations. the first session focused on the role, value, and limits of scientific andtechnical data and information in the public domain. this was followed in the second session by an overview ofthe pressures on the public domain. session three explored the potential effects on research of a diminishingpublic domain, and the final session focused on responses by the research and education communities for preserving the public domain and promoting open access to various types of sti.different aspects of the issues discussed in this symposium have been addressed in some detail already inseveral reports previously published by the national academies. in 1997, the report, bits of power: issues inglobal access to scientific data, examined the scientific, technical, economic, and legal issues of scientific dataexchange at the international level.2 in 1999, another report, a question of balance: private rights and the publicinterest in scientific and technical databases, looked at the competing public and private interests in scientificdata and analyzed several different potential legislative models for database protection in the united states fromthe perspective of the scientific community.3 in 2000, the study, the digital dilemma: intellectual propertyrights in the information age, discussed the conundrums of protecting intellectual property rights in informationon digital networks.4 and, in 2002, the national academies released a report that investigated the resolving ofconflicts in the privatization of publicsector environmental data.5most recently, the national academiesõ u.s. national committee for codata collaborated with the international council for science (icsu), the united nations educational, scientific, and cultural organization(unesco), the international committee on data for science and technology (codata), and the internationalcouncil for scientific and technical information (icsti) to convene a related event, the international symposiumon open access and the public domain in digital data and information for science.6 this meeting, held in march2003 at unesco headquarters in paris, focused on the same categories of issues as the september 2002 publicdomain symposium, except that it reviewed the existing and proposed approaches for preserving and promotingthe public domain and open access to s&t data and information on a global basis, with particular attention to theneeds of developing countries, and identified and analyzed important issues for follow up by icsu in preparationfor the world summit on the information society.this publication presents the proceedings of the september symposium. the speakersõ remarks were tapedand transcribed, and in most cases subsequently edited. however, in several instances, the speakers opted toprovide a formal paper. the statements made in these proceedings are those of the individual authors and do notnecessarily represent the positions of the steering committee or the national academies. finally, the committeealso commissioned a background paper by stephen maurer, òpromoting and disseminating knowledge: thepublic private interface,ó which is available on the national academiesõ web site only.7vipreface2 see national research council (nrc). 1997. bits of power: issues in global access to scientific data, national academy press,washington, d.c.3 nrc. 1999. a question of balance: private rights and the public interest in scientific and technical databases, national academypress, washington, d.c.4 nrc. 2000. the digital dilemma: intellectual property rights in the information age, national academy press, washington, d.c.5 nrc. 2001. resolving conflicts arising from the privatization of environmental data, national academy press, washington, d.c.6 the proceedings of the march open access symposium will be published in print and online by the national academies press in 2004. foradditional information on the symposium, see the codata web site at http://www.codata.org.7 copies of this paper are available on the national academies web site at http://www7.nationalacademies.org/biso/maurerbackgroundpaper.html or from the national academiesõ public access records office.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.acknowledgmentsthe office of international scientific and technical (s&t) information programs and the board on international scientific organizations of the national research council of the national academies wish to express theirsincere thanks to the many individuals who played significant roles in planning the symposium on the role ofscientific and technical data and information in the public domain. the symposium steering committee waschaired by david lide, jr., formerly of the national institute of standards and technology. additional membersof the steering committee included hal abelson, massachusetts institute of technology; mostafa elsayed,georgia institute of technology; mark frankel, american association for the advancement of science; maureenkelly, independent consultant; pamela samuelson, university of california, berkeley; and martha williams,university of illinois at urbanachampaign. r. stephen berry of the university of chicago served as chair of thesymposium.the office of international s&t information programs also would like to thank the following individuals (inorder of appearance) who made presentations during the workshop (see appendix a for final symposium agenda):william a. wulf, president, national academy of engineering; r. stephen berry, university of chicago; jamesboyle, duke university school of law; suzanne scotchmer, university of california, berkeley; paul david,stanford university; dana dalrymple, u.s. agency for international development; rudolph potenzone, lionbioscience; bertram bruce, university of illinois, urbanachampaign; francis bretherton, university of wisconsin; sherry brandtrauf, columbia university; jerome reichman, duke university school of law; robert cookdeegan, duke university; justin hughes, yeshiva university cardozo school of law; susan poulter, universityof utah school of law; david heyman, center for strategic and international studies; julie cohen, georgetownuniversity school of law; peter weiss, national weather service; stephen hilgartner, cornell university; danieldrell, u.s. department of energy; tracy lewis, university of florida; jonathan zittrain, harvard universityschool of law; stephen maurer, esq.; harlan onsrud, university of maine; ann wolpert, massachusetts instituteof technology; bruce perens, formerly with hewlett packard; shirley dutton, university of texas at austin; andmichael morgan, wellcome trust.this volume has been reviewed in draft form by individuals chosen for their technical expertise, in accordancewith procedures approved by the national academiesõ report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published reportviithe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.viiiassessment of mars science and mission prioritiesas sound as possible and to ensure that the report meets institutional standards for quality. the review commentsand draft manuscript remain confidential to protect the integrity of the process.we wish to thank the following individuals for their review of selected papers: hal abelson, massachusettsinstitute of technology; bonnie carroll, information international associates; robert chen, columbia university;joseph farrell, university of california, berkeley; mark frankel, american association for the advancement ofscience; oscar garcia, wright state university; gary king, harvard university; monroe price, yeshiva university; pamela samuelson, university of california, berkeley; and carol tenopir, university of tennessee.although the reviewers listed above have provided constructive comments and suggestions, they were notasked to endorse the content of the individual papers. responsibility for the final content of the papers rests withthe individual authors.finally, the office of international s&t information programs would like to recognize the contributions ofthe following national research council (nrc) staff and consultants: paul uhlir, director of internationals&t information programs, was project director of the symposium and coauthored the discussion frameworkswith jerome reichman, professor at duke university school of law and nrc consultant; stephen maurerserved as a consultant on this project and prepared a background paper on òpromoting and disseminatingknowledge: the public private interfaceó; subhash kuvelker of the kuvelker law firm also served as a consultant and moderated the presymposium online discussion forum; julie esanu helped to organize the symposium and served as the primary editor of the proceedings; and amy franklin and valerie theberge organizedand coordinated the logistical arrangements and assisted with the production of the manuscript.viiiacknowledgmentsthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.prefaceixixcontentssession 1: the role, value, and limits of scientific and technical (s&t) data andinformation in the public domain1discussion framework3paul uhlir, national research councilthe role, value, and limits of s&t data and information in the public domain in society2the genius of intellectual property and the need for the public domain10james boyle and jennifer jenkins, duke university school of lawthe role, value, and limits of s&t data and information in the public domain for innovationand the economy3intellectual propertyñwhen is it the best incentive mechanism for s&t data and information?15suzanne scotchmer, university of california, berkeley4the economic logic of òopen scienceó and the balance between private property rights andthe public domain in scientific data and information: a primer19paul david, stanford university5scientific knowledge as a global public good: contributions to innovation and the economy35dana dalrymple, u.s. agency for international development6opportunities for commercial exploitation of networked science and technologypublicdomain information resources52rudolph potenzone, lion biosciencethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.xassessment of mars science and mission prioritiescontentsthe role, value, and limits of s&t data and information in the public domain for educationand research7education56bertram bruce, university of illinois, champaignurbana8earth and environmental sciences60francis bretherton, university of wisconsin9biomedical research65sherry brandtrauf, columbia universitysession 2: pressures on the public domain10discussion framework73jerome reichman, duke university school of law11the urge to commercialize: interactions between public and private research and development87robert cookdeegan, duke university12legal pressures in intellectual property law95justin hughes, yeshiva university cardozo school of law13legal pressures on the public domain: licensing practices99susan poulter, university of utah school of law14legal pressures in national security restrictions104david heyman, center for strategic and international studies15the challenge of digital rights management technologies109julie cohen, georgetown university school of lawsession 3: potential effects of a diminishing public domain16discussion framework119paul uhlir, national research council17fundamental research and education125r. stephen berry, university of chicago18conflicting international public sector information policies and their effects on thepublic domain and the economy129peter weiss, national weather service19potential effects of a diminishing public domain in biomedical research data133stephen hilgartner, cornell universitythe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.prefacexicontentsxisession 4: responses by the research and education communities in preserving thepublic domain and promoting open access20discussion framework141jerome reichman, duke university school of law21strengthening publicdomain mechanisms in the federal government: a perspective frombiological and environmental research161ari patrinos and daniel drell, u.s. department of energy22academics as a natural haven for open science and publicdomain resources: how farcan we stray?165tracy lewis, university of florida23new legal approaches in the private sector169jonathan zittrain, harvard university school of law24designing publicðprivate transactions that foster innovation175stephen maurer, esq.new paradigms in academia25emerging models for maintaining scientific data in the public domain180harlan onsrud, university of maine26the role of the research university in strengthening the intellectual commons:the opencourseware and dspace initiatives at massachusetts institute of technology187ann wolpert, mitnew paradigms in industry27corporate donations of geophysical data191shirley dutton, university of texas at austin28the single nucleotide polymorphism consortium194michael morgan, wellcome trust29closing remarks198r. stephen berry, university of chicagoappendixes201afinal symposium agenda203bbiographical information on speakers and steering committee members206csymposium attendees215dacronyms and initialisms225the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.xiiassessment of mars science and mission prioritiesthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.1session 1: the role, value, and, limits ofscientific and technical data andinformation in the public domainthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 1331discussion frameworkpaul uhlir11note: this presentation is based on an article by j. h. reichman and paul f. uhlir, a contractually reconstructed research commons forscientific data in a highly protectionistic intellectual property environment, 66 law and contemporary problems 315 (winterspring2003), and is reprinted with the permission of the authors.factual data are fundamental to the progress of science and to our preeminent system of innovation. freedomof inquiry, the open availability of scientific data, and full disclosure of results through publication are thecornerstones of public research long upheld by u.s. law and the norms of science. the rapid advances in digitaltechnologies and networks over the past two decades have radically altered and improved the ways that data can beproduced, disseminated, managed, and used, both in science and in all other spheres of human endeavor. newsensors and experimental instruments produce exponentially increasing amounts and types of raw data. this hascreated unprecedented opportunities for accelerating research and creating wealth based on the exploitation of dataas such. every aspect of the natural world, from the subatomic to the cosmic, all human activities, and indeedevery life form can now be observed and stored in electronic databases. there are whole areas of science, such asbioinformatics in molecular biology and the observational environmental sciences, that are now primarily datadriven. new software tools help to interpret and transform the raw data into unlimited configurations of information and knowledge. and the most important and pervasive research tool of all, the internet, has collapsed thespace and time in which data and information can be shared and made available, leading to entirely new andpromising modes of research collaboration and production.much of the success of this revolution in scientific data activities, apart from the obvious technologicaladvances that have made them possible, has been the u.s. legal and policy regime supporting the open availabilityand unfettered use of data. this regime, which has been among the most open in the world, has placed a premiumon the broadest possible dissemination and use of scientific data and information produced by government orgovernmentfunded sources. this has been implemented in several complementary ways: by expressly prohibiting intellectual property protection of all information produced by the federal government and in many stategovernments; by contractually reinforcing the traditional sharing norms of science through open data terms andconditions in federal research grants; and by carving out a very large and robust public domain for noncopyrightabledata, as well as other immunities and exceptions favoring science and education, from proprietary informationotherwise subject to intellectual property protection.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.4the role of s&t data and information in the public domainpublicdomain information definedit is worthwhile at the outset to clearly define what we mean by òpublicdomain information.ó jeromereichman and i characterize it as òsources and types of data and information whose uses are not restricted byintellectual property or other statutory regimes and that are accordingly available to the public for use withoutauthorization or restriction.ó for analytical purposes, data and information in the public domain can be dividedinto two major categories:1.information that is not subject to protection under exclusive intellectual property rights or other statutoryrestriction; and2.information that qualifies as protectible subject matter under some intellectual property regime, but that iscontractually designated as unprotected.the first major category of publicdomain information can be further divided into three subcategories: (i)information that intellectual property rights cannot protect because of the nature of the source that producedit; (ii) otherwise protectible information that has lapsed into the public domain because its statutory term ofprotection has expired; and (iii) ineligible or unprotectible components of otherwise protectible subjectmatter.this presentation focuses primarily on categories 1(i) and 2, so i will just say a few words aboutcategories 1(ii) and (iii) here. information that has lapsed into the public domain because it has exceeded thestatutory term of protection under copyright is currently the life of the author plus 70 years in the unitedstates. this category constitutes an enormous body of creative works with great cultural and historicalsignificance. because of the long lag time in entering the public domain, however, it is not of greatimportance to most types of research.the other subcategory of information that is not subject to protection under exclusive intellectual propertyrights consists of ineligible or unprotectible components of otherwise protectible subject matter, such as an idea,fact, procedure, system, method of operation, concept, principle, or discovery, all of which are expressly excludedfrom statutory copyright protection. thus, all ideas or facts contained in an otherwise copyrighted work havebeenñat least in the pastñunprotected and could be used freely. although this publicdomain material is highlydistributed among all types of works, it is of particular concern to research and education and will be discussed insome detail later in the symposium.finally, a third, related category is information that becomes available under statutorily created immunities and exceptions from proprietary rights in otherwise protected material. instead of being in the publicdomain because it is unprotectible subject matter, it is otherwise protected content that is allowed to besubject to certain unprotected uses under limited circumstances, subject to casebycase interpretation. suchimmunities and exceptions allow for the use of proprietary information for purposes such as scholarship,research, critical works, commentaries, and news reporting, but their specific nature and extent varies greatlyamong different jurisdictions. known as òfair usesó in the united states, they also tend to be quite controversial and are frequently in dispute by rights holders. although copyright law has not formally treated theseimmunities and exceptions as public domain per se, a number of them may be construed as functionalequivalents of òpublicdomain uses.ó because many immunities and exceptions are allowed only in thecontext of notforprofit research or education, this category of publicdomain uses, although relativelysmall, is especially important.the main goal in this opening presentation is to coarsely map out the scope and nature of the public domainthat has served the american scientific enterprise so well for many years. we divide the producers of scientificdata into three sectors, namely, government agencies (primarily but not exclusively federal), academic and othernonprofit research institutions, and privatesector commercial enterprises. figure 11 presents a conceptualframework for analysis of this information regime, as well as a useful summary of the relative rights across thespectrum of information producers in all three sectors. because of time constraints, i will focus only on the scopeof the public domain in government and academia.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 15government information in the public domainthe role of the government in supporting scientific progress in general, and in the creation and maintenance ofthe research commons in particular, cannot be overstated. the u.s. federal government produces the largest body ofpublicdomain data and information used in scientific research and education. for example, the federal governmentalone now spends more than $45 billion per year on its research (as opposed to òdevelopmentó) programs, bothintramural and extramural, with a significant percentage of that invested in the production of primary data sources; inhigherlevel processed data products, statistics, and models; and in scientific and technical information, such asgovernment reports, technical papers, research articles, memoranda, and other such analytical material.at the same time, the united statesñunlike most other countriesñoverrides the canons of intellectualproperty law that could otherwise endow it with exclusive rights in governmentgenerated collections of data orother information. to this end, section 105 of the 1976 copyright act prohibits the federal government fromclaiming protection in its publications. a large portion of the data and information thus produced in governmentprograms automatically enters the public domain, year after year, with no proprietary restrictions, although thesources are not always easy to find. much of the material that is not made available directly to the public can beobtained by citizens through requests under the freedom of information act.closed private sectorip or statutory restrictions:trade secret or exclusive licensenational security, privacy, or confidentialityconditionally closed private sectorlegal and economic rights enforced, but some concessions made by contract (e.g., for nfps)conditionally open private sectorconditional ip by contract: " open access"at no or low cost, but some or all legal rights retainedopen private sector expired ip or pd by contract:no restrictionsno cost to userclosed university or nfp organizationip or statutory restrictions:trade secret or exclusive licensenational security, privacy, or confidentialityconditionally closed private university or nfp organizationconditional ip by contract: legal and economic rights enforced, but some concessions made (e.g., for nfpsor privileged class of users)conditionally open private university or nfp organizationconditional ip by contract: "open access"at no or low cost, but some legal rights retained; nonexclusive licenseopen private university or nfp organizationexpired ip or pd by contract:no restrictionsno cost to userclosed state government and state universityno pd:fully restricted by statute for security, privacy or confidentialitytrade secretconditionally closed state government and state universityconditional ip or pd:if pd, some restrictions and cost as in a.3.if ip, legal, and economic rights preserved, but with some concessionsconditionally open state government and state universitypure legal pd:no restrictionssome cost for user to accessopen state government and state universitypure legal and economic pd:no restrictionsno cost to userclosed federal governmentno pd:fully restricted by statutes for national security, privacy, or confidentialityconditionally closed federal government partial/conditional pd:some statutorily mandated restrictions on access and usepossibly some cost to accessconditionally openfederal governmentpure legal pd:no restrictionssome cost for user to accessopen federal governmentpure legal and economic pd:no restrictionsno cost to userfigure 11conceptual framework for analysis of the s&t information regime. copyright 2000 by paul f. uhlir. notes:òpdópublic domain; òipóintellectual property; ònfpónotforprofit.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.6the role of s&t data and information in the public domainthe federal government is also the largest funder of the production of scientific data by the nongovernmentalresearch community, as a major part of that $45 billion investment in public research, and many of the governmentfunded data also become available to the scientific community in the research commons. however, governmentfunded data follow a different trajectory from that of data produced by the government itself, as i will discusslater on. the following are a number of wellestablished reasons for placing governmentgenerated data andinformation in the public domain:¥a government entity needs no legal incentives from exclusive property rights that are conferred by intellectual property laws to create information, unlike individual authors or privatesector investors and publishers. boththe activities that the government undertakes and the information produced by the government in the course ofthose activities are a public good.¥the taxpayer has already paid for the production of the information. one can argue that the moral rights inthat information reside with the citizens who paid for it, and not with the state entity that produced it on behalf ofthe citizens.¥transparency of governance and democratic values are undermined by restricting citizens from access toand use of public data and information. as a corollary, citizensõ first amendment rights of freedom of expressionare compromised by restrictions on redissemination of public information, and particularly of factual data. it is nocoincidence that the most repressive political regimes have the lowest levels of available information and thegreatest restrictions on expression.¥finally, there are numerous positive externalitiesñparticularly through network effectsñthat can be realized on an exponential basis through the open dissemination of publicdomain data and information on theinternet. many such benefits are not quantifiable and extend well beyond the economic sphere to include socialwelfare, educational, cultural, and good governance values.the federal governmentõs policies relating to scientific data activities date back to the advent of the era of òbigscienceó following world war ii, which established a framework for the planning and management of largescalebasic and applied research programs. the hallmark of big science, or òmegascience,ó has been the use of largeresearch facilities or research centers and of experimental or observational òfacilityclassó instruments. the datafrom many of these government research projects, particularly in the past two decades, have been openly sharedand archived in public repositories. hundreds of specialized data centers have been established by the federalscience agencies or at universities under government contract.scientific and technical articles, reports, and other information products generated by the federal government are also not copyrightable and are available in the public domain. most research agencies have wellorganized and extensive dissemination activities for such information, typically referred to as òscientific andtechnical information.ólimitations on government information in the public domainwithout delving at this point into the growing pressures on the public domain, we need to outline a number ofcountervailing policies and practices that limit the free and unrestricted access to and use of governmentgenerateddata and information. first, there are important statutory exemptions to publicdomain access based on nationalsecurity concerns, the need to protect personal privacy of human subjects in research, and to respect confidentialinformation. these limitations on the publicdomain accessibility of federal government information, althoughoften justified, must nonetheless be balanced against the rights and needs of citizens to access and use it.another limitation derives from the fact that governmentgenerated data are not necessarily provided costfree. the federal policy on information dissemination, the office of management and budgetõs (omb) circulara130, stipulates that such data should be made available at the marginal cost of dissemination (i.e., the cost offulfilling a user request). in practice, the prices actually charged vary between marginal and incremental costpricing, and these fees can create substantial barriers to access, particularly for academic research.another major barrier to accessing governmentgenerated data and information in practice, which i havealready alluded to, arises from the failure of agencies to disseminate them or to preserve them for longtermthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 17availability. as a result, large amounts of publicdomain information are either hidden from public view or havebeen irretrievably lost.yet another limitation derives from omb circular a76, which bars the government from directly competingwith the private sector in providing information products and services. this policy substantially narrows theamount and types of information that the government can undertake to produce and disseminate. as regardsscience, the traditional view has been that basic research, together with its supporting data, are a public good thatproperly fall in the domain of government activity, although the boundary between what is considered an appropriate public and private function continues to shift in this and other respects.finally, the government is required to respect the proprietary rights in data and information originating fromthe private sector that are made available for government use or, more generally, for regulatory and other purposes,unless expressly exempted. to the extent that more of the production and dissemination functions of research dataare shifted from the public to the private sector, this limitation becomes more potent.academic information in the public domainthe second major source of publicdomain data and information for scientific research is that which isproduced in academic or other notforprofit institutions with government and philanthropic funding. databasesand other information goods produced in these settings become presumptively protectible under any relevantintellectual property laws unless affirmative steps are taken to place such material in the public domain. in thiscase, the public domain must be actively created, rather than passively conferred. this component of the publicdomain results from the contractual requirements of the granting agencies in combination with longstanding normsof science that aspire to implement òfull and open accessó to scientific data and the sharing of research results topromote new endeavors.the policy of full and open access or exchange has been defined in various u.s. government policy documents and in national research council reports as òdata and information derived from publicly funded researchare [to be] made available with as few restrictions as possible, on a nondiscriminatory basis, for no more than thecost of reproduction and distributionó (that is, the marginal cost, which on the internet is zero).2 this policy ispromoted by the u.s. government (with varying degrees of success) and for most governmentfunded or cooperative research, particularly in large, institutionalized research programs (such as òglobal changeó studies or thehuman genome project) and even in smallerscale collaborations involving individual investigators who are nototherwise affiliated with privatesector partners. it is necessary, however, to look beyond the stated policies andrules built around governmentfunded data to gain some deeper insights into how they are actually implemented inacademic science. in this regard, it is useful to further subdivide the producers of governmentfunded data into twodistinct, but partially overlapping, categories.the zone of formal data exchangesin the first category, the research takes place in a highly structured framework, with relatively clear rules setby government funding agencies on the rights of researchers in their production, dissemination, and use of data.publication of the research results constitutes the primary organizing principle. traditionally, the rules and normsthat apply in this sphereñwhich we call òthe zone of formal data exchangesóñaim to achieve a bright line ofdemarcation between public and private rights to the data being generated, with varying degrees of success.for example, government research contracts and grants with universities and academic researchers seek toensure that the data collected or generated by the beneficiaries will be openly shared with other researchers, at leastfollowing some specified period of exclusive useñtypically limited to 6 or 12 monthsñor until the time ofpublication of the research results based on those data. this relatively brief period is intended to give the researcher sufficient time to produce, organize, document, verify, and analyze the data being used in preparation ofa research article or report for scholarly publication. upon publication, or at the expiration of the specified period2 nrc. 1997. bits of power: issues in global access to scientific data, national academy press, washington, d.c.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.8the role of s&t data and information in the public domainof exclusive use, the data in many cases (particularly from large research facilities) are placed in a public archivewhere they are expressly designated as free from legal protection, or they are expected to be made availabledirectly by the researcher to anyone who requests access.in most cases, publication of the research results marks the point at which data produced by governmentfunded investigators should become generally available. the standard research grant requirement or norm hasbeen that, once publication occurs, it will trigger public disclosure of the supporting data. to the extent that thisrequirement is implemented in practice, it represents the culmination of the scientific norms of sharing. from hereon, access to the investigatorõs results will depend on the method chosen to make the underlying data publiclyavailable and on the traditional legal normsñespecially those of copyright lawñthat govern publications.these organizing principles derive historically from the premise that academic researchers typically are notdriven by the same motivations as their counterparts in industry and publishing. publicinterest research is notdependent on the maximization of profits and value to shareholders through the protection of proprietary rights ininformation; rather, the motivations of academic and notforprofit scientists are predominantly rooted in intellectual curiosity, the desire to create new knowledge and to influence the thinking of others, peer recognition andcareer advancement, and the promotion of the public interest.science policy in the united states has long taken it for granted that these values and goals are best served bythe maximum availability and distribution of the research results, at the lowest possible cost, with the fewestrestrictions on use, and the promotion of the reuse and integration of the fruits of existing results in new research.the placement of scientific and technical data and databases in the public domain, and the longestablished policyof full and open access to such resources in the government and academic sectors, reflects these values and servesthese goals.the zone of informal data exchangesin the second category, individual scientists establish their own interpersonal relationships and networks withother colleagues, largely within their specialized research communities. in this category, which we call òthe zoneof informal data exchanges,ó based on the work of stephen hilgartner and sherry brandtrauf, the scientists willmore likely be working autonomously in what we refer to as òsmall scienceó research. they will generate and holdtheir data subject to their personal interests and competitive strategies that may deviate considerably from thepractices established within the zone of formal exchanges and institutionalized by structured federal researchprograms. in this informal zone there will be other (nonfederal) sources of funding used, or the federal support thatis available will be less prescriptive about open terms of data availability. moreover, for research involving humansubjects, strong regulations protecting personal privacy adds an additional cloak of secrecy.the òsmall science,ó independent investigator approach traditionally has characterized a large area of experimental laboratory sciences, such as chemistry or biomedical research, and field work and studies, such asbiodiversity, ecology, microbiology, soil science, and anthropology. the data or samples are collected andanalyzed independently, and the resulting data sets from such studies generally are heterogeneous andunstandardized, with few of the individual data holdings deposited in public data repositories or openly shared.the data exist in various twilight states of accessibility, depending on the extent to which they are published,discussed in papers but not revealed, or just known about because of reputation or ongoing work, but kept underabsolute or relative secrecy. the data are thus disaggregated components of an incipient network that is only aseffective as the individual transactions that put it together. openness and sharing are not ignored, but they are notnecessarily dominant either. these values must compete with strategic considerations of selfinterest, secrecy, andthe logic of mutually beneficial exchange, particularly in areas of research in which commercial applications aremore readily identifiable.what occurs is a delicate process of negotiation, in which data are traded as the result of informal compromises between private and public interests and that are worked out on an ad hoc and continued basis. smallscience thus depends on the flow of disaggregated data through many different hands, all of whom collectivelyconstruct a fragile chain of semicontractual relations in which secrecy and disclosure are played out against acommon need for access and use of these resources.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 19the picture that we paint of òbigó and òsmalló science, and of the formal and informal zones of data exchangeis, of course, overstated to clarify the basic concepts. the big science system is not free of individual strategicbehavior that is typical of the small sciences, and the latter often benefit from access to public repositories wheredata are freely and openly available, rather than through the ad hoc transactional process. however, the òbrokerednetworksó typical of small science are endemic to all sciences, and access to data is everywhere becoming moredependent on negotiated transacting between private stakeholders.for the purposes of this symposium, we have chosen big science geophysical and environmental research thatuses large observational facilities and small science biomedical experimental research that involves individual orsmall independent teams of investigators as emblematic of these two types of research and related cultures. weuse them to provide realworld examples of the opportunities and challenges now inherent in the role of scientificand technical data and information in the public domain.the impetus for this symposiumalthough the scope and role of publicdomain data and information are well established in our system ofresearch and innovation, for reasons that subsequent speakers in this symposium will elaborate, there is anothertrend that is currently under way, which may be characterized as the progressive privatization and commercialization of scientific data and by the attendant pressures to hoard and trade them like other private commodities. thistrend is being fueled by the creation of new legal rights and protectionist mechanisms largely from outside thescientific enterprise, but increasingly adopted by it. these new rights and mechanisms include greatly enhancedcopyright protection in digital information, the ways in which access to and use of digital data are being contractually restricted and technologically enforced, and the adoption of unprecedented intellectual property rights incollections of data as such.this countervailing trend is based on perceived economic opportunities for the private exploitation of newdata resources and on a legal response to some loss of control over certain proprietary information products in thedigital environment. at the same time, it is disrupting the normative customs and foundation of science, especiallythe traditional cooperative and sharing ethos, and producing both the pressures and the means to enclose thescientific commons and to greatly reduce the scope of data and information in the public domain.viewed dispassionately, the need to appropriately reconcile these two competing interests in a sociallyproductive framework is imperative and the goal of such a reconciliation seems clear. a positive outcome wouldmaximize the potential of private investment in data collection and in the creation of new information andknowledge, while preserving the needs of public research for continued access to and unfettered use of data andother publicdomain inputs. these pressures, their potential impact on science, and the potential means forreconciling them in a winwin approach are the subjects of the next three sessions, respectively, of this symposium.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.10the role of s&t data and information in the public domain102the genius of intellectual property andthe need for the public domainjames boyle & jennifer jenkins11the oral presentation was given by james boyle, william neal reynolds professor, duke university school of law. the prose versionwas cowritten with jennifer jenkins, director of the duke center for the study of the public domain. for further discussion of the issues inthis presentation, see james boyle, òthe second enclosure movement and the construction of the public domain,ó 66 law & contemp.probs. (winter/spring 2003), draft available at http://jamesboyle.com/; james boyle, òthe second enclosure movement?ó duke law schoolconference on the public domain webcast archive (nov. 9, 2001), available at http://www.law.duke.edu/pd/realcast.htm; and òtensionsbetween free software and architectures of controló (apr. 5, 2001), available at http://ukcdr.org/issues/eucd/boyletalktext.shtml.i will start by talking about the genius of the intellectual property system and about what this system issupposed to do. then i will turn to a broad, overall vision of recent expansions of intellectual property protectionand talk a bit about the available conceptual tools, both economic and noneconomic, for thinking about appropriatelimits on intellectual property protections. i will conclude with some suggestions about an appropriate largerframework for conducting the inquiry around which this symposium is built.although i am going to say a number of negative things about the current state of intellectual propertyprotection, i want to start by stressing what a wonderful idea intellectual property protection is. the economiststell us that intellectual property allows us to escape the problems caused by goods that are nonrival and nonexcludable by conferring a limited monopoly (although some would argue about that nomenclature) and allowingproducers of information or innovation to recover the investment that they put into it. but the real genius ofintellectual property is that it tries to do something that my colleague jerry reichman calls òmaking marketsó: thatis, it tries to make markets where markets would otherwise be impossible.for many of you, making markets might seem a cold and unattractive phrase. but you should view it with theromance with which economists view the possibility of making markets. when i say markets, you should getexcited: you should think about a spontaneous and decentralized process where all of us have the possibility toinnovate and to recover something of the costs of our innovation.if you are a radical feminist and want to write a book condemning the contemporary workplace, if you have a uniqueimage of what can be done with a slack key guitar, if your pen can draw the horrors of cubicle life in a way that would amusemillions over their breakfast newspaper, the idea is that you could venture out there into the world; and if the market works(if the intermediaries do not stifle your expression, which they often do), you would be able to offer your creation to thepublic. similarly, innovations in other areasñthe drug, the invention, the postit noteñwould be put to the test of adecentralized set of needs and desires. the market would in turn allow us to say, òthis is what we want.óthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 111at present it seems that we want postit notes and viagra and britney spears: the point is that this particularconstruction of culture and innovation is made possible by intellectual property. it is that transaction between thesuccessful innovator or cultural artist and the public that intellectual property tries to make possible, together ofcourse with all the failed innovations, for which there are no audiences, or at least no interested audiences.the genius of intellectual property, then, is to allow the possibility of a decentralized system of innovation andexpression, a system in which i do not have to ask for polka or house music to be subsidized by the state, where ido not need to go to some central authority and plead the case for the funding of a particular type of innovation.instead, through the miracle either of my own initiative (and the capital markets) i can say òi have faith in this, andwe will see whether i am right or wrong.ó i will make a bet, or my investors will make a bet. that is the geniusof intellectual property.now i am going to say a lot of very negative things, but do remember that initial positive idea. i want to startwith an image of the old intellectual property system. (this is going to sound something like a golden age, whichis not what i intend, but given the time constraints, i have to simplify somewhat drastically.)the first thing to realize is that intellectual property rights used to be quite difficult to violate, because thetechnologies and activities necessary to do so were largely industrial. you could tell a friend about a book, lendit to him, or sell it to him, but you would need a printing press readily to violate its intellectual property protection.so intellectual property was an upstream regulation that operated primarily between largescale industrial producers competing against each other. horizontally it operated to stop them from simply taking the book du jour andripping that off, or taking the drug formula and ripping that off. but it had relatively little impact on individualusers or consumers and on everyday creative acts, whether in the sciences or in the arts.second, intellectual property protection was extremely limited in time. prior to 1978, 95 percent of everything that was written, as my colleague jessica litman has pointed out, went immediately into the public domain,because a work was not protected by copyright unless you affixed a copyright notice onto it. even if you did affixa copyright notice, the copyright would expire after some period of time. we started off in the united states at 14years of copyright protection. even up until 1978, this initial term had been extended only to 28 years, at the endof which you had to renew your copyright to maintain protection. a considerable percentage of the things undercopyright went into the public domain at that point, because they were not renewed, because there was no salientmarket. they were then available for all to use. this has changed completely. now the default is that everythingis automatically protected. every note written in this auditorium will not pass into the public domain until afteryour lifetime plus 70 years.third, intellectual property was tightly limited in scope. if you look at the areas that intellectual property nowcovers, you can see a sort of steroidlike bloating in every dimension. i look back at some of the confident phrasesin my old syllabi: òno one would ever claim that a business method could be patented.ó wrong. òno one wouldever claim that you could have intellectual property rights over a simple sequence of genomic data, likecgattgac.ó wrong. and of course, òcopyright protection only lasts for life plus 50 years.ó wrong.intellectual property rights now cover more subject matter, for a longer time, with greater penalties, and at afiner level of granularity. these rights are now backed by legally protected technical measures, which try andmake intellectual property protection a part of your use of a work, so you do not have the choice to violate anintellectual property right. they may also come with an associated contract of adhesion, which tells you how,when, and why you may make use of the work.we have, in other words, a bloating or expansion on every substantive levelñbreadth, length, depth, andscope. but also a kind of expansion that, because of the changes produced by digital technology and other copyingtechnologies, has changed fundamentally from being industrial regulation that affected upstream producersñthepublisher, the manufacturer of the drugñto personal regulation that applies literally on the òdesktop.óin a networked world, copying is implicated in everyday acts of communication and transmission, evenreading. it is hard for us to engage in the basic kinds of acts that make up the essence of academia or of thesciences without at least potentially triggering intellectual property liability. it is in fact the paradox of thecontemporary period that at the moment when the idea of every person with his or her own printing press becomesa reality, so does his or her own copyright police; and for every personõs digital lab, the potential of a patent policenearby.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.12the role of s&t data and information in the public domain so we have this enormous expansion of intellectual property rights. although i used the negative termsòbloatingó and òexpansion,ó in theory this could all be perfect. it might be that our legislators, respondingdoubtless to carefully phrased economic arguments rather than to straightforward economic payments, had wonderfully hit the economically optimal balance between underprotection and overprotection. that could havehappened.but can we at least be skeptical? do we really want, for example, to conduct the following experiment withthe american system of science and innovation? òwe never really knew how the american system of scienceworked in the first place, but, by and large, it did. now letõs change its fundamental assumptions and see if it stillworks.ó this is an interesting experimental protocol. and, if human subjects are not involved, it might even be auseful one. but it would seem that if you only have one system of science and innovation, then breaking it mighthave negative consequences. so fundamental tinkering in the absence of strong evidence that the tinkering isneeded would not seem like the optimal strategy.if, then, we are slightly skeptical and are engaged in a process of trying to reflect on what the correct balanceis between intellectual property and the material that is not protected by intellectual property, what are theconceptual, practical, political, and economic tools that we have at our disposal?well, by simplifying 300 years of history into a minute and a half, there is a longstanding tradition of hostilityto and skepticism of intellectual property. thomas jefferson in the united states and thomas macaulay in britainexpressed this very nicely. it is an antimonopolistic sentiment coming from the scottish enlightenment. it is asentiment that sees the biggest danger as being that monopoly makes things scarce, makes them dear, and makesthem bad, as macaulay said, and that intellectual property is a monopoly like any other. it may be a necessary evil,but it is an evil, and we should therefore carefully circumscribe it. we should put careful limits around itñconstitutional limits, limits of time, and so forth.the antimonopolistic critique of intellectual property offers, of course, a useful set of correctives to ourcurrent steroidfueled expansion. but what we really do not see for nearly 150 or 200 years is any mention ofsomething called òthe public domain.ó instead, there is an assumption that what is not covered by intellectualproperty will be òfree,ó a term that is tossed around just as loosely by the philosophers i have mentioned as it is bythe contemporary philosophers of the internet. what do we mean by òfreeó? no cost? easily available?uncontrolled by a single entity? available with a flat fee on a nondiscriminatory basis? òfreeó as in òfree speechóor òfreeó as in òfree beer,ó as richard stallman famously observed? although there are a lot of definitions, it is notvery clear what norm of freedom we are trying to instantiate.now, over the past 25 to 30 years, one sees in debates about intellectual property an affirmative mention of thepublic domain. the supreme court said that it would be unconstitutional to withdraw material from the publicdomain, or to restrict access to that which is already available. academics, including my colleagues david langeand jerry reichman, have written very interestingly about the role and function of the public domain, and i haveadded a little bit to that discussion myself. paul david, suzanne scotchmer, and others have also contributed tothese discussions in fascinating ways. but i think they would all agree that when we talk about the òpublicdomain,ó it is not quite clear how that term is defined.what is the public domain? is material made available through the fairuse privilege under copyright law partof the public domain or not? one could debate that. is material that is in the public domain only that materialwhich is entirely uncovered by intellectual property, such as a book whose copyright term has expired? or can itinclude the little chunk of unprotected material within a work that is otherwise covered by intellectual property,such as the ideas or facts underlying protected expression? we do not have a very good notion.how does the public domain function? we really do not know that very well either. the intellectualproperty system we have inherited had a strategy of braiding a thin layer of intellectual property rights arounda public domain of ideas and facts, which could never be owned. but one could own the expression or theinvention made out of those ideas and facts, leaving the ideas above and facts below in the public domain for thenext generation to build on. that actually sounds like an interesting strategy of a mixed public domain andproperty system.however, this system is one that we are busily demolishing through expanding intellectual property protections, without, so far as i can tell, either good empirical evidence that it is necessary to change its fundamentalthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 113premises or good economic models that the changes will work. there is remarkably little empirical study of theactual effects of intellectual property, or of the actual need for particular intellectual property rights rather than theconcept itself. i think that this is what we should demand. as one of my colleagues has said: we should notsuccumb either to merely good data or to good economic models, we need a little bit of both.where does this leave us? we are in a situation now that is akin to where the movement to protect theenvironment was in the 1940s and 1950s. at that time, there was no environment per se. this way of quiltingtogether a series of problems and issues and ecosystemsñproblems of pollution, ecosystem functions, our relationship to the biosphereñwas literally invented in the 1940s, 1950s, and 1960s by a series of brilliant scientific,popular, and other works, which told us that we should see these problems as a whole.we are in the process of creating our òenvironmentó in the intellectual world. that environment, in my view,is the public domain, and we are still looking for our writers of silent spring or a sand county almanac. we needthe writers who would manage both to conjure up what it is the public domain does for us and to delineate the waysin which it functions. we need the writers who would give us the equivalent of the notion of ecologicalinterconnectionsñthe unexpected reciprocal connections that turn out to be fundamental. we need the writers whowould give us the sense of modesty that environmentalists gave us about interventions in living systems, the sensethat perhaps we do not understand terribly well how the public domain operates; so if we tinker with it radically,things will not automatically go well. we seem to lack that sense of modesty; indeed, we seem to be possessed ofan extraordinary hubris.we also lack the associated scholarly analysis, from economic, legal, historical, and other perspectives. thereis a need to look at the optimal balance and interconnection between that which is protected and that which is not;to look at the types of production systems that the internet makes possible, like peertopeer production; to followthe kinds of fascinating studies like the one produced by jerry reichman and paul uhlir on the possibility ofestablishing wellfunctioning scientific commons in the new world of scientific inquiry.as we look at the contemporary mixture of that which is protected and that which is not, it is apparent that wecannot simply say that everything should be free. as jerry reichman and paul uhlir point out, there are multiplesubtle types of limitations and restrictions that may actually be necessary to allow for the free flow of data. norcan we romanticize the past of big science under the cold war model, under which so much research was actuallypaid for by the state but material was not automatically available to everyone. big science, as we know, has hadits own monopolies and blockages.instead, we need new cognitive orientations from which to think about these issues. we need, for example, totake a leaf out of the books by the theorists of the commons, such as elinor ostrom, who have studied how wellfunctioning commons continue. we have to analyze carefully how commons are not in fact automaticallytragedies, but, as carol rose has described, can be comedies: collective resources that are better managed collectively than they would be individually. (the environmentalists have taught us that rhetoric is no substitute forcareful science, but we also need a process of actually getting people to understand the stakes.)so how are we to proceed? our job in the national academies is to produce studies, to produce expertopinions, and to concentrate on this field. i would suggest here too that the environmental movement offers ussome hints. one of the fascinating things about the environmental movement is the way that elite scientific,economic, and other forms of academic inquiry are put together with a popular movement that is much broaderbased. a similar approach is needed here to understand the impact of intellectual property expansions. we cannotsimply focus on the old questions of the impact of the bayhdole act, or whether there should be a researchexemption to patent law, although these are indeed important issues. the reason that we cannot do that is becausethere is an urgency to the task, which is going to require speed greater than that produced by the occasionallyglacial process of convincing congress or other bodies, including private bodies, that they have taken a wrongstep.the urgency of our task is underscored by a number of developments accompanying the recent expansions inintellectual property. first, there is a generational shift occurring. many of the people in the audience werebrought up in an era in which the mertonian ideals of science were taken for granted. they start from theassumption that data should be free and need to be convinced that this assumption should be changed in particularcases. (i will not say they cannot be convinced, but they need to be convinced; and the burden of proof lies withthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.14the role of s&t data and information in the public domainthe people who would advocate restriction.) i would argue that, for many younger scientists, this ideal is no longertrue. the idea of paying for data is no stranger than paying for reagents. they start from the assumption that dataare a costly input like all others, rather from the belief that some economies actually work better when fundamental, upstream research is available freely, even if property rights are introduced further downstream. i do not thinkone can assume that scientists will automatically continue to have the same kind of skepticism that they currentlyexpress toward intellectual property expansions like those enacted in the european directive on the legal protection of databases and in recently proposed database protection legislation in the united states.a second development is that the universities and the science establishment, traditionally the òpublic defenders for the public domain,ó are changing. when new expansions of intellectual property were proposed, thiscommunity, together with librarians and a few others, would suggest a little restraint, would advocate limitationson expansions that went a little far. this was an extraordinarily valuable role for them to play in the legislativeprocess. but it is a role that we can no longer take for granted. universities, in particular, are now majorbeneficiaries of intellectual property expansion. every university has a chief technology officer or an intellectualproperty licensing officerña person whose job it is to maximize revenue. that person does not get promoted bygiving away a lot of data. it would be crazy for us to think that, under that kind of impetus, the universities willalways play the same role in the policymaking process that they have done before, unless activity is taken throughboth the national academies and other learned societies to awaken us to the danger posed.the types of actions that need to be taken span the traditional boundaries between the arts and sciences, orbetween the realms of patent and copyright. we need to explore a wide range of options, from the equivalent ofthe nature conservancyña private initiative to preserve a particular resource that is under threatñto the largescale scientific study, to the economic modeling that tells us the limits of the ecosystem, to the coalition buildingthat is exemplified by organizations like greenpeace. happily, organizations paralleling these types of functionsare currently moving forward, such as creative commons, the electronic frontier foundation, duke law schoolõsnewly created center for the study of the public domain, and public knowledge. we need to support theseorganizations and tie together the interests of these and other groups engaged in individual struggles by articulatingour common interest in protecting a valuable, shared resource.so what should we take away from all of this discussion? i would offer the following central points to guidethe discussion that follows. debates about the desirable limits of intellectual property are as old as the field itself.similarly, debates about the role of public and private financing of scientific investigation have a long anddistinguished history. in striking respects, however, contemporary events have shown the limits of both theòantimonopolyó perspective on intellectual property and of the assumption that òpublicó automatically equalsòfreeó or that òprivateó automatically means òcontrolled.ó the hypertrophy of intellectual property protectionsover the past 20 years has exacerbated these problems, while the increasing reliance of universities on patentfunding and licensing income promises to destabilize an already unstable political situation. as i pointed outearlier, universities traditionally played the role of òpublic defender for the public domainó in the legislative andpolicy process. that role can no longer be relied on, and much work needs to be done to make sure it does notaltogether disappear. this is a point of fundamental importance to our intellectual property policy. finally, thetraditional alternative to propertized scientific researchñgovernmentfunded, cold warstyle scienceñhas bothpractical and theoretical problems.what is to be done? the answer is a complex one. theoretically we need a much better understanding of therole of the public domain and of the commonsñtwo terms that are not equivalentñin innovation policy. practically, we need a series of concrete initiatives, aimed at both public and private actors, to stave off the ill effects ofòthe second enclosure movement.ó some of those initiatives are already under way, but much more effort isneeded. on the positive side, this symposium represents, if not the solution, at least an important recognition of themagnitude of the problem. given the role of the national academies, it is hard to think of a more pressing issuefor it to confront over the next 10 years.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.153intellectual propertyñwhen is it the best incentivemechanism for s&t data and information?suzanne scotchmer11 see n. gallini and s. scotchmer, 2002. òintellectual property: when is it the best incentive system,ó in innovation, policy and theeconomy, vol 2, a. jaffe, j. lerner, and s. stem, eds., mit press, massachusetts. also available at http://socrates.berkeley.edu/~scotch/gands.pdf.intellectual property has invaded the academy, especially in the sciences. along with others, economists havebeen thinking about this invasion, and the proper roles of the public and private sectors in the conduct of science.in particular, the focus has been on the question of whether the invasion of intellectual property is either inevitableor appropriate. as i tried to focus my thoughts on how to summarize economic thinking on this topic, and reviewedconversations with colleagues, it seemed to me that instead of trying to explain, as a positive matter, howeconomists view the relationship between the public and private domain, i should comment on the justifications ihear for intellectual property that i view as wrong. so i have unofficially retitled this talk, òwhy intellectualproperty: four wrong answers.óthe first wrong answer is òotherwise, we wonõt get inventions.ó the second is òintellectual property hasbenefits without costsó; it rewards inventors and, if licensed, does not hurt users. the third wrong answer is thatòthe private sector is more efficient than the public sector.ó the fourth is that, òwithout intellectual property, evenpublic domain inventions will not be used efficientlyóñthe bayhdole argument.consider the first wrong answer, that otherwise we wonõt get inventions. this is wrong as an historical matter,as a factual matter about the modern world, and as a logical matter. as an historical matter, it has never been truethat the only way we get inventions is by inciting private actors to make them in return for profit. many alternativemechanisms have been used. among the most prolific users of alternative mechanisms have been the french, butalso europeans more generally.some examples may be well known to you. in the 1840s, the french simply bought the invention of photography and made it freely available, thus engendering a great many improvements to it. the english, in contrast,made it proprietary, thus inhibiting further research on photography in england. napoleon needed to feed his vastarmies marching on russia and elsewhere. to find a way to do this, the french offered a prize for a way to preservefood. this led to canning and heat sterilization, still in use today. the french also offered a prize for waterturbines. the germans offered a prize for demonstrating maxwellõs equations, which led to the inventions ofhertz. and, of course, a very important and well known prize was that given for discovering a way to calculatelongitude at sea. that was an english prize.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.16the role of s&t data and information in the public domainin short, history informs us that there are many incentive systems other than intellectual property. some ofthese are embodied in our own public institutions, such as the national science foundation and the nationalinstitutes of health ñ a fact that we often forget, and certainly do not emphasize in the current climate where evenpublicly funded discoveries are subject to intellectual property.something that everyone should know is that about half of research and development is funded by publicsponsors. if you pull up basic raw data from the organisation for economic cooperation and development website,2 you will find that about onethird of u.s. research and development is federally funded. in other countries,which devote less money to military expenditures, a higher fraction of research and development is funded by thegovernment. in latin america, it is well over half. in most european countries it is a little under half. the fractionfunded by taxpayers is not trivial anywhere in the world. however, the question of public/private domain is not justwho funds the research, but whether the discoveries are put in the public domain. increasingly, even the discoveries that are publicly funded are restricted by intellectual property, e.g., the many discoveries made in nationallaboratories funded by the department of energy.wrong answer number two is that òintellectual property rewards inventors, and doesnõt hurt users provided itis licensed.ó it is true that intellectual property rewards inventors, and may even be an important engine of growthand of invention. but, as i have just pointed out, there are plenty of alternatives. this is not a definitive answer.and the second part of this statement, that it doesnõt hurt users provided it is licensed, is misleading. it is importantto notice that, even if licensed, intellectual property still creates deadweight loss, including for scientific researchtools.a license price is a price like any other. the licensor makes profit by licensing at a proprietary price. such aprice excludes users; else the licensor could probably increase profit by increasing the price. at a higher price,some users will be retained, and the additional profit received from the retained users will outweigh the profit lostfrom those that refuse the license. thus, distributing the use of a research tool by licensing instead of putting it inthe public domain will reduce its use just as any other monopoly price reduces use. this may seem obvious to you,but wrong answer number two is a common justification. while licensing certainly puts the intellectual property inuse, it does not solve the problem of deadweight loss.the argument about deadweight loss is illustrated by figure 31, which shows the proprietary price. on thehorizontal axis, we have arrayed all the users by their willingness to pay. if the proprietor charges a single price,then all the users arrayed to the left, for whom willingness to pay is higher than the price, will take a license. eachis getting some surplus despite the proprietary price, because each is willing to pay more than he actually has topay. on the other hand, the proprietary license price is excluding all the other users to the right. that is preciselythe social burden of intellectual property.there is one small caveat to this story, namely, that price discrimination can cure the deadweight loss thatarises from charging a single price to all users or potential users. if the proprietor could charge each user exactlyhis willingness to pay, then there is no reason to exclude, because the way to maximize profit is to embrace all theusers within a system of discriminatory pricing. although no user is excluded, the proprietor gets the entiresurplus.i mention the price discrimination caveat because it is a deeply subversive argument. it implies that there isnothing wrong with intellectual property. the argument has appeared in at least one court opinion, procd, aseventh circuit opinion.3 if you believed that perfect price discrimination was possible or likely in most cases, itwould remove the basic reason that intellectual property is a burden on society, namely, deadweight loss, andremoves any argument for reining it in. i believe the problem of excluding users is fundamental and basic, so iwould not want the argument to be subverted in that way. price discrimination can be very difficult, at least theperfect price discrimination that underlies this argument. in my view, we should take seriously the deadweight lossdue to intellectual property, and take steps to preserve the public domain.wrong answer number three is that òthe private sector is more efficient than the public sector.ó first of all,what does this mean? the public sector does not make inventions. scientists make inventions. most public sector2 see www.oecd.org/pdf/m00026000/m00026476.pdf for additional information.3 procd v. zeidenberg, 8 f.3d. 1447 (7th cir. 1996).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 117research is done by the private sector, that is, by scientists either in industry, the academy, or in national laboratories. the same people do inventions whether they are in the private sector or the public sector. only about 25percent of publicly funded research is performed by employees of the government. most of the rest is contractedout to industry and universities.4the question is not whether the public sector is more efficient than the private sector. the question is, how doyou give incentives not to waste money? how do you give incentives to invest in the right things? there are plentyof incentive mechanisms that mimic the incentives for effort that intellectual property creates, while avoidingdeadweight loss.it is often said in favor of intellectual property that the large reward of an intellectual property right will inciteeffort, and such effort is of great benefit to society. that is true, but, as i have already pointed out, prizes serve thatsame function. every academic knows that under the funding systems of, for example, the national institutes ofhealth and the national science foundation, you will not get funded the second time if you did not produce thefirst time. these are prize systems, and they are very good at inciting effort. whether there is a disparity betweenthe effort that such funding incites, and the effort incited under intellectual property rights, is a nuanced question.the incentives for effort are not different in kind, but only in degree, and the incentives created by both systemscan, in fact, be modified.but wrong answer number three, that the private sector is more efficient, misses the main point, even if true.the main question in my view is not who does the research, but rather the terms for using the output. the issue isproprietary pricing versus public domain. there may well be some reason for concern on the efficiency question,but i do not think that gives us license to ignore the question of access.finally we come to wrong answer number four, that òwithout intellectual property, basic inventions will notbe used even if they are in the public domain.ó that is part of the argument for the 1980 bayhdole act, whichauthorized the patenting of publicly sponsored inventions. the argument makes no sense to many economists. tomany economists, if someone makes a useful invention and puts it in the public domain, people will use it. even dw cs pm willingness to pay willingness to pay figure 31deadweight loss illustration, which shows the proprietary price.4 see national science board. 2002. science and engineering indicators 2002, national science foundation, arlington, va. available athttp://www.nsf.gov/sbe/srs/seind02/start.htm.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.18the role of s&t data and information in the public domainif potential users do not know what inventions are available, they have every incentive to find out. in any otherrealm of property, we would naturally assume that would happen. why should we ascribe less rationality topotential users of knowledge?one rejoinder is that basic inventions need collateral investments in order to become commercially viable. inorder to get the private sector to make the collateral investments, they need exclusive licenses on an underlyingproperty right.however, that is not an answer. if improvements need protection not available under the law, then the bayhdole act is trying to overcome a deficit of intellectual property law. that is a completely different spin on thestory. my view is that we should fix the thing that is broken, rather than something else. if collateral improvementsneed protection, then they should be protected. protecting collateral improvements or applications is not a reasonto allow restrictions on use of the underlying basic research, which was performed at public expense. if the thingthat is broken is that there is not sufficient protection for improvements and commercialization, then that is thething to fix. if fixed, the underlying basic research could be preserved in the public domain to stimulate improvements and commercialization in many parallel paths simultaneously, with no restrictions on use.let me conclude by coming back to the question, why intellectual property? i have given four answers that iregard as wrong. are there any òrightó answers? there are, in my view, some right answers, but they are verylimited in scope, and they do not apply very convincingly to scientific knowledge or to scientific data. i believethere are two right answers.the first is that the private sector may have better knowledge of what the private sector needs than a publicsponsor would. there is a set of issues not about the efficiency of how research and development gets done, andnot about monopoly pricing versus open access, but rather about the right things to invest in. there are manyrealms of scientific inquiry, especially those that are closely tied to industry and industrial development, that wewould not want to put under the jurisdiction of a public sponsor. under intellectual property rights, the privatesector will apply at least a weak test of whether an r&d investment is worth the cost, namely, whether it is likelyto turn a profit. in many realms, it is useful to apply that test.however, this perspective also seems to be a clear rationale for treating basic research differently from followon applications. nobody disagrees that we need genomic data or weather data. nobody disagrees that we needcertain kinds of environmental data. if there is no disagreement, the argument for intellectual property, in my view,is weak. we do not need the commercial sector to use their superior private knowledge or to apply the profit testas to whether the investment in r&d is a good one.the second justification for intellectual property is that it concentrates costs among the users. there is a clearrationale, for example, for funding computer games through intellectual property, because the people who usethem are a narrow population of 12 year olds. but there is no such justification, in my view, for using intellectualproperty to fund vaccines for infectious diseases, basic scientific knowledge, census data or weather data thatbenefit all of us. whether we know it or not, we are all users. the test of concentrating the costs among the userssimply does not apply with very much force for those kinds of scientific discoveries.in summary, i believe there are two valid tests for whether a particular subject matter should be supportedunder private incentives and an intellectual property regime rather than supported by public sponsors to be put inthe public domain. it seems to me that most scientific research done in universities and national laboratories doesnot pass these tests. the other four justifications that i have heard for the expansion of intellectual property rightsare not, in my view, sound arguments.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.194the economic logic of òopen scienceó and thebalance between private property rights and the publicdomain in scientific data and information: a primerpaul davidthe progress of scientific and technological knowledge is a cumulative process, one that depends in thelongrun on the rapid and widespread disclosure of new findings, so that they may be rapidly discarded ifunreliable, or confirmed and brought into fruitful conjunction with other bodies of reliable knowledge. òopenscienceó institutions provide an alternative to the intellectual property approach to dealing with difficult problemsin the allocation of resources for the production and distribution of information. as a mode of generating reliableknowledge, open science depends upon a specific nonmarket reward system to solve a number of resourceallocation problems that have their origins in the particular characteristics of information as an economic good.there are features of the collegiate reputational reward systemñconventionally associated with open sciencepractice in the academy and public research institutesñthat create conflicts between the ostensible norms ofòcooperationó and the incentives for noncooperative, rivalrous behavior on the part of individuals and researchunits who race to establish òpriority.ó these sources of inefficiency notwithstanding, open science is properlyregarded as uniquely well suited to the goal of maximizing the rate of growth of the stock of reliable knowledge.high access charges imposed by holders of monopoly rights in intellectual property have overall consequences for the conduct of science that are particularly damaging to programs of exploratory research that arerecognized to be vital for the longterm progress of knowledgedriven economies. like noncooperative behaviorsamong researchers in regard to the sharing of access to raw datasteams and information, and systematic underprovision of the documentation and annotation required to create reliably accurate and uptodate public databaseresources, lack of restraint in privatizing the public domain in data and information can significantly degrade theeffectiveness of the entire research system. considered at the macrolevel, open science and commercially oriented research and development (r&d) based on proprietary information constitute complementary subsystems.the public policy problem, consequently, is to keep the two subsystems in proper balance by public funding ofopen science research, and by checking excessive incursions of claims to private property rights over material thatwould otherwise remain in the public domain of scientific data and information.information and the public goods problem in researchacknowledging the peculiar character of information as an economic commodity is a necessary point ofdeparture in grasping the basic propositions that have been established by modern economic analyses of knowledgeproducing (research) activities of all kinds.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.20the role of s&t data and information in the public domaindata, information, and knowledge as economic goodsan idea is a thing of remarkable expansiveness, being capable of spreading rapidly from mind to mind withoutlessening its meaning and significance for those into whose possession it comes. in that quality, ideas are moreakin to fire than to coal. thomas jefferson, writing in 1813, remarked upon this attribute, which permits the sameknowledge to be jointly used by many individuals at once: òhe who receives an idea from me, receives instructionhimself without lessening mine; as he who lights his taper at mine receives light without darkening me. . . .ó (seedavid, 1993, on jeffersonõs observations in this connection).modern economists have followed nelson (1959) and arrow (1962) in recasting this insight and pointing outthat the potential value of an idea to any individual buyer generally would not match its value to the socialmultitude, since the latter would be the sum of the incremental benefits that the members of society derived fromtheir use of the idea. the individual benefits conveyed, however, will not readily be revealed in a willingness topay on the part of everyone who would gain thereby; once a new bit of knowledge is revealed by its discoverer(s),some benefits will instantly òspill overó to others who are therefore able to share in its possessionñat littleincremental cost. why should they then offer to bear any of the initial costs that were incurred in bringing theoriginal thought to fruition?commodities that have the property of òexpansibility,ó permitting them to be used simultaneously for thebenefit of a number of agents, are sometimes described as being ònonrivaló in use. this characteristic is a form ofnonconvexity, or an extreme form of decreasing marginal costs as the scale of use is increased: although the costof the first instance of use of new knowledge may be large, in that it includes the cost of its generation, furtherinstances of its use impose at most a negligibly small incremental cost. it sometimes is noticed that this formulation ignores the cost of training potential users to be able to grasp the information and know what to do with it. butwhile it is correct to point out that there can be fixed costs of access to the information, these do not vitiate theproposition that reuse of the information will neither deplete it nor impose further costs. it may well be costly toteach someone how to read the table of the elements or the rules of the differential calculus. nevertheless, anynumber of individuals thus instructed can go on using that knowledge without imposing further costs either onthemselves or upon others.a second peculiar property of ideas, which has to be underscored here, is that it is difficult, and generallycostly, to retain exclusive possession of them whilst putting them to use. of course, it is possible to keep a pieceof information or a new idea secret. the production of results not achievable otherwise, however, disclosessomething about the existence of a method for doing so. quite understandably, scientific and technical resultsobtained by methods that cannot or will not be disclosed are felt to be less dependable on that account; theirproduction is deemed to be more in the nature of magical performances than as contributions to the body of reliableknowledge. even the offer of a general explanation of the basis for achieving a particular, observable result may besufficient to jeopardize the exclusivity of its possession, because the knowledge that something can be done isitself an important step toward discovering how it may be done.public goods and the òappropriability problemóthe dual properties of nonrival usage and costly exclusion of others from possession define what a òpurepublic good.ó the term òpublic goodó does not imply that such commodities cannot be privately supplied, nor doesit mean that a government agency should or must produce it. nevertheless, it follows from the nature of pure publicgoods that competitive market processes will not do an efficient job of allocating resources for their productionand distribution, simply because where such markets work well they do so because the incremental costs andbenefits of using a commodity are assigned to the users. in the case of public goods, such assignments are notautomatic and they are especially difficult to arrange under conditions of competition.one may see the essence of the problem posed by the public goods characteristics of knowledge by asking:how can ideas be traded in markets of the kind envisaged by disciples of adam smith, except by having aspectsof their nature and significance disclosed before the transactions were consummated? rational buyers of ideas, noless than buyers of coal, and of fish and chips, first would want to know something about what it is that they willthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 121be getting for their money. even were the prospective deal to fall through, it is to be expected that the potentialpurchaser would enjoy (without paying) some benefits from what economists refer to as òtransactional spillovers.óthe latter are not without considerable value, because there may be significant commercial advantages from theacquisition of even rather general information about the nature of a discovery, or an inventionñespecially one thata reputable seller has thought it worthwhile to bring to the attention of people engaged in a particular line ofbusiness.this leads to the conclusion that the findings of scientific research, being new knowledge, would be seriouslyundervalued were they sold directly through perfectly competitive markets. some degree of exclusivity of possession of the economic benefits derived from ideas is therefore necessary if the creators of new knowledge are toderive any profit from their activities under a capitalist market system. intellectual property rights serve this end inthe form of patent and copyright monopolies. but imposing restrictions on the uses to which ideas may be put alsosaddles society with the inefficiencies that arise when monopolies are tolerated; a point harped upon by economistsever since adam smith. in addition, as will be seen below, access charges that holders of intellectual property arefree to impose on its users, and the secrecy practices that business companies embrace to protect their investmentsin the r&d process when it aims at securing proprietary control of new information, are a further source ofinefficiencies in societyõs utilization of existing bodies of knowledge.the economic logic of open sciencefor the purposes of this basic exposition, the institutional conditions of primary interest are those that sharplydistinguish the sphere of open science supported by public funding and the patronage of private foundations, on theone hand, from both the organized conduct of scientific research under for commercial profit under òproprietaryrules,ó and the production and procurement of defenserelated scientific and engineering knowledge under conditions of restricted access to information about basic findings and their actual and potential applications.ethos, norms, and institutions of òthe republic of scienceóthe formal institutions of modern science are ones with which academic economists already are thoroughlyfamiliar, for, it is a striking phenomenon in the sociology of science that there is high degree of mimetic professional organization across the various fields of academic endeavor. whether in the social sciences, the naturalsciences, or the humanities, most fields have their professional academies and learned societies, journal refereeingprocedures, public and private foundation grant programs, peerpanels for merit review of funding applications,organized competitions, and prizes and public awards. within the sciences proper, however, there are recognizednorms and conventions that constitute a clearly delineated ethos to which members of the academic researchcommunity generally are disposed to publicly subscribe, whether or not their individual behaviors conformliterally to its strictures. the norms of òthe republic of scienceó that were famously articulated by robert merton(1973, especially chapter 13) sometimes are summarized under the mnemonic cudos: communalism, universalism, disinterestedness, originality, skepticism. (see ziman, 1994, p. 177.)the òcommunaló ethos emphasizes the cooperative character of inquiry, stressing that the accumulation ofreliable knowledge is a social, rather than an individual program; however much individuals may strive tocontribute to it, the production of knowledge which is òreliableó is fundamentally a social process. therefore, theprecise nature and import of the new knowledge ought not to be of such immediate personal interest to theresearcher as to impede its availability or detract from its reliability in the hands of coworkers in the field.research agendas, as well as findings, ought therefore to be under the control of personally (or corporately)disinterested agents. the force of the universalist norm is to allow entry into scientific work and discourse to beopen to all persons of òcompetence,ó regardless of their personal and ascriptive attributes. a second aspect ofopenness concerns the disposition of knowledge: the full disclosure of findings and methods form a key aspect ofthe cooperative, communal program of inquiry. disclosure serves the ethos legitimating and, indeed, prescribingskepticism, for it is that which creates an expectation that all claims to have contributed to the stock of reliableknowledge will be subjected to trials of verification, without insult to the claimant. the òoriginalityó of suchthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.22the role of s&t data and information in the public domainintellectual contributions is the touchstone for the acknowledgment of individual claims, upon which collegiatereputations and the material and nonpecuniary rewards attached to such peer evaluations are based.an essential, differentiating feature of the institutional complex associated with modern science thus is foundin its public, collective character, and its commitment to the ethos of cooperative inquiry and to free sharing ofknowledge. like other social norms, of course, these are not perfectly descriptive of the behaviors of individualswho identify themselves with the institutions; indeed, not even of those who would publicly espouse their commitment to òopennessó and scientific cooperation. it is evident that, as is the case with public goods more generally,the temptation to òfreerideó on the cooperative actions of others creates tensions within the system. consequentially, the fact that it has withstood those strains points to the existence of some positive, functional value that itconveys to a substantial number of the scientists working in the òpublic/academicó research sector, and to theparticular benefits that society as a whole has been able to derive through public patronage of this mode ofpursuing knowledge.a functionalist rationale for the norms and institutions of open sciencewhile for most of us the idea of science as the pursuit of òpublic knowledgeó may seem a natural, indeed aòprimitiveó conceptualization, it is actually a social contrivance; and by historical standards, a comparativelyrecent innovation at that, having taken form only as recently as the sixteenth and seventeenth centuries (see david,1998a, 1998b for further discussion of the historical origins of open science). although economists have onlylately begun to analyze the workings of the institutional complex that characterizes òmodern science,ó it has notbeen difficult to reveal a functionalist rationale: it centers on the proposition that greater economic and socialutility derives from behaviors reinforced by the ideology of the open pursuit of knowledge and the norms ofcooperation among scientists (see dasgupta and david, 1987, 1994; david, 2001). that analysis also demonstratesthe òincentive compatibilityó between the norm of disclosure and the existence of a collegiate reputationbasedreward system grounded upon validated claims to priority in discovery or invention. the core of this rationale is the greater efficacy of open inquiry and complete disclosure as a basis for thecooperative, cumulative generation of predictably reliable additions to the stock of knowledge. in brief, opennessabets rapid validation of findings, and reduces excess duplication of research efforts. wide sharing of informationputs knowledge into the hands of those who can put it to uses requiring expertise, imagination, and material facilitiesnot possessed by the original discoverers and inventors. this enlarges the domain of complementarity amongadditions to the stock of reliable knowledge, and promotes beneficial spillovers among distinct research programs.as important as it is to emphasize these societal benefits of the cooperative mode for the advancement ofscience, it is equally significant to appreciate that the norms of information disclosureñapplied both to findingsand to the methods whereby research results have been obtainedñhave a second functional role. they are not anextraneous ethical precept of the open science system, but figure in the mechanism that induces individualscientific effort. the open science reward system typically is a twopart affair, in which researchers are offeredboth a fixed form of compensation and the possibility of some variable, resultsrelated rewards. the fixed part ofthe compensation package, e.g., a basic stipend or salary, tied to teaching in the case of academic researchinstitutions, provides individual researchers a measure of protection against the large inherent uncertainties surrounding the process of exploratory science. the variable component of the reward is based upon expert evaluationof the scientific significance of the individualõs contribution(s) to the stock of knowledge. this involves both theappraisals by peers of the meaning and significance of the putative òcontribution,ó and acceptance of theindividualõs claim to òmoral possessionó of the finding, on the grounds of having priority in its discovery orinvention. the latter is particularly important, as it connects the incentives for disclosure of findings under thecollegiate reputational reward system of open science with the social efficiency of sharing new information. one can fairly reasonably monitor minimal levels of input of effort into research activities (turning up in thelab, attending scientific meetings, etc.), or the performance of associated duties such as giving lectures, supervising students, and so on. but beyond that, the assessment of individual performance on the basis of inputs is quiteinadequate, and incentives offered for new discoveries and inventions must perforce be based upon observableòoutputsóñresults. the core of the problem here is that the outputs that one wants to reward are intangible andthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 123unique in the sense of being novel. the repetition of someone elseõs finding is not entirely devoid of social utility,for it may contribute to assessment of the reliability of the scientific assertions in question. but, although youwould want to pay a second miner for the ton of coal he brought to the surface moreorless the same amount thatthe preceding ton of coal earned for its producer, this is not the case for the second research paper that repeatedexactly what was reported in its predecessor. to be rewarded for a novel idea, research technique, or experimentalresult, one must be seen to have been in the vanguard of its creators. it is the prospect of gaining such rewardsñwhether in reputational standing and the esteem of colleagues, enhanced access to research resources, formalorganizational recognition through promotions accompanied by higher salary, accession to positions of authorityand influence within professional bodies and public institutions, and the award of prizes and honorsñthat servesto induce races to establish òpriority,ó and hence to secure rapid disclosure of òsignificantó findings.because the evaluation of significance will rest largely with the researchers scientific peers, this incentivemechanism performs the further function of directing his or her research efforts toward goals that are more likelyto win wider appreciation precisely because it will advance the work of others in the scientific community. whatthis means is that the incentives created for disclosure under the regime of open science are linked with theorientation of problem choice and the formation of collective research portfolios that are biased toward òresearchspillovers,ó rather than òproductdesign spilloversó in the sphere of commercial innovation.open science and proprietary research: regime complementarities and system balancethe advantages to society of treating new findings as public goods in order to promote the faster growth of thestock of knowledge are to be contrasted with the requirements of secrecy for the purposes of extracting materialbenefits from possession of information that can be privately held as intellectual property. this point can beformulated in the following overly stark, unqualified way. science (qua social organization) is about maximizingthe rate of growth of the stock of knowledge, for which purposes public knowledge and hence patronage or publicsubsidization of scientists is required, because citizens of the republic of science cannot capture any of the socialsurplus value their work will yield if they freely share all the knowledge they obtain. by contrast, the regime oftechnology (qua social organization) is geared to maximizing wealth stocks, corresponding to the current andfuture flows of pure òeconomic rentsó (private profits) from existing data, information and knowledge, andtherefore requires the control of knowledge through secrecy or exclusive possession of the right to its commercialexploitation. the conditions that are well suited to appropriate value from possession of the rights to exploit newknowledge, however, are not those that favor rapid growth in the stocks of reliable knowledge.this functional juxtaposition suggests a logical argument for the existence and perpetuation of institutionaland cultural separations between the communities researchers forming the republic of science and those engagedin proprietary scientific pursuits within the realm of technology: the two distinctive organizational regimes servedifferent and potentially complementary societal purposes. indeed, neither system alone can perform effectivelyover the long run as the two of them coupled together, since their special capabilities and limitations are complementary. maintaining them in a productive balance, therefore, is the central task towards which informed scienceand technology policies must be directed.it follows from this logic that such policies need to attend to the delineation of intellectual property rights andprotections of the public domain in scientific data and information as the respective codified knowledge infrastructures of the dual regimes, and to the maintenance of balance between them. to do so will be easier if there is aclearer understanding of the ways in which public expenditures for the support of open science serve to enhancethe value of commercially oriented r&d as a socially productive and privately profitable form of investment. suchan understanding on the part of representatives of the academic science communities is just as important as it is forgovernment policymakers, intellectual property lawyers, and business managers.applicationoriented payoffs from open, exploratory research: the uncertain lurewhen scientists are asked to demonstrate the usefulness of research that is exploratory in character andundertaken to discover new phenomena, or explain fundamental properties of physical systems, the first line ofthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.24the role of s&t data and information in the public domainresponse often is to point to discoveries and inventions generated by such research projects that turned out to be ofmore or less immediate economic value. indeed, many important advances in instrumentation, and generic techniques, such as polymerase chain reaction and the use of restriction enzymes in ògenesplicingó may be offered inillustration of this claim. these byproducts of the openended search for basic scientific understanding also mightbe viewed as contributing to the òknowledge infrastructureó required for efficient r&d that is deliberately directedtowards results that would be exploitable as commercial innovations.the experience of the 20th century also testifies to the many contributions of practical value that trace theirorigins to large, governmentfunded research projects that were focused upon the development of new enablingtechnologies for publicmission agencies. consider just a few recent examples from the enormous and diverserange that could be instanced in this connection: airline reservation systems, packet switching for highspeedtelephone traffic, the internet communication protocols, the global positioning system, and computer simulationmethods for visualization of molecular structuresñwhich has been transforming the business of designing newpharmaceutical products, and much else besides.occasionally, such new additions to the stock of scientific knowledge are immediately commercializable andyield major economic payoffs that, even though few and far between, are potent enough to raise the average socialrate of return on basic, academic research well above the corresponding private rate of return earned on industrialr&d investment. inasmuch as the high incremental social rate of return in cases like those just mentioned derivesfrom the wide and rapid diffusion of knowledge that provide new òresearch tools,ó the open dissemination,validation, and unobstructed elaboration of these discoveries and inventions are directly responsible for themagnitude of the òspilloveró benefits they convey.the skeptical economistõs response to recitations of this kind, however, is to ask whether a more directedsearch for the solutions to the applied problems in question would not have been less costly. would that approachnot be more expedient than waiting for serendipitous, commercially valuable results to emerge from costlyresearch programs that were conducted by scientists with quite different purposes in mind? this is a telling point,at least rhetorically, simply because the theme of such òspinoffó stories is their utter unpredictability. to arguethat these ògifts from athenaó are in some sense òfree,ó requires that the research program to which they wereincidental was worth undertaking for its own sake, so that whatever else might be yielded as byproducts was a netaddition to the benefits derived. yet, the reason those examples are being cited is the existence of skepticism as towhether the knowledge that was being sought by exploratory, basic science was worth the cost of the publicsupport it required. perhaps this is why the many examples of this kind that scientists have brought forward seemnever enough to satisfy the questioners.the discovery and invention of commercially valuable products and processes are seen from the viewpoint ofòthe new economics of scienceó to be rarer among the predictably òusefuló results that flow from the conduct ofexploratory, open science. without denying that òpureó research sometimes yields immediate applications aroundwhich profitable businesses spring up, it can be argued that those direct fruits of knowledge are not where thequantitatively important economic payoffs from the open conduct of basic scientific inquiry are to be found.microlevel complementarities between exploratory science and proprietary r&dmuch more critical over the long run than spinoffs from exploratory science programs are their cumulativeindirect effects in raising the rate of return on private investment proprietary r&d performed by business firms.among those indirect consequences, attention should be directed not only to òinformational spilloversó that lendthemselves readily to commercialization, but to a range of complementary òexternalitiesó that are generated for theprivate sector by publicly funded activities in the sphere of open science, where research and training are tightlycoupled.resources are limited, to be sure, and in that sense research conducted in one field and in one organizationalmode is being performed at the expense of other kinds of r&d. but what is missed by attending exclusively to thecompetition forced by budget constraints, is an appreciation of the ways in which exploratory science and academic engineering research activities support commercially oriented and missiondirected research that generatesnew production technologies and products.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 125first among the sources of this complementary relationship is the intellectual assistance that fundamentalscientific knowledge (even that deriving from contributions made long ago) provides to applied researchers,whether in the public or in the private sector. from the expanding knowledge base it is possible to derive time andcostsaving guidance as to how best to proceed in searching for ways to achieve some prespecified technicalobjectives. sometimes this takes the form of reasonably reliable guidance as to where to look first, and much of thetime the knowledge base provides valuable instructions as to where it will be useless to look. how else does theventure capitalist know not to spend time talking with the inventor who has a wonderful new idea for a perpetualmotion machine?one effect this has is to raise the expected rates of return, and reduce the riskiness of investing in appliedr&d. gerald holton (1996), a physicist at harvard and historian of science, has remarked that if intellectualproperty laws required all photoelectric devices to display a label describing their origins, òit would list prominently: ôeinstein, annalen der physik 17(1905), pp.132148.õó such credits to einstein also would have to beplaced on many other practical devices, including all lasers.the central point that must be emphasized here is that, over the longrun, the fundamental knowledge andpractical techniques developed in the pursuit of basic science serves to keep applied r&d as profitable aninvestment for the firms in many industries as it has proved to be, especially, during the past halfcentury (seedavid, mowery, and steinmueller, 1992). in this role, modern science continues in the tradition of the precious ifsometimes imprecise maps that guided parties of exploration in earlier eras of discovery, and in that of thegeological surveys that are still of such value to prospectors searching for buried mineral wealth.that is not the end of the matter, for a second and no less important source of the complementary relationshipbetween basic and applied research is the nexus between university research and training on the one hand, and on theother the linkage of the profitability of corporate r&d to the quality of the young researchers that are available foremployment. seen from this angle, government funding of open exploratory science in the universities today issubsidizing the r&d performed by the private business sector. properly equipped research universities have turnedout to be the sites of choice for training the most creative and most competent young scientists and engineers, as manycorporate directors of research well know. this is why graduates and postdoctoral students in those fields are sent orfind their own way to university labs in the united states, and still to some in the united kingdom. it explains whybusinesses participate (and sponsor) òindustrial affiliatesó programs at research universities.the òtacit dimensionó and knowledge transfers via the circulation of researchersa further point deserving emphasis in this connection is that a great deal of the scientific expertise availableto a society at any point in time remains tacit, rather than being fully available in codified form and accessible inarchival publications. it is embodied in the craftknowledge of the researchers, about such things as the proceduresfor culturing specific cell lines, or building a new kind of laser that has yet to become a standard part of laboratoryrepertoire. this is research knowledge, much of it very òtechnologicaló in nature in that it pertains to howphenomena have been generated and observed in particular, localized experimental contexts, which is embodied inpeople. under sufficiently strong incentives it would be possible to express more of this knowledge in forms thatwould make it easier to transmit, and eventually that is likely to happen. but, being possessed by individuals whohave an interest in capturing some of the value of the expertise they have acquired, this tacit knowledge istransmitted typically through personal consultations, demonstrations, and the actual transfer of people (see cowan,david, and foray, 2000, for further treatment of the economics of tacitness and codification of knowledge).the circulation of postdoctoral students among university research laboratories, between universities andspecialized research institutes, and no less importantly, the movement of newly trained researchers from theacademy into industrial research organizations, is therefore an important aspect of òtechnology transferóñdiffusing the latest techniques of science and engineering research. the incentive structure in the case of this mode oftransfer is a very powerful one for assuring that the knowledge will be successfully translated into practice in thenew location, for the individuals involved are unlikely to be rewarded if they are not able to enhance the researchcapabilities of the organization into which they move.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.26the role of s&t data and information in the public domaina similarly potent incentive structure may exist also when a fundamental research project sends its personnelto work with an industrial supplier from whom critical components for an experimental apparatus are beingprocured. ensuring that the vendor acquires the technical competence to produce reliable equipment within thebudget specifications is directly aligned with the interests of both the research project and the business enterprise.quite obviously, the effectiveness of this particular form of usersupplier interaction is likely to vary directly withthe commercial value of the procurement contracts and the expected duration and continuity of the researchprogram.for this reason, big science projects or longrunning public research programs may offer particular advantagesfor the collaborative mode of technology transfers, just as major industrial producersñsuch as the large automotive companies in japanñare seen to be able to set manufacturing standards and provide the necessary technicalexpertise to enable their suppliers to meet them. by contrast, the transfer of technology through the vehicle oflicensing intellectual property is, in the case of process technologies, far more subject to tensions and deficienciesarising from the absence of complete alignment in the interest of the involved individuals and organizations. but,as has been seen, the latter is only one among the economic drawbacks of depending upon the use of intellectualproperty to transfer knowledge from nonprofit research organizations to firms in the private sector.wonõt the private sector sponsor academic, òopen scienceó style research?business firms do indeed cooperate with academic research units and, in some cases, they undertake on theirown to support òbasic researchó laboratories whose employed scientists and engineers are permitted, and evenencouraged, to rapidly submit results for publication in scientific and technical journals. large r&dintensivecompanies also adopt strategies of liberally licensing the use of some their patented discoveries and inventions.their motives in adopting such policies include the desire to develop a capability to monitor progress at thefrontiers of science, the hope of being able to pick up early information concerning emerging potential lines ofresearch with commercial innovation potential, being better positioned to penetrate the secrets of their rivalsõtechnological practices, and recruiting talented young scientists whose careergoals are likely to be advanced bythe exposure that can be afforded to them by publication and active participation in an open science community.even some small and mediumsize firms sometimes opt to freely disclose inventions that are patentable. (thismay be done as a means of establishing the new knowledge within the body of òprior art,ó and thereby, withoutincurring the expenses of obtaining and defending a patent, effectively precluding others from securing propertyrights that would inhibit the firm from exploiting the knowledge for its own production operations.)yet, in regard to the issue of whether market incentives fail to elicit private sector funding of exploratory r&dconducted in the open science mode, the relevant question for society is one that is quantitative, not qualitative.one cannot adequately answer the question òwill there be enough businessmotivated support for open science?ómerely by saying, òthere will be some.ó economists do not claim that without public patronage scientific andtechnical research would entirely cease to be conducted under conditions approximating those of the open sciencesystem. rather, their analysis holds that there will not be enough investment in that style of inquiry, that is to say,not as much as would be carried out were individual businesses able to anticipate capturing all the benefits of suchknowledgeproducing investmentsñas is the case for society as a whole.moreover, business support for academicstyle r&dñas distinguished from industrial contracting for universitybased, applicationsoriented research with intellectual property rights assigned to the sponsoring firmsñismore likely to commend itself as a longterm strategy. consequently, it is likely to be sensitive to commercialpressures to shift research resources towards advancing existing product development, and improving existingprocesses, rather than searching for future technological options. this implies that exploratory lines of inquiryfunded in this way are vulnerable to wasteful disruptions. large commercial organizations that are less prone tobecoming assetconstrained and, of course, the public sector therefore are better able to take on the job ofmonitoring what is happening on the international science and technology frontiers. considerations of these kindsare important in addressing the issue of how to find the optimal balance for the national research effort betweensecrecy and disclosure of scientific and engineering information, as well in trying to adjust the mix in the nationalresearch portfolio of exploratory open science style programs and proprietary, applicationsdriven r&d projects.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 127property and the pursuit of knowledgeintellectual property rights and the norms of disclosurethe past two decades have witnessed growing efforts to assert and enforce intellectual property rights overscientific and technological knowledge through the use of patents, copyrights, and other, more novel forms of legalprotection. (the latter include the special legislation introduced in the united states in 1980 to extend copyrightprotection to the òmask workó for photolithographic reproduction of very large microelectronic circuits on siliconwafer, and the european unionõs protection of databases by new national statutes implementing an ec directiveissued in 1996.) these developments have coincided with two other trends that similarly have tended to expand thesphere of private control over access to knowledge, at the expense of the public knowledge domain.one trend has been the rising tide of patenting activity by universities, especially in the areas of biotechnology, pharmaceuticals, medical devices, and software. this movement started in the united states, where itreceived impetus under the 1980 bayhdole act that permitted patent applications to be filed for discoveries andinventions issuing from research projects that were funded by the federal government. it has since spread internationally, being reinforced by the efforts in other countries to foster closer research collaboration between universities and public research institutes on the one hand, and private industry on the other. the other trend has seen aconcerted effort by all parties to secure copyright protection for the electronic reproduction and distribution ofinformation, in part to exploit the opportunities created by electronic publishing and in part to protect existingcopyright assets from the competition that would be posed by very cheap reproduction of information in digitalform over electronic networks.during the past two decades a renewal of enthusiasm for expanding and strengthening private property rightsover information has given rise to a rather paradoxical situation. technological conditions resulting from theconvergent progress in digital computing and computermediated telecommunications have greatly reduced thecosts of data capture and transmission, as well as of information processing, storage, and retrieval. these developments are working to give individuals, and especially researchers, unprecedentedly rapid and unfettered access tonew knowledge. at the same time, and for reasons that are not entirely independent, the proliferation of intellectualproperty rights and measures to protect these is tending to inhibit access to such information in areas (basicresearch in general, and most strikingly in the life sciences and computer software) where new knowledge hadremained largely in the public domain (see david, 2000, on the connection between the technological and theinstitutional trends).thus, it may be said that a good bit of intellectual ingenuity and entrepreneurial energy is being directedtowards the goal of neutralizing the achievements of information scientists and engineers by creating new legallysanctioned monopolies of the use of information, thereby creating artificial scarcities in fields where abundancenaturally prevails and access to that abundance is becoming increasingly ubiquitous. the consequent imposition ofdata and information access charges above the marginal costs of producing and distributing information results ina double burden of economic inefficiency when it falls upon researchers who use those informationgoods asinputs in the production of more information and knowledge. the firstorder effect is the curtailment of the use ofthe information, or the increased cost of using it to produce conventional commodities and services, and hence theloss of utility derived from such products by consumers. a second round of inefficiency is incurred by theinhibition of further research, which otherwise would be the source of more public goods in the form of newknowledge.to understand the full irony of this situation, one has simply to return to the point of departure in thisdiscussion: the observation that knowledge is not like any other kind of good, and certainly does not resembleconventional commodities of the sort that are widely traded in markets. intellectual property cannot be placed onan equal footing with physical property, for the simple reason that knowledge and information possess a specificcharacteristic that economists refer to as ònonrival in useóñthe same idea and its expression may be usedrepeatedly and concurrently by many people, without being thereby òdepleted.ó this contrasts with the propertiesof ordinary òcommoditiesó that are consumed: if marie eats the last slice of cake in the kitchen, that piece cannotthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.28the role of s&t data and information in the public domainalso be consumed by camille; whereas, both girls may read the same novel either simultaneously or sequentially,and in so doing they will not have rendered the story any the less available for others to enjoy.the allocation of property rights in the case of informationgoods does not attempt to confer a right ofexclusive possession, as do property laws governing tangible goods such as land. indeed, to claim a right ofpossession one must be able to describe the thing that is owned, but no sooner do you describe your idea to anotherperson than their mind comes into (nonexclusive) possession of it; only by keeping the information secret can youpossess it exclusively.what the creation and assigning intellectual property rights does then is to convey a monopoly right to thebeneficial economic exploitation of an idea (in the case of patent rights) or of a particular expression of an idea (inthe case of copyright) that has been disclosed, rather than being kept secret. this device allows the organization ofmarket exchanges of òexploitation rights,ó which, by assigning pecuniary value to commercially exploitable ideas,creates economic incentives for people to go on creating new ones, as well as finding new applications for oldones. by tending to allocate these rights to those who are prepared to pay the most for them, the workings ofintellectual property markets also tend to prevent ideas from remaining in the exclusive (secret) possession ofdiscoverers and inventors who might be quite uninterested in seeing their creations used to satisfy the wants andneeds of other members of society.another potential economic problem that is addressed by instituting a system of intellectual property rights isthe threat of unfair competitionñparticularly the misappropriation of the benefits of someone elseõs expenditureof effortñwhich might otherwise destroy the provision of information goods as a commercially viable activity.the nub of the problem here is that the cost of making a particular information good available to a second, third,or thousandth user are not significantly greater than those of making it available to the first one. when th”o listensto a piece of music, modern reproduction and transmission technologies will permit quentin, manon, and millionsof others to listen to the same piece without generating significant additional costs. the costs of the first copy ofa cd are very great, compared to the cost of òburningó a second, third, or millionth copy of that cd. ever since thegutenberg revolution, the technical advances that have lowered the costs of reproducing òencodedó material (text,images, sounds) also has permitted òpiratesó to appropriate the contents of the first copy without bearing theexpense of its development. unchecked, this form of unfair condition could render unprofitable the investmententailed in obtaining that critical first copy.producers of ideas, texts, and other creative works (including graphic images and music) are subject toeconomic constraints, even when they do not invariably respond to variation in the incentives offered by themarket. if they had no rights enabling them to derive income from the publication of their works, they might createless, and quite possibly be compelled to spend their time doing something entirely different but more lucrative. sothere is an important economic rationale for establishing intellectual property rights. a strong case also can bemade for protecting such rights by the grant of patents and copyrights, especially as that way of providing marketincentives for certain kinds of creative effort leaves the valuation of the intellectual production to be determined expost, by the willingness of users to pay; it thereby avoids having society try to place a value on the creative workex ante, as would be required under alternative incentive schemes, such as offering prospective authors andinventors prizes or awarding individual procurement contracts for specified works.property institutions, transactions costs, and access to informationthe solution of establishing a monopoly right to exploit that òfirst copyó (the idea protected by the patent orthe text protected by copyright), alas, turns out not to be a perfect one. the monopolist will raise the price of everycopy above the negligible costs of its reproduction, and, as a result, there will be some potential users of theinformation good who will be excluded from enjoying it. the latter represents a waste of resources, referred to byeconomists as the òdeadweight burden of monopolyó: some peopleõs desires will remain unsatisfied even thoughthey could have been fulfilled at virtually no additional cost. economists as a rule abhor òwaste,ó or òeconomicinefficiency,ó but they believe in and rather like the power of market incentives. not surprisingly, then, the subjectof intellectual property policies has proved vexatious for the economics profession, as it presents numerousthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 129situations in which the effort to limit unfair competition and preserve incentives for innovation demonstrablyresults in a socially inefficient allocation of resources.there is not much empirical evidence as to how altering the legal conditions and terms of intellectual propertyrights translates into change in the overall strength of economic incentives for the producers, or about the effectiveness of bigger incentives in eliciting creative results; nor is it a straightforward matter to determine the way inwhich holders of a particular form of intellectual property right would choose to exploit it, and the consequentmagnitude of the resultant social losses in economic welfare (the deadweight burden). without reliable quantitative evidence of that kind, obviously, it is hard to decide in which direction to alter the prevailing policy regime inorder to move toward the notional optimum for any particular market.the difficulties of arriving at òscientific closureó on such matters, combined with conflicts of economicinterests over the distribution of the benefits of new knowledge, quite understandably, have sustained a longhistory of intense debate in this area. in each era of history new developments affecting the generation or thedistribution of knowledge, give rise to a revival of these fundamental questions in new guises. today, the hotissues arise from questions concerning the desirability of (a) curtailing patent monopolistsõ rights by lettinggovernments impose compulsory licensing of the local manufacture of certain pharmaceutical products, or of somemedical devices; (b) providing those engaged in noncommercial scientific research and teaching with automaticòfair useó exemptions from the force of intellectual law; and (c) permitting purchasers of copyright protected cdsto freely share music tracks with others by means of peertopeer distribution over the internet.there is no easy general solution to this class of economic problems, and useful answers to the basic questionsraised (e.g., are new rights that would better address the new circumstances required, and, if so, what form shouldthey take?) will vary from one case, area, or situation to the next. most economic and legal analysis favorsprotecting broad classes of intellectual works, rather than very specific forms that are more likely to be renderedeconomically obsolete. but having flexible legal concepts that are meant to be applied in novel situations createsadded uncertainties for innovators. there is likely to be a protracted period of waiting and struggling to have thecourts settle upon an interpretation of the law that is sufficiently predictable in its specific applications to providea reliable guide for commercial decision making.another general principle that finds widely expressed approval is that of harmonizing intellectual propertyrights institutions internationally, so that arbitrary, inherited legal differences among national entities do notinterpose barriers to the utilization of the global knowledge base in science and technology. the catch in this,however, is that harmonization rarely is a neutral procedure. representatives of polities usually are loathe to cedeproperty rights that their constituents already possess, and, consequently, programs of harmonization turn out toimpart an unwarranted global bias towards expanding the range of property rights that will be recognized andraising the strength of the protections afforded.a more tenable broad policy position on this contested terrain may be derived from the recognition that thegeneration of further knowledge is among the major important uses of new knowledge, and, at the same time, thereare enormous uncertainties surrounding the nature and timing of the subsequent advances that will stem from anyparticular breakthrough. this is especially true of fields where new discoveries and inventions tend more readily torecombine in a multiplicity of ways that generate further novelties. a reasonably clear policy implication followsfrom this, and from the additional observation that although we will seldom be able to predict the details and futuresocial value attaching to the consequences of a specific advance in knowledge, it is far more certain that there will bea greater flow of entailed discoveries if the knowledge upon which they rest remains more accessible and widelydistributed. therefore, rather than concentrating on raising the inducements to make òhardtopredictó fundamentalbreakthroughs, it will be better to design intellectual property regimes in ways that permit noncollusive pooling andcross licensing. as a practical matter, this consideration generally would call for raising the novelty requirements forpatents, awarding protection for narrower claims, requiring renewals with increasing fees, and other, related measures. all of these steps would encourage entry into the process of generating further knowledge by utilizing thebreakthroughs that have occurred and been adequately disclosed (see david and foray, 1995).the import of this is to strictly limit the scope of grants of monopoly rights over research tools and techniques,curtailing the freedom of the rightsholders to levy whatever òtaxó they wished upon others who might use suchinventions and discoveries in order to generate still further additions to the knowledge base. collective knowledgethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.30the role of s&t data and information in the public domainenhancement is thwarted when discoveries cannot be freely commented upon, tested by replication, elaboratedupon, and recombined by others. in other words, intellectual property regimes designed to make it easier for manyto òsee farther by standing on the shoulders of giantsó would appear likely to be more fruitful than a strategy thatrenders those shoulders less easily mounted by others, in the hope that this would stimulate the growth of more,and taller ògiants.óthe extension of monopoly rights over the application of particular research tools in the life sciencesñtechniques such as polymerase chain reaction and monoclonal antibodies, new bioinformatic databases and searchengines, as well as generic information about the structure of genetic material and the way that these govern theproduction of proteinsñis coming to be seen as especially problematic. the issuing of such patents may indeed beresponsible for stimulating more commerciallyoriented r&d investment by pharmaceutical companies, andothers who look forward to selling them access to new information. yet, intellectual property protection in thissphere is likely to impose heavy dynamic welfare losses on society. it will do so by impeding access to existinginformation, or by increasing the wastage of resources in functionally duplicative research aimed at avoidingpatent licensing charges. this raises the cost not simply of research directed toward producing a specific newproduct (e.g., diagnostic test kits for a particular class of geneticallytransmitted conditions), but also of exploratory research that may enable the future creation of many applications, including those that still are undreamed of.to use the evocative phrasing of a leading european scientist, cooperatively assembled bioinformatic databasesare permitting researchers to make important discoveries in the course of òunplanned journeys through information space.ó if that space becomes filled by a thicket of property rights, then those voyages of discovery willbecome more troublesome and more expensive to undertake, unanticipated discoveries will become less frequent,and the rate of expansion of the knowledge base is likely to slow.popular wisdom maintains that ògood fences make good neighbors.ó this may apply in the case of twofarmers with adjacent fieldsñone growing crops and the other grazing cattleñor gold diggers excavating neighboring concessions. but unlike land, forage, or other kinds of exhaustible resources, knowledge is not depleted byuse for consumption; data sets are not subject to being òovergrazed,ó but instead are likely to be enriched andrendered more accurate as more researchers are allowed to comb through them (david, 2001).the issues just examined are entangled with other, and include difficult problems concerning the institutional(as distinct from the technological) determinants of human beings ability to enhance their òcapabilitiesó by findingand making use of existing repositories of knowledge and sources of information (foray and kazancigil, 1999).they involve special problems of access to scientific and technological knowledge relevant to developing countries, and raise complex issues of what it implies for resource allocation to insist that every individual in a societyhas a right to benefit from the collective advance of human knowledge affecting such fundamental, capabilityenhancing conditions as health and education.a delicate attempt at regaining a better balance between protection of the public domain of knowledge fromfurther encroachments by the domain of private property rights, is needed at least in regard to some sectors whereservices are recognized to profoundly affect human wellbeing (e.g., health, education). the notion of a universalright to health appears to have the strength to countervail against the national and international campaigns led bypharmaceutical companies to secure intellectual property owners the right to unregulated exploitation of theirpatents. but one must not be deluded into supposing that appeals to principles of equity alone will be sufficient indeciding such contests in the area of political economy.some subtleties in gauging the effects of ipr protections on the conduct of scienceapart from a few anecdotes of the sort that already have been introduced in this discussion, is there any reallysystematic body of empirical material on which to base the warnings that are being sounded about the possibleimpact of increased international property rights on exploratory research, and academic science more generally?should we not insist on policy action being evidencebased? more specifically, how can we evaluate the potentialfor the enforcement of database rights to impede the exploitation of bioinformatic techniques, and scientificdiscoveries more generally? heller and eisenberg (1998) popularized the phrase òtragedy of the anticommonsó inan article published in science, suggesting that the great increase of patenting in the biomedical and in biotechnolthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 131ogy area more generally might actually inhibit innovation, especially where inventions and discoveries requiredthe assembling of an assortment of complementary bits of knowledge and research tools, each of which might beowned by distinct parties. the argument, basically, is that fragmented ownership rights invite opportunisticbargaining strategies (holdouts, overpricing), and so raise negotiation costs that projects which otherwise wouldbe privately profitable, and possibly of great social value, would not be undertaken. but heller and eisenberg didnot substantiate the existence of a potent òanticommons effectó by producing concrete instances of such losses.not surprisingly, the claim that granting greater protection to iprs might adversely affect investment in theproduction of knowledge, by increasing the costs of collaborative projects among the property holders, has beenreceived with skepticism in some quarters, and therefore has attracted the attention of empiricallyminded economists. a recent study by walsh, arora, and cohen (2002) set out to look for evidence on the question of whetherserious anticommons effects had materialized in the biotechnology sector, which is where heller and eisenbergõsarticle warned it was likely to appear. their methodology was the survey of participants in business and academicresearch organizations, and the thrust of their findings was that while there were a few isolated instances of seriousdifficulties in working out the ipr arrangements among firms, and between firms and universities, their interviewsdisclosed nothing resembling a òtragedy.ó rather more serious reservations were sounded, however, regarding theimpediments to research discoveries that might be caused by the patenting of fundamental research tools, an issuethat walsh, arora, and cohen treated as distinct from that of the anticommons effectñeven though eisenberg(2001) evidently regards the two as closely related.while this study was carefully carried out and reported in a balanced and qualified fashion, it gave littleattention to the issue of what sort of indications might be elicited by interviews that would establish the existenceof anticommons tragedies. the interview protocols followed in questioning managers, lawyers, and researchers inbiotechnology and pharmaceutical companies, and also a much smaller number of university scientists (numbering10 among the 70 interviews conducted) are not described in any detail by walsh, arora, and cohen. in particular,the question of exactly what constitutes a òblockageó or òbreakdownó to a biomedical research project is neverspecified. so it is difficult to evaluate the responses that the study reports elicited, but there clearly are potentialproblems in interpreting some of the reported testimony. (the same must be said with regard to the interviewmaterial from the national institutes of health working group on research tools, discussed by eisenberg[2001].)to illustrate the methodological problem, let us suppose the procedure involved asking interviewees: òhaveyou experienced serious obstacles to research you have tried to undertake, due to conflicts arising over intellectualproperty rights, difficulties that prevented you from pursuing worthwhile projects?ó the reported responses, byand large, would be consistent with the informants having replied: ònot really, we are quite able to do our thing.óone might wonder what the investigators thought they might be told. would the heads of research departments saythat their research performance was no longer as good as it once had been (no matter what the alleged cause)? thepoint here is a simple and familiar one: the way that interview questions are phrased is a delicate matter in such aninquiry.in the case of the university biomedical researchers interviewed by walsh, arora, and cohen (2002), a typicalresponse to the question of whether patenting of research tools was impeding their research seems to have beensomething along the lines: ònot really. we donõt pay attention to it, and the firms seem reluctant to enforce theirpatents against us.ó now it is quite correct that if property rights are granted people do not voluntarily comply withthe intent of the law establishing those rights, and the rightholders do not enforce their legal claims, it cannot besaid that there will be any effect of the statutory change. but, surely, to conclude that òthere is no problemó in sucha case is rather misleading. there is no effect because the cause has yet to happen. the proper conclusion is: wecanõt say, what the effect of the ipr regime will be in this instance, except that when a cease and desist injunctionis brought against one of these professors, and her university is charged with patent infringement and sued fordamages (perhaps by another university that holds the patent on one of those research tools), it is going to be a bigshock.the problem of interpreting the evidence of the survey responses in other respects is a bit more subtle than wassuggested by the previous remarks about possible reporting biases, arising from the framing of the questions. theanticommons argument can be given a naive interpretation or one that is economically sophisticated, and correthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.32the role of s&t data and information in the public domainspondingly, the evidence can be read either naively or in a sophisticated manner. let us start by consider thealternative styles of interpretation of the putative òanticommons effectó:¥nałve. parties to a potentially productive coalition will see only the value of cooperating for a commonbenefit and will ignore the possible costs of contracting. so, if ipr has the effect of raising the partiesõ valuationof their own contribution to the collective project, and makes it possible for them to deny others access to thatcontribution, the negotiation of cooperative agreements will be surprisingly difficult, and frequently these will fail.one should be able to find records, or elicit testimony of such failures. look for them in order to test the anticommons hypothesis.¥sophisticated. a wellknown essay by ronald coase, the nobel laureate in economics, pointed out thatthe institutional arrangements that assigned property rights to some agents would only affect the efficiency ofresource allocation among them if there were zero òcosts of transacting,ó of arriving at a contract in which thegains from trade would be secured for the collectivity and distributed among them. agents understanding thisshould consider ex ante the likely benefit of a contract for the exchange of assets (entitlement to rent streams), andthe costs of negotiating such a contract. if they do this, they will take account of changes in institutions that alterthe property rights of the parties, and the likely affect of this on the nature of the contracting process and its costs.so a change in property rights that raises everyoneõs assessment of what they should get from the same cooperativeproject would be perceived as raising the costs of contracting sufficiently to render it foolish to pursue someprojects, namely those at the lower marginal utility end of the ranking. note that i use òutilityó to cover bothprojects with lower expected rates of return, and those where there is higher intrinsic technical or commercial risk(in the sense of higher variance relative to the expected rate of return).rational agents, therefore, will discard projects as not being worth serious consideration. if they are asked forexamples of projects that were òblockedó or òabandonedó because of high transactions costs, they should notreport any higher frequency of such events following the institutional change than they reported before. theremight be an initial òlearningó period in which mistakes were more frequent, but there is no reason why theequilibrium level of failed negotiations should be raised.what the sophisticated view of the matter suggests is that to find the òeffectó of the institutional change, onehas to estimate what would happen in the counterfactual world: what projects that were not seriously consideredwould have been considered. the foregoing line of thought might suggest that the nature of the projects that wouldbe undertaken following the change in the ipr regime would be found to have shifted toward those with thefollowing characteristics:1.the distribution of initial property rights among the participating was already highly asymmetric and therelative disparity in bargaining power would not be materially changed by the altered property rights regime, so theestimated transaction costs would not be significantly affected;2.the private expected rate of return was higher than the norm for the previously undertaken projects, and socould justify the higher contracting costs of putting the collaboration together;3.the riskreturn ratios for the projects undertaken are found to have been lower than those among theprojects previously undertaken.in other words, an institutional change that raises the marginal costs of transactions of a particular kind neednot actually increase the amount of resources consumed by such transactions; rather it may push resources intoother channels, and leave a gap between the marginal rates of return that the realized projects in that area yield(gross of negotiation expenses) and the rates of return on other kinds of projects. that gap is a measure of the socialburden of òroyalty stacking,ó òblocking patents,ó etc.unfortunately, ideas are extremely heterogeneous. it is rare that the same options for exploration presentthemselves at successive points in time; in the world of research the dictum of heraclitus holdsñone cannot dipinto precisely the same stream of ideas twice. because the set of projects to create knowledge will not presentthemselves after the legal system protecting ipr changed will not be exactly the set that was available before, thegap of discarded opportunities can never be measured exactly. what one could do, in principle, is to examine thecharacteristics of the entire portfolio of research projects that were being undertaken before, and after the instituthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 133tional innovation. of course, it would be necessary in doing so to control for the influence of other temporallycorrelated changes that would affect those portfolio characteristics.the hunt for evidence of a òtragedyó in the form of a lost opportunity is inextricably encumbered by theproblem of documenting a counterfactual assertion of the form: if we had not done that, the world would now bedifferent. in discussion of such propositions, rhetorical victories tend to go to the side that can shift the burden ofproof to the shoulders of their opponents, simply because conclusive proof of a counterfactual assertion will beelusive. thus, for those who see the system of ipr protection as fundamentally benign, the debating strategy is todemand that the critics show the social efficiency losses that they claim it is causing. but why should thatassignment of the burden of proof be accepted? although in the case of ordinary, physically embodied commodities there is a theoretical presupposition that competitive markets and welldefined private property rights cansupport a sociallyoptimal equilibrium in the allocation of resources, that presumption ceases to hold in the realmof information and knowledge.conclusionthe elision effected by the application of the metaphor of òpropertyó to the domain of ideas has been fruitfulin many regards. but there is a danger in permitting those who are enthusiastic for more and stronger ipr toemploy this as rhetorical device as a way of avoiding the burden of proof. they should be asked to show that themoves already made in that direction have not been economically damaging, that further encroachments into thepublic domain of scientific data and information would not be still more harmful, and that society would notbenefit by adopting a policy that was just the opposite of the one they support.referencesarrow, kenneth j. 1962. òeconomic welfare and the allocation of resources for inventions,ó in the rate and direction ofinventive activity: economic and social factors, r.r. nelson ed., princeton university press, princeton.cowan, r., p.a. david and d. foray. 2000. òthe explicit economics of knowledge codification and tacitness,ó industrialand corporate change, 9(2), (summer): pp. 211253.dasgupta, p. and p.a. david. 1987. òinformation disclosure and the economics of science and technology,ó in arrow and theascent of modern economic theory, g. feiwel, ed., new york university press, new york, pp. 519542.dasgupta, p., and p.a. david. 1994. òtoward a new economics of science,ó research policy, vol. 23: pp. 487521.david, p.a. 1993. òintellectual property institutions and the pandaõs thumb: patents, copyrights, and trade secrets ineconomic theory and history,ó in global dimensions of intellectual property rights in science and technology, (m.wallerstein, m. mogee, and r. schoen, eds.), national academy press, washington, d.c.david, p.a. and d. foray. 1995. òaccessing and expanding the science and technology knowledge base,ó sti review:oecdñscience, technology, industry, no.16, fall: pp. 1368.david, p.a. 1998a. òcommon agency contracting and the emergence of ôopen scienceõ institutions,ó american economicreview, 88(2), may.david, p.a. 1998b. òreputation and agency in the historical emergence of the institutions of ôopen scienceõ,ó center foreconomic policy research, publication no. 261, stanford university (march 1994, revised december 1998).david, p.a. 2000. òthe digital technology boomerang: new intellectual property rights threaten global ôopen scienceõ,ó(presented at the world bank abcde conference in paris, 2628 june, 2000.) department of economics working paper0016, stanford university, october 2000. (proceedings of the world bank annual conference on development economics europe, 2000, j. bas and j.e. stiglitz, eds., cdrom edition, fall 2001.)david, p.a. 2001. òthe political economy of public science,ó in the regulation of science and technology, helen lawtonsmith, ed., palgrave, london.david, p.a., d.c. mowery, and w.e. steinmueller. 1992. òanalyzing the payoffs from basic research,ó (with) economics ofinnovation and new technology, vol. 2(4): pp. 7390.eisenberg, r.s. 2001. òbargaining over the transfer of proprietary research tools: is this market failing or emerging,ó ch. 9 inexpanding the boundaries of intellectual property, r. dreyfuss, d. l. zimmerman and h. first, eds., oxford universitypress, new york.foray, d. and a. kazancigil. 1999. òscience, economics and democracy: selected issues.ó mostunesco discussion paperno 42, unesco, paris.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.34the role of s&t data and information in the public domainheller, m.a. and r.s. eisenberg. 1998. òcan patents deter innovation: the anticommons in biomedical research,ó science280 (1 may): pp. 698701.holton, g. 1996. einstein, history, and other passions: the rebellion against science and the end of the twentieth century,addisonwesley publishing, reading, ma.merton, r.k. 1973. the sociology of science: theoretical and empirical investigations. n.w. storer, ed., chicago universitypress, chicago.nelson, r.r. 1959. òthe simple economics of basic scientific research,ó journal of political economy, 67.walsh, j.p., a. arora, and w.m. cohen. 2002. òresearch tool patenting and licensing and biomedical innovation,ó forthcoming in the operation and effects of the patent system, the national academies press, washington, d.c. in 2003.ziman, john. 1994. prometheus bound: science in a dynamic steady state. cambridge university press, cambridge.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.355scientific knowledge as a global public good:contributions to innovation and the economydana dalrymple1òthe preeminent transnational community in our culture is science,ó richard rhodes, 1986òif the potential of modern science is to be realized, there is no alternative to global public goods and institutions,ó lawrence summers, 2000introductionscientific knowledge in its pure form is a classic public good. it is a keystone for innovation, and in its moreapplied forms is a basic component of our economy. although recent technical advances have stimulated itsgeneration and greatly accelerated its spread, other forces may limit its publicdomain characteristics.the concept of public goods is not new. although it is being applied in an increasing number of areas of socialimportance, this does not yet seem to be true of the natural sciences. science is seldom mentioned in the publicgoods literature, or public goods in scientific literature. yet the combination is a logical and useful one. a feweconomists and health specialists have recognized this, but the same cannot be said of the scientific communitymore generally.2the related concept of public domain is also not new. although it is even broader conceptually (see drache,2001), it has found a more specific meaning, particularly among lawyers, in the context of intellectual propertyrights (iprs). lawyers, however, seem to assume the availability of public goods and scientific knowledge andtheir focus may be limited to national and local legal systems.why such limited or partial attention to what should seem a most appropriate and useful common concept? isit because the three professional groups most likely to be involvedñscientists, economists, and lawyersñhave notviewed public goods in a broader and more integrated light? this symposium provides a most appropriateopportunity to begin to try to bridge the gap.in doing so, a few definitions might help set the stage. data and information are at once both key componentsin the generation of scientific knowledge and among its major products: they are both inputs and outputs (arrow,1962, p. 618). however, knowledge in general is broader, less transitory, and more cumulative. it is derived fromperception, learning, and discovery. scientific knowledge, in particular, is organized in a systematic way and istestable and verifiable. it is used to provide explanations of the occurrence of events (mayr, 1982, p. 23).1acknowledgments: the author benefited greatly from the advice and assistance of a number of individuals during the preparation of thischapter, particularly john barton, paul david, and vernon ruttan.2among economists, the most prominent proponent of scientific goods in the international arena is jeffrey sachs, who has been primarilyconcerned with expanding health and agricultural research in and for developing nations (sachs, 1999, 2000a). his most recent effort, as partof a world health organization (who) study, has led to a proposal for a $1.5 billion annual expenditure for a new global health researchfund (who, 2001, pp. 8186; jha, et al., 2002). a previous who report (1996) argued that research and development expenditures in healthwere an important international public good. dean jamison, who was involved in that report, has also written on the subject elsewhere (2001).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.36the role of s&t data and information in the public domainthe topic itself is, of course, quite ambitious. moreover, this may be the first attempt to take it on in a fairlycomprehensive way. hence i will only attempt to provide an introduction. three main topics will be taken up, invarying proportion: (1) principal concepts, (2) provision and use, and (3) implementation. the focus will be onscientific knowledge at the international level, particularly with respect to developing countries. my perspective isthat of an agricultural economist and sometime historian. my approach involves a rather wideranging review ofliterature blended with long personal experience in international agricultural research. others might well followquite different routes and illustrate different dimensions. i encourage them to do so.principal conceptsin examining scientific knowledge as a global public good, i will start by building on several venerableconcepts and components. some of them have been partially woven together before; others have not. each has itsown history and is important to understanding the whole. and they need to be combined with some contemporaryeconomic perspectives.historical perspectivesthe starting point is public goods, which were long considered, at most, at the national level and for publicinstitutions and services. hence there is a need to expand the definition in several directions: to knowledge as aglobal public good, to global scientific knowledge, and to recognition of the role played by iprs.knowledge as a public goodadam smith laid the basis for the concept of public goods in the wealth of nations in 1776 when he stated:the third and last duty of the sovereign is that of erecting and maintaining those public institutions and those publicworks, which, though they may be in the highest degree advantageous to a great society, are, however, of such anature, that the profit could never repay the expense to any individual or small number of individuals, and for whichit cannot be expected that an individual or small number of individuals should erect or maintain.the development of more sophisticated theories of public goods began in the last quarter of the 19th century(machlup, 1984, p. 128). recent use of the term by economists is usually traced back to two short articles by paulsamuelson in the mid1950s (1954, 1955). it became a central concept in public finance, in part due to the writingsof musgrave and buchanan (machlup, 1984, pp. 128129; olson, 1971; buchanan, 1968). public goods, as theyhave generally come to be known, have two distinct characteristics: (1) they are freely available to all and (2) theyare not diminished by use. these properties are often expressed by economists, as we shall see later, in terms ofnonexcludability and nonrivalry.in the context of scientific knowledge, a ògoodó is viewed here, following some dictionary variants, as havingor generating two key qualities: (1) it is tangible in the sense that it is capable of being treated as a fact, orunderstood and realized; and (2) it has intrinsic value in terms of relating to the fundamental nature of a thing. itis neutral with respect to the ògoodó effect on society, although that also is usually presumed to be good (to bediscussed later), and excludes money.the public goods characteristic of ideas and knowledge has long been noted, first by st. augustine, sometimebetween 391 and 426 (wills, 1999), and then by thomas jefferson, in 1813 in a frequently cited letter on patents (1984).3their views were carried further by powell in 1886 when he stated: òthe learning of one man does not subtract from thelearning of another, as if there were to be a limited quantity to be divided into exclusive holdings. . . . that which one mangains by discovery is a gain to other men. and these multiple gains become invested capital. . . .ó3òhe who receives an idea from me, receives instruction himself without lessening mine; as he who lights his taper at mine, receives lightwithout lessening mine.óthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 137the nature and spread of scientific knowledge4the adjective òscientificó can be traced to aristotle and at some point entered the romance languages. its usein english, however, dates only to about 1600 and was synonymous with knowledge. in its earlier incarnations, itreferred to demonstrable knowledge as compared with intuitive knowledge. it was at first referred to as naturalphilosophy in english. emphasis was on deductive logic, which was useful for confirming what was alreadyknown, but not for original discovery.the situation began to change in the early 1600s with the writings of francis bacon, who was abeliever in inductive logic and the experimental method. the latter made it possible to discover andunderstand new facts about the world. from about 1620, with the publication of his novum organuum,there was a shifting of the philosophical point of view toward baconõs interpretation. this process reachedits full realization by 1830.with the new meaning of scienceñònatural philosophersó would no longer doñthere was an increasingneed for a new word for its practitioners. in 1834, william whewell of cambridge university rather casuallyproposed the term òscientist.ó in 1840 he more seriously but very briefly said: òwe need very much a nameto describe a cultivator of science in general. i should incline to call him a scientistó (whewell, 1840/1996,p. cxii).5bacon, among his other insights, was perhaps the first to record his views on the wider nature ofknowledge when he wrote: òfor the benefits of discoveries may extend to the whole human raceó and òforvirtually all timeó (bacon, 1620/2000, p. 99). he clearly saw the benefits of attempting to reach beyondnational boundaries, as was evident in his treatment of three levels of ambition, the third of which was put inthese terms: 6 òbut if a man endeavor to establish and extend the power and dominion of the human race itselfover the universe, his ambition . . . is without a doubt both a more wholesome thing and more noble than theother twoó (henry 2002, p. 16).7the age of global exploration that followed columbus did much to bring about a global exchange of biological material and associated information. in the view of one historian, ònothing like this global range of knowledgehad ever been available before,ó and it proved to be òa boost to europeõs incipient ôscientific revolutionõ.ó òin thisway, the exchange òmade a major contribution to the longrun shift in the world balance of knowledge and poweras it tilted increasingly toward the westó (fern⁄ndezarmesto, 2002, p. 167).bonaparte gave a different twist to the process. as part of his òexpeditionó to egypt in 1798, which includedsome of the finest scientific minds in france, he founded the òinstitute of egypt.ó the essential goals of theinstituteõs researchers, in contrast to the military side, òwere the progress and propagation of sciences in egyptóand òthe conquest of knowledge and the application of knowledge to manõs lotó (herold, 1962, pp. 28, 151, 164176; also see sol”, 1998).other approaches were undertaken by other european colonial powers during the next century. botanicalgardens were to play a particular role (drayton, 2000; plucknett et al., 1987; pardey et al., 1991). brazilian rubberseeds and the peruvian cinchona tree, a source of quinine, were prime targets (alvim, 1994, p. 426; drayton, 2000,pp. 236, 249; honigsbaum, 2000). in the latter case, collecting expeditions were defendedñwith some justificationñas a òduty to humanityó (honigsbaum, 2001, p. 81). science and research played a larger role later, notablyin the spanish caribbean, in helping improve the production of export crops (mccook, 2002).4the first part of this subsection is largely based on a littleknown article by ross (1962)ñreprinted in ross (1991)ñand to a lesser degreeon henry (2002) and bacon (2000, introduction by jardine). for a further discussion, see òscientific knowledgeó in machlup (1980, pp. 6270). the subject is also briefly noted by barzun (2000, pp. 191, 544) and bernal (1965, p. 32).5merton (1997) indicates that whewell, evidently stung by the hostile reception of the term by his english colleagues (òman of scienceóprevailed until about 1910), used the word only once during the remainder of his career.6with respect to the first two, bacon wrote: òthe first is of those who desire to extend their own power in their native country; which kindis vulgar and degenerate. the second is of those who labor to extend the power of their country and its dominion among men. this certainlyhas more dignity, though not less covetousness.ó7thus it is not surprising that when the royal society of london was established in the 1660s, òforeign corresponding membersñôtheingenuous from many considerable parts of the worldõñwere eagerly recruitedó (barzun, 2000, pp. 210211).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.38the role of s&t data and information in the public domaindevelopment of intellectual property rights8many forms of knowledge are, of course, presently linked with iprs. iprs may seem a product of moderntimes, but they had their roots in the middle ages, when less attention may have been given to the socialdimension. concern about their effect on social welfare appears to have been of more recent and domestic origin.patents have existed in various forms since the late 1400s. the first general promise of exclusive rights toinventors was made in a statute enacted in venice in 1474 (machlup, 1984, p. 163). they became more widelyadopted in europe in the 1600s. the u.s. congress first passed a patent statute in 1790 and in 1836 the patentoffice was established. it had a trained and technically qualified staff. kahn (2002) states that the system wasbased on the presumption that social welfare coincided with the individual welfare of inventors. courts subsequently òexplicitly attempted to implement decisions that promoted economic growth and social welfare.ócopyrights have also existed in various forms since the late 1400s. in the united states, the earliest federallaw to protect authors was passed in 1790. policymakers, according to kahn, felt that copyright protection wouldserve to increase the flow of learning and information, and by encouraging publication would contribute to theprinciples of free speech. moreover, the diffusion of knowledge would also ensure broadbased access to thebenefits of social and economic development.in practice, patents were fairly narrowly construed and copyright was interpreted more casually. kahn viewsthis as appropriate: social experience shows that they warrant quite different treatment if net social benefits are tobe realized.contemporary economic perspectivesthe next step in building a conceptual base is to move to some more recent perspectives, largely by economists, about the role played by knowledge in thinking about economic growth and then, in a more applied way, ininternational development programs.knowledge and economic growththe role of knowledge, and particularly of scientific knowledge, in economic growth has received relativelylittle concerted study.9 kenneth boulding was one of the first to draw attention to the connection.10 in 1965, hestated at a meeting of the american economic association:the recognition that development, even economic development, is essentially a knowledge process has slowly beenpenetrating the minds of economists, but we are still too much obsessed by mechanical models, capitalincomeratios, and even inputoutput tables, to the neglect of the study of the learning process (boulding, 1966, p. 6).only machlup (1980, 1984) appears to have taken up the subject in a comprehensive manner, but even he did notget very far into the development side.why? part of the problem is that it is difficult to reduce knowledge to numerical form so that it can be used ineconomic models. as boulding also said: òone longs, indeed, for a unit of knowledge, which might perhaps be calleda ôwit,õ analogous to the ôbitõ used in information theory; but up to now at any rate no such practical unit has emergedó(boulding, 1966, pp. 23; also see desai 1992, p. 249 and romer, 1994, p. 20). another problem was that there wasan uneasy relationship between growth economics that was macroeconomic in orientation and development economics that was more microeconomic and multidisciplinary in its approach (ruttan, 1998; altman, 2002).8this subsection is largely drawn from kahn (2002). additional historical information is provided in machlup (1984, p. 163) and david(1993, pp. 4454).9knowledge plays an implicit role in other studies, especially of those relating to the effects of research and development or information(see, for example, machlup, 1984, pp. 179182).10boulding might seem to have been preceded, on the basis of titles, by hayek in 1937 and again in 1945, but hayek was focused oneconomic or market knowledge and information (similar to information about attributes, to be noted below).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 139romer, a macroeconomist, did go on to do some seminal work (first reported in 1986, with further contributions in 1990, 1993, and 1994). it was based on the assumption that longrun growth is driven primarily by theaccumulation of knowledge. knowledge (harking back to powell, 1886) was considered the basic form of capital.new knowledge is the product of research and will grow without bound. in the industrial sector its production mayhave an increasing marginal product, in part because the creation of new knowledge by one firm can have apositive effect on the production possibilities of other firms because knowledge cannot be perfectly patented orkept secret. hence, knowledge, even if generated for private gain, has an important public good characteristic.romer suggested that an intervention that shifts the allocation of current goods away from consumption andtoward research is likely to improve welfare.knowledge for developmentthe importance of knowledge for development was reflected by the world bank during the period whenjoseph stiglitz was chief economist. at that time the bank devoted its world development report for 19981999to òknowledge for development.ó it distinguished two types of knowledge: technical and attributes.11 the unevendistribution of the former was referred to as the knowledge gap and the latter as an information problem. both aremore severe in developing than in developed countries.closing the knowledge gap was viewed as involving three steps: (1) acquiring knowledge from the restof the world and creating it locally through research and development, (2) absorbing knowledge, and (3)communicating knowledge. the green revolution, òthe decadeslong worldwide movement dedicated to thecreation and dissemination of new agricultural knowledge,ó was presented as òa paradigm of knowledge fordevelopment.óin a separate article, stiglitz (1999) reiterated some of these points, but went on to note that although òresearchis a central element of knowledge for development,ó it is also a òglobal public good requiring public support at theglobal level.ó the latter requires collective action, and òthe challenge facing the international community iswhether we can make our current system of voluntary, cooperative governance work in the collective interests ofall.ó this indeed is a central question.provision and usethe provision and use of scientific knowledge for the benefit of society are inviting to dream about, butare of course more difficult to realize. in this section an attempt will be made to identify and relate some ofthe more important steps and considerations. they start with some relevant aspects of the nature of knowledge, then move to the generation and embodiment of knowledge, factors influencing embodied knowledge,and finally consideration of the regulatory structure (as it applies to iprs) to the provision and use ofknowledgebased goods.nature of knowledgeas the world bank (19981999, p. 1) put it in one of its more lyrical moments, òknowledge is like light,weightless and intangible, it can easily travel the world, enlightening the lives of people everywhere.ó there is, ofcourse, more to the story as the bank quickly goes on to admit. although easy to transmit, knowledge can beexpensive to produce. and scientific knowledge must normally be transformed into some more tangible form,sometimes referred to as embodied knowledge (processes, products, policies) to be of social benefit. the latter areoften grouped under the category of technology.11examples given of technical knowledge, or knowhow, are nutrition, birth control, software engineering, and accounting. knowledgeabout attributes includes the quality of a product, the credibility of a borrower, or the diligence of a worker and are critical for effectivemarkets.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.40the role of s&t data and information in the public domainrelationship of science and technologyscience is traditionally viewed as providing the insights needed for the development of technologies. barzun(2000, p. 205) notes that earlier in history, technologyñin the form of the practical artsñòcame earlier and wasfor a long time the foster mother of science.ó he continues, òinventors made machines before anybody couldexplain why they worked . . . practice before theory.ó similarly, david (1992, p. 216) notes that òtechnologicalmastery may run far ahead of science and is in many regards both a stimulus to scientific inquiry and the meanswhereby such inquiries can be conducted.óover time, the relative role of science has probably increased and in the minds of many, the relationshiphas reversed. in barzunõs words, òscience finds some new principle and applied science . . . embodies it in adevice for industry for domestic use.ó the unqualified version of this concept is sometimes termed thesimplest linear model and is probably greatly oversimplified (see david, 1992, p. 216). in reality, manyfeedback loops are involved.the role of the public and private sectors in this process has also changed somewhat. traditionally, the publicsector has been associated with the provision of basic knowledge and the private sector with more appliedtechnology and products. the former might be exemplified at the extreme end by federal support for highenergyphysics and, more usefully, to basic health research. but there are exceptions to this pattern: one is agriculturalresearch, originally one of the most important areas of federal support for research, which is relatively applied innature (for background, see dupree, 1957). and, more recently, certain portions of the private sector have becomevery involved in some important basic research, such as on the human genome, which has at least partially foundits way into the public domain. whether the private sector will find a satisfactory profit in some of this remains tobe seen, but it has altered the basic paradigm.the conventional model has also been altered by the increasing number and variety of interactions, formal andinformal, between the public and privatepublic sectors. the usual pattern has the public sector financing theprivate sector to produce what will become public goods (however, some intellectual property issues may complicate this picture). but some arrangements, and these are more unusual, have the private sector providing support touniversities, in part to gain access to their scientists and scientific knowledge.scientific goods and badsthe term public ògood,ó despite a more neutral definition adopted at the outset of this chapter, usually impliesa positive social benefit. in the case of science, however, there is often considerable question about the degree towhich some ògoodsó are good.this was not always the case. during the first flush of science in the 1800s, it would appear that it was viewedquite positively. in 1884, john wesley powell, possibly reflecting the more ebullient spirit of the times statedòthe harvest that comes from welldirected and thorough scientific research has no fleeting value, but abidesthrough the years, as the greatest agency for the welfare of mankind.ó the belief that science could be harnessedfor the benefit of all continued and perhaps peaked in the 1950s (watson, 2002, p. 375).this viewpoint, to the extent that it was shared, began to change with the advent of atomic energy and itsunfortunate direct and indirect longterm effects. for other products of science, the outcome was moremixed. silent spring revealed the dark side of ddt, but it is still one of the best and lowestcost methods ofmosquito control in developing countries (honigsbaum, 2001, p. 286; undp, 2001, p. 69). as freemandyson (1979, p. 7), a physicist, has written: òscience and technology, like all original creations of the humanspirit, are unpredictable. if we had a reliable way to label our toys as good and bad, it would be very easy touse technology wisely.óthis, of course, is not the case, and so the presumed and actual ògoodsó and òbadsó of science continue to bereported and debated daily. the agricultural area has shared fully (and perhaps more than proportionally lately),particularly with respect to biotechnology and genetically modified organisms, which can be, or become, allthingsñgood, bad, and indifferentñdepending on timing and circumstances.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 141this uncertainty, along with the possibility that the research may not accomplish even what it was originallyintended to do, also has economic implications. it means that not all scientific research activities are in retrospectnecessarily a good investment. this is true of both public and private research.12generation and embodiment of scientific knowledgescientific knowledge is usually the product of a process. sometimes it is informal and individual, but mostoften it is more formal and communal. here we will look very briefly at the linkage of invention and scientifictheory, the introduction of the research laboratory, and of the generalized process of moving from ideas andscientific knowledge to public goods.linkage of invention and scientific theoryinvention is as old as mankind. but it often consisted of chance, trial and error, and individual intuition. suchactivities seldom led to further innovative activity or to sustained economic growth. this was, according to mokyr(2002), because invention without a scientific base was difficult to duplicate and quickly ran into diminishingreturns. and such science as existed was not very concerned with practical applications.the situation began to change around 1750 with the growth of what mokyr calls òpropositionaló knowledge,which includes both scientific and artisanal or practical knowledge. during a subsequent period, which he calls theòindustrial enlightenment,ó a set of social changes occurred that resulted in growth of scientific knowledge, anincrease in the flow of information, and an attempt to connect technique with theory. the process was facilitatedbecause it was a period of relatively open science: knowledge was a public good. but up until about 1850, thecontribution of formal science to technology remained modest.establishment of research laboratoriesit has been stated that in the 1800s, a century that was differentiated from its predecessors by technology, òthegreatest invention . . . was the invention of the method of inventionó (whitehead, 1925, pp. 140, 141). researchlaboratories òinstitutionalized the process of transforming intellectual and physical capital into new knowledgeand new technologyó (ruttan, 2001, p. 82). the paths were somewhat different in agriculture and industry.the early stages of innovation were fairly simple in agricultural societies. in the late 1700s and early 1800s,more progressive (and often more affluent) farmers experimented and expanded the envelope of knowledge.however, by the mid1800s, the advantages of a more structured approach became increasingly evident in theunited states. the house committee on agriculture, in its report establishing the u.s. department of agriculturein 1862, stated that accurate knowledge of nature òcan be obtained only by experiment, and by such and so longcontinued experiments as to place it beyond the power of individuals or voluntary associations to make themó(congressional globe, 1862). thereafter, publicsponsored research at the federal and state level gradually beganto accelerate in the united states.13industrial research also dates from the latter half of the 1800s. the first corporate research laboratories wereestablished in europe by the chemical industry, particularly the dyestuffs sector, in the late 1860s and 1870s(homburg, 1992; mokyr, 2002, p. 85). edison was the first individual in the united states to establish a significant12the evaluation of the benefits of any public enterprise is complicated by the need to take into account what is vividly known as the excessburden or deadweight loss of taxation. i have discussed this concept elsewhere in terms of public agricultural research (dalrymple, 1990).13some european nations, especially england and germany, initially moved more quickly than the united states. for further details on thedevelopment of public agricultural research in the united states, see dupree (1957, pp. 149183); ota (1981); and ruttan (2001, pp. 207211). the u.s. department of agriculture established its first field trials on what is now the mall between 12th and 24th streets andindependence and constitution avenues in 1865.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.42the role of s&t data and information in the public domainresearch organization when he did so at menlo park, new jersey, in 1876. the first u.s. industrial laboratory wasestablished by general electric in schenectady, new york, in 1900 (reich, 1985).since the turn of the 20th century, public and private research laboratories have played a key role in innovation and creation of new knowledge. it has not been, in historical terms, a long period.from ideas to public and private goodsgiven some thoughts about the nature of knowledge and research structures, it may now be useful to attemptto show how they and other elements interact in the process of moving from ideas to public and private goods andto social benefit. my view of the highly interactive process is outlined in figure 51.there are roughly four stages. the first is the generation of ideas and concepts by researchers and others. thesecond stage is the process of moving, through an interactive research process, from (a) ideas and concepts to (b)disembodied or pure knowledge, and from there to (c) embodied or applied knowledge (technology for short). thethird stage involves the intellectual property dimension, which i will return to in greater degree below. the fourthstage is the movement of the ògoodsóñpublic, publicprivate, and privateñinto use in society and the resulting,one hopes, social benefits. in the first three stages, actions taken will likely be influenced, although to a differingdegree, by perceptions of social needs and opportunities.throughout, there may be considerable crossover between the public and private sectors. research initiated inthe public sector can end up being utilized by the private sector and vice versa. numerous other forms ofinteraction may occur. the result can be a hybrid good (van der meer, 2002). this pattern, which is growing, canbe very productive and useful for society, but it may also complicate the intellectual property dimension.factors influencing use and adoptionit is one thing to provide knowledgebased public goods. their adoption and productive use is another. manyfactors may be involved. three important categories follow from the previous section.social benefits¥freely disclosed¥disclosed with limitationspublic patents¥not disclosed (trade secrets)¥disclosed with limitationspatentscopyrights*publicprivate interactions¥informal¥licensing¥geographic segmentation embodied or applied knowledge(products, policies)disembodied or pure knowledgeideas and conceptspublic researchand developmentnonresearch sourcesprivate researchand development* knowledge includes both data and information derived from perception, discovery, and learning. scientific knowledge is normally cumulative and verifiable. public/private goodspublic goodsprivate goodsfigure 51the evolution of scientific knowledge into public and private goods.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 143user characteristicsalthough the availability of knowledgebased public goods is a necessary condition, it is not a sufficient one. potential users must themselves have a sufficient degree of knowledge to identify, understand,and possibly adapt the public goods available to them. they must also have an appropriate policy and legalenvironment, adequate infrastructure, and adequate financial resources. the more one gets into technology, the greater the degree of need for adaptation to local conditions. this is particularly true in theagricultural area where environmental and natural resource conditions are important and may vary sharply.adaptation may well involve a further research and development capacity at the regional, national, andlocal level.ògoodó characteristicsthe main identifying characteristics of public goods, in economic terms, are their nonrival nature and theirnonexcludability. nonrival means that use by one firm or individual does not limit use by another. nonexcludablemeans use is not denied to anyone. these conditions are seldom found in totally pure form and may be limited formost purposes to (a) disembodied or pure knowledge; (b) noncopyrighted publications; and (c) some products ofpublic research programs, such as those produced by federal laboratories.14virtually everything else, strictly speaking, is an impure public good or a private good. but the degree ofimpurity varies widely and in most cases contains a significant public goods dimension. moreover, the impurity,insofar as it involves a useful privatesector contribution, may play an important role in improving the quality andusefulness of the product and maximizing social welfare. and purely private products may contribute significantlyto social welfare.the path to marketi have attempted to integrate these economic characteristics with intellectual property considerations in amarket power context in figure 52. clearly, the left side represents the relatively pure public goods situation andthe right side the relatively pure private good situation. the extreme forms of each variant are not very common.this leaves the middle as the most prevalent category composed of partially rival and excludable goods. partialexcludability is maintained with governmental participationñprincipally through copyrights, patents, or tradesecrets.15the differing paths result in differing degrees of market power or control, ranging from none (or purecompetition) to complete (or a monopoly). again the extremes are uncommon, but may be personified on thepure competition end by, as i have mentioned, some products of public research in agriculture and on themonopolistic end by actual trade secrets by the private sector. most everything else results in partial marketpower. but in any case, as a recent study concluded, ò . . . knowledge has become to an even greater degree thanbefore the principal source of competitive advantage for companies and countriesó (commission on intellectualproperty rights, 2002, p. 13).14 many state universities are beginning to patent and license their products as a source of revenue.15 both figures 51 and 52 contain reference to public patents. these are patents taken out by a public entity to ensure that the patenteditem stays in the public domain. it is made available under license at no cost or at a very nominal charge. this was, for instance, standardpractice at tennessee valley authority during the 1950s (personal communication from vernon ruttan). such patents are now uncommon.the principal reason is that the bayhdole act of 1980 allowed federal government contractors and grantees to take title to any subsequentinventions. moreover, as a result of the technology transfer act of 1986, the federal government may keep royalties from licensing itsinventions. defensive patents under 35 u.s.c.157, which provides for invention registration, are still possible, but probably would also belimited by the more general profit focus. (the previous four sentences are based on a personal communication from richard lambert, thenational institutes of health, august 9, 2002.)the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.44the role of s&t data and information in the public domainintellectual property systems and scientific informationclearly the various forms of intellectual property play a key role in the innovation process for knowledgebased public goods, especially those with some degree of privatesector involvement. what is their likely effect onthe generation, flow, and use of scientific information? fortunately, eisenberg (1987) and a more recent commission on intellectual property rights (cipr) (2002) have examined several of these issues.16secrecysecrecy may seem like an odd item to include under intellectual property systems, but one of the principaltypesñtrade secretsñrelies on a legal system for enforcement. legal trade secrecy affords a remedy in court whenindividuals disclose information to others who subsequently breach this confidence or who otherwise misappropriate the information acquired. actual trade secrecy is a strategy for protection in circumstances where not all therequirements for legal trade secrecy have been met. eisenberg (1987) thinks that, although both types involvesubstantial nondisclosure, legal trade secrecy may be more disruptive of scientific communication than actualsecrecy.secrecy1use by one firm or person does not limit use by another.2use by one firm or person precludes use by another. 3there is a research exemption in most countries, but not yet in the united states (except under the plant varietyprotection act).4generally limits the permissible subject matter and potentially exclusive basic research discoveries.5except in cases of trespass and theft.6assumes that pure exclusion or market power is not possible in the case of knowledge, at least for very long.nonrival goods 1(public and private)rival goods 2(private)nonexcludable(pure public goods)partially excludable(govt. involvement)excludable(no govt. involvement)(disclosure)copyrightpatents 3trade(legal protection4)actual(no legal protection5)publicationpublic patents((knowledgeno market power(pure competition)strong market power 6(monopolistic competition)partial market power(imperfect competition)figure 52knowledgebased public and private goods and the path to market: an economic and intellectual propertyperspective. sources: romer (1990, 1994) and eisenberg (1987).16the commission was established by claire short, the secretary of state for international development in the united kingdom, in may2001. john barton of stanford served as chair. its emphasis was on developing countries.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 145patent lawpatent laws may be more congruent with scientific norms because they are based on disclosure.17 the fit,however, may be less than perfect because (a) patents may operate to delay the dissemination of knowledge toother researchers, and (b) the granting of rights to exclude others from using patented inventions for a period ofyears threatens the free use and extension of new discoveries. eisenberg (1987, p. 180) concludes that, althoughthere are substantial parallels between patent laws and scientific norms, òthe conjunction may nevertheless causedelay in the dissemination of new knowledge and aggravate inherent conflict between the norms and rewardstructure of science.ó resolution, she thinks, will involve adjustments on both sides.although the purpose of the patent system was to stimulate invention and provide an incentive to technicalprogress, the cipr (2002, p. 123) states that over time, òthe emphasis has shifted toward viewing the patentsystem as a means of generating resources required to finance r&d and to protect investments.ó the system, intheir view, fits best a model of progress where the patented product is the result of a discrete outcome of a linearresearch process. by contrast, they note that for òmany industries, and in particular those that are knowledgebased, the process of invention may be cumulative, and iterative, drawing on a range of prior inventions inventedindependently . . . .ó (p. 124). they suggest that the cumulative model fits more current research than the discretemodel. hence a patent system that was developed with a discrete model in mind may not be optimal for a moreknowledgebased cumulative model.copyrightthe degree to which copyrights have played a significant role in stimulating or limiting the dissemination ofscientific information to date seems to be uncertain, but this may be in the process of changing, and not for thebetter. as the cipr (2002, p. 18) has noted, copyright protects the form in which ideas are expressed, not the ideasthemselves. the form, however, may have a significant impact on their use, and this is becoming more of an issuein view of changes in information technology.in any case, copyrights probably have been of much greater importance to developed than to developing nations.this position may also change: the cipr (2002, p. 106) stated, òwe believe that copyrightrelated issues havebecome increasingly relevant and important for developing countries as they enter the information age and struggleto participate in the knowledgebased global economy.ó in this case, òthe critical issue . . . is getting the right balancebetween protecting copyright and ensuring access to knowledge and knowledgebased productsó (p. 106).in addition to economic questions, there can also be more philosophical concern about basic rights. as kahn(2002, p. 53), in a paper also prepared for the commission, put it, òeven in cases where a strong copyright may benecessary to provide the incentives to create, it might be advisable to place limits of the power of exclusion in orderto promote social and democratic ends such as the diffusion and the progress of learning.ó we will doubtless hearmuch more about these issues in the future.implementation: an exampleto this point, i have largely dealt with concepts relating to scientific knowledge as an international publicgood. although many of the individual components have an extended history, heretofore they do not seem to havebeen linked together to the degree that one might expect. if so, this might suggest that the overall concept is fairlytheoretical and untestedñpossibly not implementable. fortunately, that is not the case. there is a substantialinstance of, as barzun was earlier quoted as saying, òpractice before theoryó (barzun, 2000, p. 205).in the early 1970s, the consultative group on international agricultural research (cgiar) was establishedand has operated until recently without any knowledge of many of the more conceptual issues discussed here. and17there is a complication in the case of plant germplasm. as ronald cantrell, the director general of the international rice researchinstitute, puts it, ògermplasm is unlike other types of intellectual property in that the average practitioner cannot completely benefit from thedisclosure unless a cross can be made with the patent protected germplasmó (personal communication, august 19, 2002). this underlines theneed for research exemptions (see figure 52, footnote 3).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.46the role of s&t data and information in the public domainyet, in retrospect, it conforms very closely to what one might envisage a group oriented to sciencebased globalpublic goods to be like and to do. it provides both confirmation of the concept and insights about what is involvedin implementing it.18origin and naturethe cgiar is an informal group of donorsñnumbering about 60ñwho together sponsor 16 internationalagricultural research centers largely located in, and working on, problems of importance in agriculture and naturalresources in developing nations. the centers are established as independent organizations with international status,boards, and staffs. they are both regional and global in their orientation and as public entities produce publicgoods. their focus is on applied research and the development of improved technologies and policies that can bewidely adopted, although they generally require local adaptation.the cgiar system includes a chair who is a vice president of the world bank, a small group ofinternational organizations as cosponsors (the food and agriculture organization of the united nationsthroughout and some others that have come and gone19), a general secretariat housed at the world bank, andthe technical advisory committee (tac) housed at the food and agriculture organization. tac has beena particularly useful component: it was recognized at the outset that the donor representatives to the cgiarwould not necessarily be scientists and that some outside source of continuing scientific and technical advicewould be needed.20 the tac members, totaling about 14, were half drawn from developed countries and halffrom developing counties. the group is currently being transformed into a science council with somewhatmodified duties.a group such as the cgiar was vitally needed by developing nations for several reasons. the principal onewas that, as of the 1960s, little research attention had been given to food crops for domestic consumption. theprincipal emphasis of colonial powers was on tropical export crops, often grown under plantation conditions. thusnational research systems, with a few exceptions, had relatively little capacity in commodities targeted to domesticfood use. moreover, the private sector was almost completely inactive in this area. thus the developing countriesfound it difficult to increase agricultural productivity to meet the needs of an expanding population. early effortsto simply bring in foreign technology virtually all failed because of the previously mentioned need for adaptation.training programs proved to have much more value.the cgiar and its centers provided the opportunity to generate the global public scientific goods that couldbe adapted to regional and national needs (and in some instances used directly). the research efforts were and arecarried out in a collaborative manner with national scientists so there is a builtin feedback loop. concurrently,during the 1970s and 1980s, donors such as the u.s. agency for international development, invested heavily inthe improvement of national research programs, both with respect to facilities and training. by the early 1980s, avast improvement had taken place in the national research programs in many developing countries. since then,however, the funding situation in many developing countries has stagnated or declined and the cgiar itself beganto experience similar difficulties.18this section draws on my involvement with the cgiar system since 1972 and material that has been reported in greater detail indalrymple (2002). historical background on the cgiar is provided in baum (1986). current information on the cgiar and its centers canbe obtained from the groupõs web site (www.cgiar.org). an introduction to other international and bilateral agricultural research programs iscontained in gryseels and anderson (1991, pp. 329335). further information on publicprivate relationships in agricultural research isprovided in byerlee and echeverria (2002).19the relationship of the world bank with the cgiar is reviewed by andersen and dalrymple (1999). the united nations developmentprogram was a charter member of this group but in recent years has sharply diminished its support for the provision of many public goods(including the cgiar); instead it has, somewhat paradoxically, initiated a series of studies and reports that elucidate the virtues of globalpublic goods (see, for example, kaul, grunberg and stern, 1999). the united nations environment program was a member for a while butwithdrew for financial reasons. recently the international fund for agricultural development has become a member.20the tac model has reportedly been emulated by the global environmental facility and the global water partnership.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 147promise and perilsthe basic original premise of the cgiar has been realized to a remarkable, but variable, degree. thesystem has worked quite well, although it has had its problems, and the center organizational pattern hasproved sound. synergies and spillovers of collaborative research in the crop area have been substantial. inaddition to their own research, the centers accumulate, utilize and organize, and pass on data and information in every direction. they have become focal points for knowledge in their particular areas of work andnational programs have been stimulated. increases in productivity and in turn the food supply have resulted in a lowering of prices to consumers below what they would have been otherwise. the overall resultis a significant contribution to innovation and to the economies of the developing nations (evenson andgollin, 2002).a group such as the cgiar, however, continually faces program and management challenges and financialperils. these are not unique to public research, but are multiplied at the international level. the array of pressingproblems is immense; views on what should be tackled vary (local views may vary from those at the national,regional, and global level); and the operation and management of research facilities in a developing country, witha mixture of international and local staff, can be challenging. the task can be further complicated by a variety ofexternal or exogenous issues such as nationalistic inhibitions about sharing biodiversity, concerns about iprs, andthe global debate about genetically modified organisms.21the budget for all of this is rather modestñonly $337.3 million in 2001, and has stagnated for the past decade.it represents only a small portion (less that 5 percent) of all public funding for agricultural research in developingnations. and it is totally dependent on voluntary contributions from year to year. unlike many other multilateralprograms, the cgiar has not been established by treaty. this is an advantage in that it can operate moreinformally and has considerable flexibility. it is, however, a disadvantage in that when some donor nations face abudget crunch, recently the case for japan, treaty commitments take first place. also, longterm support formultilateral programs in science and technology is probably not near the top of the list for many donors. therehave been recent efforts to develop an endowment to provide stability of funding for the center genebanks at least,but this has a way to go.this situation is compounded by a shift in the nature of the funding. there are two major categories: (a)unrestricted (core institutional support) and (b) restricted (special projects). the unrestricted provides mostof the support for the longerterm and more globally oriented research, which is where the system has itsstrongest comparative advantage. much of the restricted funding is for more localized and shortertermresearch or development activities that are generally specified by the donor. because the cgiar centers haveno other source of institutional support, as they are not governmental agencies, unrestricted funding is veryimportant. yet an increasing proportion of the funding is restrictedñreaching about 57 percent in 2001. twoforces appear to be at play. the first, largely external, occurs when a large donor of unrestricted funding facessevere budget cuts, or as new donors come in with constraints on the use of their funds. the second is moreinternal and may be a combination of donor fatigue with longterm institutional support, a desire to adapt toemerging or changing priorities (or simply something new or flashier), and a wish to more clearly identifywith specific projects.22clearly the cgiar needs to broaden its funding base beyond developmental groups where science has, atbest, a somewhat tenuous hold. most of the national programs that support science, such as the national science21the cgiar system established a small central advisory service on intellectual property in 1999 to facilitate the exchange of experienceand knowledge among its centers and to provide advice on a wide range of intellectual property issues. some recent issues are noted incommission for intellectual property rights (2002, p. 144). more general issues are discussed in byerlee and fisher (2002) and longhorn etal. (2002).22there was an earlier parallel in the spanish caribbean. òduring the early nineteenth century, governments and agricultural interest hadfunded specific research projects but had been reluctant to provide sustained funding for research institutionsó (mccook, 2002, p. 3).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.48the role of s&t data and information in the public domainfoundation in the united states, focus on domestic project grants.23 if such grants could be expanded to providemore of an international dimension, it could certainly provide help in terms of project support, but it would donothing for the fundamental problem of institutional support.24 that may well prove to be the achilles heel ofinternational research efforts.prospectsthus it seems that the 30 years of the cgiar experience confirms the promise of the concept of knowledgebased global public goods. it demonstrates that, although it is not an easy task, it is possible to establish a structurefor carrying out a global program in science and technology. it illustrates that such programs can be operated,again not without difficulty, over a wide area and an extended period of time. it shows that these programs can beproductive, stimulate innovation, and make many contributions to society. but it also illustrates the perils ofmaintaining such a program, and particularly retaining a global focus, even in times that call out for it. theproblems of funding longterm public scientific programs at the global level, in the absence of an internationalfunding mechanism (which undoubtedly would have its own problems), are indeed the heart of the matter.concluding remarksscientific knowledge in its relatively pure form is, as stated at the outset, the epitome of a global public good.it is normally freely available to all and is not diminished by useñindeed it may grow with use. moreover, due tothe miracles of modern communication, it can be transmitted around the world almost instantly. it can provide thebasis for major contributions to the innovation process and to economic growth.but to play these roles, a number of conditions must be met. first, there must be a process for generatingknowledge somewhere, and this may not be an inexpensive or simple process. second, knowledge must beembodied in some sort of socially useful technology, which also requires effort and resources. third, bothknowledge and technology must retain some sort of public goods dimension in terms of being freely available tobe of maximum social benefit. fourth, there must be some ability on the part of recipients or users to adapt thetechnology to their conditions and needs.these stepsñand others may also be involvedñinvolve an interplay of research of various types and iprs inthe form of patents and copyrights. the research process takes many forms but where it is publicly financed, theproducts traditionally have been public goods. where they were sponsored by the private sector, proprietary rightsare involved. and when this happens, as is increasingly the case with ipr in both sectors, the public domaindimension is certainly complicated and even diminished in quantitative terms. there may also be a qualitativeeffect in that research investment may be directed into areas where the social rates of returns are below the privaterates of return (see david, 1992, p. 230; van der meer, 2002, p. 126).while iprs are, so far, less a problem in developing countries, those nations generally suffer a more basicrestraintñweak research programs in both the public and private sectors. over 30 years of experience with thecgiar has demonstrated that it is possible to help fill this gap, but not without some effort and resources. the23international center scientists may participate in grant proposals submitted to, say, the national science foundation (nsf), but there arefew such examples. one basic problem is that cgiar centers are focused on applied research, whereas nsf grants are usually for moreadvanced research, normally headed by a domestic institution. this orientation, however, would be significantly broadened by a provision inthe authorization bill for the nsf for fiscal year 20022007 (h.r. 4664), which cleared congress on november 15, 2002, and was sent to thewhite house. section 8, specific program authorizations, item 3c on plant genome research, provides for òresearch partnerships to focuson ñ (i) basic genomic research on crops grown in the developing world . . . (iv) research on the impact of plant biotechnology on the social,political, economic, health, and environmental conditions in countries in the developing world . . . competitive, meritbased awards forpartnerships under this subparagraph . . . shall include one or more research institutions in one or more developing nations . . . ó (italicsadded). if approved by the president, the next, and probably more difficult, step is the appropriation process.24the international foundation for science, headquartered in stockholm, has rather limited resources and is oriented to relatively smallgrants to developing country scientists. (for more information see www.ifs.se/). a global research alliance, with a secretariat in southafrica and an evident interest in industrial research, was formed in mid2002 (www.researchalliance.net).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 149principal problem is, not surprisingly, the maintenance of longterm public funding in the face of a seeminglyendless array of other urgent calls on their use. these forces are particularly prevalent in foreign assistanceprograms (the key source of funding for the cgiar), and may be complicated by political issues at both ends andnatural disasters and civil disturbances in developing nations. some promising international health researchactivities currently are getting under way with the support of a major foundation, but the narrow base of supportmay provide a problem in the future.thus, there is a substantial gap between potential and reality. on one hand, both scientific promise andcommunication opportunities were never greater. on the other hand, funding for the provision of public goodsneeded by much of humankind, particularly those in developing nations, is seriously constrained. iprs, meant tofacilitate innovation and economic growth, may, in some cases, be coming to have a less benign effect due to theirquantitative and qualitative effects on the public domain.the situation calls for a wider and deeper understanding of the international public goods dimension ofscientific and technical knowledge. as stiglitz (1999, p. 320) has commented: òthe concept of public goods is apowerful one. it helps us think through the social responsibilities of the international community.ó similarly,sachs (2000b) has stated: ò. . . international public goods are not just a nice thing that we need to add on. they arethe fundamental thing that has been missing from our template for the past 30 years.óbut to bring this wider understanding about, there will have to beñfor a startña closer and more interactiverelationship between scientists, economists, and lawyers. much could be gained if bridges could be built betweenthem and with policy makers. this paper has been an initial attempt to begin to do so and to provide a frameworkfor further thought. i would be delighted if it prompted further consideration of this most vital subject.referencesaltman, d. 2002. òsmallpicture approach to a big problem: poverty,ó new york times, august 20, c2.alvim, p. 1994. ònonchemical approaches to tropical tree crop disease management: the case of rubber and cacao inbrazil,ó in anderson, j. r., agricultural technology: policy issues for the international community. cab international,wallingford (u. k.), 426.anderson, j., and d. dalrymple. 1999. the world bank, the grant program, and the cgiar; a retrospective review. theworld bank (washington), operations evaluation department. oed working paper series no. 1.arrow, k. 1962. òeconomic welfare and the allocation of resources for invention,ó in the rate and direction of inventiveactivity: economic and social factors (a report of the national bureau of economic research). princeton universitypress, princeton, 609626.bacon, f. 2000 (1620). the new organon. ed. by l. jardine and m. silverthorne. cambridge university press, cambridge, ix,xiv, 66.barzun, j. 2000. from dawn to decadence: 1500 to the present, 500 years of western cultural life, harper/collins, newyork.baum, w. c. 1986. partners against hunger: the consultative group on international agricultural research. the worldbank, washington, d.c.bernal, j. d. 1965. science in history: the emergence of science (vol. 1). m.i.t. press, cambridge, 32.boulding, k. e. 1966. òthe economics of knowledge and the knowledge of economicsó in american economic review 56(2), 113.buchanan, j. 1968. the demand and supply of public goods. rand mcnally & company, chicago.byerlee, d. and k. fisher. 2002. òassessing modern science: policy and institutional options for agricultural biotechnologyin developing countriesó in world development 30, 943944.byerlee, d. and r. echeverria. (eds.), 2002. agricultural research policy in an era of privatization. cabi publishing,wallingford and new york.cipr/commission on intellectual property rights. 2002. integrating intellectual property rights and development policy:report of the commission on intellectual property rights. commission on intellectual property rights, london. see[www.iprcommission.org]. reviewed in òintellectual property: patently problematic,ó the economist, september 14,2002, 7576.congressional globe. 1862. òu.s. department of agriculture,ó vol. 33, february 17, 855856. see office of technologyassessment (1981, 31).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.50the role of s&t data and information in the public domaindalrymple, d. g. 1990. òthe excess burden of taxation and public agricultural researchó in echeverria, r. g. (ed.),methods for diagnosing research system constraints and assessing the impact of agricultural research, vol. ii.international service for national agricultural research, the hague, 117137.dalrymple, d. g. 2002. òinternational agricultural research as a global public good: a review of concepts, experience, andpolicy issues.ó u.s. agency for international development, office of agriculture and food security, washington,unpublished paper, september 2002.david, p. 1992. òknowledge, property, and the system dynamics on technological change.ó proceedings of the world bankannual conference of development economics. world bank, washington, d.c., 215255.david, p. 1993. òintellectual property institutions are the pandaõs thumb: patents, copyrights, and trade secrets in economictheory and history,ó in global dimensions of intellectual property rights in science and technology, national academies press, washington, d.c.desai, a., 1992. òcomment on ôknowledge, property, and the system dynamics of technological changeõ by david,óproceedings of the world bank annual conference on development economics. world bank, washington, d.c.drache, d. (ed.). 2001. the market or the public domain? global governance and the asymmetry of power. routledge,london and new york.drayton, r. 2000. natureõs government: science, imperial britain, and the ôimprovementõ of the world. yale universitypress, new haven and london.dupree, a. h. 1957. science in the federal government: a history of policies and activities to 1940. the belknap press ofharvard university press, cambridge.dyson, f. 1979. disturbing the universe. harper & row, new york.eisenberg, r. s. 1987. òproprietary rights and the norms of science in biotechnology research,ó yale law journal 97(december), 177231.evenson, r. and d. gollin, d. (eds.). 2002. crop variety improvement and its effect on productivity: the impact of international agricultural research. cab international, wallingford and new york (forthcoming).fern⁄ndezarmesto, f. 2002. near a thousand tables: a history of food. the free press, new york.gryseels, g. and j. anderson, j. 1991. òinternational agricultural researchó in pardey, p., roseboom, j., anderson, j. (eds.),agricultural research policy: international quantitative perspectives. cambridge university press, cambridge, 309339.hayek, f. a. 1937. òeconomics and knowledge,ó economica 4, 3354.hayek, f. a. 1945. òthe use of knowledge in society,ó american economic review 35, 519530.henry, j. 2002. knowledge is power; francis bacon and the method of science. icon books, cambridge, 6, 8, 16 (quote).herold, j. c. 1962. bonaparte in egypt. harper & row, new york.homburg, e. 1992. òthe emergence of research laboratories in the dyestuffs industry, 18701900,ó british journal for thehistory of science 25, 91111.honigsbaum, m. 2001. the fever trail: in search of the cure for malaria. farrar, straus and giroux, new york.jefferson, t. 1984. letter to isaac mcpherson, august 13, 1813. in: thomas jefferson, writings. the library of america, newyork, 1291.jamison, d. 2001. òwho, global public goods, and health research and developmentó in global public policies andprograms: implications for financing and evaluation, ed. by c. gerrard, m. ferroni, a. mody. world bank, operationsevaluation dept., washington, d.c., 107111.jha, p., et al. 2002. òimproving the health of the worldõs poor,ó science 295, 20362039.kahn, b. z. 2002. òintellectual property and economic development: lessons form american and european history.ó commission on intellectual property rights, london., 2124 (patents), 3738 (copyright), 48. source: www.ipcommission.org.kaul, i., grunberg, i. and m. stern (eds.). 1999. global public goods: international cooperation in the 21st century.published for the united nations development program by oxford university press, oxford.longhorn, r., hensonapollonio, v., and j. white, j. 2002. legal issues in the use of geospatial data and tools foragriculture and natural resources management: a primer. international maize and wheat improvement center(cimmyt), mexico, d. f.machlup, f. 1980. knowledge: its creation, distribution, and economic significance, vol. i, knowledge and knowledgeproduction. princeton university press, princeton.machlup, f. 1984. knowledge: its creation, distribution, and economic significance. vol. ii, the economics of informationand human capital. princeton university press, princeton.mayr, e. 1982. the growth of biological thought: diversity, evolution, and inheritance. the belknap press of harvarduniversity press, cambridge, ma.mccook, s. 2002. states of nature: science, agriculture, and environment in the spanish caribbean, 17901940. universityof texas press, austin, 2749.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 151merton, r. k. 1997. òdegendering ôman of scienceõ: the genesis and epicine character of the word scientistó in sociological visions. ed. by kai erikson. rowman & littlefield, lanham (md.), 225253.mokyr, j. 2002. the gift of athena: historical origins of the knowledge economy. princeton university press, princeton.office of technology assessment. 1981. òthe role and development of public agricultural research,ó an assessment of theunited states food and agricultural research system. office of technology assessment, congress of the united states,washington, d.c., 2749. (chp. iii, prepared by d. dalrymple.)olson, m. 1971. the logic of collective action: public goods and the theory of groups. harvard university press, cambridge, 14.pardey, p., roseboom j. and j. anderson. 1991. òregional perspectives of national agricultural researchó in pardey,roseboom, anderson. eds., agricultural research policy: international quantitative perspectives. cambridge university press, cambridge, 215, 236, 247.plucknett, d., smith, n., williams, j. and n. anishetty. 1987. gene banks and the worldõs poor. princeton university press,princeton, 4158.powell, j. w. 1884. in: testimony before the joint commission. u.s. congress (49th congress, 1st session). senate misc. doc.82, 1886 (testimony on december 18, 1884), 180.powell, j. w. 1986. in: testimony before the joint commission (letter to w. b. allison, february 26, 1886), 1082.reich, l. s. 1985. the making of american industrial research: science and business at ge and bell, 18761926. cambridgeuniversity press, new york, 43, 6271.rhodes, r. 1986. the making of the atomic bomb. simon & schuster, new york, 749788 (opening quote from 788).romer, p. 1986. òincreasing returns and longrun growth,ó journal of political economy 94, 10021037.romer, p. 1990. òendogenous technological change,ó journal of political economy 98, 71102.romer, p. 1993. òidea gaps and object gaps in economic development,ó journal of monetary economics 32, 543573.romer, p. 1994. òthe origins of endogenous growth.ó journal of economic perspectives 8, 322.ross, s. 1962. òscientist: the story of a word,ó annals of science 18, 6585.ross, s. 1991. nineteenthcentury attitudes: men of science. kluwer academic publishers, boston, 139.ruttan, v. w. 1998. òthe new growth theory and development economics: a survey,ó the journal of development studies35, 126.ruttan, v. w. 2001. technology, growth, and development: an induced innovation perspective. oxford university press,new york, oxford, 8285, 436437.sachs, j. 1999. òhelping the worldõs poorest,ó the economist, august 14, 1719.sachs, j. 2000a. ònew map of the world,ó the economist, june 24, 8183.sachs, j. 2000b. òglobalization and the poor.ó unpublished talk presented to the cgiar on october 23 and recorded in theòtranscript of proceedings,ó cgiar secretariat library, world bank, washington, d. c.,194216.samuelson, p. 1954. òthe pure theory of public expenditure,ó review of economics and statistics 36, 387389.samuelson, p. 1955. òdiagrammatic exposition of a theory of public expenditure.ó review of economics and statistics 37,350356.smith, a. 2000 (1776). the wealth of nations. the modern library, new york, 778.sol”, r. 1998. les savants de bonaparte. editions du seuil, paris.stiglitz, j. 1999. òknowledge as a global public good,ó in kaul, grunberg and stern (1999), eds., 308325 (listed above).summers, l. 2000. speech to the united nations economic and social council (ecosoc), new york, july 5. (text availablefrom federal news service, washington; via lexisnexis database service.)undp (united nations development program). 2001. human development report 2001: making new technologies work forhuman development. oxford university press, new york.van der meer, k. 2002. òpublicprivate cooperation in agricultural research: examples from the netherlandsó in byerlee andecheverria (eds.), 125126.watson, p. 2002. the modern mind: an intellectual history of the 20th century. harper/collins (perennial edition), new yorkwhewell, w. 1840 (1996). the philosophy of the inductive sciences. rotledge/thoemmes press, london, vol. 1 (vol. 2 inl846 edition, 560).whitehead, a. n. 1925 (1957). science and the modern world (lowell lectures 1925). macmillan, new york.wills, g. 1999. saint augustine. viking/penguin, new york, 145.world bank. 19981999. world development report: knowledge for development. oxford university press, new york, iiiv,17, 131133.world health organization (who). 1996. investing in health r&d. geneva, who/tdr 961.who. 2001. macroeconomics and health: investing in health for economic development. geneva, 2314, 8185. seewww.cmhealth.org. summary in jha, et al. (2002), listed above.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.526opportunities for commercial exploitationof networked science and technologypublicdomain information resourcesrudolph potenzone.1see chapter 4 of these proceedings, òthe economic logic of ôopen scienceõ and the balance between private property rights and thepublic domain in scientific data and information: a primer,ó by paul david.i would like to address, the importance of having access to data, both data that are proprietary and data that arein the public domain. i certainly agree with paul davidõs comments that it is a mixture of relationships that is vitalto the good of increasing our scientific knowledge.1why are data so important to us, and why do we worry so much about them? clearly, when you start aresearch program, no matter what the topic or field, you have to know what information is available on the subject.you have to know all the data that have been accumulated on a particular subject to do good research. you haveto be able to get your hands on them, access them, and actually use them. ultimately, the process of research isreally about generating new data, and so as you study the existing information, you build your own concepts andideas and generate new data. the research process is one of data generation.hopefully, if the research is successful, and you are able actually to accomplish something, the data turn intosome new knowledge. depending on your particular area or affiliation, you may want to invent the greatest newthing since sliced bread, maybe a new pharmaceutical or you have discovered some whole new unifying theory ofwhatever. yet it is all based on the ability to get access to the right information at the right time.information resources are vitally important to every project, and it does not matter as you work on your projectwhere relevant data come from. in fact, it is hard to predict where the best data will be found for any particularproject. even at the time the new data are generated, it is not certain what those data will be used for. how oftendo you find a bit of information or data that are in the most obscure place, not related to the topic you are studying,but in fact are relevant to the new project that you are working on? the availability of data is ultimately importantto the ability to generate good science.what happens to these good data once they are generated? the reality is that most data never enter a databasethat is available publicly, whether it is a feebased database or a free database. data are often locked in papers,reports, or lab notebooks. maybe they get published and are available in a peerreviewed journal or are archivedelectronically. however, by and large, those data are not accessible and are not able to be found because theyactually have never been brought into a database. in particular, if the data have a particular commercial interest,and the pharmaceutical industry has been quite aggressive about this, they may find their way into a commercialthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 153database and become quite accessible. otherwise, it really depends on someoneõs personal initiative either to getfunding to create a database through one of the government sources of funding or finding some support so that theinformation can be put into a form that is actually accessible.i will describe two different scenarios for the pharmaceutical industry. one is chemical information (which iswhere i have spent much of my career), compared with what i call the bioinformatics movement; they differ inhow the data have been handled and collected. historically, chemical informatics has been a very commercializedactivity. three of the larger data repositoriesñbeilstein, derwent, and chemical abstracts service (cas)ñareorganizations that all started out building printed repositories and then ultimately turned into electronic sources.they are highly commercialized, profitable activities that have served a truly critical role in the preservation andavailability of chemical information.beilstein institute, founded in 1881, first published its handbook with 1,500 compounds. the final versionwas printed in 1998, with the oldest references going back to 1771. interestingly, this was converted in electronicform with some assistance by the german government and today holds 9 million compounds, a lot of informationand data, and is distributed on a commercial basis by a reed elsevier company.derwent, which is today a thompson company, was founded from ideas initiating from monty hyams in1948. he was trying to make some sense out of the patent literature and began writing some simple abstracts aboutwhat was being published at the time. this information became useful. people started getting more interested, andit turned into a commercial operation. today, derwentõs world patent index is global in scope, covering 40different patentissuing authorities and details in over 8 million separate inventions. it is a very large repository. ifyou work in the area of pharmaceutical research and development, you have to go to the derwent database tounderstand what the patents are about.cas has a similar history. they were founded in 1907, with the goal of monitoring and abstracting the worldõschemicalrelated literature. today, with the internet and all of the information that we have to deal with, it is mindboggling to think that in 1907 this operation was formed because there was too much information to handle. casis a subsidiary of the american chemical society. there are 20 million organic and inorganic compounds registered at cas; 21 million biological sequences; and almost 42 million separate and unique chemical entitiesregistered in their system, all of them complete with names and references to the published literature, allowingscientists to find more information about these items. it is an incredible amount of information.my observations are that chemical informatics has been quite commercialized and brings in quite a bit ofmoney. in the area of pharmaceuticals, this has all been organized and put out there to be used because of its highcommercial interest. these three companies and others look at scientific journals, books, patents, conferences, anddissertations; they do the work, and they extract a significant fee for it. the data are organized and then madeavailable to the community, but at a price. the reason these data are not free is not because the underlyinginformation is not free, because in many cases it is. but the fact that these companies have organized it and broughtthis together in a searchable (i.e., more useful) fashion gives these databases a very high value. this makes lifemuch easier in terms of the chemical information community.there is clearly value in that much of the original research was publicly funded research. there has also beena significant cost of creating these data sources. although the pricing is fairly significant for these groups, theycertainly have provided a service to the community. frankly, whether these operations could have continued toexist in an electronic form without the current funding support is doubtful.there are certainly hundreds, maybe thousands, of other databases and data repositories in chemistry that donot get picked up by these services. are these less important to us as a scientific society? there are certainly costbarriers that prevent all the good data from being collected and organized in a reasonable fashion. the publicfunding has not been available to make these data generally available. i think that the funding tends to go towardcollecting more data, and yet the funding for making the repositories of these data and making them available hasnot been there. my personal opinion is that funding authorities should consider the utility of the resulting data theypay to have generated from the start of these projects.an interesting contrast to this illustration is in the bioinformatics arena, which in some ways is the antithesisof the chemical data franchise. here, largely publicly funded projects have been formed by different highlymotivated groups to put together what have become literally hundreds of sequence databases. a few of these arethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.54the role of s&t data and information in the public domainbeing commercialized, and so there are some annotation systems in place, and there are a number of companieswho provide commercial databases. yet the vast majority are still collected and made available for free, and withlittle support.figure 61 provides an illustration. if you look at the increase in information and amount of the sequencingspeed, coupled to how much information is out there in terms of known sequences, and the growing complexity ofinformation as we get into systems biology and expression information, there is absolutely an astounding amountof information that is suddenly becoming available in this arena.from lion biosciencesõ perspective, we have been the beneficiary of some of the public work. the structureretrieval system has been in the community for over 10 years, largely driven by scientists in terms of itscapability, with over 500 parsers to search different kinds of databases. it was developed at the european molecular biology laboratory (embl). this has now been turned over to lion bioscience to commercialize and to keepthe product growing and maturing. in terms of differential pricing, we continue to make this freely available toacademic institutions, but are charging a fair price to the industrial community. we have merged this with our newtechnology called discovery center.there are over 800 data sources that are relevant in bioinformatics research, which interrelate to each other.examples include genbank, swissprot, embl, and all the various pieces. even if we believe that the cost ofdisseminating information on the internet is free, which it really is not, the working scientist has to navigatethrough this network of data. he or she must sort through these many sources to find out the particular kinds ofinformation they are looking for. this complexity suggests that they are most likely missing something in terms ofthe kinds of capabilities that we are making available through the proliferation of databases.these databases are the fuel for the research projects in genomics and proteomics. these data sources camefrom the broader community, but they lack the financial incentives because they are all free to motivate thecommercial services in terms of bringing these together. the users are individual scientists, who have to look at thelocal unpublished data that they are generating in their laboratory or calculations that they are doing. they have to. . . and sequencing speed . . .transistorsper chipmoore's law: chip speed doubles every 18 months110010.0001.000.0019601970198019902000808680286386dx486pentiumpentium promerced (p7)increases in processing power . . . 0.010.101.0010.00100.001,000.001984198919941999thousands of base pairs/day(single unitat maximumthroughput)manualautomatedhigh throughputtechniquesbase pairs(m)110010,000198019902000publicly available genome datahuman genome 3b base pairs. . . lead to a flood of genetic knowledge . . .. . . as well as system informationcmyb(u22376)proteasomeiota (x594171)m1 (u05259)cyclind3 (m92287)myosin light chain (m34211)rbap48 (x74262)snf2 (d26156)e2a (m31523)inducible protein (l47738)dyneinlight chain (u32944)topoisomeraseii (z15115)irf2 (x15949)tfiiex63469)acylcoenzyme a dehydrogenase (m91432)hrt1 (s50223)snf2 (u29175)(ca2+)atpase(z69881)srp9 (u20998)mcm3 (d38073)decoxyhypusine synthase(u26266)op 18 (m31303)rabaptin5 (y08612)heterochromatinprotein p25 (u35451)il7 receptor (m29696)adenosine deaminase(m13792)fumarylacetoacetate(m55150)zyxin(x95735)ltc4 synthase(50136)lyn (m16038)hoxa9 (u82759)cd33 (m23197)adipsin(m84526)cystatinc (m27891)proteoglycan1 (x17042)il8 precursor (y00787)azurociden(96326)p62 (u46751)cyp3 (m80254)mcl1 (l08246)leptinreceptor (y12670)atpase(m62762)il8 (m28130)cathepsind (m99043)lectin(m57710)mad3 (m69043)cdiic(m81695)ebp72 (x85116)lysozyme(m19045)properdin(m83652)catalase(x04085)genes tested on gene chipall patients1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20aml patients1 2 3 4 5 6 7 8 9 10cmyb(u22376)proteasomeiota (x594171)m1 (u05259)cyclind3 (m92287)myosin light chain (m34211)rbap48 (x74262)snf2 (d26156)e2a (m31523)inducible protein (l47738)dyneinlight chain (u32944)topoisomeraseii (z15115)irf2 (x15949)tfiiex63469)acylcoenzyme a dehydrogenase (m91432)hrt1 (s50223)snf2 (u29175)(ca2+)atpase(z69881)srp9 (u20998)mcm3 (d38073)decoxyhypusine synthase(u26266)op 18 (m31303)rabaptin5 (y08612)heterochromatinprotein p25 (u35451)il7 receptor (m29696)adenosine deaminase(m13792)fumarylacetoacetate(m55150)zyxin(x95735)ltc4 synthase(50136)lyn (m16038)hoxa9 (u82759)cd33 (m23197)adipsin(m84526)cystatinc (m27891)proteoglycan1 (x17042)il8 precursor (y00787)azurociden(96326)p62 (u46751)cyp3 (m80254)mcl1 (l08246)leptinreceptor (y12670)atpase(m62762)il8 (m28130)cathepsind (m99043)lectin(m57710)mad3 (m69043)cdiic(m81695)ebp72 (x85116)lysozyme(m19045)properdin(m83652)catalase(x04085)cmyb(u22376)proteasomeiota (x594171)m1 (u05259)cyclind3 (m92287)myosin light chain (m34211)rbap48 (x74262)snf2 (d26156)e2a (m31523)inducible protein (l47738)dyneinlight chain (u32944)topoisomeraseii (z15115)irf2 (x15949)tfiiex63469)acylcoenzyme a dehydrogenase (m91432)hrt1 (s50223)snf2 (u29175)(ca2+)atpase(z69881)srp9 (u20998)mcm3 (d38073)decoxyhypusine synthase(u26266)op 18 (m31303)rabaptin5 (y08612)heterochromatinprotein p25 (u35451)il7 receptor (m29696)adenosine deaminase(m13792)fumarylacetoacetate(m55150)zyxin(x95735)ltc4 synthase(50136)lyn (m16038)hoxa9 (u82759)cd33 (m23197)adipsin(m84526)cystatinc (m27891)proteoglycan1 (x17042)il8 precursor (y00787)azurociden(96326)p62 (u46751)cyp3 (m80254)mcl1 (l08246)leptinreceptor (y12670)atpase(m62762)il8 (m28130)cathepsind (m99043)lectin(m57710)mad3 (m69043)cdiic(m81695)ebp72 (x85116)lysozyme(m19045)properdin(m83652)catalase(x04085)genes tested on gene chipall patients1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20all patients1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20aml patients1 2 3 4 5 6 7 8 9 10aml patients1 2 3 4 5 6 7 8 9 10aml patients1 2 3 4 5 6 7 8 9 10figure 61genomics generates a flood of information. sources: genbank; sequenom; 3700.com; applied biosystems;human genome project; food and agriculture organization of the united nations; database on genome size; and intel corp.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 155look at the other data around the company or research institution. these data could be at other sites around theworld. they have to look at the internet and all the various databases that are out on the internet. they have to lookat commercial databases, where they are loaded internally or externally. these researchers need to be able to haveall of this synthesized for them in some fashion so that it ultimately facilitates their work, which is the research andcreation of the new knowledge that we all like to talk about.having access to all these data is essential, whether they come from oneõs own lab or from some small college.if those particular data are relevant to the project you are working on, it is absolutely critical that all these datacontinue to be collected and made available in some reasonable form. commercial databases provide an essentialpart of the information chain that we have to consider. however, noncommercial sources of data are also vital forthe scientific community. these often fill huge gaps that, for whatever the reason in the commercial sector, havenot been funded and have not been supported and actually fill out what i like to call our òdata portfolio.óit is the integration of all this information that ultimately will enable us to continue to assist the workingscientific community, to push back the frontiers of science, and to expand human knowledge into the future.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.56the role of s&t data and information in the public domain567the role, value, and limits of s&t data andinformation in the public domain for educationbertram bruce.i want to focus on education, which i define broadly to include learning in k12 schools and universities,informal learning, as well as learning in the workplace. i want to make an argument that attention to the role of thepublic domain in education is not only important for education, but also can give us a better understanding of whatthe public domain means and how to think about it in a larger society. i am going to group my comments into fourareas. the first is to lay out a context for thinking about education and its relation to information today. the secondis to describe biology workbench, a tool that has been used increasingly in k12 and college education. third isa classroom example to show some of the things that students are doing with this tool. finally, i will address someof the implications.responding to a changing worldwe hear a lot of talk about societal change and change in the workplace. one of the constants of education hasbeen how we cope with change. we see today new technologies for communication and transportation, business,industry, manufacturing, medicine, and so on; globalization; immigration; evolving languages and incorporatingwords from around the world; a shift to knowledge work; and changing social values in organizations. manypeople call this a paradigm shift. they say it calls for a new kind of 21st century education, which responds to anew kind of world that we are living in. we might call this the turn of the century problem in education. but it isimportant in thinking about these changes to realize there have been changes in the past. many people argue thatthe change from the 19th to the 20th century led to greater changes on all of these dimensions, at least in the unitedstates. for example, the technology changes during the late 19th and early 20th centuries included the telegraphand telephone, the phonograph, radio, motion pictures, mass printing, and other inventions with profound consequences for science and education.all of these changes in the larger society led to many kinds of changes in education. first of all, there was ahuge expansion of schooling as the waves of immigrants in the late 19th century were incorporated into the schoolsystem. the progressive education movement developed. new subjects were created. in 1880, one study complained that there were over 30 high school subjects taught and that was too many. in one recent count, there were4,000 different subjects offered in high school today.the beginning of research universities and what people call the american library movement, all of thesethings happened early in the last century in response to changes similar to those we see today. during this time, onethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 157of the key notions that developed was inquirybased learning. it is the idea that in a world that is rapidly changing,where there are vast amounts of informationñconflicting and redundant informationñit is often hard to find whatis crucial. in this context, we need to have learning tools that are open ended, inquiry based, group and teamworkoriented, and relevant to new careers. this is something the national science foundation has been pushing, ashave the boyer commission on undergraduate education and a wide variety of other groups. it contrasts with thetextbookoriented learning that many of us experienced.one model for this starts with the idea that it is not just important for students to be able to solve problems.they need to learn how to ask good questions; to find problems as well as solve them. second, they need to learnhow to investigate complex domains of knowledge, not just to read the chapter and answer questions at the end, butto integrate multiple sources of information. third, they need to learn to be active creators of meaning, to constructknowledge, not just to follow directions. fourth, they need to learn how to work with others, to discuss and tounderstand different perspectives. finally, they need to reflect on what they have learned and articulate thosemeanings for themselves and others.biology workbench: openworld learningwe could spend a long time talking about inquirybased education, but one way to convey that and to bring itback to the publicdomain data and information is to take one concrete example in the area of bioinformatics. dr.potenzone talked about the vast amounts of information that are available now for doing molecular biology, forinvestigating gene sequences, diseases, and so on.1 bioinformatics is developing as a distinct science and, in fact,many people are arguing that biology itself is being transformed into an informationdriven science. biologyworkbench is one of the tools that has been built to address this. it is a webbased interface to a set of tools anddatabases, which researchers can use to access information stored throughout the world. investigations that mightpreviously have taken two years in the lab can now be done in a day sitting at the computer. there are tools forsequence alignment of proteins and genes, visualization tools, a digital library of articles, and so on. new knowledge has come out of using the workbench. people in pharmaceutical companies, universities, and other placeshave made discoveries that would certainly have taken much longer without a tool like this.in addition to looking at sequences and sequence alignment, a user can use the workbench to visualize thestructure of molecules, for example, that of hemoglobin in both its normal and the sickled form that causes sicklecell anemia. this visualization shows a mutated region of the molecule, in which it is easy to understand how onesickled molecule can hook into other molecules and create the sickling phenomenon.researchers also use this tool to investigate relationships among species. so, for example, users can comparehorses, chickens, cows, vultures, dogfish, tuna, and moles to examine their degree of relatedness. by looking at thesimilarity, researchers can build phylogenetic trees or cladistic diagrams. these show that the tuna and the dogfishare more closely related to each other than to the other organisms, such as the horse, cow, and mole. the mammalsare all more closely related, and the horse and cow are more closely related than either is to the mole, and so on.using biology workbench, a user can become an active investigator of the kinds of studies reported regularlyin the new york times science section. for example, when some new discovery about relatedness of organismscomes out, a reader could verify or challenge those results using a home computer connected to the web.a tool like this creates great possibilities for education. it also poses challenges. many educators feel uncomfortable with tools like this, or what my group has called openworld learning, in which there are open, dynamically changing data, computational tools, and community interactions.imagine an instructor who prepares a lesson, checks it out the night before, and goes in the next day to teachabout it. by the time the class begins, the data have changed. when students look at the computer, they find adifferent answer, a different set of information, because these databases are being constantly changed. thisscenario reflects the first characteristic of openworld learning, that the set of data is open and changing. the1see chapter 6 of these proceedings, òopportunities for commercial exploitation of networked science and technology public domaininformation resources,ó by rudolph potenzone.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.58the role of s&t data and information in the public domainbiology workbench also implies an open computational environment: many of the tools are open source and newtools are being created all the time. third, it is an open community, which encourages direct communicationbetween industry and university and between researchers and schools.classroom examplesas an example of this approach, a recent article in nature argues that neanderthals and humans could not haveinterbred because of genetic differences. students investigated this claim and found that neanderthals and humanswere actually more similar than other organisms, which are known to interbreed, such as horses and llamas.another example focused on fungi being more closely related to animals than to plants. in another case, studentslooked at cetaceans, whales, porpoises, and so on and how they are related to hippos.students often take on these investigations as part of their class experiences. as one teacher said, this enablesthem to do projects in which they have to learn things that are not covered in the textbook. in addition, they getaccess to technologies that professional scientists are using. this means that students are not only using the toolsto learn things, but also learning about the tools and the practices of science. they are also learning how tocollaborate and how to articulate their knowledge.this kind of investigation is not possible without access to the workbench. students could get books, but thenumber of books needed would be too expensive in most schools. not only that, the information is rapidlychanging. through the biology workbench, students were able to find articles online that talked about researchthat they were investigating. in effect, they entered the scientific community, became participating, practicingscientists, and potentially could make their own contributions to the larger scientific literature. instead of simplybeing recipients of knowledge created elsewhere, the students become creators of knowledge and participants inthe knowledgemaking community.challenges and opportunitiesi want to make a few comments here about challenges and opportunities and then some closing commentsabout the public domain, education, and democracy. inquirybased learning is not a universal approach amongeducators. moreover, many educators do not view publicdomain data and information as an unvarnished good.where most of us here today say we need more access to information, different models of education more or lessaccommodate and welcome use of public domain or information.the fact that information is becoming more abundant, more complex, and rapidly changing is exciting to somepeople and scary to others. there is a challenge in any case. even if you think, as i do, that it is absolutely crucialto education today, it is a challenge to think about how to make this kind of information not only available but trulyaccessible to students, particularly students in marginalized groups and with less than the latest equipment. thereason it is important is it creates so many kinds of opportunities. one is access to resources for inquiries. studentscan now investigate questions that they could only pose before. now they can engage seriously in carrying throughan investigation to seek answers, which then generate new questions for further inquiries.the very fact that this information is in multiple forms is exactly a reason students should be given theopportunity to engage with it. they need to learn how to cope with this abundance of information, media, andgenres of representation. by using tools like biology workbench, which is but one example in one domain,students can become part of a larger community of inquiry: they learn not only the concepts or the skills of biology,but also learn what it means to be a biologist. this kind of activity elides many of the distinctions between practiceand research, students and teachers, learners and researchers, and learning and research.james boyle spoke about the generational difference in how people think about public domain information.2he also spoke about how environmental studies in the 1960s changed the way we thought about the environment.2see chapter 2 of these proceedings, òthe role, value, and limits of s&t data and information in the public domain in society,ó byjames boyle and jennifer jenkins.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 159those are educational issues and they exemplify why education should be at the heart of the debate about publicdomain knowledge. how do each of us acquire the beliefs and values that tell us what is just, what is feasible, whatis desirable, independent of any particular law or policy of the moment?it is common to talk about schooling and society as two separate realms. we think of school as the place whereideas from society go, once they are well formulated, well worked out. we think of society as the place wherestudents go once they are fully prepared. but we treat school and society as two different worlds, which just touchon graduation day.john dewey, who did much of his writing during that revolution in education of a century ago, challengedpeople to rethink dichotomies, such as that of school and society. as he did with similar analyses of public andprivate, individual and social, or child and curriculum, dewey pointed out that treating those terms as oppositionalleads to an impoverished understanding of both. he went on to argue that education was fundamentally aboutdemocracy and that a democratic society cannot exist without an educational system, which encourages and fostersthe development of individuals who are capable of selfgovernment. at the same time students cannot learn aboutdemocracy and about a democratic society if they do not have the chance to participate in it, both in the classroomand in the larger society.because data and information are inherent to meaningful communication, the public domain is absolutelycrucial, not only for the development of knowledge in general and not only for learning, but ultimately for thedevelopment of a just and equitable society.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.60the role of s&t data and information in the public domain608the role, value, and limits of s&t data andinformation in the public domain for research:earth and environmental sciencesfrancis bretherton.this presentation is based in part on a national research council report called resolving conflicts arisingfrom the privatization of environmental data, which is available on the national academies web site.1 first, iwant to emphasize that i am a scientist, an environmental scientist insofar as that ever exists. i am a meteorologistwith some experience in oceanography, but i have made it my business over the past 20 years to learn about all myother colleagues, what they do as geologists, chemists, ecosystem people, people interested in the cryosphere, andso on. they are all environmental scientists, whether they recognize it or not. the environment sciences are not ahomogeneous domain. there are many different sorts of environmental scientists. twenty years ago, they neverused to talk to each other at all. one of the great changes that has happened in the past 15 to 20 years is that thereis a group of people trying to look at how the system functions as a whole and how the various pieces areinterrelated.my priority in this presentation is to explain to nonscientists the special data needs of environmental science.there are some differences from the bioinformatics area. in particular, there are few startups in the environmentalsciences. there are some, but not very many. the other important difference is that our topic is fundamentallyinternational. many environmentalistsõ views are global and other governments and countries are partners in thatenterprise. to come to sensible public policies about the environment, we have to work collaboratively with thoseother nations. it simply is not feasible to devise a strategy for the united states and expect the rest of the world tofollow that strategy.there are a number of issues surrounding my presentation, which i am not going to deal with directly. inparticular, we have heard already about some sui generis intellectual property rights in databases, which have beenintroduced in the european union. that is of great concern to environmental scientists in the united states becausethe europeans are our collaborators, and the database directive has led to restrictions on the availability ofenvironmental data of various sorts. we are very concerned about what would happen if the united states alsowent the same way. if it did, i have no doubt that the world would follow.another issue here is that some foreign governments are trying to sell their data, typically in europe, but notuniversally. those are government agencies acting as quasicommercial enterprises. that, likewise, gives us great1national research council. 2001. resolving conflicts arising from the privatization of environmental data, national academy press,washington, d.c. available on the national academies press web site at http://www.nap.edu/catalog/10237.html?seside.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 161concern. i do not believe they are actually going to succeed. it turns out that the market is very thin and there areno indications that any of them are even beginning to cover their costs for reasons that i will come to later.additional issues are that the united states has a policy of encouraging publicðprivate partnerships in theinformation provision area. these partnerships have to be thought through very carefully as to what the respectiveroles are of the partners and, in particular, what are the relative data rights. that has to be done on a casebycasebasis; it is not something that can be written into legislation.finally, the u.s. policy on commercialization of space is also introducing tensions into this area. manyapplications, such as satellite observations, provide useful data for environmental science and potentially havecommercial applications. that interface is, in fact, troublesome.let me now focus on the imperatives for environmental research and education, which is the primary purposeof this talk. as i have already indicated, there has been a movement over the past 20 years among the scientificcommunity to face up to the fundamental problem, which is understanding human interactions with the naturalenvironment. that is a huge canvas and i am certainly not going to touch on more than small pieces of it today.what i will assert, however, is that longterm global data, by that i mean many decades, are essential to documentwhat is going on and to unravel a lot of the interconnections that exist between, for example, the ecosystems andclimate. these data are also important in the distillation of interconnections to enhance the understanding of whatis occurring, which can be conveyed not only among the specialists, but also to our children and grandchildren. ifwe do not understand what we are doing to their futures, they are not going to be in a position to do very muchabout it. a central requirement within all of this is a dependable, coherent observing and information systemthrough which researchers can synthesize core information products. i am going to come back to this again andagain, but let me just introduce an analogy.the key analogy here is to a tree, as you can see in figure 81.2 the roots are where the data are actuallycollected in many different countries with many different types of instruments. as you move into the trunk and goup the trunk, data are being collated, crosschecked, and put into higherorder information products. that conversion of data to information is really seamless. there is a key point in this process, which i have labeled òcoreproducts,ó that get distributed by a whole variety of mechanisms to the end uses and that are represented by the2the tree analogy was originally proposed in the nrc resolving conflict report previously cited. for further explanation of the variouscomponents, please see chapter 3 of that report.leavesbranchestrunkrootscore productsdata, informationrequirementsfigure 81 an environmental information system. source: national research council. 2001. resolving conflicts arisingfrom the privatization of environmental data, national academy press, washington, d.c.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.62the role of s&t data and information in the public domainleaves in the tree. core products, for example, can include calibrated and verified data derived from a single raingauge.when considering such an enterprise, one has to take the systems point of view and start with what are the enduses, what are we trying to cater to, what are the priorities for that, and then come back to what are the coreproducts that might be produced and what are the implications.however, it is also clear that you do not pay for a system like that, which is expensive, simply on the basis ofresearch. there is a lot of research money going into this, but it is not nearly enough to pay for the completesystems that we have and need. indeed, you have to serve multiple users and applications, which will generate abroader social return for the taxpayers as a whole to justify the large costs. we also need to foster consensus onscientific understanding and policy action. this implies that other countries have to be involved in what we aretalking about. they have to participate actively in the system. that includes building research capacity, particularly in developing nations that may not yet have it. that is the benefit that they get out of a system of this sort inexchange for participating in the data collection.the public requires reliable information that is properly interpreted. if the public does not believe what iscoming out of a system of this sort, it is a waste of money. i already have mentioned that many environmentalissues are international and global in scope. i would like to emphasize that the contributions of foreign governments come in kind, rather than through direct payments. they are based on what those governments do withintheir own borders or with their own systems because money is not easily transferred internationally, as i think weall understand.finally, the natural environment is very complex and uncontrollable, and describing its behavior requiresmany observations from different places. no single scientist or group conceivably can accomplish this alone.these are the absolute imperatives for pooling resources and for sharing the data effectively.i am now going to provide some brief examples of information systems, starting with a most familiar one ofweather and climate. think of a data buoy out in the tropical pacific. it is measuring the winds and the atmospherictemperature, and there is a 300meter cable below that is measuring the temperatures in the ocean. all of these dataare being telemetered back through a satellite and are available on the web. researchers, students, and people inmany other sectors who have a need for such information can look up these data on the web site and get thecomplete picture of the ocean and atmospheric temperatures for the past five days.another example is a processed satellite image to give the type of vegetation that is present. this providesinformation about land use, which is a fundamental part of the environment. it is frequently socially determined.another type of information system includes one used to predict and assess fish stocks in fisheries around theworld. this is a major concern because, of course, a lot of the worldõs people depend on fish for their protein. thetake is increasing, but the stocks are rapidly decreasing and more species are being fished out. as is the case withdata about other natural resources, the same information can be used to both deplete and protect them.earthquake hazards provide yet another example. there is a worldwide seismological network measuringearthquakes. of particular interest to this group of researchers is that proprietary data from the big oil and gasexploration companies are now being donated into the public domain. these are data that had significant commercial value when they were collected, but are now outdated and are being donated in the public domain. there arecosts of assimilating and storing those data, but they can be very valuable for research purposes.3let us return to the tree analogy and start to fill out some of the details (see figure 82). in the roots, there isa mix of systems. one is the international networks, the contributions that are being made by different countriesand telemetered around the world as needed. there are also national networks doing the same things and there arealso different types of measurements being made, some of which are satellite and others in situ measurements. toget a successful system, we need all of them, and they have to work together seamlessly. that is a major enterprise.the main point is that the total cost is mostly in the roots. collecting the data and pulling them together iswhere the cost lies. as you move up the roots to the trunk, it is the preparation of core data products, which is theprimary function, and they have to be made available in the public domain at marginal cost. otherwise we are3see chapter 27 of these proceedings, òcorporate donations of geophysical data,ó by shirley dutton.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 163cheating ourselves by investing all that government money in the roots and not taking full advantage of it. thecrucial thing about the trunk is that both the input data and the algorithms must be open to scientific scrutiny,because without that, the outputs are not credible. if they are not credible, then we are wasting our money.finally, moving up to the branches and leaves, we have the distribution and use of the data and information.it is as complex as the roots. each u.s. federal science and technology agency sponsors a system like this, such asthe national oceanic and atmospheric administration for weather, but it is different for different agencies. theywill have their own internal agency requirements and also have their own distribution system.the leaves represent the end uses. for example the energy, forestry, and insurance industries are all big usersof this information, as is education through the integration of data in textbooks and things like that. the generalpublic is very interested in a lot of these data for recreational purposes. there also is a whole set of issues aboutsetting environmental policy on regulations.finally, the branch represents a distribution system tailored to identifiable user groups by reformatting coreproducts, adding additional information, or otherwise increasing value to that group. these branches are alwaysdeveloping and changing. diversity of the branches is another major feature, which is not always fully appreciated.the fundamental premise is that those products at the top of the trunk have to be in the public domain atmarginal cost. having said that, the branches do not have to be public domain. in fact, many of them currently arenot. they are in the form, for example, of valueadded weather data. that is perfectly in order, provided that allthose products are starting from the same base of publicdomain information coming from the core.there are also opportunities for the private sector down in the roots. there are now commercial satellites thatprovide 1meter resolution of what is going on in your own backyard. the satellite companies are selling thesedata. there are various legitimate purposes for these data, some of which are needed for environmental studies.the point is, if the purchase of such data from the private sector is the cheapest way to get what the governmentneeds, it is entirely fair that they should buy it from the commercial concerns. however, i want to emphasize thatwhat the government buys has to include the rights to the data they actually purchase. if they cannot afford to dothat, then they have to reduce the amount of data they purchase. you cannot mix restricted data with publicdomaindata in the trunk because it ruins the transparency and essentially compromises the whole research enterprise.to conclude, publicly funded, shareduse, longterm observational information is essential for sound publicpolicy concerning human interactions with the natural environment. core products of the trunk of such systemsmust be in the public domain and available at marginal cost of reproduction. valueadded, privatesector distributhe rootsobservations & data collectionintegration &validationdistribution& useinternational networksmeasurementsystemsnational networksinformationleavesbranchestrunkrootsrequirementsfigure 82 analogy of how the networks come together seamlessly. source: national research council. 2001. resolvingconflicts arising from the privatization of environmental data, national academy press, washington, d.c.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.64the role of s&t data and information in the public domaintion systems may enhance the enterprise if they show benefits. they do not have to be, however, in the privatesector. for example, scientists have their own climate and weather data distribution system, which is paid for outof research funds and justified on that basis. finally, purchase from private vendors of all rights to a limitedamount of data may under certain circumstances be costeffective.there is one concluding issue that i would like to note. i have presented a model of various environmentalsystems. it turns out that not a single one includes a recognizable mechanism by which the different stakeholdersñthe government agencies, the policymakers, the scientists, and the privatesector participantsñcan actually gettogether and work out mutually satisfactory winwin situations of some of the conflicts that are arising. a keyconflict that tends to arise is just what is the right definition of the core products at the top of the trunk. that is theplace where conflicts most come into focus, but the point is we have no forum for working those out. that needsto be established.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.659the role, value, and limits of s&t data andinformation in the public domain forbiomedical researchsherry brandtrauf1.i will explore the boundaries and tensions between public and private data in the domain that has been referredto earlier in this symposium as òsmall science;ó that is, the arena of biomedical scientists doing research individually or in small groups. for the theoretical underpinnings of the talk, i am relying on a paper that stephenhilgartner and i published some years ago in the journal knowledge.2i recently read the book brunelleschiõs dome by ross king, the interesting story of the radical design for thesanta maria del fiore cathedral in florence.3 it describes the architect, filippo brunelleschi, doing research for hisdesigns in the vast ruins of ancient rome. to this day, what he sought in the ruins is unknown because, fearful oflosing priority in his architectural work, he recorded his notes on strips of parchment in a series of symbols andarabic numerals. in effect, he withheld his data from his compatriots as well as from those in later generations whowould have liked to understand the classical principles and discoveries on which he relied. and that was hardly thefirst episode of a researcher withholding data. hundreds of years earlier, roger bacon advised all scientists to usewhat he called òconcealed writingó in recording their discoveries. even in olden times, the dark side of withholding data was evident: use of such cryptic methods for recording data sometimes interfered with scientistsõ voluntary exchanges. for example, as king also describes,4 when galileo wanted to communicate to kepler that he haddiscovered the rings around saturn, he did so in an anagram that, unscrambled, read, òobservo altissimumplanetam tergeminimó (i have observed the most distant of planets to have a triple form). unfortunately, keplerread the anagram as saying òsalve umbistineum geminatum martia prolesó (hail twin companionship, childrenof mars), which puts quite a different spin on this scientific communication.it is clear, then, that the question of under what circumstances and with whom to share data has long been oneof some interest among scientists. at one time, the issue tended to revolve around the question of priority indiscovery. a classic social science perspective on science, first laid out by the sociologist robert k. merton and1the author gratefully acknowledges the national science foundation, grant no. bir9112290, for its support for the research on xraycrystallographers and data sharing described in this talk.2hilgartner s. and s. i. brandtrauf. 1994. òdata access, ownership, and control: toward empirical studies of access practices,ó inknowledge: creation, diffusion, utilization 15(4):35572. see also hilgartner, s. 1997. òaccess to data and intellectual property: scientificexchange in genome research,ó pp 2839, in national research council, intellectual property rights and research tools in molecularbiology, national academy press, washington, d.c.3king, r. 2000. brunelleschiõs dome. new york: penguin books.4id. at 25.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.66the role of s&t data and information in the public domainelaborated by quite a number of subsequent social scientists, points to the dominant norms of science, which heindicated were organized skepticism, universalism, disinterestedness, and what he called communism, which is ofmost relevance to this symposium: the idea that findings belong not to the individual but to the entire scientificcommunity, that is, they become part of the public domain.5 this notion of collective ownership was always, insome sense, prescriptive rather than descriptive of the behavior of scientists. one has to look no further than jameswatsonõs book the double helix6 to know that, but nowadays, with commercial interests so permeating thescientific process, even a pretense of normativeness is often gone. at one time, scientists who were unwilling toshare data were often responding to concerns that other scientists would steal their findings to get credit fordiscoveries rightly their own. it might be said that they wanted credit more than ownership. these types ofconcerns have certainly survived. but a change in recent years has been the extent to which commercial interestshave affected the desire to establish ownership over biomedical research data. under these circumstances, researchers and their commercial entities want not only credit but also ownership. because of this desire for bothownership and credit, scientists often restrict access to their data by keeping them privatized, either by their ownchoice or at the insistence of their commercial collaborators. these restrictions often revolve around publication ofthe data, which may be delayed or suppressed entirely. however, they sometimes affect informal exchanges ofdata as well. the magnitude of these concerns will be discussed in later sessions, as will the concerns that arisewith disputes over data access. here, it is sufficient to note that supporters of a free flow of scientific data believethat resistance to data sharing and disputes over data sharing can:¥waste resources by leading to duplication of efforts,¥slow the progress of science because scientists cannot easily build on the efforts of others or discover errorsin completed work, and¥lead to a generalized level of mistrust and hostility among scientists in place of what should be a communityof scientists.let me give one example from a study i conducted several years ago together with stephen hilgartner, whowill be speaking with you later in this meeting. we studied datasharing practices among xray crystallographers.one of these scientists reported to us that an industry group had published a paper with an incomplete structure,containing just what he referred to as òthe juicy parts of the analysis.ó he wrote to ask them for their coordinates,and they responded, òwell, maybe in a couple of years after we look at it a little bit more.ó three years later, hefinally gave up waiting and went ahead and did the structure for a homologous substance, for which he intended todeposit coordinates and to publish. not only was he looking forward to a significant publication, but he wasespecially gleeful about the possibility of harming the first group by putting into the public domain the very datathey sought to keep private. this is surely not the most productive way for science to proceed. one could not evenregard this as productive from the standpoint of replication because the original data were not made accessible tobe replicated. it does, however, provide an example of the òdisappearing property rightsó referred to by paul uhlirand jerry reichman.exploring these issues requires a detailed analysis of what the basic terms mean. at the least, we need tounderstand what we mean by data and what we mean by sharing. in addition, we need to consider how, with whom,and under what circumstances and conditions scientists share and withhold data, recognizing that sharing andwithholding constitute a spectrum of entities. few scientists can afford the time and resources involved in sharingeverything with everyone and few, if any, refuse to share anything. the hypothetical scientist who shares everything could never be productive, since she is spending all her time emailing and talking on the phone. the chimerawho shares nothing would have a career that is nasty, brutish, and short. he would never publish or speak atmeetings and probably would never even talk to colleagues. indeed, he could not really be said to have colleagues.nobody makes everything public and nobody keeps everything private. data sharing constitutes a flexible concept5merton, r.k. 1973 [1942]. òthe normative structure of science,ó in the sociology of science. edited by n.w. storer. university ofchicago press, chicago.6watson, j.d. 1968. the double helix: a personal account of the discovery of the structure of dna. atheneum, new york.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 167that incorporates a variety of actions at different points in the scientific process. i will come back to this flexibleconcept of sharing a bit later.the concept of data also is flexible. for purposes of our datasharing study, hilgartner and i found that it wasnecessary to define data broadly and fluidly. unfortunately, these issues are often explored using an atomisticapproach that imposes artificial distinctions between the input and output of scientific work. in this approach,which might be called the òproduce and publish model,ó scientists first produce data or findingsñthe output of theprocess. second, these findings are disseminated through publication or more informal channels. and third, thedata, then in the public domain, become the input for other scientists in their own research projects. in this way, theoriginal findings become evaluated, certified, and incorporated within, or perhaps rejected from, the public corpusof scientific knowledge.according to this model, which i have oversimplified, restrictions on access constitute departures from thenormal and normative course of science described by the mertonian norms and by many who are interested in datasharing. in our research, on the other hand, hilgartner and i came to believe in the need for a more processorientedmodel that was directed more toward continuity and flow and less toward the notion of data as a clearly definedand fixed entity. this more flexible approach, which receives support from the ethnographic literature of science,7we came to call the òdata stream model.ó within the framework of this model, data are not classified as discreteand atomistic òinputó or òoutput,ó but are rather seen as part of an evolving stream of scientific production.these data streams have several important properties for purposes of our analysis. first, they are composed ofa heterogeneous collection of entities. hilgartner and i include within the rubric of data any of the many differentthings that scientists use or produce during the process of scientific research. scientists use a variety of terms forthese entities that represent the contributions to and byproducts of their work, including findings, preliminaryresults, samples, materials, laboratory techniques and knowhow, protocols, algorithms, software, instrumentation,and the contents of public databases: any and all information and resources that are used in or generated byscientific work. the meanings of these terms vary across fields and subfields and therefore can become confusing.the elements of the data stream are, by their very nature, situational in character. the fact that they are heterogeneous means that access to them comes in different forms and brings along different practical considerations.providing access to a reagent differs from providing access to a lab technique. in addition, as a further dimensionof their heterogeneity, these elements vary as to a variety of characteristics, among them perceived factual status,scarcity, novelty, and value. some entities may be well established and others more novel. some may be easilyaccessible and others quite rare. some may be accepted by most of the scientists in a field and others may beregarded as less reliable. over time, of course, these attributes, each of which is related to access, shift and changeflexibly. for example, as data become better established and enter the core of accepted science, decisions aboutaccessñto whom, how, what, and whenñchange as well.a second critical characteristic of data streams after heterogeneity is the fact that they are composed of chainsof products. elements of the data stream are modified over the course of research and laboratory practice andassume different forms as samples are modified and converted to statistics, which, in turn, find their positions intables and charts and eventually in scientific papers. these changes alter not only form but also utility, affecting inturn decisions about access. this notion that elements of the data stream are connected as chains underlines the factof the data streamõs continuousness. and, as a continuous stream, it can be diverted in whole or part in any of aninfinite number of different directions. the consequence of this for data sharing is that there cannot be a correctsingle way to provide access to data. therefore, it is unlikely that a single definitive statement can apply, as apolicy matter, to all elements of the data stream at all points in time.using this fluid concept of the data stream, some exchanges of data will be formal ones such as by publicationin peerreviewed journals. however, many of the most critical exchanges of data will be informal. indeed, these7see, e.g., knorrcetina, k. 1992. òthe couch, the cathedral, and the laboratory: on the relationship between laboratory and experimentin science,ó in science as practice and culture, a. pickering, ed., university of chicago press, chicago; knorrcetina, k. 1981. themanufacture of knowledge,pergamon, new york; latour, b. 1987. science in action, harvard university press, cambridge, ma; latour, b.and s. woolgar. 1979. laboratory life, sage, beverly hills, ca; and lynch, m. 1985. art and artifact in laboratory science. routledge &kegan paul, boston.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.68the role of s&t data and information in the public domaininformal exchanges may be far more significant for the progress of science than is publication, as important as itmay be. to lay out the nature of some of these informal exchanges, i interviewed a biomedical researcher workingin the area of genetics, asking him to articulate some of these informal exchanges and to lay out as comprehensively as possible the ways in which he gave access to his own data and got access to other peopleõs data. theexchanges he spoke about could be divided into three broad categories: sending people, sending things other thanresults, and sending results. these are, of course, arbitrary and overlapping categories, but within the broadlyinclusive meaning of the word òdataó that hilgartner and i have used, they all involve data sharing.in the category of sending people, he spoke of having members of his group visit other labs to learn a newtechnique or having people come to his lab to learn such a technique. sometimes, if the technique is extremelycomplex or critical, he might himself visit another lab. this is perhaps more common with junior researchers.nevertheless, a fairly senior crystallographer spoke of spending his sabbatical after he was already tenured insomeone elseõs lab learning how to produce an enzyme he wanted to crystallize. another crystallographer indicated that, although he had been working in a related field, when he decided to move into crystallography, he wentto one of the most active programs and worked there for several years to master the techniques. sometimes theresearcher might accept or send a graduate student who is to learn not a single technique but the entire researchprocess. this might be an explicit exchange or it might occasionally be a bit covert, such as when someone hiresa graduate from one lab to do a postdoc in another lab, motivated by the fact that the postdoc is familiar with thetechniques used in the first lab. the same process might occur with someone who had simply worked in the firstlab and was now looking for another job. as an example of this, one of the crystallographers told of trying to growdifferent substances and of being overwhelmed by all of the details in the new techniques he was trying to master.one of his colleagues in a related field suggested, òwhy donõt you hire one of our grad students to sterilize themedia for you and show you how to inoculate media and that sort of thing?ó this strategy worked famously forhim and he was able eventually to master the techniques on his own. in these ways, a portion of the data stream isdiverted by virtue of the movement of people, a very important informal mode of access to data.within the category of sending people, we might also include the giving of talks. our informant scientistcategorized the different types of talks he had been asked to give within the previous six months. in one instance,he spent a day with another research group, describing his work, his lab techniques, and where he intended to gonext in his research. he viewed that the quid pro quo for this experience was the expectation that someone from therecipient group likely would be asked to visit his group in the future. in addition, he had been asked to give avariety of talks, some of which he delivered and some of which he did not, depending on òwhat was in it foró him.if, for example, he would be speaking in a place to which he had wanted to get access for some reason (includingthe fact that their work seemed interesting to him), he would give the talk. he had also been invited to addresssmall groups of different sizes, including seminars of graduate students.the extent to which these processes proceed smoothly or at all depends in part on the attributes of the elementsof the data stream that i discussed earlier. where fields are competitive and samples and techniques are rare, theremay be less inclination to undertake some of these informal modes of sharing. the potential for commercializationmay affect the process as well. some of the crystallographers expressed the feeling that these processes hadcombined to result in a loss of openness in the field. one scientist who had been in the field for many yearsbemoaned the loss of less competitive times. at one time, this scientist said, òif someone had a problem, theyõdcall up and say, look, iõm interested, can we collaborate, or do you mind if i work [on this problem] or somethinglike that.ó now, with the increasing competition in that field, this crystallographer believed that these overtureswere less likely to be made, and, if made, were less likely to be successful. another believed that the mostimportant aspect of his program, accounting for the high level of productivity of the participants, was the fact thatthere was what he called a òwhole catalytic massó of individuals who collaborated rather than competed. comparethis with one crystallographer who told me that he was moving to a new institution in which, supported by apharmaceutical company, he would not even be permitted to speak to the members of his own department abouthis work. each of these instances reflects differences in the processes of access to data, sharing of data, anddiversion of the data stream.in the category of sending things other than results, the geneticist indicated that some lab groups did not wish toparticipate in the exchange of people. instead, they used their knowhow, reagents, or instrumentation as a kind ofthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 169currency in the exchange of data. groups unwilling to export their protocol might be willing to import anotherscientistõs samples and perform their protocol on them, providing that scientist with their findings. often, this occursas part of a contract process. other motivations for a group to treat another labõs samples include the desire to¥maintain quality control over their procedure by being the only ones to employ it,¥have their names on additional papers,¥work with a particularly interesting data set, or¥corroborate the accuracy of their method using a new data set.like the movement of people, these processes also represent modification of the data stream.other kinds of òthings other than resultsó that are shared include unique samples, clones, or reagents. thiskind of sharing can occur informally or formally. the informant scientist described two instances in which thisoccurred. in one case, he was given access to a rare reagent, but was required to sign a material transferagreement in which he promised to use the materials for research only, to provide the donor lab with access to hisresults, and not to pass the reagent on without express permission. such agreements may require the addition of thedonorsõ names to future scientific papers, although this particular agreement did not. in a second instance, therequests for samples became so onerous that when a commercial company became involved and handled thedistribution, it was a great relief not to have to handle it any more. one of the crystallographers, on the other hand,complained of being unable to get a critical reagent from a pharmaceutical company that refused him on thegrounds that they were already collaborating with another group. this refusal stopped this thread of the scientistõsresearch entirely, and he had been, at the time of the interview, unable to get the reagent from any other source.another crystallographer told me that he was unable to get a certain enzyme. òunless you are well known, a nobelprize winner,ó one has to make the substance oneself. he indicated that if a scientist is a notable person in the field,other scientists would be more inclined to give him or her materials, hoping something dramatic would be donewith them that would bring reflected glory on their producer. still another crystallographer ended up paying anacademic colleague in another lab thousands of dollars to produce the material he needed.computer programs are often treated in similar ways, sometimes with the stricture attached that the developerõsname be on further papers or that further sharing be with the approval of the developer of the program, sometimeswithout restrictions, either with or without a financial cost attached. other sharing within this category relates toinstrumentation. some instruments are small enough and inexpensive enough so that every lab will have their own.one example would be glassware. other instruments are large, not portable, and very expensive. they must be sharedin situ, with the samplesñwith or without people attachedñcoming to the instruments. one of the crystallographersdescribed this process with respect to a magnetic device at another institution. complaining about having to queue upfor access to the magnetic device, this scientist regarded wealthy labs as fortunate in being able to send personnel todo the experiments themselves. smaller labs that could not spare the personnel to do so were perceived as achievinglower positions in the queue as a result. in our parlance, the characteristics of this element of the data stream, the factthat it was heavily in demand, rare, and expensive, colored the access process. however, it is important to note thatthis crystallographer had been successful in completing dozens of experiments over the period of the relationship andrecognized the process as a sharing of data on both of their parts, with one lab providing the samples and the otherproviding the instrumentation. in our terms, it would be considered a merging of the data stream.which pattern is followed in these situations is shaped by the characteristics of the data entities in questionñrare, proliferate, easy and cheap to make, expensive or difficult to makeñas well as on those of the scientistsñjunior or senior, part of a large lab or small lab, part of a common network created by past experience such as acommon postdoc or academic institution or strangers. do they each have something that the other wants such asthe materials and the reflected glory discussed above or is the exchange more unequal? each of these factors willcolor the access pathways and the results will vary in each instance.what is perhaps most often referred to as data sharing is the sharing of findings. this process is also shapedby the attributes of the data stream, the nature of the findings and of the actors. one of the crystallographers spokeof releasing data to a scientistñnot a crystallographerñin another country who was working on the same problemfrom a different angle. if this scientist could corroborate the crystallographerõs data using his own theoreticalmodel and methods, it would strengthen the crystallographerõs findings and suggest new directions for research forthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.70the role of s&t data and information in the public domainboth of them. this possibility, compounded perhaps by the fact that the two scientists were in different fields andtherefore were not in direct competition, led to a comfortable and extensive collaboration between the two.our informant geneticist has been the recipient of requests for findings that can be pooled with other data toincrease the significance of findings and to test the reliability and accuracy of findings. such requests are fairlycommon in certain areas of research such as epidemiology. i asked him what happens. his response was a shortversion of the whole long story: òsome people share and some donõt.ó some of this willingness or aversion tosharing is chargeable to personal idiosyncrasy. after all, even back in nursery school, some people shared thelegos and some people did not. but the more interesting issues involve the social structural and economicconsiderations that shape the choices people make. without a full understanding of such considerations, it isdifficult to alter these choices by fiat. for example, the geneticist indicated that at times findings are releasedbecause of mandates by either journals or funders. he made the point, however, that even where disclosure wasmandatory, findings were often released without important details whose lack rendered the data significantly lesshelpful for downstream users. sometimes, data were incomplete. one of the crystallographers reported an occasion in which the coordinates of a structure were released for publication purposes omitting a water, without whichthe coordinates were not terribly helpful. another told of the release of only the central chain. in some instances,results may be coded in such a way that the critical information cannot be accessed.the informant geneticist reported his belief that some scientists intentionally modify their data so as to makethem less useful to subsequent users, but other times the data are simply not in usable form because of the formatin which they were originally collected. if the data producers must do any kind of real work in terms of modifyingthese findings to make them more usable to the downstream user, that person may well expect to be rewarded. forexample, when this scientist requested that a data set to which he had been given access be updated, he was askedto include the names of the original data producers on subsequent scientific papers.sometimes, scientists are not averse in principle to releasing their data but believe that because of the natureof the data set, they must delay release. one crystallographer reported that colleagues had said, òyou canõt have it,itõs a mess, so please, now, itõs not good enough, so they didnõt give it to [him] for six, eight months, but they gave[him] enough of the overall orientation, [so that he] could do work with it even at that . . . point.ó he further statedthat, ònever has anyone said, no, you canõt have it, but if it isnõt finished and if itõs not there, you canõt blamethem.ó although this delay was presumably temporary, another crystallographer said that he often refrains fromsharing the source code from his selfdeveloped software because it takes too long to explain how he deals witheach of the many glitches in it. in these cases again, the characteristics of the elements of the data stream contributeto shaping the timing and circumstances of access.my point in laying out these details is that much of the significant sharing of data occurs not throughpublication but in these less formal contexts that i described. as one of my informants put it, òbeing in touchreplaces abstracts and publications. the most interesting stuff i hear is either presented at meetings or heard on anemail. even the fastest publication is slow compared to that and if you have to wait until you see stuff in print,youõre out of the loop.ó one of the crystallographers referred to presenting an abstract as a òlittle trickó in theinterests of òoneupmanship,ó but it also reflects the way structural incentives in science can result in sharing. theway to influence the smartest scientists, one of the crystallographers said, is through òtalks at national meetingsthat they happen to be at, discussions, interactions with highprofile people who they happen to run into at ameeting.ó too much of a focus on data sharing through formal publication and the incentives and disincentivesthat exist to publish at time a as opposed to time b will miss much of this critical sharing of data.8the publication process is, indeed, one important mechanism for the sharing of data and for entering scientificinformation into the public domain. what is equally if not more crucial for the progress of science, however, is theeffect of social and economic pressures on the informal sharing of data by scientists and on the flow of datathrough the different scientific fields. as we consider the solutions to the problems of access and of the privatizationof scientific data, we need to keep a close eye on these informal mechanisms of data sharing and on the ways inwhich they are shaped by the climate of science and society.8ironically, the publication process itself may lead to a certain amount of unintended informal data sharing as when a colleague reviewingan article for a major journal called one crystallographer to report on the progress of another group working on the same structure as he was.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.71session 2: pressures onthe public domainthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2737310discussion frameworkjerome reichman1.the digital revolution has made investors acutely aware of the heightened value that collections of data andinformation may acquire in the new information economy. attention has logically focused on the incentive andprotective structures for generating and disseminating digital information products, especially online. althoughmost of the legal and economic initiatives have been focused onñand driven byñthe entertainment sector,software, and large publishing concerns, significant focus has been devoted to the possibility that commoditizationof even publicsector and publicdomain data would stimulate substantial investments by providing new means ofrecovering the costs of production. moreover, investors have increasingly understood the economic potential thatawaits those who capture and market data and information as raw materials or inputs into the upstream stages ofthe innovation process.what follows focuses first on pressures to commoditize data in the public sector and then on legal andtechnological measures that endow database producers with new proprietary rights and with novel means ofexploiting the facts and data that copyright law had traditionally left in the public domain. these pressures ariseboth from within the research community itself and from forces extraneous to it. how that community responds tothese pressures will over time determine the future metes and bounds of the information commons that supportscientific endeavors.if, as we have reason to fear, current trends will greatly diminish the amount of data available in the publicdomain, this decrease could initially compromise the scientific communityõs ability to fully exploit the promise ofthe digital revolution. moreover, if these pressures continue unabated and become institutionalized at the international level, they could disrupt the flow of upstream data to both basic and applied science and undermine theability of academia and the private sector to convert cumulative data streams into innovative products and services.the pressures discussed here also pose serious conflicts between the norms of public science and the norms ofprivate industry. we contend that failure to resolve these conflicts and to properly balance the interests at stake inpreserving an effective information commons could eventually undermine the national system of innovation.1this presentation is based on an article by j. h. reichman and paul f. uhlir. òa contractually reconstructed research commons forscientific data in a highly protectionistic intellectual property environment,ó 66 law and contemporary problems (winterspring 2003),and is reprinted with the permission of the authors.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.74the role of s&t data and information in the public domaincommoditization of data in public scienceduring the past 10 years, there has been a marked tendency to shift the production of sciencerelevantdatabases from the public to the private sector. this development occurs against the background of a broader trendin which the governmentõs share of overall funding for research and development vis‹vis that of the privatesector has decreased from a high of 67 percent in the 1960s to 26 percent in 2000. furthermore, since the passageof the bayhdole act in 1980, the results of federally funded research at universities have increasingly beencommercialized either by publicðprivate partnerships with industry or directly by the universities themselves.reducing the scope of governmentgenerated datathe budgetary pressures on the government are both structural and political in nature. on the whole, mandatedentitlements in the federal budget, such as medicare and medicaid, are politically impossible to reduce; as theircosts mount, the money available for other discretionary programs, including federally sponsored research, hasshrunk as a percentage of total expenditures.this structural limitation is compounded by the rapidly rising costs of stateoftheart research, includingsome researcher salaries, scientific equipment, and major facilities. with specific regard to the information infrastructure, researchers earmark the lionõs share of expenses to computing and communications equipment, with theremainder devoted to managing, preserving, and disseminating the publicdomain data and information that resultfrom basic research and other federal data collection activities. the governmentõs scientific and technical data andinformation services are thus the last to be funded and are almost always the first to suffer cutbacks.for example, the national oceanic and atmospheric administrationõs (noaa) budget for its national datacenters remained flat and actually decreased in real dollars between 1980 and 1994, whereas its data holdingsincreased exponentially and the overall agency budget doubled (mostly to pay for new environmental satellites anda groundbased weather radar system that are producing the exponential data increases). information managers atmost other science agencies have complained about reductions in funding, for both their data management andscientific and technical information budgets.these chronic budgetary shortfalls for managing and disseminating publicdomain scientific data and information have been accompanied by recurring political pressures on the scientific agencies to privatize their outputs.until recently, for example, the common practice of the environmental and space science agencies was to procuredata collection systems, such as observational satellites or groundbased sensor systems, from private companies.such procurements were made under contract and pursuant to government specifications based on consensusscientific requirements recommended by the research community. private contractors would thus build and deliverthe data collection systems, which the agencies would then operate pursuant to their mission. all data from thesystem would then belong to the government and would enter the public domain.today, however, industry has successfully pursued a strategy of providing an independent supply of thegovernmentõs needs for data and information products rather than building and delivering data collection systemsfor government agencies to operate. this solution leaves the control and ownership of the resulting data in thehands of the company, and allows it to license them to the government and to anyone else willing to pay. becauseof this newfound role of the government agency as a cash cow, there has recently been a great deal of pressure onthe science agencies, particularly from congress, to stop collecting or disseminating data inhouse and to obtainthem from the private sector instead.this approach previously resulted in at least one welldocumented fiasco, namely, the privatization of thenasaðnoaa landsat program in 1985, which seriously undermined basic and applied research in environmental remote sensing in the united states for the better part of a decade. more recently, the commercial space act of1998 directed nasa to purchase space and earth science data collection and dissemination services from theprivate sector and to treat data as commercial commodities under federal procurement regulations. the meteorological data valueadding industry has directed similar lobbying pressures at noaa. the photogrammetric industry has likewise indicated a desire to expand the licensing of data products to the u.s. geological survey and toother federal agencies.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 275efforts have also been made by various industry groups to limit the online information dissemination servicesof several federal science and technology agencies. in the cases of the patent database of the u.s. patent andtrademark office, the pubmed central database of peerreviewed life science journal literature provided on a freeand unrestricted basis by the national institutes of health national library of medicine, and certain types ofweather information disseminated by the national weather service, such efforts have proved unsuccessful to date.however, publisher groups did succeed in terminating the department of energyõs pubscience web portal forphysical science information.commercial exploitation of academic researchturning to governmentfunded research activities, the trend of greatest concern for purposes of this chapter isthe progressive incorporation of data and data products into the commercialization process already under way inacademia. the original purpose of the bayhðdole act and related legislation was primarily to enable universitiesto obtain patents on applications of research results. more recently, this activity has expanded to securing bothpatents and copyrights in computer programs. now, databases used in molecular biology have themselves becomesources of patentable inventions, and the potential commercial value of these databases as research tools hasattracted considerable attention and controversy.these and other databases have increasingly been subject to licensing agreements prepared by universitytechnology transfer offices, which may be prone to treat databases like other objects of material transfer agreements. the default rules that such licensing agreements tend to favor are exclusive arrangements under onerousterms and conditions that include restrictions on use, and even grantback and reachthrough clauses claiminginterests in future applications.moreover, there is a growing awareness in academic circles generally that data and data products may be ofconsiderable commercial value, and individual researchers have become correspondingly more wary of makingthem as available as before. this trend, together with the pressures on government agencies described previously,would pose serious problems for the research communityõs abilities to access and use needed data resources underany circumstances. in reality, these problems could become much greater as new legal and technological fencingmeasures become more broadly implemented.intellectual property, econtracts, and technological fencestraditional copyright law was friendly to science, education, and innovation by dint of its refusal to protecteither facts or ideas as eligible subject matter; by limiting the scope of protection for compilations and other factualworks to the stylistic expression of facts and ideas; by carving out express exceptions and immunities for teaching,research, and libraries; and by recognizing a catchall, fallback òfairuseó exception for nonprofit research andother endeavors that advanced the public interest in the diffusion of facts and ideas at relatively little expense toauthors. reinforcing these policies were various judicial decisions and partially codified exceptions for functionally dictated components of literary works, which take the form of nonprotectible methods, principles, processes,and discoveries. on the whole, these principles tended to render facts and data as such ineligible for protection andto allow researchers to access and use facts and data otherwise embodied in protectible works of authorshipwithout undue legal impediments.in contrast, recent legal developments in intellectual property law and contracts law have radically changedthe preexisting regime. these and other related developments now make it possible to assert and enforceproprietarial claims to virtually all the factual matter that previously entered the public domain the moment it wasdisclosed.some of the earliest changes were intended to bring u.s. copyright law into line with longstanding norms ofprotection recognized in the berne convention. for example, the principle of automatic copyright protection, theabolition of technical forfeiture due to lack of formal prerequisites, such as notice, and the provision of a basic termof protection lasting for the life of the creator plus 50 years were all measures adopted in the predigital era for thisreason.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.76the role of s&t data and information in the public domainbeginning in the 1980s, however, the united states took the lead in reshaping the berne convention itself toaccommodate computer programs, which many commentators and governments had preferred to view as òelectronic information toolsó subject to more procompetitive industrial property laws, including patents, unfair competition, and hybrid (or sui generis) forms of protection. by the 1990s, a coalition of òcontent providersó concernedabout online copying of movies, music, and software in the new digital environment had persuaded the u.s.government to press for still more farreaching changes of international copyright and related laws. these effortsled to the codification of universal copyright norms in the traderelated intellectual property rights (trips)agreement of 1994 and to two 1996 world intellectual property organization (wipo) treaties on copyrights andrelated rights in cyberspace, which endowed authors with a bevy of new exclusive rights tailormade for onlinetransmissions and imposed unprecedented obligations on participating governments to prohibit electronic equipment capable of circumventing these rights. all of these new norms and obligations, ostensibly adopted todiscourage marketdestructive copying of literary and artistic works, then became domestic law, often with noregard for their impact on science and sometimes with deliberate disregard of measures adopted to safeguardscience and education at the international level.at the same time, and as part of the same overall movement, the coalition of content providers that hadcaptured congressõ attention took aim at two closely related areas in which much more than marketdestructivecopying was actually at stake. the first of these was to validate the uncertain status of standardform electroniccontracts used to regulate online dissemination of works in digital form. because traditional contracts and saleslaws can be interpreted in ways that limit the kinds of terms that can be imposed through òshrinkwrapó or òclickonó licenses, and the onesidedness of the resulting òadhesion contracts,ó the coalition pushing the highprotectionist digital agenda has also sponsored a new uniform law, the uniform computer information transactions act,to validate such contracts in the form they desire, and it has lobbied state legislatures to adopt them.the last major component of the highprotectionistsõ digital agenda was an attempt by some of the largestdatabase companies to obtain a sui generis exclusive property right in noncopyrightable collections of information,even though facts and data had hitherto been offlimits even to international copyright law as reformed under thetrips agreement of 1994. these efforts culminated in the european communityõs directive on the legalprotection of databases adopted in 1996; in a proposed wipo treaty on the international protection of databasesbuilt on the same model, which was barely defeated at the wipo diplomatic conference in december 1996; andin a series of database protection bills that have been introduced in the u.s. congress and that attempt to enactsimilar measures into united states law.most of the developments outlined above resulted from efforts that were not undertaken with science in mind,although publishers who profit from distributing commercialized scientific products promoted some of the changesthat appear most threatening for scientific research, especially database protection laws. the following subsectionsshow that all these measuresñwhatever their ostensible purposeñhave the cumulative effect of shrinking theresearch commons.i will first briefly note the impact of selected developments in both federal statutory copyright law and incontract laws at the state level. i then discuss current proposals to confer strong exclusive property rights onnoncopyrightable collections of data, which constitute the clearest and most overt assault on the public domain thathas fueled both scientific endeavors and technological innovation in the past.expanding copyright protection of factual compilations: the revolt against feistthe quest for a new legal regime to protect databases was triggered in part by the u.s. supreme courtõs 1991decision in feist publications, inc. v. rural telephone service co., which denied copyright protection to the whitepages of a telephone directory. as discussed above, that decision was notable for reaffirming the principle thatfacts and data as such were ineligible for copyright protection as òoriginal and creative works of authorship.ó italso limited the scope of copyright protection to any original elements of selection and arrangement that otherwisemet the test of eligibility. in effect, this meant that secondcomers who developed their own criteria of selectionand arrangement could in principle use prior data to make followon products without falling afoul of the copyrightownerõs strong exclusive right to prepare derivative works. taken together, these propositions supported thethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 277customary and traditional practices of the scientific community and facilitated both access to and use of researchdata.in recent years, however, judicial concerns about the compilersõ inability to appropriate the returns from theirinvestments have induced leading federal appellate courts to broaden copyright protection of low authorshipcompilations in ways that significantly deform both the spirit and the letter of feist. at the eligibility stage, so littlein the way of original selection and arrangement is now required that the only print media still certain to beexcluded from protection are the white pages of telephone directories.more tellingly, the courts have increasingly perceived the eligibility criteria of selection and arrangement aspervading the data themselves to restrain secondcomers from using preexisting datasets to perform operations thatare functionally equivalent to those of an initial compiler. in the second circuit, for example, a competitor could notassess used car values by the same technical means as those embodied in a firstcomerõs copyrightable compilation,even if those means turned out to be particularly efficient.2 similarly, the ninth circuit prevented even the use of asmall amount of data from a copyrighted compilation that was essential to achieving a functional result.3copyright law provides a very long term of protection, and it endows authors, including eligible databaseproprietors, with strong rights to control followon applications of the protectible contents of their works. stretching copyright law to cover algorithms and aggregates of facts (and even socalled òsoft ideasó) as these recentdecisions have done conflates the ideaexpression dichotomy and indirectly extends protection to facts as such.opponents of sui generis database protection in the united states cite these and other cases as evidence thatno sui generis database protection law is needed. in reality, these cases suggest that, in the absence of a suitableminimalist regime of database protection to alleviate the risk of market failure without impoverishing the publicdomain, courts tend to convert copyright law into a roving unfair competition law that can protect algorithms andother functional matter for very long periods of time and that could create formidable barriers to entry. thistendency, however, ignores the historical limits of copyright protection and ultimately jeopardizes access to theresearch commons.the digital millennium copyright act of 1998: an exclusive right toaccess copyrightable compilations of data?with regard to copyrightable compilations of data distributed online, amendments to the copyright act of 1976,known as the digital millennium copyright act of 1998 (dmca), may have greatly reduced the traditional safeguards surrounding research uses of factual works. technically, section 1201(a) establishes a right to prevent thedirect circumvention of any electronic fencing devices that a content provider may have employed to control accessto a copyrighted work delivered online. section 1201(b) then perfects the scheme by exposing manufacturers andsuppliers of equipment capable of circumventing electronic fencing devices to liability for copyright infringementwhen such equipment can be used to violate the exclusive rights traditionally held by copyright owners.in enacting these provisions, congress seems to have detached the prohibition against gaining unauthorizeddirect access to electronically fenced works under section 1201(a) from the balance of public and private interestsotherwise established in the copyright act of 1976. as professor jane ginsburg interprets this provision, aviolation of section 1201(a) is not an òinfringement of copyrightó because it attracts a separate set of distinctremedies set out in section 1203 and because it constitutes òa new violationó for which those remedies areprovided.4 on this reading, unlawful access is not subject to the traditional defenses and immunities of thecopyright law, and one is ònot. . .permitted to circumvent the access controls, even to perform acts that are lawfulunder the copyright act,ó including presumably the userõs rights to extract unprotectible facts and ideas or toinvoke the òfair useó defense.5 on the contrary, òcongress may in effect have extended copyright to cover ôuseõ of2ccc info. services, inc. v. maclean hunter market reports, inc., (44 f.3d 61 (2d cir. 1994)3cdn inc. v. kapes (197 f.3d 1256 (9th cir. 1999)4jane c. ginsburg, òu.s. initiatives to protect works of low authorship,ó in expanding the boundaries of intellectual property: innovation policy for the knowledge society, by rebecca s. eisenberg, at 6364.5ibid., at 6264.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.78the role of s&t data and information in the public domainworks of authorship, including minimally original databases. . .because ôaccessõ is a prerequisite to ôuse,õ [and] bycontrolling the former, the copyright owner may well end up preventing or conditioning the latter.ó6while the precise contours of these provisions remain to be worked out in future judicial decisions, they couldpotentiate the ability of both publishers and scientists to protect online collections of data that were heretoforeunprotectible in print media. if, for example, a database provider combined the noncopyrightable collection of datawith a nominally copyrightable component, such as an analytical explanation of how the data were compiled, theòfig leafó copyrightable component might suffice to trigger the òno direct accessó provisions of section 1201(a).7in that event, later scientific researchers could not circumvent the electronic fence in order to extract or use thenoncopyrightable data, even for nonprofit scientific research, because section 1201(a) does not recognize thenormal exceptions to copyright protection that would allow such use and scientific research is not one of the fewvery limited exceptions that were codified in section 1201(d)(j).later researchers would thus have to acquire lawful access to the electronically fenced database under section1201(a) and then attempt to extract the noncopyrightable data for nonprofit research purposes under section1201(b), which does in principle recognize the traditional users defenses as well as the privileges and immunitiescodified in sections 107122 of the copyright act of 1976. even here, however, later scientists could discover thatthe technical devices they had used to extract nonprotectible data from minimally copyrightable databases independently violated section 1201(b) of the dmca because those devices were otherwise capable of substantialinfringing uses.8 in practice, moreover, the posterior scientistsõ theoretical opportunity to extract noncopyrightabledata by technical devices that did not violate section 1201(b) could already have been compromised by theelectronic contracts these scientists will have accepted in order to gain lawful access to the online database in thefirst place and thus to avoid the crushing power of section 1201(a). in that event, the scientists would almostcertainly have waived any user rights they had retained under section 1201(b), unless the electronic contractsthemselves became unenforceable on one ground or another, as discussed below.in effect, the dmca allows copyright owners to surround their collections of data with technological fencesand electronic identity marks buttressed by encryption and other digital controls that force wouldbe users to enterthe system through an electronic gateway. to pass through the gateway, users must accede to nonnegotiableelectronic contracts, which impose the copyright ownerõs terms and conditions without regard to the traditionaldefenses and statutory immunities of the copyright law.the dmca indirectly recognized the potential conflict between proprietors and users of ineligible material,such as facts and data, that section 1201(a) of the statute could thus trigger, and it empowered the copyrightoffice, which reports to the librarian of congress, to exempt categories of users whose activities could beadversely affected.9 while representatives of the educational and library communities have petitioned for relief onvarious grounds, including the need of researchers to access and use noncopyrightable facts and ideas transmittedonline, the authorities have so far declined to act. it is too soon to know how far owners of copyrightablecompilations can push this socalled òright of accessó at the expense of research, competition, and free speechwithout incurring resistance based on the misuse doctrine of copyright law, on the public policy and unconscionability doctrines of state contract laws, and on first amendment concerns that have in the past limited copyrightprotection of factual works. for the foreseeable future, nonetheless, the dmca empowers owners of copyrightable collections of facts to contractually limit online access to the preexisting public domain in ways that contrastdrastically with the traditional availability of factual contents in printed works.onesided electronic licensing contractsdata published in print media traditionally entered the public domain under the classical intellectual propertyregime described above. further ensuring that result is an ancillary copyright doctrine, known as òexhaustionó or6ibid., at 63.7ibid., at 63.8ibid., at 6567.917 u.s.c. ¤ 1201(a)(1)(c)(d).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 279òfirstsale doctrine,ó which limits the authorsõ powers to control the uses that third parties can make of copyrightedliterary works distributed to the public in hard copies.under this doctrine, the copyright owner may extract a profit from the first sale of the copy embodying anoriginal and protectible compilation of data, but cannot prevent a purchaser from reselling that physical copy orfrom using it in any way the latter deems fit, say, for research purposes, unless such uses amount to infringingreproductions, adaptations, or performances of the expressive components of the copyrighted compilation. ineffect, copyright law not only made it difficult to protect compilations of data as such, it denied authors anyexclusive right to control the use of a protected work once it had been distributed to the public.the firstsale doctrine thus complements and perfects the other sciencefriendly provisions described above,unless individual scientists, libraries, or scientific entities were to contractually waive their rights to use copies ofpurchased works in the manner described above. such contractual waivers always remain theoretically possible,and publishers have increasingly pressed them upon the scientific and educational communities in the onlineenvironment for reasons discussed below. nevertheless, it was not generally feasible to impose such waiversagainst scientists who bought scientific works distributed to the public in hard copies, and even when attempts todo so were made, such contracts could not bind subsequent purchasers of the copies in question. the upshot wasthat, precisely because authors and publishers could not rely on contractual agreements, they depended on thedefault rules of copyright law, which are binding against the world. these default rules, in turn, impose legislatively enacted òcontracts,ó which balance public and private interests by, for example, defining the uses thatlibraries can make of their copies and by further allowing a set of òfair usesó that scientists and other researcherscan invoke.against this background, online delivery of both copyrightable and noncopyrightable productions possessesthe inherent capabilities of changing the preexisting relationship between authors and readers or between òcontentprovidersó and òusers.ó by putting a collection of data online and surrounding it with technological fencingdevices, publishers can condition access to the database on the wouldbe userõs acquiescing to the terms andconditions of the formerõs òclickon,ó standardform, nonnegotiable contract (known as a òcontract of adhesionó).in effect, online delivery solves the problems that the printing press created for contractually restricting the use ofpublished works and it thus restores the òpower of the twoparty dealó that publishers lost in the sixteenth century.the power of the twoparty deal that online delivery makes possible is conceptually and empirically independent of statutory intellectual property rights, which makes it of capital importance for the theses discussed here. itmeans that anyone who makes data available to the world at large can control access to them and control their useby contract in ways that were inconceivable only a few years ago. nevertheless, statutory intellectual propertyrights can reinforce the contractual powers of online vendors to prohibit wouldbe users from disarming encryptiondevices to gain entry or to limit the ability of wouldbe users to extract uncopyrightable facts and ideas fromcopyrightable works delivered online, or even to limit their ability to invoke the statutory defense of fair use. thedmca lends itself to these ends, ostensibly with a view to impeding marketdestructive copying, but with theresult of strengthening the copyright monopoly at the expense of the public domain.online delivery, coupled with technological fencing devices, potentially confers these same contractualpowers on content providers in the absence of supporting intellectual property regimes, such as the dmcadiscussed above, and the new database protection rights to be discussed below. the highly restrictive digital rightsmanagement technologies that are being developed include hardware and softwarebased òtrusted systems,óonline database access controls, and increasingly effective forms of encryption. these emerging technologicalcontrols on content, when combined with the statutory intellectual property and contractual rights, can supersedelongestablished user rights and exceptions under copyright law for print media and thereby eliminate largecategories of data and information from publicdomain access.moreover, because electronic contracts are enforceable in state courts, they provide private rights of actionthat tend to either substitute for or override statutory intellectual property rights. electronic contracts becomesubstitutes for intellectual property rights to the extent that they make it infeasible for third parties to obtainpublicly disclosed but electronically fenced data without incurring contractual liability for damages. they mayoverride statutory intellectual property rights, for example, by forbidding the uses that libraries could otherwisemake of a scientific work under federal copyright law, or by prohibiting followon applications or the reversethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.80the role of s&t data and information in the public domainengineering of a computer program that both federal copyright law and state trade secret law would otherwisepermit.to the extent that these contracts are allowed to impose terms and conditions that ignore the goals and policiesof the federal intellectual property system, they would establish privately legislated intellectual property rightsunencumbered by concessions to the public interest. by the same token, a privately generated database protectedby technical devices and electronic adhesion contracts is subject to no federally imposed duration clause and,accordingly, will never lapse into the public domain.whether electronic contractsñespecially the nonnegotiable, standardform òclick onó and òshrinkwrapó contractsñare in fact enforceable remains an open and controversial question. in addition to technical obstacles toformation based on general contracts law principles, courts may deem such contracts unenforceable under theòpublic policyó defense of state contracts law, under the preemption doctrine that supports the integrity of thefederal intellectual property system, or under some combination of the two. in practice, however, courts appearreluctant to exercise such powers even when their right to do so is clear. the most recent line of cases, led by theseventh circuitõs opinion in procd v. zeidenberg,10 has tended to validate such contracts in the name of òfreedomof contract.óin this same vein, the national council of commissioners for uniform state law has proposed a uniformcomputer information transactions act (ucita), which, if state legislatures enacted it, would broadly validateelectronic contracts of adhesion and largely immunize them from legal challenge. for example, ucita permitsvendors of information products to define virtually every transaction as a òlicenseó rather than a òsale,ó and ittolerates perpetual licenses. it could thus override the firstsale doctrine of copyright law and any analogous doctrinethat might be embodied in the proposed database protection laws discussed below. the proposed uniform law wouldthen proceed to broadly validate massmarket òclickonó and òshrinkwrapó licenses that imposed all the provisionsvendors could hope for, with little regard for the interests of scientific and educational users, or the public in general.a detailed analysis of ucitaõs provisions is beyond the scope of this discussion. suffice it to say, however,that its less than transparent drafting process so favored the interests of sellers of software and other informationproducts at the expense of consumers and users generally that a coalition of 16 state attorneys general vigorouslyopposed its adoption, and the american law institute withdrew its cosponsorship of the original project. nonetheless, two statesñmaryland and virginiañhave adopted nonuniform versions of ucita, and major software andinformation industry firms continue to lobby assiduously for its enactment by other state legislatures.if present trends continue unabated, privately generated information products delivered onlineñincludingdatabases and computer softwareñmay be kept under a kind of perpetual, massmarket trade secret protection,subject to no reverse engineering efforts or publicinterest uses that are not expressly sanctioned by licensingagreements. contractual rights of this kind, backed by a onesided regulatory framework, such as ucita, couldconceivably produce an even higher level of protection than that available from some future federal database rightsubject to statutory publicinterest exceptions. the most powerful proprietary cocktail of all, however, wouldprobably emerge from a combination of a strong federal database right with ucitabacked contracts of adhesion.new exclusive property rights in noncopyrightable collections of datathe challenge of protecting collections of information that fail to meet the technical eligibility requirements ofcopyright law poses a hard problem that has existed for a halfcentury or longer, and at least three differentapproaches have emerged over time. one solution was to allow a domestic copyright law to accommodate òlowauthorshipó literary productions, with some adjustments to the bundle of rights at the margins. a second approach,adopted in the nordic countries, was to enact a shortterm sui generis regime, built on a distinctly copyrightlikemodel, that would protect catalogs, directories, and tables of data against wholesale duplication, without conferring on proprietors any exclusive adaptation right like that afforded to authors of true literary and artistic works. athird approach, experimented with at different times and to varying degrees in different countries, including the1086 f.3d 1447 (7th cir. 1996)the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 281united states, was to protect compilers of information against wholesale duplication of their products underdifferent theories rooted in the òmisappropriationó branch of unfair competition law.what changed in the 1990s was the convergence of digital and telecommunications networks, which potentiatedthe role of electronic databases in the information economy generally and which made scientific databases inparticular into agents of technological innovation whose economic potential may eventually outstrip that accruingfrom the patent system. notwithstanding the robust appearance of the presentday database industry under freemarket conditions, analysts asked whether inadequate investment in complex digital databases would not inevitablyhinder that industryõs longterm growth prospects if freeriding secondcomers could appropriate the contents ofsuccessful new products without contributing to their costs of development and maintenance over time. in otherwords, if copyright, contract law, digital rights management technologies, residual unfair competition laws, andvarious protective business practices inadequately filled a gap in the law, then regulatory action to enhance investment might be justified. this utilitarian rationale, however, raised new and still largely unaddressed questions aboutthe unintended social costs likely to ensue if intellectual property rights were injudiciously bestowed upon the rawmaterials of the information economy in general and on the building blocks of scientific research in particular.any serious effort to find an appropriate sui generis solution to the question of database protection accordinglyshould have engendered an investigation of the comparative economic advantages and disadvantages of regimesbased on exclusive property rights as distinct from regimes based on unfair competition laws and other forms ofliability rules. this investigation also should have taken account of larger questions about the varying impacts ofdifferent legal regimes on freedom of speech and on the conditions of democratic discourse, which, in the unitedstates at least, are of primary constitutional importance. instead, the commission of the european communities cutthe inquiry short by adopting the directive on the legal protection of databases in 1996.11 this directive required alle.u. member countries (and affiliated states) to pass laws that confer a hybrid exclusive property right on publisherswho make substantial investments in noncopyrightable compilations of facts and information.the european union database directive in briefthe hybrid exclusive right that the european commission ultimately crafted in its directive on the legalprotection of databases does not resemble any preexisting intellectual property regime. it protects any collection ofdata, information, or other materials that are arranged in a systematic or methodological way, provided that they areindividually accessible by electronic or other means.12 to become eligible for protection, the database producer mustdemonstrate a òsubstantial investment,ó as measured in either qualitative or quantitative terms,13 which leaves thecourts to develop this criterion with little guidance from the legislative history. the drafters explicitly recognized thatthe qualifying investment may consist of no more than simply verifying or maintaining the database.in return for this investment, the compiler obtains exclusive rights to extract or to utilize all or a substantialpart of the contents of the protected database. the exclusive extraction right pertains to any transfer in any form ofall or a substantial part of the contents of a protected database;14 the exclusive reutilization right, by contrast,covers only the making available to the public of all or a substantial part of the same database.15 in every case, thefirstcomer obtains an exclusive right to control uses of collected data as such, as well as a powerful adaptation (orderivative work) right along the lines that u.s. copyright law bestows on òoriginal works of authorship,ó16 eventhough such a right is alien to the protection of investment under existing unfair competition laws. in a recentinterpretation of this provision, a u.k. court vigorously enforced this right to control followon applications of anoriginal database against a valueadding secondcomer.17 it took this position even though the proprietor was thesole source of the data in question and there was no feasible way to generate them by independent means.11directive 96/9 of the european parliament and the council of 11 march 1996 on the legal protection of databases, 1996 o.j. (l77)2.12ibid., art. 1(2).13ibid., art. 7(1).14art. 7(2)(a).15art. 7(2)(b).1617 u.s.c. ¤¤101 (òderivative worksó), 103, 106(2) (u.s.)17british horseracing bd. ltd. v. william hill org. ltd., 201 e.w.c.a. civ. 1268 (eng. c.a.).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.82the role of s&t data and information in the public domainthe directive contains no provision expressly regulating the collections of information that member governments themselves produce. this lacuna leaves european governments that generate data free to exercise eithercopyrights or sui generis rights in their own productions in keeping with their respective domestic policies. thisresult contrasts sharply with the situation in the united states, where the government cannot claim intellectualproperty rights in the data it generates and must normally make such data available to the public for no more thana costofdelivery fee.the directive provides no mandatory publicinterest exceptions comparable to those recognized under domestic and international copyright laws. an optional, but ambiguous, exception concerning òillustrations for teachingor scientific researchó applies to extractions but not reutilization.18 it may be open to flexible interpretation, andsome member countries, notably the nordic countries, have implemented this broader version. other countries,notably france, italy, and greece, have simply ignored this exception altogether, which defeats the commissionõssupposed concerns to promote uniform law.the directiveõs sui generis regime does exempt from liability anyone who extracts or uses an insubstantial partof a protected database.19 however, such a user bears the risk of accurately drawing the line between a substantialand an insubstantial part, and any repeated or systematic uses of even an insubstantial part will forfeit thisexemption.20 judicial interpretation has so far taken a very restrictive view of this exemption, and one cannoteffectively make unauthorized extractions or uses of an insubstantial part of any protected database without seriousrisk of triggering an action for infringement.qualifying databases are nominally protected for a 15year period.21 in reality, each new investment in aprotected database, such as the provision of updates, will requalify that database as a whole for a new term ofprotection.22 in this and other respects, the scope of the sui generis adaptation right exceeds that of u.s. copyrightlaw, which attaches only to the new matter added to an underlying, preexisting work and expires at a certain time.23finally, the directive carries no national treatment requirement into its sui generis component. foreigndatabase producers become eligible only if their countries of origin provide a similar form of protection or if theyset up operations within the european union.24 nonqualifying foreign producers, however, may nonetheless seekprotection for their databases under residual domestic copyright and unfair competition laws, where available.25the e.c.õs directive on the legal protection of databases thus broke radically with the historical limits ofintellectual property protection in at least three ways. first, it overtly and expressly conferred an exclusive propertyright on the fruits of investment as such, without predicating the grant of protection on any predetermined level ofcreative contribution to the public domain. next, it conferred this new exclusive property right on aggregates ofinformation as such, which had heretofore been considered as unprotectible raw material or as basic inputs availableto creators operating under all other preexisting intellectual property rights. finally, it potentially conferred the newexclusive property right in perpetuity, with no concomitant requirement that the public ultimately require ownershipof the object of protection at the end of a specified period. the directive thus effectively abolished the very conceptof a public domain that had historically justified the grant of temporary exclusive rights in intangible creations.the database protection controversy in the united statesthe situation in the united states differs markedly from that which preceded the adoption of the e.c.õsdirective on the legal protection of databases. in general, the legislative process in the united states has becomerelatively transparent. since the first legislative proposal, h.r. 3531, which was modeled on the e.c. directive and18op. cit., note 11, arts. 9, 9(b).19ibid., art. 9(b).20ibid., arts. 7(2), 7(5), 8(1).21ibid., art. 10.22ibid., art. 10(3).23see 17 u.s.c. ¤¤103, 302.24op. cit., note 11, , art. 11.25ibid., art. 13.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 283introduced by the house committee on the judiciary in may 1996, this transparency has generated a spirited andoften highlevel public debate. very little progress toward a compromise solution had been reached as of the timeof writing, however, which is hardly surprising given the intensity of the opposing views, the methodologicaldistance that divides them, and the political clout of the opposing camps.we are, accordingly, left with the two basic proposals that were still on the table at the end of the legislativesession that ended in 2000 at an impasse. these proposals, as refined during that session, represent the baselinepositions that each coalition carried into the current round of negotiations. one bill, h.r. 354, as revised in january2000, embodied the proponentsõ last set of proposals for a sui generis regime built on an exclusive property rightsmodel (although some effort was made to conceal that solution behind a facade that evoked unfair competitionlaw). the other bill, h.r. 1858, set out the opponentsõ views of a socalled minimalist misappropriation regime asit stood on the eve of the current round of negotiations.the exclusive rights model. the proposals embodied in h.r. 354 attempted to achieve levels of protectioncomparable to those of the e.c. directive by means that are more congenial to the legal traditions of the unitedstates. the changes introduced at the end of the 2000 legislative session softened some of the most controversialprovisions at the margins, while maintaining the overall integrity of a strongly protectionist regime.the bill in this form continued to define òcollections of informationó very broadly as òinformation . . .collected and . . . organized for the purpose of bringing discrete items of information together in one place orthrough one source so that persons may access them.ó26 like the e.c. directive, this bill then cast eligibility interms of an òinvestment of substantial monetary or other resourcesó in the gathering, organizing or maintaining ofa òcollection of information.ó27 it conferred two exclusive rights on the investor: first, a right to make all or asubstantial part of a protected collection òavailable to others,ó and second, a right òto extract all or a substantialpart to make available to others.ó here the term òothersó was manifestly broader than òpublicó in ways thatremained to be clarified.h.r. 354 then superimposed an additional criterion of liability on both exclusive rights that is not present inthe e.c. model. this is the requirement that, to trigger liability for infringement, any unauthorized act of òmakingavailable to othersó or of òextractionó for that purpose must cause òmaterial harm to the marketó of the qualifyinginvestor òfor a product or service that incorporates that collection of information and is offered or intended to beoffered in commerce.ó the crux of liability under the bill thus derived from a òmaterial harm to marketsó test thatis meant to cloud the copyrightlike nature of the bill and to shroud it in different terminology.here a number of concessions were made to the opponentsõ concerns in the last public iteration of the bill(january 11, 2000), some of them real, others nominal in effect. the addition of òmaterialó to the market harm testmay, for example, have addressed complaints that proponents viewed òone lost saleó as constituting actionableharm to the market.at the same time, the revised bill contained convoluted and tortuous definitions of òmarketó that the clintonadministration hoped would reduce the scope of protection in the case of followon applications.28 on closerinspection, however, these definitions provided a static picture of a moving target that amounted to a mostlyillusory limitation on the investorõs broad adaptation right. notwithstanding these socalled concessions, the billeffectively assigned most followon applications to any initial investor whose dynamic operations expand therange of potentially protectible matter with every update, ad infinitum.the bill then introduced a òreasonable useó exception that was intended to benefit the nonprofit user communities, especially researchers and educators,29 and that conveyed a sense of similarity to the òfairuseó exception incopyright law.30 once again, this became largely illusory on closer analysis, because under the proposed bill, the26h.r. 354, ¤1401(1), the òcollections of information antipiracy act,ó 106th congress (2000). here the overlap with copyright law is sopalpable that it is hard to conceive of any assemblage of words, numbers, facts, or information that would not also qualify as a potentiallyprotectible collection of information.27ibid., ¤1402(a).28 ibid.., ¤¤1401(3)(a), (b).29ibid., ¤1403(2).30 17 u.s.c. ¤¤102(b), 107122.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.84the role of s&t data and information in the public domainvery facts, data, and information that copyright law exclude themselves became the objects of protection, and therewere no other significant exceptions. hence, virtually every customary or traditional use of facts or data compiledby others that copyright law would presumably have allowed scientists, researchers, or other nonprofit entities tomake in the past now becomes a prima facie instance of infringement under h.r. 354. these users would in effecthave either to license such uses or be prepared to seek judicial relief for òreasonablenessó on a continuing basis.because university administrators dislike litigation and are risk averse by nature, and this provision put the burdenof showing reasonableness on them, there is reason to expect a chilling effect on customary uses by theseinstitutions of data heretofore in the public domain.the bill recognized an òindependent creationó norm, which presumably exempts any database, howeversimilar to an existing database that was not the fruit of òcopying.ó31 this provision codified a fundamental normof copyright law, and the european commission made much of a similar norm in justifying its own regulatoryscheme. in reality, this òindependent creationó principle would produce unintended and socially deleteriousconsequences when transposed to the database milieu precisely because many of the most complex and importantdatabases are inherently not able to be independently regenerated. sometimes the database cannot be reconstitutedbecause the underlying phenomena are onetime events, as often occurs in the observational sciences. in otherinstances, key components of a complex database can no longer be reconstituted with certainty at a later date. anyindependently regenerated database suffering from these defects would necessarily contain gaps that made itinherently less reliable than its predecessor.these problems point to a more general phenomenon that affects competition in large or complex databases.even when, in principle, such databases could be reconstituted from scratch, the high costs of doing soñascompared with the addon costs of existing producersñwill tend to make the secondcomerõs costs so high as toconstitute a barrier to entry. meanwhile, the firstcomerõs comparative advantage from already owning a largecollection that is too costly to reconstitute will only grow more formidable over time, an economic reality thatprogressively strengthens the barriers to entry and tends to reinforce (and, indeed, to explain) the predominance ofsolesource data suppliers in the marketplace.governmentgenerated data would have remained excluded, in principle, from protection, in keeping withcurrent u.s. practice,32 which differs from e.u. practice in this important respect. however, there is considerablecontroversy surrounding the degree of protection to be afforded governmentgenerated data that subsequentlybecome embodied in valueadding, privately funded databases. all parties agree that a private, valueaddingcompiler should obtain whatever degree of protection is elsewhere provided, notwithstanding the incorporation ofgovernmentgenerated data. the issue concerns the rights and abilities of third parties to continue to access theoriginal, governmentgenerated data sets. the proponents of h.r. 354 have been little inclined to accept measuresseeking to preserve access to the original data sets, despite pressures in this direction.h.r. 354 imposed no restrictions whatsoever on licensing agreements, including agreements that mightoverrule the few exceptions otherwise allowed by the bill.33 despite constant remonstrations from opponents aboutthe need to regulate licensing in a variety of circumstancesñand especially with respect to solesource providersñthe bill itself did not budge in this direction. on the contrary, new provisions added to h.r. 354 in 2000would have set up measures that would prohibit tampering with encryption devices (òanticircumvention measuresó) and with electronically embedded òwatermarksó in a manner that paralleled the provisions adopted foronline transmissions of copyrighted works under the dmca. because these provisions would have effectivelysecured a database against unauthorized access (and tended to create an additional òexclusive right of accessówithout expressly so declaring), they would only have added to the database ownerõs market power to dictatecontractual terms and conditions without regard to the public interest. these powers were further magnified by theimposition of criminal sanctions in addition to strong civil remedies for infringement.3431op. cit., note 20, ¤1403(c).32ibid., ¤1404.33ibid., ¤1404(e).34ibid., ¤¤14061407.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 285the one major concession that was made to the opponentsõ constitutional arguments concerned the questionof duration. as previously noted, the e.c. directive allows for perpetual protection of the whole database so longas any substantial part of it is updated or maintained by virtue of a new and substantial investment, and theproponentsõ early proposals in the united states echoed this provision. however, the u.s. constitution clearlyprescribes a limited term of duration for intellectual property rights,35 and the proponents finally bowed topressures from many directions by limiting the term of duration to 15 years.36any update to an existing database would have qualified for a new term of 15 years, but this protection wouldapply, at least in principle, only to the material added in the update. in practice, however, the inability to clearlyseparate old from new matter in complex databases, coupled with ambiguous language concerning the scope ofprotection against harm to òlikely, expected, or plannedó market segments, could still have left a loophole for anindefinite term of duration.the unfair competition model. the opponentsõ bill, the consumer and investor access to information act of1999, h.r. 1858, was introduced by the house commerce committee in 1999, as a sign of good faith, in responseto criticsõ claims that the opponentsõ coalition sought only to block the adoption of any database protection law.h.r. 1858 began with a definition of databases that is not appreciably narrower than that of h.r. 354, except foran express exclusion of traditional literary works that òtell a story, communicate a message,ó and the like.37 inother words, it attempted to draw a clearer line of demarcation between the proposed database regime andcopyright law, to reduce overlap or cumulative protection as might occur under h.r. 354.the operative protective language in h.r. 1858 was short and direct, but it relied on a series of contingentdefinitions that muddy the true scope of protection. thus, the bill would prohibit anyone from selling or distributing to the public a database that is (1) òa duplicate of another database . . . collected and organized by anotherperson or entity,ó and (2) òis sold or distributed in commerce in competition with that other database.ó38 the billthen defined a prohibited duplicate as a database that is òsubstantially the same as such other database, as a resultof the extraction of information from such other database.ó39here, in other words, liability would attach only for a wholesale duplication of a preexisting database thatresults in a substantially identical end product. however, this basic misappropriation approach became furthersubject to both expansionist and limiting thrusts. expanding the potential for liability was a proviso added to thedefinition of a protectible database that treats òany discrete sections [of a protected database] containing a largenumber of discrete items of informationó as a separably identifiable database entitled to protection in its ownright.40 the bill would thus have codified a surprisingly broad prohibition of followon applications that make useof discrete segments of preexisting databases, subject to the limitations set out below.a second protectionist thrust resulted from the lack of any duration clause whatsoever, with the prohibitionagainst wholesale duplicationñsubject to limitations set out belowñconceivably lasting forever. this perpetualthreat of liability would have attached to wholesale duplication of even a discrete segment of a preexistingdatabase, if the other criteria for liability were met.these powerfully protective provisions, put into h.r. 1858 at an early stage to weaken support for h.r. 354,were offset to some degree by other express limitations on liability and by a codified set of misuse standards tohelp regulate licensing. to understand these further limitations, one should recall that liability even for wholesaleduplication of all, or a discrete segment, of a protected database would not attach unless the unauthorized copywere sold or distributed in commerce and òin competition withó the protected database.41 the term òin competitionwith,ó when used in connection with a sale or distribution to the public, was then defined to mean that theunauthorized duplication òdisplaces substantial sales or licenses likely to accrue from the original databaseó and35 u.s. constitution, art. i, sec. 8, cl. 8.36 op. cit., note 16, at ¤1409(i).37h.r. 1858, ¤101(1), the òconsumer and investor access to informatin act of 1999,ó 106th congress (1999).38ibid., ¤102.39ibid., ¤101(2).40ibid., ¤101(1)(b).41ibid., ¤102.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.86the role of s&t data and information in the public domainòsignificantly threatens. . .[the firstcomerõs] opportunity to recover a reasonable return on the investmentó in theduplicated database.42 both prongs had to be met before liability would attach.it follows that even a wholesale duplication that was not commercially exploited or that did not substantiallydecrease expected revenues (as might occur from, for example, nonprofit scientific research activities) couldpresumably have escaped liability in appropriate circumstances. similarly, a followon commercial product thatmade use of data from a protected database might have escaped liability if it were sold in a distant market segmentor required substantial independent investment.h.r. 1858 then further reduced the potential scope of liability by imposing a set of welldefined exceptionsand by limiting enforcement to actions brought by the federal trade commission.43 there were express exceptions comparable to those under h.r. 354 for news reporting, law enforcement activities, intelligence agencies,online stockbrokers, and online service providers.44 there was also an express exception for nonprofit scientific,educational, or research activities,45 in case any such uses were thought to escape other definitions that limitliability to unauthorized uses in competition with the firstcomer. still other provisions clarified that the protectionof governmentgenerated data or of legal materials in valueadding embodiments would remain contingent uponarrangements that facilitate continued public access to the original data sets or materials.46 a blanket exclusion ofprotection for òany individual idea, fact, procedure, system, method of operation, concept, principle or discoveryówisely attempted to provide a line of demarcation with patent law and to ward off unintended protectionistconsequences in this direction.47another important set of safeguards emerged from the draftersõ real concerns about potential misuses of eventhis socalled òminimalistó form of protection. these concerns were expressed in a provision that expressly deniedliability in any case where the protected party òmisuses the protectionó that h.r. 1858 would afford. a relatedprovision then elaborated a detailed list of standards that courts could use as guidelines to determine whether aninstance of misuse had occurred.48 these guidelines or standards would have greatly clarified the line betweenacceptable and unacceptable licensing conditions, and if enacted, they could have made a major contribution to thedoctrine of misuse as applied to the licensing of other intellectual property rights as well.in summary, the underlying purpose of h.r. 1858 was to prohibit wholesale duplication of a database as aform of unfair competition. it thus set out to create a minimalist liability rule that would prohibit marketdestructive conduct rather than an exclusive property right as such, and in this sense, it initially posed a strongcontrast to h.r. 354. over time, however, different iterations of the bill, designed to win supporters away fromh.r. 354, made h.r. 1858 surprisingly protectionistñespecially in view of its de facto derivative work right.42ibid., ¤101(5).43ibid., ¤107.44ibid., ¤¤103(b), (c), 104(b), (e), 106(a).45ibid., ¤103(d).46ibid., ¤¤101(b), 104(f).47ibid., ¤104(d).48ibid., ¤¤106(b), 106(b)(16).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.8711the urge to commercialize: interactions betweenpublic and private research developmentrobert cookdeegan1.almost everything that i am going to present will be blindingly obvious. i see my job as synthesizing some ofwhat has been talked about previously and to look at some overall trends. the reason we are here is because welike what science and technology produce. there has been a lot more spending by governments in research anddevelopment (r&d) and even faster growth in r&d spending by private entities in the postwar era.why are we doing that? because we buy the products and services that come at the end of that process and wehave been buying a lot of them. r&d has been a source of economic growth. governments like it because itcreates wealth, and people who have more money are happier voters and it feeds back on itself. it is a virtuouscycle. we have a robust system of innovation. we are coming to the end of a decade when it seemed like thatgrowth was never going to end.i am going to go through some of these overall trends, and then i will look at biomedical research as aparticular sector of interest. i will spend most of my time talking about genomics because genomics is a posterchild, representing an area where intellectual property and the public domain are intersecting, colliding, andcausing conflict constantly, and quite conspicuously. the amount of funding going into biomedical research hasincreased by three to four orders of magnitude in real dollars since world war ii. that is an unbelievable amountof growth in five decades. the scale of effort, the number of people doing it, the amount of money, and thecommitment of social resources have all mushroomed in a relatively short period. it has happened in both thepublic and the private sectors, led by government funding, at least in the case of life sciences, and it has beenfollowed with a time lag of several years by investment in the private sector.the r&d growth in both public and private sectors has led to conflict. the human genome project isespecially good at generating conflict. when it does so, it usually is on the front pages of time, newsweek, thenew york times, and the washington post. you all have heard the stories i am going to recount. i am just goingto try to tease out some of the structures underneath the surface.what drives the growth of r&dñin science, in academia, in government, and in industry? the practice ofgenerating new knowledge has become more capital intensive. research costs a lot of money, and it takes a lot ofpeople. the scale and the complexity have increased. we need machines to generate the data. we need computersto keep track of data. this has been increasing on a large scale since world war ii in almost every discipline.1the author would like to acknowledge the assistance of stephen mccormack and leroy walters and the dna patent database atgeorgetown university.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.88the role of s&t data and information in the public domainin the political arena, drawing lessons from world war ii, governments woke up to the fact that universities andacademic centers seem to be the focal front end of a big process that generates a lot of dollars at the back end. theyrealized that universities are valuable resources and began to pay attention to the policies that foster their development.in particular, they thought about their role in economic growth, in addition to the traditional university roles of creatingknowledge and disseminating knowledge. at the same time, we have seen intellectual property grow in strength andquantity; this has been happening historically during this postwar period, most prominently in the united states.i am going to shift for a moment to the life sciences, specifically on drug development and biotechnology, theproducts and therapeutics where most of the money is made. it is about a $200 billion a year enterprise, muchlarger than at the end of world war ii. the overall policy framework has been a simple one that was crafted in theimmediate postwar era: the government funds basic research, which spills over and people make useful things outof the new knowledge and techniques, which are turned into products and services, the socalled pipeline model.that scenario actually is not too different from how many drugs have been discovered.the government has funded a lot of science. the past five decades have seen spectacular budget growth at thenational institutes of health (nih). i do not know what is going to happen after this year, but we are at the end ofanother doubling. there have been many doublings of the nih budget since world war ii.we have also encouraged patenting. the bayhdole act solidified policies that were already falling into place inthe 1980s. the federal court system was reorganized, creating a single appeals court for patent cases. the presumption in favor of patents increased. the structure of the judicial system presiding over intellectual property decisionsreinforced the technology transfer statutes addressed earlier today. moreover, there have been many other policiesthat try to fosterñat the state level, at the national level, and in private industryñthe private development of thesepublic resources to bolster the public domain in science. what have the results of these policies been?there are many startup firms. in biotechnology, i do not think anybody knows how many firms there are, butthere are more than 2,000. in 1992 you could not have called any firm in the world a genomics firm. there are nowapproximately 400 plus genomics firms, at least 300 of which are still operatingñin less than a decade, all of thesecompanies were created.there have been many patents issued to academic institutions, 3,000 of them last year. private r&d investment in universities has grown, as has licensing income from academic intellectual property. last year, the mostrecent survey from the association of university technology managers found that universities received about$1.1 billion. that is about 3 percent of the total that they spend on r&d. the patent logjams and intellectualproperty strictures on research that we are talking about, however, are some of the unintended consequences of thepolicies that have been largely successful (see figure 111).¥ transaction costs¥ money costs¥ material transfer and database agreements¥ technical licensing offices and firm lawyers as intermediaries¥ secrecy in academe¥ nondisclosure agreements¥ trade secrecy until patents are filed¥ sludge in information pipelines¥ anticommons as an unrealized but worrisome threat: patent thickets, royalty stacks, innovations foregone figure 111unintended consequences.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 289biomedical research modelsi now want to focus on genomics. my background is in human genetics. i did my first research on the geneticsof alzheimerõs disease. a long lineage of human clinical research goes all the way back to pasteur and beyond.many areas of human clinical research, including human genetics, had only glancing acquaintance with the publicdomain. data hoarding, keeping things secret, using your data so that you could get the next publication, andmaking sure that nobody else got access to data were common behaviors. within science, particularly withinhuman genetics, there were not strong open disclosure norms. there were other communities doing molecularbiology at the same time on yeast and drosophila that had òopenscienceó norms. those norms were the onesadopted as the models for the human genome project.within biomedical research, different norms for disclosure pervade different fields, so that open science andsecretive science are often working in parallel. i merely want to point out that going all the way back to pasteur,there have been norms of secrecy. geison has shown that pasteur was highly secretive, particularly regardinghuman experimentation (one provision of his will was to keep his laboratory records secret). given this history, wecannot think of some golden era that we are trying to return to, at least not in human genetics. one of the goals ofthe human genome project, as articulated by the 1988 national research council report mapping and sequencing the human genome,2 was to tilt in favor of open science over the more territorial norms of human genetics andclinical research.pharmaceutical development is one of the most patentdependent sectors in the whole economy. there was atremendous amount of government and nonprofit funding flowing into genetics and genomics for the better part ofa decade, before the private genomics effort began around 19921993. there is also an intricate mutualismbetween the public domain and the private domain in genomics that creates databases, products, and sequenceinformation. genomics attracted many players, including national and state governments, as well as private firms,because genomics was òhot.ó it was in the news and everybody wanted a piece of the action. but action towardwhat end? the policies that we are talking about focus on information flow.three different models drive commercial genomics, which were created in that first wave of genomic startupsfrom 1992 to 1994. one business plan is represented by human genome sciences (hgs). it started from owning theintellectual property from a nonprofit organization created in 1992 called the institute for genomic research. theinstitute and hgs were initially looking for human genes by picking out the parts of the genome that were known tocode for proteins and then looking for those proteins that were most likely to be thrown outside the cell, to span cellsurface membranes, to bind dna, or to serve other known functions. hgs would look for dna sequences corresponding to proteins that might be valuable to pharmaceutical companies or to themselves to develop into pharmaceuticals, and they focused on characterizing the whole gene, sequencing it, and then doing some biology to figure outwhat it did. the hgs strategy was very focused on intellectual property. there was a patent lawyer, or somebodywith legal training in patents, associated with every project team.hgs was very careful about how they set up walls of nondisclosure around their arrangements. there wassome outlicensing of their technologies to major firms so that they could get money, but there was not much in theway of publications. the main output and the main contribution to the public domain happens when hgs gets apatent because then it is published. then the sequence information that was part of their patent application ispublished and it becomes part of the public domain, except that it is constrained by the patent rights that areassociated with that work.incyte turned seriously to dna sequencing of genes about a year before hgs. it had a somewhat similarscientific strategy, but a different business plan. incyte was also looking for the juicy bits of the genome tosequence and characterize genes. incyte had a somewhat different business model in that it was working withmultiple, large pharmaceutical players, and it did not appear to have focused on exactly the same kinds ofsequences as hgs. incyte was creating a database that highpaying customers could access. through incyte, largepharmaceutical companies could avoid having to do all the sequencing themselves. they could license access todna sequence information on genes from incyte. incyteõs business plan has changed several times since the early2national research council. 1988. mapping and sequencing the human genome, national academy press, washington, d.c.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.90the role of s&t data and information in the public domainyears (as has the companyõs full name), but gene sequencing was the original core idea. incyte pursued a modelthat allowed multiple players to get access to its data. it also had looser boundaries and there was a little bit moreleakage into the public domain, and a few more publications.after these two companies had been doing genomics for 5 years, celera genomics corporation came along in1998 and proposed to sequence the whole human genome. celera was going to do it in a slightly different wayfrom the way the publicdomain project was sequencing the human genome. celeraõs initial goal was to create adatabase of genomic sequence (that is, of the genome in its native state on the chromosomes before being editedinto shorter and more compact known genes). celera needed powerful informatics, and indeed spent more oncomputers and programming than on dna sequencing and laboratory biology. because of a natural asymmetry,celera would always have more data than was in the public domainñthat is, celera could draw on the publicdomain and at the same time create its own proprietary sequencing data. celera would always have one leg up onthe public genome project and they could sell a service, which was access to their data. they were doing thatnonexclusively, but even more nonexclusively than incyte, and they had more academic collaborators. celera wasselling licenses to get access to their data using discriminate pricingña lower pricing level to academic institutions, to the howard hughes medical institute, and other academic research and higher prices (and more intensiveservices) for pharmaceutical companies.celeraõs intellectual property strategy was somewhat different from incyte and hgs. celera was filingprovisional patent applications that could be converted to patent applications, and a few dozen had ripened intopatents (52 as of december 2002 compared with 711 for incyte and 277 for hgs). the interesting thing about thecelera model is that publication in scientific journals was an inherent part of the business plan. celera planned topublish in scientific journals, and the information they publish or that they post on their web site is available forother people to use. i do not think we could use the word òpublic domainó for these data and information becausethere are restrictions on their use.these three companies have three different models. if you think of a continuum with the public domain on oneend and the private domain on the other, these companies are at intermediate points along a continuum ofcontributing data into sources where scientists can use them.there was a strong ideology that grew out of the nematode and the yeast research communities that access tothe data should be free and rapid. the sequencing centers in the human genome project are getting paid by thegovernment or nonprofit organizations (e.g., the wellcome trust) and are making the data available so everybodycan get access to them. the scientific community wanted policies to ensure that the highthroughput sequencingcenters did not get unfair advantage because of sole access. my reading is that the rapid disclosure policy was notso much a reaction against commercial practices but rather a concern about hoarding data. the idea was that theyeast and nematode genetics science communities were healthier because of the way they shared data. they aremore spontaneous. they are more creative and do better science, in part because they share their data at an earlierstage. out of that movement grew some very concrete policies, including the socalled bermuda rules or bermudaprinciples, which grew out of a meeting that the wellcome trust and other funders held in bermuda. the policywas an agreement to dump data quickly into public sequence databases. as such, the high throughput sequencingcenters that were funded as part of big human genome projects agreed to provide their data very quickly, usuallyon a 24hour basis, a remarkable forcing out of information into the public domain. that is the open extreme ofpublicdomain policy making, defining the opposite pole from hgs.however, within the academic sector, many intermediate points on the continuum are represented. manyuniversity laboratories have òreasonable delaysó for publication of some research. there are thousands andthousands of labs that do sequencing. most of them do not behave according to the bermuda principles. theyrelease data in òpublishable unitsó after a gene has been fully sequenced and partially characterized and once thesequence data are verified. researchers in academic genetics, that is, who identify themselves as geneticists, reportwithholding data to honor an agreement with a commercial sponsor or to protect the commercial value of the data.however, i will point out that the big reason for nonsharing or withholding of data is the effort required. so inmany university molecular biology laboratories, data and materials may be unpublished for protracted periods.yet in private industry, there may be rapid open disclosure of data. here is the topsyturvy part of the humangenome project. pharmaceuticals are very patent dependent; it is famous in business schools for being an informationintensive, secrecyintensive business. however, in 1994, merck, a private firm, funded the human expressed sequencethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 291tag and cdna sequencing effort at washington university, the results of which were added to the public domain. merckdid not get to see the data before anybody else. why did a private firm pay for information to be generated and dumpedinto the public domain? i believe it is because of what incyte, hgs, and other genomics startups were doing. merckpursued one scientific strategy to counter the appropriation of dna sequence data by funding research to put data intothe public domain to defeat the strategies being pursued by some startup genomics firms.i have already talked about the bermuda principles. they were adopted by the major genome sequencingcenters. the snp consortium is another interesting model. a òsnpó is a single nucleotide polymorphism.there is a difference in the a, g, t, or c at one place in the genome, which differs among individuals. if you canfind a sequence difference, it is a snp; polymorphism merely means òdifferenceó to geneticists. snps areuseful as markers on the chromosomes of humans or any other organism. snps are particularly valuable forlooking for genes of unknown function and location, or for studying the whole genome at once. some companies were established to find and patent snps. as a result, some academic institutions and 13 private firmsformed a consortium to make sure that this stays in the public domain. however, the snp consortium did notjust dump the data. they filed patent applications and then characterized the snp markers enough so that theycould be sure that nobody else could patent them.3 at that point, they would abandon the patent. it is a verysophisticated intellectual property strategy that in the end was intended to bolster the public domain. it requirescoordination, lots of paperwork, and it costs money to file and process applications, but it appears to be aneffective defensive patenting strategy.the publicly funded mammalian gene collection and its parallel program, the cancer genome anatomyprogram, pursue policies to promote rapid data disclosure into the public domain. under mammalian genecollection government contracts, groups do the same thing that incyte and hgs were doing, which was tosequence identified genesñthe juicy bits of the genome that are translated into proteins. in this case, nih had togo to the department of commerce to declare òexceptional circumstancesó under the bayhdole act. as acondition under those contracts, the government gets to keep all the patent rights. because the government is notfiling patent applications, it is basically a de facto nonpatenting strategy. that is the only case that i know of wherethe exceptional circumstance clause of the bayhdole act has been invoked, although i know others at nih arediscussing other possible uses.remember that when the genome project started out, it was supposed to be a publicdomain infrastructureproject so we could all do human genetics faster and at less cost. those of us who thought about the humangenome project in the early years were not thinking primarily of commercial potential, but look at what happenedto the funding streams in the year 2000. it looks like over $1.5 billion of public and nonprofit funding went intogenomics in the year 2000 (see figure 112). the aggregate r&d spending of ògenomicsó firms is in the3for more information, see chapter 28 of these proceedings, òthe single nucleotide polymorphism consortium,ó by michael morgan.1,6532,06190005001,0001,5002,0002,500governmentand nonprofitgenomics firmspharmaceuticalsand biotechnologydollars (million u.s.)figure 112private and nonprofit genomics funding, 2000. source: world survey of funding for genomics researchstanford in washington program at http://www.stanford.edu/class/siw198q/websites/genomics/.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.92the role of s&t data and information in the public domainneighborhood of $2 billion (genomics firms devote some substantial part of their r&d to genomics). majorpharmaceutical firms spent another $800 million to $1 billion on genomics in 2000, based on an estimate of 3 to5 percent of r&d of members of the pharmaceutical research and manufacturersõ association. that does notinclude the privately held genomics firms that tend to be smaller but are much more numerous. these figures arequite uncertain, but if they can be used as a rough indicator, about a third of genomics funding comes fromgovernment and nonprofit organizations, and twothirds of the funding, at least in 2000, was spent in private r&d.figure 113 shows government and nonprofit genomics research funding for 2000 by country. the unitedstates, not surprisingly, is number one; the genome project in many ways originated here, even if the science didnot. if you normalize for gross domestic product, you see that many countries are investing more in genomics asa fraction of their r&d and as a fraction of their economy than the united states. estonia, the united kingdom,sweden, the netherlands, japan, and germany are all higher than the united states. this happened very fast. in1994 there were eight publicly traded genomics companies; in 2000 there were 73 publicly traded firms. in 2000,commercial genomics grew to about $94 billion or $96 billion capitalization (before declining precipitously in2001 and 2002).what was going on under the surface? let me return to this privatepublic mutualism. the main target thateverybody was aiming at was that $200 billion market for therapeutic pharmaceuticals, expected to grow muchlarger in future years. genomics was thought of as a way to develop those products faster at the front end of thediscovery process. pharmaceutical firms were interested but they came late to the game.how do i know that? well, let us look at the patent holdings. figure 114 is only a thousand of the dnabasedpatents that were issued from 1980 to 1999. initially, we read every patent and coded them to be able to get thesedata, and then we went back to figure out how many patents were owned by whom through 1993. we then used thesame patent search algorithm to identify dnabased u.s. patents and augmented the database through 1999, thebasis for the bar charts on which institutions own dnabased u.s. patents (see figure 114). the u.s. governmentis the number one patent holder followed by the university of california. incyte, a genomics company, wasnumber three. chiron, which is a firstgeneration biotech firm, was number four, and most of their patents actually0100200300400500600700united statesjapanunited kingdomgermanycanadaeuropean commissionnetherlandsfranceswedensnp consortiumchinarussiakoreaestoniabelgiumaustraliadollars (millions, u.s.)figure 113government and nonprofit genomics research funding, 2000.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 293came from a firm called cetus, which they acquired. you have to get to number five before you get to a companythat is in the business of creating the end products that everybody is aiming at: glaxosmithkline. this representsfive different kinds of players that own the intellectual property here.there were no dnabased patents in 1970. there were some dna and rnabased patents issued before1980, but not many. a dna sequencing method was patented in 1973; i do not think anybody uses it, but it ispatented and it is in the dna patent database. starting in the mid1990s, there was the beginning of an exponential rise. it kept that way through 1996 or 1997. the growth dipped in 1999, which may be a policy dip, because thepatent office that year began to change the rules for dna sequence patents. they raised the threshold basically,changing to a higher utility standard for examining dna sequence patentsñrequiring applicants to show aòcredible, substantial, and specificó utility in the patent. the u.s. patent and trademark office also demanded ahigher standard of written description of the invention in genebased patents. the growth increases again in 2001and it looks like we will be below the 2001 number for 2002, although there are a few months of patents we havenot examined. the years of exponential growth may be over, but the bottomline message here is there are 25,000pieces of intellectual property that have already been created.a pattern of ownership can also be observed. of those patents that were issued between 1980 and 1993 that weread through and coded, companies only own about half of these patents. if you add up the others, which aremainly academic research institutions, almost half of the patents are owned by òpublicó organizations such asuniversities, nonprofit research centers, and government. that is a very unusual ownership pattern. overall,academic institutions own only 3 percent of u.s. patents. figure 115 illustrates ownership of 1,078 dnabasedpatents from 1980 through 1993.what is going on here? molecular genetics appears to be a field in which privatesector actors step in to bolsterthe public domain. the public domain or the public funders create intellectual property subject to the bayhdole050100150200250300350400450500u.s. govt.univ. of californiaincytechironglaxosmithklinegenentechamgenaventisnovoamerican home productslillyhopkinsharvardhoffmannla rochemerckmitnovartisstanfordhuman genome sciencesnumber of patentsfigure 114number of patents in the dna patent database, 19801999. note: data through the end of december 1999.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.94the role of s&t data and information in the public domainact, which gets protected by intellectual property rights. private firms draw on public science and also pursueprivate r&d, which produces more intellectual property owned by private firms.a couple of generalizations do flow from that. one interpretation is that, the more money you make, the moreyou are going to plow into r&d. that is the argument that is made by the pharmaceutical and the biotech firms,and it is probably true. if firms make more money, they are going to spend more on r&d, particularly if r&d isthe way they believe they made their initial profit. as a result, there will be more innovation, there will be more ofthe products and services that we like, and, of course, we are going to have to pay more for them. we have beendoing that for two decades. one thing to consider is whether doubledigit rates of growth are sustainable. we maybe beginning to encounter resistance to growth in drug and device expenditures in the early part of the twentyfirstcentury. it is very clear that at least for these genomic startups, private firms believed that their patent portfoliomattered. their intellectual property mattered and that partially drove this high level of private investment ingenomics and in biotechnology more generally.one of the really interesting things about genomics as a case study is that, in fact, we have a scenario that feelslike a race between the public sector and the private sector. i am not sure competition is the right word because youhave one group of people that are dumping data into the public domain and another group that are developing datato make a profit. but they are doing the same things in their labs. in the private sector, they have done it in a verycapitalintensive way that tends to be fairly centralized.public and private sectors are pursuing similar lines of research, and we have a natural experiment that hasbeen going on now for almost a decade, the outcome of which we do not really know. we can say that the academicsector has been a more important part of this story than it has been in most other technologies, such as informaticsand computing, although universities were important there too. but it is not possible to compare whether morepublic good will come from the hgs and incyteõs private sequencing, celeraõs quasipublic sequencing, or thestrong publicdomain policies under the human genome project and mammalian gene collection. we may neverknow, as products, services, and discoveries are apt to draw from many streams.the big fight over the publication in february of the 2001 sequence data is how much is going to be put intothe public domain where everybody can use it without restriction, how much of it will be publicly accessible withsome restrictions, and how much of it is kept behind closed doors. that is not a resolved debate, but it is very rich.in 15 years we may be able to make more educated guesses about what our policy should have been over the pastfour or five years.we have races for money, but we also have races for credit, and they are inextricably intertwined. it is veryclear that part of celeraõs business strategy was to be well known and famous, as well as to sell databasesubscriptions. therefore, is not just about money and it is not just about credit, it is about both. that is true on bothsides of the academic and industrial divide.nonprofitresearch institutes13%privateuniversities14%publicuniversities9%forprofitcompanies52%u.s. government6%other6%figure 115ownership of dna patents, 19801999.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2959512legal pressures in intellectual property lawjustin hughes1.i wondered whether this audience should be addressed as a political constituency or as a science community.i think that is important to think about because it is a dilemma implicit in this symposium and a lot of the scientificcommunityõs study of intellectual property (ip) issues: do you look at intellectual property policy as a subject ofstudy or do you address ip policy as a battleground in which the science community will, whether you like it or not,either be warriors or cannon fodder.the proof of a major change in policy should either be a good empirical case or a very good theoretical case.the reason i adopt that position is because whether you are a law professor, who is supposed to be committed tothe truth, or you are a scientist, who is supposed to be committed to the truth, it is a position that you can take intothe activist battlefield and you can do it with principles. you can discuss these policy issues and be a policy activiston what the law will be, my position is that you should not do a major change in ip law, unless you can show mea good empirical case to make the change or a very good theoretical case.i wrote a paper entitled òpolitical economies of harmonization, database protection and information patentsó1 for a conference this summer in paris, cosponsored by the university of maryland, and professor briankahin was one of the organizers. i talk about that position in relationship to the fight we have been having in theunited states for years over extra copyright protection of databases and also in relationship to software patents.the name of that paper really should be òeverything you wanted to know about database politics, but youdidnõt know who to ask who would be stupid enough to be honest with you.ó i have often wondered if ishould edit a lot of the things i report on. i wrote this paper not only at the behest of professor kahin, who was agreat influence, but also professor reichman, who does a great job about writing about database politics anddatabase issues, but he always seems to write something like òthere was intense lobbying.ó i decided i would saya little more than there was intense lobbying.i wanted to briefly talk about three ip areas and, with each of these ip areas, i want to return to two themes.the first theme, which we have heard in this symposium, is it is not just about what the law is, it is about thetools of law and how they are used, misused, or not used by private players. this is very important in thescientific communityõs assessment of what kind of ip laws are supportable or acceptable to the community.second, i wanted to talk about scienceõs interest versus broader publicdomain arguments. we were invited to1see j. hughes. 2002. òpolitical economics and harmonization: database protection and information patents,ó conference on frontiers ofownership in the digital economy, june 1011, paris. see http://cip.umd.edu/hughesifri.doc.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.96the role of s&t data and information in the public domainsay controversial things. on that topic, i am going to go against jamie boyleõs support of what i call thetogetherness principles.let me talk about the three ip areas: patents, copyrights, and database protection. in the area of patents,probably unquestionably in the past few years, the most cataclysmic development has been the policy followingstate street bank of allowing patents over business methods. the business method patent issue is not somethingthe scientific community should dwell on at great length because it is not likely to affect it a great deal. businessmethod patents are probably not a place where the scientific community could have a great deal of influence inturning the tide. ibm, which is consistently the number one patenting entity in the united states, is againstbusiness method patents. of course, the reason is because ibm has been patenting everything else, and they did notthink to patent business methods. they suddenly found themselves outflanked by dell computer, who forced themto take several patents on dell business methods.in the patent area, the important issues that the scientific community has rightfully focused on arepatenting of research tools and of express sequence tags. the scientific community needs to approach theseissues and ask whether there is a problem because there has been a lot of good legal scholarship that identifiesproblems in the area of research tools and express sequence tags, an anticommons problem of too muchpropertization, too much overlapping of property rights. recently, there was a study done by wes cohen ofcarnegie mellon and others that, admittedly with a small sample, concluded that there was not yet anybreakdown, any anticommons problem because of ip. there are undoubtedly some increased transactioncosts and there are clearly financial transfers going on, but there is not yet the kind of breakdown thatprofessor reichman talked about. why not?one reason is there is not much enforcement, which is very important because law professors tend to focus onwhat the law is; whereas as activists you need to be concerned about how the law is enforced. it appears to be thecase that the holders of research tool patents to date do not seem to go after universities or nonprofit research.another issue is to what degree this patenting of research tools has benefited the university community. when youlook at the statistics, it is interesting because when you take the u.s. patent pool as a whole, fairly consistently yearin and year out, universitiesñboth private and public institutionsñcommand about 2 percent of the patents.however, in certain biomedical categories of patenting, the university numbers are much higher. in the mid1970sin three of those categories, universities had about 8 percent of the patents. in those same categories by the mid1990s, universities had 25 percent of the patents. so, if patent fees and licensing fees are being paid, an increasingpercentage of them are being paid to universities. that does not necessarily mean more money for research. wehave to be honest about that. that could just be transaction costs and that could just mean that universities arehiring more technical management officers and more patent lawyers.there is a real issue of studying and looking at what is happening in these areas. i said that the standardthat you have to take into the battlefield, if you want to do this in a principled way, is there should be nomajor change in the law until you can show a good empirical or theoretical case. i think that is true for majorchanges in the law. for smaller changes in the law, it is all right to advocate reform on the basis of yourintuitions and on the basis of what you believe is the right way to tinker with the system. as james boyle saidearlier, we are experimenting with a massively important entity, operation, and aspect of our society. if youpropose a major change, you might sink the whole ship, but if you propose tinkering and small amendments,you have a different way of organizing opposition procedures of the u.s. patent and trademark office (pto)or you have a slightly different way of organizing exceptions for encryption or reverse engineering. that issomething that i think you should go out and advocate. on that count, i think it was very good that peopleforced the issue with the pto about patenting of express sequence tags, because that forced the pto toreevaluate the issue and in a sense back down. as a result, there is substantially less patenting and stricterpatenting in this area of biomedical technology.let me turn to copyright issues. here science has to decide, as a community, its place in the broaderbattlefield. i think that the togetherness principle might be the right strategy, but it might also be the wrongstrategy. paul uhlir and jerry reichman presented about the possibility of an express research commons, separatefrom our general understanding of the public domain.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 297let me spin out a few things here. professor boyle observed earlier that younger scientists may be accustomedto paying for data, just as they pay for reagents.2 that strikes me as right. doesnõt that seem ironic to you? ayounger generation of scientists is now accustomed to paying for data, just as a younger generation of collegestudents is unaccustomed to paying for music.professor boyle also raised an interesting question that i want to try to answer. he said we do not knowwhether fair uses are part of the public domain. i think that the principled answer, and the answer that the scientificcommunity needs to take up, is that some fair uses are part of the public domain. the fair uses that are part of thepublic domain are those that are needed for a robust, democratic, civil society, and there are elements of the berneconvention, which is the multilateral treaty that establishes legal norms for a copyright, that suggest that. fair usesthat are part of the public domain are also those uses that directly advance progress in science and the useful arts.note that i have torn the public domain apart. there are many uses that are not part of the public domain. makingthe fifth copy of the madonna music track is not necessarily part of the public domain, and it is certainly not partof the public domain that the scientific community needs to care about.the parts of fair uses that are critical to the public domain are those that are transformative usesñreuses ofinformation, ideas, and expressions that advance the civic or scientific dialogue. i think that most intellectualproperty scholars are not troubled by traditional ip rights. they are troubled by the digital locks that are beingdeployed by private players and the laws that make it illegal to tamper with the digital locks.professor cohen is going to talk later about digital rights management technologies.3 the one place where thescientific community has come together against these digital rights management technologies and the digitalmillennium copyright act (dmca) is the issue of encryption technology. the argument that many computerscientists have made is that the dmca frustrates their scientific advances by creating a dark and ominous shadowover much of what they want to do in computer security research and encryption research.i wanted to make some observations about that. the dmca has some exceptions written into it for securitytesting and for encryption research. i do not know if these exceptions are adequate, but neither does the computerscientist community and neither do the most shrill voices who are saying the dmca should be repealed. justbecause someone gets a cease and desist letter from an overactive lawyer or an overstaffed corporate legaldepartment does not mean that it is the proper interpretation of the dmcañthat they cannot engage in theparticular activity they are engaging in.brian kahin and i were involved at different points in drafting the dmca. when it came to encryptionresearch, there were some people at the table who we thought knew a great deal about it. they were the spooks. ifyou ever have to draft legislation with the national security agency and the central intelligence agency, goodluck. negotiating with them was kind of like a hot or colder game. i would give them wording and they would saycolder, colder. i would give them different wording and they would say warmer, warmer. but this is how theencryption exceptions received a certain amount of scrutiny when they were drafted by people who presumablyknow what legitimate encryption researchers need. now there have been some bad stories, such as the attempt bythe recording industry to go after professor felten. that is one of those stories that i call òanecdataóñthese horrorstories over which we try to construct theories about how something is or is not working in ip law and policy.the problem of studying the effects of the dmca on computer science is different because it has become sopoliticized. i am not sure how you do empirical research, how you find out from computer scientists whether thedmca has impacted their activities. we recently held a convocation of ip professors, and one of them wanted todo empirical research. his first question was to ask the computer scientists how the dmca has affected their lives.given the politics, the polemics, and the rhetoric, i can tell you what that kind of survey is going to get you. whatis needed is a very careful survey that, without mentioning ip, tries to determine if there has been a shift in researchactivities in the encryption community, a shift in the normal activity and the functioning of the computer security2see chapter 2 of these proceedings, òthe genius of intellectual property and the need for the public domain,ó by james boyle andjennifer jenkins.3see chapter 15 of these proceedings, òthe challenge of digital rights management technologies,ó by julie cohen.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.98the role of s&t data and information in the public domaincommunity in response to fear about the dmca. that is a situation where, as scientists or as researchers, you needto keep on your hat of neutrality and not engage in the rhetoric that the dmca is bad.let me talk a little bit about database protection, which is a serious issue. paul uhlir, jerry reichman, briankahin, and i, along with many others in this room, have worked on it for years. i was surprised to hear professorreichman say that university technology managers are imposing conditions on databases built with federal funding. if that is true, we have a problem. however, the problem is not ip law. we need to galvanize the administration so that the national science foundation, the national institutes of health, the department of energy, and allthose federal agencies that fund these databases go after their funding and say thou shalt not impose these kinds ofconditions on the data that i have paid for.let me talk about the european union database directive. the good news is that it is not doing anythingliterally. stephen maurer, bernt hugenholtz, and harlan onsrud have done some work looking at whether therehas been any growth in the commercial database industry in europe following the implementation of the databasedirective. it is not working to date.now, keep my words carefully in mind. i said to date it is not working. when you ask the europeancommission if the directive is serving as an incentive for the generation of commercial databases, they say òyes.ótheir evidence appears to be that they go around barnes and noble occasionally and look at database products orthey count the number of litigations that they can find. the litigation all involves databases that existed before thedirective came out. as such, it is not doing much on incentive structure. scariest of all, the european commissioncommissioned a survey on the effects of the database directive. they invited me to fill it out. two importantquestions are not on the survey; have you introduced any new database products since the promulgation of thedirective? was the directive instrumental in influencing your decision to introduce any new database products?as far as we know, the database directive is not doing what they thought it would do. having said that, keepin mind your neutral position. we should not expect it to have done anything yet. it has been promulgated only fora couple of years. we know that the business community is not the most attuned to how ip law works. all you haveto do is read rembrandts in the attic to understand how clueless they generally are about patent and copyright law,which has been around for centuries.if the european database directive is going to have an incentive effect, it will not do so in just a couple years.so, although i am pleased that there are no results yet, i do not want to say that the directive is a failure. all we cansay now is that the directive is not working. now, the other piece of good news that i want to mention about thedirective is that none of the cases has involved anyone in the scientific community. all of the cases appear to becorporate entities in the european union slugging it out with each other. now, that is important. i am not sayingthis is not a great concern to science, but it is interesting to look at, and it also makes it more viable for science torequest an exception because clearly no one is concerned about what we have been doing.in the united states, as professor reichman said, there has been equipoise between political forces for severalyears, and we are not going to get any international movement until the united states is somewhere. bothprofessor reichman and i probably agree that we need to have some modest database protection law in the unitedstates to serve as a counterpoise to the european union. it is just that we do not have a convincing case,empirically or in theory, for anything nearly as strong as what they have done. that is really what the scientificcommunity ultimately should worry about.i have preached that you can go into battle with this positionñthere should not be big changes in the law,unless there is a great empirical case or a great theoretical case, but i want to add that it is fine to go into battle onamending the law with your intuitions. if you are thinking about introducing something into the system that mightkill off a lot of things, then you ought to hold back and take a principled position and insist that the other peoplewho want the change, whether the change is more ip or the change is less ip, ought to prove it.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2999913legal pressures on the public domain:licensing practicessusan r. poulter.imagine, as suggested in a recent article in the national law journal,1 that you enter a bookstore (or a library)and you find some books, unlike the ones you typically find, that have locks or snaps on the cover. there is a noticeon the cover that states, òas a condition of opening (or purchasing) this book, you agree to the terms containedinside.ó since your curiosity is engaged, you open the book, only to find additional terms and conditions that state,among other things,you agree that you will not copy any portion of this work or disseminate any information contained herein withoutthe express written permission of the publisher and that you will not publicly criticize this work or the authorsthereof. you further agree that in the event you make any commercial use of this work or develop any commercialenterprise or product from the use of this work, you will negotiate an agreement for such use with the publisher,terms of which may include but are not limited to a reasonable royalty.òridiculous,ó you say? in the paper world this would be a startling development. in the world of electronicpublishing and databases, however, such terms can already be found.licensing practices are indeed a significant pressure on the public domain; they have the potential to becomethe vehicle for restricting access to and use of public information that trumps other existing and proposed forms ofprotection, such as copyright and database protection.the term òlicensingó in the context of electronic publishing means contracting for various activities involvingthe access and use of articles or the information in the articles. it includes licensing an article for publication(which can also occur through the conveyance of copyright, as has been typical in scientific publishing) andlicensing the subscriber to access and read (and print) a copy. by way of comparison, paper copies of journals,including the accompanying scientific data, are typically sold outright; the electronic forms of journals moretypically are licensed, however.there are pressures on the public domain at the level of access to articles and the underlying data and on theuses that can be made of scientific and technical information once accessed. access to articles (and the data theycontain) requires subscription, and few would question the need for a mechanism to recoup the costs of electronicpublishing. the primary issues are reasonableness of the subscription price and the scope and duration of thelicense. a more significant issue, in my view, is whether the subscriber (user) has the right to use the data; that is,1mcmanis, charles r. òdo not support ôprivatizingõ of copyright law,ó the national law journal 24 (oct. 13, 1997) (online archive).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.100the role of s&t data and information in the public domainthe right to extract, reanalyze, repeat, build upon, and extend the work. these rights typically have been availableto any reader of paper publications, subject only to the limits of patent law. as other speakers have noted, however,the online environment facilitates technologically enforceable restrictions on access and acquiescence to contractterms that could restrict further use of the data. moreover, these terms usually will not be negotiated in the usualsense of the word because of the unequal bargaining power of subscriber and publisher.will licensing provisions that restrict further use of data be upheld? will such contracts become commonplace? for the remainder of this discussion, i will talk about the applicable legal principles and then will highlightsome developments in scientific publishing that may indicate which way the wind is blowing.the legal environmentthe primary legal principle applicable to licensing provisions, or at least the starting point for any analysis, isòfreedom of contract.ó freedom of contract is not unlimited, however, and traditionally has been subject tolimitations imposed by various other legal doctrines and policies, such as preemption by federal law (includingcopyright and patent law), competition policy, the doctrine of unconscionability in contracts, and perhaps evenfirst amendment principles. lemley, reichman, and others have written extensively about these possibilities inlimiting objectionable terms in electronic licenses.2with regard to restrictions on the use of scientific data, preemption under copyright law seems the mostpromising because the u.s. supreme court held in feist publishing v. rural telephone that copyright does notprotect facts, and protects compilations of facts only to the extent that the selection and arrangement are original.3thus, copyright does not prohibit the extraction of isolated facts from a compilation, even significant portions ofthem, so long as the selection and arrangement are not copied (assuming it is original enough to be protected in thefirst place). the supreme court has not had occasion to decide, however, whether feistõs declaration that facts arefree for all to use can be trumped by contract. that question has been addressed in a number of lower courtdecisions considering òshrinkwrapó and òclickwrapó licenses that typically accompany commercial software andother digital or electronic products.for a time, most courts held, on contract formation grounds, that the shrinkwrap licenses were invalid and thattheir terms did not bind the purchaser. because the consumer was not aware of the terms of the license beforemaking the contract (i.e., agreeing to purchase for the stated price), the license was not part of the contract betweenbuyer and seller. (this problem need not be an obstacle in the online publishing environment, in any event,because the subscription to an online journal or database can be conditioned on acceptance of terms made knownto the subscriber at the time of the subscription, before the contract is made.)the trend toward invalidating shrinkwrap licenses began to wane in later cases, however. in procd v.zeidenberg, the seventh circuit upheld a shrinkwrap license that accompanied a cdrom containing a nationwide telephone directory, overcoming the contract formation issue.4 more significantly for our purposes, judgeeasterbrook held that the licenseõs prohibition on further copying and use of the data on the cdrom wasenforceable, rejecting the argument that the copyright act preempted terms that were in conflict with the policy ofcopyright as expressed in feist.the procd court appeared to be concerned about the impact of the defendantõs complete copying of thedatabase on the plaintiffõs investment of about $10 million and on the incentives for others to invest in electronicdatabases of public information. the problem with the decision, however, is that the courtõs reasoning suggeststhat other kinds of restrictions, such as a prohibition of extraction of any part of the database or reachthroughprovisions attempting to capture royalties on downstream uses and developments, would be upheld. the supreme2see, e.g., lemley, mark a. 1999. beyond preemption: the law and policy of intellectual property licensing, 87 cal. l. rev. 111; andreichman, j. h. and jonathan a. franklin. 1999. òprivately legislated intellectual property rights: reconciling freedom of contract withpublic good uses of information,ó 147 u. penn. law rev. 875.3see feist publications, inc. v. rural telephone service co., 499 u.s. 340 (1991).4see procd v. zeidenberg, 86 f.3d. 1447 (7th cir. 1996).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2101court has not addressed the preemption issue in the context of contractual limitations on copying and use, but inthe meantime, the lower courts are tending to uphold online contracts and find that preemption does not apply.it is unclear whether there is room for a more nuanced preemption analysis of license terms limiting furtheruse of electronic information. the procd court applied the express preemption of section 301 of the copyrightact. this might be considered a narrow preemption analysis, because section 301 requires a close comparison ofthe right asserted and the subject matter protected by the contract, with the subject matter and rights of copyrightlaw. another court might consider òsupremacy clauseó preemption, which depends on whether the state right inquestion interferes with the policy of the copyright law. under this kind of preemption, the court might concludethat, because a strong policy underlying copyright law is that facts should be freely available for all to use,contracts cannot restrict their use once they have been made public (or available to the public). this kind ofpreemption analysis might also allow a database creator to prevent wholesale copying of a database but prohibit arestriction that would prevent extraction and use of more limited amounts of data.several proposed changes to the law will also impact licensing rights, generally strengthening the hand ofcontent providers. database protection has been considered in congress since the mid1990s, at least partly inresponse to the european commission database directive. a database protection statute will likely strengthencontract claims because a federal law would expressly sanction protection of databases. moreover, databaseprotection is not the only legislative initiative that will strengthen the hand of electronic database providers andpublishers. in 1999 the national conference of commissioners on uniform state laws proposed a new uniformlaw, intended for consideration by state legislatures, that would generally validate shrinkwrap or clickwraplicenses. the uniform computer information transactions act (ucita), although instigated by the softwareindustry, is not limited to software licenses, but would cover online publishing.ucita contains a number of provisions unfriendly to consumers, but for our purposes perhaps the issue ofmost concern is that the law appears to endorse the concept that, even where information is marketed to a largegroup or to the general public (i.e., a òmassmarketó license), the publisher can restrict the further disseminationand use of information provided pursuant to the license, whether or not that information qualifies for copyrightprotection. in other words, the commentary prepared by the drafters endorses the kind of approach endorsed inprocd.at present, ucita has been adopted in only two states, virginia and maryland, and three other states haveenacted legislation to prohibit the enforcement of ucita against their citizens. ucita also has been revised, butnot to address the problem identified above. renewed efforts to enact ucita in other states seem likely.electronic publications and databases in sciencethe remainder of this discussion will provide a sampling of practices and events in scientific publishing thatare essentially anecdotal, although in some instances they are very significant anecdotes. i also will mention somecountervailing developments, even trends, although i will leave most of that to the speakers later in the program.at the level of access, high prices (particularly for commercial journals) are a widespread concern. but morebroadly, a typical oneyear subscription permits online access only during the year of the subscription. a subscription may provide access to several years of back issues; beyond that period, back issues usually are chargedseparately. clearly, making back issues available online is a costly activityñthe issues here are primarily ones ofhow electronic publishing products are packaged. it is worth noting, however, that the limited period of accessmeans that even a longterm subscriber has no access once her subscription ends, rather than being able to retainaccess to those issues to which she subscribed.a number of practices also limit the usefulness of online publications, especially for the typical researchertoday who does most literature research online. in one instance, a leading scientific journal required an individualsubscription for fulltext access online, even where the institution had an institutional subscription. (this practicehas apparently been discontinued, however, by the journal in question.) commonly, linking to references within anonline article requires subscription to the referenced journal as well. another worrisome trend is that proprietarydatabases are the only source of some information developed with public funds (see the commercial space act of1998).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.102the role of s&t data and information in the public domainat the level of restrictions on further use of data, in other fields, online database providers assert the right torestrict the use of data extracted from their databases, even when that information is available elsewhere and is notseparately entitled to any form of intellectual property protection. an analogous area involves the use of materialtransfer agreements (mtas) for the sharing of unpatented specimens or samples. mtas commonly prohibitcommercial use of the materials, or include òreachthroughó provisions, requiring negotiation of licenses fordownstream commercial uses.traditionally, professional society journals have not imposed limits on use of data. recently, however,science magazine contracted with celera genomics for the publication of the venter et al. paper on the sequencingof the human genome, without making the sequence data freely available to all subscribers. contrary to the usualpractice, celera (rather than science) maintains the database of sequence data supporting the article, and readersmust access the data through the celera web site. access for academic, noncommercial users is free, and suchusers can download one megabase per week. they may not commercialize the data, but there are no reachthroughprovisions on developments from searching the database. access for commercial users requires the execution of amta, again prohibiting commercial use, or negotiation of a separate agreement with celera. despite the expectation that celera data would make their way into genbank, the free public repository of genetic sequence data, anarticle that appeared as recently as august 2002 indicated that no celera data had been deposited.the justification for scienceõs arrangement with celera is that it made important data available that otherwisewould have been accessible only through commercial arrangements, if at all. the arrangement is precedent setting,however, and is contrary to scientific norms in most fields in that it permits the authors to òhave their cake and eatit tooóñgarnering scientific recognition while retaining the ability to exploit the underlying data by limitingaccess.the influence of the sciencecelera deal remains to be seen. it does, however, provide a precedent for ascientific publication to require less than full disclosure of the underlying data. it further is a precedent foragreements prohibiting commercial use of data, perhaps requiring reachthrough provisions, and for restrictingextraction and further publication of data. how will journals evaluate other cases where authors want similararrangements, perhaps even academic authors who want recognition while maintaining a head start much like thesituation with xray crystallographers in past years that engendered a great deal of criticism? will journal editorsbe able to determine when the benefits of such an arrangement outweigh the negatives by making available datathat otherwise would not be available? or will such arrangements allow restrictions on data that otherwise wouldbe published without restriction? ultimately, will restrictions and reachthrough provisions delay and inhibitworthwhile downstream uses of science and technological information?members of scientific professional organizations can be expected generally to be opposed to proprietary rightsin data or publication with less than full disclosure, although some, especially those who perceive commercialvalue to the data they generate, may seek to make such arrangements themselves. the structure of scientificpublishing may also facilitate such arrangements. many journals now use supplemental, online repositories fordetailed experimental procedures and data, which could easily be subjected to licenses that include use restrictions.these supplemental data repositories are not always published in the paper edition of the journal, although manyjournals now make them available without an online subscription. notwithstanding their current availability,online restrictions could completely control access to and use of data.not all developments, however, point in the direction of greater restrictions. authors of some journals haveretained the right to use their scientific papers for their own research and educational purposes, including, in somecases, the right to post their articles online. these movements have the potential to facilitate new kinds of highlyspecialized, interlinked information products. a recent report from the american association for the advancement of science (aaas) supports these efforts.5 but authorsponsored movements will likely have limited utilityif the goal is widespread access. a major function of journals is to vet scientific publications for quality; unlessthese òentrepreneurialó databases establish similar vetting systems, they are likely to be incomplete.5see american association for the advancement of science (aaas), 2002. seizing the moment: scientistsõ authorship rights in thedigital age. aaas, washington, d.c.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2103scientific authors have also tried to influence the other side of the equation through various efforts to persuadejournals to allow free access online after a period of subscriptiononly access, say 6 months to a year. thismovement has precedentñthe american astronomical society has all journals online, with free access after 3years. other initiatives include the public library of science, pubmed central, and the budapest open archivesinitiative. a movement by life scientists to boycott journals unwilling to agree to this concept apparently hasfizzled, but the movement to unrestricted access seems to be gaining momentum, with the decision of science (theleading publication of the aaas) to allow unrestricted access after 1 year.in summary, digital lockup is a realistic possibility. technological and legal tools support the kinds ofrestrictions that can deplete the public domain. science today generates many large data sets (e.g., gene sequences)that can be managed only in digital form. moreover, the legal framework does not currently set limits on licensingprovisions needed to preserve public domain. member interests may restrain professional society publications anddatabases from imposing more onerous restrictions, although it is unlikely that there will be unanimity on thisissue. directions are even less clear for proprietary and industrysponsored databases.trends are not uniformly pointing toward greater restrictions, however. authorsõ rights movements and themovement toward free online access are welcome developments. the real question is how the system of technological tools and legal rules can be adjusted to strike the right balanceñmaximizing the data and information in thepublic domain to òfurther the progress of science and the useful arts.óthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.104the role of s&t data and information in the public domain10414legal pressures in national security restrictionsdavid heyman.i will address the new national security restrictions and the pressures that they may impose on the publicdomain. the key questions to address today are what information can we create and share with the world and whatinformation cannot be openly shared, in light of the increased threat environment? i want to start by reading to youfrom a government directive that attempts to answer this question in part. it was issued in december 2001 andprovides regulations on publications. it says that no publication is allowed to have the following, contents that:òinsult or slander other peopleó; òpublicize pornography, gambling and violence, or instigate crimesó; òleak statesecrets, endanger national security, or damage national interestsó; òendanger social ethics and outstanding nationalcultural traditionsó; òdisrupt public order and undermine social stabilityó; or òpublicize cults and superstitions.ó1this document is from the peopleõs republic of chinaõs òregulations on management of publications,óissued december 31, 2001. i bring it to our attention because clearly the question as to what information can andshould reasonably be controlled is really open for interpretationñsome of the regulations in china seem quitereasonable, some are questionable, and some are beyond the pale. what is important here is that there is a line,which at some point we cross, as you probably did while listening to me. it is a line that contemplates legitimaterestrictions on traditional publicdomain information. i believe if our policies go to one extreme of that line, werisk national and economic security, and if our policies go to the other extreme, we risk freedom. our job today andin the future is to clarify that line and shine a light on it so that reasonable people can make reasonable policy.i am going to focus on federal investments in science and technology and the contributions these investmentsmake to creating and disseminating publicdomain information and, ultimately, to improving the security andwellbeing of our nation.let us first look at the new pressures that are being put on the public domain as a result of the attacks onseptember 11. i think what was so shocking about the terrorist attacks was the realization that the terrorists livedamong us and used our open society against us. our experience turned out to be a bit of a perversion of pogoõs famousquote, òwe have met the enemy and it is us.ó we felt victimized after september 11 in part because the terroristsexploited the very aspects of american society that make our country strong: its openness, easy access to information,freedom of association, ease of mobility, and right of privacy. the terrorists lived among us and used our freedoms1 see òprc regulations on management of publications,ó issued on december 31, 2001. translated from the foreign broadcast information service and available online at http://www.fas.org/irp/news/2002/05/xin123101.html.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2105against us. the recognition that we were vulnerable to this, and the fear that this vulnerability caused, generated awave of response by the government to limit those very attributes of an open society that were used against usñinparticular, access to publicdomain information. i will discuss some specific examples later in my remarks.as a result of this experience, we are now witnessing two key components of publicdomain informationbeing constricted due to national security pressures: first, the creation of publicdomain information, and second,its availability for others to use.let us start by looking at the creation of publicdomain information. science can provide us with the capability to acquire information about the nature of the physical world, as well as the technological alternatives that wedo not presently possess. this information, in the long run, is vital to the future of the u.s. economy, our nationaldefense, and general wellbeing. science can only do this through sustained investments over the long term andthrough the continuous development of a talented workforce to perform research and development (r&d). today,the united states is investing more in r&d than it ever has in the past, even with adjustments for inflation.nonetheless, the u.s. share of total world r&d is decreasing. more research capabilities are becoming availableoutside the united states. over the past 50 years, we have seen the united states go from performing more than 70percent of the worldõs total r&d in dollars spent to a point where today the rest of the world performs approximately twice as much r&d as the united states.the changing investment patterns are having an impact on where research results are being produced and wherethey are found. for example, since world war ii, u.s. scientists have led the world in authoring scientific publications, a measure of where scientific discoveries are being made. recently, as a result of the increased quality andvolume of scientific activity in many countries, more and more articles are submitted to journals from scientistsoutside of the united states. in the physical sciences, in the early 1980s, u.s. publications accounted for nearly 70percent of all articles. today, u.s. publications have fallen to approximately 25 percent of the worldõs total.the changes in the u.s. science and engineering workforce are more interesting, particularly in the context ofnational security. as i noted, there has been an increasing amount of research and technical resources availableoutside the united states and a growing internationalization of science and technology. as a result, u.s. scientistsand engineers comprise a diminishing share of the total global technical workforce. fewer u.s. scientists arepursuing physical sciences and other comparable hard sciences, and those that are pursuing them prefer academiaand industry to a government career. last, more foreignborn students are being trained as scientists and engineersin the united states, and approximately 50 percent remain in the united states to become part of the workforce. atthe same time, domestically, the increased investments in the private sector and the employment opportunities thatemerged as a result of the technology boom in the 1990s created a honey pot in the private sector and competedwith the government to fill the valuable r&d positions.to summarize, the creation of sciencebased publicdomain information, in a simplified world, is a product ofinvestment and effort. as more of the u.s. share in total world r&d diminishes, so likely does its share of thecreation of the worldõs total publicdomain information. as investments shift from civilian to military priorities, ascan be anticipated with a war on terrorism, we might expect to see some crowdingout of investments that lead topublicdomain information as well. and lastly, as the u.s. workforce becomes increasingly reliant on foreignscientists and engineers, the challenges for controlling transfer of technology will become greater, limitations oninteractions with foreign scientists may increase, and we will likely see more restrictions on access to publicdomain information. this is, in fact, what we are seeing.the availability of publicdomain information has been squeezed significantly more by recent national securitydevelopments. there is a multitude of ways in which publicdomain information is made available. they includepublications, web sites, conferences, and presentations, as well as through working collaborations. security professionals are not only concerned with what information is being published, but how it is transferred to the public and,in particular, how interactions among scientists and engineers serve as a mechanism for exchanging information.the postseptember 11/postanthrax attacks security environment has raised concerns regarding the possiblemalicious use of scientific and technical information and puts greater pressures on scientific institutions tostrengthen security to prevent the unintended transfer of technology to those who would harm us. we felt thispressure when we realized that the perpetrators of the september 11th attacks lived secretly in our neighborhoods,and operated freely within our open society. some of the september 11 terrorists entered the united states onthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.106the role of s&t data and information in the public domainstudent visas, but never matriculated to the school to which the visa applied; some received pilot training in theunited states. these examples raised concerns that terrorists or other enemies of the united states may seek togain entry to the united states under acceptable and rather innocent guises, but may, in reality, seek to enter thecountry to acquire knowledge, skills, or technologies to mount future attacks against the united states.at about the same time, we also saw the publication of three research papers that have generated significantalarm. first, a study published in the journal of virology described an experiment by australian scientists to reengineer a relative of smallpox, called mousepox, in a way that made the virus far more deadly.2 some have arguedthat if the same technique were applied successfully to smallpox, the consequences to society could be devastating.similarly, the proceedings of the national academy of sciences published a study by scientists at the university ofpennsylvania that provided details about how smallpox uses a protein to evade the human immune system.3 again,such information, critics suggest, could be quite harmful if misapplied. lastly, in july 2002, science magazinepublished a paper in which scientists at the state university of new york at stony brook described how to makepoliovirus from mail order dna.4 the publication of that study spurred rep. dave weldon, a florida republican,to introduce a resolution criticizing science for publishing òa blueprint that could conceivably enable terrorists toinexpensively create human pathogens for release on the people of the united states.ó5as a result of these and other developments, there are a number of growing efforts today to protect and limitaccess to scientific information, including efforts to restrict the activities of foreign nationals and the interactionsof u.s. nationals with foreign nationals.the bush administration, the u.s. congress, and some scientific communities have adopted or are considering implementing new security measures that could dramatically shrink the availability of publicdomain information. these include increased foreign student monitoring,6 restricted access to certain technical materials or tools,expanding export controls, tightening visa requirements,7 and limiting publications.8 security reforms also includeefforts to limit information historically provided to or already in the public domain;9 to expand the use of a2 see jackson, r.j., ramsay, a.j., christensen, c.d., beaton, s., hall, d.f., ramshaw, i.a. 2001. òexpression of mouse interleukin4 by arecombinant ectromelia virus suppresses cytolytic lymphocyte responses and overcomes genetic resistance to mousepox,ó journal of virology, 75(3):120510, feb.3 see rosengard, a.m., liu, y., nie, z., jimenez, r. 2002. òvariola virus immune evasion design: expression of a highly efficient inhibitorof human complement,ó proceedings of the national academy of sciences, (13):880813, june 25.4 see cello, j., paul, a.v., wimmer, e. 2002. òchemical synthesis of poliovirus cdna: generation of infectious virus in the absence ofnatural template,ó science, 297(5583):10168, aug. 9.5 see house resolution 514. 2002. 107th congress. introduced by dave weldon, july 26.6 the usa patriot act of 2001 (public law no: 10756), requires universities and òother approved educational institutions [including] anyair flight school, language training school, or vocational schooló to build and maintain a sizable database on its students and transmit thatinformation to the department of justice, the immigration and naturalization service (ins), and the office of homeland security. thedatabase system, called the student and exchange visitor information system, would automatically notify the ins of a studentõs failure toregister or when anything goes wrong in the studentõs stay. further, failure of a university to provide the information may result in thesuspension of its allowance to receive foreign students (the ability to issue i20s or visaeligibility forms).7 the patriot act also allows for the u.s. attorney general to detain immigrants, including legal permanent residents, for seven daysmerely on suspicion of being engaged in terrorism. the bill denies detained persons a trial or hearing, where the government would berequired to prove that the person is, in fact, engaged in terrorist activity. further, in september 2002, the ins implemented the initial phase ofthe national security entryexit system (nseers) at selected ports of entry. under the nseers program, certain individuals will beinterviewed, fingerprinted and photographed upon entry into the united states, and their fingerprints will be checked against a database ofknown criminals and terrorists. these individuals also must periodically confirm where they are living and what they are doing in the unitedstates, as well as confirm their departure from the united states.8 taking the first significant step toward selfregulation in this area, in february 2002 the publishers of some of the most prominent sciencejournals in the country issued a pledge to consider the restriction of certain scientific publications in the name of security. the statementoutlined the unique responsibility of authors and editors to protect the integrity of the scientific process, while acknowledging the possibilitythat òthe potential harm of publication [of certain research may outweigh] the potential societal benefits.ó9 in october 2001, attorney general ashcroft revised the federal governmentõs policy on releasing documents under the freedom ofinformation act, urging agencies to pay more heed to òinstitutional, commercial, and personal privacy interests.ó the administration wantsthe new department of homeland security exempted from many requirements of the freedom of information act. in march 2002, thepresidentõs chief of staff issued a memo to executive agencies requesting that they safeguard information that could reasonably be expectedto assist in the development or use of weapons of mass destruction, including information about current locations on nuclear materials. as aconsequence, federal agencies have removed a range of information from their websites and other public access points.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2107category of classification known as sensitive, unclassified information; to broaden the enforcement of a conceptcalled deemed exports, which is the oral transfer of technology between people; broaden classification authority inthe executive branch;10 and to impose new restrictions on fundamental research.a concern raised by all of these developments is the impact that they may have on the scientific communityand, consequently, on the scientific enterprise. additional security requirements may wittingly or unwittinglydiminish the amount of scientific and technical data available in the public domain. furthermore, requirementsmay slow the production of new knowledge or reduce the ability for some to publish or present their findingsbecause of new classification concerns, which, in turn, will diminish peer recognition, career advancement, and,ultimately, morale. at that point, students may choose not to matriculate to u.s. universities, and scientists maychoose to leave their government positions or reject government funding, rather than endure the environment inwhich they must operate. and who will replace them and who will do their work?this scenario is not fabricated, and it is not without precedent. in fact, it is exactly what we witnessed at thedepartment of energy (doe) a few years ago. between 1998 and 2000, the united states faced three nationalsecurity crises involving the potential loss of scientific and technical information. first, a highlevel congressionalinvestigation determined that china had stolen advanced missile technology from the united states, from u.s.corporations, as well as plans for the w88, one of the nationõs most sophisticated nuclear weapons. second, ascientist at one of the doeõs premier national security laboratories was accused of giving sensitive nuclearinformation to china. this is the wen ho lee case. last, less than a year after the first two issues surfaced, twocomputer harddrives containing classified nuclear weapons information disappeared from a doe laboratory forover a month. these incidents spurred dramatic reforms from both the legislative and the executive branches,including the institution of numerous new security measures at doe to protect scientific and technical informationand to prevent access of foreign nationals to the labs in certain circumstances.concerned about the consequences of these new reforms, then secretary of energy william richardsonestablished a highlevel commission led by former deputy secretary of defense john hamre to assess the newchallenges that doe faces in operating premier science institutions in the twentyfirst century, while protectingand enhancing national security. an analysis by the commission, which included former fbi director williamwebster, former deputy director robert bryant, and numerous nobel scientists, reveals that, although mostreforms were well intentioned, many security reforms were misguided or misapplied and only exacerbated existingtensions between scientists and the security community, contributing to a decline in morale and, in some instances,productivity. i recommend the report, which goes into much more detail on this topic.11in the end, the commission found òthat doeõs policies and practices risk undermining its security andcompromising its science and technology programs.ó relevant to todayõs discussion, the commission foundmanagement dysfunction that impairs doeõs ability to fulfill its missions. in other words, good policy could beundermined by poor management.in the area of information security, the commission found that the process for classifying information was, in fact,disciplined and explicit. however, the same could not be said for the category of sensitive, unclassified information, forwhich there is no usable definition at the department, no common understanding of how to control it, no meaningful wayto control it that is consistent with its various levels of sensitivity, and no agreement on what significance this categoryhas for u.s. national security. consequently, security professionals found it difficult to design clear standards forprotection, and scientists felt vulnerable to violating rules on categories that are ill defined. as a consequence, scientistsand engineers began opting out of conferences and in some cases opting out of publishing.we have to understand that heightened security is, in fact, appropriate and necessary after september 11. butwe should also be deliberate and learn from the doeõs experience, or, like the doe, we will risk undermining thevery security we seek and diminish the scientific programs vital to our national security and our economy.10 through executive orders issued in december 2001, and may and september 2002, the secretary of health and human services, theadministrator of the environmental protection agency, and the secretary of agriculture were respectively granted the authority to classifyinformation originally as òsecret.ó11 see center for strategic and international studies (csis). 2002. science and security in the 21st century: a report to the secretary ofenergy on the department of energy laboratories, csis, washington, d.c.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.108the role of s&t data and information in the public domainin conclusion, i would like to offer a few principles from the doe experience that may be instructive on thequestion of limitations for publicdomain information.the first principle focuses on security. any policies that we seek to derive or practices we wish to employregarding safeguarding scientific and technical information must be determined by collaboration between thescientific and technical community and the intelligence and security communities. scientists cannot be expected tobe aware of all the risks they face from hostile governments and agents. at the same time, security professionalscan only understand what is at stake by working with scientists. in fact, these two communities must depend oneach other to do their shared job successfully.second, we must know what we want to protect. what is secret? we cannot make judgments on the architecture of security without first understanding the nature and conduct of science and the scientific environment inwhich we operate. for example, it is crucial to understand that today classified work has come to be dependent onclassified science and technology, and unclassified science, in turn, has become more international and connectedby digital communications. there are consequences to this in terms of whom u.s. scientists and engineers seek tocollaborate with and whom they seek to employ. there are also costs if we choose to impose limitations on thismethodology of work.third, we must know what threats we face. we must understand that we exist in a changing world of dynamicthreats and national security interests. technological advances not only serve our social goals, but they also mayenable our adversaries not only to exploit our critical systems but also our key personnel. there are legitimatesecurity concerns that we must confront.fourth, we must understand that absolute protection is impossible. security is a balance of resources, whichare limited, and risks, which can never be eliminated. we will have to make choices that will mitigate the risks.that means that we must understand the value of what we seek to protect, the consequences of it being compromised, and the cost of protecting it.fifth, security processes should minimize disruptions to scientific activity. security procedures must strike abalance. they must be unobtrusive enough to permit scientific inquiry, but effective enough to maintain strongsecurity.sixth, we should control information where there are no other costeffective alternatives to ensuring nationalsecurity. i think this is similar to what justin hughes proposed earlier. finally, if information security is required,use understandable, meaningful, and workable classification systems to protect information. i think these principles represent hard decisions, which we must make to manage our growing information society and vitalscientific enterprise today.misguided or misapplied limitations on scientific activities motivated by security concerns pose a clear threatto science and society today. information, in the end, is the oxygen that feeds science, our economic system, andour democracy. consequently, we must be deliberate in defining the line between what should be in the publicdomain and what should be restricted.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 210910915the challenge of digital rightsmanagement technologiesjulie cohen.i am going to discuss the challenge of digital rights management (drm) technologies for the public domain.first i will address the technologies themselves, some of the functions that they can be used to implement, andsome of the implications of those functions. then, i will address the digital millennium copyright act (dmca),a law designed to provide drm technologies with an extra layer of legal protection, and discuss the implicationsof that extra layer of legal protection.i should preface my remarks by noting that i disagree with justin hughes about how bad the dmca is.1 icome from a family of scientists and i like data, but sometimes if you wait for a lot of evidence, you have waitedtoo long. one thing that we have seen quite clearly in this symposium is that the scientific enterprise is a verycomplex system that exhibits enormous path dependencies. that is in large part the message of the other speakerswho have argued that making major changes in a complex system that one does not fully understand can be fraughtwith peril. this can be true even for what could be characterized as òtinkeringó with the system if one does notunderstand all of the path dependencies. when i discuss drm technologies, the dmca, and their longtermimplications for scientific research and the public domain, i will lay out some of what i think are the worstcaseconsequences. not everything that i am about to describe is happening yet. many of the technologies that i willmention are in experimental use somewhere, usually in markets for video games or digital music files rather thanin markets for scientific databases. in my view, the best way to prevent the worst case consequences of drmtechnologies and the dmca is to make sure that everybody sees them coming.digital rights management technologiesdrm technologies include, first, technologies that can be used to impose direct functionality restrictions ondigital content. a simple example is encryption technology that restricts access to a database to those individualsor devices having the appropriate password or key, but drm technologies also can impose more complexrestrictions. for example, they can be designed to prevent users from taking particular actions with the data, or toregulate the manner in which they make take those actions. thus, drm technologies can prevent or limit the acts1see chapter 12 of these proceedings, òlegal pressures in intellectual property law,ó by justin hughes.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.110the role of s&t data and information in the public domainof copying, extracting some of the data, or transferring some of the data to a different document or to a differentdevice, such as another computer or a personal digital assistant.second, drm technologies can be used to effectuate òclickwrapó contractual restrictions. it is possible to usea combination of direct functionality restrictions and clickwrap contract restrictions to produce a fairly broadrange of regulation of the behavior of database users. clickwrap restrictions might be used to implement a payperuse scheme that allows metered access, and possibly some copying, for a fractional fee. alternatively, theymight be used to impose narrower restrictions, such as a prohibition against disclosure of the data to the public, oragainst use of the data for a commercial purpose, or against use of the data to reverse engineer a computer program.one recent case brought by the new york state attorney general involved a clickwrap contract restrictionprohibiting the publication of a critical review of a software package.2finally, drm technologies can be designed to effectuate what i will call selfhelp, such as disabling access tothe database or to some portion of the database if the system detects an attempt to engage in some sort ofimpermissible action, or detects unauthorized files residing on the userõs computer. for example, if a copyrightholder can detect unauthorized mp3 files somewhere on an individualõs system, it might use that fact to disableaccess to a lawful subscription service. a lot of myth and legend surround the potential capabilities of selfhelptechnologies, and i have not heard anything (yet) to suggest that these capabilities are being implemented in thescientific database realm, but certainly they are the subject of experimentation elsewhere in the market for drm.implications of drm technologies for scientific researchwhat are some implications of these technologies for access to and use of publicdomain information? first ofall, direct functionality restrictions will have some obvious implications for access to and use of unprotected,uncopyrightable information. authentication restrictions can inhibit initial access to the information, allowingaccess based on the userõs device or domain or on possession of a valid subscriber identification. in casesinvolving collaborative research, this can generate added transaction costs because researchers will have to makesure that everyone with whom they want to collaborate is coming from an authorized device, domain, or subscriberidentification. if the drm restrictions prevent excerpting or extraction of the data, this will hinder research effortsthat require extraction and manipulation. if the drm software or hardware is designed to require proprietary fileformats for any data that are extracted, these restrictions may cause other kinds of problems. papers intended forpublication may be subject to limits imposed by the demands of the drm system. direct functionality restrictionsalso raise the risk of loss of access to data, either because a subscription has expired or because the system hasinvoked selfhelp functionality to disable itself. loss of access in turn raises the possibility of damage to other filesor programs that may reside on the researcherõs system. payperuse provisions and other clickwrap restrictions also have some important longterm implications forresearch. first, the pricing of some of these subscriptions can represent a significant cost. in addition, the use ofclickwraps to restrict subsequent use and disclosure raises concerns about secrecy and freedom to publish.it is worth separately highlighting some of the potential effects of drm technologies on libraries.libraries will have the headache of managing all of the authentication restrictions. they also will need toworry about loss of access to back issues of journals and databases when subscriptions expire. that is nothow their print and microfiche collections have worked, and it is a fairly significant concern for obviousreasons. libraries also need to be concerned about loss of control over the formats for archival storage,search, and retrieval of data. search tools have to be able to interact with the file structure of the databasesthey are designed to search. if the file structures are proprietary for reasons related to the imposition of drm,then a search engine capable of interacting with that proprietary wrapper may also be considered a proprietary tool. this in turn raises questions about who will be permitted to develop those tools, and whatrestrictions will be placed on their use. all of these issues are critical to librariesõ mission of facilitatingaccess to information by their user communities.2people v. network associates, inc., no. 400590 (n.y. sup. ct. jan. 14, 2003).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2111the digital millennium copyright actone might respond to this òparade of horriblesó by noting that nothing can be built that cannot be hacked. sofar, that has been true, but congress and the content industries have fought back. in 1998, congress enacted thedmca, which has four main types of provisions.first, the dmca has an anticircumvention provision, which is relatively simple. it prohibits circumventionof a technological measure that effectively controls access to a protected work. note what this provision does notsay. it does not say: òthou shalt not circumvent a technological measure that effectively protects the right of thecopyright owner in a protected work, such as the right to copy the protected work.ó circumvention of such copycontrol measures is not prohibited. the statute prohibits only circumvention of accesscontrol measures. butconsider the different ways that a drm system can be designed to work. access to a database may be provided viaa clickwrap system that treats every act of opening up the database and using it as a separate act of access to thedatabase. if so, then arguably the anticircumvention provision applies to each instance of use, even by anauthorized user. so too if the drm measure requires the application of some sort of password or key every timeone wants to use the database. one cannot circumvent that password for any reason, because it is a technologicalmeasure that effectively controls access to a protected work.second, the dmca contains what i will call antidevice provisions. these provisions are somewhat morecomplex. they prohibit manufacturing, distributing, or trafficking in a technology that meets any one of threecriteria: (1) it is primarily designed or produced for circumvention of a technological protection measure; (2) it hasonly limited commerciallysignificant purpose or use other than to circumvent; or (3) it has been knowinglymarketed for use in circumvention. the antidevice provisions apply both to devices for circumventing accesscontrol measures and to devices for circumventing drm measures that effectively protect rights of the copyrightowner, such as the right to copy. think back to the wording of anticircumvention provision. one is not allowed tocircumvent to unlawfully gain access to a work. in theory at least, nothing prohibits circumvention to get arounda technology that protects a right of the copyright owner. but where are users going to get the tools that wouldallow them to undertake lawful acts of circumvention? the antidevice provisions exist to ensure that such toolscannot be offered on the market.technically, the antidevice provisions protect only drm measures that are applied to copyrighted works.some types of databases may not be covered by copyright. but database providers still may apply drm measuresto these databases, and if they use drm standards that are also widely used for copyrighted works, then the antidevice provisions probably will prevent the sale of circumvention technologies anyway.third, the dmca includes some exceptions to the anticircumvention and antidevice provisions. an exception for nonprofit libraries allows some circumvention of access control technologies, but only to decide whetheror not to make an acquisition of a work, not subsequent to the acquisition decision. libraries are not exemptedfrom the antidevice prohibitions so they cannot develop circumvention tools in any case.as one might predict, there is a reasonably broad exemption for law enforcement, intelligence agencies, andthe like to circumvent drm measures, and to develop circumvention tools.another exception allows circumvention and the development of circumvention tools for the purpose ofreverse engineering computer software to create interoperable software, subject to some conditions: (1) the copy ofthe software must have been lawfully obtained; (2) the interoperability information must not previously have beenreadily available; (3) the circumvention and tools are for the sole purpose of reverse engineering; and (4) theinformation and tools can be shared with other people only for that purpose.the dmca also includes an exception for ògood faith encryption research,ó with some fairly stringentconditions on who can qualify. the research has to satisfy a criterion of necessity, and the researcher must havemade a goodfaith effort to obtain authorization from the copyright owner to undertake the circumvention. thereare òmanneró limits on the dissemination of information and circumvention tools that are similar to the ones thatapply to the reverse engineering exception. the researcher can disseminate the information gained from thecircumvention only in a manner that is calculated to further the research, and not in a manner likely to facilitateinfringement. the ògood faith encryption researchó exception also includes some credentialing requirements forthe researchers themselves. one thing that helps the court determine whether someone qualifies to claim thethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.112the role of s&t data and information in the public domainexemption is whether that person is engaged in a legitimate course of training, study, or research in computerscience and therefore (by necessary implication) not just some hacker somewhere. finally, the information that isgained from the research must be shared with the copyright owner.there is also an exception for computer security testing, which is subject to conditions similar to those thatapply to the reverse engineering and good faith encryption research exceptions. the circumvention must satisfy asole purpose criterion and there are manner limitations on subsequent dissemination of the information gained.finally, the dmca contains some provisions identifying other rights that it supposedly does not affect. theseinclude limitations or defenses to copyright infringement, such as fair use, rights of free speech or the press, andprinciples governing vicarious or contributory liability for the design of electronics, computing, or telecommunications equipment.recent dmca litigationwhat do all of these provisions actually mean, and what are their implications for the use of publicdomainand scientific and technical (s&t) data and for collaborative research? three recent highprofile cases shed somelight on these questions.the first case, universal city studios v. reimerdes,3 is often referred to as the òdecss case.ó a 15yearoldnorwegian, jon johansen, developed a technology called decss, which circumvents the content scramblingsystem (css) that protects the movies encoded on dvds. according to his later court testimony, he did this tocreate a linuxbased dvd player; i.e., an opensource dvd player. the initial decryption program, however, wasa windowsbased program, because johansen was working from a windowsbased dvd player. johansen sharedthis program, decss, fairly widely. one of the organizations that ended up with decss was a hacker magazinebased in new york city called 2600.com, which put the program on its web site. 2600.com and its principals werepromptly sued by members of the motion picture association of america, and were permanently enjoined fromposting the program. the court also enjoined the defendants from knowingly providing links to any other web sitethat provided decss. both parts of that injunction were upheld on appeal.the second case involved professor ed felten, who took the òsdmi challenge.ó the strategic digital musicinitiative (sdmi) was a project to develop a secure technology for protecting digital music files. the recordingindustry association of america (riaa) challenged researchers to try and crack the prototype sdmi technology.professor felten and his team at princeton succeeded. rather than submit their results confidentially to thetechnology provider and claim a cash prize, felten decided to publish them and arranged to present the paperinitially at a computer security conference. the riaa notified the conference organizers and princeton universityõslegal counsel of the possibility of a lawsuit if the paper was presented. felten withdrew the paper from thatconference and publicized the circumstances, generating considerable uproar within the scientific community. theriaa immediately issued a press release stating that it did not intend to sue. felten filed a declaratory judgmentsuit against the riaa and the technology provider, challenging the lawfulness of what he claimed was a threatenedsuit or possible prosecution for violation of the dmca. he also arranged to present the paper at a differentconference, and did so. subsequently, the court granted the riaaõs motion to dismiss the suit. it ruled that therewas no credible threat of suit or prosecution after the other parties disclaimed intent to sue.4the final case, united states v. elcom,5 was the first criminal prosecution under the dmca. it involved amoscowbased software firm, elcom, which developed a technology that disabled certain drm features ofadobeõs ebook reader software so that one could, for example, make a copy of an ebook to back it up or totransfer it to a different device. elcom distributed its software via a web site that was accessible in the unitedstates. shortly thereafter, one of its leading programmers, dmitry sklyarov, came to the united states to attend asoftware conference. sklyarov was arrested at the airport, extradited to the northern district of california, and3see universal city studios, inc. v. reimerdes, 111 f. supp. 2d 294 (s.d.n.y. 2000), affõd sub nom. universal city studios, inc. v. corley,273 f.3d 429 (2d cir. 2001).4see felten v. recording industry association of america, no. 01cv2669 (d.n.j.).5see united states v. elcom, ltd., 203 f. supp. 2d 1111 (n.d. cal. 2002).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2113arraigned under the criminal provisions of the dmca. he subsequently cut a deal securing his release in exchangefor agreement to testify in the governmentõs prosecution of elcom, his employer. the court denied defensemotions to dismiss the case on constitutional grounds and also on the ground that the court lacked personaljurisdiction over elcom for actions taken in russia. ultimately, however, the jury acquitted elcom of all charges,finding that even if elcom had violated u.s. law, it had not done so willfully.what do these cases tell us? first, they tell us some things about the general scope of the dmcaõs prohibitions. in the decss case, defendants argued that the css for dvds could not possibly be the kind of technologicalmeasure protected by the statute because it was so easy to hack. specifically, they noted that the statute refers onlyto òeffectiveó technological measures, and argued that css was relatively ineffective. it probably will not surpriseyou that the court ruled this cannot possibly be what the dmca means, because otherwise the statute would notprotect very much. to be protected under the dmca, an òeffectiveó technological measure does not have to behackproof. this, though, means that given the broad language of the statute, virtually anything could qualify asthe kind of technological measure that is protected by the dmca. the statute protects any measure that requiresthe application of authorized information or an authorized process to gain access to the work, or that prevents orrestricts the exercise of a right of the copyright owner. a simple password requirement that one could get aroundquite easily might qualify. the dmca, therefore, potentially covers many kinds of drm gateways.also under the heading of general scope, the cases establish that one can be liable for knowingly linking toanother site that offers a circumvention tool. according to the reimerdes court, the requirement of knowledge isintended to avoid first amendment problems, and parallels the requirements for defamation liability. here it isimportant to remember that knowledge can be established based on notice, and that the copyright industries arefairly diligent about sending out such notices. even if the server hosting a circumvention tool is located outside theunited states, then, a copyright owner can use notices to ensure the disruption of links that might lead u.s.basedusers to the circumvention tool.a final thing that we know about the general scope of the dmca is prosecutors can rely on it to arrest peoplefrom other countries when they get off the plane in the united states, and that courts will uphold personaljurisdiction over those arrested. elcom suggests, however, that the harshness of this rule may be mitigated inpractice by the difficulty of establishing willfulness directed specifically toward u.s. law.second, the cases tell us some things about the relationship of the dmca to the doctrines of fair use andcontributory copyright infringement, both of which are designed to avoid overly broad infringement liability thatmight threaten other important public policies. the courts have concluded that there is no general fair use defenseavailable under the dmca. as already noted, the dmca does have a provision stating that defenses to copyrightinfringement, including fair use, are not affected. the courts have reasoned, however, that a cause of action underthe dmca is not a cause of action for copyright infringement. instead, it is a separate and distinct cause of actionfor circumventing drm measures or for manufacturing or distributing a circumvention technology. nowhere inthe dmca did congress provide a fairuse defense to either of those causes of action. therefore, the dmcacontains no openended safety valve, comparable to the fairuse doctrine, designed to avoid overly broad anticircumvention liability.contributory copyright infringement is a doctrine that protects technology providers in certain circumstances.one can sue a technology provider for providing a technology that facilitates copyright infringement, but there willbe no liability if the technology is capable of substantial noninfringing use. in the sony betamax case,6 thesupreme court held that the vcr was capable of many substantial noninfringing uses; therefore, sony could notbe held liable simply because people could also use vcrs to engage in unlawful copying. in contrast, the courtshave read the dmcaõs antidevice provisions to say that there is no òsubstantial noninfringing useó defense to acharge of manufacturing or distributing circumvention tools. recall that the antidevice provisions cover technologies that are primarily designed for circumvention, that have only limited commercially significant use other thanto circumvent, or that are knowingly marketed for circumvention. those are very different standards than whetherthe technology has òsubstantial noninfringingó uses. therefore, the court in the decss case concluded that the6sony corp. of am. v. universal city studios, inc., 464 u.s. 417 (1984).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.114the role of s&t data and information in the public domaincontributory infringement doctrine has been overruled by congress in the circumvention context to the extent ofany inconsistency with the new statute.third, the first wave of dmca litigation highlights the narrowness of the reverse engineering and encryptionresearch exceptions. the defendants in the decss case argued that they could claim both of those exceptions.they noted that decss was developed in the course of reverse engineering and for encryption research intendedto produce a linuxcompatible dvd player. the court rejected this argument on the ground that the defendants, asthird parties not involved in the reverse engineering process, lacked standing to invoke either exception. the clearimport of the courtõs ruling is that disseminating information to the general public to solicit participation in aprocess of reverse engineering or encryption research will not shield the recipients of that information.fourth, the courts have uniformly rejected constitutional challenges to the dmca. in all three cases, thechallengers argued that the statute was facially overbroad and therefore violated the first amendment because itregulated far more speech (in the form of computer code) than was necessary to achieve the legitimate purpose ofpreventing infringement. in reimerdes and elcom, the courts reasoned that defendants lacked standing to argueoverbreadth on the basis that others might use the disputed technologies to make fair uses of copyrighted works.since defendants themselves had not done so, whether the statutory prohibitions might be unconstitutional asapplied to somebody else was irrelevant. this issue remains unresolved, and it is difficult to predict how the courtsmight rule on it. the felten case, however, suggests a way for courts to avoid doing so.the only party who clearly was making a fair use, felten, had to surmount the initial threshold of demonstrating that there was some reasonable likelihood of suit or prosecution based on his activities. as already noted, thecourt did not think that any chilling effect on feltenõs speech existed once the riaa issued its press release statingthat it would not sue. that seems inconsistent with some other first amendment case law about chilling effects.for the present, however, it seems that in dmca cases, courts will require more than a threat hastily retractedfollowing bad publicity to demonstrate a chilling effect. to a very real extent, this ruling insulates the copyrightindustries from suit, and the dmca from constitutional threat, by bona fide fair users.the reimerdes and elcom courts also rejected arguments that under article i of the constitution, congresslacked the power to enact the dmca in the first place. again, they reasoned that since defendants themselves didnot seem to be making any fair uses of protected works, there was no need to consider whether there would beconstitutional problems if the statute prevented others from making fair uses. the courts also opined that òhorseand buggyó fair use would save the statute in any event. by this, i mean the kinds of fair uses that one can makewithout direct copying. for example, one can point a video camera at a dvd playing on oneõs computer screen, ordirectly transcribe by hand the contents of a protected ebook. the courts opined that the fairuse privilege does notgive users the right to make the best technological kind of fair use that they could possibly want to make.finally, the early dmca cases shed some light on the ways that the dmca is likely to be deployed in thefuture. whatever the differences between these three cases, the bottom line is that all of these cases have involvedthe antidevice provisions. they have all involved people who were charged with or, in feltenõs case, threatenedwith charges for creating technologies that could be used by other people to circumvent accesscontrol or copycontrol measures. as i noted earlier, the main purpose of the statute seems to be making sure that actual orpotential circumvention tools do not get created or disseminated. in holding that technology providers have nostanding to invoke possible fair uses by others to support constitutional defenses, courts reinforce this message,and ensure that wouldbe fair users who are not technologically savvy are out of luck. even though users technically retain circumvention privileges in cases involving copy controls (as opposed to access controls), they have tobe able to develop the tools themselves.implications of the drm/dmca regimeif these first few dmca cases hold up, what are the longterm implications of drm plus the dmca forresearch and innovation? i am going to talk briefly about four issues.first, the new regime of drm controls backed by law intensifies the impediments to information sharing andcollaboration previously discussed. it is helpful to keep in mind that we are talking about two different kinds ofinformation: (1) ordinary information, i.e., the content that is actually protected by the drm technology, such asthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 2115weather or fisheries data; and (2) technical information with circumvention potential, such as cryptographicsystems or computer security information. for the former, if drm restrictions apply, circumvention is eitherprohibited (for access controls) or impossible as a practical matter for most people. for the latter, there is verylimited standing to invoke the reverse engineering, encryption research, and security testing exceptions. it seemsthat they can be invoked only by the person who actually does the initial act of circumvention, and the scope ofdissemination of that information is quite limited. further, some technical investigations will require permission ofor sole use by the rights holder.second, does felten mean that there is nonetheless a zone of safety for academic computer science researchers? some have argued that felten was never really in any danger of being sued, and therefore that other academicresearchers are in no danger. i certainly agree with justin hughesõ comment that the riaaõs initial threat to suefelten was monumentally stupid. it is not completely clear to me, though, that no future threats remain. academicresearchers still must determine when an academic research paper containing technical information with circumvention potential is also a prohibited technology. if a research paper contains computer code, as many such papersdo, i do not think that the language of the statute would clearly exempt it. for that reason, i think there is certainlya residual chill that applies to those researchers who want to put code in their papers. some notable foreigncomputer scientists filed declarations in the elcom case saying that they would no longer attend conferences in theunited states because they were afraid of being arrested. some of that was theater. evaluation of threats, however,is partly subjective. if people say they are afraid, the fact that a welltrained copyright lawyer might conclude thattheir fears are boundless may in some cases be beside the point.third, it is worth noting separately that the dmca is profoundly hostile to the opensource software community. focusing on the language of the reverse engineering, encryption research, and computer security testingexceptions makes that crystal clear. the way that collaboration works within opensource communities is thatresearchers put information they have learned on the web or on discussion lists and invite anybody who isinterested to help with the project at hand. such widespread sharing of information does not seem to fall within thekind of behavior contemplated by the exceptions. the exceptions direct courts to consider whether the informationis disseminated to others in a manner calculated to further research or, alternatively, in a manner calculated tofurther infringement. i think that it is going to be very hard to argue that sharing information about cryptographyresearch or reverse engineering with the opensource community as a whole, which includes anybody who wantsto join it, satisfies that condition. in addition, the encryption research exception includes a credentialing provision,which directs the court to consider whether the person claiming the exception is employed or engaged in alegitimate source of study in computer science. the computer science research community is far broader than that,and this wording seems clearly to privilege a certain kind of scientific elite over others who might want to tinkerwith and improve software and enhance understanding of programming techniques. i think we all understand thatis not the way that scientific progress historically has worked in this country. our scientific tradition includes manypeople who invented pretty cool stuff in their garages. the encryption research exception seems to contemplate avery different sort of regime.finally, it is important to consider the potential network effects of drm systems and standards. recall, onceagain, that the universe of scientific and technical innovation is a very complex system that exhibits a lot of pathdependencies. one cannot just drop drm technology into the world of computer software and networking andexpect nothing else to happen. we need to consider the effects these standards are going to have, both as initiallyimplemented in discrete areas of the system and as they start to migrate deeper into the network.an initial class of network effects relates to the modification of other standards and technologies to increasetheir interoperability with drm systems and increase the efficacy of the larger drm/dmca system. all consumer electronics equipment and blank media will have to be made interoperable with these technologies. manufacturers who decline to comply with drm standards may be shut out of content markets; manufacturers whodesign their equipment and media to override or ignore drm standards may be vulnerable to charges that theyhave created circumvention tools. this has implications for researchers even if you think that the most egregiousinstantiations of drm will apply only to such things as music files and video games. quite possibly, the sameblank media that researchers want to use to write their research papers will be encoded with drm protection at thebehest of copyright owners who simply want to make it more difficult to copy music. in other words, the standardsthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.116the role of s&t data and information in the public domaindo not have to penetrate all the way into markets for s&t data in order to have an effect on the conduct of researchand innovation.what happens if drm standards do gradually extend into s&t markets, or if they migrate even deeper intothe various technical layers of computers and computing networks? some people who are developing drmsystems have realized that they are relatively easy to circumvent if they are implemented in particular applicationsor peripheral devices, and that they would be harder to circumvent if embedded in computer operating systems andeven harder to circumvent if one could wire them into hardware or embed them in the basic internet protocol.more widespread extension of drm regimes will reshape the ways in which information storage, retrieval,and exchange are handled. earlier, i raised the question of who will develop search tools that can interact withindividual drm systems. we now can extend this point to network searches more generally. if one needs a licenseto develop a search engine, what kinds of consequences will that have for the development of innovative searchtechnologies? if archiving and storage become proprietary activities, will the risks of format obsolescence increase? maybe we do not have enough data to answer these questions. it is certainly a change from the way thedevelopment of search tools has worked so far.if drm functionality continues to migrate deeper into the computing layer, we also may see decreasedpenetration of opensource systems simply because it is going to be difficult legally to create the kinds ofinteroperability that are necessary for opensource systems to attain greater market share. in the predmca world,if consumers wanted their dvd players or their word processing program to behave in a certain way, the opensource community could do that relatively easily. if members of that community wanted to make it happen, theywould. but if the information about how to make these systems interoperate with other components of thecomputing platform is protected under the dmca, achieving interoperability will be much more difficult.conclusionit is usually easy to convince academics and researchers that the worstcase potential consequences of aphenomenon are worth studying more closely. yet the worstcase consequences of drm regimes and the protection given them under the dmca are worth more than further study. the culture of scientific research is in someways extraordinarily robust, but in other ways it is extraordinarily fragile. in particular, it is premised on a seriesof assumptions about the public domain, and about access to and use and sharing of information, that may soonwarrant serious revision. in my view, waiting for these worstcase consequences to materialize would be a terriblemistake.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.117session 3: potential effects of adiminishing public domainthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 311911916discussion frameworkpaul uhlirin the first session of this symposium, we described some of the potentially limitless possibilities for researchand innovation that might ensue from using digital technologies to exploit scientific data available from the publicdomain as it was traditionally constituted. however, these prospects dim the moment we consider the ramificationsfor science from the economic, legal, and technological assaults on the public domain that are described in thesession 2 presentations. here, we explore some of the likely negative implications of these trends for science andinnovation unless science policy directly addresses these risks.in the interests of clarity, i outline the effects of present trends on a sectoral basis, in keeping with thefunctional map of publicdomain data flows presented previously.1 i begin with the governmentõs role as primaryproducer of such data and then consider the implications of present trends for academia and for our broaderinnovation system.if a basic trend is to shift more data production and dissemination activities from government to the privatesector, one should recognize at the outset that the social benefits can exceed the costs under the right set ofcircumstances. in principle, private database producers may operate more efficiently and attain qualitatively betterresults than government agencies. positive results are especially likely when markets have formed; competitionoccurs; and the public interest, including the needs of the research community that was previously served by thegovernment activity, continues to be met.there are also numerous drawbacks associated with this trend, however, that require careful consideration. tobegin with, the private data supplier will seldom be in a position to produce the same quantity and range of data asa government agency and still make a profit while charging prices that users can afford. in other words, thegovernment agency has typically taken on the task of data production and dissemination as a public good preciselybecause the social need outweighs the market opportunities. social costs begin to rise if the profit motive inducesthe private supplier to reduce the quantity and range of data to be produced or made available. for example, aprivate data producer typically markets highly refined data products to end users in relatively small quantities,whereas basic research, particularly in the observational sciences, generally requires raw or less commerciallyrefined data in voluminous quantities. on the whole, overzealous privatization of the governmentõs data produc1for information on these data flows, please refer to chapter 1 of these proceedings, òsession 1 discussion framework,ó by paul uhlir.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.120the role of s&t data and information in the public domaintion capabilities poses real risks for both science and innovation, because the private sector simply cannot or willnot duplicate the governmentõs publicgood functions and still make a profit, not to say extract maximum rents.moreover, unless the private sector can demonstrably produce and distribute much the same data moreeffectively and with higherquality standards than a government agency, privatization may become little more thana sham transaction. on this scenario, the wouldbe entrepreneur merely captures a government function and thenlicenses data back to a captive market at much higher prices and greatly increased restrictions on access and use.in the absence of marketinduced competition, there is a very high risk of trading one monopolist with favorablepolicies toward science and the broader societyñthe governmentñfor another monopolist driven entirely by theprofit motive and the restrictions that makes necessary.absent a sham transaction, one cannot say a priori that any given privatization project necessarily results in anet social loss. the outcome will depend on the contracts the agency stipulates and on the steps it is willing to taketo ensure continued access to data for research purposes on reasonable terms and conditions. in contrast to buyingdata collection services, the licensing of data and information products from the private sector raises seriousquestions about the types of controls the private sector places on the redistribution and uses of such data andinformation that the government can subsequently undertake. if the terms of the license are onerous to thegovernment and access, use, and redistribution are substantially restricted, as they almost always are, neither theagency nor the taxpayer is well served. this is particularly true in those cases where the data that need to becollected are for a basic research function or serve a key statutory mission of the agency.a classic example of what can go wrong was the privatization of the landsat earth remote sensing program inthe mid1980s. following the legislatively mandated transfer of this program to the earth observation satellite(eosat) company, the price per scene rose more than 1,000 percent, and significant restrictions were imposedeven on nonprofit research uses. use by both government and academic scientists plummeted, and subsequentstudies showed the extent to which both basic and applied research in environmental remote sensing was set back.this experiment also failed in commercial terms, as eosat became unable to continue operations after a fewyears.the legal and technological pressures identified in this symposium will also affect the uses that are made ofgovernmentfunded data in academic and other nonprofit institutions. they will intensify the tensions that alreadyexist between the sharing norms of science and the need to restrict access to data in pursuit of increased commercial opportunities.although the enhanced opportunities for commercial exploitation that new intellectual property rights (iprs)and related developments make possible are clear, they will affect the normative behavior of the scientificcommunity gradually and unevenly. academics are already conflicted in this emerging new environment, andthese conflicts are likely to grow. as researchers in public science, they need continued access to a scientificcommons on acceptable terms, and they are expected to contribute to it in return. as members of academicinstitutions, however, they are increasingly under pressure to transfer research results to the private sector for gain,and they themselves may want to profit from the new commercial opportunities.the government itself fuels these conflicts by the potentially contradictory policies that underlie its funding ofresearch. one message reminds scientists of their duties to share and disclose data, in keeping with the traditionalnorms of science. the other, more recent, message delivered by the bayhdole act urges them to transfer the fruitsof their research to the private sector or to otherwise exploit the intellectual property protection their research mayattract.at the moment, these conflicts are strongest where the line between basic and applied science has collapsed,and where commercial opportunities are inherent in most projects. obvious examples are biotechnology andcomputer science. in the future, the enactment of a powerful ipr in collections of data might be expected to pushthese tensions into other areas where the lines between basic and applied research remain somewhat clearer and thepressures to commercialize research results have been less noticeable thus far. in exploring the implications ofthese developments for academic research, we continue to focus attention on the two distinct, but overlapping,research domains we previously characterized as òformaló and òinformal.óin what we term the formal sector, science is conducted within structured research programs that establishguidelines for the production and dissemination of data. typically, data are released to the public in connectionthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 3121with the publication of research results. data may also be disclosed in connection with patent applications andsupporting documentation. one should recall that, even without regard to the mounting legal and technologicalpressures, there are strong economic pressures that already limit the amount of data investigators are inclined torelease at publication or in patent applications, there are growing delays in releasing those data as researchersconsider commercialization options, and more of the data that are released come with various restrictions.the enactment of a hybrid ipr in collections of data such as the e.c. database directive would introduce adisruptive new element into an already troubled academic environment. to some extent, this development wouldtend to erase some of the previous distinctions between the òformaló and òinformaló domains. in both domains,access to data might nonetheless have to be secured by means of brokered, negotiated transactions, and thisoutcome is rife with implications. for present purposes, it seems clear that any database protection law, coupledwith the other legal and technological measures discussed previously, will further undermine the sharing ethos andencourage the formation of a strategic trading mentality, based on selfinterest, that already predominates in theinformal domain.we also predict that these pressures will necessarily tend to blur and dilute the importance of publication asthe line of demarcation between a period of exclusive use in relative secrecy and ultimate dedication of data to thepublic. suddenly, such a right would make it possible to publish academic research for credit and reputation whileretaining ownership and control of the underlying data, which would no longer automatically lapse into the publicdomain. once databases attract an exclusive property right valid against the world, the legal duty of scientistspublishing research results to disclose the underlying data would depend on codified exceptions permitting use forverification and for certain òreasonableó nonprofit research and educational purposes. we recognize that this newproprietary default rule must ultimately be reconciled in practice with the disclosure obligations of the federalfunding agencies. our point is that the new default rule nonetheless places even published data outside of thepublic domain, and we note further that much academic research is not federally funded or is not funded in waysthat waive such disclosure requirements.moreover, the role of academic journal publishers in this new legal environment bears consideration. atpresent, scientists tend to assign their copyrights to such publishers on an exclusive basis, and many of thesejournals now produce electronic versions, sometimes exclusive of a print version. this already complicatesmatters because, as discussed in session 2, the data that traditional copyright law puts into the public domain maybe fenced to a still unknown extent by the technological measures that the digital millennium copyright actreinforces. if, in addition, a database law is enacted, any data that the scientist assigns to the publisher with thearticle will become subject to the statutory regime. the publisher would then be in a position to control subsequentuses of the data and to make them available online under a licensed subscription or payperuse basis, and withadditional restrictions on extraction or reuse.even if individual scientists are willing and able to resist the demands for exclusive assignments of both theircopyrights and any new database rights, the fact remains that publication of the article in a journal will no longerautomatically release the data into the public domain as before. on the contrary, unless the scientist waives the newrestrictive default rule, even the data, revealed in the publication itself, will remain subject to the scientistõsexclusive right of extraction and reuse, at least as formulated under the e.c. database protection model.with or without a new statutory database right in the united states, scientists in public research also appearcertain to come under increasing pressure to retain data for commercial exploitation. the research universities arealready deeply committed to maximizing income under bayhdole, with varying degrees of success, and they willlogically extend these practices and procedures to the commercialization of databases as valuable research tools. akey question is whether they will make the commercialized data available for academic research on reasonableterms and conditions.as with governmentgenerated data, university efforts to commercially exploit their databases could producenet social gains under the right set of circumstances. in addition to the incentives to generate new and more refineddata products that an ipr may promote, greater efforts may be made to enhance the quality and utility of selecteddatabases than would otherwise be the case. absent such incentives, many scientists may not take pains toorganize and document their data for easy use by others, particularly outside their immediate discipline, and theymay not refine their data beyond the level needed to support their own research need and related publicationthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.122the role of s&t data and information in the public domainobjectives. legal incentives may thus stimulate the production of more refined databases, especially where markets for such products have formed.at the same time, these new commercial opportunities tempt university administrators and academics toattenuate or modify the sharing and openaccess norms of science and to circumvent obligations in this regard thatthe federal agencies have established. were this to occur on a largescale basis, the unintended harms to researchcould greatly exceed those we are accustomed to coping with concerning patented inventions under bayhdole.the licensing of academic databases, reinforced by a codified ipr, would thus limit the quantity and quality ofdata heretofore available from the public domain.at present, the primary bulwarks against such a breakdown of the sharing ethos are the formal requirementsof the federal funding agencies, which in many cases continue to require that data from the research projects theyfund should be transferred at some point to public repositories, or made available upon request. to avoid thenegative results we envision, the agencies would have to strengthen these requirementsñand their enforcementñand adapt them to the emerging highprotectionist intellectual property environment. we elaborate further on thistopic in the next session. the point for now is that, absent express overrides that universities voluntarily adopt orthat funding agencies impose in their research grants and contracts, the new restrictive default rules of ownershipand control will automatically take effect if congress enacts a database protection law. indeed, they could becomegeneral practice even without such a law as the result of routine, unregulated database licensing practices.in the informal zone, researchers are not yet ready to publish, or they are working independently on òsmallscienceó projects beyond the formal controls and requirements of a federal research program that requires openaccess or public deposit. this includes research funded by state governments, foundations, and the universitiesthemselves, which leave more discretion in these matters to researchers, and by private companies, which normally require secrecy.much of what has been said about the effects of the new legal and technological pressures on the formalacademic zone thus applies with even greater force to the informal zone because the impetus to commercialize datawill encounter fewer regulatory constraints. the changing mores likely to undermine disclosure and open access inthe formal zone will make it ever harder to organize cooperative networks in the less structured and more unrulyinformal domain.these tendencies would predictably become more pronounced over time as more scientists became aware ofthe new possibilities to retain ownership and control of data, even after publication of research results. indeed, onewould logically expect that strategic behavior in the informal zone would increasingly be geared to efforts tomaximize advantages from postpublication opportunities. should this occur, academics themselves would exertpressure on the federal system that defends open access and on their universities to fall in line with the needs ofcommercial partners.one can thus project a kind of cascading effect if a strong database protection right is enacted and the scientificcommunity fails to take steps to preserve and reinforce the research commons. on this view, todayõs formal zone builtaround release of data into the public domain at publication would begin to resemble the informal zone, while thatsame informal zone would look more and more like the private sector. under these circumstances, one cannotnecessarily assume that the openaccess policies currently supporting the formal sector would continue in force, inwhich case, even basic research could be adversely affected, as occurred in the united kingdom in the 1980s.what the new equilibrium that will result from the conflict between these privatizing and commercializingpressures on the one hand, and the traditional norms of public science on the other, will look like cannot bepredicted with any degree of certainty. in previous articles, however, we have outlined the cumulative negativeeffects that such tendencies likely would have on scientific endeavor. for the sake of brevity, we recall them herein summary form:¥less effective domestic and international scientific collaboration, with serious impediments to the use, reuse,and transformation of factual data that are the building blocks of research;¥increased transaction costs driven by the need to enforce the new legal restrictions on data obtained fromdifferent sources, by the implementation of new administrative guidelines concerning institutional acquisitionsand uses of databases, and associated legal fees;the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 3123¥monopoly pricing of data and anticompetitive practices by entities that acquire market power, or by firstentrants into niche markets that predominate in many research areas; and¥less dataintensive research and lost opportunity costs.what could well be the greatest casualty are the new opportunities that digital networks provide to createvirtual information commons within and across disciplinespecific communities that are built around optimalaccess to and exchange of scientific data. to the extent that public science becomes dominated by brokeredintellectual property transactions, then the resulting combination of high transaction costs, unbridled selfinterest,and anticommons effects would defeat the fragile cooperative arrangements needed to create and maintain suchvirtual information commons and the distributed research opportunities they make possible.finally, to see why some critics in the united states harbor deep concerns about the longterm consequencesof the e.u.õs approach, it suffices to grasp how radical a change it would introduce into the domestic system ofinnovation and to consider how great the risks of such change really are. traditionally, u.s. intellectual propertylaw has not protected investment as such, a tradition that still has constitutional underpinnings. at the same time,the national system of innovation depends on enormous flows of mostly governmentgenerated or governmentfunded scientific and technical data and information upstream, which everyone is free to use, and on free competition with respect to downstream information goods.the domestic intellectual property laws protected downstream bundles of information in two situations only:copyrightable works of art and literature and patentable inventions. however, the following conditions apply inboth cases:¥these regimes require relatively large creative contributions based on free inputs of information and ideas;¥they presuppose a flow of unprotected information and data upstream; and¥they presuppose free competition with regard to the products of mere investment that are neither copyrightable nor patentable.as previously observed, the e.c.õs database directive changes this approach, as would the last parallelproposal, h.r. 354, to enact strong database rights in the united states. specifically, these sui generis regimesconfer a strong and, in the european union, potentially perpetual exclusive property right on the fruits of mereinvestment, without requiring any creative contribution. they also convert data and informationñthe previouslyunprotectible raw materials and basic inputs of the modern information economyñinto the subject matter of thisnew exclusive property right.the sui generis database regimes would thus effectuate a radical change in the economic nature and role ofiprs. until now, the economic function of iprs was to make markets possible where previously there existed arisk of market failure due to the publicgood nature of intangible creations. exclusive rights make embodiments ofintangible public goods artificially appropriable, they create markets for those embodiments, and they make itpossible to exchange payment for access to these creations.in contrast, an exclusive ipr in the contents of databases breaks existing markets for downstream aggregatesof information, which were formed around inputs of information largely available from the public domain. ineffect, the sui generis database regimes create new and potentially serious barriers to entry to all existing marketsfor intellectual goods owing to the multiplicity of new owners of upstream information in whom they vestexclusive rights, any one of whom can hold out and all of whom can impose onerous transaction costs analogousto the problem of multimedia transactions under copyright law. this thicket of rights fosters anticommons effects,and the database laws appear to be ideal generators of this phenomenon.under the new sui generis database regime, in short, there is a builtin risk that too many owners of information inputs will impose too many costs and conditions on all the information processes we now take for granted inthe information economy. at best, the costs of research and development activities might be expected to rise acrossthe entire economy, well in excess of benefits, owing to the potential stranglehold of data suppliers on the rawmaterials. this stranglehold will increase with market power if databases are owned by solesource providers.over time, the comparative advantage from owning a large, complex database will tend progressively to elevatethese barriers to entry.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.124the role of s&t data and information in the public domainsupporters of strong database protection laws and of strong contractual regimes to reinforce them believe thatthe benefits of private property rights are without limit, and that more is always better. they expect that thesepowerful legal incentives will attract huge resources into the production of electronic databases and informationgoods. in contrast, critics fear that an exclusive property right in noncopyrightable collections of data, coupledwith the proprietorsõ unlimited power to impose adhesion contracts in the course of online delivery, will compromise the operations of the national system of innovation, which depend on the relatively free flow of upstream dataand information. in place of the explosive production of new databases that proponents envision, opponents of astrong database right predict a steep rise in the costs across the information economy and a progressivebalkanization of that economy, in which fewer knowledge goods may be produced as more tithes have to be paidto more information rent seekers along the way.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 312512517fundamental research and educationr. stephen berry.in this presentation i will emphasize fundamental research and focus less on education, but i will comment onthe impacts on education. first, i am going to look at this issue from the viewpoint of a scientist.two fundamental characteristics govern the way scientists carry out their activities. first, they depend onopen access to information, because that information continually is expected to be used and to be challenged. oneof the most important ways in which that information is used is in sustaining the verifiability that makes sciencedifferent from virtually any other subject. it is the verifiability, which is the second characteristic, that makesscientific knowledge a firmer kind of knowledge that anything else we have. this information includes not onlydata in databases, but also the information found in journals and textbooks, the interpretation of data, and theconcepts that underlie these.i want to address almost exclusively information that is generated by either governments or notforprofitinstitutions; i will not address proprietary information. in session 2, we heard that information generated byscience supported this way constitutes a public good. the justification for the support of that research is theproduction of the public good that comes from the science. a public good is one that does not diminish with useand has virtually no marginal costs for all of the users after the first user. but there is a special characteristic toscientific public goods: not only does the value of the scientific information not diminish, but it increases with itsuse. to satisfy the intent of the supporter of the research, society has to use that information and maximize its use,if possible, to achieve the values of the public good.historically, the scientific community and the publishing community in the broad senseñthat is, the privatepublishers, the professional societies, and government through its own publicationsñalways had a symbioticrelationship, as long as paper publishing was the sole outlet for the distribution and archiving of this information.that all changed with the internet, which provided a faster, cheaper, and more efficient way for the scientificcommunity to distribute and share its information. i think probably the sharpest example of that is the onlineeprint archive that paul ginsparg started in the area of highenergy physics.1when that technological development happened, the relationship changed. it was no longer that comfortablesymbiotic relationship; many scientists wanted to make use of the new medium. many publishers, including someprofessional societies, did not want to use the internet as a principal mode of distribution. in fact, many publishers1see the arxiv.org eprint archive web site at http://arxiv.org for additional information.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.126the role of s&t data and information in the public domainsaw the use of electronic distribution not as a new way to provide a different kind of added value, but as a threatto the way that they make their living.we have to keep one point in mind, which is very difficult for people outside the scientific world to realize.this issue became apparent to me when we were carrying out the bits of power study.2 the study committeeconsisted of scientists, technologists, economists, and lawyers. during the first two meetings, the scientists andtechnologists and the lawyers and the economists were making no contact. they were talking as if they were indifferent worlds, but there was a key step that was a breakthrough. that was the articulation of the realization thatfor scientists the motivation is not the same as it is for the author of a novel. it is not making money frompublication. for the scientist, the primary motivation, the currency if you will, is the propagation of ideas. this isthe reason why scientists want to publish the results of their work. the scientistõs primary goal is to distribute ideasand influence the thinking of others. if you use that as the basis of a value system, then an economist can slightlyrecast traditional economics with this other currency.with this realization, the scientists and the lawyers and the economists on the committee were able to talk toeach other in a very productive way. we simply had to find a way to establish the bridge to allow the economiststo use their tools with the analog of what they normally use as the basis of evaluation. the financial monetary basisand the idea distribution basis were compatible when there was only one way to distribute the information in astorable, preservable way. in addition, the existing social and legal structure made open access via copyright andits exceptions. protective, or restrictive, approaches changed that, or at least raised the specter. those restrictionsbasically created an incompatibility, or threatened to create an incompatibility, between the way that the scientistsoperate and the way that the publishers operate. that incompatibility has been very difficult to explain, becausepeople outside the scientific world usually do not understand the motivations of scientists.the federal agencies that support the research have an interest in maintaining the distribution and archiving ofthe scientific information. and, of course, when a body, a law, or an activity acts to inhibit the distribution of thatscientific information, then it is acting against the interests of the funding agency and acting against the interestsof the national goals that justify the funding agency. that inhibition diminishes the publicgood value of thatinformation.in extreme terms, which apply more to the case of the european union database directive than to anything wehave enforced in the united states right now, this thwarting of the distribution of information created in thenational interest can be thought of as a theft of government property. the privatization, the inhibition of distribution, is in effect stealing from the government and putting into private hands the information that the governmentcreated for the purpose of public distribution.people argue that scientists withhold information. however, the socially acceptable withholding of information in the scientific community is basically to allow scientists to (a) verify and establish the validity of what theyare doing and (b) to be able to study, capture, and exploit their own research. so, for example, when crystallographers keep coordinates for one year, it is a way that the researcher with two graduate students in a small departmentcan take the results of his own measurements and study them for that year. if the coordinates were publishedearlier, then a group of 30 could very easily do the studies much faster and publish in a few weeks something thatwould take the group of one faculty member and two graduate students several months to do. this is a kind ofcourtesy within the scientific community that is well accepted. it is a recognition, call it a soft spot or a weaknessin the system, in which scientists compete with each other, and it is accepted.there are some journals that require that data be deposited in publicly available databases. this is counter tothat acceptance of the temporary withholding of information. by and large in the scientific community withholding data is really a bad thing socially. scientists are very much looked down upon or scorned for withholding data.there is an ethic in this community that most of the time works pretty well.let us turn to the question of whether there could be a sound stable market for scientific information of thekind that would be captured by the legislation that has been proposed in the united states or by the europeanunion database directive. the value of small bits of scientific information is uncertain. the uncertainty of the2see national research council. 1997. bits of power: issues in global access to scientific data, national academy press, washington,d.c.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 3127value of scientific data diminishes as it is aggregated more and more. for example, all the data generated by theresearch supported by the mathematical and physical sciences division of the national science foundation have ahigh value. but what part of that body of data contributes the high value is very unpredictable. the value actuallymay not be achieved for a number of years.one of the difficulties we have to contend with is that, although the aggregate data have a very high value, theway we decide what research to do or the way we fund the production of that information is at a much moredesegregated basis. so the value of the data produced by the programs supported by one program officer at thenational science foundation is very uncertain. the value of the data produced by one division of the nationalscience foundation is somewhat less uncertain, because it is aggregated. but the funding is not done on theaggregated basis, the funding is done on a very disaggregated basis. consequently, because of the high riskassociated with each decision in the funding process, the result is that we cannot establish the value of theinformation produced by any one research project or even one program officerõs set of projects. this is one of theways that congress justifies the relatively low support for the kinds of research that the united states supports, thenational institutes of health currently notwithstanding. real venture research is particularly unlikely to be adequately funded until it has proved itself.what will privatizing do in this pinched market? it essentially will price the basic science community out ofbuying the information it needs. or, alternatively, the scientific community may very well find its own ways tosustain itself, with its own new ways to distribute information outside the commercial market. the scientist doesnot have to publish in the existing journals, he does not have to deposit his data on a privatized basis. he can findhis own pathways to do it.to see how this is a plausible course, we can just look at the fact that scientists do have this other motivationñto maximize the distribution of information. the basic science community may find its own way to provide theinformation in a public domain, or some other openaccess mechanism, outside the commercial publishing community. we have existing models for pathways that the scientific community can create for itself, such as thearxiv.org and the protein data base.professional societies represent a range of models that go all the way from astronomers, mathematicians, andphysicists that have moved very much toward open access and public domain all the way to the other pole, to theamerican chemical society, which basically sees its publications as the principal source of its own support andtherefore is very protective of its publications. one thing that we have not seen yet, and i think we will, isprofessional societies examining other models to support their publishing activities. the organizations that haveconsidered it necessary to support themselves through publication have not yet started to look at other possibleways of doing this, but i think that we can expect to see that in the next four or five years.there is a question now of who will pay for the distribution of scientific information. we have heard in thissymposium that if a truly competitive market that would establish suitable pricing would do it, then that would befine. if basic scientific information cannot be managed in a stable way by a competitive market, then society facesa choice: which is more important to the society, the sustenance of scientific enterprise or the sustenance of theprivate information business? we have evidence from the fact that the federal government is a very important, keysupporter of basic research and that we place a high value on the maintenance of science.what is the responsibility of the private sector? if we look back at the publishing business during the 1960sand 1970s, when there was a lot of money for science and a lot of money for highly specialized journals, librarieswere able to pay for the subscriptions on specialized journals. publishing all kinds of scientific journals was areasonable and profitable thing for publishers. however, a responsible publisher must monitor the profitability ofevery one of its ventures.we have heard about the number of subscriptions that are being dropped by the university libraries thatprovide the principal sustenance for scientific publishers. as a result of the decreasing subscriptions, publishersmust determine whether to discontinue these journals. publishers have been very reluctant to take the responsibility to decide whether continuing to publish highpriced scientific journals is profitable. i challenge the publishersto examine one by one the specialized journals that they publish and decide whether they should continue to do so.i think it is a business decision that is hard to face up to, because it has been very profitable. but it is not clear thatit is going to continue to be profitable. if the publishers decide it is not, then they should drop the journals. thethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.128the role of s&t data and information in the public domainscientific community simply will have to find other alternatives to distribute its information, and we have somemodels for that to happen.the public funder of research has some responsibilities. we talked about the costs of publication, but thesecosts along with those of collecting and distributing the information are far lower than the costs of the research.when we think of the observational sciences, i include the gathering of meteorological and astronomical data aspart of the research, rather than as part of the publication process. this research done for public good is valuelessunless the results are distributed. as such, the supporter of the research carries the responsibility to see that thereis some mechanism to distribute the information. if the market mechanism does not do it, then the publisher of theinformation must be some institution or some mechanism supported by the supporter of the research. that smalladded cost for getting the information out has to be included.let us turn now to education. education has thrived on access to scientific information through fair use formany years, and we will count on that in the future. but there is a problem that i will not discuss in much detailabout the use of online and distance education, and the vehicles that are used for this. are these going to becomecaptured, privatized, and turned into the kinds of instruments that are not available for fair use? this is one of thenew problems that education faces. as you know, there are opensource materials available, as well as commercially marketed counterparts.one effect on education is already apparent, which is the impact of the nondisclosure constraints in someuniversityindustry collaborations. some of these collaborations have nondisclosure restraints that literally prohibit graduate students from one research group talking to the graduate students in another group about their work.this is an erosion of the environment in which we want our graduate students to be trained. this is a very serioussurrender of principles of education to essentially gain a fast buck.i think it is very disturbing that in much of our discussion even at this symposium, we have talked aboutuniversities as though their primary function is turning out commercially useful research. the primary purpose ofa university, the primary product of a university, is educated students, and we must never lose sight of that. wemust never surrender the mechanisms that produce truly educated students for secondary purposes such as commercially productive research. this is a very important perspective that we have to retain.let me go back now to the online education issue, which will lead me to a final perspective. in the case ofonline education where we have both models, open source and commercial, why not let them compete? let us dothe experiment and see whether the commercial products are the ones that people want to use, or the opensourceones, or both. we may very well have two kinds of users in the long run.what are the next steps? in education, in scientific data, we are not at a stage where we have a clearcut courseahead of us; we are going through a period of adaptation. we do not know what will be best. the only sensiblething for us to do is to try the different alternatives and see what works where. the worst thing would be to followa restrictive course through legislation. the most productive course we could take is a permissive one to allow thedifferent modes of activity to compete with each other. the digital millennium copyright act in this sense isgoing in exactly the wrong direction, because it is an inhibiting, rather than a permissive legislation. we needlegislation that encourages the competition between different methods and allows us to try different options andsee what works where.i hope that we can recognize and adjust to that before we reach a crisis in which, for example, the scientificcommunity strangles. my own personal hope, optimistic and naive as it may be, is that if we do face theserestrictive forms of legislation, then the scientific community will be inventive enough to find its own way to solveits problems and sustain itself independent of those who insist on capturing the real estate and listing databases atthe cost of whatever the scientific community might have to pay.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 312912918conflicting international public sectorinformation policies and their effects onthe public domain and the economypeter weiss1.it is regrettably a well kept secret in washington that the open and unrestricted policies regarding dissemination of u.s. federal governmentproduced, taxpayerfunded information are not based merely on abstract notionsof government transparency or support for the scientific endeavor, but are based on a fundamental economicconceptñinformation and data paid for by the taxpayers are an important input to our overall economy. the largequantities and varieties of taxpayerfunded information have been demonstrated to be important inputs in a numberof industries. economic research is somewhat sparse on this, but it is clear that the information retrieval anddatabase industries are highly dependent on the open and unrestricted availability of government information. forexample, the industry grew from $4 billion in 1994 to an expected $10 billion in 2002, and the number of databasevendors grew from 900 in 1991 to 2,400 vendors in 1999. these numbers show that the internet revolution and theability to use government information as an input to valueadded commercial products has been a significanteconomic boon.focusing on one particular sector of information, meteorological and related environmental information, weknow that the weather and climate impact about a third of the gross domestic product of the united states, about$3 trillion. many industries are weather sensitive including construction, agriculture, energy, and tourism. that hasresulted in two interesting phenomena. the united states has a large and growing commercial meteorologicalindustry. you have probably heard of the weather channel, which is globally unique in its size and scope. theweather channel can exist only because of the federal governmentõs policy of open and unrestricted access to thetaxpayerfunded meteorological information, model outputs, satellite data, surface observations, oceanic observations, and so on. there is no weather channel in europe, and there are reasons for that which we will explore later.in addition, the financial community has learned that it can assist weathersensitive industries to hedge theirrisks. so, for example, if you are a natural gas marketer in the midwest, you make more money in a cold winter andless money in a warm winter. if you run a resort on the florida coast, you make more money when it is warm andsunny. if you are a ski resort operator, you make more money when it is cold and snowy. your fortunes vary,depending on the conditions that occur in any given year. the financial markets now can help you hedge thoserisks through financial instruments commonly called derivatives, which act as insurance policies. that industryhas boomed in just five years and is now a nearly $12 billion industry. the reason it can prosper is because of the1the views expressed are those of the author and do not necessarily represent those of the national oceanic and atmospheric administration/national weather service.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.130the role of s&t data and information in the public domainfact that all the meteorological information, current and historic, gathered and generated by the u.s. governmentis openly and unrestrictedly available to the financial community to provide these specialized services.in the united states we hold as selfevident the truth that taxpayerfunded information belongs to the taxpayers. however, that truth is not broadly accepted worldwide. indeed, the united states stands close to alone infollowing and advocating an open and unrestricted data policy. some notable exceptions are japan, australia, newzealand, and the netherlands. rather, much of the world, for example great britain, france, and germany, treatsits government information not as a public good, but rather as a private revenuegenerating mechanism to supplement or offset agency appropriations.it has only been in the past few years that researchers and economists have been starting to think about theeconomic effects of openaccess policies. one seminal study was funded by the european commission.2 for thepurposes of this discussion, remember that the european union economy and the american economy are about thesame size. the european commission study found that the united states spends twice as much money in creatingpublicsector information than the europeans do in total, but the economic value to society in terms of job creation,wealth creation, and taxes is a factor of five larger in the united states than it is in europe.the united states follows a policy that encourages, even sometimes forces, federal agencies actively todisseminate that information to all comers, so that we can have, for example, database industries, more robustpublishing industries, commercial meteorology firms like the weather channel, or weather risk management firmsthat are selling new forms of insurance. european government restrictions on dissemination and use of publicsector information are thwarting the kind of economic development in these sectors that we have already seen inthe united states.for example, in the united states the commercial meteorology sector totals approximately half a billiondollars annually. it includes about 400 companies employing about 4,000 people. in europe, the commercialmeteorology sector is a factor of ten smaller. again, the european economy and the american economy are ofapproximately equal size.why does this phenomenon exist? i claim it exists because european government agencies often restricttaxpayerfunded information for shortsighted reasons. an even more telling economic statistic illustrates thisissue. the value of contracts issued by the weather risk management industry over the four years ending in 2001was over $7 billion; it is now nearly $12 billion, adding nearly $5 billion in notional value in one year alone. bycontrast, according to research done by the weather risk management association and pricewaterhouse cooper,the european weather risk management market is $720 million over the same fiveyear period.3 again, the reasonfor this is the difference in the public information policies of the united states versus those of europe. europeangovernment agencies often assert copyright as well as the sui generis database protection right on taxpayerfundedinformation. in the united states, they do neither.a specific example is illustrative of this phenomenon. a particular firm requested the entire historic record ofmeteorological observations in the united states, from 1948 on, from the national oceanic and atmosphericadministrationõs (noaa) national climate data center (ncdc). following the policies of the paperworkreduction act of 1995 and office of management and budget (omb) circular a130, ncdc burned a stack ofcds for them, 15 gigabytes of data, and charged a little over $4,000, which covered dissemination costs, includingtime, effort, labor, burning the cds, postage, etc. this same research firm requested analogous data from thegerman government, their entire postwar meteorological record. the german government quoted $1.5 million.the volume of the data is significantly smaller, because it is one country in europe versus the entire united states.they also quoted 4,000 german marks, which now would be $2,500, for the historical record of only oneobserving station in germany. the united states has well over a thousand, the german government fewer than200. the interesting thing about this example is that, because this firm could not afford the german data, it did2pira international. 2000. commercial exploitation of europeõs public sector information. final report for the european commission,directorate general for the information society, pira international, ltd, university of east anglia and knowledgeview, ltd.3see also mr. weissõ powerpoint presentation from the symposium at http://www7.nationalacademies.org/biso/stisymposiumweiss.ppt.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 3131without. so not only did the german weather service quote an astronomical price for their data, but they did notmake a pfennig off the data because the research firm could not afford them.what is wrong with this picture? an example relating to basic scientific research paints it starkly. there is ateam at the india institute of technology in new delhi attempting to develop a method to predict monsoons bycomparing a long series of climate model output data with the record of actual observations of the monsoons overthe same time period. whether the onset, strength, and duration of the monsoons can be predicted with any skillis a fundamental research question. the team has access to all of the u.s. model reanalysis data for a 30yearperiod, essentially for free from our ncdc. the european center for mediumrange weather forecasts, which isthe equivalent to what we do here at noaa, quoted the researchers a price they could not afford. the researchteam asked if they could get free access to use the data for this basic scientific research purpose potentiallyaffecting the lives of over a billion people annually. they were refused.there is now an emerging realization in europe about the benefits of open access to publicsector information.the european commission commissioned that seminal study previously described. they have recently releasedtwo very interesting documents, one from the directorate general for the information society on a more openpublicsector information policy and one from the directorate general on the environment espousing an openpolicy for environmental data.4a draft directive on the òreuse and commercial exploitation of public sector documentsó is quite broad,encompassing most publicsector data and information not otherwise protected due to privacy or security considerations. it urges more transparency in the pricing practices of member statesõ agencies, but does not tackle theissue of dissemination cost pricing versus cost recovery pricing5 or the question of the propriety of restrictive termsintended to control downstream uses of the information. although weak in these areas, the draft directive doesseem to have significant support both at the commission level and, perhaps more significantly, in the europeanparliament.a draft directive on public access to environmental information could have a more significant shorttermimpact on european agency practices. it contains a strong definition of covered environmental information, whichextends to most information about the environment, including meteorological data quite specifically. the definition is significantly more specific than the 1990 environmental information directive it is intended to replace,which european meteorological services have construed as being limited to information relevant to environmentalregulatory enforcement and not to meteorological, climatological, or other data that merely describe the state of theenvironment. most importantly, it sets a cost of dissemination standard for the pricing of this information, whichwould preclude cost recovery pricing for data. it too is essentially silent regarding restrictions on downstream use.this draft environmental information directive is also garnering significant support in the european parliament.because it would replace an existing directive, it may be adopted more promptly than the draft publicsectorinformation directive, which is new.it is unclear, given the political realities and institutional interests in europe, what the practical effects thesedocuments will have, should they be adopted. but they certainly are a step in a forwardlooking direction.at the national level, the netherlands stands out as having adopted a policy for taxpayerfunded publicsectorinformation that is very similar to the u.s. paperwork reduction act and omb circular a130. indeed, much ofthe recent growth in the european weather risk management sector is attributable to a large group of contractsissued in the netherlands to ensure their construction industry from weatherrelated risk. the reason it exists isbecause the dutch government has adopted an open, unrestricted policy with regard to its historic meteorological4commission of the european communities. 2000. òproposal for a directive of the european parliament and of the council on the reuseand commercial exploitation of public sector documents.ó com207. brussels, july 5. council of the european union. 2002. òcommonposition adopted by the council on 28 january 2002 with a view to the adoption of a directive of the european parliament and of the councilon public access to environmental information and repealing council directive 90/313/eec.ó 11878/1/01 rev 1. brussels, january 29.5office of management and budget circular a130 section 8(a)(1) defines cost pricing versus cost recovery pricing: òagencies should . . .(c) set user charges for information dissemination products at a level sufficient to recover the cost of dissemination but no higher. they shallexclude from calculation of the charges costs associated with original collection and processing of the information.ó omb. 1996. circular no.a130, òmanagement of federal information resources,ó 61 federal register 6428, february 20 at http://www.whitehouse.gov/omb/circulars/a130/a130.html.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.132the role of s&t data and information in the public domainrecord and current observations. so it should not be surprising that the largest commercial meteorology firm ineurope also happens to be dutch. by contrast, the weather risk management market and the commercial meteorological sector are relatively much smaller in great britain, france, and germany.recent economic research, most of it european, reviewed in my paper òborders in cyberspace,ó6 leads tosome general conclusions. first, cost recovery is not the best approach to maximizing the economic value ofpublicsector information to society as a whole, not even from the viewpoint of government finances. again, forexample, the german weather service did not make $1.5 million by selling its historic record because the researchfirm could not afford it. second, prosperity effects are maximized when data are sold at marginal cost. directgovernment funding and free provision to all are favored with their contribution to national welfare maximized atthe point where marginal benefits equal marginal costs. that may sound like economistsõ rhetoric, but the recentresearch suggests it is true.in the area of atmospheric sciences, as i said, there is relatively little commercial meteorology or weather riskmanagement activity in europe because most european governments do not have openaccess policies, resulting indata not being readily, economically, and efficiently available. because the size of the european and u.s. economies are approximately the same, there is no reason for the european market not to grow to u.s. size with theaccompanying revenue generation and job growth. a significant contributor to these disparities is a difference ininformation policies between the united states and europe.luckily there is a slowly emerging recognition in europe that open access to government information iscritical to the information society, environmental protection, and economic growth. however, the slowly growingtrend toward more liberal policies faces opposition from government agencies themselves. for example, thegerman parliament recently rejected a modest freedom of information act. the political argument on which itwas rejected was that the public has no particular right to know about the internal workings of the government.great britain enacted its first freedom of information act in 2000. according to my colleagues in the britishpress, it has many loopholes, but at least they are moving in a positive direction.this concept of government commercialization and the idea of the òentrepreneurial bureaucratóñwhich iclaim is an oxymoronñdo not succeed in the face of economic realities and under open competition policies. mypaper documents a number of instances of anticompetitive practices by european government agencies.in sum, the research to date strongly suggests that open government information policies foster significant butnot easily quantifiable economic benefits to society. hence, the necessary impetus for adopting open informationpolicies worldwide may turn on further economic research to better quantify the benefits of open and unrestrictedpublicsector data. that economic research should prove relevant not only to the question of governmentalpolicies, but also to the larger questions about the value of the socalled òpublic domainó to society over all.6see peter weiss. 2002. òborders in cyberspace: conflicting public sector information policies and their economic impactssummaryreport,ó at http://weather.gov/sp/bordersreport.pdf.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 313313319potential effects of a diminishing public domain inbiomedical research datastephen hilgartner.in session 2, my colleague sherry brandtrauf presented some of the findings of our studies of data accesspractices among biomedical researchers.1 i envy her ability to talk about data that we actually collected, asopposed to my charge, which is to talk speculatively about what might happen if the public domain were todiminish in the biomedical area. i can only present guesses and conjectures about what might in fact occur.the central question that i want to examine is what might happen to research systems if the public domaindiminishes. i will confine my attention to òsmall scienceó biomedical research, which does not include areas likelarge clinical trials, which arguably are a form of big science, given their many collaborators and complexorganizational structures (e.g., gusto iii investigators, 1997). i will focus on areas such as molecular biology,crystallography, structural biology, and cell biology. in these fields, academic research is usually conducted on abenchtop scale by small research groups, and there are many independent laboratories working and often directlycompeting. in these particular scientific cultures, it is understood that the scientist who runs the laboratoryñtheòlab headó as he or she is calledñis the only person who can speak for the lab. other people, such as postdocs, canonly speak with special authorization from that person (knorr cetina, 1999). so this kind of science features aculture of autonomous, independent, highly entrepreneurial operators who work to build a research enterprise,produce findings, get grants, keep a lab going, and so forth. i want to speculate about what kinds of changes youmight expect in this cultural setting in science, based on what we know from ethnographic studies about the socialpractices that regulate access to data in these areas of science.although sherry brandtrauf described some of our work on how scientists control access to data andresources, there are several points i want to underline about data access practices. first, it is very important torecognize that these practices are specific to particular research communities. many scientists tend to talk about allscience as if it were uniform in the ways it handles data access, without recognizing the diversity of scientificcultures. thus, it is common to observe that òall scientists want to publish,ó as if this were a universal truth aboutscientists. indeed, there is no doubt that this statement is true at a very general level in all areas of academicscience, as well as in some industrial contexts (hicks, 1995). but the details of how publication is managedñwhatconstitutes òenoughó for a paper, which data are òreadyó to be published when, who decides, and how strategicconcerns about competition are addressedñvary tremendously across different scientific fields. scientists do not1see chapter 9 of these proceedings, òthe role, value, and limits of s&t data and information in the public domain on biomedicalresearch,ó by sherry brandtrauf.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.134the role of s&t data and information in the public domainsimply publish everything that they produce; they engage in strategic maneuvers about who is going to get accessto what data and materials under what terms and conditions. publication is only one move (albeit an extremelyimportant one) in an extremely complex process. the practices used to regulate access to data are quite differentin molecular biology, as opposed to highenergy physics, as opposed to a large clinical trial. there are differentexpectations and rules about control over the flow of data in those settings. as a result, any analysis of howchanges in the public domain might affect science must focus on particular research communities, not science asa whole. when i refer to scientists, i am referring to researchers working in molecular biology and other benchtopbiomedical fields that exhibit similar cultures.data access practicesbrandtrauf and i set out to create a theoretical framework and analytic method for comparing data accesspractices across diverse scientific fields. we concluded that such a framework must treat the category òdataó asproblematic; that is, one cannot focus on what the scientists themselves in a particular area regard as òdata,ó as iftheir notion of data were unambiguous and universal to all fields, but instead to consider the full range of forms ofdata and heterogeneous resources that researchers produce and use (hilgartner and brandtrauf, 1994).in molecular biology, these data and resources include all sorts of written inscriptions (such as sequence data)and biomaterials. they also include instruments, software, techniques, and a variety of òintermediate results.ó inthe laboratory, these entities are woven together into complicated assemblages. an isolated, single biologicalmaterial sitting alone in a test tube is a useless thing; to be scientifically meaningful, it must be linked using labelsand other inscriptions to the source of the sample and its particular characteristics. moreover, to use the material,one needs a laboratory equipped with an appropriate configuration of people, techniques, instruments, and so forth.as scientific work proceeds, materials and inscriptions are processed and reprocessed, so these assemblagescontinuously evolve, producing new data and materials (latour and woolgar, 1979). many of the items found ina laboratory can be found in any laboratory, but some of the itemsñespecially those toward the òleading edgesó ofthese evolving assemblagesñare available only in a few places, or perhaps only in one place. these scarce andunique items can convey a significant competitive edge. for example, the laboratory that first develops a usefulnew technique, the researcher who collects a particularly interesting set of dna samples, and the creator of apowerful new algorithm all end up controlling strategically important resources. they can enter into negotiationsabout collaborations and other exchanges from a strong position, owing to the value and scarcity of the resource.in smallscale biomedical research, with its many independent operators, a dynamic, invisible economy existsbelow the radar screen of what looking at the published literature reveals. there is a huge range of transactionsgoing on all the time. scientists have to decide whether to publish a result immediately or delay publication untilan even better result is achieved. in many areas, such as gene hunting, several research groups may be racing toreach the same goal, and an early publication from one group may help competing groups to catch up (hilgartner,1997). given such strategic considerations, scientists have to decide whether to publish right away, or to delaypublication, or to provide information on a limited basis to specially targeted audiences. often, they work tonegotiate agreements with the heads of other academic laboratories, or perhaps with commercial organizations.many of these exchanges entail at least temporary restrictions on publication. as scientists work to build collaborations, they seek to avoid arrangements that will cause them to become merely the provider of a òservice,ó asmolecular biologists put it, to another lab without benefiting themselves (knorr cetina, 1999). sometimes researchers provide these services expecting a quid pro quo later. sometimes they provide them out of the goodnessof their hearts. sometimes they provide them because funding agencies or other policy makers encourage them todo so (hilgartner, 1998). but complex negotiations, replete with strategic gamesmanship and uncertainty, areroutine in smallscale biomedical research.public domainshaving briefly characterized the strategic role of data and the wide variety of transactions that surround dataand resources, it is finally time to turn to my main question: what would happen to this area of science if the publicthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 3135domain were to diminish? to address this question, it is first necessary to consider how the public domain fits intothis research area. the public domain is a complex concept, and it is important to recognize that scientists may notthink about this concept in precisely the same way that lawyers and legal scholars do. legal categories permeatescientistsõ consciousness, but not in the systematic, formalized ways that one might find in a law review article.for this reason, when scientists talk about òthe public domain,ó the concept that they have in mind may not neatlymap onto a formal, legal definition. when a scientist asks whether a resource is in the public domainñor, morecolloquially, òis that public?óñwhat they mean is something like òcan i get it? can i use it? what do i have to doto get it and what encumbrances will restrict my use of it?ó in other words, the central issues are usuallyavailability and the terms of access.legal ownership is only one of many things that constitute availability and shape the terms of access. soscientists deal less with the public domainñif we construe that as a legal category produced in court decisions,statutes, briefs, and formal legal negotiationsñthan with resources that are more, and sometimes less, òavailable.óshifting from a legal concept of the public domain to this more pragmatic concept centered on access directs ourattention not only to formal ownership, but also to the practical difficulties of obtaining data and resources. whenmolecular biologists refer to some data and resources as òpublicó they typically mean that they are readilyavailable to any scientist. i will refer to data that are public in this sense as òpublic resources.ó important publicresources are found in many domains: from scientific literatures, to internetaccessible databases, to biomaterialsrepositories, to stock centers that house strains of organisms (fujimura, 1996; kohler, 1994).scientists also may regard instruments as public resources if they are available at reasonable prices on openmarkets. in contrast, some instruments are not public resources. for example, access to beam time on a synchrotron may be allocated by peer review (whitedepace et al., 1992). similarly, an instrument that is not yetcommercially available might be offered to selected scientists for beta testing through a special arrangement thatprovides early access: òwe have this new instrument,ó says the firm, òand you want to try it out. well, you get touse it first, but let us know how you like it, and if you like it, tell your friends and cite our product in your publishedwork.ó the point is that so long as the instrument is for sale (at a reasonable price), scientists often describe it asòpublic,ó even though the instrument in fact is probably someoneõs intellectual property.as the above discussion suggests, scientists do not deal with an abstract public domain; they interact withdiverse public domains, including open literatures, open databases, open materials repositories, and open markets.the plural termñpublic domainsñis important here, both to emphasize their diversity and to underline how thesepublic domains are not coterminous with abstract definitions of the public domain. these public domains are whatwe should consider when thinking about the effects on scientific research of increasing privatization and restriction of domains that were once public.effects of diminishing public domainsbefore launching into a speculative discussion of the possible effects of diminishing public domains, one mustask a crucial question: can public domains really diminish? one might be forgiven for suspecting that they cannot.after all, the scientific literature continues to expand rapidly, and biomedical science is experiencing an unprecedented deluge of biomolecular data (lenoir, 1999). for example, the volume of dna sequence informationavailable in public databases has been increasing exponentially, and it probably will continue to grow rapidly forawhile. (of course, at the same time, we know that the amount of sequence data available in private databases isalso growing, although it is harder to estimate how rapidly, because such information is private.)even given an expanding literature and an explosion of data, public domains clearly can diminish in at leastsix ways. first, absolute reductions in public domains occur when particular items are removed from them forvarious reasons. second, items that were previously conceived of as things to be shared openly among scientistscan be redefined as proprietary (mackenzie et al., 1990). third, there can be delays of the release of informationinto public domains. we know that such delays occur with some regularity in such highimpact fields as genetics,and this constitutes at least a shortterm limitation of the scope of such public domains as the scientific literature(hilgartner, 1997; campbell et al., 2002). fourth, items in public domains can have new encumbrances attached tothem. these encumbrances might include publication restrictions, reachthrough licenses and similar mechanisms,the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.136the role of s&t data and information in the public domainor just the transaction costs of negotiating access. fifth, some items available for purchase can be acquired only athigh prices. finally, the relative size of public domains versus private domains can diminish, even when bothpublic and private domains are growing. such reductions in the relative size of public domains arguably constitutea form of diminishment. in short, there are a variety of ways that public domains can remain òpublicó in the sensethat i have described, but at the same time diminish.there are three different orders of effects that you might expect if public domains diminish. the first orderincludes direct effects on the transactions that drive this fastmoving world of exchanges among biomedicalscientists. secondorder effects involve changes in research communities and cultures and how they manage dataaccess, such as a shift toward more restrictive practices. thirdorder effects include the ways that diminishingpublic domains might alter the position of science in the wider polity.let us turn initially to direct effects on transactions. as a starting point for considering these effects, imaginea small academic laboratory that engages in several kinds of transactions. it obtains some inputs for its researchfrom public domains; it releases some of its outputs to public domains, such as the literature; and it gets someinputs from (and deploys some outputs in) restrictedaccess transactions. òi give you this, you give me that, maybethis deal is more to my advantage than to yours, but there will be another exchange later and that one will work outthe other way.ó importantly, whenever this laboratory acquires an entity from a public domain, it immediatelybegins to process it, manipulate it, and combine it with other data and materials. through this processing thelaboratory reprivatizes the entityñor, more precisely, produces new entities that end up under its exclusivecontrol. put otherwise, laboratories not only release material into public domains, but they also continuallyincorporate entities from public domains into their own private domains. viewed in this light, laboratories emergenot only as mechanisms for creating new knowledge, but also as devices for redrawing the contours of the publicprivate boundary.what effects might diminishing public domains have on such a laboratoryõs transactions? if public domainsdiminish, then there will be less material in them, at least in relative terms. thus, we might expect that perhaps ourimaginary laboratory would acquire fewer inputs from public domains. if fewer inputs are obtained from publicdomains, then the laboratory must get them from some other source (or do without). most likely, it will acquire ahigher percentage of its inputs from restrictedaccess transactions, and this will lead the laboratory into negotiations with people who hold resources privately.of course, these restrictedaccess transactions will most likely entail quid pro quos, such as confidentialityagreements or rights to prepublication review. as a result, one might expect the laboratory to release fewer of itsoutputs into the public domain, or at least to do so later. more generally, if many laboratories became increasinglyentangled in proprietary agreements, this might drag down the quality of public domains on a wider scale, forexample, by limiting their content or causing delays in the introduction of new information. indeed, one canimagine a synergistic process that would increasingly lead laboratories to rely on restrictedaccess transactions,producing a progressive impoverishment of the public domain that would, in turn, encourage further reliance onrestrictedaccess transactions. if things were to go very badly, such effects could reduce the vitality and creativityof biomedical science for some of the same reasons that the lack of a strong public domain restricts the creative useof european weather data.2i want to move on to possible secondorder effects. what kinds of effects might diminishing public domainshave on research communities and research cultures? research communities play a key role in constituting publicdomains in science. if you think of public domains not as an abstract legal category, but instead as material entitiesproduced actively through social action, then research communities are central players in building them. it takes atremendous amount of work to make science and scientific information òpublicó (callon, 1994). research communities accomplish this in part by building institutional arrangements that lead individual laboratory scientists to putthings into public domains. the published literature itself is the solidified sediment of a huge set of institutionalarrangements that give academic scientists incentives to publish information. there were not always scientificjournals; their history goes back to the enlightenment, and over time, they have grown into a central scientificinstitution. today, a complex set of institutional arrangementsñfrom the tenure system to the research funding2see chapter 18 of these proceedings, òpotential effects of a diminishing public domain in environmental information,ó by peter weiss.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 3137system to a socialization process that makes publication important to a scientistõs identityñencourages researchers to publish.building such institutions entails creating informal expectations and formal rules, and these expectations andrules are historical achievements, not timeless, stable features of science. social and technological change createsopenings for institutional innovations that can influence the contours of public domains. the emergence of dnasequence databases, a new kind of public domain in science, provides a good example. dna sequencing began ina small way in the 1970s, and a visionary group of scientists conceived of the los alamos sequence library(lasl) at the end of the decade. lasl gathered previously published sequences, which at that point werepublished in print in scientific journals, and prepared them in machinereadable form to permit mathematicalanalysis. in this way, these scientists created a new kind of public domainñthe sequence databaseñfor biology(cinkosky et al., 1991; hilgartner, 1995).lasl later evolved into genbank. early in the history of genbank, sequence data began accumulating so fastthat journals became reluctant to publish it. genbank decided to ask scientists to submit sequences directly to thedatabase, but the incentive structures did not encourage them to do so. sending in sequence data took time andrequired effort, and there was little payoff in terms of scientific credit for submitting sequences. only later didgenbank, with help from the relevant scientific journals, negotiate a new deal to compel scientists to submitsequence data to the public databases (hilgartner, 1995, p. 253). a policymaking journal publication contingenton database submission was first implemented by nucleic acids research in 1988, and many other journalsfollowed suit (nucleic acids research, 1987; mccain, 1995). this example illustrates how new institutionalarrangements, combined with technological developments such as dna sequencing and the internet, can bedeployed to constitute important new public domains in science.however, the ability to seize such opportunities depends on the existence of a scientific culture conducive tocreating collective resources. excessive concern with the protection of intellectual property can erect barriers toestablishing new public domains. to illustrate this point, consider a counterfactual example. imagine that youwere trying to set up the first sequence database today. one proposed plan (which follows closely the model oflasl) might be to copy all the dna sequences from the published literature, draw them together in machinereadable form, and provide access to the entire collection on the internet. but in this postbayhdole era, withmore than two decades of increasing commercialization of biology, would such a proposal be taken seriously?perhaps not. and if it were, there is little doubt that one would need to convene a small army of universitytechnology transfer officials, lawyers, and technology licensing specialists to negotiate about ownership of thedatabase.of course, even given the increasing importance of proprietary regimes in biomedical science, commercialentities may at times decide to create public domains with unrestricted access. in session 2, robert cookdeeganmentioned the example of dbestñthe expressed sequence tag (est) database funded by merck.3 michaelmorgan will discuss the single nucleotide polymorphism consortium,4 which is a good example of a situation inwhich large pharmaceutical companies funded the development of a publicdomain resource (in part to preventother companies from creating monopolies over that resource). clearly, public domains can still be constituted ina commercialized culture, but the question then becomes, how often will this happen? can the scientific community safely assume that large corporations will create public domains in the future whenever they are needed? ithink not.i want to close by briefly mentioning possible thirdorder effects. given the centrality of scientific knowledgeand science advice to many critical public issues, it is worth considering how changes in public domains mightaffect the position of science in the wider polity. arguably, the rapid commercialization and privatization ofscience has the potential to undermine the enlightenment notion of science as a special form of knowledge, opento public scrutiny and collective verification (shapin and schaffer, 1985). if fundamental data pertaining to3see chapter 11 of these proceedings, òthe urge to commercialize: interactions between public and private research and development,óby robert cookdeegan.4see chapter 28 of these proceedings, ònew paradigms in industry: the single nucleotide polymorphism (snp) consortium,ó by michaelmorgan.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.138the role of s&t data and information in the public domainimportant public issues get caught up in proprietary arrangements that make it difficult for people to access them,reanalyze them, criticize them, or incorporate them into critiques of things going on in the world, then the notionthat science is public knowledge would be seriously threatened.perhaps such effects are the hardest to predict and the hardest to be certain about. but it is clearly worth askinghow far science can move in the direction of privatization before people stop perceiving it as a credible anddisinterested source of public knowledge, and instead begin to think of science as just another private interestñone that cannot be scrutinized and cannot be counted on to speak the truth.referencescallon, michel. 1994. òis science a public good?ó science, technology, and human values, 19(4): 395424.campbell, eric g., brian r. clarridge, manjusha gokhale, lauren birenbaum, stephen hilgartner, neil a. holtzman, anddavid blumenthal. 2002. òdata withholding in academic genetics: evidence from a national survey,ó journal of theamerican medical association, vol. 287(4), january 23/30, pp. 473480.cinkosky, m. j., j. w. fickett, p. gilna, and c. burks. 1991. òelectronic data publishing and genbank,ó science, vol. 252, pp. 12731277.fujimura, joan h. 1996. crafting science. harvard university press, cambridge, ma.gusto iii investigators. 1997. òa comparison of reteplase with alteplase for acute myocardial infarction,ó new englandjournal of medicine, 337(18): 11181123.hicks, diana. 1995. òpublished papers, tacit competencies and corporate manage of the public/private character of knowledge,ó industrial and corporate changes, vol. 4(2), pp. 401424.hilgartner, stephen. 1995. òbiomolecular databases: new communication regimes for biology?ó science communication, vol.17(2), pp. 24063.hilgartner, stephen. 1997. òaccess to data and intellectual property: scientific exchange in genome research,ó in nationalresearch councilõs intellectual property and research tools in molecular biology: report of a workshop. nationalacademy press, washington, d.c.hilgartner, stephen. 1998. òdata access policy in genome research.ó pp. 20218 in arnold thackray, ed., private science:biotechnology and the rise of the molecular sciences, university of pennsylvania press, philadelphia.hilgartner, stephen and sherrybrandtrauf. 1994. òdata access, ownership, and control: toward empirical studies of accesspractices,ó knowledge: creation, diffusion, utilization, vol. 15(4), pp. 35572.knorr cetina, karin. 1999. epistemic cultures, harvard university press, cambridge, ma.kohler, robert e. 1994. lords of the fly: drosophila genetics and the experimental life. university of chicago press,chicago, il.latour, bruno and steve woolgar. 1979. laboratory life. sage publications, beverly hills, ca.lenoir, timothy. 1999. òshaping biomedicine as an information science.ó in proceedings of the 1998 conference on thehistory and heritage of science information systems, mary ellen bowden, trudi bellardo hahn, and robert v. williams,eds., asis monograph series, information today, inc., medford, nj, pp. 2745.mackenzie, michael, peter keating, and alberto cambrosio. 1990. òpatents and free scientific information in biotechnology:making monoclonal antibodies proprietary,ó science, technology, and human values, vol. 15(1), pp. 6583.mccain, k.w. 1995. òmandating sharing: journal policies in the natural sciences,ó science communication, vol. 16, pp. 403436.nucleic acids research. 1987. òdeposition of nucleotide sequence data in the data banks,ó nucleic acids research, vol.15(18), front matter.shapin, steven and simon schaffer. 1985. leviathan and the airpump, princeton university press, princeton, nj.whitedepace, susan, nicholas f. gmur, jean jordansweet, lydia lever, steven kemp, barry karlin, andrew ackerman,and jack presses, eds. 1992. national synchrotron light source: experimenterõs handbook, national technical information service, springfield, va.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.139session 4: responses by the researchand education communities inpreserving the public domain andpromoting open accessthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 414114120discussion frameworkjerome reichman.a contractually reconstructed research commonsfor science and innovationthe presentations in session 2 of this symposium described the growing efforts under way to privatize andcommercialize scientific and technical information that was heretofore freely available from the public domain oron an òopenaccessó basis. if these pressures continue unabated, they will result in the disruption of longestablished scientific research practices and in the loss of new research opportunities that digital networks andrelated technologies make possible. we do not expect these negative synergies to occur all at once, however, butrather to manifest themselves incrementally, and the lost opportunity costs they are certain to engender will bedifficult to discern.particularly problematic is the uncertainty regarding the specific type of database protection that congressmay enact and any exceptions favoring scientific research and education that such a law might contain. as we havetried to demonstrate, however, the economic pressures to privatize and commercialize upstream data resourceswill continue to grow in any event. moreover, legal means of implementing these pressures already exist, regardless of the adoption of a sui generis database right. therefore, given enough economic pressure, that which couldbe done to promote strategic gains will likely be done by some combination of legal and technical means.if one accepts this premise, then the enactment of some future database law could make it easier to imposerestrictions on access to and use of scientific data than at present, but the absence of a database law or theenactment of a lower protectionist version would not necessarily avoid the imposition of similar restrictions byother means. in such an environment, the existing elements of risk or threat to the sharing norms of public sciencecan only increase, unless the scientific community adopts countervailing measures.we, accordingly, foresee a transitional period in which the negative trends identified above will challenge thecooperative traditions of science and the public institutions that have reinforced those traditions in the past, withuncertain results. in this period, a new equilibrium will emerge as the scientific community becomes progressivelymore conflicted between their private interests and their communal needs for data and technical information as apublic resource. this transitional period provides a window of opportunity that should be used to analyze thepotential effects of a shrinking public domain and to take steps to preserve the functional integrity of the researchcommons.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.142the role of s&t data and information in the public domainthe challenge to sciencethe trends described above could elicit one of two types of responses. one is essentially reactive, in which thescientific community adjusts to the pressures as best it can without organizing a response to the increasingencroachment of a commercial ethos upon its upstream data resources. the other would require science policy toaddress the challenge by formulating a strategy that would enable the scientific community to take charge of itsbasic data supply and to manage the resulting research commons in ways that preserve its publicgood functionswithout impeding socially beneficial commercial opportunities.under the first alternative, the research community can join the enclosure movement and profit from it. thus,both universities and independent laboratories or investigators that already transfer publicly funded technology tothe private sector can also profit from the licensing of databases. in that case, data flows supporting public sciencewill have to be constructed deal by deal with all the transaction costs this entails and with the further risk ofbargaining to impasse. the ability of researchers to access and aggregate the information they need to producediscoveries and innovations may be compromised both by the shrinking dimensions of the public domain and bythe demise of the sharing ethos in the nonprofit community, as these same universities and research centersincreasingly see each other as competitors rather than partners in a common venture. carried to an extreme, thiscompetition of research entities against one another, conducted by their respective legal offices, could obstruct anddisrupt the scientific data commons.to avoid these outcomes, the other option is for the scientific community to take its own data managementproblems in hand. the idea is to reinforce and recreate, by voluntary means, a public space in which the traditionalsharing ethos can be preserved and insulated from the commoditizing trends identified above. in approaching thisoption, the communityõs assets are the formal structures that surround federally funded data and the ability offederal funding agencies to regulate the terms on which data are disseminated and used. the first programmaticresponse would look to the strengthening of existing institutional, cultural, and contractual mechanisms thatalready support the research commons, with a view to better addressing the new threats to the public domainidentified above. the second logical response is collectively to react to new information laws and related economic and technical pressures by negotiating contractual agreements between stakeholders to preserve and enhance the research commons.as matters stand, the u.s. government generates a vast public domain for its own data by a creative use ofthree instruments: intellectual property rights, contracts, and new technologies of communication and delivery. bylong tradition, the federal government has used these instruments differently from the rest of the world. it waivesits property rights in governmentgenerated information, it contractually mandates that such information should beprovided at the marginal cost of dissemination, and it has been a major proponent and user of the internet to makeits information as widely available as possible. in other words, it has deliberately made use of existing intellectualproperty rights, contracts, and technologies to construct a research commons for the flow of scientific data as apublic good. the unique combination of these instruments is a key aspect of the success of our national researchenterprise.now that the research commons has come under attack, the challenge is not only to strengthen a demonstrablysuccessful system at the governmental level, but also to extend and adapt this methodology to the changinguniversity environment and to the new digitally networked research environment. in other words, universities, notforprofit research institutes, and academic investigators, all of whom depend on the sharing of data, will have tostipulate their own treaties or contractual arrangements to ensure unimpeded access to, and unrestricted use of,commonly needed raw materials in a public or quasipublic space, even though many such institutions or actorsmay separately engage in transfers of information for economic gain. this initiative, in turn, will require thefederal government as the primary funderñacting through the science agenciesñto join with the universities andscientific bodies in an effort to develop suitable contractual templates that could be used to regulate or influencethe research commons.implementing these proposals would require nuanced solutions tailormade to the needs of government,academia, and industry in general and to the specific exigencies of different scientific disciplines. the followingsections describe our proposals for preserving and promoting the publicdomain status of governmentgeneratedthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4143scientific data and of governmentfunded and privatesector scientific data, respectively. we do not, however,develop detailed proposals for separate disciplines and subdisciplines here, as these would require additionalresearch and analysis.the government sectorto preserve and maintain the traditional publicdomain functions of governmentgenerated data, the unitedstates will have to adjust its existing policies and practices to take account of new information regimes and thegrowing pressures for privatization. at the same time, government agencies will have to find ways of coping withbilateral data exchanges with other countries that exercise intellectual property rights in their own data collections.we do not mean to imply a need to totally reinvent or reorganize the existing universe in which scientific dataare disseminated and exchanged. the opposite is true. as we have explained, a vast public domain for the diffusionof scientific datañespecially governmentgenerated datañexists and continues to operate, and much governmentfunded data emerging from the academic communities continues to be disseminated through these wellestablished mechanisms.facilities for the curation and distribution of governmentgenerated data are well organized in a number ofresearch areas. they are governed by longestablished protocols that maintain the function of a public domain, andin most cases ensure open access (either free or at marginal cost) and unrestricted use of the relevant datacollections. these collections are housed in brickandmortar data repositories, many of which are operateddirectly by the government, such as the nasa national space science data center. other repositories are fundedby the government to carry out similar functions, such as the archives of the hubble space telescope scienceinstitute at johns hopkins university.under existing protocols, most governmentoperated or governmentfunded data repositories do not allow theconditional deposits that look to commercial exploitation of the data in question. anyone who uses the datadeposited in these holdings can commercially exploit their own versions and applications of them without needingany authorization from the government. however, no such uses, including costly valueadding uses, can removethe original data from the public repositories. in this sense, the valueadding investor obtains no exclusive rights inthe original data, but is allowed to protect the creativity and investment in the derived information products.the ability of these government institutions to make their data holdings broadly available to all potentialusers, both scientific and other, has been greatly increased by direct online delivery. however, this potential isundermined by a perennial and growing shortage of government funds for such activities; by technical andadministrative difficulties that impede longterm preservation of the exponentially increasing amounts of data tobe deposited; and by pressures to commoditize data, which are reducing the scope of government activity and tendto discourage academic investigators from making unconditional deposits of even governmentfunded data tothese repositories.the longterm health of the scientific enterprise depends on the continued operations of these public datarepositories and on the reversal of the negative trends identified earlier in this chapter. here the object is topreserve and enhance the functions that government data repositories have always played, notwithstanding themounting pressures to commoditize even governmentgenerated data.implementing any recommendations concerning governmentgenerated data will, of course, require adequatefunding, and this remains a major problem. in most cases, however, it is not the big allocations needed to collector create data that are lacking; it is the relatively small but crucial amounts to properly manage, disseminate, andarchive data already collected that are chronically insufficient. these shortsighted practices deprive taxpayers ofthe longterm fruits of their investments in the scientific enterprise. science policy must give higher priority toformulating workable measures to redress this imbalance than it has in the past.policymakers should also react to the pressures to privatize governmentgenerated research data by devisingobjective criteria for ascertaining when and how privatization truly benefits the public interest. at times,privatization will advance the public interest because the private sector can generate particular datasets moreefficiently or because other considerations justify this approach. very often, however, the opposite will be true:especially when the costs of generating the data are high in relation to known, shortterm payoffs. two recentthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.144the role of s&t data and information in the public domainnational research council studies have attempted to formulate specific criteria for evaluating proposedprivatization initiatives concerning scientific data.1 the science agencies should make the formulation of suchcriteria for different areas of research a top agenda item. in so doing, the agencies also need to analyze the resultsof past privatization initiatives with a view to assessing their relative costs and benefits.once the validity of any given privatization proposal has been determined by appropriate evaluative criteria,the next crucial step is to build appropriate, publicinterest contractual templates into that deal to ensure thecontinued operations of a research commons. the public research function is too important to be left as anafterthought. it must figure prominently in the planning stage of every legitimate privatization initiative preciselybecause the data would previously have been generated at public expense for a public purpose. after all, theprocess of privatization aims to shift the commercial risks and opportunities of data production or dissemination toprivate enterprise under specified conditions that promote efficiency and economic growth. however, the processshould not pin the functions of the research enterprise to the success of any given commercial venture; and it mustnot allow such ventures to otherwise compromise these functions by the charging of unreasonable prices or by theimposition of contractual conditions unduly restricting public, scientific uses of the data in question.there are two situations in which model contractual templates, developed through interagency consultations,could play a critical role. one is where data collection and dissemination activities previously conducted by agovernment entity are transferred to a private entity. the other is where the government licenses data collected bya private entity for public research purposes. in both cases, the underlying contractual templates should implementthe following researchfriendly legal guidelines:(1)a general obligation not to legally or technically hinder access to the data in question for nonprofit scientificresearch and educational purposes;(2)a further obligation not to hinder or restrict the reuse of data lawfully obtained in the furtherance of nonprofitscientific research activities; and(3)an obligation to make data available for nonprofit research and educational purposes on fair and reasonableterms and conditions, subject to impartial review and arbitration of the rates and terms actually applied, to avoidresearch disasters such as the landsat deal in the 1980s.in cases where the public data collection activity is transferred to the private sector, care must be taken to ensurethat the private entity exercises any underlying intellectual property rights, especially some future database right, ina manner consistent with the public interestñincluding the interests of science. to this end, a model contractualtemplate should also include a comprehensive misuse provision like that embodied in section 106 of h.r. 1858:sec. 106. limitations on liability(b)misuseña person or entity shall not be liable for a violation of section 102 if the person or entity benefitingfrom the protection afforded a database under section 102 misuses the protection. in determining whether a personor entity has misused the protection afforded under this title, the following factors, among others, shall be considered:(1)the extent to which the ability of persons or entities to engage in the permitted acts under this title has beenfrustrated by contractual arrangements or technological measures;(2)the extent to which information contained in a database that is the sole source of the information containedtherein is made available through licensing or sale on reasonable terms and conditions;(3)the extent to which the license or sale of information contained in a database protected under this title has beenconditioned on the acquisition or license of any other product or service, or on the performance of any action,not directly related to the license or sale;(4)the extent to which access to information necessary for research, competition, or innovation purposes have beenprevented;(5)the extent to which the manner of asserting rights granted under this title constitutes a barrier to entry into therelevant database market; and1see national research council. 1997. bits of power: issues in global access to scientific data, national academy press, washington,d.c., and national research council. 2001. resolving conflicts arising from the privatization of environmental data, national academypress, washington, d.c.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4145(6)the extent to which the judicially developed doctrines of misuse in other areas of the law may appropriately beextended to the case or controversy.the larger principle is that, in managing its own public research data activities, the government can and shoulddevelop its own database law in a way that promotes science without unduly impeding commerce. this principleis not new; the government already has a workable information regime, as described in the first session of thissymposium. however, the government will need to adapt that regime to the pressures arising from the new highprotectionist legal environment and ensure that its agencies are consistently applying rational and harmonizedpublicinterest principles. otherwise, the traditional publicdomain functions of governmentgenerated data couldbe severely compromised. this in turn would violate the governmentõs fiduciary responsibilities to taxpayers andraise conflicts of interest and questions concerning sham transactions.the academic sectorin putting forward our proposals concerning the preservation of a research commons for governmentfunded data,it is useful to follow the distinction between a zone of formal data exchanges and a zone of informal data exchangespreviously discussed in session 1.2 consistent with our earlier analysis, we emphasize that the ability of governmentfunding agencies to influence data exchange practices will be much greater in the formal than the informal zone.the zone of formal data exchangeswhere no significant proprietary interests come into play, the optimal solution for governmentgenerateddata and for data produced by governmentfunded research is a formally structured, archival data center alsosupported by government. as discussed, many such data centers have already been formed around largefacilityresearch projects. building on the opportunities afforded by digital networks, it has now become possible toextend this timetested model to highly distributed research operations conducted by groups of academics indifferent countries.the traditional model entails a òbricksandmortaró centralized facility into which researchers deposit theirdata unconditionally. in addition to academics, contributors may include government and even privatesectorscientists, but in all cases the true publicdomain status of any data deposited is usually maintained. examplesinclude the national center for biotechnology information, directly operated by the national institutes of health,and the national center for atmospheric research, operated by a university consortium and funded primarily bythe national science foundation (nsf).a second, more recent model, enabled by improved internet capabilities, also envisions a centralized administrative entity, but this entity governs a network of highly distributed smaller data repositories, sometimes referredto as ònodes.ó taken together, the nodes constitute a virtual archive whose relatively small central office overseesagreed technical, operational, and legal standards to which all member nodes adhere. examples of such a decentralized network, which operate on a publicdomain basis, are the nasa distributed active archive centers underthe earth observing system program and the nsffunded long term ecological research network.these virtual archives, known as òfederatedó data management systems, extend the benefits and practices ofa centralized brickandmortar repository to the outlying districts and suburbs of the scientific enterprise. theyhelp to reconcile practice with theory in the sense that the investigatorsñmost of whom are funded by governmentanywayñare encouraged to deposit their data in such networked facilities. the very existence of these formallyconstituted networks thus helps to ensure that the resulting data are effectively made available to the scientificcommunity as a whole, which means that the social benefits of public funding are more perfectly captured and thesharing ethos is more fully implemented.at the same time, some of the existing ònetworks of nodesó have already adopted the practice of providingconditional availability of their data: a feature of considerable importance for our proposals. by òconditionalavailability,ó we mean that the members of the network have agreed to make their data available for public science2see chapter 1 of these proceedings, òsession 1 discussion framework,ó by paul uhlir.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.146the role of s&t data and information in the public domainpurposes on mutually acceptable terms, but they also permit suppliers to restrict uses of their data for otherpurposes, typically with a view to preserving their commercial opportunities.3the networked systems thus provide prospective suppliers with a mix of options to accommodate depositsranging from true publicdomain status to fully proprietary data that has been made available subject to rules themember nodes have adopted. the element of flexibility that conditional deposits afford make these federated datamanagement systems particularly responsive to the realities of presentday university research in areas of scientificinvestigation where commercial opportunities are especially prominent.basic proposalsour first proposition is that the government funding agencies should encourage unconditional deposits ofresearch data, to the fullest extent possible, into both centralized repositories and decentralized network structures.the obvious principle here is that, because the data in question are government funded, improved methods shouldbe devised for capturing the social benefits of public funding, lest commercial temptations produce a kind of defacto freeride at the taxpayersõ expense.when unconditional deposits occur in a true publicdomain environment removed from proprietary concerns,the legal mechanisms to implement these expanded data centers need not be complicated. single researchers orsmall research teams could contribute their data to centers serving their specific disciplines, with no stringsattached other than measures to ensure attribution and professional recognition. alternatively, as newly integratedscientific communities organize themselves, they could seek government help in establishing new data centers ornodes that would accept unrestricted deposits on their behalf. private companies could also contribute to a truepublicdomain model or organize their own variants of such a model; these practices should be encouraged as amatter of public policy.if the unrestricted data were deposited in federal governmentsponsored repositories, existing federal information law and associated protocols would define the public access rights. the maintenance of publicinterest datacenters in academia, however, is problematic without government support. these data centers can become partlyor fully selfsupporting through some appropriate fee structure, but resorting to a fee structure based on paymentsof more than the marginal cost of delivery quickly begins to defeat the public good and positive externalityattributes of the system, even absent further use restrictions.leaving aside the funding issue, the deeper question that this first proposal raises is how the universities andother nonprofit research entities will resolve the potential conflict between the pressures to disclose and deposit theirgovernmentfunded data and the valuable proprietary interests that are increasingly likely to surface in a highprotectionist intellectual property environment. one cannot ignore the risk that the viability and effectiveness of thesecenters could be undermined to the extent that the beneficiaries of government funding can resist pressures to furtherimplement the sharing ethos and even decline to deposit their research data because of their commercial interests.despite their educational missions and nonprofit status, both universities and individual academics are increasingly prone to regard their databases as targets of opportunity for commercialization. this tendency willbecome more pronounced as more of the financial burden inherent in the generation and management of scientificdata is shouldered by the universities themselves or by cooperative research arrangements with the private sector.in this context, the universities are likely to envision split uses of their data and will prefer to make them availableon restricted conditions. they will logically distinguish between uses of data for basic research purposes by othernonprofit institutions and purely commercial applicants. even this apparently clearcut distinction might breakdown, moreover, if universities treat databases whose principal user base is other nonprofit research institutions ascommercial research tools.the point is that the universities may not want to deposit data in designated repositories, even with government support, unless the repositories can accommodate these interests, and the repositories could compromisetheir public research functions if they are held hostage to too many demands of this kind. the same potential3an example of an international network that operates on the basis of conditional deposits is the global biodiversity information network,headquartered in denmark, which is substantially supported by u.s. government funding. for additional information, see http://www.gbif.org.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4147situation exists for individual databases made available by universities (as opposed to their contributions to larger,multisource repositories). this state of affairs will accordingly require still more creative initiatives to parry theeconomic and legal pressures on universities and academic researchers to withhold data.with these factors in mind, our second major proposal is to establish a zone of conditionally available data toreconstruct and artificially preserve functional equivalents of a public domain. this strategy entails using propertyrights and contracts to reinforce the sharing norms of science in the nonprofit, transinstitutional dimension withoutunduly disrupting the commercial interests of those entities that choose to operate in the private dimension.to this end, the universities and nonprofit research institutions that depend on the sharing ethos, together withthe government science funding agencies, should consider stipulating suitable òtreatiesó and other contractualarrangements to ensure unimpeded access to commonly needed raw materials in a public or quasipublic space.from this perspective, one can envision the accumulation of shared scientific data as a community asset held in acontractually reconstructed research commons to which all researchers have access for purposes of public scientific pursuits.one can further imagine that this public research commons exists in an everexpanding òhorizontal dimension,ó as contrasted with the commercial operations of the same data suppliers in what we shall call the òverticalóor private dimension. the object of the exercise would be to persuade the government, as primary funder, to joinwith universities and scientific bodies in an effort to develop suitable contractual templates that could be used toregulate the research commons. these templates would ensure that data held in the quasipublic or òhorizontalódimension would remain accessible for scientific purposes and could not be removed or otherwise appropriated tothe private or òverticaló dimension. at the same time, these contractual arrangements would expressly contemplatethe possibilities for commercial exploitation of the same data in the private or vertical dimension, and they wouldclarify the depositorõs rights in that regard and ensure that the exercise of those rights did not impede or disruptaccess to the horizontal space for research purposes.ancillary considerationsin fashioning these proposals, we are aware that considerable thought has recently been given to the construction of voluntary social structures to support the production of large, complex information projects. particularlyrelevant in this regard are the opensource software movement that has collectively developed and managed thegnu/linux operating system and the creative commons, which seeks to encourage authors and artists toconditionally dedicate some or all of their exclusive rights to the public domain.4 in both these pioneeringmovements, agreed contractual templates have been experimentally developed to reverse or constrain the exclusionary effects of strong intellectual property rights.although neither of these models was developed with the needs of public science in mind, both providehelpful examples of how universities, federal funding agencies, and scientific bodies might contractually reconstruct a research commons for scientific data that could withstand the legal, economic, and technological pressureson the public domain identified in this paper. in what follows, we draw on these and other sources to propose thecontractual regulation of governmentfunded data in two specific situations: (1) where governmentfunded, universitygenerated data are licensed to the private sector and (2) where such data are made available to otheruniversities for research purposes.licensing governmentfunded data to the private sectorin approaching this topic, one must consider that the production of scientific databases in academia is notalways dominated by activities funded by the federal government. it may also entail funding by universitiesthemselves, foundations, and the private sector. although these sources seem likely to grow in the future, especially if congress adopts a database protection right, the governmentõs role in funding academic data production4see chapter 23 of these proceedings, ònew legal approaches in the private sector,ó by jonathan zittrain, for a description of the creativecommons initiative.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.148the role of s&t data and information in the public domainwill nonetheless remain a major factor, at least in the near term (although its role will vary from project to project).as we discussed earlier in this symposium, this presence gives the federal funding agencies unique opportunitiesto influence the datasharing policies of its beneficiary institutions.ideally, funders and universities would agree on the need to maintain the functions of a public domain to thefullest extent possible, to provide open access for nonprofit research activities, and to encourage efficient technological applications of available data. at the same time, technological applications and other opportunities forcommercial exploitation of certain types of databases will push the universities to enter into private contractualtransactions that, if left totally unregulated, could adversely affect the availability of the relevant data for publicresearch purposes. the reconciliation of the conflicts between enhancing the public research interests and freedomof contract will require carefully formulated policies and institutional adjustments.assuming the existence of sufficient funds, the maximum availability of academic data for research purposesis assured if those data have been deposited in the public data centers. to the extent that agencies successfullyencourage academics and their universities to deposit governmentfunded data into either old or new repositoriesestablished for this purpose, the researchfriendly policies of these centers should automatically apply. as long asthese policies are not themselves watered down by commercial and proprietary considerations, they shouldgenerally immunize the research function from conflicts deriving from private transactions.however, the universities or their researchers may very well balk at depositing commercially valuable data inthese repositories unless a relative degree of autonomy is preserved for depositories to negotiate the terms of theirprivate transactions and to impose restrictions on the uses of the data deposited for commercial purposes. this raisestwo important questions. the first concerns the willingness of data centers themselvesñwhether of the centralizedbrickandmortar variety or virtual networksñto accept conditional deposits that impose restrictions on use forcertain purposes in the first place. the second question, closely tied to the first, concerns the extent to which federalfunding agencies should further seek to define and influence the relations between universities and the private sectorto protect the public research functionñespecially when the data in question have not been deposited in an appropriate repository or when they have been so deposited but the repository permits conditional deposits.regarding the first of these questions, we previously observed that the emerging ònetwork of nodesó model ismore likely to accommodate conditional deposits or availability than are the traditional centralized data centers.nevertheless, the practice remains controversial in scientific circles in that it deviates from the traditional norm ofòfull and open access.ó for present purposes, we shall simply state our view that the possibilities for maximizingaccess to scientific data for public nonprofit research will not be fully realized in a highly protectionist legal andeconomic environment unless the scientific community agrees to experiment with suitably regulated conditionaldeposits.the second question, concerning the need to regulate the interface between universities and the private sectorwith regard to governmentfunded data, acquires important contextual nuances when viewed in the light of thepolicies and practices that currently surround the bayhdole act and related legislation. the bayhdole actencourages universities to transfer the fruits of federally funded research to the private sector by means of thepatent system. in a somewhat similar vein, federal research grants and contracts allow researchers to retaincopyrights in their published research results. by extension, the same philosophy could apply to databases produced with federal funding, especially if congress were to adopt a sui generis database protection right, withincalculably negative results, unless steps were taken to reconcile the goals of bayhdole with the dual nature ofdata as both an input and an output of scientific research and of the larger system of technological innovation.it would also be a mistake for the science policy establishment to wait for the enactment of database legislation before considering the implications of blindly applying the spirit of bayhdole to any database law thatcongress may adopt. because databases differ significantly from either patented inventions or copyrighted research results, policy makers should anticipate the advent of some database legislation and address the problems itmay cause for scienceñparticularly in regard to governmentfunded data. special consideration must be given tohow the power to control uses of scientific data after publication would be exercised once a database protectionlaw was enacted.we do not mean to question the underlying philosophy or premise of bayhdole, which has produced sociallybeneficial results. its very success, however, has generated unintended consequences and raised new questions thatthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4149require careful consideration. in advocating a program for a contractually reconstructed research commons, one ofour explicit goals is, indeed, to ensure that academics and their universities benefit from new opportunities toexploit research data in an industrial context. this goal reflects the policies behind bayhdole. at the same time,it would hardly be consistent with the spirit of bayhdole to allow the commercial partners of academic institutions to dictate the terms on which governmentfunded data are made available for purposes of nonprofit scientificresearch.on the contrary, a real opportunity exists for government funders and universities to develop agreed contractual templates that would apply to commercial users of governmentfunded data in general. in effect, the publicscientific community would thus develop a database protection scheme of its own that would override the lessresearchfriendly provisions of any sui generis regime that congress might adopt. in so doing, the scientificcommunity could also significantly influence the datalicensing policies and practices of the private sector, beforethat sector ends up influencing the datalicensing practices of university technology transfer offices.if one takes this proposal seriously, a capital point of departure would be to address the problem of followonapplications, which has greatly perturbed the debate about database protection in general. the critical role of dataas inputs into the information economy weighs heavily against endowing database proprietors with any exclusiveright to control followon applications. this principle becomes doubly persuasive when the government itself hasdefrayed the costs of generating the data in question, in which case an exclusive right to control valueaddedapplications takes on a cast of reverse freeriding. the solution is to allow secondcomers freely to extract and usedata from any given collection for bona fide valueadding purposes in exchange for adequate compensation of theinitial investor based on an expressly limited range of royalty options. if the rules developed by universities andfunding agencies imposed this kind of òcompensatory liabilityó regime on followon applications of governmentfunded academic data, in lieu of any statutorily created exclusive right, there is reason to believe it wouldsignificantly advance both technological development and the larger public interest in access to scientific data.universities and funding agencies could also adopt clauses similar to those proposed above in the context ofgovernmentgenerated data, including a general prohibition against legally or technically hindering access to anydatabase built around governmentfunded data for purposes of nonprofit scientific research. clauses that obligateprivate partners not to hinder the reuse of data in the construction of new databases to address new scientificresearch objectives seem particularly important, as are clauses requiring private partners to license their commercial products on fair and reasonable terms and conditions. also desirable are clauses forbidding misuse of anyunderlying intellectual property rights and establishing guidelines that courts should apply in evaluating specificclaims of misuse.moreover, when considering relations with the private sector, attention should be given to the high costs ofmanaging and archiving data holdings for scientific purposes and to the possibilities of defraying some of thesecosts through commercial exploitation. although government support ought to increase, especially as the potentialgains from a horizontal ecommons become better understood, the costs of data management will also increasewith the success of the system. for this reason, universities may want to levy charges against users in the privatesector or the vertical dimension, in order to help defray the costs of administering operations in the horizontaldomain and to make this overall approach more economically feasible.finally, care must be taken to reduce friction between the scientific data commons as we envision it anduniversitiesõ patenting practices under the bayhdole act. for example, any agreed contractual templates mighthave to allow for deferred release of data, even into repositories operating as a true public domain, at least for theduration of the oneyear novelty grace period during which relevant patent applications based on the data could befiled. other measures to synchronize the operations of the ecommons with the ability of universities to commercialize their holdings under bayhdole would have to be identified and carefully addressed. we also note that thereis an interface between our proposals for an ecommons for science and antitrust law, which would at least requireconsultation with the federal trade commission and might also require enabling legislation.in sum, to successfully regulate relations between universities and the private sector in the united states,where most of the scientific data in question are government funded (if not government generated), considerablethought must be given to devising suitable contractual templates that universities could use when licensing suchdata to the private sector. these templates, which should aim to promote the smooth operations of a researchthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.150the role of s&t data and information in the public domaincommons and to facilitate general research and development uses of data as inputs into technological development, could themselves constitute a model database regime that optimally balances public and private interests inways any federally enacted law might not. to succeed, however, these templates must be acceptable to theuniversities, the funding agencies, the broader scientific community, and to the specific disciplinary subcommunitiesñall of whom must eventually weigh in to ensure that academics themselves observe the norms that theywould thus have collectively implemented.in so doing, the participating institutions could avoid a race to the bottom in which single universities mightotherwise trade away more restrictions on open access and research to attract more and better deals from theprivate sector. unless science itself takes steps of this kind, there is a serious risk that, under the impetus of bayhdole, the private sector will gradually impose its own database rules on all governmentfunded data productsdeveloped with their university partners.interuniversity licensing of scientific datawhatever the merits of our proposals for regulating transfers of scientific data from universities to the privatesector, the need for science policy to regulate interuniversity transfers of such data seems irrefutable. in thiscontext, most of the data are generated for public scientific purposes and at public expense, and the progress ofscience depends on continued access to, and further applications of, such data. not to construct a researchcommons that could withstand the pressures to privatize governmentfunded data at the interuniversity levelwould thus amount to an indefensible abdication of the public trust by encumbering nonprofit research with hightransaction and exclusion costs. all the same, implementing this task poses very difficult problems that are likelyto exacerbate the conflicts of interests between the open and cooperative norms of science and the quest foradditional funding sources we previously identified.one may note at the outset that these conflicts of interest are rooted in the bayhdole approach to the transferof technology itself. this legislative framework stimulates universities to protect basic research results throughintellectual property rights and to license those rights to the private sector for commercial applications. if congressenacted a strong database protection law, it could extend bayhdole to this new intellectual property right. in sucha case, bayhdole would simply pass the relevant exclusive rights to extract and utilize collected data straightthrough the existing system to the same universities and academic researchers who now patent their researchresults and who would thus end up owning all the governmentfunded data they had generated.moreover, the bayhdole legislation makes no corresponding provision for beneficiary universities to givedifferential and more favorable treatment to other universities when licensing patented research products. on thecontrary, there is evidence that in transactions concerning patented biotechnology research tools, universities haveviewed each othersõ scientists as a target market. in these transactions, universities have virtually the samecommercial interests as private producers of similar tools for scientific research. such interuniversity deals haveaccordingly been constructed on a casebycase basis, often with considerable difficulty, by technology transferoffices striving to maximize all their commercial opportunities.without any agreed restraints on how universities are to deal with collections of data in which they hadacquired statutorily conferred ownership and exclusive exploitation rights, their technology transfer offices couldsimply treat databases like patented inventionsñdespite the immensely greater impact this could have on bothbasic and applied research. in this milieu, reliance on goodfaith accommodations hammered out by the respectivetechnology transfer offices would, at best, make interuniversity exchanges resemble the complicated transactionsthat already characterize relations between highly distributed laboratories and research teams in the zone ofinformal exchanges of scientific data. all the vices of that zone would soon be imparted into the more formal zoneof interuniversity relationships. at worst, this would precipitate a race to the bottom as universities tried tomaximize their returns from these rights, in which case some technology transfer offices could be expected tocontractually override any modest research exceptions a future database law might have codified.at the same time, the bayhdole legislative framework may itself suggest an antidote for resolving thesepotential conflicts of interest, or, at least, a sound point of departure for addressing them. section 203 of the bayhdole act explicitly recognizes that the public interest in certain patented inventions may outweigh the benefitsthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4151usually anticipated from private exploitation under exclusive property rights. in such cases, it authorizes thegovernment to impose a compulsory license or otherwise to exercise òmarch inó rights and take control of theinvention it has paid to produce. in fact, these publicinterest adjustments have never successfully been exercisedin practice.nevertheless, the principle (if not the actual practice) behind these provisions presents a platform on whichuniversities and federal funding agencies can build their own mutually acceptable arrangements to promote theircommon interest in full and open access to governmentfunded collections of data. our goal, indeed, is to persuadethem to address this challenge now, before a database protection law is enacted, by examining how to ensure thesmooth and relatively frictionless exchange of scientific data between academic institutions, regardless of anyexclusive property rights they may eventually acquire and notwithstanding any other commercial undertakingswith the private sector they may pursue. absent such a proactive approach, we fear a slow unraveling of thetraditional sharing norms in the interuniversity context and an inevitable race to the bottom.¥structuring interuniversity data exchanges in a highprotectionist legal environment. because theissues under consideration here pertain to uses of governmentfunded data produced by academics for universitysponsored programs, one looks to òfull and open accessó as the optimal guiding principle and to the sharing normsof science as the foundation of any arrangement governing interuniversity licensing of data. on this approach, thegovernmentfunded data collections held by universities would be viewed as a single common resource for interuniversity research purposes. the operational goal would be to nurture and extend this common resource within ahorizontally linked administrative framework that facilitated every universityõs public research functions, withoutunduly disrupting commercial relations with the private sector that some departments of some universities willundertake in the vertical dimension.to achieve this goal, universities, funding agencies, and interested scientific bodies would have to negotiatean acceptable legal and administrative framework, analogous to a multilateral pact, that would govern the commonresource and provide daytoday logistical support. ideally, the participating universities or their designated agentswould operate as trustees for the horizontally constructed common resource, much as occurs with what the freesoftware foundation does with the gnu system. in this capacity, the trustees would assume responsibility forensuring access to the holdings on the agreed terms and for restraining deviant uses that violate those terms orotherwise undermine the integrity of the commons. the full weight of the federal granting structure could then bemade to support these efforts by mandating compliance with agreed terms and by directly or indirectly imposingsanctions for noncompliance.alternatively, a less formal administrative structure could be built around a set of agreed contractual templatesregulating access to governmentfunded data collections for public research purposes. on this approach, theparticipating universities would retain greater autonomy, there would be less need for a fully fleshed out òmultilateral pact,ó and the monitoring and other transaction costs might be reduced.in a less than perfect world, however, there are formidable obstacles standing in the way of a negotiatedcommons project, over and above inertia, that would have to be removed. initially, the very concept of an ecommons needs to be sold to skeptical elements of the scientific community whose services are indispensable to itsdevelopment. academic institutions, science funders, the research community, and other interested parties mustthen successfully negotiate and stipulate the pacts needed to establish it, as well as the legal framework toimplement it. transaction costs would all need to be monitored closely and, whenever possible, reduced throughout the various development phases.once the research universities became wholeheartedly committed to the idea of a regime that guaranteed themuniversal access to, and shared use of, the governmentfunded data that they had collectively generated, these organizational problems might seem relatively minor. the difficulties of winning such a commitment, however, cannot beoverestimated in a world where university administrators are already conflicted about the efforts of their technologytransfer offices to exploit commercially valuable databases in the genomic sciences and other disciplines with significantpotential for commercial development. the prospect that congress will eventually adopt a hybrid intellectual propertyright in collections of data could make these same administrators reluctant to lock their institutions into a kind ofvoluntary pool of any resulting exclusive property rights, even for public scientific research purposes.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.152the role of s&t data and information in the public domainconceptually, the problems inherent in organizing a pool of intellectual property rights so as to preserveaccess to, and use of, a common resource have become much better understood than in the pastñowing to theexperience gained from both the opensource software movement and the new creative commons initiative. theseprojects demonstrate that there are few, if any, technical obstacles that cannot be overcome by adroitly directingrelevant exclusive rights and standardform contracts to public, rather than private, purposes.the deeper problem is persuading university administrators that they stand to gain more from open access toeach othersõ databases in a horizontally organized research commons than they stand to lose from licensing data toeach other under more restrictive, casebycase transactions. although we believe this proposition to be true,following it could amount to an act of faith, albeit one that resonates with the established norms of science, andwith the primary mission of universities.to the extent that the universities may have to be sold on the benefits of an ecommons for data, with a viewto rationalizing and modifying their disparate licensing policies, this project would require statesmanship, especially on the part of the leading research universities. it may also require pressure from the major governmentfunders and standardsetting initiatives by scientific subcommunities. funding agencies, in particular, must beprepared to discipline wouldbe holdouts and to discourage other forms of deviant strategic behavior that couldundermine the cohesiveness of those institutions willing to pool their resources. in this regard, account will haveto be taken as well of the universitiesõ patenting interests, which will need to be suitably accommodated.assuming a sufficient degree of organizational momentum, there remains the thorny problem of establishingthe terms and conditions under which participating universities could contribute their data to a horizontallyorganized research commons. the bulk of the departments and subdisciplines involved would almost certainlyprefer a brightline rule that required all deposits of governmentfunded data to be made without conditions andsubject to no restrictions on use. this preference follows from the fact that most science departments currently seeno realistic prospects for licensing basic research data, even to the private sector, and have not yet experienced theproprietary temptations of exclusive ownership that a sui generis intellectual property right in noncopyrightabledatabases might eventually confer.at the same time, such a brightline rule could utterly deter those subdisciplines that already license data oncommercial terms to either the private or public sectors, or that contemplate doing so in the near future. thesesubdisciplines would not readily forego these opportunities and would, on the contrary, insist that any multilateralnegotiations to establish a horizontal commons devise contractual templates that protected their commercialinterests in the vertical dimension. if, moreover, congress enacts a de facto exclusive property right in collectionsof data, it would probably deter other components of the scientific community, who might become unwilling toforego either the prospective commercial opportunities or other strategic advantages such rights might makepossible.in a word, a brightline rule requiring unconditional deposits in all cases could thus defeat the goal of linkingall university generators of governmentfunded data in a single, horizontally organized research commons. at thesame time, the goal of universality could, paradoxically, require negotiators seeking to establish the system todeviate from the norm of full and open access by allowing a second type of conditional deposit of data into thehorizontal domain by those disciplines or departments that were unwilling to jeopardize present or future commercial opportunities.¥resolving the paradox of conditional deposits. science policy in the united states has long disfavored atwotiered system for the distribution of governmentfunded data. such twotiered systems for government oracademic data distribution have been favored and promoted by the scientific community in the european union,but these initiatives have been strongly opposed by u.s. science agencies and academics. under such a system,database proprietors envision split (or twotier) uses of certain data and will only make them available on conditions that govern the different types of uses they have expressly permitted.typically, splitlevel arrangements distinguish between relatively unrestricted uses for basic research purposes by nonprofit entities and more restricted uses for commercial applications by private firms that license datafrom scientific entities. the latter conditions may range from a simple menu of pricediscriminated paymentoptions to more complicated provisions that regulate certain data extractions, seek grantbacks of followonthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4153applications by secondcomers, or impose reachthrough clauses seeking legal or equitable rights in subsequentproducts. in some cases, moreover, the distinction between profit and nonprofit uses of scientific data becomesblurred, and the two categories may overlap, which adds to the costs and complications of administration. forexample, universities may treat some databases as commercial research tools and impose a price discriminationpolicy that provides access to the research community at a lower cost than to forprofit entities. this becomes morelikely when there is a privatesector partner.we recognize that a decision to allow participating universities to make conditional deposits of governmentfunded data to a collectively managed research consortium represents a secondbest solution, one that conflictswith the goal of establishing a true public domain based on the premise of full and open access to all users. theallowance of restrictions on use breaks up the continuity of data flows across the public sector and necessitatesadministrative measures and transaction costs to monitor and enforce differentiated uses. it also entails measuresto prevent unacceptable leakage between the horizontal and vertical planes, and it may result in charges for publicinterest uses that exceed the marginal cost of delivery, even in the horizontal plane.we nonetheless doubt that a drive for totally unconditional deposits of governmentfunded data could succeedin the face of mounting worldwide pressures to commoditize scientific data, and we fear that excessive reliance onthe orthodox position would, in the end, undermineñrather than saveñthe sharing ethos. even if one disregardsthe prospects for strengthened intellectual property protection of noncopyrightable databases, too many universities have already begun to perceive the potential financial benefits they might reap from commercial exploitationof genomic databases in particular and biotechnologyrelated databases in general. their reluctance to contributesuch data to a research commons that allowed private firms freely to appropriate that same data could not easily beovercome. adoption of a database protection law would then magnify this reluctance and encourage the respectivetechnology transfer offices to find more ways to commercially exploit more of the governmentfunded dataproducts that were subsequently invested with proprietary rights.even if a consortium of universities were to formally consent to such an unconditional arrangement, theirtechnology transfer offices might soon be demanding an exceptional status for any databases that containedcomponents produced without government funds. they could persuasively argue that private funds for mostjointly created data products could decrease or even dry up if both customers and competitors could readily obtainthe bulk of the data from the public domain. once it became clear that an admixture of privately funded data couldelicit the right to deposit data in a research commons on conditions that protected commercial exploitation of thedatabases in question, academics with an eye to cost recovery and profit maximization would logically makepersistent efforts to qualify for this treatment. they would thus seek more private investment for this purpose orseek to obtain the universityõs own funds for the project. either way, there would be a perverse incentive toprivatize more data than ever if the only legitimate way to avoid dedicating it all to the public domain was to showthat some of it had been privatized.in other words, if the quasipublic research space accommodated only unconditional deposits of data, it couldfoster an insuperable holdout problem as participating universities found ways to detach and isolate their commercially valuable databases from such a system. in these circumstances, a failure to obtain a bestcase scenariopremised on òfull and open accessó would quickly degenerate into a worstcase scenario, characterized by growinggaps in the communally accessible collection and an unraveling of the sharing ethos, that would require casebycase construction of interuniversity data flows and could sometimes culminate in bargaining to impasse.in our estimation, the worstcase scenario is so bad, and the pressures to commoditize could become so greatin the presence of a strong database right, that steps must be taken to ensure universal participation in a contractually reconstructed research commons from the outset by judiciously allowing conditional deposits of governmentfunded data on standard terms and conditions to which all the stakeholders had previously agreed. indeed, the goalis to develop negotiated contractual templates that clearly reinforce and implement terms and conditions favorableto public research without unduly compromising the ability of the consortiumõs member universities to undertakecommercial relations with the private sector.at stake in this process is not just a few thousand patentable inventions, but, rather, every governmentfundeddata product that has potential commercial value to other universities as a research tool or educational device.sound data management policies thus point to a secondbest solution that would preserve the integrity of the interthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.154the role of s&t data and information in the public domainuniversity commons by disallowing the principal ground on which concerted holdout actions might take root, byensuring that only researchfriendly terms and conditions apply in both the horizontal and vertical dimensions, andby making it too costly for any institution to deviate from the agreed regulatory framework governing the twotiered regime.those who object to this proposal will argue that it unduly undermines the òfull and openaccessó principle bytempting more and more university departments or subdisciplines to opt for conditional deposits than would otherwise have been the case. on this view, once a negotiated twotiered model were set in place, universities would comeunder intense pressures to avoid the true public domain or openaccess option even when there was no need to do so.however, a universal and functionally effective interuniversity research commons simply cannot be constructed with a brightline, true publicdomain rule applied across the board for the reasons we previously set out.a brightline rule also carries with it the wellrecognized difficulty of distinguishing forprofit from notforprofitresearch activities when single libraries increasingly engage in both. in contrast, a regime based on conditionaldeposits overcomes this problem by allowing a scientific entity to contribute to and benefit from the data commonsso long as it respects the agreed norms bearing on that arrangement. in this respect, a normative accommodationwill have displaced legal distinctions that cannot feasibly be enforced.moreover, the very contractual templates that make the construction of such a commons feasible in a twotiered system should also mitigate its social costs. even if conditional deposits are allowed, many subdisciplineswill continue to have no commercial prospects and no need to invoke the contractual templates that regulatethem. when this is the case, peer pressures reinforced by the funding agencies should make it difficult, if notimpractical, for members of those communities to opt out of the traditional practice of making data availableunconditionally.when, instead, given communities find themselves forced to deal with serious commercial pressures, thenegotiated contractual solutions that enabled them to make the data conditionally available for public researchpurposes should also tend to preserve and implement the norms of science. in particular, the applicable contractualtemplates should immunize deposited data from the vagaries of casebycase transactions under the aegis ofuniversitiesõ technology transfer offices and would also limit the kinds of restrictions privatesector partners mightotherwise seek to impose on universities.at the end of the day, a set of agreed contractual templates permitting conditional deposits in the interests ofa horizontally linked research commons would provide a tool universities could use with more or less wisdom. ifused wisely, this tool should ensure that more data are made available to a contractually reconstructed researchcommons than would be possible if member universities could not protect the interests of their commercialpartners. this same tool may also provide incentives for the private sector to work with universities in producingbetter data products than the latter alone could generate with their limited funds.¥other hard problems. allowing universities to deposit governmentfunded data into a contractually reconstructed research commons, on conditions designed to protect their commercial relations with the private sector,solves two difficult problems. first, it avoids the risk that large quantities of governmentfunded data wouldremain outside the system on the grounds that they had been commingled with privately funded components.second, it ensures that any negotiated contractual templates the research consortium adopts to govern its horizontal space will apply to all the data holdings subject to its jurisdiction, including databases to which the privatesector had contributed. however, it does not automatically determine the precise conditions that the agreedcontractual templates should apply to interuniversity licensing of data subject to their collective jurisdiction. inthe process of defining these conditions, moreover, those who negotiated the òmultilateral pactó among universities, federal funders, and scientific bodies needed to launch the consortium would have to resolve a number ofcontentious issues.the guiding principle that should apply to interuniversity licensing of data available from the quasipublicspace is that depositors may not impose any conditions that impede the customary and traditional uses of scientificdata for nonprofit research purposes. a logical corollary is that they should affirmatively adopt the measures thatmay prove necessary to extend and apply this principle to the online environment. because the data underthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4155discussion are government funded for academic purposes to begin with, the òopen accessó and sharing norms ofscience should then color any specific implementing templates that regulate access and use.with regard to access, the customary mode of implementing these norms would be to make data available toother nonprofit institutions at no more than the marginal cost of delivery. in the online environment, thesemarginal costs are essentially zero. this represents the preferred option whenever the costs of maintaining the datacollection are defrayed by public subsidy or by nonexclusive licenses to private firms in the vertical dimension.if, however, the policy of free or marginally priced access appears unable to sustain the costs of managing agiven project at the interuniversity level, an incremental pricing structure may become unavoidable. the optionsfor such a pricing structure range from a formula allowing partial incremental cost recovery when a project ispartially subsidized to a formula providing full cost recovery when this is necessary to keep the data collectionalive. this may be accomplished through a paying interuniversity consortium, such as the interuniversityconsortium for political and social science research at the university of michigan, or by means of more ad hoccostrecovery methods. examples of subcommunities that have found it necessary to rely on the second option arelargely in the laboratory physical sciences.the prices charged other nonprofit users to access data in the research commons should never exceed the fullincremental cost of managing the collective holdings. this premise follows from the fact that the initial costs ofcollecting or creating the data were defrayed by the government or by some combination of sources (includingprivate sources) that normally subscribe to the openaccess principle.however, when private firms have defrayed a substantial part of the costs of generating the database inquestion, there are few, if any, standard solutions. occasionally, even a private partner might view the collectiveholdings as a valuable resource for its own pursuits, to which it agrees to contribute on an eleemosynary basis. inthe more typical cases, the private partner is likely to view the research community as the target market for adatabase it paid to create and from which it must derive its expected profits.in that event, the collection of additional revenues from privatesector access charges should depend entirelyon freedom of contract, although a likely demand that public research users pay access charges that exceeded datamanagement costs would pose a hard question. on the one hand, as beneficiaries of government funding, theuniversities should forego òprofitsó from charges levied to access their partly publicly funded databases for publicresearch purposes. on the other hand, a private partner will not readily forego such profits, especially if it hadinvested in the project precisely because of its potential commercial value as a research tool. if the universityshared these òprofitsó with its private partner, this practice would deviate from the basic principle governing interuniversity access generally, and it would encourage other universities to seek private partners for this purpose,which in turn would yield both social costs and benefits.in these cases, care must be taken to avoid adopting policies that would discourage either publicprivatepartnerships for the development of socially beneficial data products or the inclusion of such products in ahorizontal, quasipublic research space. at the same time, there is a potential loophole here that would allowuniversities to deviate from the general rules applicable to that space if the private partner could impose marketdriven access rights for nonprofit research purposes, and its partner university shared in those profits.we know of no standard formula for resolving this problem. if the database is also of interest to the privatesector, price discrimination and product differentiation are the preferred techniques for reducing access chargeslevied for public research. in any event, the trustees that manage the interuniversity system should monitor andevaluate these charges, and their powers to challenge unreasonable or excessive demands would become especially important in the absence of any alternative or competing sources of supply.this strategy, however, begs the question of whether and to what extent the universities should be allowed toretain their share of the òprofitsó from access charges levied against public research users. as matters stand, this isan issue that can be addressed only by the relevant discipline communities themselves, in the absence of somegeneral norm that would not pose insuperable administrative burdens to implement.once access to databases available to the research commons has legitimately been gained, further restrictionson uses of the relevant data should be kept to a minimum. in principle, contractual restrictions on reuses of publiclyfunded data for nonprofit research purposes should not be permitted. this principle need not impede the use ofthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.156the role of s&t data and information in the public domainconditions that require attribution or credit from researchers who make use of such data, and it can also bereconciled with provisions that defer access by certain users for specified periods of time or that impose restrictions on competing publications for a certain period of time.this ideal principle runs into trouble, however, when confronted with the difficult problem posed by commercially valuable followon applications derived from databases made available to the research commons. it is onething to posit that the academic beneficiaries of publicly funded research should be limited to the recovery of coststhrough access charges and should not be entitled to additional claims for followon uses by other nonprofitresearchers. quite different situations arise when the funding is public, but a private firm has invested its ownresources to develop a followon application for commercial pursuits or when the initial datagenerating projectentailed a mix of private and public funds and the product subsequently gives rise to a commercially valuablefollowon application. these hard cases become even harder if the followon product primarily derives its commercial value from being a research tool universities themselves need to acquire.assuming, as we do, that a primary objective of any negotiated solution is to avoid gaps in the data madeavailable for public research purposes in the horizontal domain, there is an obvious need for agreed contractualtemplates that would respect and preserve the commercial interests in the vertical plane identified above. this goaldirectly conflicts, however, with the most idealistic option set out above, which is to freely allow all followonapplications based on data made available to the research commons, regardless of the commercial prospects orpurposes and without any compensatory obligation beyond access charges (if any).this option would represent a true publicdomain approach to governmentfunded data, and it would fit withinthe traditional legal framework applied in the past to collections of data. however, it might be expected to discouragepublicprivate partnerships formed to exploit followon applications of publicly funded databases, contrary to thephilosophy behind the bayhdole act, although this risk is tempered by the fact that all wouldbe competitors whoinvested in such followon applications would find themselves on equal footing in this respect. this option wouldcertainly discourage publicprivate partnerships formed to produce scientific databases from making them availableto the commons if that decision automatically deprived them of any rights to followon applications.a second option is to leave the problem of commercially valuable followon applications to freedom ofcontract, in which case universities and their private partners could license whom they please and exclude the rest.this solution is consistent with proposals to enact a de facto exclusive property right in noncopyrightable databases and with the philosophy behind bayhdole. it would also alleviate disincentives to make databases derivedfrom a mix of public and private funds available to the nonprofit research community.however, this second option would relegate the problem of followon applications to the universitiesõtechnology transfer offices once again, which might be tempted routinely to impose the kind of grantback andreachthrough clauses that are already said to generate anticommons effects in biotechnology and that areinconsistent with the dual nature of data as both inputs and outputs of innovation. just as a true publicdomainapproach tends unintentionally to impoverish the commons we seek to construct, so too a true laissezfaireapproach undermines the effectiveness of that same commons and triggers a race to the bottom, as universitiesseek private partners solely for the purpose of occupying a privileged position with respect to followonapplications.a third option is to allow freely followon applications of databases made available to the research commonsfor commercial purposes while requiring their producers to pay reasonable compensation for such uses under apredetermined menu that fixes a range of royalties for a specified period of time. for maximum effect, a corollaryòno holdoutó provision should obligate all universities engaged in publicprivate database initiatives to make theresulting databases available to the research commons under this òcompensatory liabilityó framework.this approach enables investors in publicprivate database initiatives to make their data available for publicresearch purposes without depriving them of revenue flows from followon applications needed to cover the costsof research and development or of the opportunities to turn a profit. at the same time, it avoids impeding access tothe data for either commercial or noncommercial purposes, in which aspect it mimics a true public domain, and itcreates no barriers to entry. moreover, a compensatory liability approach implements the policies behind the bayhdole act without the overkill that occurs when publicly funded research results are subjected to exclusive propertyrights that impoverish the public domain and create barriers to entry to boot.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4157these, or other, options would require further study and analysis as part of the larger process of reconstructingthe research commons we propose. it should be clear, moreover, that any solutions adopted at the outset must beviewed as experimental, subject to review in light of actual results.the zone of informal data exchangesthe zone of informal data exchanges is populated by single researchers or laboratories or by small teams ofassociated researchers whose work is typically expected to lead to future publications. because this zone operates largelyin a prepublication environment or outside the ambit of federal granting agencies, the constraints of government funderson uses of data are relatively less prescriptive, and a considerable amount of the data being produced may not be fundedby federal agencies at all. if funding is provided by other nonprofit sources or by state governments, the end results stillpertain to public science and its ultimate disclosure norms, but the controls are not standardized. to the extent thatprivatesector funding is also involved, even the norms of public science may not apply.quantitatively, the amount of scientific data held in this informal zone appears large. despite the relativedegree of invisibility that prepublication status confers, these holdings are also of immense qualitative importancefor cuttingedge research endeavors. although these data may not be as well prepared as those released for broad,open use in conjunction with a publication, they typically will reflect the most recent findings. moreover, thisinformal sector seems destined to grow even more important in the near future as it increasingly absorbs scientificdata that were not released at publication as well as the data researchers continue to compile after publication. ifcongress were to adopt a strong intellectual property right in noncopyrightable databases, this informal zone couldexpand further to include all the published data covered by an exclusive property right that had not otherwise beendedicated to the public domain.as previously discussed, actual secrecy is taken for granted in this zone, and disclosure depends on individually brokered transactions often based on reciprocity or some quid pro quo. these fragile data streams, which havealways been tenuous due to personal and strategic considerations, have increasingly broken down owing to denialsof access and to a trading mentality steeped in commercial concerns that is displacing the sharing ethos.left to themselves, the legal and economic pressures operating in the informal zone are likely to further reducedisclosures over time and to make the informal data exchange process resemble that of the private sector. thattrend, in turn, undermines the new opportunities to link even highly distributed data holdings in virtual archives orto experiment with new forms of collaborative research on a distributed, autonomous basis, as digital networkshave recently made possible. the positive synergies expected from organized peertopeer file sharing on an openaccess basis cannot be realized if researchers decline to make data available at all out of a fear of sacrificingnewfound commercial opportunities or other strategic advantages. nor will these new opportunities fully developif those who are nominally willing to make data available impose onerous licensing terms and conditionsñreinforced by intellectual property rightsñthat multiply transaction costs, unduly restrict the range of scientificuses permitted, or otherwise embroil those uses in anticommons effects.here, the immediate goal of science policy should be to reduce the technical, legal, and institutional obstaclesthat impede electronic peertopeer file exchanges and to generally facilitate exchanges of data on the most openterms possible across a horizontal or quasipublic space. at the same time, the measures adopted to implement thispolicy must avoid compromising or inhibiting the interests of individual participants who seek commercialapplications of their research results in a private or vertical sphere of operations. this twopronged approach couldstabilize the status quo and reinvigorate the flagging cooperative ethos in the zone of informal data exchanges asmore individual researchers and small communities experienced the benefits of electronically linked access tovirtual archives and discovered the productive gains likely to flow from collaborative, interdisciplinary, and crosssectoral uses.from an institutional perspective, however, organizing and implementing such a twopronged approach todata exchanges in the informal zone presents certain difficulties not encountered in the formal zone of interuniversity relations. here the playing field is much broader, the players are more autonomous and unruly, and thepower of federal funders directly to impose topdown regulations has traditionally been weak or underutilized. themoral authority of these funders nonetheless remains strong, and peer pressures in support of the sharing ethosthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.158the role of s&t data and information in the public domainwould become more effective if a consensus developed that the twopronged approach we envision actuallyyielded tangible benefits at acceptable costs.much, therefore, depends on shortterm, bottomup initiatives that rely on individual decisions to opt forstandardized, researchfriendly licensing agreements in place of the defensive, ad hoc transactions that currentlyhinder the flow of data streams in this sector. here, the solution is to provide individual researchers with a tool kitfor constructing prefabricated, exchange transactions on communityapproved terms and conditions. the tool kitwould contain a menu of standardform contractual templates that individual researchers could use to license data,and the templates adopted would be posted online to facilitate electronic access to networks of nodes. thesetemplates would cover a variety of situations and offer a range of ad hoc choices, all aimed at maximizingdisclosure in both digital and nondigital mediums for public research purposes.for this endeavor to succeed, however, the templates in question would clearly need to allow participatingresearchers and their communities to make data available on conditions that expressly precluded licensees fromunauthorized commercial uses or followon applications. although this suggests the need to deviate from truepublicdomain principles once again, one should remember that, in the informal zone as it stands today and islikely to develop, secrecy and denial of access are already wellestablished, countervailing practices. one canhardly argue that permitting conditional availability would undermine the norms of science in this zone, given theinability of those norms to adequately defend the interests of public research in unrestricted flows of data at thepresent time.the object, rather, is to invigorate those sharing norms by reconciling them with the commercial needs andopportunities of the researchers operating in the informal zone, to elicit more overall benefits for public scienceunder a secondbest arrangement than could be expected to emerge from brokered individual transactions in ahighprotectionist legal environment. this strategy requires a judicious resort to conditionality that would make itpossible to forge digitally networked links between individual data suppliers and that would let their data flowacross those links into a quasipublic space relatively free of restrictions on access and use for commercialpurposes.given the larger number of players and the disparity of interests at stake, a logical starting premise is that onlya small number of standard contractual templates seems likely to win the support of the general scientific community, at least initially. a true public domain option, of course, should be available for all willing to use it. for therest, a limited menu of conditional publicdomain provisions, such as those offered by the creative commons,should be sufficient. clauses that delay certain uses for a specified period, or that delay competing publicationsbased on, or derived from, a particular database for a specified period of time should also pass muster, so long asthey remain consistent with the practices of the relevant scientific subcommunity. in the absence of any underlyingintellectual property right, an additional clause reserving all other rights and excluding unauthorized commercialuses and applications would complete the limited, òcopyleftó concept. we believe that even a small number ofstandard contractual templates that facilitated access and use of scientific data for public research purposes couldexert a disproportionately large impact on the increasingly open, collaborative work in the networked environment.in the scientific milieu, however, difficult problems of leakage and enforcement could also arise. to addressthese problems, the scientific community, perhaps under the auspices of the american association for the advancement of science, would need to consider developing institutional machinery capable of assisting individualresearchers who feared that their data had been used in ways that violate the terms and conditions of the standardform licensing agreements they had elected to employ.more complex or refined contractual templates are also feasible, but their use should normally depend less onindividual choice and more on the consensus approval of disciplinespecific communities. moreover, in theinformal zone, efforts to influence the terms and conditions applicable to privatesector uses seem much less likelyto succeed than are similar efforts in the interuniversity context.attempts to overregulate the zone of informal data exchanges should generally be avoided at this stage, lestthey stir up unwarranted controversy and deter the more ambitious efforts to regulate interuniversity transactionsdescribed above. the success of those efforts in the zone of formal data exchanges should greatly reinforce thenorms of science generally. it would also exert considerable indirect pressures on those operating in the informalthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4159zone to respect those norms and to emulate at least the spirit of any agreed contractual templates that had provedtheir merit in that context. the more that universities succeed in amalgamating their governmentfunded holdingsinto an effective, virtual archive or repository, the more that pressures would be brought to bear on individualresearchers, research teams, and small communities to similarly make their data available in more formallyconstituted repositories. as a body of practice develops in both the formal and the informal zones, the mostsuccessful approaches and standards would become broadly adopted, and the desire to obtain the greater benefitslikely to flow from more formalized arrangements should grow.meanwhile, efforts to regulate the zone of informal data exchanges should be viewed as an opportunity tostrengthen the norms of science and to facilitate the creation of virtual networked archives electronically linkingdisparate and highly distributed data holders. the overall objective should be to generate more disclosure thanwould otherwise have been possible if all the players exercised their proprietary rights in total disregard of the needfor a functioning research commons for nonprofit scientific pursuits. if successful, these modest efforts in theinformal zone could alleviate some of the most disturbing erosions of the sharing ethos that have already occurred,and they could encourage federal funding agencies to take a more active role in regulating broader uses of researchdata. a successful application of òcopyleftó techniques to the informal zone of academic research could also serveas a model for encouraging disclosure for public research purposes of more data generated in the private sector.the private sectorscientific data produced by the private sector are logically subject to any and all of the proprietary rights thatmay become available. here, the policy behind a contractually reconstructed research commons is not to defendthe norms of science so much as to persuade the private sector of the benefits it stands to gain from sharing its owndata with the scientific community for public research purposes. the goal is thus to promote voluntary contributions that might not otherwise be made to the true public domain or to the conditional domain for public researchpurposes on favorable terms and conditions.from the perspective of publicinterest research, of course, corporate contributions of otherwise proprietarydata to a true public domain are the preferred option. although the copyright paradigm reflected in the supremecourtõs feist decision presumably made the factual contents of commercially valuable compilations published inhard copy formats available for such purposes, the federal appellate courts have lately rebelled against feist andmade it harder for secondcomers to separate noncopyrightable facts and information from the elements of originalselection and arrangement that still attract copyright protection. online access to noncopyrightable facts and datais further restricted by the stronger regime that prohibits tampering with technological fences that was embodiedin the digital millennium copyright act of 1998, although the full impact of these provisions on scientific pursuitsremains to be seen. meanwhile, many commercial database publishers may be expected to continue to lobby hardfor a strong database protection law on the e.u. model that would limit unauthorized extraction or use of thenoncopyrightable contents of factual compilations, and it appears likely that congress will again seek to enact adatabase protection statute in 2004.in contrast to the researchfriendly legal rules under the print paradigm, all the factual data andnoncopyrightable information collected in proprietary databases are increasingly unlikely to enter the publicdomain and will instead come freighted with the restricted licensing agreements, digital rights managementtechnologies, and sui generis intellectual property rights that characterize a highprotectionist legal environment.under such a regime, open access and unrestricted use become possible if privatesector database compilersdonate their data to public repositories or contractually agree to waive proprietary restrictions on controls thatwould otherwise impede access and use for public research purposes.some examples of both donated and contractually stipulated publicdomain data collections from the privatesector already exist. an example in the first category is provided in the presentation by shirley dutton.5 anexample of the second type of arrangement is also provided by michael morgan in his presentation.65see chapter 27 of these proceedings, òcorporate donations of geophysical data,ó by shirley dutton.6see chapter 28 of these proceedings, òthe single nucleotide polymorphism consortium,ó by michael morgan.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.160the role of s&t data and information in the public domainalthough pure publicdomain models initiated by industry will no doubt continue to be the exception ratherthan the rule, the availability of data on a conditional publicdomain basis, or at least on preferential terms andconditions to the notforprofit research community, should enjoy far broader acceptance and ought to be promoted. certainly, the existence of contractual templates, along the lines being developed by the creative commons, could help to encourage privatesector entities to make conditional deposits of data for relatively unrestricted access and use by publicinterest researchers.scientific publications by privatesector scientists provide another valuable source of research data. however,these scientists labor under increasing pressures either to limit such publications altogether or to insist thatpublishers allow supporting data to be made available only on conditions that aim to preserve their commercialvalue. although many academics in the scientific community oppose this practice, it is exactly what wouldproliferate if privatesector scientists held exclusive property rights in the data that allowed them to retain controleven after publication. this sobering observation might induce the scientific community to reconsider the need toallow privatesector scientists to modify the brightline disclosure rules otherwise applied to publicsector scientists to encourage them to disclose more of their data for nonprofit research purposes.even when companies remain unwilling to make their data available to nonprofit researchers on a conditionalpublicdomain basis, there is ample experience with price discrimination and product differentiation measuresfavorable to academics. to the extent that the public research community does not constitute the primary marketsegment of the commercial data producer, either of these approaches will help promote access and use bynoncommercial researchers without undue risks to the data vendorõs bottom line. the conditions under which sucharrangements might be considered acceptable by commercial data producers will vary according to discipline areaand type of data product, but it is in the interest of the public research community to identify such producers ineach discipline and subdiscipline area and to negotiate favorable access and use agreements on a mutuallyacceptable basis.the terms and conditions acceptable to private firms operating in the vertical dimension that opt into a publicaccess commons arrangement might be fairly restrictive in their allowable uses, as compared with the conditionsapplicable under the standardform templates implementing any of the other options discussed above. however,the goal of securing greater access to privately generated data with fewer restrictions justifies this approachbecause it makes data available to the research community that would otherwise be subject to commercial termsand conditions in a more researchunfriendly environment.finally, the importance of regulating the interface between universitygenerated data and privatesector applications was treated at length above, with a view to ensuring that the universitiesõ eagerness to participate incommercial endeavors did not compromise access to, and use of, federally funded data for public researchpurposes. here, in contrast, it is worth stressing the benefits that can accrue from data transfers to the private sectorwhenever a framework for reducing the social costs of such transfers has been worked out to the satisfaction ofboth the research universities and the public funding sources. these arrangements are especially important if theexploitation, or applications, of any given database by the private sector would not otherwise occur in a nonproprietary environment.price discrimination and product differentiation can also facilitate socially beneficial interactions between theprivate sector and universities. for example, companies might consider licensing certain data to commercialcustomers on an exclusiveuse basis for a limited period of time, after which the data in question would be licensedon preferential terms to nonprofit users or even revert to an openaccess status. this strategy may work successfully in the case of certain environmental data, where most commercially valuable applications are produced inreal time or nearreal time and can then be made available at lower cost and with fewer restrictions for retrospective research that is less time dependent. such an approach might not work in other research areas, such asbiotechnology, however, where a delay in access may not be an acceptable tradeoff or that delay is too long topreserve competitive research values.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 416116121strengthening publicdomain mechanisms in thefederal government: a perspective from biologicaland environmental researchari patrinos and daniel drell1.i want to begin by noting that these remarks are based on a commentary that we wrote for the june 6, 2002issue of nature and reflect our views, not those of the u.s. department of energy (doe).2the office in which we serve, the office of biological and environmental research at the doe, is the steward ofabout half a billion dollars. our responsibility is to oversee and manage its spending for the greatest good to the u.s.science effort in service to doe missions and, by extension, our citizenry. our office supports research in variousscientific areas, among them environmental sciences, global climate research, medical technologies and imaging, andgenomics including the human genome project that we started in 1986. among other things, we work very hard toadhere to practices and policies that promote openness of data access because we believe that experience has demonstrated that, paraphrasing ivan boesky, openness is good, openness works, and this maximizes the benefits to us all.but we have to recognize and adapt to new realities. the u.s. government expenditures for fundamentalresearch total in the neighborhood of $45 billion per year. recent figures for fiscal year 2000 for genomics indicatethat the u.s. government invests about $1.8 billion in this area, but the private sector invests close to $3 billion.glossing over the imprecision of these numbers, it is clear that the expenditures on òpostfundamental,ó or exploitationfocused, research by the private sector are roughly comparable, at least in its order of magnitude, toexpenditures by the federal government. what the private sector chooses to do with research results and data is upto them. as more privatesector funding goes toward òupstreamó (more fundamental) science, and the distinctionsblur, the challenge we face is not to decry this situation any further but to try to work out accommodations thatpromote science. we are optimistic that it can be done, and we are particularly encouraged by professor berryõspresentation.3 we want to emphasize his point that, before we move toward more restrictive data policies, weshould experiment, collect data, and see what happens.in february 2001, two significant papers were published, one in nature4 and the other in science,5 reportingon the òdraftó sequence of the 3.2 billion base pair human genome. the nature paper derived from the multiyear1the authors acknowledge with gratitude the helpful comments of colleagues, as well as robert cookdeegan (duke university). theviews expressed herein are those of the authors and do not reflect policy of either the u.s. department of energy or the u.s. government.2see a. patrinos and d. drell. 2002. òthe times they are achangin,ó nature, 417: 589590 (6 june).3see chapter 17 of these proceedings, òpotential effects of a diminishing public domain on fundamental research and education,ó by r.stephen berry.4the genome international sequencing consortium. 2001. òinitial sequencing and analysis of the human genome,ó nature 409: 860921.5j. c. venter et al. 2001. òthe sequence of the human genome,ó science 291 (5507): 1304.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.162the role of s&t data and information in the public domaineffort of the international human genome sequencing consortium, led by the u.s. national human genomeresearch institute but also comprised of the doe, the wellcome trust, and many other partners, whereas thescience paper resulted from the more recent effort of celera genomics corporation, a private company based inmaryland. although the consortium had practiced, since 1996, a policy of unrestricted rapid release (and deposition into genbank) of their sequencing data as they proceeded, celera put its data up on its web site(www.celera.com) upon publication in science. celera and science worked out a limitedaccess arrangement. onapril 5, 2002, science published two papers6,7 reporting the draft genome sequences for two subspecies of rice,oryza sativa; one is from syngenta international inc.õs torrey mesa research institute and was published withlimitations on data access essentially identical to those associated with celeraõs human genome sequence publication. considerable controversy has resulted from these policies; however, this presents a challenge to makeconstructive suggestions for ways to move forward that might reduce the impasse and perhaps promote greaterdata sharing.one possibility that has been proposed in various contexts8 is to start a clock on the deposition of certain datawhereby a journal or other depository agrees to restrict access to the source data underlying a paper for a specifiedduration; other data could be housed with a trustee who ensures that the data were indeed deposited at the agreedtime. careful provisions would be required both for how long the clock is set to run as well as precisely when itstarts, but the idea is to permit a set duration for commercial exploitation (including the filing of patent applications) on inventions derived from the data. the u.s. patent and trademark office allows up to one year before aprovisional patent application is converted to a utility patent application, giving an applicant time to performadditional research toward developing an invention while retaining the early priority date; thus one year might bea reasonable time for such a clock to run, but this would be subject to negotiation. this is similar to past practiceswith databases such as the protein data bank.the responsibility for implementing this scheme could rest with the journal or with a respected nonprofitfoundation (e.g., the institute for scientific information or the federation of american societies for experimentalbiology). in consultation with genbank (or a relevant public repository), the journal (or foundation) could provideaccess to the necessary files upon the expiration of the clock. it would be very useful to know the consequences ofvarying clock òperiods,ó as well as just how much privately held data actually contribute to the commercialviability of a company; such studies would provide valuable insights.as time goes by, data lose value both as new discoveries (and, in particular, new technologies for reacquisitionof the same data) are made and as science, as is its wont, proceeds unpredictably into new areas. this clockmechanism would allow a company to publish valuable data that would otherwise remain private while offeringsome protection for a limited duration for the company to use the data exclusively. this role might be uncomfortable for journals and trustees, so it is important to explore fully a mechanism that all sides would have confidencein. an added concern could arise if implications for national or international security (for example, potentialdetection signatures in a pathogenõs genomic sequence) emerged while the data were held on deposit beforepublication.there is an urgent need to find ways of giving incentives to the private sector, which now controls vast amountsof valuable data that have no obvious shortterm commercial value but could be of great potential research value.most of the human genome sequence (about 98 percent of it) is noncoding; allowing greater access to this part of itwould not seem to threaten celeraõs stated goal of discovering candidate drugs based on those portions of the genomethat encode expressed proteins. in addition, as is becoming ever clearer, the sheer volume of data from highthroughput sequencing centers (such as the sanger center in great britain or doeõs joint genome institute)challenges even the most advanced and sophisticated labs to mine it for value within a reasonable time frame.can incentives be defined that would induce celera, syngenta, and other similar companies to relax the accessrestrictions for some of their data? that question deserves to be explored because the benefits move in both directions, with academic expertise becoming more available to privatesector companies and the science carried out in the6j. yu et al. 2002. òa draft sequence of the rice genome (oryza sativa l. ssp indica),ó science 296: 7992.7s. a. goff et al. 2002. a draft sequence of the rice genome (oryza sativa l. ssp. japonica),ó science 296: 92100.8petsko, g. 2002. ògrain of truth,ó genome biology 3(5): 1007.11007.2.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4163private sector becoming more accessible to academic scientists. ideally, this becomes a òwinwinó for both sectors.echoing professor berryõs comment earlier, this could be subjected to some exploration and evaluation.there are several precedents for successful publicprivate collaboration. the single nucleotide polymorphism (snp) consortium9 and the image consortium10 both involved partnerships that placed into the publicdomain valuable genomic sequence information (the first on single base pair variants useful for trait mapping, thesecond of complementary dna [cdna] sequences representing expressed human genes); the commercial partners became valued contributors. they made the assessment that the value of restricting such data was notsufficient in contrast to the benefits of making these data and resources freely available tools for the intelligenceand creativity of the widest possible base of researchers.another instructive example, one that is representative without being unique, comes from the keck graduateinstitute in california. industrysponsored research carried out at the keck institute, involving keck faculty, isbuilt upon a carefully negotiated contract. both parties work out who does what, when and where, and who willown what results. of particular concern is the role of students whose educational needs must take precedence; thismeans carefully negotiating disclosure issues and rights to publish so that the attainment of their degrees is notrestricted. fundamentally, this requires the parties involved defining a work plan, benchmarks and milestones, andthe terms of a mutually acceptable contract. although this may not be easy, and the exact conditions need to betailored to the specific parties and their needs, the keck institute has succeeded in using negotiated contracts withprivatesector companies to attract research support, advance both the scientific work of their faculty and theeducation of their students, and contribute to the commercialization of scientific knowledge. in fact, practices suchas this are not uncommon, although they do not garner the publicity that the disputes do.are some constraints on data access preferable to not seeing the data at all? we believe they are. is theacademic scientific community willing to forego the science being done outside the groves of academe? if thisis to be the policy, then an increasing fraction of the 60 percent or so of genomics research conducted in the labsof private firms will remain unavailable to academic and government scientists. that is, in our view, too high aprice to pay.our case is reinforced by actual practices among most genomics firms. they do not publish their data. themany firms sequencing cdnas and identifying snps, for example, have information that would be immenselyvaluable to academic researchers if it were publicly available. the firms have, however, chosen not to publishthose data, preferring instead to patent genes as they are characterized and to sell access to their databases underagreements that protect data as trade secrets. that is their right. we should, however, be creating incentives forcompanies to publish data when they choose to and to facilitate such publication within proprietary constraints,rather than clamoring for policies that will push firms toward nondisclosure.sir isaac newton is widely credited with the observation that òif i have seen farther than others, it is becausei was standing on the shoulders of giants.ó the steady progress of science is founded on the traditional concept thatindividual scientists assemble knowledge òbrick by brick.ó we believe that full and unrestricted access to fundamental research data should remain a guide star of science because centuries of experience suggest that it is themost efficient approach to promoting scientific progress and realizing its many benefits. however, we must alsoaccept the current realities.at no time has science ever been the exclusive province of those in academia; however, today theproportion of highquality science taking place in the private sector (e.g., the invention of polymerase chainreaction technology and the development of crelox recombinase gene knockout technology) is impressive asnever before. the potential in the private sector for productively collaborating with the academic or government scientist is greater than ever before. we should not bemoan this development but should welcome it.privatesector science has its legitimate interests too. the burden of argument is on the academic sector toattract and justify greater openness on the part of privatesector science and to state clearly what the benefitsto the private sector can be.9see the snp consortium web site at http://snp.cshl.org/, as well as chapter 28 of these proceedings, òthe single nucleotide polymorphism consortium,ó by michael morgan for additional information.10see the image web site at http://image.llnl.gov/ for additional information.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.164the role of s&t data and information in the public domainwe offer the following conclusions for consideration by the national academies as they explore the role ofscientific and technical data and information in the public domain:(1)openness, by and large and as a guide for public funding of fundamental basic research, is a verysuccessful policy because it generates data that in unpredictable ways lead to exciting insights into natureõsworkings.(2)it is the appropriate role of the private sector to exploit open basic research to develop and commercializevaluable products. it is what the private sector is good at. the amount of their investment is large (and may incertain areas exceed that of the public sector) and the quality of the resulting discoveries is very high.(3)we need to explore aggressively compromises and quid pro quos to attract privatesector companies toloosen their hold on that portion of their data that could benefit fundamental research, but in ways that do notthreaten their intellectual property concerns. by working together, in creative ways, everybody can benefit.(4)we suggest some mechanisms, none particularly novel, that could be used to increase private sectorpublic sector collaboration. importantly, we think this is an area of potential opportunity.(5)different schemes (e.g., timers, impartial trustees, incentives, bilateral contracts, publicsectorðprivatesector consortia) can be put to experimental test to learn which work better, with what partners, and under whatcircumstances. we do not pretend to have all the answers, but we do assert that the exploration is worth undertaking.(6)if we succeed, the scientific and financial benefits can be enormous; if we fail, so too could be the costs.science will continue to advance regardless; but for selfevident reasons, we all would like to see it advance asrapidly as possible in the united states.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 416516522academics as a natural haven for open science andpublicdomain resources: how far can we stray?tracy lewis.i would like to start with the observation that has been made at this symposium, which is that, until recently,academic institutions have been regarded as a safe haven or safe harbor for open science. i would like to brieflyreview the rationale for that point of view. then i would like to ask, to what extent is it possible to export the normsof openness away from the ivory tower to a corporate setting? finally, i would like to address the issue of how theacademic sector tries to accommodate both private and public sponsors of research.the argument for why open science is such a good fit for academics hinges on two perspectives. the first isthe idea that scientists derive great satisfaction from posing questions and solving problems, and to maximize thatsatisfaction, they should operate in an open environment in which they can share their ideas with colleagues andbase their solutions on information they receive from colleagues and students. a second rationale for open sciencein academia, which is a bit less transparent but equally important, is that open science is really the glue that holdstogether the academic job market for scientists.the argument that paul david put forth in a very convincing fashion is that, by their very nature, scientistswork in specialized areas that laypersonsñin particular, university administrators and sponsorsñcannot knowmuch about.1 as such, there is an information gap between the people who are paying the bills and employing thescientists and the scientists themselves. this presents special problems then for how to evaluate the scientistsõoutput, what scientists to hire, what scientists to retain, and so forth.the ingenious solution that openness allows is that other scientists working in the area, the peers, can be usedas an information source to evaluate the operative science. of course, this requires that there be full and opendisclosure and that the information banks or the public domain upon which peer review takes place is as completeand current as possible. the scientists themselves derive benefit from peer review. not only do they get feedbackthey also derive personal recognition from their peers and establish a professional reputation, and they signal theirvalue to the marketplace. in addition, the information value of peer review is one that sponsors can use in makingresource decisions about which scientific programs to promote and which scientists should get grants. therefore,the openscience norm can be seen as a social equilibrium held together by a number of selfreinforcing factors.one question we might ask is, does this paradigm work well outside of academics? when i sought to answerthat, i came up with two examples. one of them is quite currentñthe opensoftware movement. the open1see chapter 4 of these proceedings, òôopen scienceõ economics and the logic of the public domain in research: a primer,ó by pauldavid.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.166the role of s&t data and information in the public domainsoftware movement, i would argue, has many attributes that are quite similar to the academic setting. it startedfrom academic origins in the 1960s and 1970s as collaboration between university and private foundation scientists. it benefited from some visionary leadership on the part of richard stallman and linus trevault, who had avision of promoting software in an opensource way; that is, the stock of knowledge regarding software should beallowed to pass unfettered or unrestricted to downstream users and developers. not only was the spiritual leadership and the institutional infrastructure in place, but there was also a very important contractual relationshipestablished, the socalled general public license, which provided the legal mechanism for having this knowledgepass downstream for use without any restrictions.aside from this, there are a number of supporting factors in the opensource movement, many of which haveacademic types of characteristics. for instance, the developers of software often benefit directly from extending andmodifying the software. often the developers are working with the users. with openness it is possible for thedevelopers of software to delegate to the users decisions as to how to improve the software. after all, it is the userswho ought to have a much better idea of which aspects of the software code should be developed and changed. as aresult of this interaction between users and developers, there is an interesting and valuable cross fertilization of ideas.we see the same needs for gratification among programmers. they enjoy sharing their latest challenges insolving software problems, just as academic scientists derive similar benefits. this is important because, like academic scientists, there are probably relatively few people who can appreciate the efforts that programmers put forthand the value of their products. thus opennessñthe ability to share new ideas and to document new approachesñallows the workers in this industry to gain personal fulfillment. it also allows programmers, like academics, to signaltheir abilities in the marketplace so that potential employers recognize their capabilities and value.one interesting question for economists is to ask, òwould there ever be any corporate support for opensoftware?ó they already exist in the marketplace. there are firms like red hat, who derive benefit from supporting open software because they can sell complementary products, they can manage the software in such a way tomake it more accessible, and they can provide instruction manuals and information to move the software from thedeveloper to the user. there are other firms such as ibm who find it in their best interest to support open softwarebecause they can use that open software with their own proprietary hardware and software products to offer a muchimproved product to the marketplace. so there is a combination of the open product and the proprietary product toproduce an even better product. the existence of sidebyside open software and proprietary software brings intoquestion whether open software will survive and, if so, in what segment of the market.open software has some inherent advantages over proprietary software. as i mentioned before, it gives rise togreater progress and cross fertilization because it is open and because users of the software give direct feedback tothe developers as to the kind of products that they want. a second major advantage, which i have alreadymentioned, is that it is more pleasurable for a programmer to work in an opensoftware type of environment. hegets more feedback, he gets more direct gratification from sharing his exploits with his peers, and he is also moremobile in the marketplace because his strengths and capabilities can be signaled more directly to other employers.there are, of course, advantages as well for proprietary software. one is that proprietary software companiescan directly recoup revenues from the sale of their product. they do not need to develop complementary products,as is the case in open software. proprietary software is more likely to appeal to a general audience because theprocesses of developing the software, explaining how it is used, and making it more user friendly are activities thata proprietary software manufacturer can afford to undertake as the costs can be recouped from doing so. opensoftware, however, tends to be more difficult, less accessible, and is a product that in reality is confined mostly toinformation technology professionals.taking all this together, what we would predict, and i think what we are seeing so far, is that there will be acontinuing role for open software. most likely, it will be in the information technology professional segment of themarket, but proprietary software will probably exist side by side with open software in a different market segmentñthe more general user segment. this certainly is an interesting example of how some of the norms ofopenness we find in academics may survive in nonacademic or corporate types of settings.another example i will talk about briefly is the experience of the early days of silicon valley in the 1970s and1980s. during that period, silicon valley was an exceptional region in the sense that there was an unprecedentedera of technological progress and innovation. it was characterized by free exchange of information wherebythe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4167scientists and researchers at different firms were allowed, and in some cases even encouraged, to share informationwith their colleagues at other firms, either informally over coffee or after work, or in formal settings at conferencesand seminars. the information exchange among firms resulted in a considerable amount of cross licensing oftechnologies and ideas.this unusual degree of openness was supported by several factors. one was an academiclike preference fordisclosure. scientists and engineers were an integral part of the commercial success of the firms in this fledglingindustry. in addition, scientists and engineers had a very strong allegiance to their professional academic affiliations, probably more so than the allegiance that they had to these young upstart firms that were very small,sometimes with just 50100 employees. as a result, given the importance of the scientists and engineers in thisindustry, they demanded and received as an employment condition the right to operate in an open environmentwhere they were free to exchange ideas with colleagues at other firms. they were relatively free even to movefrom one firm to the next and take some of their intellectual ideas and property with them. although the firms inthis industry perhaps did not like that, there was little they could do if they wanted to attract and maintain the verybest scientific and engineering talent available in silicon valley at that time.there were additional factors that gave rise to openness. one was that silicon valley benefited from whateconomists would call agglomeration economies, meaning that there was a large concentration of companies insilicon valley working on similar or complementary products. coincident with that was a very large, welltrainedworkforce of scientists and engineers concentrated in the valley at nearby schools in the san francisco bay areasuch as berkeley and stanford. this meant that information flows between firms and between colleagues atdifferent firms were very easy also, it was easy for scientists and engineers to move from one firm to another inthe valley. all of these factors conspired to produce, at least for some period of time, a very open environment ina corporate type of setting.what conclusions can we draw from this? the first conclusion is somewhat reassuring. one can find exampleswhere degrees of the open norm do exist outside of academia. the second is that in each of these cases in whichopenness did survive the corporate settings had some striking similarities with academic settings. these were allsettings in which the primary players in the industry were the people doing the scientific and engineering work.these workers demandedñand, because of their importance receivedñspecial treatment, in being allowed tooperate in an open environment. they were sometimes even encouraged to exchange ideas at professional meetings and to consult with colleagues at different firms. given the special circumstances of these examples, therealso is a negative message i think which comes from this. i would not expect that the norms of openness are likelyto overcome the proprietary norms of most corporate settings, except in exceptional cases where the nature of theindustry is such that it resembles an academic setting.i now would like to address the issue of how universities can accommodate both public and private sources ofresearch support. the description of the scientific open norm is somewhat more of an idea than a reality. in reality,academic institutions have for some time been facing increasing pressure to privatize. there are a number ofexplanations for this. the most fundamental one is that public funds are in short supply. not only do we rely onpublic funds to support such admirable goals as research in public goods, but there are a whole host of other publicgoods such as education, welfare reform, and national defense that likewise are deserving of funding and arecompeting for scarce federal dollars. in addition, relying on public funding, while in principle seems like a goodway to solve a lot of the proprietary concerns that one incurs when relying on private funding, nonetheless, is nota costless activity. some analysts estimate the cost of raising one additional dollar of tax revenue ranges between30 and 80 cents. this reflects the distortionary impact taxation has on individual employment and investmentdecisions. so again, the vehicle of relying on public funding in some cases is a fairly expensive and costly way togo. it should not surprise one that universities face a growing gap between their research desires and the quantityof public support for research. universities need to fill this gap. increasing reliance on private funding is theobvious solution.other factors, including the passage of the bayhdole act have created the infrastructure for universities totransfer intellectual property to the commercial sector. coincidentally technology shocks in biomedical andcomputer science research have enhanced the commercial value of university research. this has induced universities to seek and obtain greater private research funding.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.168the role of s&t data and information in the public domainin addition, one can argue that the presence of private funding is likely to be disruptive to the fragile socialequilibrium that i described earlier, which supports the norm of openness in academics. a number of speakershave described already how that process would unwind. however, i would like to point out that, although there aresome obvious challenges to academic institutions to accommodate various sources of funding, there is also a silverlining. private corporations, such as biotechnology firms who wish to establish a capacity to apply basic researchfor their own business, may find it worthwhile to strike alliances with universities and to establish their ownresearch groups. to the extent that they wish to do so, they may be forced, if they want to get into that line ofbusiness, to accept some of the norms of openness that go along with academic research. they may also find thataccepting these norms can be beneficial to them. it allows them to commit to do scientifically objective research.it gives them an advantage in attracting the best scientists and in establishing a reputation for being a leadingtechnology firm. these opportunities aside, some very challenging steps remain for academic institutions to taketo successfully accommodate different funding sources.in concluding, i make three suggestions regarding strategies universities might undertake. first i suggest thatuniversities carefully manage the portfolio of private and public research they undertake. here, they could learn alesson from accounting firms. accounting firms have learned recently that packaging auditing and consultingservices at the same time to the same clients is a bad idea. this situation positions the accounting firm in a hugeconflict of interest. it is difficult for the auditing arm of an accounting firm to issue an honest statement about thefinancial health of a client, knowing that in doing so, it may risk losing the lucrative management consultingbusiness that it has with that client. we have seen in recent months some of the abuses that can occur. accountingfirms have learned that it makes sense to sell off the management consulting activity to an independent firmthereby breaking up these two activities. why? because these two activities when grouped together just do notmesh. they present such perverse incentives that one could not possibly expect one firm to perform thesecontradictory activities in a satisfactory way.this principle applies to the university as well. universities should discipline themselves to reject privateresearch that would enlist their advocacy or that would restrict their ability to disclose research findings. theyshould separate out research that hinders the universities from providing public education and research. oneshould apply the same principle to individual faculty. faculty should not be asked to undertake multiple tasks,which inherently conflict and interfere with each other.a second, and related, suggestion is universities should adopt jobrelated compensation. universities undertakingresearch from different sponsors are going to ask their faculties to engage in various activities. it makes sense tocompensate a faculty member based on his performance on the tasks he undertakes. finetuning compensation to theparticular job each faculty performs allows the university to target salary and resources to the most valuable areas. it alsoallows the university to compensate faculty according to whether their research is privately or publicly sponsored.my final suggestion concerns the transfer of research and technology to the corporate sector. i recommend theuniversity tailor revenuesharing arrangements to suit the type of research transfer it undertakes. given thedifferent research output the university may transfer, it is unlikely that one arrangement, such as exclusivelicensing, will fit all applications. instead the university should develop a menu of transfer mechanisms conditioned on two important factors. one would be the corporate sponsorõs requirements for cost recovery andexclusive access to research findings. the second factor would be the opportunity costs to other researchers ofhaving incomplete or delayed access to the research findings. transfer agreements should reflect these factors incomputing compensation for transfer of research findings to corporate sponsors. transfers permitting greatercirculation of research to the public domain should be performed at lower cost to the sponsor.some of the symposium speakers have suggested one cannot expect universities acting alone to be faithfulagents for the public good. if this is true, one might establish a standard for sharing arrangements. this wouldprevent a òrace to the bottomó where universities offer overly attractive transfers to compete against others forprivate funding. standards should, however, provide universities enough flexibility to tailor transfers to differenttypes of research.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 416916923new legal approaches in the private sectorjonathan zittrain.in this presentation i hope to shed light on the current state of the public domain, especially with respect totechnical and scientific data, and then describe several approaches that tend to eschew the public domain in itslegal sense in favor of a rights regime that more subtly allocates power between author and downstream users.when we talk about access and the public domain generally, what do we mean? it might be useful to discussthem in light of scientific and technical data that have occupied a large part of this symposium.the proprietary, the public domain, and the space betweenfor much scientific and technical data, typically one cannot assert a copyrightñat least within the unitedstates. these are the type of data for which there is not enough creative work in the expression to merit copyrightprotection. as a result, those who want to protect (or, depending on oneõs point of view, hoard) these data are leftto other devices, such as secrecyñsharing the data with some, but not the rest of usñand contract. in this case,when the proprietor chooses to let others access the data, they impose extra òprivateó law, created in the transactionbetween publisher and consumer.contract, however, is always limited by òprivity.ó i might have an agreement with you that you promise notto further share my data, but then once you do and the person with whom you have shared the data further sharesthem, it is no longer easy for me to limit consumption. that is, i might have an actionable disagreement with theperson who violated the contract with me, but not usually with consumers downstream.furthermore, there is digital rights management, which julie cohen has addressed.1 even if a work is notprotected legally, one may simply òprivicateó itñthat is, publish it far and wide, but publish it in a manner, thanksto technology, that makes it hard for people to do with it what they will, simply because their respective computerswill not let them.finally, tied to technologies of digital rights management are laws concerning circumvention of those technologies. if a person attempts to figure out how to do something that their computer will not permit, and if theythen seek to apply or share that knowledge, in many instances they could go to jail.1see chapter 15 of these proceedings, òthe challenge of digital rights management technologies,ó by julie cohen.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.170the role of s&t data and information in the public domainin the case of creative work, one can have all the protections previously mentioned for the scientific andtechnical data, with the addition of copyright protection. because copyright is the default rule for creative works,copyright holders avoid the privity problem that occurs with contract.how does this work? if i create a work, i can assert copyright in it, provided that it is creative enough that ican. i ògive it to youóñi license a copy to you or you pay me to own a copy of the workñbut if you further makecopies and the people downstream from you make unauthorized, unprivileged copies, i can go after all of you asa matter of law. in fact, since 1976, i do not even have to put the copyright mark in a symbol on my creative workto have the copyright attach.so if you are surfing the internet and encounter a wonderful haiku and there is no copyright symbol on it, itdoes not mean that the work is not copyrighted. there are plenty of peopleñlawyers usuallyñwho get up atlecterns like this one and sow fear, uncertainty, and doubt, warning that works published online are in all probability copyrighted, so if in doubt, do not do anything with the work.if this is the regime we have for these two types of data, what kinds of material do we have in the publicdomain today? we have work created by nonhoarders, those òcrazyó people who give their work away forreasons that in the last session were explained to be actually quite rational. that sort of work can become partof the public domain either by choice or by patron encouragement or even requirement. for example, if aresearcher receives a u.s. government grant, the terms of the grant might require that the researcher share thedata and let others make derivative works from them. yet often the patron in these types of arrangements isa universityñand as others have discussed during this symposium, universities today are torn betweenwhether they want to be dot edu or dot com. while they are trying to figure that out, universities may not bethe ones to rely upon to encourage or require materials to be shared freely; indeed, they may have theopposite agenda.so that is how the system, generally, is working. for someone wishing to release into the public domainscientific and technical data for which one cannot assert copyright in the first instance, that person need simply failto take the previously discussed steps to protect that type of datañthat is, fail to keep the data secret, fail to writea contract, and fail to create and apply a digital rights management system.but if the work at issue is a copyrightable work, under the current system one has to take certain steps todisclaim it. a person can choose to distribute this type of material to others, either formally or informally;however, they may not know the legal steps necessary to enter it into the public domain. instead, it is more oftenthe case that the person chooses not to enforce the rights that are legally retained and that other people come toknow that.a final way that creative material enters the public domain is through copyright expiration. copyright is for alimited time, which at this moment for corporatecreated works in the united states is 95 years. of course, thesecopyrights have been extended retroactively repeatedly, rendering such entry mostly theoreticalñunless thesupreme court holds for the plaintiffs (for whom i am cocounsel) in eldred v. ashcroft, a case currently pendingbefore the court.2we also have the informal, de facto public domain, brought about by the existence of photocopiers, personalcomputers, and the internet. even with a set of òbackgroundó rights reserved to the author in any creative work, the factis that most published worksñwhether or not they are published onlineñare largely available for use. we have had aculture that permits a certain amount of copying for personal use, and many activities that would count as legalisticviolations of copyright are neither frowned upon nor fought against by copyright holders, much less the general public.the situation as it stands, then, is in flux. the largescale publishers who usually benefit from some level ofcopying and sharing are now well aware that photocopiers, personal computers, and the internet existñand that intheir current incarnations, they represent a threat to prevailing business models. these publishers are unhappy,they are litigious, and they are hiring good coders to write digital rights management systems.3 they have a certainzealous righteousness to their position and freely use the language of theft to describe what is going on when, for2see eldred v. ashcroft 537 u.s.  (2003). the supreme court decided in favor of the defendant. for additional information, see http://www.supremecourtus.gov/opinions/02pdf/01618.pdf.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4171example, consumers use peertopeer networks to transfer copyrighted music files among themselves. it seems thatthey hope that their indignationñarguably borne out of a newly expansionist cultural view of copyrightñwill beadopted by future generations.an acknowledgment of the everwidening gulf between the attitudes of intellectual property producers andconsumers is evident in a recent report on the creative industries published by the british patent office.4 the reportsought to consider ways of improving òthe publicõs perception of the need for intellectual property and itsrelevance to so much of what we do in our jobs, at home, at leisure and in education.ó5 the report states: òthereis general acceptance that terminology used by ip practitioners as a result of legislative authority is cumbersomeand not user friendly. an important step in achieving greater understanding and acceptance of licensing and thevalue of intellectual property is therefore finding an alternative way of referring to intellectual property.ó (thebritish patent office evidently has no favored replacement for the phrase, suggesting only that the matter be placedunder further consideration.) another recommendation in the report is that òschool children should recognize theirown creativity by including the copyright symbol on their course work.ó6this reveals the cultural battle between society looking at the intellectual property regime as something that ismerely holding them back from what they reasonably want and by all rights could do (before the sleeping giantsawoke) and society viewing the intellectual property regime as a useful instrument with which to protect theirintellectual fruits from misappropriation.attempts to manipulate cultural views aside, the current situation remains that there are two separate baselinesfor making use of othersõ work. for informal useñthe kind of use that i have made by including othersõ clip art inmy accompanying powerpoint presentationñour consciences are basically the limit. for formal use, however, weturn to lawyers. if i wanted to take this presentation and publish it as part of a book, i would have to obtaincopyright permissions. i am working on an internet law casebook right now with four coauthors. putting this booktogether, i am obliged to send out clearance letters for every fragment of othersõ work that i want to incorporate.it is a formal publishing enterprise; in a formal situation, with a company that represents a viable target for legalaction, all of the defaults are reversed. unless we literally get clearance, i cannot include the fragment in my book.yet, again, if the book were simply a presentation for my students, all the defaults would flip back.it is in light of these tensions between producersõ and consumersõ perceptions of ip and between the differentstandards for using otherõs work that we turn to approaches to promoting the public domain and open access topublic domain or nearpublic domain materials.new approaches for promoting the public domainfreeing codeyou have heard in this symposium about approaches used in software licensing. as to that, i believe softwareapproaches are blazing a trail, yet they are also legally untested. in other words, we have no idea whether some ofthe approaches i am about to describe actually work as a legal matter.3while most recent cases have involved music and movie publishers, other major publishers have been waging this battle for a long time,and other speakers at this symposium noted the proprietary mindset of many scientific publishers, especially the elite ones. cases from themid1980s involving mead (reed elsevierõs predecessor) and west arose in large part because of the new dimensions of infringement thatwere made possible by advances in technology. in addition, in williams & wilkins company v. u.s. (487 f.2d 1345 (1973) (affõd by equallydivided u.s. supreme court at 420 u.s. 376 (1975)) claims by a publisher of scientific and medical journals against the national institutes ofhealth and the national library of medicine for royalties on articles photocopied from copyrighted publications were denied. williams &wilkins was an early and influential case on the subject of the ramifications of the availability of photocopiers on infringement claims and fairuse defenses. more recently, reed elsevier and thomson have been among the most forceful lobbyists for strong database protectionlegislation in the united states and elsewhere.4report from the intellectual property group of the governmentõs creative industries task force, available online at http://www.patent.gov.uk/copy/notices/pdf/ipgroup.pdf.5id. at pg. 3.6id. at recommendation 7.11, pg. 23.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.172the role of s&t data and information in the public domainthe first approach has been employed by the gnu organization to promote the free software movement.what is free software? there are four components to it: the freedom to run the programñeven if you did not writeit yourselfñfor any purpose; the freedom to study how the program works and adapt it to your own needs; thefreedom to redistribute copies so you can help your neighbor; and the freedom to improve the program and releaseyour improvements to the public so the community as a whole benefits.freedom here is of course described not in terms of author but rather consumer of the work, or someone whomay want to do something with it. the ideology of gnuõs founder, richard stallman, is that it would be best if allsoftware worked this wayñand copyright is not something, if all its rights are asserted, that allows those freedoms.so what is stallmanõs approach, short of repealing copyright law for software (which, to be sure, he would like todo)? he does not urge software authors to release their code into the public domain simply by failing to copyrightit. this approach is disfavored because in the absence of any information about copyright, the consumer may beconfused, not knowing whether or not the program is truly free to be used as he or she wishes. another concern thatwould arise if software were released into the public domain is the òproprietizingó of that software. if somebodytakes a piece of software released into the public domain and makes something even better with itñone starts withmosaic and ends up with internet explorerñshe can copyright the result. it is entirely legal to take material fromthe public domain and use it as the basis for new, derivative, copyrighted work. so to prevent the privatization ofwhat had originally been free, stallman rejects the idea of releasing work into the public domain. instead, hesuggests asserting copyright with a carefully crafted òcopyleftó license. one restriction of copyleft is that whensoftware is based on an original work under copyleft, the new work must inherit the copyleft license. this makesit so that all derivative works are covered by a license whose substantive terms operate to keep the work freeñfreefrom downstream proprietization.we have seen some other approaches of this sort in the private sector. mozilla.org is a recent entry to themarketplace, providing the netscape source code but putting certain restrictions in its corresponding public licenseso that downstream changes must themselves be free under the same kind of license. sun microsystems has takena similar approach, using a selfdescribed opencommunity processñand trademark as the instrument of control.with this approach, people can do what they want with certain implementations of java, but if they stray too far inways that might proprietize java, sun asserts trademark infringement. the derivative product can then no longerbe called java.we also have a third example from the private, nongovernmental sector: the internet engineering task force,a group that has developed certain fundamental protocols, such as simple mail transfer protocol and transmissioncontrol protocol/internet protocol, that make the internet work.7 this is an example of what james gleick calledòthe patent that never was.ó8 the internet engineering task force does not release these standards into the publicdomain. instead they have an organization, namely, the internet society, that holds the copyright for the purposeof replicating those standards and keeping them open a la copyleft. internet standards are free for use, but they infact are copyrighted by the internet society, and you would have to answer to them were you to try to proprietizederivative standards.from code to contentcan the model formalized by stallman for software be applied to other creative work? to address thatquestion, i turn to a discussion of creative commons.creative commons is a relatively new organization, founded to seed the lessons learned from the opensource(or òfree softwareó) movement.9 creative commons starts with the conceptual understanding that copyright itselfis a bundle of separable rights. copyright need not mean one holds back all rights. it can mean that one holds backsome rights while giving up others. creative commons aims to create a standardizedñindeed, machinereadableñway for people to set elements of their works free categorically; that is, cleanly and clearly, but notnecessarily wholly.7see the internet engineering task force web site at http://www.ietf.org for additional information.8see james gleick. 2000. the patent that never was, available online at <http://www.around.com/patent.html>.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4173creative commons does have a humanreadable as well as a machinereadable component; it is a basicòcommons deedó that is attached to a work using creative commons templates. this deed explains in plainlanguage what can and cannot be done with the work, either by consumers or by wouldbe authors who wish tocreate something new from it. there is also a lawyerreadable component, which is the legal code, the actual textof the license. the machinereadable component is the metadata, tags that make it so that computers can index,search, or display the work for others.to keep the process simple, creative commons templates allow control by an author along five axes.10 first,one can introduce the work entirely into the public domain using language provided from creative commons tomake it clear that it is so released. if someone wants to put a work, or an excerpt of a work, in their textbook, theyneed not worry about sending a permission letter to anybody because the work is declared publicdomain material.another axis deals with attribution; there is a license signifying that others can do as they please with thework, but that they must also give the original author credit for having created it.the third axis is ònoncommercialóñmeaning that, in general, one can copy or otherwise exploit the work, butonly for noncommercial purposes. i am not sure creative commons has worked through exactly what the boundaries of noncommercial are, so this is likely to be developed further as the organization matures. the endpoints, atleast, are clear: one could not put a work so licensed into an anthology and sell it, but if somebody wants to takethe work and talk about it or even repost it in full on a web log, that would be fine.note, by the way, that the noncommercial axis is very different from the conceptual orientation of the freesoftware movement. richard stallman does not care if software (or its derivatives) gets sold for money, so long as,once it is sold, the person who buys it can make as many copies as he or she wants and can see how the code works.a fourth axis among the creative commons license attributes is òno derivative works,ó which means that awork can be copied, but it has to be copied exactly as it is found; one cannot incorporate it into any derivativework.finally, there is copyleft, which is to sayñpresuming that the author has not said òno derivative worksóñthatconsumers are required to inherit the licensing terms, replicating them in any derivative work that they create.given this general plan, you can imagine creative commons as an organization functioning under a fewdifferent models:(1)a central conservancy. the idea would be for creators not only to adopt the licenses, but also store thework with creative commons. if people went to <creativecommons.org> and searched for òowl,ó they could findpictures of owls that people have taken or rendered, with the respective license terms attached. creative commonshas rejected this model for a number of reasonsñsuch as the fact that they cannot necessarily build a functioningdigital library given the kind of infrastructure that would be required to support it. other reasons include the factthat creative commons could not necessarily verify or validate the works coming inñand perhaps the work wouldnot really be authored by the person submitting it, thereby subjecting creative commons to possible liability forhosting it.(2)another option would be a distributed conservancy. in this conception, the material is hosted somewhereon the internet, or perhaps in a library as a physical object, but creative commons would maintain a central indexof works under creative commons licenses. one could go to <creativecommons.org> and look for owls, findseveral owls that meet the description of the clip art one wants, and follow the links to the work. that mightattenuate the legal liability that creative commons could face for contributors misrepresenting or simply beingmistaken about what rights they can convey or release with regard to the material in question.(3)finally, creative commons could build a completely distributed conservancy with no index at all. themain product of creative commons in this conception would be the licenses themselves. how would there then bean index? one would simply go to a search engine like google, type in òowlsó and add search terms matchingmetatags indicating the presence of creative commons licenses.as far as i can tell, creative commons is somewhere between models (2) and (3) in this taxonomy.9see the creative commons web site at http://www.creativecommons.org for additional information.10see http://www.creativecommons.org/learn/licenses/ for an explanation of the creative commons licensing options.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.174the role of s&t data and information in the public domainmy own sense so far is that creative commons is a spectacular project. it is interesting to note that it is notnecessary or even likely that a work associated with creative commons will literally enter the public domain; theproject is more complex and more flexible. should we then ask whether the glass is half empty or half full?creative commons gives people an easy meansña computerreadable, searchable meansñto assert what theydesire with respect to content they create. this may result in more content being more clearly omitted from thepublic domain than clearly placed there, as candidates for creative commons licenses could be drawn from worksthat would otherwise be released entirely into the public domain, rather than works that would otherwise becopyrighted in the traditional way. digital rights management systems could easily be placed on top of themachinereadable code, making the licenses selfenforcing. so you might even think of creative commons asconverging with what the british patent office report called for: a greater understanding by people of the rightsthey have and can assert in the works that they create; an easy, simple, nonlawyerly way of expressing that desire;and, to the extent possible through machine execution, having it be so. one should note that this is a goaldifferentñperhaps laudable, but differentñfrom the goal of having as much material as possible enter into thepublic domain.another aspect of this discussion is important to note: the problem of derivative works is less a problem inrelation to creative works than it is in software. in creating software, the purpose of using anotherõs source code isto create a new code that does something else. in this sense, one directly òbuilds on the shoulders of giantsó withsoftwareñsomething one does more conceptually than literally with creative work. an author does not literallyhave to incorporate anotherõs poem into her own to have a successful or meaningful new work.finally, in the area of the scientific and technical data not subject, at least in the united states, to copyrightprotection, creative commons offers an opportunity to affirmatively and publicly catalogue the data, allowing theauthor to make clear the fact of his or her original production of them for attributive purposes. scientific researchers thus can encourage maximum dissemination of their discoveries and methods, without sacrificing the fact oftheir own contributions to those discoveries.what, then, is the real value of creative commons? first, it helps us to identify works intended for the publicdomain. second, it helps people join the cultural melee. this is a battle over the description of rights, an assertionof copyright as an instrument, and not just an instrument for control. that part we understand quite well. creativecommons helps to underscore the fact that a legitimate use of copyright is not simply to stop others from copying,but also to give permissionñto imagine that something other than òall rights reservedó could be the phrase thatfollows oneõs assertion of copyright ownership.will creative commons work? that will depend on the value of the work committed to the public domain, orat least to public use, under the creative commons system. exactly what sorts of authors and work it will attract,we have no idea. this is one of those ideas so new that one really has to make a leap of faith to see if it is going towork. and then, if it does work, it will seem obvious that it was a great idea whose time had long since come.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 417517524designing publicðprivate transactionsthat foster innovationstephen maurer.most university technology licenses are extremely conventional. the university selects one partner and givesit an exclusive patent license. the partnerñwho now has a monopoly in the universityõs technologyñpromises topay royalties. however, this òexclusive licensingó model is only one possible transaction. many alternativebusiness models are possible and some have already been tried. we need to figure out which of these new ideas arewise and which would be utter disasters.i will start by reminding you that patents have important drawbacks for society and that it is often preferableto leave discoveries unprotectedñthat is, putting them into òthe public domain.ó i will then look at why universities focus so heavily on exclusive licenses. finally, i will discuss 10 alternative licensing models that can often doa better job of spreading knowledge.intellectual property versus the public domainit is easy to forget that intellectual property (ip) rights are, in fact, monopolies. they create incentives by letting theinventor stop others from using his invention. this creates an artificial scarcity in knowledge in exactly the same waythat a bakerõs cartel creates an artificial scarcity in bread. legislators and judges have always known this. in fact, the firstpatent statutes were at least as concerned with limiting the ip monopoly as creating incentives. that is why the englishparliament called its first patent statute òthe statute of monopolies.ó thomas jefferson agreed. when he set up theamerican patent system, he said that his central taskñwhich i think is our central task, tooñwas to draw òa linebetween the things that are worth to the public the embarrassment of an exclusive patent, and those which are not.ó1in addition to monopoly, patents have two other potential drawbacks. if i am allowed to patent a particularidea, how do i make money from it? the most obvious way is to hire someone to develop it, i.e., to turn it intoproducts. but if i want to make a profit, i must keep my costs down. so i am not going to hire everyone in theworld. instead, i will i hire one person or maybe two, and see what they develop. this approach works well whendevelopment is obvious and straightforward, but that is not always the case. in fact, there are two reasons why itmay be better to leave the idea in the public domain. first, suppose that the product made from the idea ispatentable. in that case, the whole world ends up racing to develop the idea. this means that society gets theproduct faster. this is a very valuable benefit when the underlying idea represents a fundamental advance like, for1quoted in graham v. john deere co., 383 us 1 at 11 (1966).the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.176the role of s&t data and information in the public domainexample, the laser. if we believe that universities produce more than their fair share of fundamental advances, weshould put their discoveries into the public domain.second, some ideas are òembryonic,ó i.e., the strategy for developing them is not immediately obvious. again,many university inventions fit this profile. suppose that a biologist discovers that a certain protein binds to theoutside of a cell but has absolutely no idea what it does. turning that kind of idea into a workable product is fraughtwith peril. if the idea is patented, the patent holder will hire one or two people to attempt development. they couldeasily fail. but if the idea is not patented, 500 people may decide to take a shot at the problem. if just one of themsucceeds, society will receive a huge payoff.why universities like exclusive licensesif patents and exclusive licenses have such drawbacks, why do universities favor them? there are at least fourreasons. the first reason is ideological. congress passed the bayhðdole act because it believed that industrywould not develop university inventions unless it received ip rights. there is some evidence for this position. ifyou go out and ask companies if they would have done a particular investment without licensing rights, theysometimes tell youñand it may be an honest answerñthat they would have said òno.ó but there are also manycases where companies were already using the invention before the university got wind of it and demandedroyalties. now the university has gone from disseminating knowledge to taxing it. at least potentially, bayhdolehas become a drag on innovation.the second reason that exclusive licenses are popular is that they have a powerful constituency. there is thelicensing officer, who is praised for bringing in cash. there is the faculty entrepreneur, who would like to get rich.and there is the university administrator or state legislator, who hopes to make the university at least partially selfsupporting. all of these groups want to maximize revenues. but the best way to maximize revenues is to act likea monopolist. in other words, stick with the exclusive licensing model.the third reason that exclusive licenses are popular is that universities tend to produce embryonic ideas.university technology officers and faculty members frequently say that they have no way of knowing whether anindividual idea will be valuable or not. so they obtain as many ip rights as they can. now it is one thing to patentan idea because you plan to develop it. that is an investment. but it is quite another to patent an idea in case it laterturns out to be valuable. that is a lottery. the result is overpatenting and a shrunken public domain.the final reason is that many people think that òexclusive licensesó and òstartup companiesó are evidence ofeconomic development. this is misguided in my view and comes from the fact that it is much easier to countlicenses than to track the number of people who use the public domain. nevertheless, the fact remains that manyuniversities subsidize patents and exclusive rights transactions. at the same time, hardly anyone subsidizes thecreative commons or other efforts to expand the public domain. so the net effect is that society is paying peopleto shrink the public domain.looking at the alternativesi want to describe three categories of alternative transactions. i will argue that the first category consists oftransactions that are unambiguously good for society. the second category consists of transactions where theòcoreó value of scientific datañthe right to use, modify, and republish informationñis unimpaired. i argue thatthis situation is always preferable to a conventional exclusive license. finally, i will describe a variety of transactions that are usually preferable to an exclusive license. even though these need to be judged on a casebycasebasis, we should probably encourage them. one way to do this is to put such transactions into some type ofòfavoredó or òsafe harboró category.situation 1: no public fundingthe first category involves situations in which the experiment cannot be done with public money or charity.in other words, nothing will happen without the private sector. of course, you should still try to negotiate the leastthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4177restrictive license you can. but suppose that your negotiating partner insists on an exclusive license. should youagree to give the private sector a monopoly right? many people are surprised to learn that the answer is òabsolutely, yes.ó a monopoly price is bad, but it is still preferable to not getting the information at all. you should dothe exclusive license.the clearest historical example i know about is cyclotrons in the 1930s. it was the first big physics researchprogram. it absolutely could not have been done with government money. e.o. lawrence made a deal with theresearch corporation. the research corporation figured that if it got enough cyclotron patents it could makecheap versions of radium and corner the medical isotopes market. now, as most of you know, world war ii andthe manhattan project intervened, so that medical isotopes became plentiful. but suppose they had not. supposethe research corporation had cornered the market. even if the new isotopes were only a few pennies cheaper thanradium, society would have still been better off. would they have been as well off as if the government had fundedthe whole thing? no. but at least we would have had the information.there is a point here about the federal science agencies. we have to reinvent the way that agencies do things.in this example, i assumed that the government was never going to fund cyclotrons. in many cases, however,scientists do not really know whether the government is willing to fund their projects or not. only the agencyknows. so my suggestion requires candor: the agency must be willing to say òweõre never going to fund thisñgo out and make the best deal you can.ósituation 2: tangential applicationsit turns out that scientific data have uses besides òdoing science.ó can we sell those uses while leaving thecore ability to consult, modify, and republish data in the public domain? and, if we do, can we earn enough moneyto support the database? the dot coms have produced several business models that are worth considering.the first model is to sell companies the right to use the database as content or a traffic builder. the basic ideais that posting data on a website òattracts eyeballsó to the hostõs other products. i will discuss an example of sucha deal below.the second model is to run an alert service. suppose you have a database and you keep putting new data intoit. if you know that somebody is interested in a particular category of data, you might send them an email alertwhenever relevant information is added. that service is worth something.the third model is to sell advertisements. in the general dot com world ads are incredibly lucrative. althoughads are less lucrative in science, some journals have said that they can generate about $250 per published paper.finally, there is data delivery. companies sometimes pay hundreds and even thousands of dollars to receivedata in special formats. for example, you can sell them a cdrom version that they can archive or make specialarrangements to deliver the data behind their firewall.i can see the emergence of a principle where we should always choose transactions that leave people free todo science. such deals are always preferable to traditional exclusive licenses. they are also preferable to somerecent experiments designed to make academic databases selfsupporting. the most famous example is probablythe swis/prot database in biology. it imposes substantial restrictions on academic usersõ ability to modify andrepublish entries.situation 3: safe harborsthe third category consists of business models that are usually preferable to exclusive licenses. unlike thefirst two categories, you cannot say that such terms are always preferable. however, it still makes sense to createsome type of favorable presumption or òsafe harboró that such deals ought to be approved.the first type of business model involves selling updates. dr. bretherton talked about the seawifs (seaviewing wide fieldofview sensor) experiment, where commercial fisheries receive data 2 weeks before thescientists do. nasa paid approximately 40 percent of what this mission normally would have cost. frankly, itsounds like they got a good deal. updates have also been tried in biology. for example, cardiff universityõshuman gene mutation database has been very controversial. celeraõs commercial subscribers receive humanthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.178the role of s&t data and information in the public domaingene mutation database updates 1 full year before the general public does. biologists are up in arms about that,and maybe they should be. but now we are arguing about how long the embargo should be. the answer will almostcertainly vary from case to case.the second model is to provide an enhanced version of a basic (nocharge) database. companies will oftenpay a lot of money for extra bells and whistles. in principle, the money can help fund your basic service. mybackground paper for this conference describes a deal between a corporation called synx and a biology databasecalled the human genome database.2 synx would have received the right to produce a premium version of thehuman genome database; in return, it would have paid royalties and delivered certain software to make thepublicdomain version stronger.the third model involves offering academic users a different, cheaper subscription rate. this strategy isambiguous because it can be used to maximize revenue. for example, celeraõs academic licenses generate a bigpercentage of the companyõs total income stream. on the other hand, you can also imagine setting prices tominimize the cost of academic subscriptions subject to the database breaking even. should you do that? there is alarge group of people who say that data should always cost exactly nothing. on the other hand, no such rule existsfor other inputs you need to do science. for example, hardly anyone complains that nature charges for a subscription. so it is a plausible question: do data prices become reasonable when they are comparable to journalsubscriptions?the fourth model involves disappearing ip rights. just because patents last 20 years does not mean thatexclusive rights have to. during the 1980s, harvard did a deal with monsanto where the exclusive right disappeared long before the patent did.the fifth model involves fixedfee contracts. for reasons i will not get into, many of the economic distortionscaused by monopolies go away if royalties are fixed in advance. what is really pernicious is the socalled runningroyalties, where i get a slice of your sales. so to the extent that universities negotiate fixed fees, that is a goodimprovement.the last model is nonexclusive licenses. these are tricky. if a university charges high enough royalties, it canend up licensing everyone in the world and still enforce a monopoly price. so nonexclusive licensing is only asolution if the university also charges royalties that are something less than the fullblown monopoly price. oneinteresting feature of the novartis and monsanto deals is that they both stipulated that the corporate sponsor wouldreceive a nonexclusive right to discoveries. a cynic might say that this did not matter, because if i already have anonexclusive right i am in an excellent position to demand exclusivity. after all, the university knows that this isthe best way to maximize revenue. so these deals could be a kind of catõspaw arrangement. but what if theuniversity committed itself to give out at least one nonexclusive license? variations on this theme are worthexploring.the politics of voting òyesóat this point, i want to make a confession. i once negotiated a traffic builder agreement between a worldwidecommunity of 600 mutations biologists and a company called incyte. the deal we made was that incyte would paythe community $2.3 million to build a worldwide database of human mutations information, which currently doesnot exist. in return, incyte would have received oneñand only oneñexclusive right. it would be the onlycommercial company allowed to post the database on its web site. there may be a more minimal version of a rightthat you could give to a company in exchange for real money, but it is very hard to imagine. politically, i think thatthis is a bit of a test case. if you cannot get a community to agree to this deal, you probably cannot get it to agreeto anything.so what happened? the community held a meeting to discuss incyteõs proposalñand, after protracted wrangling, decided not to hold any vote at all. as far as i can tell, there were two reasons for this. first, most memberswere hesitant: òincyteõs proposal looks fine to me, but i just saw this 20 minutes ago. maybe thereõs a catch here.2see s.m. maurer, òpromoting and disseminating knowledge: the public/private interfaceó at http://www7.nationalacademies.org/biso/maurerbackgroundpaper.html.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4179give me six months to think about this and iõll get back to you.ó of course, six months later the deal was no longeron the table.the other problem was that members were afraid that they would be punished for voting òyes.ó òeven if it isa good idea, this deal is so novel that somebodyõs bound to criticize me in the pages of nature.ó this problem onlygot worse after the representative from the national institutes of health (nih) gave her opinion. she completelyignored the incyte proposal and suggested that her agency might consider a grant application instead. people askedthemselves, òif nih wonõt venture an opinion, why should i?ówhat agencies can doboth of the foregoing problems involved a failure of leadership. society needs to decideñin advanceñwhattypes of transactions it wants. and it needs to give people the confidence to say òyesó if somebody goes out andobtains a suitable offer. neither of these things is likely to happen as long as saying òyesó requires a personaldecision by 600 individual members.i believe that funding agencies have a role here. however, it is not a traditional one. deciding whether aparticular transaction is in societyõs interest is not like peer review. you cannot answer it by appealing to scientificmerit. instead, the agency has to decide whether it has enough money to fund a particular experiment and, if not,how many rights it is willing to give the private sector to get the job done. you can even imagine a day when nihshows up at negotiations between the private sector and the academics and says, òiõll chip in some money if youloosen these restrictions.ó that will require some heroic changes, but it is not fundamentally unreasonable. in fact,nih has already become much more willing to write regulations that tell people to make sure that any privatesector deals include particular provisions. so they are getting into a hortatory mode.finally, the idea of doing new types of privateðpublic deals is not a theoretical subject. my own experience isthat industry will stand in line to talk to you. as soon as we talked to incyte, celera got jealous and asked to getinvolved. in the end, we had three or four firms offering to help. so these deals are feasible. it can be done. theonly question is whether individual scientists feel that they have a green light to do them. if you look at thestatistics, individual scientists originate most grant proposals, put together the most exclusive license deals, andcreate most startup companies. so my final advice is that the funding agencies need to decide which deals aredesirable, announce some clear guidelines, and then stand back. the scientists will do the rest.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.180the role of s&t data and information in the public domain18025emerging models for maintaining scientific datain the public domainharlan onsrudafter sitting through the presentations yesterday, i went back to my room and wrote a new talk. and afterhearing further presentations today i think we need a reality check, or perhaps more accurately a view from thetrenches. as we all know, the price that private scholarly publishers set for maximizing profits is not the same pricethat one would set for maximizing the scientific uses or distribution of the scholarly literature or databases. if apublisher can double the price for access to their journals or databases and lose fewer than half of their subscribers,their overall profits will increase. so this is of course what they have been doing.to get my biases on the table, i teach at one of the universities that has been marginalized by this process. inthis new information age, at my university, our ability to access the scholarly literature actually is decreasing eachyear as our library is forced to subscribe to fewer and fewer electronic journals. in addition, unlike paper journalswhere we still had a copy on the shelf, what we had access to last year is now gone.so how should we as scientists react to this situation? one of my first reactions was tell our dean of librariesto simply cancel wholesale all the publications of those publishers that are the worst offenders, to take a politicalstance. she says that it is alright for the massachusetts institute of technology (mit) to do so but if she does it shewill look like a crackpot. she has professors literally begging her to continue to subscribe to journals that nowannually are approaching the cost of cars. of course, no matter how high the price goes, the mits and caltechs willbe in that upper portion of the academic market best able to afford access to such journals. it is those of us in thelower third of research university wealth that are particularly hardpressed.should we simply abandon scholarly work and research in our poorer states? that is, should we allow themarketplace to determine which universities will have access to the core research literature and which will not?when you look at a state like maine we have one of the highest high school graduation rates in the nation, andthose graduates rank very high in the national assessment of educational progress in science. should we admitthat these highly qualified students should not have the opportunity to contribute to the advancement of science atthe university level? with a statewide average per capita income well below the national average, few of ourmaine high school students can choose to attend universities at outofstate or ivy league tuition rates.so if you believe in the democracy of education, what can we as individual scientists and professors do? iargue to my peers that support of the public domain begins at home. i webcast my graduate course class sessionsopenly on the web. i publish my syllabi, class notes, course materials, and most of my peerreviewed journalarticles openly on the web. if we can do it at poor universities why is it that our leading universities do not alreadyhave hundreds of their courses openly webcast and openly archived? it is not a big technical burden. just do it.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4181the faculty senate at my university proposed, and the university system administration adopted, a new formalintellectual property policy with a strong presumption of ownership by professors in the copyrightable teaching,scholarly, and multimedia works they produce. furthermore, all professors are highly encouraged by this policy tomake their works available through openaccess licenses or to place them in the public domain. thus, at myuniversity, it is now clear that faculty members do have the power to place works into the public commons.let us assume that the creative commons project proves to be a great success and hundreds of scientists startattaching openaccess licenses to their articles and datasets before submitting them for peer review. i alreadyattach such licenses to my submitted journal articles, and those articles are rejected summarily by most publisherswithout even being subjected to the peer review process.1 i receive letters from corporate attorneys telling me whythey cannot possibly accept the open commons license. will increased submissions by other scientists help placepressure on the publishers? possibly, but my guess is that most scientists will simply buckle.therefore, perhaps my university should pass a formal policy stating that any peerreviewed publication thatis not allowed to be within an openly accessible archive six months after publication shall constitute a lowgradepublication comparable to a nonrefereed publication. after all, such publications inherently are of less value to theglobal scientific community due to their limited accessibility. thus, on any applications or nominations forpromotion, tenure, or honors, such a publication could not be listed as a qualifying prestigious publication. it doesnot qualify as being in the peerreviewed, openarchival literature. if we impose this requirement at my universityalone, we will indeed probably look like a bunch of crackpots. if, however, the caltechs, mits, harvards,columbias, dukes, and other elite universities represented in this room start pursuing this approach, it very wellmay have an effect. however, those are the same elite universities that actually benefit from the current researchand publishing paradigms.administrators at these firsttier universities have very little incentive to make waves. ten years ago theoffice of technology assessment reported that 50 percent of all federal research funding went to 33 universities,and my impression is that those numbers have not changed much.2 these are the same universities that receive themajor share of corporate funding and are the primary beneficiaries of bayhðdole. it would take very enlightenedadministrators believing in a broader sense of scientific peerage and longterm preservation of science to actuallyrisk altering an incentive system of which their own faculty are the primary beneficiaries.3 will it happen? whoknows? hope springs eternal.perhaps in reporting the progress of past work when submitting research proposals to the national sciencefoundation and the national institutes of health, scientists should be requested to list only those published articlesthat are available in openly accessible electronic archives. right now when scientists at universities in the mediumto lower tiers of wealth are requested to peer review research proposals, we have no way of accessing much of thereferenced work because our libraries do not have access to those works. this degrades the peer review processand the overall quality of science. in my department, we serve as editors for and regularly publish in journals thatour library says it cannot afford to subscribe to. the national science foundation and the national institutes ofhealth have the power to fix this situation. however, it is not even on their radar screens. why not? because it isnot a high priority for the top 50 research universities because those universities will never be cut out of theliterature access loop by marketplace dynamics. the current situation, however, is perpetuating a caste system inthe ability to do highquality research in our nation. we can talk all we want to about economic and legal theories.however, to arrive at a sustainable intellectual commons in scientific and technical information, we will need to1 selfarchiving of an electronic prerefereed version can help circumvent some legal issues. see, by example, frequently asked questions athttp://www.eprints.org. however, this approach currently when applied generally results in cumbersome metadata and corrigenda maintenance issues.2 fuller, steve. 2002. òthe road not taken: revisiting the original new deal,ó chap. 16 in mirowski and sent, eds., science bought andsold, university of chicago press, chicago, il, p. 447 referring to office of technology assesment, 1991. federally funded research:decisions for a decade, u.s. government printing office, washington, d.c., pp. 263265.3 we obviously have some enlightened administrators. by example, caltech has been a leader in research selfarchiving (http://caltechcstr.library.caltech.edu) whereas mit has been a leader in making teaching materials accessible (http:// http://ocw.mit.edu/index.html).yet motivations and constraints vary among universities. thus, partial solutions by one university in addressing academic literature accessproblems may be impractical or not as useful for use or emulation by many others.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.182the role of s&t data and information in the public domainprovide incentive systems whereby it becomes very obvious to individual researchers that they will be far betteroff in terms of prestige and other rewards if they publish in forums where their works will be openly and legallyaccessible in longterm archives. again, i advise the funding agencies to just do it.now, please do not misunderstand my comments. there are actually some great benefits in pursuing researchin the hinterlands of science. i work with a small group of scientists that have managed on average a couple milliondollars of research funding over the past few years. all of these professors regularly receive offers to moveelsewhere. but, as paul david said yesterday,4 to do truly innovative work that is on the fringes of establishedresearch fields you are sometimes far better off to actually break away from those fields. that perfectly describesthe researchers i work with. they are doing very highquality research. access to scientific data and informationis just as critical to our faculty and students as it is to those at the top research universities, but we currently workunder a very different access environment.go ahead and point at the publishers but, as pogo said, òwe have met the enemy and he is us.ó we hold muchof the solution to our own access problems in our own hands. the solution rests in the hands of scientists, fundingagencies, and university administrators. our goal should be to provide all university students with the same accessto scientific and technical data and literature that the leading research universities have. we would all be far betteroff.i did not come here to talk about any of what i have just talked about, however. i came to talk about somesuccess stories, some emerging approaches for the widespread sharing of data and information that are actuallyworking and hold out great promise for scientists globally. in particular i wanted to talk about citeseer (formerlyresearch index), which purportedly provides access to the largest collection of openly accessible fulltext scientific literature on the web. its legal and technological approaches are very different from most other archivingefforts on the web, and the social dynamics it has created in the scientific community also are very different. thissystem currently contains over five million citations and a halfmillion fulltext articles. how can it be legal tohave a halfmillion fulltext articles openly accessible through this system when no one gave research indexpermission to copy those articles?citeseer5most of you are familiar with at least some of the major specialty collections of fulltext journal articles thatare freely accessible on the web. for instance, the nasa astrophysics data system has 300,000 fulltext articlesonline; highwire press has about the same number but focused in the biomedicine and life science fields. there isalso the arxiv at los alamos, pubmed central, etc. most of these online archives deal with intellectual propertyissues on a journalbyjournal negotiation basis or have scientists submit original work directly to their archive.scientists and graduate students in my research field typically need to access articles and datasets across abroad range of disciplines, including various branches of engineering, computer science, the social sciences, andeven the legal literature. many of us would prefer the ability to cite across any and all scholarly domains and linkfrom any citation we find on the web to the fulltext copy of that article on the open web. one approach that isbeing used to index and access the computer science literature is to search and crawl the entire web. they do thisusing an algorithmic approach to find citations that are germane to the computer science literature, and then thesystem allows you to directly link to any fulltext article that is found. it works on a citationtocitation basis. frommy perspective this is far preferred for indexing and accessing literature across and among scholarly domains. ifyou go to the citeseer web site6 today you will find about five million distinct citations within the computerscience literature that have been drawn from about 400,000 fulltext online articles. the system also has some4 see chapter 5 of the proceedings, òthe economic logic of ôopen scienceõ and the balance between private property rights and thepublic domain in scientific data and information: a primer,ó by paul david.5 the following description of citeseer and the legal foundations of the approach are based on a presentation by the author at the dukeuniversity school of law conference on the public domain on november 10, 2001. for more information, see http://www.law.duke.edu/pd/realcast.htm.6 see http://citeseer.com for more information.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4183useful automated tools for sifting the wheat from the chaffñin other words, for getting at the most cited andrespected articles within a specific subdomain of interest.the legal problem with this approach is in obtaining permissions to copy the halfmillion articles. you needto automatically copy the journal articles to test the article against your profile conditions, extract and index thecitations, as well as then host copies of the fulltext pdf or postscript files. the developers have not botheredasking publishers for copyright permissions, and no publisher in the computer science community has yet tocomplain. the system developers appear to have taken the position that (1) they gain at least some legal protectiongranted to web crawlers by the digital millennium copyright act (dmca); (2) if publishers or authors do notprotect their web sites from web crawlers, that is their fault; and (3) if you object to the web crawler copying anyof your articles, the system managers will be happy to remove those articles but please protect your site in thefuture or the crawler is likely to pick them up again.7many of the fulltext articles that the crawlers have copied from web sites were of course placed there by theprofessors and scientists who wrote them. can one assume that these professors retained the copyrights in theirpublished works? or should one assume that scientists transferred all or a portion of their copyrights to thepublisher? if authors did transfer their rights to publishers, which is certainly very common in my research field,does that mean that citeseer is acting similar to a napster for the computer science literature? after all, it is afacility that contributes to the illegal sharing of copyrighted articles among scientists. however, unlike napster,the original authors or talent are not complaining or losing any money because scientists typically are not intopublishing articles for compensation. furthermore, unlike napster, many of the lead publishers in the computerscience community are member organizations whose members would rise up in revolt if their professional organization objected to the system.this is an extremely valuable resource and it is used by thousands and thousands of scientists every day.although you and i might use google, our faculty and graduate students run to check citeseer. in fact, they willalways check citeseer before resorting to the commercial online databases that the university subscribes to. leegiles at penn state is one of the people who set this crawler running. the project started as a side assignment to oneof his graduate students to find some articles on the web that they figured must exist but that they could not findthrough normal channels. the code found what they wanted, and then they started to use it for broader and broadersearches. so there was no initial grand scheme to create this capability. it has evolved over time as various peoplefound it useful, improved the code, and let it run.with 5 million citations and growing, you can now ask questions like who is the most cited person across allof the modern computer science literature? you can come to a conference and know the general citation ranking ofevery person in the room, who has the most cited article addressing a topic like the public domain, which journalis the most influential on the topic, and who has the most cited article in the most respected journal. you cancompare the citation records of those articles that are available online with those that are not and discover that youare 4.5 times more likely to be cited if your articles are openly available online. professors are now activelyshipping in the urls where their articles may be found so the crawler can pick up their missing fulltext articles.if you are an academic, your goal in life is to be cited, to be a recognized authority, to know that your work matters.citeseer has created a dynamic of professors making certain that all their articles are available and readilyaccessible on the web.so far, the private scientific publishers have not been complaining about this situation. my guess is they arenot likely to unless they want to be boycotted by the general computer science community. i also ponder whetherthe scholarly community reactions would be the same if a crawler was currently indexing and copying all openlyavailable law review articles or biology articles.let us assume that you are in one of these other scholarly domains and you want to solve the legal dilemmafor your own discipline. how would you do it? in my case, i would set up a web site and call it the publiccommons of geographic information science. we actually have a mock site up, but eventually i would want thissite hosted by the university consortium for geographic information science, the group of 70 universities andresearch institutions housing the leading giscience research programs in the united states, to give it credibility.7 terms of service at http://citeseer.nj.nec.com/terms.html.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.184the role of s&t data and information in the public domainthe commons or open library has four components: (1) open access to giscience literature, (2) open access togiscience course materials, (3) the public commons of geographic data, and (4) opensource gis software. ourbasic rule in designing this online material is to keep it very simple for the scientist. in a single paragraph we tellthem what is wrong with the current publishing paradigm. in the second, we present a solution. third, we walkthem through four steps that solve their journal copyright and access problems.step 1 focuses on submitting articles to giscience journals. in submitting your work to a scientific journal forpeer review, we recommend that you place the following notice on your work prior to submission: òthis work,entitled [xxx], is distributed under the terms of the public library of science open access license . . . . ó thatlicense essentially says anyone can copy the article for free as long as attribution is given. then we provide anoptional statement: òalthough this license is in effect immediately and irrevocably, the authors agree to not makethe article available to any publicly accessible archive until it is first published, is withdrawn from publication, oris rejected for publication.ó note that we give scientists one recommendation and no options. we do not give thema suite of openaccess licenses to choose from. most research scientists and engineers could care less aboutanalyzing the law and social policy. they just want your best shot at supplying a legal solution for the discipline.the creative commons project is taking a similar approach by offering only a very limited set of license provisions for users to chose from.step 2 concerns which giscience journals will publish articles subject to prior rights to the public existing inthe article. ultimately, i think most journals in my field will have to come around. however, the solution is not tobeg journals in your discipline to accept an openaccess license. one potential solution is to list the primaryjournals in the field and then have scientists report back whether use of the license was accepted. those journalsthat allow openaccess licenses will have a substantial competitive advantage in attracting submissions, particularly when most scientists discover that they are four times as likely to be cited by other scientists if their articlesare openly available.step 3 focuses on how the researcher can ensure that others will find their published articles through a widelyaccessible citation indexing system. this step includes a description of citeseer. the problem in the gisciencediscipline is that our scientists publish across a broad range of literature, not just the computer science literature.therefore, we either need to set up a citeseer capability with algorithms and keywords developed for our specificdomain or hope that someone comes along to scale up citeseer to cover all science literature on the web.step 4 involves ensuring that a researcherõs article, once published, is maintained in a longterm publicelectronic archive in addition to sitting on a server at his university or on the server of his publisher. of coursethere is no longer a default right to archive in a world of electronic licensing. scientists can overcome the legalimpediments to archiving by following the openaccess licensing approach recommended in step 1. however,many longterm technical archiving issues still remain.my primary interest is in developing a public commons of geographic data. the challenges in that instance arefar greater than for openaccess sharing of journal articles, particularly if the vision is one of hundreds of thousandsof people creating spatial datasets, generating the metadata for those works, and then freely sharing the files.libraries in our local communities do not exist as a result of operation of the marketplace. public librariesexist in our communities because a majority of citizens agree that the tax money we spend on them results insubstantial benefits for our communities. similarly, do not expect digital libraries that provide substantial publicgoods benefits to be developed or maintained by the marketplace.further legal discussionthe computer science literature implementation of citeseer links to and maintains copies of over a halfmillion fulltext online journal articles that may be freely accessed. yet citeseer is in a different legal position tothat of most other online archives. no authors or publishers are asked permission regarding whether the citeseerweb crawler may copy and retain articles. how can the copying and provision of access to over a halfmillionarticles without gaining explicit permission of the copyright holders be legal?citeseer finds the articles that it indexes by crawling the web. to index found articles, the software copieseach pdf or postscript article it finds, converts it automatically to ascii, searches for keywords, and then extractsthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4185and processes appropriate indexing information. a link is automatically provided in the database created by thesoftware to the url where the article was found. typically this link is to the web site of the author. as a backup,in case the link to the authorõs web site is down, and as a means to provide more efficient access to the authorhosted article, the system caches a copy of the article and provides it in various formats that may be directly linkedby users.articles are seldom copied by the crawler from, for instance, commercial publishers of scientific articlesbecause those sites are typically protected by passwords or other technological protections. unless permission isgranted, citeseer indexes only articles found publicly available on the web without charge.8 furthermore, citeseerpurports to adhere to all known standards for limiting robots and caching. as the system has evolved, most articlesnow being indexed are those submitted by authors.9citeseer gains its legal basis for existence primarily through the dmca of 1998. title ii of the dmca addeda new section 512 to the copyright act to create four new limitations on liability for copyright infringement byonline service providers. the limitations are based on conduct by a service provider in the areas of (1) transitorycommunications, (2) system caching, (3) storage of information on systems or networks at direction of users, and(4) information location tools.the legal issues are varied and complex but, by example, one issue involves whether authors have authorityto post their scientific articles on the web. if citeseer picks up an article for which exclusive rights were given upby an author to a publisher, is citeseer liable or is the author liable for the violation? section (d) of the dmca oninformation location tools states explicitly that ò[a] service provider shall not be liable . . . for infringement ofcopyright by reason of the provider . . . linking users to an online location containing infringing material . . . byusing information location tools . . . .ó this exclusion from liability is followed by a list of three conditions that theoperation of citeseer appears to meet. similarly, in those instances in which an author specifically submits a urlto the system so that material can be picked up by the automated system, section (c) of the statute on informationresiding on systems or networks at direction of users states ò[a] service provider shall not be liable . . . forinfringement of copyright by reason of the storage at the direction of a user of material . . . .ó this exclusion fromliability is followed by a list of three conditions that the operation of citeseer again appears to meet.the most tenuous part of the legal position of citeseer relates to caching of the full text of articles. it is notclear that the exclusion from liability under section (b) on system caching applies to citeseer. nor does it appearto apply to most other nonprofit or academic service providers because the requirement of passwords or othercontrols are often not applied in these exchange environments. however, a similar exclusion for caching by thetypical nonprofit service provider can be argued under other sections of the copyright act. furthermore, if such achallenge were ever raised under the dmca, citeseer operations could convert immediately to a free subscriptionand registration operational environment to subvert such a challenge. another problematic legal issue is definingthe point at which allowable temporary storage of material (i.e., caching) crosses over to become longertermstorage or archiving. even though making backups of materials on the web sites of others is widespread andcommonplace across the web, òarchivingó that exceeds òcachingó arguably requires explicit permission fromcopyright holders.the world wide web, when initiated, was clearly illegal from a plain language reading of the law by mostattorneys because the web allowed and in fact required the copying of documents without explicit permission.however, the web was found by society to be so useful that ways were found to reinterpret and clarify the law toallow the innovation to spread. in a similar manner, the consensus appears to be that even if some lack of clarityexists in the law today with regard to the operation of systems such as citeseer and google, highly useful webwide indexing systems are likely to be looked upon with favor by interpreters of the law. as long as the provisosin section 512 of the dmca are met, it appears that the approach is on relatively sound legal footing.fortunately, most scientific publishers have revised their copyright policies in recent years to allow authors topost their authored articles on their own web sites. this largely negates the potential argument that citeseer8 see citeseerõs terms of service, paragraph 2, at http://citeseer.nj.nec.com/terms.html.9 correspondence from feedback@researchindex.org dated january 11, 2003. see also paragraph 4 of terms of service at http://citeseer.nj.nec.com/terms.html.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.186the role of s&t data and information in the public domainoperates much like the former napster in facilitating the illegal sharing of articles among scientists. furthermore,as required by the dmca, systems such as citeseer must remove articles in a responsible manner when requestedto do so by a copyright holder [section 512(b)(c)(d)].one of the most promising options for addressing the legal clarity issue in the long run is to encourage andfacilitate the ability of scientists to grant rights to the public to use their works prior to submission of those worksto publishers that do not already allow openaccess archiving of scientific works. by example, the creativecommons is working to facilitate the free availability of information, scientific data, art, and cultural materials bydeveloping innovative, machinereadable licenses that individuals and institutions can attach to their work.1010for additional information, see http://www.creativecommons.org and chapter 23 of these proceedings, ònew legal approaches in theprivate sector,ó by jonathan zittrain.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 418718726the role of the research university in strengtheningthe intellectual commons: the opencourseware anddspace initiatives at mitann wolpert.the challenging environment that has been described during the course of this symposium has the potential toprofoundly affect research universities. the economists among us understand that markets are not normallypassive in the face of changing economic forces, and research universities do not have the luxury of standingpassively by as intellectual property laws and norms change. research universities are missiondriven, notforprofit enterprises. they may host technology licensing offices, but their primary mission is education and nonprofit research.those outside the academy sometimes talk about the research university community as though it were somekind of monolithic industry. in truth, not only is there considerable variation among research universities, there isalso a distinct lack of uniformity within institutions. a personal story illustrates this point. shortly after i joined themassachusetts institute of technology (mit), i was worried about a piece of legislation that was coming beforethe u.s. congress. i approached the vice president of research at the institute to express my concern and urgedmit to take a position on the issue. his response was, òif you can figure out who mit is, then maybe you canpersuade them to take a position.ówe need to consider this heterogeneity, as well as traditions of intellectual freedom, when we talk about theresearch university. it is difficult to generalize, because healthy research universities have many diverse activitiesgoing on simultaneously under one roof, which is entirely consistent with the mission of such organizations. themission of mit, for example, is òto advance knowledge and educate students in science, technology and otherareas of scholarship that will best serve the nation and the world in the 21st century.ó the mission statement goeson to say that the institute òis committed to generating, disseminating and preserving knowledge and to workingwith others to bring this knowledge to bear on the worldõs great challenges.ó mitõs technology licensing office,like other university licensing offices, operated with the instituteõs mission, policies, and procedures.research universities play a significant role in the value chain of new knowledge creation in science andtechnology. at the risk of stating the obvious, research universities recruit and retain faculty and research scientists. they admit students and support them with financial aid. they cover the growing portion of the nonrecoveredexpenses associated with research. they invest in education and research technology, and they pay for the networkinfrastructure on our campuses, as well as the libraries. libraries, by the way, are investing an increasinglysignificant percentage of their material and resource budgets in support of databases and database resources.we have heard a fair amount at this symposium about the role of information technology (it) and the growthin complexity of the scientific and technical data environment. the fact is that it has affected other aspects of thethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.188the role of s&t data and information in the public domainuniversity mission as well. students expect to have ready access to the full panoply of digital content, dataincluded. faculty need an increasingly sophisticated work environment and a fair amount of it support. facultyalso need bandwidth, systems support, and library resources delivered to the desktop; all of which require institutional investmentñas do new labs, and redesigned, updated existing labs, and new buildings in which to put bigscience.research universities also need academic disciplines to consider how new research methods should becalibrated in the context of promotion and tenure decisions. there was an interesting conversation at mit not toolong ago about how faculty members can subject their work to the scrutiny of peer review when the scholar isworking in an environment that is entirely electronic and on the web.finally, it in the legal, regulatory, and compliance context has caused a distinct rise in the cost of doingbusiness for research universities. for example, libraries now license databases. the mit libraries are probablynot unusual in dedicating two fulltime people to license agreement negotiation. likewise, mitõs informationsystems department has a staff of lawyers who negotiate software licenses for the university. and there is now astaff member whose job it is to respond to calls relating to the provisions of the digital millennium copyright act(dmca). bear in mind that people can call up and demand that you take content down, and the bias is in theirfavor. research activities also incur new expenses. vice presidents of research work hard at protecting thescientists and students in their institutions.these new costs of doing business are all total overhead. they are defensive, and they are deadweight coststo the university. there has been some question earlier in this symposium about whether the dmca costs money.the answer is yes, the new legal environment costs a great deal of money in deadweight overhead.as research universities have confronted the reality of rising deadweight costs and diminishing flexibility, inan increasing draconian intellectual property (ip) environment, they have become increasingly aware of theimportance of advocating for and supporting openness. the best students will be diverse in their nationalities andreligions. these students need access, as we have heard from harlan onsrud,1 to highquality information, andthey should not have to choose between buying lunch, buying a dataset, or buying an article. faculty should be ableto teach the best way they know how, without the requirement to plow through endless permissions and approvalprocesses to obtain the ability to use in their courses information that is now (by default) protected by ip regimes.you have already heard a great deal about the importance of openness in research and about the need for workto have visibility and impact. i want to second harlan onsrudõs comments on subscriptions.2 because as a practicalmatter, if the size of a subscription base is reduced to 50 institutions, and the license terms of digital access to thatdatabase prohibit interlibrary loan, then one really has to ask the question as to whether publishing in thatparticular journal does indeed provide appropriate visibility and impact.one might reasonably conclude from these remarks that research universities, and the home that they providefor many scientists and engineers, are in deep trouble. imagine for a moment that universities are not focused onteaching students, are not conducting notforprofit research, but rather are engaged in some other enterprise.imagine a business trying to operate under the various constraints and uncertainties that apply to research universities. imagine that mit was not mit, but rather a major metropolitan newspaper publisher. the business wouldbe in a situation where its staff authors were writing material on the premises and then sending that material to anoutside third party. this newspaper would then have to buy back the work of their staff authors at an arbitrary priceso that it could be used in the publication of the newspaper. imagine, worse yet, that the third parties were nearmonopolies. imagine, moreover, that the newswire services it dealt with chose to license content to it underarbitrary and unilateral terms, so that it had no control over the data stream. then imagine that compliance with allthe ip requirements that surround the content that goes into the newspaper had become unrecoverable andunsustainable. this is the situation that universities find themselves in today.we believe, at least at mit, that new educational and information data management strategies are required.there are two initiatives under way at mit, which reflect a market response based on a commitment to openness.1see chapter 25 of these proceedings, òemerging models for maintaining scientific data in the public domain,ó by harlan onsrud.2ibid.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4189both initiatives emerged from faculty desires and needs as they were articulated and are intended to give facultya new set of tools that will enable them to create new approaches to and methods for managing their intellectualwork. both initiatives illuminate how profoundly the postdmca environment has already distorted work in theacademy, and both point to the importance of initiatives such as the creative commons.the first of these initiatives is the opencourseware initiative.3 opencourseware intends, eventually, to putall of mitõs courses on the web, free of charge. it illuminates the intellectual framework for how mit approachesthe challenge of teaching mit students. in a sense, we are publishing mit courses on the web so that theeducational strategy that mit uses can be shared openly across the world. this is an effort to create a public good,to put into the public domain what mit knows about teaching the kinds of students who come to mit.in developing this initiative, we encounter the postdmca problem. for example, who knows what agreements or license terms apply to the material that is embedded in the faculty memberõs course notes? courses arelittered with ip that may have no bonafide reference, where there is no way of tracking the ownership, and forwhich there is no way of understanding whether the person who contributed that information to a colleagueõsteaching activities also intended that it should be put up on the internet free of charge for the world to see. theseissues are very complicated, given that everything is in all probability owned by somebody.we are working through a variety of astonishingly complex issues as a result of our attempt to make a publicgood out of a traditional way of teaching. clearly there are some things that are more problematic than others, justby virtue of the way the law works. recommended and required reading is going to be difficult to post on the webwithout permission from publishers. in our first efforts to obtain permission from publishers, 80 percent ofpublishers denied permission to post materials free of charge on the internet in this context, even though what wasbeing posted was a minimal part of any one publication, and even though you could imagine it operating asadvertising for that publication.there are also complexities around software. if a faculty member uses a piece of software in his or her course,and that software comes with a particular licensing agreement to mit, what were the terms of that licensingagreement? was it negotiated by the department, or by the faculty member, or by the institute? is the facultymember using a site license or an individual license? what are the terms and conditions of use of the software thatfaculty use to manipulate and create content that they would ordinarily consider as essential to the course?data are an equally big issue in the opencourseware context. faculty would like to be able to provide actualaccess to raw data, particularly in the case of social sciences and hard sciences, so that a student visiting the sitecould understand the pedagogical intention of the faculty member. given what we have heard during thissymposium in terms of reachthrough claims in the patent environment, there is a similar concern about thecapacity of original publishers to reach through the teaching environment and constrain what can be put on an openweb site.in the legal environment in which we currently operate, opencourseware is a publication mechanism of mit.as such it becomes a highly visible target for those who would object to the use of anyoneõs ip in an environmentthat would otherwise pass the four tests of fair use in the instituteõs internal teaching activities. opencoursewareis not intended for profit, it does not use a significant percentage of an individual work. it is intended to be factualrather than creative. the market impact of any item is minimal at best. yet commercial publishers are concernedabout putting content up in the opencourseware environment. as a consequence, we are inspired at mit to thinkabout enabling new tools so that faculty can behave differently. perhaps the rules of the game can change as well.the second initiative at mit is an initiative called dspace.4 this is not a publishing enterprise. it is aninstitutional response to faculty having called the libraries and asked òcan i put my stuff with you? i have all thisdigital content, and no place to keep it. will you take it for me?ó dspace is a way for faculty to put the goodmaterial that they have prepared and are ready to share with the world in a secure stable, preservable, dependablerepository with distribution capabilities. dspace was built in partnership with hewlett packard and with additionalsupport from the mellon foundation, and it is being written in open source.3for additional information see the mit opencourseware web site at http://ocw.mit.edu/index.html.4see the dspace web site at http://www.dspace.org for additional information.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.190the role of s&t data and information in the public domainas professor bretherton was describing a structure of trees, roots, and branches in his talk,5 it seemed to methat he was characterizing a functionality such as dspace. institutional repositories of this kind present anopportunity for robust roots in that tree structure that would enable faculty to build repositories of work that theywould like to share through just such a model of distribution and management.there are a number of interesting issues that arise from the design of dspace. for example, we areinterested in the prospect of using creative commons licenses as a way of helping those who deposit materialin dspace signal the way they would like to have their material used. there are no conventions such ascreative commons licenses available for submitters right now. so if a faculty member wants to deposit his orher work in a digital repository that will serve it to the world, and maintain it over time, there is no existingset of licenses that can be built into the metadata that will tag to identify how the work can be used goingforward.we believe that a federation of interested institutions will be needed to establish and maintain sustainabilityfor a digital repository. so our great hope, and our reason for writing the code in open source, is that there will besufficient interest, first across the united states and perhaps internationally, in the idea of building digital repositories at the institutional level. despite what one hears about how easy it is to create digital content, preserving,maintaining, and keeping digital content persistently available is a research challenge. our hope in federation isthat we will be able to share that challenge across multiple institutions.there is also no clearly established model for a relationship of this kind. libraries themselves havequite a fine and interesting experiment that is now well over 30 years old called the online computerlibrary center in which libraries have banded together to share cataloging data in a notforprofit librarymanaged enterprise.6 that enterprise has been an interesting model for us as we think about how one wouldfederate digital repositories across institutions. so we think that the library community can figure out howto do this.a final challenge to dspace is that disciplines vary greatly in terms of what their expectations are for arepository. some of the early adopters of the dspace repository are faculty in ocean engineering, and they deal indatasets that are terabytes in size. some faculty have large collections of images. other faculty have much smaller,more textoriented expectations, which illustrates the fact that scientists like science, not database administration.they have always expected that libraries would be there for them, and so we are. on the other hand, the challengeto us is to help scientists take advantage of new tools.at the end of the day, research universities, scientists, and funding agencies need a new alliance. we needstrategies to advance and expand researchbased education. we need to be able to educate and conduct researchwithout draconian external rules. this probably means developing our own systems for the exchange of data andinformation on a direct institutiontoinstitution basis. we need to assure persistent availability and accessibility ofresearch data. this probably means keeping it as close to scientists as possible, and it means new ip options likethe creative commons need to be deployed.we need to solve the challenge of the borndigital world. researchers and educators now routinely producework that has no paper analog. we know that a great deal of work already has been lost, and we are deeplyconcerned that there are no easy ways to approach the longterm archiving of work that is digital only. as such, weare faced with the prospect of sentencing work to a fiveyear shelf lifeñor only for as long as the proprietarysoftware is interested in addressing the problem.last, we need to solve the archiving problems. bruce perens and i were talking about some work that he hasdone in restoring works that disney owns that were damaged. the cost and effort were phenomenal. clearly, thelosses are mounting similarly in the higher education and scientific communities. yet we do not have disneyõsmoney. we need a longterm solution to archiving.through opencourseware and dspace, mit is working hard to develop some prototypes, to share the ideasand the software behind those prototypes, and to interest others in joining us in meeting the challenge.5see chapter 8 of these proceedings, òthe role, value, and limits of s&t data and information in the public domain for research: earthand environmental sciences,ó by francis bretherton.6see the online computer library center web site at http://www.oclc.org for additional information.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 419119127new paradigms in industry:corporate donations of geophysical datashirley dutton.i would like to discuss some recent donations from private industry of geophysical data that had beencompletely proprietary and now are in the public domain. three energy companies recently donated large collections of rock samples to the university of texas at austin. i am a geologist at the bureau of economic geology,which is a research division of the university of texas. as a geologist, i find these donations of unique rockcollections to be very exciting.these data had been held completely proprietary from the time they were collected, but they are now in thepublic domain. i will stress in my presentation that these donations were made possible by the accompanyingfinancial gifts that have been given along with the data, and also by government grants that have made it possiblefor the university of texas to accept these data.before i tell you more about these donations, i would like to tell you a bit about what these data are and whywe consider them so important. geologists need rocks. they are the key for conducting academic research ingeology, and they are also important in developing natural resources. many of these rocks are very expensive toacquire and would be very difficult to replace. obviously, this is not true about all rocks. a geologist can go outwith a rock hammer and collect samples from an outcrop; these samples are not expensive to acquire and would beeasy to replace. but rocks collected from the earthõs surface only sample the thin skin of the earth. geologists needthreedimensional data. the rock samples in the industry donations were taken in wells that were drilled two orthree kilometers, or even deeper, into the earth. these are very expensive to obtain and would be very difficult toduplicate.there is an important constraint that makes geologic data perhaps a little different than other data we havebeen discussing in this symposium. we are not talking about digital data; these are actual physical collectionsñrocks that are heavy and take up a lot of room. it takes strong people to get them out of storage and to lay them outfor the researchers. these are real constraints to the ability of the university of texas to accept a data collection.it is not simply a matter of acquiring computer data disks.when oil companies drill wells, they can be very expensive; for example, a shallow well onshore in the unitedstates can cost several hundred thousand dollars. deeper wells and wells drilled offshore can cost more than amillion dollars to drill. many companies will use special drill bits to cut rock samples (cores) as a well is beingdrilled. in the united states, these cores are owned by the company that acquires them. they are able to keep thecores completely proprietary and use them for their own purposes. for the last 50 years, most large oil companiesthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.192the role of s&t data and information in the public domainin the united states had their own private core collections that they would use internally within that company. inmost cases, these cores were not made available to anyone outside the company.currently, in the oil industry, many of the research labs and private core facilities are closing. the companiesare finding that they have several choices of what to do with their core collections. they can keep them, but thereis an expense involved in doing this. it is expensive to maintain a facility or rent warehouse space to store rock.another choice is to discard the cores. this may be the least expensive choice, but it is not without cost. corecollections represent a large volume of material, and discarding the rocks would involve some expense. or theycan choose to donate the data, and several companies have recently done this. as a matter of fact, i am not awareof an instance of any large oil company that has closed a repository and thrown away the rocks. oil companyexecutives realize that this is valuable material that would be a shame to throw away. so they are exploring theoptions and developing business models for donating it instead.the decision to donate material to a public facility must make business sense to the company considering thedonation. although cost avoidance, such as staff salaries for maintaining the collection, rental of warehouse space,and other operational costs, is a big factor in the decision to transfer data from the private to the public domain, itis rarely the only reason. these decisions are usually based on several issues that include cost savings, as well asother factors such as continuing access to the data without the overhead, preservation of the data in case there is areturn to it for unforeseen reasons (oil and gas data might someday be used to explore for water, for example),goodwill with the community (data may be used for education and outreach), future disposal costs, and, whenendowments to fund the facility are included, tax incentives.the first of these donations was made in 1994 by shell oil company to the university of texas at austin.they developed a model that allowed the university to accept this donation. shell had a huge amount of rockmaterial to donate, and with no additional resources, the university would not have been able to accept it. however,in addition to giving the rock material, shell also donated the midland, texas warehouse where they had stored thecores, so the university would have a place to keep the rocks. the other important factor was shellõs donation of$1.3 million toward an endowment; the money in the endowment could be used by the university of texas tooperate the facility and make the data available to anyone who wanted to use them. thus, we have an example ofa physical collection that had always been proprietary, which has now entered the public domain. it can be used byacademic researchers, and it can even be used by geologists from other oil companies now.the amount of money that was provided by shell was very generous, but it was not enough to completelyendow the facility. the university of texas received a bridging grant from the department of energy (doe) tooperate the facility for five years. this was very important, and enabled the donation to proceed. with thisgovernment grant to pay operating expenses, the university could reserve the money that shell had donated.additional money was raised to increase the endowment, and now the amount of money in the endowment coversthe operating expenses of this facility.a few years after shell made this donation, another energy company, altura, also wanted to donate theircores. the altura cores were also stored in midland. it did not make sense to have two different warehousefacilities in different parts of midland, so altura donated money to build another warehouse adjacent to the formershell warehouse. this way the cores could physically be in the same place, and the same staff would be able tocurate both collections. this was another important model, a little different than shellõs donation, which allowedthe altura donation to happen.in august 2002, british petroleum (bp) made a large donation of cores and cuttings from oil and gasexploration wells. they also donated the warehouse in houston where they had stored this material, along with aresearch building and 12 acres of land. again, following the shell model, they made a $1.5 million donationtoward an endowment. this monetary donation makes a very important contribution toward an endowment to runthe facility. the university will be attempting over the next several years to build up that endowment so it isenough to operate the facility. doe has provided another critically important grant for the initial operatingexpenses of this facility.one exciting aspect of the bp donation is the office and lab facility that they gave along with the warehouse.the building includes modern core examination rooms with roller tables and excellent lighting for viewing cores.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4193there are also lab rooms that can be used for various analyses. finally, there are other facilities, includingconference and lecture rooms.in addition to giving the rock material, all three of these companies have provided data to go along with it.these rocks would be worthless if we did not have any information about them, such as where they came from. soa key part of each donation is the information that comes with it. the companies have provided the name of thewell, the geographic location where the well was drilled, and the depth of the cored interval. that information iskey for researchers to figure out what the samples are and whether they would be useful for a particular researchproject. in most cases the companies have also provided a unique well number, which allows a researcher to getinformation from some of the commercial databases. for example, if a researcher knew that there was a coreavailable from a particular well, she could then purchase a geophysical log from that well from a commercialdatabase that owns the rights to the geophysical data. the information that has been supplied in addition to the rockis key to its use in the academic community. all these data are now available on the internet, so anyone anywherecan search the holdings and decide if there is material that they would like to use.why, you may wonder, would a company donate proprietary geologic data to a university? the answer is thatthe company had completed its use of the core for exploratory purposes and therefore felt the best use of thematerial would be to donate it to an academic research facility. as lord john browne of madingley, bp chairmanand ceo, stated in announcing the bp donation, òthese are valuable books, but weõve read them.ó at the timethese cores were taken, they were acquired for a specific purpose. the company may have been operating a fieldand needed rock information about that field to produce it more efficiently. or the company may have beenexploring a particular geographic part of the country and needed background information about what the reservoirsin general are like in that area, and these cores provided key information. most of these companies no longer areexploring in the united states, and in many cases they have sold their fields to smaller companies. from their pointof view, they have obtained the information they needed. however, they realize that the cores retain tremendousresearch value, and so they are willing to make these data publicly available.as a result, these proprietary geologic data are entering the public domain, and they form important materialfor current areas of research, such as stratigraphy, sedimentology, and diagenesis, as well as areas of research thatwe have not even considered yet. this material is now preserved, so someday in the future, when somebodyrealizes how to use it in paleoclimatology research, for example, or some other field that we currently do not knowhow we can use the information, it has been preserved and future scientists can use it. if there is a new analyticaltechnique that is developed in the future, the material will still be there, and new uses can be made of these rocksamples. of course, it is also a very important source of material for student theses and dissertations, and it is stillof value in oil and gas exploration and production. even though there are different companies operating in theunited states now, usually smaller companies, these data are still of importance to companies producing frommature oil fields.in conclusion, these companies have developed a business model for donating proprietary rock data to theacademic community. these donations allow valuable, irreplaceable physical collections to enter the publicdomain. these donations were made possible by the accompanying donations of the physical space in which tostore the rocks, and financial resources toward the cost of curating the cores and making them available to others.finally, government grants have made a very important contribution to enabling the university of texas to acceptthese donations by providing critical funds for the initial operating expenses while an endowment builds.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.194the role of s&t data and information in the public domain19428new paradigms in industry:the single nucleotide polymorphism consortiummichael morgan.i was asked to comment on my understanding of the european union legislation protecting databases andwhat effect it has had. from a personal perspective in the biomedical sciences and as an academic user, my answeris none. i think it is fair to say that until it has some effect, we are not going to take much notice of it. europeanlegislation is often a mystery, and i am not sure how much the united kingdom had to do with that draftlegislation. i believe the database directive has now been incorporated into u.k. law.i want to discuss both the single nucleotide polymorphism (snp) consortium and the human genome project.i am afraid most of my presentation will be thin on law and possibly too high on rhetoric. having been engaged in apersonal and direct way with these issues as a trained scientist, i find it quite difficult to be always as objective as iought to be. to paraphrase winston churchill, i have always thought that lawyers should be on tap and not on top.the human genome project is a consortium involving laboratories and funding agencies around the world.the major funding organizations in the united states are the national institutes of health and the department ofenergy, and in the united kingdom a private organization, the wellcome trust. the u.k. government did not putvery much resource into this initiative. one of the things that we first discussed as we began to think how toorganize the human genome project was what to do about the dna, who does this dna belong to, and was theresomething special about the fact that we were dealing with human dna rather than mouse or rat dna.as you have already heard, we developed a series of principles that have been called the bermuda rules orbermuda principles. in essence, in return for the enormous largesse given to very few, selected sequencing groups,the sequence data would be deposited into the public databases every 24 hours.1 the raw information would beprovided for people to use as best they could. that was a grassroots movement. that was not imposed by thefunding agencies. it is remarkable how in the scientific business we are so dependent on these agencies beingchampions and leaders. however, this policy was very much due to two scientists, john sulston in the unitedkingdom and bob waterston at the university of st. louis. they had to persuade the scientific community, theleaders of all the major research groups capable of taking on this task, and there was eventually agreement on theseprinciples, which were ratified at subsequent meetings held in bermuda.the human genome project was progressing rather well, until the announcement in may 1998 of the establishment of celera. in principle, of course, there is no reason why a private entity should not go about sequencingthe human or any other genome and using that information as it chose to do. the real issue that set the scientific1for additional information on the bermuda principles, see www.wellcome.ac.uk.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4195world alight was the fact that efforts were made to close down the publicdomain activity. there was a meeting atcold spring harbor, during which dr. craig venter told the national institutes of health to give up their humansequencing program and focus on the mouse.by pure chance, the wellcome trust was considering a proposal to double the financing available to thesanger center, now the sanger institute, to sequence the human genome. the governors approved the award, andjohn sulston and i flew to cold spring harbor to announce the news to the assembled scientists and to talk to thefunding agencies. as we all know, the human genome project survived and there was one project in the publicdomain and one in the private domain. but you need to ask yourselves, what would have happened if the wellcometrust had not by that time become reasonably wealthy and if there had not been a publicdomain human genomeproject. would those data now be available to scientists around the world?there were negotiations to try to bring the private and public programs together. there were exchangesof letters. finally, because time was pressing and we wanted to move forward, the wellcome trust releaseda letter that in essence stopped the negotiations. celera did not react very well to this, and a cartoon waspublished in nature purporting to show that the gene of human aggression had at last been isolated. thefollowing week, there was a joint statement issued by the white house and 10 downing street, whichincluded a declaration supporting the release of data. unfortunately, it was misinterpreted, particularly by thefinancial press. it had an effect that had not been foreseen, which was the suggestion that it was inappropriatein any way to use the patent system to capture intellectual property (ip) coming out of genome research,which was not the intent. there was also a simultaneous announcement by both camps of completion of aworking draft. i want to make it clear that the wellcome trust is not opposed to the appropriate patenting ofip, i just wanted to mention that, whereas we regard the human sequence as something that itself should notbe patented, the proteins derived from that and therapies to interfere with that are entirely appropriatelyprotected by the various ip rules. each of us contains three billion nucleotides in our dna. each of us is extremely different, yet our dna is99.9 percent the same. but that means there are three million differences, and those differences will be distributedthroughout the three billion nucleotides. if you want to do a bit of mathematics as to what the various combinationscan be, you can see that it is truly astronomical and explains why we are all so different. as a species we are moreclosely related through our dna than any other species on the planet, and we understand, or at least we have somereasonable understanding, of why that is so.these are some of the reasons why snps were seen to be important by the pharmaceutical industry. they insome way will reflect our different susceptibility to disease, our different susceptibility to the action of drugs. ifyou can understand that information with respect to whom is susceptible, for example, to diabetes or hypertension,you will be able to segment the population, advise on different changes in lifestyle, and develop appropriate drugtherapies. there are many other uses. as such, industry recognized the need to understand the variation in thehuman genome, the snps. they could see that if these could be mapped out and we could understand where theywere and correlate them with disease, that would be very powerful. glaxosmithkline estimated that it would costapproximately $250 million to get a reasonable first map of snps. even for a large company like glaxo, $250million is a large portion of the research budget. multiply that by the fact that each pharmaceutical company wouldneed its own snp map and you can see why, from a purely economic reason, industry was interested in somehowmitigating those expenses. so they all got together to explore whether a combined effort would enable them to geta snp map cheaper, or share the cost. it was only later in the process that the possibility of public funds via thewellcome trust became a possibility.one of the reasons that putting the data into the public domain was appealing was, if you get a cabal ofcompanies together to produce information that they will share only among themselves, you can run afoul,particularly in the united states, of antitrust legislation. so we developed a model for a notforprofit company, a501(c)(3) in the united states, where the partners would join, agree to the workings of the organization, work outa work plan, and fund it.the companies involved were not only the major pharmaceutical companies in the united states and europe,but also ibm and motorola. motorola wanted to put snps on chips, exactly what is now happening. the missionof the consortium was to gather these data to serve the medical community, the life sciences community, and thethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.196the role of s&t data and information in the public domainmembership. industry needs proper quality control. if this resource is to in any way be used in the drug industry,it will no doubt come under the rules of the food and drug administration in the united states and the equivalentagencies around the world, so this needed to be an industry standard. as such, quality issues that do not normallyworry academic groups had to be built into the business plan.very early on, it was agreed that we needed an ip legal task force to work out how these data should be madeavailable and in what form. it became clear that there was a risk that simply releasing the information might enableother entities to download the data, subject them to some form of ip protection, and then sell the data back to themember companies. the senior executives did not want to go back and explain to their boards that they spent a lotof money and then now had to buy the information. so a series of data policies were initiated. by january, sixmonths after the formation of the snp consortium, the information was released via a webbased site.by the end of the project, we had mapped 1.25 million of these snps, and all were released into the publicdomain. i want you to remember that number, 1.25 million. glaxoõs original program was going to cost $250million and was going to end up with 150,000 mapped snps.a number of scientific groups around the world were commissioned under contractual conditions to determinehow the data were to be put together and to determine the quality assurances, and a data center that would captureall of the information was set up. none of the data would be released to the public or to the companies until theyhad been validated and mapped, and then they would be released to the companies and the public at exactly thesame time. nobody saw the data ahead of anybody else. the release policy made the data available at approximately quarterly intervals, which enables the snps to be mapped. this enables the patent to be applied for inbundles of snps to establish prior art, which is the only purpose of itña protective patenting policy.the ip policy was to maximize the number of snps into the public domain at the earliest possible date andensure that they remained free of thirdparty encumbrances, so that the map could be used by all without financialor other ip obligationsñno charge to access the data, no licensing fees, nothing. this plan was simply to ensurethat there was a priority date for the snps, which would then prevent anybody else from capturing them. that hasworked, but the legal task force, using lawyers from each of the companies and the wellcome trust and using veryexpensive external lawyers, took a long time to come up with a deal that we all thought would work. again, it wasextremely important that we got an agreement that nobody would have prior access to the information, and thatremains the case.as i mentioned earlier, this program from scientific and economic perspectives was spectacularly successful,in that with a budget of $44 million, not $250 million, we mapped close to 1.5 million snps rather than the targetof 150,000. as such, industry was very pleased. and, of course, we thought maybe this model could be appliedelsewhere.we are in the process of setting up a structural genomics consortium that will provide a highthroughputresource for determining the structures of human proteins. the data release principles have been agreed on,although the details have yet to be fully worked out as part of the contract between the wellcome trust and thecompanies. however, our ip experience informs us that the raw structural data should not be patented and shouldbe made freely available to researchers everywhere. again, all these coordinates, once they are validated, will bereleased into the public domain on the web. there will be no restrictions, and they will be provided to the publicand the consortium members at the same time.at the moment, we are working with the u.k. department of health and the u.k. medical research councilon a national population collection. we will try to recruit 500,000 volunteers, aged between 40 and 65, to ask themto donate blood, and then a dna database will be established. access rules for the database are still underdevelopment, but it will be freely accessible. there are many things that still need to be done, including furtherpublic consultation, because there are significant privacy issues that need to be resolved. but the principles ofaccess have been agreed. the aim is to make the information available to anybody who wants to use it for healthcare and public health benefits.one unique thing about this database is that people who use the data will generate new data. as a quid pro quo,those new data will have to be redeposited in the database, so that the database will continue to grow. and, ofcourse, the very important thing for this database is to ensure that the way in which the samples and the data areused is consonant with the consent that has been obtained from each of the volunteers.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4197the wellcome trust is committed to advancing health care through support of biomedical research. we fundthe research on the basis of scientific merit, not for direct commercial benefit, although we need funds to fundresearch. by that same token, we are supportive of the protection of ip in an appropriate way and an appropriatetime in the value chain, and with appropriate licensing terms. we are under an obligation under the charitycommission in the united kingdom to ensure that useful results of research are applied to public good. youcannot have exploitation of research results without protection of the ip. as such, it is a challenge.the release of this basic information is, in my opinion, definitely in the public interest, especially for genomesequences and protein structure. access to this information furthers research progress rather than hindering it.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.198the role of s&t data and information in the public domain19829closing remarksr. stephen berry.i will try to present a brief summary of the symposium from a very personal viewpoint. the first session of thesymposium focused on the challenge that we are confronting in terms of scientific and technical data and information (sti) in the public domain, then session 2 looked at how these are evolving and what are the new pressures.session 3 examined potential consequences of changes and particularly the diminishing of the public domain, andthe symposium ended with a final session on potential responses from the education and research communities inpreserving the public domain and promoting open access. this structure gave a sense first of realism, then offoreboding, and finally, the last set of talks provided a sense of optimism and turned the perspective in a verypositive direction. there are some points that kept being emphasized that we might keep in mind. first, openness,we now can argue, is a demonstrable good. the meeting began with the sense that there were many reasons toquestion the value of openness relative to the value of protection. the case for economic health being fostered byopenness is a case that we can make, and one that needs to be made much more in the circles where decisionsrelative to the public domain or especially laws are being considered.we also heard about the importance of experimentation, the importance of trying different ways of doingthings, and particularly of the likely optimum course being one that has many different pathways. we heard overand over the dangers that a digital millennium copyright act type of approach produces by essentially forcingalmost necessarily a single track. in addition, we heard from a number of speakers about the counterproductivecharacter of inhibitory legislation versus the very desirable character of permissive legislation. this is a case thatthis meeting has argued needs to be made much more forcefully in the legislative community.i think the point of despair during the meeting was when we heard about worst cases that restrict or couldpotentially inhibit open access to and use of public domain sti and the potential consequences. these casesillustrate the dangerous course with regard to the continuance of the scientific enterprise as we know it now.the best cases, even the pretty good cases, by contrast, look quite positive. the final set of presentations fromthe education and research communities leaves us with a sense that the chances are pretty good that the world isgoing to turn out all right, provided that the points of view expressed and the evidence presented here is relayed tothe people who are making these decisions. the rational decisions seem to be very clearly defined in favor ofopenness and public domain.we need to continue to reexamine the institutional relationships that involve scientists, their universities or thegovernment or their laboratories, and the publishers of books and journals. it was clear from some of the talks that,in many ways, universities and scientists can afford to get tough, for example, to decide not to publish in thethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.session 4199journals or with the book publishers that are forcing universities like the university of maine to drop subscriptions.if a significant fraction of the scientific world is not going to cite my papers if they are published in journal x, whyshould i publish in that journal? the scientific community still has to deal with not only the commercial publishers,but also with some of its professional organizations that still retain the same kind of protective positions that wedecry in commercial publishers. however, scientists seem to be less willing to criticize when professional organizations maintain those publications and adhere to restrictive practices.the next stage of this discussion, one that was not discussed here, is one that is the responsibility for all of us.we have to ask ourselves, who can we reach that can make a difference, and how can we put together thepersuasive case for the legislators and the lobbyists that we have heard here? we can talk among ourselves, but itis in the rooms where the database legislation is being discussed, where there seems to be a little rising against theeuropean database directive, where in the committee rooms the digital millennium copyright act might bereexamined. at this stage, we have collected a good deal of evidence that we need to put in a form to allow us totalk to people who can really change things.the net result of this symposium is that this is a step forward. we must thank the organizing committee andpaul uhlir and jerry reichman for putting together a very exciting and productive symposium.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.201appendixesthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix a203203appendix afinal symposium agenda.thursday, september 57:45 a.m.registration and continental breakfast8:30welcoming remarks, william a. wulf, president, national academy of engineering8:40symposium overview, r. stephen berry, university of chicago8:50session 1: the role, value, and limits of scientific and technical data and information in thepublic domaindiscussion framework, paul uhlir, national research council9:10ñin society, james boyle, duke university school of law9:30ñfor innovation and the economy¥intellectual propertyñwhen is it the best incentive mechanism for scientific and technical data and information? suzanne scotchmer, university of california, berkeley¥òopen scienceó economics and the logic of the public domain in research: a primer, paul david, stanford university10:15break10:35ñfor innovation and the economy (continued)¥scientific knowledge as a global public good: contributions to innovation and the economy, dana dalrymple, u.s. agency for international development¥opportunities for commercial exploitation of networked science and technology public domain information resources, rudolph potenzone, lion bioscience11:20discussion of issues from presentations12:00lunchthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.204appendix a1:00ñfor education and research¥education, bertram bruce, university of illinois, urbanachampaign¥earth and environmental sciences, francis bretherton, university of wisconsin¥biomedical research, sherry brandtrauf, columbia university2:00discussion of issues from presentations2:30session 2: pressures on the public domaindiscussion framework, jerome reichman, duke university school of law2:50the urge to commercialize: interactions between public and private research and development,robert cookdeegan, duke university3:10break3:30legal pressures¥in intellectual property law, justin hughes, cardozo school of law¥in licensing, susan poulter, university of utah school of law¥in national security restrictions, david heyman, center for strategic and international studies4:45the challenge of digital rights management technologies, julie cohen, georgetown universityschool of law5:10discussion of issues from session 25:55adjourn6:00ð7:30receptionfriday, september 68:00 a.m.continental breakfast8:30session 3: potential effects of a diminishing public domaindiscussion framework, paul uhlir, national research council8:50ñon fundamental research and education, r. stephen berry, university of chicago9:20ñin two specific areas of research¥environmental information, peter weiss, national weather service¥biomedical research data, stephen hilgartner, cornell university10:00discussion of issues from session 310:30break11:00session 4: responses by the research and education communities in preserving the public domainand promoting open accessdiscussion framework, jerome reichman, duke university school of law11:20strengthening publicdomain mechanisms in the federal government: a perspective frombiological and environmental research, daniel drell, u.s. department of energy11:45academics as a natural haven for open science and publicdomain resources: how far can westray? tracy lewis, university of florida12:10lunchthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix a2051:00new legal approaches in the private sector, jonathan zittrain, harvard university school of law1:20designing publicðprivate transactions that foster innovation, stephen maurer, esq.1:50new paradigmsñin academia¥the role of the research university in strengthening the intellectual commons: the opencourseware and dspace initiatives at mit, ann wolpert, mit¥emerging models for maintaining scientific data in the public domain, harlan onsrud, university of maine2:20ñin industry¥opensource software in commerce, bruce perens, hewlett packard¥corporate donations of geophysical data, shirley dutton, university of texas at austin¥the single nucleotide polymorphism consortium, michael morgan, wellcome trust3:20break3:30discussion of issues from session 44:00closing remarks, r. stephen berry, university of chicago4:15adjournthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.206appendix b206appendix bbiographical information on speakers andsteering committee members.speakersr. stephen berry (symposium chair) is the james franck distinguished service professor emeritus, departmentof chemistry, at the james franck institute at the university of chicago. he received his a.b., a.m., and ph.d.from harvard university after an 18month instructorship at harvard, in 1957 he became an instructor in thechemistry department of the university of michigan, and in 1960 he moved on to yale as an assistantprofessor. in 1964 he joined the university of chicagoõs chemistry department and james franck institute(then the institute for the study of metals) as an associate professor. he became a professor in 1967 and jamesfranck distinguished service professor in 1989. in 1983 he received a macarthur prize fellowship, and he hasspent extended periods at the university of copenhagen, oxford, universit” de parissud, and the freiuniversit−t berlin. dr. berryõs research interests include electronic structure of atoms and molecules; photocollisional detachment of negative ions; photochemistry of reactive organic molecules; vibronic coupling processes such as autoionization, predissociation, and internal vibrational relaxation; thermodynamics of finitetime processes; dynamics and structure of atomic and molecular clusters; phase changes in very small systems;chaos and ergodicity in fewbody systems; and, most recently, as an outgrowth of the cluster studies, dynamicson manydimensional potential surfaces and the origins of protein folding. he has also worked extensively withthe efficient use of environmental energy and other resources. dr. berry is also interested with issues of scienceand law and with management of scientific data, activities that have brought him into the arena of electronicmedia for scientific information and issues of intellectual property in that context. he is a member and homesecretary of the national academy of sciences (nas). he has been involved in many activities of the nationalacademies, including chairing the national resource councilõs (nrc) study on the bits of power: issues in theglobal access to scientific data.james boyle is william neal reynolds professor of law at the duke university school of law and author ofshamans, software and spleens: law and the construction of the information society (harvard university press,1996) as well as many articles and essays about intellectual property and social and legal theory. he is a memberof the board of creative commons and of the academic advisory board of the electronic privacy and informationcenter. he is working on a book called the public domain. his web site is http://jamesboyle.com.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix b207sherry brandtrauf is associate research scholar at the center for the study of society and medicine of thecolumbia college of physicians and surgeons. trained at columbia in law and sociology, she teaches and doesresearch on areas in which law and medicine overlap. particular areas of interest include the ownership ofscientific data, occupational health, genetic testing, conflicts of interest, and the ethics of research on vulnerablepopulations. she recently completed an individual project fellowship at the open society institute, researching thenature of the pharmaceutical industryõs interactions with medical students and residents and the effects of suchinteractions on the practice of medicine. in addition, under a grant from the jewish womenõs foundation of newyork, she recently prepared an online information booklet for ashkenazi jewish women dealing with genetictesting for brca mutations. she sits on the columbiapresbyterian medical center institutional review boardand pediatric ethics committee.francis bretherton obtained his ph.d. in applied mathematics at the university of cambridge, england. hisresearch areas include atmospheric dynamics and ocean currents. he has been a member of the faculty at cambridge, at the johns hopkins university, and at the university of wisconsinmadison, where he is now professoremeritus in the department of atmospheric and oceanic sciences. from 1974 to 1981, he was director of thenational center for atmospheric research in boulder, colorado, and president of the university corporation foratmospheric research. since then, he has been deeply involved in planning national and international researchprograms on climate and changes in our global environment. from 1982 to 1987, he was chair of the nasa earthsystem sciences committee, which formulated the strategy for the u.s. global change research program. dr.bretherton chaired the global observing system space panel under the auspices of the world meteorologicalorganization from 1998 to 2000. he has served on many committees of the nrc, most recently as chair of thecommittee on geophysical and environmental data, and is also a member of advisory panels and boards to thenational oceanic and atmospheric administration (noaa) on climate and global change research and onclimate system modeling.bertram bruce is a professor of library and information science at the university of illinois at urbanachampaign,where he has been a member of the faculty since 1990. before moving to illinois, he taught computer science atrutgers (19711974) and was a principal scientist at bolt, beranek and newman (19741990). his research andteaching focus on new literacies, inquirybased learning, and technology studies. a major focus of his work is withthe distributed knowledge research collaborative studying new practices in scientific research. other studiesinclude research on education enhancements to biology workbench (a computational environment that facilitatesbioinformatics research, teaching, and learning); plants, pathogens and people; physics outreach; and search.his analytical work has focused on changes in the nature of knowledge, community, and literacy. he serves on theeditorial boards of educational theory, computers and composition, discourse processes, computer, international journal of educational technology, and interactive learning environments.julie cohen is professor of law at the georgetown university law center. she teaches and writes about intellectual property and information privacy issues, with particular focus on computer software and digital works and onthe intersection of copyright, privacy, and the first amendment in cyberspace. she is a member of the advisoryboard of the electronic privacy information center, the advisory board of public knowledge, and the board ofacademic advisors to the american committee for interoperable systems. from 1995 to 1999, professor cohentaught at the university of pittsburgh school of law. from 1992 to 1995, she practiced with the san franciscofirm of mccutchen, doyle, brown & enersen, where she specialized in intellectual property litigation. professorcohen received her a.b. from harvard and her j.d. from harvard school of law. she was a former law clerk tothe honorable stephen reinhardt of the u.s. court of appeals for the ninth circuit.robert cookdeegan is director of the center for genome ethics, law, and policy at duke university. he is alsoa robert wood johnson health policy investigator at the kennedy institute of ethics, georgetown university,where he is completing a primer on how national policy decisions are made about health research. until july 2002,the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.208appendix bhe directed the robert wood johnson foundation health policy fellowship program at the institute of medicine(iom), the national academies. in 1996 dr. cookdeegan was a cecil and ida green fellow at the university oftexas, dallas, following his work on the national academiesõ report on allocating federal funds for science andtechnology. from 1991 through 1994, he directed iomõs division of biobehavioral sciences and mental disorders (since renamed neuroscience and behavioral health). he worked for the national center for human genomeresearch from 1989 to 1990, after serving as acting executive director of the biomedical ethics advisory committee of the u.s. congress, 19881989. an alfred p. sloan foundation grant culminated in the gene wars: science,politics, and the human genome. he continues as a consultant to the dna patent database at georgetownuniversity. dr. cookdeegan came to washington, d.c. in 1982 as a congressional science fellow and stayed fivemore years at the congressional office of technology assessment, ultimately becoming a senior associate. hereceived his b.s. degree in chemistry, magna cum laude, from harvard, and his m.d. degree from the universityof colorado. dr. cookdeegan chairs the royalty fund advisory committee for the alzheimerõs association, issecretary and trustee of the foundation for genetic medicine, and former chair of section x (social impacts ofscience and engineering) for the american association for the advancement of science (aaas), where he is alsoa fellow.dana dalrymple is senior research advisor in the office of agriculture and food security, u.s. agency forinternational development and agricultural economist in the foreign agricultural service of the u.s. department ofagriculture. he has helped administer u.s. government involvement in, and support of, the consultative group oninternational agricultural research and its network of 16 centers for 30 years. dr. dalrymple is a longtime studentof the development and adoption of agricultural technology in agriculture, both in the united states and internationally, and has published widely on this subject. he has also served as an analyst on studies of agricultural researchconducted by the nas (as part of a larger project) and the office of technology assessment of the u.s. congress. hehas recently reviewed the role of international agricultural research as a global public good. he received his b.s. andm.s. degrees from cornell university, his ph.d. in agricultural economics from michigan state university, and wasa member of the department of agricultural economics at the university of connecticut.paul david is professor of economics at stanford university and senior fellow of the stanford institute foreconomic policy research, where he leads the program on knowledge networks and institutions for innovation.since 1994 he has held a joint appointment as senior research fellow of all souls college, oxford. he is an electedfellow of the international econometrics society, of the american academy of arts and sciences, and of thebritish academy; he was president elect and president of the economic history association and served until 2001as an elected member of the council of the royal economics society. in 1997 the university of oxford conferredon professor david the title professor of economics and economic history òin recognition of distinction.ó he isknown internationally for his contributions in american economic history, economic and historical demography,and the economics of science and technology. much of professor davidõs research has been directed towardcharacterizing the conditions under which microeconomic and macroeconomic processes exhibit òpathdependentdynamics.ó he has published more than 130 journal articles and chapters in edited books, in addition to authoringand editing a number of books under his name.daniel drell is a biologist with the human genome program, office of biological and environmental research(ober), of the u.s. department of energy. his responsibilities include oversight of the component of the obergenome program devoted to ethical, legal, and social implications. prior to joining the ober in april 1991, dr.drell worked as a visiting scientist in the hla laboratory of the american red cross holland laboratory wherehe participated in ongoing research and service activities on hla typing of donor blood samples for the bonemarrow matching program. from 1986 to 1990, he was at the george washington university medical center,ending up as an assistant professor in the department of medicine and associate director of the immunogeneticsand immunochemistry laboratories. from 1983 to 1986, he was a staff fellow in the laboratory of oral medicineat the national institute of dental research, national institutes of health, conducting research in the areas of thecellmediated immunology of type 1 diabetes mellitus and autoimmune myositis. he has also held positions at thethe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix b209baylor college of medicine, the population councilõs center for biomedical research, and the laboratory ofdevelopmental genetics at sloankettering. dr. drell received his ph.d. in immunology from the university ofalberta, edmonton, and his b.a. in biology from harvard college. he has published nearly 30 scientific articlesand abstracts.shirley p. dutton is a senior research scientist and senior technical advisor to the director, bureau of economicgeology, the university of texas at austin. she received a b.a. in geological sciences from the university ofrochester, graduating with highest honors and election to phi beta kappa. she earned m.a. and ph.d. degrees,also in geological sciences, from the university of texas at austin. she has worked at the bureau of economicgeology since 1977, and her technical expertise is in clastic sedimentology and diagenesis. dr. dutton was adistinguished lecturer for the american association of petroleum geologists in 1987. she has authored or coauthored 172 published papers and abstracts.david heyman joined the center for strategic and international studies in november 2001 as a senior fellow forscience and security initiativesñstudies that explore the increasingly interconnected world of science, technology,and national security policy. previously he served as a senior advisor to the secretary of energy at the u.s.department of energy from 1998 to 2001 and in the white house in the office of science and technology policyin the national security and international affairs division from 1995 to 1998. prior to joining the clintonadministration, mr. heyman briefly worked as a consultant with ernst & young in their international privatizationand economics group in london, and was the director of international operations for a new york softwarecompany developing supplychain management systems for fortune 100 firms. he has worked in europe, russia,and the middle east. mr. heyman carried out his undergraduate work at brandeis university, with a concentrationin biology; his graduate work at the johns hopkins university school of advanced international studies was intechnology policy and international economics.stephen hilgartner is associate professor in the department of science and technology studies at cornelluniversity. professor hilgartnerõs research focuses on social studies of science and technology, especially biology, biotechnology, and medicine; biology, ethics, and politics; science as property; ethnography of science; andrisk. his recent book, science on stage: expert advice as public drama (stanford, 2000), explores the processesthrough which the expertise of science advisors is established, contested, and maintained. he is the author of manyarticles, book chapters, and reviews, including a series of works on data access and ownership. his work hasappeared in such journals as the journal of molecular biology; the journal of the american medical association;social studies of science; science, technology, and human values; science communication; and the americanjournal of sociology. professor hilgartner is chair of the ethical, legal, and social issues committee of thecornell genomics initiative. he is a member of the council of the society for social studies of science. he is alsoa member of the steering group of the section on societal impacts of science and engineering of the aaas. heis currently completing a book on genome mapping and sequencing in the 1990s.justin hughes is assistant professor at the benjamin n. cardozo school of law and was an attorney advisor in theu.s. patent and trademark office from 1997 to 2001, focusing on initiatives in internetrelated intellectualproperty issues, eleventh amendment immunity issues, intellectual property law in developing economies, and oncopyright appellate filings for the united states (including the napster litigation). professor hughes practiced lawin paris and los angeles and clerked for the lord president of the malaysian supreme court in kuala lumpur. heis a former henry luce scholar, mellon fellow in the humanities, and american bar association (aba) baxterscholar at the hague court. he was a visiting professor at the university of california at los angeles school oflaw. he received his b.a. from oberlin and his j.d. from harvard.tracy lewis was the james w. walter eminent scholar of entrepreneurship at the university of florida andbecame the newly appointed director of the duke university innovation center and professor of economics at thefuqua school of business in january 2003. his research and teaching interests are in the areas of innovationthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.210appendix bprocesses, industrial organization, and financial and incentive contracting. professor lewis has published numerous articles on the management of openaccess resources and has been a consultant for several federal regulatoryagencies and private corporations on issues of patent policy and protection of intellectual property.stephen m. maurer has practiced intellectual property litigation since 1982. he currently coteaches a course oninternet law and economics at the school of public policy at the university of california, berkeley. mr. maurerhas written extensively on patent reform, proposed database legislation, and science policy. over the past fouryears, he has worked with academic scientists trying to build a variety of commercially selfsupporting databases.in 2000 he helped a worldwide community of academic biologists (the mutations database initiative or mdi)negotiate a $3.2 million memorandum of understanding with incyte genomics. under the proposed agreement,incyte would have helped mdi scientists build a unified, computationally advanced archive for worldwide humanmutations data. in return, mdi would have granted incyte the exclusive right to host the database on a commercialweb site. academic and commercial users would have remained free to download and use the database for all otherpurposes. mr. maurer has also helped lawrence berkeley national laboratory physicists design a commercialventure for archiving highenergy accelerator data. he is a graduate of yale university and harvard school oflaw. he has published in many journals, including nature, science, economica, sky & telescope, and beam line.michael morgan is director of research partnerships & ventures and chief executive of the wellcome trustgenome campus in cambridge, england. dr. morgan joined the wellcome trust in 1983 and now is responsiblefor development of new enterprises such as the synchrotron project (a joint project with the u.k. and frenchgovernments) and the single nucleotide polymorphism consortium. he plays a major role in the internationalcoordination of the human genome project and is also responsible for scientific establishments such as thewellcome trust genome campus. three independent institutions are located on the campus: the wellcome trustsanger institute, the european bioinformatics institute, and the mrc human genome mapping resource centre.dr. morgan is a graduate of trinity college, dublin, and obtained his ph.d. from leicester university.harlan onsrud is professor of spatial information science and engineering at the university of maine and aresearch scientist with the national center for geographic information and analysis. he is immediate pastpresident of the university consortium for geographic information science, a nonprofit organization of over 70universities and other research institutions dedicated to advancing understanding of geographic processes, relationships, and patterns through improved theory, methods, technology, and data. professor onsrud is chair of thenrcõs u.s. national committee on data for science and technology (codata) and also serves on the nrcõsmapping science committee. a licensed attorney and engineer, his research focuses on the analysis of legal,ethical, and institutional issues affecting the creation and use of digital spatial databases and the assessment of thesocial impacts of spatial technologies. he teaches a range of legal courses to students in engineering, informationsystems, and computer science, including a graduate course in information systems law.bruce perens is one of the founding linux developers and is best known as the creator of the open sourcedefinition, the canonical guidelines for opensource licensing. mr. perens is cofounder of the open sourceinitiative, the linux standard base, and software in the public interest. his first free software program, electricfence, is widely used in both commercial and free software development. his busybox software became astandard tool kit in the embedded systems field and is thus included in many commercial print servers, firewalls,and storage devices. mr. perens has 19 years experience in the feature film business and is credited on the films òabugõs lifeó and òtoy story ii.ó as both a leader of opensource software and a longtime participant in the filmindustry, he is uniquely qualified to comment on the oftpainful intersection of copyright protection and the publicdomain. mr. perens was formerly employed as senior strategist for linux and open source at hewlett packard. heis the primary author of that corporationõs opensource policy manual.susan r. poulter is professor of law at the s.j. quinney college of law, university of utah, in salt lake city.she holds b.s. and ph.d. degrees in chemistry, both from the university of california, berkeley. after a periodthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix b211during which she taught chemistry at the university of utah, she received a j.d. from the university of utahcollege of law in 1983, where she was executive editor of the utah law review and was inducted into the orderof the coif. she joined the faculty of the college of law in 1990, after seven years in private law practice. sheteaches in the areas of environmental law, intellectual property, and torts. professor poulter has written andlectured on scientific evidence and environmental law. she is a coauthor of the reference guide on medicaltestimony in the second edition of the federal judicial centerõs reference manual on scientific evidence. she isa member of the council of the section of science and technology law of the aba and has served as a sectionrepresentative to the national conference of lawyers and scientists (ncls), a joint committee of the aba andthe aaas. currently, she is the aba cochair of the ncls. she is a member of the advisory board of the aaasproject on courtappointed scientific experts and participated in the aaas workshop on intellectual propertyand scientific publishing in the electronic age.rudolph potenzone became president and ceo of lion bioscience, inc. in march 2001. with over 20 years ofcheminformatics experience, he most recently served as senior vice president for marketing and development atmdl, where he managed the design, development, and marketing of mdlõs software and database products.previously, dr. potenzone was director of research and new product development at chemical abstracts service,as well as holding senior positions at polygen/molecular simulations, inc. dr. potenzone holds a ph.d. in macromolecular science from case western reserve university in cleveland, ohio, and a b.s. in biophysics andmicrobiology from the university of pittsburgh.jerome h. reichman became bunyan a. womble professor of law at duke university in july 2000, where heteaches in the field of contracts and intellectual property. before coming to duke, he taught at vanderbilt, michigan,florida and ohio state universities and at the university of rome, italy. he graduated from the university of chicago(b.a.) and attended yale school of law, where he received his j.d. degree. professor reichman has written andlectured widely on diverse aspects of intellectual property law, including comparative and international intellectualproperty law and the connections between intellectual property and international trade law. other recent writingshave focused on intellectual property rights in data, the appropriate contractual regime for online delivery of computer programs and other information goods, and new ways to stimulate investment in subpatentable innovationwithout impoverishing the public domain. professor reichman has served as consultant to the u.s. national committee for codata at the national academies on the subject of legal protection for databases. he also is an academicadvisor to the american committee for interoperable systems; a consultant to the technology program of the unconference on trade and development; and was a consultant on the un development programõs flagship project oninnovation, culture, biogenetic resources, and traditional knowledge.suzanne scotchmer is professor of economics and public policy at the university of california, berkeley. hergraduate degrees are a ph.d. in economics and an m.a. in statistics, both from the university of california,berkeley. she has held visiting appointments or fellowships at the university of cergypontoise, tel avivuniversity, university of paris i (sorbonne), boalt school of law, the university of toronto school of law, yaleuniversity school of law, stanford university, the hoover institution, and the new school of economics,moscow. she has published extensively on the economics of intellectual property, rules of evidence, tax enforcement, cooperative game theory, club theory, and evolutionary game theory. her work has appeared ineconometrica, american economic review, quarterly journal of economics, the rand journal of economics,the journal of public economics, journal of economic theory, the new palgrave dictionary of law and economics, and as commentary in science. she was previously on the editorial board of the american economic review,the journal of economic perspectives, and the journal of public economics, and is currently on the editorial boardof the journal of economic literature and regional science and urban economics. in 19981999 she served onthe nrc committee on promoting access to scientific and technical data for the public interest and hasappeared before several other committees of the nrc, mostly regarding intellectual property. she has served as aconsultant for the u.s. department of justice, antitrust division, and for private clients in disputes regardingintellectual property.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.212appendix bpaul f. uhlir is director of the office of international scientific and technical information programs at thenational academies in washington, d.c. his current area of emphasis is on scientific and technical data management and policy and on the relationship of intellectual property law in digital data and information to research anddevelopment policy. in 1997 he received the nas special achievement award for his work in this area. mr. uhlirhas been employed at the national academies since 1985, first as a senior staff officer for the space studies board,where he worked on solar system exploration and environmental remote sensing studies for nasa, and then asassociate executive director of the commission on physical sciences, mathematics, and applications. beforejoining the national academies, he worked in the general counselõs office and as a foreign affairs officer atnoaa. he has directed and published over 20 nrc studies and written or edited over 40 articles and books. mr.uhlir has a b.a. in history from the university of oregon, and a j.d. and m.a. degrees in international relations,with a focus on space law and arms control, from the university of san diego.peter n. weiss began work with the strategic planning and policy office of the noaa national weather service,in march 2000. his responsibilities include domestic and international data policy issues, with a view towardfostering a healthy publicprivate partnership. mr. weiss was a senior policy analyst and attorney in the office ofinformation and regulatory affairs, office of management and budget (omb), since 1991. mr. weiss analyzedpolicy and legal issues involving information resources and information technology management, with particularemphasis on electronic data interchange and electronic commerce. he is primary author of the information policysections of omb circular a130, òmanagement of federal information resources,ó and was a member of theadministrationõs electronic commerce working group (see òa framework for global electronic commerceó).from 1990 to 1991, mr. weiss was deputy associate administrator for procurement law, office of federalprocurement policy. in this position, he analyzed legal and policy issues affecting the procurement process. majorprojects included examination of legal and regulatory issues involving procurement automation, policies andfederal acquisition regulation revisions to facilitate electronic data interchange, as well as automatic data processing procurement legal and policy issues. from 1985 to 1990, mr. weiss was the assistant chief counsel forprocurement and regulatory policy, office of advocacy, u.s. small business administration. from 1981 to 1985,mr. weiss was in private practice in washington, d.c. mr. weiss holds a b.a. from columbia university and aj.d. from the catholic university of america, columbus school of law. a recent publication is òinternationalinformation policy in conflict: open and unrestricted access versus government commercialization,ó in borders in cyberspace, kahin and nesson, eds. (mit press, 1997).ann j. wolpert is director of libraries for the massachusetts institute of technology (mit), and has reportingresponsibility for the mit press. her responsibilities include membership on the committee on copyright andpatents, the council on educational technology, the deanõs committee, and the presidentõs academic council.she chairs the management board of the mit press and the board of trustees of technology review, inc. priorto joining mit, ms. wolpert was executive director of library and information services at the harvard businessschool. her experience previous to harvard included management of the information center of arthur d. little,inc., an international management and technology consulting firm, where she was also engaged in consultingassignments. she is active in the professional library community, currently serving on the board of the bostonlibrary consortium, on the board of the association of research libraries; and as a member of the editorialboards of library & information science research and the journal of library administration. she is a trustee ofsimmons college in boston, massachusetts, and presently serves as an advisor to the publications committee ofthe massachusetts medical society. recent consulting assignments have taken her to the campuses of incae incosta rica and nicaragua and to the malaysia university of science and technology, selangor, malaysia. afrequent speaker and writer, ms. wolpert has recently contributed papers on such topics as library service toremote library users, intellectual property management in the electronic environment, and the future of researchlibraries in the digital age. she received a b.a. from boston university and an m.l.s. from simmons college.wm. a. wulf is president of the national academy of engineering and vice chair of the nrc, the principaloperating arm of the national academies of sciences and engineering. he is on leave from the university ofthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix b213virginia, charlottesville, where he is at&t professor of engineering and applied sciences. among his activitiesat the university are a complete revision of the undergraduate computer science curriculum, research on computerarchitecture and computer security, and an effort to assist humanities scholars exploit information technology. dr.wulf has had a distinguished professional career that includes serving as assistant director of the national sciencefoundation; chair and chief executive officer of tartan laboratories inc., pittsburgh; and professor of computerscience at carnegie mellon university, pittsburgh. he is the author of more than 80 papers and technical reports,has written three books, and holds two u.s. patents.jonathan zittrain is the jack n. and lillian r. berkman assistant professor of entrepreneurial legal studies atharvard school of law, and a faculty director of its berkman center for internet & society. his research includesdigital property, privacy, and speech, and the role played by private òmiddle peopleó in internet architecture. hecurrently teaches òinternet & society: the technologies and politics of controló and has a strong interest increative, useful, and unobtrusive ways to deploy technology in the classroom. he holds a j.d. from harvardschool of law, an m.p.a. from the j.f.k. school of government, and a b.s. in cognitive science and artificialintelligence from yale. he is also a 15year veteran sysop of compuserveõs online forums.steering committee membersdavid r. lide, jr. (chair) is a consultant and the former director of the national institute of standards andtechnologyõs standard reference data division. his expertise is in physical chemistry and scientific information.he has served as president of the international codata, chair of codataõs publication committee, and chairof the u.s. national committee for codata. he is currently a member of the international council for science/codata ad hoc group on data and information, which focuses on problems of intellectual property rights andaccess to data, and a fellow of the international union of pure and applied chemistry. dr. lide is the editorinchief of crcõs handbook of chemistry and physics.hal abelson is the class of 1922 professor of computer science and engineering with mitõs department ofelectrical engineering and computer science. he holds an a.b. from princeton, and a ph.d. in mathematics frommit. in 1992, professor abelson was designated as one of mitõs six inaugural macvicar faculty fellows inrecognition of his significant and sustained contributions to teaching and undergraduate education. he was 1992recipient of the bose award (mitõs school of engineering teaching award) and the winner of the 1995 taylor l.booth education award given by the institute of electrical and electronic engineers (ieee) computer society,cited for his continued contributions to the pedagogy and teaching of introductory computer science. professorabelson has a longstanding interest in using computation as a conceptual framework in teaching, and he iscurrently teaching a class on ethics and law on the electronic frontier. he is a fellow of ieee.mostafa elsayed is the julius brown chair, regentsõ professor and director of the laser dynamics laboratoryat the school of chemistry and biochemistry of the georgia institute of technology, where he joined the facultyin 1994. professor elsayed received his b.sc. from ain shams university, cairo, egypt, and his ph.d. fromflorida state university. after being a research associate at harvard, yale, and the california institute of technology, he was appointed to the faculty of the university of california at los angeles in 1961. he was an alfred p.sloan, john simon guggenheim fellow, a visiting professor at the university of paris, a sherman fairchilddistinguished scholar at california institute of technology, and a senior alexander von humboldt fellow at thetechnical university of munich. professor elsayed is the editorinchief of the journal of physical chemistry aand b and was an editor of the international reviews of physical chemistry. professor elsayed is a member of thenas and the third world academy of science and is a fellow of the american academy of arts and sciences, theamerican physical society, and the aaas.mark frankel, ph.d., directs the scientific freedom, responsibility and law program at the aaas. he isresponsible for developing and managing aaas activities related to science, ethics, and law. he serves as staffthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.214appendix bofficer to two aaas committeesñthe committee on scientific freedom and responsibility and the aaasabanational conference of lawyers and scientists. he is editor of professional ethics report, the programõs quarterlynewsletter, and is a fellow of aaas.maureen kelly recently served as vice president for planning at biosis, which is the largest abstracting andindexing service for the life sciences community, and is now a consultant. ms. kelly worked in different capacitiesfor biosis since 1969. previously she had production responsibility for the bibliographic and scientific content ofbiosis products. while in that position, she led the team that developed the system for capturing and managingindexing data in support of biosisõs new relational indexing. ms. kelly has authored a number of papers onmanaging and accessing biological information. she is currently secretary of the aaas section on information,computing, and communication. she has served on various professional society research and publishing committees, including participating in several nas ejournal summit meetings. ms. kelly is a member of the u.s.national committee for codata and served on the nrc committee on promoting access to scientific andtechnical data in the public interest: an assessment of policy options.pamela samuelson is a professor of law and information management at the university of california at berkeleyand codirector of the berkeley center for law and technology. her expertise is in intellectual property law, andshe has written and spoken extensively about the challenges that new information technologies are posing forpublic policy and traditional legal regimes. prior to joining the faculty at berkeley, professor samuelson was atthe university of pittsburgh school of law, where she had taught since 1981. a graduate of yale school of law,she has also practiced with the new york firm of willkie farr & gallagher and served as the principal investigatorfor the software licensing project at carnegie mellon university. in 1997 professor samuelson was named afellow of the john d. and catherine t. macarthur foundation. in 1998 she was recognized by the national lawjournal as being among the 50 most influential female lawyers in the country and among the eight most influentialin northern california. she was recently elected to membership in the american law institute and named a fellowof the association of computing machinery. in 2001 she was appointed to a university of california, berkeley,chancellorõs professorship for distinguished research, teaching, and service for her contributions to both boalthall and the school of information management and systems. professor samuelson was a member of the u.s.national committee for codata and the nrcõs computer science and telecommunications boardõs committee on intellectual property rights and the emerging information infrastructure.martha e. williams is director of the information retrieval research lab and a professor of information scienceat the university of illinois at urbanachampaign. her research interests include digital database management,online retrieval systems, systems analysis and design, chemical information systems, and electronic publishing.she has published widely on these topics and has been editor of the annual review of information sciences andtechnology (since 1975), computer readable databases: a directory & data sourcebook (19761987), andonline review (since 1977). professor williams was chair of the board of engineering information, inc. from1980 to 1988, was appointed to the national library of medicineõs board of regents from 1978 to 1981, andserved as chair of the board in 1981. in addition, she served on several nrc committees, including the numericaldata advisory board (19791982). she has an a.b. from barat college and an m.a. from loyola university. ms.williams was a member of the nrcõs committee for a study on promoting access to scientific and technicaldata for the public interest.the role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix c215215appendix csymposium attendees.hal abelsonmassachusetts institute of technologyhal@mit.eduallan adlerassociation of american publishersadler@publishers.orgprudence adlerassociation of research librariesprue@arl.orgkerri allensparckerri@arl.orgdebra aronsonfederation of american societies forexperimental biologydaronson2op@faseb.orgcynthia banickiu.s. patent and trademark officecbanicki@uspto.govbruce barkstromnasa langley research centerb.r.barkstrom@larc.nasa.govolga baryshevathe national library of russiabarysh@nlr.rudavid becklerconsultantd2beckler@aol.comr. stephen berryuniversity of chicagoberry@uchicago.edugail blaufarbnational cancer instituteblaufarg@mail.nih.govkim bonneruniversity of maryland university collegekbonner@umuc.eduscott borenboren@mindspring.comjames boyleduke university school of lawboyle@law.duke.eduthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.216appendix csusan bragdoninternational plant genetic resources institutes.bragdon@cgiar.orgsherry brandtraufcolumbia universityspejo@aol.comfrancis brethertonuniversity of wisconsinfbretherton@charter.netglenn browncreative commonsglenn@creativecommons.orgmichael brownthe institute for genomic researchmbrown@tigr.orgbertram bruceuniversity of illinois, urbanachampaignchip@uiuc.eduelizabeth buffumnasa/center for aerospace information for asrcaerospace corp.ebuffum@sti.nasa.govpaul buggu.s. office of management and budgetpbugg@omb.eop.govmerry bullockamerican psychological associationmbullock@apa.orgsan cannonfederal reserve boardscannon@frb.govbonnie carrollinformation international associates, inc.bcarroll@infointl.comamy centanniu.s. department of veterans affairsamy.centanni@mail.va.govjoan cheveriegeorgetown university librarycheverij@georgetown.edusangdai choiuniversity of marylandsdchoi777@hotmail.combonnie chojnakiwhite memorial (chemistry) librarybc128@umail.umd.educharles clarkdefense intelligence agencyafclacx@dia.osis.govjeff clarkjames madison universityclarkjc@jmu.edujulie cohengeorgetown university school of lawjec@law.georgetown.edusarah comleyinternational observersscomley@mail.comrobert cookdeeganduke universitybob.cd@duke.educharlotte cottrillu.s. environmental protection agencycottrill.charlotte@epa.govchristian cuppdefense technical information centerccupp@dtic.milpaul cutlerthe national academiespcutler@nas.edulisa dacostathe institute for genomic researchldacosta@tigr.orgthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix c217dana dalrympleu.s. agency for international developmentddalrymple@usaid.govpaul davidstanford universitypad@stanford.edurudolph dichtluniversity of colorado cooperative institute forresearch in environmental sciencesdichtl@kryos.colorado.edujulie dietzelglairuniversity of marylandjdietzel@wam.umd.edudr. sidney dragganu.s. environmental protection agencydraggan.sidney@epamail.epa.govdaniel drellu.s. department of energydaniel.drell@science.doe.govbeatrice drokeu.s. food and drug administrationbdroke@oc.fda.govnhathang duongdefense technical information centercduong@dtic.milshirley duttonuniversity of texas at austinshirley.dutton@beg.utexas.edualice ecclesuniversity of marylandaeccles@wam.umd.eduanita eisenstadtnational science foundationaeisenst@nsf.govjulie esanuthe national academiesjesanu@nas.edulloyd etheredgepolicy sciences centerlloyd.etheredge@yale.educynthia etkinu.s. government printing officecetkin@gpo.govv. jeffrey evansnational institutes of healthjeffevans@nih.govphoebe fagannational institute of standards and technologyphoebe.fagan@nist.govsusan falloncommunity of scienceskf@cos.comcindy fanguniversity of maryland, college parkxfang@wam.umd.edusuzanne fedunoknew york university bobst librarysuzanne.fedunok@nyu.edumartha feldmannational agricultural librarymfeldman@nal.usda.gove.t. fennessythe copyright groupefennessy@att.netwalter finchnational technical information servicewfinch@ntis.govnigel fletcherjonesnature america, inc.n.fletcherjones@natureny.comcarolyn floydnasa langley research centerc.e.floyd@larc.nasa.govthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.218appendix cpeter folgeramerican geological unionpfolger@agu.orgolga francoisuniversity of maryland university collegeofrancois@umuc.eduamy franklinthe national academiesafranklin@nas.eduken fultonthe national academieskfulton@nas.edujohn gardeniercenter for disease control and prevention, nationalcenter for health statisticsdrgarden@starpower.netlorrin garsonamerican chemical societylgarson@acs.orgruggero giliarevskyallrussian institute for scientific and technicalinformationgiliarevski@viniti.rukarl glaseneragronomy, soil science societieskarlglasener@cs.comjerry glennstars of the futurejglenn@igckyrille goldbeckuniversity of maryland, college parkkkg4@georgetown.edukaren goldmannational institutes of healthgoldmank@mail.nih.govdov greenbaumyale universitydov.greenbaum@yale.edujane griffithnational library of medicinejbgriffith@nlm.nih.govrobert hardycouncil on governmental relationsrhardy@cogr.edusam hawalau.s. census bureausam.hawala@census.govjanet heckuniversity of maryland, college of informationstudiesjheck@wam.umd.edustephen heinigassociation of american medical collegessheinig@aamc.orgstephen hellernational institute of standards and technolgysrheller@nist.govjennifer hendrixamerican library associationjhendrix@alawash.orgrobert hersheyengineering and management consultinghershey@cpcug.orgdavid heymancenter for strategic and international studiesdheyman@csis.orgstephen hilgartnercornell universityshh6@cornell.eduderek hillnational science foundationdhill@nsf.govwill hiresjohns hopkins universitywill.hires@jhuapl.eduthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix c219cynthia holtgeorge washington universityholt@gwu.edujustin hughescardozo school of lawhughes@ymail.yu.eduruthann humphreysjohns hopkins university applied physics labr.humphreys@jhuapl.edujoe ingramjohn wiley & sons, inc.jingram@wiley.comvsevolod isayevrussian embassyvsevis@rategier.rukenan jarboeathena alliancekpjarboe@athenaalliance.orgjennifer jenkinsduke university school of lawjenkins@law.duke.eduscott jenkinsfdc reports, inc.s.jenkins@elsevier.comlaura jenningsnational imagery and mapping agencyjenningl@nima.mildouglas jonesuniversity of arizonajonesd@u.library.arizona.eduheather josephbioone, inc.heather@arl.orgjanet joythe national academiesjjoy@nas.edubrian kahinuniversity of marylandbk90@umail.umd.edusubhash kuvelkerkuvelker law firmkuvelker@mindspring.commyra karstadtu.s. environmental protection agencykarstadt.myra@epa.govatsushi katouniversity of tokyoakato@ip.rcast.utokyo.ac.jpmaureen kellyconsultantmckelly@ix.netcom.comchristopher keltyrice universityckelty@rice.edumiriam keltynational institute on agingmk46u@nih.govbonnie kleinu.s. department of defensebklein@dtic.miljeremiah knightuniversity of marylandjeremiahknight@yahoo.comdavid kornassociation of american medical collegesdkorn@aamc.orgthomas krauseu.s. patent and trademark officethomas.krause@uspto.govkeith kupferschmidsoftware and information industry associationkeithk@siia.netthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.220appendix ccatherine langrehrassociation of american universitiescatherinelangrehr@aau.educarol leeinternational finance corporationclee@ifc.orgron leearnold & porterronaldlee@aporter.comrolf lehmingnational science foundationrlehming@nsf.govdave levyu.s. environmental protection agencylevy.dave@epa.govdavid lewisblackmask onlinellewis@blackmask.orgtracy lewisuniversity of floridalewistr@dale.cba.ufl.edurose lianalytical sciences, inc.rli@asciences.comjean liddellauburn universityliddeje@auburn.edudavid lide, jr.national institute of standards and technologydrlide@post.harvard.eduyoung limelectronics and telecommunications researchinstituteyylim@etri.re.krdavid lipmannational institutes of healthlipman@ncbi.nlm.nih.govjacqueline liptoncase western reserve universityjdl14@po.cwru.edurichard llewellyniowa state universityrllew@iastate.edukarin lohmanhouse science committeekarin.lohman@mail.house.govjun luouniversity of maryland, college of informationstudiesjun@wam.umd.edubenjamin lumfdc reportsb.lum@elsevier.compatrice lyonspalyons@bellatlantic.netmary lyonsuniversity of maryland, college of informationstudieslyonsmary@yahoo.comneil macdonaldfederal technology reportnamacdee@aol.comconstance malpasnew york academy of medicinecmalpas@nyam.orgcheryl marksdivision of cancer biology, national cancerinstitutemarksc@mail.nih.govdavid martinsenamerican chemical societydmartinsen@acs.orgcarol masonuniversity of maryland, college of informationstudiesmasoncl@wam.umd.eduthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix c221eric massantreed elsevier inc.eric.massant@lexisnexis.compaul massellu.s. census bureaupaul.b.massell@census.govyukiko masudauniversity of washington school of lawymasuda@u.washington.edustephen maurerconsultantmaurer@econ.berkeley.eduannemarie mazzathe national academiesamazza@nas.edupatrice mcdermottomb watchpatricem@rtk.netstephen merrillthe national academiessmerrill@nas.edupeggy merrymanu.s. geological survey librarymmerryma@usgs.govhenry metzgernational institutes of healthmetzgerh@exchange.nih.govlinda milleruniversity corporation for atmospheric research/unidatalmiller@unidata.ucar.edujohn mitchellpublic knowledgejmitchell@publicknowledge.orgkurt molholmdefense technical information centerkmolholm@dtic.milrichard monasterskychronicle of higher educationrichm@chronicle.csmmichael morganwellcome trustm.morgan@wellcome.ac.ukrebecca moseru.s. environmental protection agencymoser.rebecca@epa.govsuzy mouchetinserm, parismouchet@tolbiac.inserm.frteresa mullinsnational snow and ice data centertmullins@kryos.colorado.eduvincent munchaudrey cohen collegemunchv@audreycohen.edug. craig murrayuniversity of marylandgcraigm@glue.umd.edumichael nelsonibm corporationmrn@us.ibm.comrichard nelsoncolumbia universityldb3@columbia.edumiriam nisbetamerican library associationmnisbet@alawash.orgvivian nolanu.s. geological surveyvpnolan@usgs.govcarlo nussnational institutes of healthnussc@mail.nlm.nih.govthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.222appendix cterry oõbryanu.s. enviromental protection agencyobryan.terry@epa.govann okersonyale universityann.okerson@yale.edujay olinuniversity of marylandjill oõneillnational federation of abstracting and informationservicesjilloneill@nfais.orgnahoko onouniversity of tokyonahokoono@ip.rcast.utokyo.ac.jpharlan onsruduniversity of maineonsrud@spatial.maine.edudianne ostrownational cancer instituteostrowd@mail.nih.govpierre oudetinsermpierre.oudet@chrustrasbourg.fraristides a. patrinosu.s. department of energyari.patrinos@science.doe.govbruce perenshewlett packardbruce@perens.comvirginia petermanhoward county, marylandvpeterman@co.ho.md.ustroy petersenduke universitytroy.petersen@law.duke.edukatharina phillipscouncil on governmental relationskphillips@cogr.edunicole pinhasinserm, parisnicole.pinhas@dicdoc.inserm.frneal pomeauniveristy of maryland university collegenpomea@umuc.edurudolph potenzonelion biosciencerudolph.potenzone@lionbioscience.comrobert pottsu.s. department of veterans affairsbob.potts@mail.va.govsusan poulteruniversity of utah school of lawpoulters@law.utah.edumatt powellnational science foundationmpowell@nsf.govmatthew quintembassy of australiamatthew.quint@auestemb.orgroberta randuniversity of miamirrand@rsmas.miami.edualan rapoportnational science foundationarapoport@nsf.govjerome reichmanduke university school of lawreichman@law.duke.eduelspeth reverethe john d. and catherine t. macarthur foundationerevere@macfound.orgthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix c223cody riceu.s. environmental protection agencyrice.cody@epa.govalison roeskeios press inc.iosbooks@iospress.comjohn rumblenational institute of standards and technologyjohn.rumble@nist.govcarrie russellamerican library associationcrussell@alawash.orgjoshua sarnoffamerican universityjsarnoff@wcl.american.edusuzanne scotchmeruniversity of california, berkeleyscotch@socrates.berkeley.edujoan shigekawathe rockefeller foundationjshigekawa@rockfound.orgituki shimboriken, gscshimbo@gsc.go.jpjoan siebernational science foundationjsieber@nsf.govelliot siegelnational library of medicinesiegel@nlm.nih.govpamela sievingnational institutes of health librarysievingp@ors.od.nih.govmary silvermanamericaõs bulletinwilliam sittigthe library of congresswsit@loc.govf. hill slowinskiworthington internationalhslowinski@worthingtoninternational.comabby smithcouncil on library and information resourcesasmith@clir.orgkent smithnational library of medicinekentsmith@nlm.nih.govlowell smithu.s. environmental protection agencysmith.lowell@epa.govlarry snavleyrensselaer polytechnic institutesnavll@rpi.edugregory snyderu.s. geological surveygsnyder@usgs.govjack snydernational library of medicinesnyderj@mail.nlm.nih.govanthony sothe rockefeller foundationaso@rockfound.orggigi sohnpublic knowledgegbsohn@publicknowledge.orgsohyun sonelectronics and telecommunications researchinstituteshson@etri.re.krshelley sperrycoalition for networked informationshelley@cni.orgmiron strafthe national academiesmstraf@nas.eduthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.224appendix cmarti szczurnational library of medicineszczurm@mail.nlm.nih.govglenn tallianational oceanic and atmospheric administrationglenn.e.tallia@noaa.govvalerie thebergethe national academiesvtheberge@nas.eduhenry tomburicalifornia state university at haywardsamuel trosowuniversity of western ontariostrosow@uwo.cakelly turnernational oceanic and atmospheric administrationkelly.turner@noaa.govpaul uhlirthe national academiespuhlir@nas.edujustin vanfleetamerican association for the advancement ofsciencejvanflee@aaas.orgwalter warnicku.s. department of energywarnickw@osti.govjennifer washburnjaw529@aol.comlinda washingtoncenters for disease control and preventionlrw1@cdc.govrebecca wasonthe american society for cell biologyrwason@ascb.orglee watkinsjohns hopkins universitylwatkins@jhu.edupeter weissnational weather servicepeter.weiss@noaa.govstephen weitzmandatapharm foundationsaw519x@aol.comeric wiesernuclear waste newsewieser@bpinews.commartha williamsuniversity of illinois at urbanachampaignmwill13@staff.uiuc.eduann wolpertmassachusetts institute of technology librariesawolpert@mit.edutamae wongthe national academiestwong@nas.edustephen woothe national academiesswoo@nas.edujulia woodwardanne arundel community collegejbwoodward@aacc.eduwilliam wulfnational academy of engineeringwwulf@nae.eduyoung ye linelectronics and telecommunications researchinstituteyylim@etri.re.krjohn youngamerican council for the u.n. university millennium projectvistasjy@md.prestige.netjonathan zittrainharvard school of lawzittrain@law.harvard.eduthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.appendix d225225appendix dacronyms and initialisms.abaamerican bar associationbpbritish petroleumcaschemical abstracts servicecgiarconsultative group on international agricultural researchcodatacommittee on data for science and technologycpchallenge programcsscontent scrambling systemdmcadigital millennium copyright actdoedepartment of energydrmdigital rights managementelsiethical, legal and social implicationseosatearth observation satellite (company)estexpressed sequence tage.u.european uniongccgnu c compilerguigraphical user interfacegnua recursive name for gnu not unix; refers to stallmanõs free software movementgplgeneral public licensehgshuman genome scienceshphewlett packardhrhouse resolutionthe role of scientific and technical data and information in the public domain: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.226appendix dieeeinstitute of electrical and electronic engineersiominstitute of medicineipintellectual propertyiprintellectual property rightsitinformation technologylasllos alamos sequence librarymdimutations database initiativemitmassachusetts institute of technologymtamaterial transfer agreementnasnational academy of sciencesnasanational aeronautics and space administrationncdcnational climate data centernclsnational conference of lawyers and scientistsnihnational institutes of healthnoaanational oceanic and atmospheric administrationnrcnational research councilnsfnational science foundationoberoffice of biological and environmental research (doe)omboffice of management and budgetptopatent and trademark officer&dresearch and developmentriaarecording industry association of americasnpsingle nucleotide polymorphisms&tscientific and technicalstiscientific and technical data and informationtactechnical advisory committeeucitauniform computer information transactions act
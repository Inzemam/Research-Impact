detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/10983electronic scientific, technical, and medical journalpublishing and its implications: proceedings of a symposium136 pages | 8.5 x 11 | paperbackisbn 9780309092173 | doi 10.17226/10983committee on electronic scientific, technical, and medical journal publishing;committee on science, engineering, and public policy; policy and globalaffairs; national research councilelectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. electronic scientific, technical, and medical journal publishing and its implications proceedings of a symposium  committee on electronic scientific, technical, and medical journal publishing committee on science, engineering, and public policy policy and global affairs division  the national academies the national academies press washington, d.c. www.nap.edu electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, n.w. washington, dc 20001  notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.  support for this project was provided by the national academy of sciences through an unnumbered internal grant. any opinions, findings, conclusions, or recommendations expressed in this publication are those of the author(s) and symposium speakers and do not necessarily reflect the views of the organizations or agencies that provided support for the project.  committee on science, engineering, and public policy, 500 fifth street, nw, washington, dc 20001; 2023342424; internet, http://www7.nationalacademies.org/cosepup/ additional copies of this report are available from the national academies press, 500 fifth street, n.w., lockbox 285, washington, dc 20055; (800) 6246242 or (202) 3343313 (in the washington metropolitan area); internet, http://www.nap.edu  copyright 2004 by the national academy of sciences. all rights reserved.  printed in the united states of america electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. bruce m. alberts is president of the national academy of sciences. the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. wm. a. wulf is president of the national academy of engineering. the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v. fineberg is president of the institute of medicine. the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. bruce m. alberts and dr. wm. a. wulf are chair and vice chair, respectively, of the national research council. www.nationalacademies.org iiielectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. steering committee on electronic scientific, technical, and medical journal publishing and its implications edward h. shortliffe (chair), columbia university medical center daniel atkins, university of michigan floyd bloom, the scripps research institute jane ginsburg, columbia university school of law clifford lynch, the coalition for networked information jeffrey mackiemason, university of michigan ann okerson, yale university mary waltham, publishing consultant principal project staff paul uhlir, project director alan inouye, senior program officer julie esanu, program officer robin schoen, program officer kevin rowan, project associate amy franklin, senior program assistant ivelectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. committee on science, engineering, and public policy maxine f. singer (chair), carnegie institution of washington r. james cook, washington state university haile t. debas, university of california, san francisco marye anne fox, north carolina state university elsa m. garmire, dartmouth college maryclaire king, university of washington w. carl lineberger, university of colorado anne c. petersen, w. k. kellogg foundation cecil b. pickett, scheringplough research institute gerald m. rubin, howard hughes medical institute edward h. shortliffe, columbia university medical center hugo f. sonnenschein, the university of chicago irving l. weissman, stanford university, school of medicine sheila widnall, massachusetts institute of technology mary lou zoback, u.s. geological survey staff richard e. bissell, executive director deborah d. stine, associate director marion e. ramsey, administrative associate velectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. vielectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. preface the use of the internet and other digital information technologies by the scientific, technical, and medical (stm) research community in the united states and most other countries has transformed many aspects of the research and publishing process. the new technologies have created fundamental changes in the production, management, dissemination, and use of all types of information. it is now possible to communicate research results much more quickly, broadly, and openly than was possible through traditional print publications in the past. researchers are now able to make available independently their data and articles online, where the information may be easily found, browsed, annotated, critiqued, downloaded, and freely shared. this is resulting in significant changes to the linear path of writing, refereeing, and reviewing of publications as all these functions can be performed concurrently. most stm publishers also now publish electronic versions of their journals, some exclusively so. the technological developments and resulting changes to the sociology of science are creating both opportunities and challenges for the effective management of scientific communication generally, and stm publishing more specifically. because of the farreaching implications of these developments, the national academy of sciences council™s committee on publications recommended that the council commission a study of the factors involved in the changing mechanisms for access to stm information in the scholarly publications and the various technical, legal, policy, and economic issues that they raise. the committee indicated that it is imperative for the national academies to address, in particular, the increasing concerns about the implications of various models for access to stm publications for the scientific community. as a result, the committee on science, engineering, and public policy was asked to appoint a committee to oversee the planning for the symposium on electronic scientific, technical, and medical journal publishing and its implications, which was held may 1920, 2003, at the national academy of sciences in washington, d.c. the symposium brought together experts in stm publishing, both producers and users of these publications, to: (1) identify the recent technical changes in publishing, and other factors, that influence the decisions of journal publishers to produce journals electronically; (2) identify the needs of the scientific, engineering, and medical community as users of journals, whether electronic or printed; (3) discuss the responses of notforprofit and commercial stm publishers and of other stakeholders in the stm community to the opportunities and challenges posed by the shift to electronic publishing; and (4) examine the spectrum of proposals that has been put forth to respond to the needs of users as the publishing industry shifts to electronic information production and dissemination. the symposium was divided into six sessions, each introduced by opening comments from a moderator, followed by several invited presentations. session 1 examined the costs involved with the publication of stm journals while session 2 looked at the related publication business models. session 3 explored the legal issues in the production and dissemination of these journals. sessions 4 and 5 looked toward the future and examined, respectively, what is publication in the future and what constitutes a publication in the digital environment. the final session provided several commentaries on the results of the symposium. this publication presents the proceedings of the symposium. the speakers™ remarks were taped and transcribed, and subsequently edited. the statements made in the enclosed papers are those of the individual authors and do not necessarily represent the positions of the steering committee or the national academies. the national academies hosted a live audio webcast of the symposium to reach a broad audience and receive additional input. this webcast can be found on the symposium web site at: http://www7.nationalacademies.org/cosepup/epublishing.html/. a summary report prepared by the symposium committee has been published separately and is available from the national academies press. edward shortliffe paul uhlir committee chair project director viielectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. acknowledgments the committee on electronic scientific, technical, and medical journal publishing and its implications would like to thank the following individuals (in alphabetical order) who made presentations during the symposium (see appendix a for the final symposium agenda): hal abelson, massachusetts institute of technology (mit); bruce alberts, national academy of sciences; kent anderson, new england journal of medicine; malcolm beasley, stanford university; robert bovenschulte, american chemical society; monica bradford, science; patrick brown, stanford university; brian crawford, john wiley & sons; james duderstadt, university of michigan; joseph esposito, sri consulting; michael jensen, harvard business school; michael keller, highwire press; david lipman, national center for biotechnology information; wendy lougee, university of minnesota; richard luce, los alamos national laboratory; james o™donnell, georgetown university; paul resnick, university of michigan; bernard rous, association for computing machinery; alex szalay, johns hopkins university; gordon tibbitts, blackwell publishing usa; and ann wolpert, mit. the committee also would like to express its gratitude to the guidance group for this project, which was formed under the committee on science, engineering, and public policy. members of that group included james cook, washington state university; paul torgerson, virginia polytechnic institute and state university (retired); and edward shortliffe, columbia presbyterian medical center, columbia university. this volume has been reviewed in draft form by individuals chosen for their technical expertise, in accordance with procedures approved by the nrc™s report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for quality. the review comments and draft manuscript remain confidential to protect the integrity of the process. we wish to thank the following individuals for their review of selected papers: martin blume, american physical society; karen hunter, elsevier health services; justin hughes, cardozo law school; james neal, columbia university; andrew odylzko, university of minnesota; and carol tenopir, university of tennessee at knoxville. although the reviewers listed above have provided constructive comments and suggestions, they were not asked to endorse the content of the individual papers. responsibility for the final content of the papers rests with the individual authors. finally, the committee would like to recognize the contributions of the following national research council staff. paul uhlir, director of the office of international scientific and technical information programs, was the project director for the symposium and principal editor of the committee™s report; julie esanu, program officer for the office of international scientific and technical information programs, helped organize the symposium and edit the report; alan inouye, interim director of the computer science and telecommunications board, and robin schoen, program officer for the board on life sciences, provided advice on the project; and kevin rowan, project associate for the committee on science, engineering, and public policy provided project support for the may symposium; and amy franklin, senior program assistant for the board on international scientific organizations, assisted with the production of this report. viiielectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. contents 1. introductory remarks 1 bruce alberts, president, national academy of sciences 2. keynote address 3 james duderstadt, president emeritus and university professor of science and engineering, millennium project, university of michigan 3. costs of publication 6 introductory comments, 6 floyd bloom, the scripps research institute overview of the costs of publication, 6 michael keller, librarian and publisher, stanford university comments by panel participants ,11 kent anderson, publishing director, new england journal of medicine robert bovenschulte, director, publications division, american chemical society bernard rous, deputy director/electronic publisher, association for computing machinery gordon tibbitts, president, blackwell publishing, usa discussion of issues, 19 4. publication business models and revenue 27 introductory comments, 27 jeffrey mackiemason, university of michigan comments by panel participants, 28 brian crawford, vice president and general manager, life and medical sciences, john wiley & sons joseph esposito, president and chief executive officer, sri consulting wendy lougee, director, university of minnesota library patrick brown, professor of biochemistry, stanford university discussion of issues, 38 5. legal issues in production, dissemination, and use 46 introductory comments,46 jane ginsburg, columbia law school copyright basics: ownership and rights, 46 jane ginsburg, columbia law school licensing, 59 ann okerson, yale university economic and noneconomic rewards to authors: the social science research network example, 61 michael jensen, jesse isidor straus professor of business administration, emeritus, harvard business school discussion of issues, 62 ixelectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 6. what is publishing in the future? 67 introductory comments, 67 daniel atkins, university of michigan implications of emerging recommender and reputation systems, 68 paul resnick, associate professor, university of michigan school of information preprint servers and extensions to other fields, 70 richard luce, research library director, los alamos national laboratory institutional repositories, 72 hal abelson, class of 1922 professor of computer science and engineering, massachusetts institute of technology discussion of issues, 74 7. what constitutes a publication in the digital environment? 80 introductory remarks, 80 clifford lynch, coalition for networked information the signal transduction knowledge environment, 81 monica bradford, executive editor, science publishing large data sets in astronomyšthe virtual observatory, 83 alex szalay, alumni centennial professor, department of physics and astronomy, the johns hopkins university genomic data curation and integration with the literature, 86 david lipman, director, national institutes of health/national center for biotechnology information discussion of issues, 87 8. symposium wrap up 95 moderator™s overview, 95 mary waltham, publishing consultant comments by panel participants, 96 malcolm beasley, theodore and sydney rosenberg professor of applied physics, stanford university james o™donnell, provost, georgetown university ann wolpert, director of libraries, massachusetts institute of technology discussion of issues, 100 closing remarks, 104 edward shortliffe, symposium chair xelectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. appendixes a. symposium agenda, 105 b. speakers™ biographies, 108 c. symposium participants, 115 xielectronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 1. introductory remarks bruce alberts, national academy of sciences this symposium is part of a longstanding effort of the national academies to promote wise policies for science. the important functions of scientific publication from a policy perspective include scientific validation, the dissemination of information, and the specific obligations created when an author publishes. the scientific validation function of publication is accomplished by peer review and by editing to ensure that the science is sound. these processes also ensure that the data and the methods are complete and clearly presented. this is what distinguishes the information in scientific journals from the vast amount of other material that is also on the internet. there are many publications that make a false claim to be scientific, and since the seventeenth century the scientific community itself has organized to discriminate between what is, and what is not, good science. without this discrimination, science could not move steadily forward by building upon what has already been discovered. publication also has a critical dissemination function. that is what is largely motivating this symposium, because the internet is a radically new way to disseminate scientific information. this powerful communication channel greatly enhances the potential reach of science; it allows us to include nations and people who would otherwise not be reachable. equally important, by creating powerful search engines that can rapidly find desired information, we can make much better use of the vast store of data and scientific information available. we could do a lot better, however. for instance, there are great opportunities for data mining that have not yet been adequately exploited. in addition, there are many journals that are still not available in electronic form. moreover, foreign journals are not comprehensively abstracted by u.s. services such as pubmed. science is an international activity, and we need to make the knowledge that is developed everywhere more readily available to other scientists. our own journal is the proceedings of the national academy of sciences (pnas). we have tried to set an example by making its electronic version as widely available as possible. the entire 16,000 pages each year of pnas are immediately available free on the internet to those in most developing nations, and this information can be freely accessed by everyone after a sixmonth delay. the national academies publish all of our reports through the national academies press. this is a different kind of scientific literaturešthe results of consensus studies on issues such as the health effects of arsenic in drinking water, the science of climate change, and thousands of other topics. there are about 10 years™ worth of national academies reportsšsome 2,900 booksšonline, all readable page by page. in addition, the pdf files for these reports are made freely available to anyone in 130 developing nations. what about the obligations of authors? the national academies held a workshop in february 2002 that was sponsored by the board of life sciences entitled "sharing publicationrelated data and materials: responsibilities of authorship in the life sciences." tom cech, the head of the howard hughes medical institute and a nobel prizewinning scientist, was the chair of the committee that organized this event. the final report from the workshop contained the following statement: the publication of scientific information is intended to move science forward. more specifically, the act of publishing is a quid pro quo in which authors receive credit and acknowledgement in exchange for disclosure of their scientific findings, providing them a forum on which other scientists can build with further research. an author, therefore, has the obligation to release both data and materials to enable others to verify and extend published findings. 1electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. publication therefore creates important obligations on the part of the authors that need to be enforced, both through the grant agencies that fund the research and through the journals that publish the work. it is only with this kind of sharing, enforced by the scientific community, that the public™s generous support of knowledge production through scientific research can produce its intended benefits. 2electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 2. keynote address james duderstadt, university of michigan the goal of this symposium is to bring together experts from an array of constituencies, including both producers and users of stm publications, to look at some of the technical changes that have occurred in electronic publishing, and how they influence decisions to publish or not; to identify the needs of the science, technical, and medical (stm) publishing enterprise itself as users of journals; to understand the responses of both the commercial and notforprofit stm publishers; and to examine a very broad spectrum of proposals and activities underway that are attempting to respond to the needs of the community with these new technologies. a major focus of this symposium is to look at business models and to try to establish the degree to which they address many of the challenges and concerns. during the discussions it is important to keep in mind the ongoing developments in the scientific enterprise itself, stimulating in part, and being stimulated by, this kind of scholarly communication. how is electronic publishing affecting the practice of scientific researchšthe communication of research results to scholars and others, perhaps including the public; the curation of data and evaluation of research; and archiving of results? the challenge is to identify the issues and problems that the stm community needs to control and resolve if it is to exploit the remarkable opportunities presented by this very rapidly evolving technology and also cope with the challenges it presents. the current situation can perhaps be described as a chaos of concerns, with the continuation of some disturbing trends that have evolved over the last couple of decades. access to stm information is increasingly expensive, and in some cases restricted. and yet, the amount of information generated at research institutes continues to grow. journal subscription prices continue to escalate, yet university library budgets fail to keep pace, particularly in these days of economic challenges at both the federal and state level. the price inflation in electronic publication resources, estimated to be running at approximately 10 percent per year over the past decade,1 has continued to run well ahead of the consumer price index. but even more dramatic has been the increase in the pricing for reference tools, increasing as much as 600 percent over the print cost of bound volumes. to these challenges should be added the growing complexity of dealing with various financial models and the licensing schemes that provide access. it is clear that these new technologies have created very fundamental changes in the production, management, dissemination, and use of all kinds of information. if one were to categorize very simply the two camps of concerns, on the part of the publishers the critical question is, how many copies of work will be sold or licensed if networks make possible planetwide access? and the nightmare, of course, is that the answer is only one. one document can be replicated time and time again, to not only serve, but perhaps collapse, the entire marketplace. on the other side, the nightmare to consumers is that in our efforts to preserve the marketplace, we will put in place an array of technical and legal protections that reduce access to what should be a public good, society's intellectual and cultural heritage. there are a lot of reactions and counterreactions at the university level. the first reaction is a budgetary one. university libraries simply cancel many subscriptions. in some cases this is mandated by limited resources. in other cases it is an effort to get the attention of the publishing industry, although what cancellation generally does is simply drive up the costs even further for the remaining subscriber base. 1 see barbara albee and brenda dingley. 2003. ﬁu.s. periodical pricesš2002,ﬂ american library association, washington, dc. available at http://www.ala.org/content/navigationmenu/productsandpublications/periodicals/americanlibraries/selectedarticles/usperiodicalprices,2002.htm. 3electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. in other instances, we have seen rebellion at the grassroots. editorial boards have protested against the commercial publisher journal prices and have resigned and moved to less expensive publishers in scientific societies. the complexity and shifting from a firstsale approach characteristic of paper to licensing have caused a good deal of experimentation. there are many other variants. one approach is reminiscent the way that dissertations used to be generated from the university microfilms collection, as an edition of one. that is, to begin to make it acceptable that there may only be one physical copy of a document but have the ability to reproduce that from an online copy, at the user's expense. another interesting approach that is emerging involves the open source or openaccess strategy. the success of the open source software movement through linux, the apache web server, and similar technology has given rise to a number of related openaccess initiatives, such as the open knowledge initiative and the mit opencourseware project. these initiatives focus on developing new financial models for the open distribution of scholarly materials, perhaps by building charges for dissemination into research grants that generate the information in the first place. this is not only consistent with the traditions and values of academia but also reinforces the definition of the university as a public good, an issue that university leaders are increasingly worrying about these days, when the rest of society tends to look at us more as a market commodity. in summary, advances in digital technology are producing radical shifts in our ability to reproduce, distribute, control, and publish information. yet, as these advances become more a part of scientific activity, they tend to run headlong into the existing practices, policies, and laws that govern traditional publishing. the issues are complex, in part, because the stakeholders are so many, so varied, and with different agendas. people who fund research want to see that the information is advanced and made available to the public. the authors, editors, and reviewers do not charge for their labor. they are motivated to contribute to the public good, but of course they also have other rewards, not the least of which is tenure. publishers, as intermediaries, although they do not pay for content, do add significant value and provide the work in published form. libraries, similarly, are intermediaries. they provide access to the users of stm content. they pay the subscription fees, but they usually do not charge for providing access. and, of course, the end users either pay for personal subscriptions or obtain the resources free through libraries. there are several more general issues that need to be considered. first, is the changing nature of science and technology research. as pointed out in the recent national science foundation (nsf) report, revolutionizing science and engineering through cyberinfrastructure,2 the process of knowledge creation itselfšexperimentation, analysis, theory development, and forming conclusionsšis increasingly occurring entirely in the digital world. that has caused a shift from the sequential process of research, publication, validation, and dissemination to more of a parallel flow model that is interactive, in which the process of publication and distribution actually becomes almost the process of research itself. the key point of the report is that distributed network computing technology is providing a new kind of infrastructure for federating people, information, computational tools and services, and specialized facilities into virtual organizationsšsocalled collaboratories or grid communities or, as the europeans call it, escience, a cyberinfrastructure. the vision put forth by this nsf report is to use this infrastructure to build ubiquitous and comprehensive digital environments. such environments will become interactive and functionally complete for research communities in terms of the people, data, information tools, and instruments, and that operate at unprecedented levels of computational storage and data transfer capacity. part of the aim is to trigger the necessary public and private investments to create this cyberinfrastructure. nevertheless, many elements of it are already in place, and it will significantly change the nature of scholarly activity, including scholarly publication. the reality today is that electronic publishing is becoming the dominant mechanism for publishing and reading scholarly materials. it opens vast possibilities, of course, but it challenges existing practices and principles, including the way in which we handle intellectual property. a new paradigm for scholarly 2 national science foundation. 2003. revolutionizing science and engineering through cyberinfrastructure: report of the national science foundation blueribbon advisory panel on cyberinfrastructure, arlington, va, january. 4electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. communications is coming into focus that is capable of providing open online access to the work of scholars without payment; online repositories of highquality, certified materials; and a stable economic model to sustain these resources. this will pose a particular challenge to libraries, shifting them from a focus on collecting and archiving knowledge resources, to assisting scholars in navigating these resources. today, the campus library has become somewhat less central to researchers' lives. the library has evolved from a place into a utility. it too is becoming a part of the internet. legal and policy issues are the second major issue area. it is clear that the emerging digital infrastructure imperils a great many of our existing practices, policies, and laws that have served intellectual activity in this country and globally so well over the past two centuries, forcing the rethinking of some fundamental premises and practices associated with intellectual property. indeed, there is a concern that many of these will be challenged to the bedrock. the third topic concerns the evolution of digital technology. in 2000 the national academies created a study group chosen from industry, higher education, and federal policy development to understand better what the implications of digital technology were for the research university, and even more broadly, for the research enterprise.3 the concern was that although the opportunities and challenges of this technology were important, many of the most significant issues were neither well recognized nor understood. among the early conclusions of this effort was that the recognition that the extraordinary evolutionary pace of digital technology shows no sign of slowing, with some aspects such as storage and wireless bandwidth evolving at superexponential rates the second conclusion was that the impact of the technology on the university will be profound, rapid, unpredictable, discontinuous, and disruptive. it will affect all of the activities of the universityœteaching, research, outreach, its organization, financing, governance, even the definition of its faculty and students. procrastination and inaction are the most dangerous courses of all during a time of rampant technological change. the report™s third major conclusion, and an interesting one, was that universities should begin the development of strategies for facing this kind of technologydriven change with a firm understanding of those key values, missions, and roles that need to be protected and preserved during a time of transformation. these include traditions such as openness, academic freedom, and the rigorous of academic inquiry. a fourth area of concern is the commercialization of academic output, as the soaring commercial value of much of the intellectual property produced on the campuses raises very significant challenges to traditions such as openness and academic freedom. finally, there are the issues of national security, which again call into the question of balancing scientific openness and with the restrictions on public information necessary for homeland security. as we address these complex issues, we might well keep in mind the well known observation of thomas jefferson: if nature has made any one thing less susceptible than all others of exclusive property, it is the action of the thinking power called an idea, which an individual may exclusively possess, as long as he keeps it to himself. but the moment it is divulged, it forces itself into the possession of everyone, and the receiver cannot dispossess himself of it. that ideas should freely spread from one to another over the globe for the moral and mutual instruction of man, and the improvement of his condition, seems to have been peculiarly and benevolently designed by nature when she made them like fire, expansible over all space without lessening their density at any point, and like the air in which we breathe, move and have our physical being, incapable of confinement or exclusive appropriation. inventions then cannot, in nature, be a subject of property. 3 see national research council. 2003. issues for science and engineering researchers in the digital age, national academy press, washington, d.c. 5electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 3. costs of publication introductory comments floyd bloom, the scripps research institute cost concerns control quality, timeliness, and access so it is natural to begin the symposium with the consideration of the costs of scientific and medical publishing. we have to consider the costs of production, the costs of paper and ink, the costs of dissemination, the costs of acquiring the content that will be produced, and those costs that make a journal competitive with other journals. we also have to be concerned with the explicit, rigorous peerreview processes of scientific journals and the quality of what is published. as we enter into the era of online publication, we encounter additional costs and concerns. how about the linking to the databases that give a publication equal access and immediacy, and carry it back into the perspective of the past? why do these things cost so much, and which of these costs could we factor out and control if we knew how to? the panel that addressed these issues included a variety of perspectives: a librarian, publishers of online and print journals of large and mediumsized societies, and a commercial publisher. overview of the costs of publication michael keller, stanford university michael keller, stanford university librarian and chairman of the board of highwire press, began the session by presenting some recent statistics compiled by michael clarke of the american academy of pediatrics, which provide a good overview of the marketplace for scientific, technical, and medical (stm) journals. although these statistics are not directly on the cost of publications of electronic journals, they do set the stage for many of the issues discussed below. according to a recent morgan stanley industry report,4 the stm journal market has been the fastest growing segment of the media industry for the past 15 years, with 10 percent annual growth over the past 18 years, and a predicted 5 to 6 percent annual growth over the next 5 years. at the same time, there has been a consolidation of stm journals into a small number of giant publishers. more than 50 percent of stm journals are published by the 20 largest publishers. according to the ingenta institute,5 until recently, the number of scientific journals doubled every 15 years but the rate of growth of new titles has substantially slowed over the past 5 years; journal size has grown in order to compensate for the lack of new titles; cost of subscriptions has escalated due to size of issues and investments in online technologies; and library budgets are under increasing pressure. there has been a proliferation of library consortia over the past 25 years. these consortia are playing an increased role in electronic journal purchasing decisions and negotiations. the ﬁbig dealﬂ began in 1997. it guarantees publishers steady income and provides libraries with steady prices and increased access to titles. the majority of big deals have been signed in the past three years, and the first large wave of contracts will be expiring in 2003. predictions for the institutional marketplace include decline of library materials budgets, decline of the big deal, a move toward smaller collections, selection of titles will return to importance, and there will be animosity toward forprofit publishers. 4 morgan stanley industry report. 2002. scientific publishing: knowledge is power, new york, ny. 5 ingenta institute. 2002. the consortium site license: is it a sustainable model? oxford, uk. 6electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. main elements of expense budgets for some stm journal publications first, the processing costs for the content (i.e., articles, reports of the results, and methods of scholarly investigation) of stm journals come in several subcategories:6 (1) manuscript submission, tracking, and refereeing operations; (2) editing and proofing the contents; (3) composition of pages; and (4) processing special graphics and color images. internet publishing and its capacity to deliver more images, more color, and more moving or operating graphics have made this expense grow for stm publishers in the past decade. the second category of expense is a familiar one, but is also one of two targets for complete removal from the publishers' costsšthe costs of paper, printing, and binding, as well as mailings. as researchers educated and beginning their careers in the 1990s replace retiring older members of the stm research community, publishers might finally switch over to entirely internetbased editions and distribute no paper at all. this transformation would thus move the costs of printing and paper to the consumer desiring articles in that form and remove binding and mailing from the equation altogether. a third category of expense is that of the internet publishing services. these are new costs, and they include many activities performed mainly by machines, though in some situations staff perform quality control pre and postpublication to check and fix errors introduced through the publishing chain. the elements of these costs vary tremendously among publishers and internet publishing services. at the high end they can include parsing supplied text into a rigorously controlled version of sgml or xml; making hyperlinks to data and metadata algorithmically; presenting multiple resolutions of images; offering numerous elaborate search and retrieval possibilities; supporting reader feedback and email to authors; supporting alerting and prospective sighting functions; delivering content for indexing to secondary publishers and distributors, as well as to internet indexing services; and supporting individualized access control mechanisms. at the low end, those characterized by pdfonly epublishing, the cost elements would include simple search, common access control mechanisms, and delivering content for indexing. the range of costs in internet publishing services is quite wide, although the actual size of this category in the expense budget is relatively small. the fourth cost categoryšpublishing supportšis everything from catering of lunches, to finance offices, including facilities and marketing. the final category is the cost of reserves. some organizations have money set aside for disasters or to address opportunities. some of these reserves are for capital projects or to hedge against key suppliers failing. a great many notforprofit organizations do not label reserves as such but have investments or bank accounts whose earnings support various programs, but whose principal could be used in a reserve function as needed. the results of a recent sampling by michael keller of six notforprofit publishers' costs are presented in table 31. the data were compiled in a wide variety of categories, so there is an element of interpretation in these results. the data are presented as a pair of ranges, one from the early 1990s and the other for the most recent data. 6 the cost budgets for science and nature, for instance, would have more elements than specified, and costs for secondary and tertiary publications include different elements than these. mr. keller did not cover these elements in stm publishing. 7electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. table 31. sample of six notforprofit publishers™ costs. range of costs of publicationpublishers n = 51992 or 19932002 or 2003edit:28.4%53.3%32.4%56.0%print:25.9%50.0%21.8%27.0%mail:4.3%16.9%4.3%10.0%internet:n/a4.3%9.1%support:10.0%29.4%3.0%33.2%reserve:0.0%12.2%0.0%7.2%society money: 0.0%1.9%0.0%2.9% how might these data be interpreted? first, it appears the publishers have much tighter control now over their budgets than they did 10 years ago. the definitions of categories are better. second, it is clear that editorial costs have not changed much but that printing, paper, and binding costs are down, at least on a unit basis. third, the costs of internet editions have entered the budgets at the level of 4.3 to 9.1 percent. in addition, respondents to mr. keller™s inquiry indicated that their publishing budgets have doubled since the early 1990s but that individual subscriptions are almost half in many cases, apparently cannibalized by institutional internet subscriptions. cost increases reported by the notforprofit publishers have been on the order of 6 percent annually. with a reduction in individual subscriptions, the number of copies of issues printed, bound, and mailed has gone down, but increases in other costs have kept budget numbers from falling. the reduction in individual subscriptions arises from two pressures. the first is the rising subscription prices themselves or membership dues for individuals. the business models of some publishers involve extra charges for online access. the second is the more easily available content through library servers, which enables a scientist to go to different sources, including especially journals, without having to resort to lots of different user names and passwords. it is simply more convenient to do it that way. clearly, the publishers that highwire has worked with are very concerned about cannibalization and have attempted in many ways to figure out how to cope with it. there may not be a single right answer. manuscript submission tracking and refereeing support applications have reduced mailing costs and made it possible for more manuscripts to be processed by existing staff. increases in the size of the journal in page equivalents, increases in graphics and colors in some instances, and the adoption of advanced internet features like supplemental information and so forth have increased the costs of internet publishing services about 50 percent higher per year over other costs. in short, there is a dynamic balancing act with regard to publishers' costs in the internet era, with some costs increasing and others decreasing. what is most intriguing, however, is the possibility of removing from 25 to 32 percent of the costs of publishing by switching to electronic journals delivered 8electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. over the network, and eliminating printing, binding, and mailing paper copy to any subscribers at all. one might further observe this as a transfer of costs from the publishers to the readers who desire to read articles on paper. perhaps this is not a new transfer of costs to readers, because many already photocopy articles and presumably cover that time and cost somehow. eliminating paper editions would also offer some promise of reducing prices to the institutions that are so clearly providing the publishers with the economic basis for publishing at all. the successful operation of true digital archivesšprotected repositories for the contents of journalsšwould permit the removal of the printing, binding, and mailing costs. a true digital archive or repository in the view of most librarians is one that is not merely an aggregation of content accessible to qualified readers or users, but one that preserves and protects the content, features, and functions of the original internet edition of the deposited journals over many decades and even centuries. true digital archives will have their standards and operational performances publicly known and monitored by publishers, researchers, and librarians alike. their operations and content then will be audited regularly. however, it is not yet well known how costly automatic data migration will be over time. in any event, it is not enough for a publisher, a library, or an aggregator or other information business to simply declare themselves to be an archive. this function must be proven constantly. the annual costs of true digital archives can have a broad range. one very inexpensive model is lots of copies keep stuff safe (lockss). these are network caches, a design that involves publishers and libraries in voluntary partnerships enabling dozens, maybe hundreds of local caches on cheap magnetic memory, and using very ordinary cpus. michael keller estimates that each of these lockss caches could operate for only tens of thousands of dollars per year. another model is that of the large, managed digital repository for multiple data formats and genres of publication. the estimate for operating and maintaining a very large repository, perhaps a petabit or two of data, at stanford is between $1 million to $1.5 million per year, half for staff and half for technology. highwire's database now is in excess of 2.5 terabits. which institutions will undertake such large managed digital repositories? almost certainly a few national libraries and university libraries will. but publishers or their internet service providers could develop and run them as well. the european union's laws will soon require deposit of digital editions in one or more national libraries. in the united states, the library of congress, along with other federal libraries, promises to develop both its own digital repository, as well as to stimulate and support a distributed network of them. if publishers undertake digital repositories, their costs will enter into the expense budget and, of course, drive journal prices higher. another cost currently confronting many publishers is the conversion of back sets of print journals to digital form, providing some level of metadata and word indexing to the contents of each article, and posting and providing access to the back sets. highwire has done a study on converting the back sets of its journals. they estimate that about 20 million pages could be converted and that the costs of scanning and converting pages to pdf, keying headers, loading data to the highwire servers, keying references, and linking references could approach $50 million, or about $150,000 per title. most of that sum is devoted to digitizing companies and other subcontractors to highwire press. if all this retrospective conversion of back sets occurred in one year, highwire would have to spend internally about $250,000 in capital costs and about $300,000 in initial staff costs, declining to annual staff expenditure of perhaps $250,000 or $275,000 thereafter. on average, for the 120 publishers paying for services from highwire that would mean about an additional $2,500 in new operating costs to highwire press each year. in other words, the increase in annual costs to publishers for hosting and providing access to the converted back sets would be a fraction of 1 percent of their current expenses each year. these figures, of course, omit any costs for digitizing and other services provided by contractors and subcontractors. although the costs of backset conversion are high, the experience of highwire suggests that the payoff could be 510 times more use of articles in the back sets than is presently experienced. articles on the highwire servers are read at the following rates: within the first three months of issue, about 95 percent of all articles get hits (this presumes that a hit means that somebody is actually reading something). in the next three months, that is, when the articles are fourtosix months old, slightly less than 50 percent of all articles get hits. and when articles are ten months or more old, on average of only 710 percent of all articles get hits. however, that rate of hits seems to persist no matter how old the online articles are. 9electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. based on citation analyses, only 10 percent of articles in print back sets older than the online set of digital versions get cited, though not necessarily read. that they should do so is entirely consistent with the belief commonly held since 2001 by publishers associated with highwire that the version of record of their journals is the online version. this is leading many publishers to digitize the entire run of their titles as the logical next step. in any case, unless other sources of funds are forthcoming, the costs of backset conversion will become a temporary cost in the expense budgets. other stm journal publishers, however, have indicated that their backset conversion and subsequent maintenance costs have been considerably higher. there are chickens and eggs in expense budgets that also complicate understanding them over time. for instance, the experience of highwire press has been that those publishers who first define and design advanced features pay the cost of developing those innovations for those that follow. at the same time, those early adopters reap the benefits of innovation in attracting authors and readers. eventually, many of the innovative features become generally adopted, and usually at lower cost of adoption than paid by the innovators to innovate. in order to maintain a reputation as innovative, however, one has to continue adopting new features. some innovation leads to lower costs. for example, highwire recently announced reductions in prices, thanks to some processing innovations it recently implemented. certainly, readers of online journals value the search engines and strategies, the navigation devices, hyperlinks, and availability of various resolutions of images. alerting services are particularly well thought of too. these sorts of online functions lead users to be more selfsufficient in their search for information and reduce calls upon librarians for help in finding relevant information. yet it is unlikely that science librarians are volunteering to reduce their staff size as a result. concluding observations a close study of the costs and benefits of electronic journal publishing from the birth of the world wide web would be a good thing to do. it would document, in a neutral way, the profound transformation of an important aspect of the national research effort. that there is likely to be as much change in the next 10 years as in the past decade does not obviate the need for the study. such a study may help develop new strategies or evolve current ones for accommodating needs of scientists and scholars to report their findings, and for ensuring the longterm survival of the history of science, medicine, and technology. university budgets are under considerable strain and will be so for at least several more years. will the deals for access to all journals from a single publisher survive or will there be new deals for multiple titles, cut just right to fit true institutional needs? why should there not be a highly diffuse distribution scheme based on authors simply posting their articles on their own sites or on an archive like cornell arxiv.org eprint archive (formerly at los alamos national laboratory), and let google or more specialized search engines bring relevant articles to readers on demand? who needs all this expensive publishing apparatus anyway? the answer lies partly in the strong need for peer review of content, expressed variously by most communities of science, and partly in the functions provided by good publishers that are valued and demanded by the scientific community itself. the ejournal survey mentioned earlier showed that the vast majority of respondents want, and have come to expect, a wide array of features making their regular searches for articles relevant to their work easy to find and to use. by implication, the readership focuses its attention in the constantly churning galaxy of new and old articles on a few journals with editorial policies and content that are known and trusted. providing relevant, reliable, and consistent levels of content in journals costs money. highly distributed, diffuse stm publishing with sketchy peer review, dependent upon new search engines to replace the wellarticulated scheme of thematic journals and citations in a multidimensional web of related articles, is a descent into information chaos. perhaps in the next decade the segment of stm publishing most at risk is the secondary publishers, the abstracters and indexers and the tertiary publishers, those producing the review and prospective articles long after the leadingedge researchers have made use of the most useful articles. none of the alternative publishing experiments underway or about to get underway operates independently of a larger stm journal publishing establishment, and none operates without costs. taking the example of the cornell arxiv in particular, although it is certainly an archive of articles to which many 10electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. in physics, mathematics, and computer science go to first and constantly, it has had negligible effect, if any, in reducing the number of peerreviewed articles published in these fields. it may have improved the articles by exposing them in preprint form to many readers, some of whom may have commented back to the author with helpful suggestions. but physics letters and physical review have continued to grow, have continued to publish peerreviewed articles, and those articles have continued to be cited. it also must be observed that some communities use and read preprints reluctantly. although there are 200 articles in the british medical journal's clinical netprints, few have received online and public peer review from readers, and fewer, if any, have been cited in that form. finally, several experiments in journals that depend nearly entirely on fees paid by authors should be noted. the new journal physics, for example, has published between 25 and 50 articles in each of its first five years of existence. but it has not yet had sufficient citations to be indexed by the science citation index. the change in the business plans of biomed central is instructive too. after trying author fees alone, it is selling memberships to institutions so that authors from member institutions do not have to pay for publications. but the membership fee is very high. the public library of science (plos) will enter this list with its first articles in the fall of 2003. it too will depend upon authors' fees for support of its operations. it is starting with an admirable $9 million war chest from the moore foundation. plos will charge $160 for print copies of its volumes. the point of mentioning these efforts is that not one has done away with the costs of publishing. someone always pays. and none of these journals with new business models has become selfsufficient. for the costs of peerreviewed publishing in science, technology, and medicine to disappear, the requirement for peer review and the demand for thoughtfully gathered, edited, illustrated, and distributed articles must disappear too. how the experiments in business models might provide competitive pressure on traditional business models and pricing is a topic for discussion and examination over time. experiments should be tried, but in michael keller™s view, the solution to the serious crisis of escalating journal costs lies in the notforprofit societies, whose purposes and fundamental economic model are very closely allied with the purposes and notforprofit economic models of our research universities and labs. comments by panel participants kent anderson, new england journal of medicine kent anderson provided comments from the perspective of the new england journal of medicine (nejm), which resides at the interface of scientific research and clinical practice, and which has published continuously for 191 years. the journal mainly serves clinicians, physicians who take care of patients. it is a key translator and interpreter of new science to general and specialist physicians around the world. in many ways, the nejm situation is unique: it is a largecirculation publication that relies on individual subscriptions, and it is owned by a small state notforprofit medical society. mr. anderson limited his comments to three areas: how the definition of ﬁpublicationﬂ is changing; how that change in definition is modifying how the nejm conducts its business of getting the journal out week to week; and finally, some cautionary notes about conducting a study of publishing costs because of the diversity of stm publications and their users. the changing definition of publication publication used to refer to the act of preparing and issuing the document for public distribution. it could also refer to the act of bringing a document to the public's attention. these definitions served us well for more than 400 years. now, publication means much more. it now means a document that is webenriched, with links, search capabilities, and potentially other services nested in it. a publication may soon be expected to be 11electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. maintained in perpetuity by the publisher. a publication now generates usage data. for publications serving physicians, however, the nejm has found that physicians are not willing to give up print. they are too pressed for time, and the print copy is too convenient for them. a recent publication by king and tenopir supported this finding as well.7 the changing conduct of publishing at the new england journal of medicine the mission of the nejm is to work at the interface of biomedical research and clinical practice. as medical and scientific findings become more complex and sophisticated, the journal invests in editors, writers, and illustrators who can analyze and interpret these findings for a clinical audience, so that its readers understand precisely how medicine is evolving. the nejm also supports significant costs for web publishing, data analysis, and reporting. it develops new services at a rapid clip. and it pays for new publishing modalities, such as onlineonly publication, early release articles, free articles to lowincome countries, free research articles after six months, and selected free articles online. the journal™s educational mission has become more complex. it has invested heavily in new continuing medical education initiatives, new ways of illustrating and presenting articles, and new ways of helping users find what they are looking for. it is more important, and in some ways more difficult than ever, to know who the journal™s readers are. the staff can no longer look at the print distribution and point to them as its readership. the web amplifies and concurrently obscures the readership. so, investments in market research, web data analysis, and warehousing and attending medical meetings with physicians have increased significantly. customer service demands have escalated, resulting in the need to develop new software and management systems, the hiring of new staff, and additional phone calls and mail. these service requirements are also adding to the cost of running a publication. online peerreview tools in parallel with paper systems have recently added a new layer of cost, with no apparent end to the investment, because periodic software upgrades will be necessary. the skill sets of the journal™s expert editors and workers also need continuous improvement to ensure that they can handle all the new inputs and maintain quality. marketing the journal™s new programs and services is another expense. the word needs to get out that there are new ways to access the journal™s information. email systems are part of the marketing and distribution effort now, and these have to be supported, databases need to be developed and maintained, and email notices consistently sent to existing and potential readers. the pressures to be fast are growing, yet high quality must be maintained. the nejm publishes information about health, and if it makes mistakes, they can be serious. recently, the journal published a set of articles on sars, the sudden acute respiratory syndrome, and it published them in two weeks or less of receiptšcompletely peer reviewed, edited, and illustrated papers. they were translated into chinese within two days of their initial publication and distributed in china in the thousands in print, with the hope they made a major difference. as such demands for faster publication mount, the journal will need to find ways to accomplish them without sacrificing quality, and this is already leading to significant investments in people and systems. often in these discussions of electronic journal publishing, the people behind all this become obscured. issues in conducting a study of journal publishing costs if the publishing costs are to be is studied well, there has to be an acknowledgment of the diversity of the publishing landscape, even in the scientific, technical, and medical publishing area. if only a few publishers participate, the selection bias could drive the study to the wrong answers. it is necessary to consider which cohorts need to be analyzed. the null hypothesis must be clearly stated. the questions to be asked must be properly framed and a reasonable control group 7 carol tenopir and donald w. king. 2002. ﬁreading behavior and electronic journals,ﬂ learned publishing, 15, 262265. 12electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. selected. in short, a study of this nature could be valuable, but it needs to be well designed, rigorously conducted, and carefully interpreted. robert bovenschulte, american chemical society electronic publishing has not only revolutionized the publishing industry, it has also tremendously changed the fundamental economics of the stm journal business. many of these issues overlap both the commercial and the notforprofit publishers. what is driving this change? the costs are increasing very rapidly, much more than would have happened if publishers had stayed with print alone. there are two obvious factors responsible for this: the cost of new technology and the increasing volume of publishing. costs of new technology with all of this expansion of technology costs, the publishers are delivering a much more valuable product to their users. there are enormous new functionalities that are being made available to scientists. the access to information is swift, convenient, and is improving productivity. technology now imbues all facets of publishingšfrom author creation and submission, all the way through to peer review, production and editing, output, and usage. the costs of the technology are not just related to the web, but apply to all the other technical systems that publishers have to create and integrate. for example, the american chemical society (acs) has 186 editorial offices worldwide for its 31 journals, and all of those offices have to be technically supported. acs must develop new technologies that support the functioning of those offices and make them more productive. in 2002, acs conducted a study through the seybold consulting group with 16 other publishers to assess the future of electronic publishing and, in particular, to get a better understanding of the cost drivers. both the median and the average cost among these 16 publishers was 5.5 percent going into just the information technology (it) function. large publishers allocated less than 2 percent, and the acs spent about 9 percent. although it is obvious that larger publishers with a lot more revenue can spend a much smaller percentage on it, they are nonetheless far outspending the mediumsized and the smaller publishers in the total amount that they can invest in their it operation. the predictions of the 16 publishers in aggregate were an average of a 21 percent annual increase in their it spending for the foreseeable future. the increasing volume of publishing the acs has gone from publishing 15,000 articles in 1993, to 23,000 in 2002ša 53 percent increase over that 9year period. over the past 2 years or so, the acs has experienced doubledigit increases in submissions. this appears to be largely spurred by the fact that online submission makes submitting an article even easier than before. moreover, the acs is receiving a much larger fraction of its submissions from outside the united states. during this same 9year period, total costs of publishing at the acs increased 64 percent, versus the 53 percent increase in articles published. but the cost per article published increased only 7.4 percent. in 1993, for every article the acs published, it cost $1,712. in 2002, the cost was $1,838. therefore, there has been quite a significant gain in publishing productivity at acs, and probably at other publishers, attributable in large measure to technology. although the technology does cost a lot more, it seems to produce efficiencies. digital repositories the acs was one of the first publishers to create a full digital archive, pdf form only, of all of its journals. this entailed a very large upfront cost. in the future, it is likely that preserving the journal literature could be the responsibility of both the library community and the publishers. 13electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. michael keller™s notion of trying to move toward one system has a lot of merit. that is not going to be easy to achieve, however. in the interim, the publishers, especially the notforprofit publishers, feel a very strong obligation to preserve that digital heritage. the end users, and particularly the scientists who write for acs journals, are not ready to give up print. and, because of the preservation issue, most librarians are not yet ready to give up print either. once print versions are eliminated, there might be a savings of 15, 20, or 25 percent of costs. ending print versions of journals is certainly a worthwhile goal. the position at the acs is to do nothing to retard the rate at which the community wants to dispense with print. the acs would be happy to reduce its price increases, possibly even hold them flat, or even provide a small reduction during a period when the print versions are being eliminated as a way of returning those costs to the community. the concern is that in very short order, with the rising volume of publication, the costs of handling those many more articles will in fact wipe out whatever transitory gains there may be from saving on print. bernard rous, association for computing machinery (acm) reasons why the costs of electronic publishing are poorly understood the costs of electronic publishing are not really well understood at all, and there are some very good reasons why this is the case. first, as has already been mentioned, electronic publishing is not a single activity. one can take pdf files created by an author and mount them on a web server with a simple index, and that is electronic publishing of a sort. or one can manage a rigorous online peerreview tracking system, convert multiple submission formats to a single structureddocument standard, do some rigorous editing, digitally typeset and compose online page formats, apply style specifications to generate web displays with rich metadata supporting sophisticated functions, and build links to related works and associated data sets integrated with multimedia presentations and applets that let the user interact and manipulate the data. this too is electronic publishing. it is miles apart and many decimal points away from the first approach. second, electronic publishing costs remain fuzzy because we are still living in a bimodal publishing world. even some direct expenses arguably can be charged to either print or electronic cost centers. where a publisher puts them often depends on the conceptual model of the publishing enterprise. on the one hand, if the publisher looks at online offerings as an incremental addon to the print version, then more costs will be charged to the print version. on the other hand, if the publisher considers the print version to be a secondary derivative of a core electronic publishing process, more costs are likely to be charged to the digital side. when it comes to the indirect costs of staff and overhead, the same considerations apply, with perhaps even greater leeway because of the guesswork that is involved in these types of cost allocations. third, the decisions to charge costs to a print or to an electronic publication are part of a political process. there are times when you want to isolate and protect an existing and stable print business, so you attribute any and all new costs to the digital side. you may also want to minimize positive margins on the digital side to avoid debate over the pricing of electronic products. at other times, the desire to show that the online baby has taken wings, is selfsustaining, and has a robust future can tilt all debatable charges to the print side. this is not to say that the books are being cooked. it is just the way you look at the business that you are running. fourth, it is very difficult to compare print and electronic costs, because the products themselves are not the same. the traditional average cost per printed copy produced is meaningless and very hard to compare to the costs of building, maintaining, operating, and evolving a digital resource as a single facility. fifth, accounting systems sometimes evolve more slowly than shifts in publishing process. new costs appropriate to online publications are sometimes dumped into preexisting print line items. sixth, electronic publishing has not reached a steady state by any means. there is still lots of development going on, some of which lowers costs, and some of which raises them. these are some of the reasons why the costs of electronic publishing remain somewhat obscure and also why a study of those costs would be both very difficult to carry out and very important to attempt. 14electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. unanticipated costs of electronic publishing it is also instructive to mention several components of electronic publishing costs that were not fully anticipated by acm when they went online. first of all, customer support costs have been phenomenally different from the print paradigm. not only is there a larger volume and variety of customer complaints and requests for change, but the level of knowledge and the expertise required to answer them is much more expensive. the cost of sales is higher. the product is different, and the market is shifting. acm no longer sells title subscriptions but rather licenses access to a digital resource. the high price tag for a global corporate license or for a large consortium means that more personal contact is required to make the sale. furthermore, such licenses are not simply sold. they are negotiated, sometimes with governments, and this requires much more expensive sales personnel. digital services are also built on top of good metadata, and metadata costs are high. the richer the metadata, the higher the costs. subject classification is costly as well. the application of taxonomies is a powerful tool in organizing online knowledge. the costs for building what has been referred to as the ﬁsemantic webﬂ are still largely unknown. there are a surprising number of opportunity costs. there are so many new features, services, and ways of visualizing data and communicating knowledge. there is a lot more work that can be done than has been done so far. finally, some upfront, onetime investments in electronic publishing turn out to be recurring costs, and some recur with alarming frequency. for example, acm is in the middle of its fourth digital library interface release since 1997. gordon tibbitts, blackwell publishing, usa is there a costtipping point in ejournals, that is, the place where all of a sudden the costs will dramatically drop, and there will be a new golden era of electronic publishing? there are two classes of commercial publishers that need to be considered in this context. one category of publishers provides publishing services for professional societies. the other type of publisher owns most of the actual material they publish. figure 31 compares the current print plus electronic publishing costs for these two types of forprofit publishers. the major difference between the two is that societyoriented publishers pay a lot more royaltiesšprofit shares, royalties, stipendsšback to the societies, who are the gatekeepers for that peerreviewed information. the other publishers keep most of their profits. 15electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. figure 31. current print + ecosts of forprofit publishers (% of sale) there is another point worth mentioning, an issue of scale. in the marketing and sales statistics in figure 31, there is about a four point difference between the two types of publishers, but the mostly owned publishers have billions of dollars in revenues, so those four percentage points translate into massive marketing dollars. figure 32 presents the current electronic journal cost components. the pie chart on the left looks at ﬁeincremental costs,ﬂ or those things that could be separated out as being purely new costs that the business is incurring that are electronic. many of the societies have a fundamental bylaw in their charter to disseminate information, sometimes for free. nonetheless, there is a cost to that, which is quantified under ﬁmarketingﬂ at about 7 percent. 16electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. figure 32. where is the cost tipping point? there is consortia selling, a brandnew thing. publishers have to negotiate with governments and large institutions, and that costs quite a bit of moneyšperhaps 13 percent of the eincremental costsšto have large consortia selling for the journal. the biggest cost in figure 32 is content management. this includes many people whose job is to get the information, organize it, and disseminate it. this new dissemination can be through web sites, palm pilots, libraries, and online systems. of course, this is not new; this is the information business. in an information business there are three primary costs for developing and maintaining information and for operations. every one of the new eincremental costs has those components. you do not build something once. you go through versions 1, 2, 3, 4, and 10. microsoft knows that very well; they keep upgrading their versions. there is an ongoing requirement to pay licenses, and ultimately humans are needed to actually operate these systems. figure 33 provides some diagrams of relative print (p) and electronic journal (e) costs. the venn diagram on the far left shows there is an intersection between p and e costs. there are some costs that are purely print or electronic. perhaps, since there is a lot of intersection of costs, there will be a collapsing of the cost structure of the electronic journal, resulting in a small p (small cost in print) and a small e (a small cost in electronic). 17electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. ppppeeeeprint journal ejournalshared costs figure 33. schematic diagrams of scenarios of relative print (p) and electronic (e) journal costs more likely, the current situation has a large print cost and a small electronic cost. or, as bernard rous put it, you could play accounting games, and you could actually go with a little p and a big e depending on what you are trying to accomplish on any particular day. unfortunately, gordon tibbitts thinks that a big p and a big e are the most likely outcome. some trendsšboth positive and negativešaffecting cost include the number of articles, supplementary data, back issues, customer support, new online features, speed requirements, archive/repository alternatives, and new business entrants. technology trends are driving costs down, for example, microsoft office 03™, word 11, and pdf+. trends are down in individual subscribers in favor of consortia, the amount of money everyone has to spend, and perhaps in flagrant pricing. there are rapidly rising institutional prices responding to the shift to online publishing, the drops in individual subscribers, and in ﬁacknowledgedﬂ business risks. the complexity of layout, graphics, and linking increase cost and the need for speed, as do the breadth of content and quality requirements. rather than a costtipping point, there is a costshifting point. it is shifting away from print toward electronic. we will see a lessening in the big p over time, but a big e will take its place. there will not be any savings. if we are to achieve any cost savings, we have to stop trying to invent the moon landing. we do not need very complex systems. some of the information just needs to get disseminated. some technologies are simpler and more widespread, and we probably should embrace them to keep costs in check. reducing the number of new versions of the same basic application is another way to control costs. finally, and most importantly, when many publishers get together and share standards and archiving, and collaborate, that is certainly a very good way to reduce costs across the board. 18electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. discussion of issues what would a study of journal cost accomplish? robert bovenschulte began the discussion by noting that doing studies of electronic publishing costs has major definitional and methodological challenges, although that is not to say they cannot be done. if such a study were done perfectly with perfect information, what would one then do with the study? michael keller said that the answer would depend on the position of the user. as a chief librarian responsible for managing a fairly substantial acquisitions budget and for serving many different disciplines, he would hope that such a study would affect the responses of the library community as a whole to these problems. the responses so far have been so fragmentary that librarians have not influenced the situation as effectively as they might. from the perspective of an internet service provider, that kind of study would help in the understanding of the comparative benefits or detriments of the various methods and operations, and cause internet service providers to become more critical in some ways than perhaps they are at the present time. as a publisher of books for stanford university press, mr. keller hopes constantly for some magic bullet that would help him take control of those costs, to better attain a selfsustaining posture. he thinks, furthermore, that it is very important for the research community to have a common understanding of what the problem set is. he would expect them, therefore, to elevate the level of discourse to something a lot more scientific than it has been to date. moving from print to electronic versions bernard rous noted that in several of the panel comments, it was implied that if only the publishers could get rid of print, there would be a huge savings in the publishing system. although there is some truth in that, it is also true that although the publications of the acm are rather inexpensive, there is a margin in the print publishing side of the business that the society cannot afford to do without. as such, there is not only a demand for print versions from some customers, libraries, and some users, but also from the publishers themselves, since the profit margins that are realized from the print side are actually necessary for them to continue in business. michael keller responded that in observing the informationseeking and using behaviors of his 15year old daughter and her cohort, he has seen an almost total ability to ignore print resources. although they certainly read novels and sometimes popular magazines, when they are writing a paper, most of them use the internet entirely. regular features of these papers now include media clips, sounds, and various graphics. it will only be 10 years before that cohort is in graduate school. another five years after that, some of them will be assistant professors heading for tenure. the students at stanford are already driving the faculty in the same direction, and a great many of the faculty have figured it out as well. mr. keller agreed completely with kent anderson™s point that, for a variety of reasons, there are hosts of readers in some communities for whom online reading and searching are not presently good options. yet for many research communities, especially in the basic sciences that advance rapidlyšsuch as physics, chemistry, mathematics, biology, and geologyšthere is real promise for removing the print version of journals altogether. that does not necessarily mean, however, that there will be a total removal of those paper, printing, and binding costs. robert bovenschulte noted that the acs adopted a policy five years ago of trying to encourage its members to stop subscribing to individual print journals, and instead to obtain them online from their institution™s subscription. the acs creates a significant pricing differential between print and web subscriptions, the electronic ones obviously being much cheaper. about three years ago the acs created a new alternative model to print plus web cost, which was web cost only, with print being a lowcost option. this approach was pioneered by academic press. this model has a lot of merit, because it makes publishers think about what the true costs are and what margin they need to sustain the business 19electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. economically, and not get caught up with the fact that the reporting system tends to indicate that the print version is profitable. michael keller added that the american society for biochemistry and molecular biology (asbmb), when it first came out with the journal of biological chemistry (jbc) online, selected two separate models, one based on the price for the print subscription, and a separate lower price for the electroniconly version on the grounds that they did not have to print on paper. they were trying to encourage institutional subscribers in particular to migrate over to that electroniconly version. floyd bloom next raised a question from a web listener, who asked, why if the british medical journal (bmj) does everything that the new england journal of medicine (nejm) does, and is available to the public without charge, can't the nejm be made similarly available? kent anderson responded that the bmj sees the future in a different way than the nejm. the nejm provides a lot of free access for people who truly cannot afford it, people in 130 developing countries, and its research articles are free upon registration after six months. nevertheless, he thinks the bmj experiment is worth watching. they have a national medical society that has the means of supporting their publishing efforts, so they are able to conduct this experiment. the nejm has chosen a path that relies on knowing that it is publishing information that people find valuable enough to pay for. the massachusetts medical society has 17,000 members in massachusetts, but the circulation of the journal in print is between 230,000 and 240,000. the exact readership is hard to estimate, however. mr. anderson thinks it is somewhere between 500,000 and 1 million readers per week. dr. bloom added that the british medical association, the publisher of the bmj, has a required membership for anyone who wishes to practice medicine in the united kingdom. therefore, they have a huge subscriber base whether they sell it or not. they also have other publications for which they charge. digital archiving issues floyd bloom asked mike keller if the true digital archive was achieved and allowed us to diminish the reliance on print, what organization would be responsible for trying to come to some agreement as to what the taxonomy and terminology should be for that kind of archiving? mr. keller responded by reiterating that the support for the publishing industry does not come from individual subscribers, who receive their copies, whether online or in print, basically at the marginal cost of providing them. the real support for the publishing industry comes from institutional subscriptions from libraries and labs. it is the libraries and labs that are most unwilling at this time to give up paper, because no one has a reliable way to store bits and bytes over many years and over many different changes of operating systems, applications, data formats, and the like. when this problem is resolved, the libraries will be challenging the publishing industry to get rid of paper, or at least not deliver those costs to them. so, who is going to do it? there are efforts underway in europe. for the past couple of years, the library of congress has managed a big planning program, the national digital information infrastructure and preservation program, with sustained congressional funding to support several different experiments around the country. there also is funding from the national science foundation (nsf) for the digital library initiative, and some of that money has gone to projects like lockss. finally, of course, there are industry segments that are quite interested in this. there are suppliers of content management systems, metadata management systems, and magnetic memory anxious to get involved. in the next year, there will be some initial efforts to demonstrate the viability of these. we are going to have to simulate the passage of time and of operating systems and so forth, but the reliability of digital archiving can be improved. within five years we will see general acceptance of some methods. mr. keller does not think that there will be a single system, but rather an articulated system with many different approaches. bruce mchenry, discussion systems, noted that all of the panel members have publications where peer review is the crucial distinction from simply having authors publish to the web themselves. he asked each panelist to describe the unique parts of their process, the best and the worst things about it, and how they would like to see it improved. kent anderson said that the peerreview process at the new england journal of medicine is rigorous. first, internal editors review paper submissions as they come in and judge them for interest, novelty, and completeness. if they move on from there, they go out to two to six external peer reviewers. 20electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. when those reviews come back, they are used to judge whether the paper will move forward from there. if it does, it is brought before a panel of associate editors, deputy editors, and the senior editors for discussion, where the paper is explained, questions are asked, and it is judged by that group, usually during a very rigorous discussion. then it is decided whether it moves on or not. if it does move on, it goes through a statistical review and a technical review. the queries are brought to the author, who must answer them or the paper does not move forward. the peerreview process lasts anywhere from a few weeks to a few years, depending on the requirements. sometimes the nejm asks the authors to either complete experiments or to give additional data. as far as weaknesses in the peerreview process, or ways it could be improved, mr. anderson thinks that one of the concerns now is that the time pressures in medicine are so great that finding willing peer reviewers is increasingly difficult. the situation could be improved by having the academic community understand the value of this interaction in the scientific and medical publishing process, and by having some sort of reflection of that in academia as a reward system so that people continue to devote the time they need to the peerreview process. dr. bloom added that as the number of submissions has risen, the number of people available to provide dependable reviews of those articles has not increased. so publishers are calling upon the same people time and time again to provide this difficult but largely unpaid service. robert bovenschulte said there is a lot of commonality at the acs with what kent anderson described, except that acs publishes 31 journals. the editorinchief of each journal has some number, in some cases a large number, of associate editors. these are typically academicians scattered around the world in 186 offices. they basically handle anywhere from 250 to 500 manuscripts a year. this is an incredible load, and they do not have the second level that kent anderson described, where the editors meet and decide. that is done by an associate editor under the general guidance and policies of the editorinchief. the rejection rate is about 35 percent, although it varies from journal to journal. as more articles are submitted, there is additional work and costs. new associate editors and new offices need to be added in order to handle that load, even if the rejection rate goes up. finally, there is agreement with kent anderson™s observation that it is harder and harder to persuade people to serve as editorsinchief, associate editors, reviewers, or members of editorial advisory boards. they simply do not have the time. the best people are in enormous demand. the web helps and has the benefit of speeding everything up. but there is the potential for a meltdown of the system in the next five years or so, particularly for those journals that are not seen as quite as essential to the community as others. bernard rous next pointed out that the acm publishes several genres of technical material, and each receives a different type of peer review. the review of technical newsletters is generally done by the editorinchief. conference proceedings in computing are extremely important, and there is a very different review process for them. there is usually a program committee, and each paper is accepted or rejected upon review by any number of people on that committee, depending on the conference. the rejection rates for certain highprofile conferences are even higher than in some of the equivalent journals in the same subject area. there is no author revision cycle in the peerreview process for conference proceedings, and there is no editorial task. the acm also publishes magazines, where the material is actually solicited. there are very few submissions that come in over the transom, so there is a different review process there, and the emphasis is not on originality of content. the material is being written for very wide audiences, so there are different editorial standards. there is a heavy rewriting of these articles in order to achieve that, and that can be very expensive. finally, the acm has about 25 journals that follow the typical journal review process, rigorous and independent review, generally using reviewers, with an author revision cycle and edit pass. gordon tibbitts noted that blackwell publishes 648 journals and the primary component of all of them is they are peer reviewed. blackwell interrelates the societies because it has so many; it constantly commingles boards and introduces the editors of journals to one another, so that there is a good crosssection of board membership, not just nationally but all around the world. that is an added value that blackwell brings to the process. blackwell also has spent considerable effort in automating the electronic office. the publisher has partnered with several other firms that work with electronic editorial office management systems, 21electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. which has really facilitated a better, faster peerreview process. the final important characteristic is that blackwell pays its editors. the publisher has been pressured quite heavily for many years now, and it has given into that pressure. there is a need to pay editorial stipends to the people who are working hard on the journals. it does drive profits down, but it also retains the editors. many of these people have five or six jobs alreadyšas chairs of medical institutions, authors, practicing surgeons, and professorsšand it provides them with some extra funding so that they can hire support staff and things like that. ebooks different from ejournals dr. mohamad alubaydli, from the national center for biotechnology information (ncbi), asked why electronic books are more expensive than books in print if electronic journals are cheaper to produce than the print version. is there a difference in the cost or is there a marketing issue? gordon tibbitts responded that although journals and books are similar, they are different product lines. the unit cost in books, and elasticity of that demand, determines the pricing. there are more people sharing ebooks freely, even though they should not be. the past three years have seen eight bankruptcies in large organizations that have tried to make money publishing books online. so, it is a very dangerous business area, and people do charge more for the online versions because they realize they will get fewer unit sales. michael keller added that customers might be charged more for an electronic book because it can do more things, such as better searching, moving pictures, or other functions. there are other business models for ebooks, however, that include reading and searching for free, paying on a page rate per download or printing, or paying a monthly charge of $20 or so to get access to tens of thousands of books. it is an industry in development and different from the science journal business. difficulties in the transition from paper to epublishing john gardenier, a retired statistician from the national institutes of health (nih), commented that the electronic medium is going to require a major reconceptualization of the entire process of demand, supply, process, technology, and so forth. one of the most significant changes has been the transition from seeking copyright protection for the expression of facts and ideas, to seeking protection and ownership of the underlying facts themselves. this became a very serious issue in 1996, with the enactment in the european union of the directive on the legal protection of databases. it has been a continuing battle ever since in the united states. if the flow of scientific information is restricted either by cost or by legalities, the capability of a society to continue to generate technological innovation diminishes, and, therefore, it becomes disadvantaged relative to what it was before, and possibly to other societies. the reason this pertains to this session is that it is going to take new sources of money and new sources of ideas in order for the notforprofits at least, and perhaps the forprofits as well, to complete the transition to electronic publishing, while maintaining academic freedom and innovation capability. a lot of that money can come from sources that have not yet begun to be tapped. the nsf was mentioned but industry has a huge stake in this as well, not just manufacturing but service industries, information industries, and so forth. we, as a community, need to do a better job of outreach to industry, and having them help us to resolve these problems. michael keller responded with two points. first, the stm publishing community in general has gone a long way in its transition to electronic publishing. second, a stable position cannot be achieved. there will continue to be changes in delivery, in possibilities for new kinds of reports, and in distribution that will mean a great deal to the scholarly community. these publishers already have been quite innovative and have moved a long way from where they were 10 or 20 years ago. 22electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. costcontainment strategies fred friend, representing the joint information systems committee in the united kingdom, said there was a consistent message from the publishers of the panel that their costs are increasing. what was not discussed was any strategy for dealing with that situation apart from just a few positive comments by gordon tibbitts. if the only strategy is to increase the subscriptions paid for by the libraries and laboratories, then there is no future in that. if the publishers are to survive, they must have a strategy for coping with increasing costs. bernard rous responded that the panelists may have created a misimpression since they were addressing costs somewhat in isolation. costs need to be taken into account together with access and with pricing. he believes that the cost per person of accessing the body of research has plummeted dramatically in the electronic context, even with the increasing costs. robert bovenschulte added that the system is the solution and that we are beginning to see real economies now. he documented some of these in his opening remarks, such as using technology to make staff more productive or dealing with fewer staff to do an increased amount of work. unless publishers continue to drive the technology to help them be more efficient, however, they will not get out of the box in which they are in danger of being trapped. gordon tibbitts thought that publishers probably should not be in the archiving business. libraries are probably better positioned to do that. publishers are innovating in ways that certainly are creative and are making them money, but in some cases their innovations probably should be shared, rather than remaining proprietary. there are many expenses publishers incur because they are still thinking in terms of printed copy and subscriptions, and not thinking of products. when publishers realize that their valueadded might be in the branding of the society, their assistance to the peerreview process, and the move away from some of these technological innovations, the costs will go down, and they can then charge less. the stm publishers are still in the infancy of learning who they are. there are many different ways of collaborating that can bring costs down dramatically. for example, open access and shareware of various types of search engines and online systems. there is no reason why publishers have to have proprietary online systems. those are the kinds of things that if shared collectivelyšwith funds from grant organizations, governments, publishers, and societiesšcan build a better environment. publishers do have value. they have value in their ability to facilitate the scientific process. they are collectively connected to more of the distribution channels, so there are economies of scale. and they add a lot of value in funding and innovating. blackwell and other publishers invest in projects that do not have a payback for 15 years, or perhaps ever. new societies are born, new science is funded, and that is something publishers add to the community. michael keller said that his solution is very different. libraries should not necessarily support all scholarly communication efforts that are brought to them. he thinks it is irresponsible for libraries to subscribe to the big package deals in which fully 55 percent of the hits in those deals come from 15 percent of the titles. the small societies that want to publish should do so and should be supported in that. the incredibly elaborate overhead that a lot of publishing now entails is unnecessary. the cornell arxiv and similar efforts provide one possible solution to this problem. it would be fine if 50 percent of all the scientific journals that are published today disappeared. those articles that have been through a cascade of rejections from journals could go into preprint archives, and receive recognition later by the number of times they are cited. reducing the flow of formal publications in half would be the goal. kent anderson observed that there is some risk involved in this. it is difficult for a publisher to know whether it is adding enough value to make a difference so that the investments will pay off. there may be some strategies for containing these new costs and bringing them in line, but at the nejm their hope is that they are adding enough value so that, over time, the small risks that they are taking in multiple channels will pay off for them. 23electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. vulnerability of secondary and tertiary publishers jill o'neill, with the national federation of abstracting and indexing services, noted that during the course of michael keller™s presentation, he suggested that secondary and tertiary publishers are perhaps in a more vulnerable position just by virtue of what they do. yet, at the same time, he expressed his concern about the next generation of scientists not being well enough equipped to know where to go for research resources that are outside the internet publishing environment. is the problem that the secondary services do not add enough value, in which case they have to focus more on information competency to teach the younger researchers how to do those appropriate searches, or do they somehow need to develop better mechanisms for searching and retrieving the precise answer from this huge corpus of literature? michael keller responded that he is supportive of secondary and tertiary publishing and has himself contributed to secondary publishing. he believes that the secondary publishers in particular are vulnerable, because they are being overtaken by the broad general search engines and by the assembling of peertopeer kinds of understandings about what is being usedšthe sort of feature and function you get when you go to amazon.com to buy a book. you select the book you want to buy, put it in your cart, and go to check out, where they inform you that there are six more books that others who have bought this book have found very interesting. that sort of function could overtake the secondary publishers. the secondary publishers have to find some ways of being more effective, more precise, but also more general. they have to look for many different approaches, and some of this must be done automatically. however, if we end up with a highly distributed and diffuse situation in which authors place their contributions on individual servers, then there will be a huge role for secondary publishing. bernard rous added that he thinks the secondary publishers, as they are now, are vulnerable, but not because secondary publishing is vulnerable. what is currently happening is that primary publishers, in order to go online, have to create secondary services. they are in the business of collecting all of the metadata that they can, and presenting those as a secondary service that lies on top of fulltext archives. secondary publishing is merging with primary publishing, especially in the aggregator business. electronic publishing by small and midsized societies ed barnas, journals manager for cambridge university press, noted that the panel is representative basically of big publishers, and there are certain economies of scale in production and technologies in developing their systems. he asked the panelists to address the question of costs in converting from print to electronic by the midsized societies that publish their own journals, or by midsized publishers, because that is not an inconsiderable concern on their part. gordon tibbitts responded first that with innovations from commercial vendors, such as adobe and microsoft, and simple services, such as etypesetting in china, india, and other places, the costs for both small and large publishers alike will be lower. he also advocated the use of simpler, standard formats that all publishers agree to adhere to, and removing the barriers of very complex web sites, which are out of the reach of even publishers the size of blackwell. kent anderson offered the thought, just to be provocative, that it may be even riskier for a midsized society to stop delivering print. the situation there may be harder to change, because it is perceived by members to be such a strong benefit. having completed two terms as chair of the publications committee for the society for neuroscience, a 30,000member society, floyd bloom noted that, in 1997, the decision was made by the society™s president that online publishing was the way to go, and they were one of the early journals to join the highwire group. within 2 years the individual subscribing base went down by 93 percent, but the readership went up nearly 250 percent, and members all over the world were thrilled to be able to have their information just as soon as those members residing in the united states had theirs. the society recognized, however, that the cost of doing so was considerably more than an initial onetime cost. therefore, a segment of the dues, at least for the present time, has been earmarked specifically for the 24electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. support of the electronic conversion. dr. bloom then asked if there was anyone in the audience who wanted to comment for a 3,000to10,000membersize society. alan kraut, with the american psychological society (a 13,000member society, but a new organization, only 10 years old), said that they published a couple of journals. over the course of their brief lifetime they have several times tested in quite elaborate detail whether they could publish their own journals online, and recently have decided they cannot. the risks, particularly for a small organization with modest reserves, simply cannot be taken. they remain a very satisfied partner with blackwell publishing. martin frank, with the american physiological society (about 11,000 members), noted that they publish 14 scientific journals. none of those circulations comes close to what the new england journal of medicine does. the society has been publishing electronically for about 10 years, beginning on a gopher server back in 1993. the reason small societies like the american physiological society can manage to publish electronically is because of the compounding effects of information exchange that occurs at the meetings that they have with highwire press. technologically, he does not think the society could have done it alone. the fact that there are partners in the nonprofit stm publishing sector that come together with highwire has made it possible for them to publish all their journals electronically. donald king, of the university of pittsburgh, commented that if you look at the overall publishing cost, and divide it by the number of scientists, the publishing costs seem to be going down. if you look at it from the standpoint of the amount of reading, it is going down even more, because scientists seem to be reading more on a perperson basis. one of the concerns he has is that publishing has traditionally been a business in which some portions of each publishing enterprise subsidize other portions. that is, in any given journal there will be some articles that are not read very much. in other cases, there may be very high quality journals that only have a small audience. and, within a publisher™s suite of journals, some of those journals are being subsidized by other journals. when looking at the future system, there needs to be a recognition that there may only be a small audience for most articles, but the fixed costs for all articles are going to be the same. there is a need to be able to continue to satisfy the demand for all of them. the costs of technological enhancements pat molholt, columbia university, noted that all of the panelists have said that publisher it costs are rising, and she questioned whether they have to do that across the board. why not have both a vanilla version that may in fact satisfy many people, as well as an enhanced version for those people who need an animated graphic or some other added functions and for which they would pay some additional fee? the technology then can indeed be developed collaboratively, so that there is a standard plugin when that is needed, and if it becomes important to the article being read, it can be invoked, but it would not have to be paid for all the time. robert bovenschulte responded that this is an excellent point, and it is one of the questions that the acs has wrestled with over the past four or five years in its strategic planningšquestions such as how many innovations are needed and what costs should be introduced. he has seen some anecdotal research suggesting that what the scientist who is reading really values is the search capability, rapid access, and linking. regarding the linking function through crossreferencing, there is now an industrywide, precompetitive endeavor that has great value. the various technological enhancements can have value, however, for individual communities and for editors. there is pressure on the publishers to do something to compete with what other publishers have done. the publishers are competing less for the library dollars than they are for the best authors. of course, that applies to editors and reviewers as well. on the other hand, many of the recent innovations that tend to drive up the it costs are used very little and are not of great value. however, it is very hard to predict which innovations will prove valuable in advance. moreover, even an innovation that is not much used today may turn out to be one that is very valuable 5 or 10 years from now. so, the publishers are all experimenting and trying to stay on the learning curve. if you get off the learning curve as a publisher, you are in deep trouble competitively. kent anderson added that the nejm consistently provides new technological upgrades or services in response to its readers™ comments, and, in some cases, based on a little bit of guesswork. 25electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. although some of these new technological capabilities are not highly used, more often than not they take off and have thousands of users. then the journal has to support the application, iterate and improve it, and find out what it indicates about what other needs the customers may have. he has been surprised at how much demand there has been for new applications. michael keller pointed out that when hyperlinking began in 1995, no one had done it before, and now it has become an absolute standard. the same is true about inserting java scripts showing the operation of ribosomes with commonly available plugins. people began to use them. some of these technologies that are developed on the margin eventually become quite popular. bernard rous said that it is difficult to decide which innovations to pursue and which to leave, but the suggestion that there be differential service levels also presents problems. the acm wanted to maintain a benefit for membership in the organization, as opposed to institutional subscriptions, so the society started differentiating levels of service, depending on whether someone was a member or a patron of an institution that licensed an acm journal. but differentiating service levels adds real cost and a lot of complexity to the system. it also adds a lot of complexity for the end user, and a lot of trouble for the society to explain the differences to the different users. finally, lenne miller from the endocrine society pointed out that while most of the participants in this session are publishers, they all have different business circumstances. for example, the endocrine society has four journals and 10,000 members. the society is rather tied to its print clinical journal because of the revenue generated by the $2 million per year of pharmaceutical advertising in it. however, the endocrine society does not get nearly as much revenue from its annual meeting as, for example, the society for neuroscience does. so, there are different business models and different business circumstances and these must be kept in mind. 26electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 4. publication business models and revenue introductory comments jeffrey mackiemason, university of michigan scholarly publishing in any medium requires substantial resources beyond the content creation costs. in the early 1990s, when digital publishing started to become a serious possibility, there was some confusion about this in some corners of the scientific community. there was a sense that publishers did not really do anything. everything was done by authors, and publishers were valueless middle people who distributed the material to readers. as soon as the web emerged in 1994, it began to be thought that the publishers could disappear. that, of course, simply is not true. publishers are providers of valueadded products and services. they perform some very crucial functions by producing documents in different media, doing the editorial and design work, marketing the material, and getting readers connected to writers, and so forth. all of those functions involve costs. even a notforprofit publisher has to recover its costs and have some sort of reserve. the forprofit firms need to get a return on their investment. this session, therefore, is devoted to a discussion of sources and types of revenue, ways of raising revenue, and different business models, particularly in a world where digital publishing is becoming much more the norm. the defining question is: how does a publisher organize delivery and rights management across modes of access so as to recover production costs and induce ongoing investment in development? as discussed in the first panel, new technologies are emerging all the time; new forms of delivery, new forms of access, and new types of services are being provided. that requires an ongoing stream of investment for things that once were thought of as onetime costs. what are the sources of value? revenue is simply a transfer of value between parties. to address a business model, particularly to start thinking outside the box and think about changing frameworks, we need to think about who values the publications or the information delivery in the first place. who might be willing to share some of that value back to the publishers and the distributors?šthe teachers, public and private researchers, students, practitioners, general public, and so forth. another question is, where does the publisher collect that value? what is the best way to organize the extraction of value from some of the stakeholders and transfer it to some of the providers and distributors? is it through the universities? we heard that the traditional publishers are focusing increasingly on library subscription models as their primary mechanism of collecting revenues. in some cases it is from the practitioners, for instance the medical practitioners who subscribe to the new england journal of medicine. there has also been much discussion about the delivery of scientific materials to developing countries, and a question about whether or not those should be delivered free as part of an expansion of access to knowledge around the world or whether developing countries should be contributing as well in some fashion. who are the providers and who are the potential toll collectors? there are a number of different services that go into the publishing process, and the publisher does not necessarily provide all of them. are those services going to be provided as a bundle by a single service provider, or will there be multiple providers providing overlapping services who will need to recover their costs, and provide different aspects of access to information content? the business models depend on how the information is accessed and what the information is. again, in a digital world, we no longer need a single standard mode, a journal. we can think about breaking up the information in many different ways, and repackaging and distributing it in different combinations. this often is referred to as the unbundling and rebundling phenomenon. the corpus of information can be broken apart into its constituent pieces, particularly if it has been nicely organized, with a good markup language and tagging system, and then repackaged in other ways. so, we can sell it by 27electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the view, by multiple views, or by time periods of viewing, among many other approaches. there also can be charges for printing and file usage, as opposed to merely viewing usage. you could allow people to keep local electronic copies for rapid access and local archiving, or there could only be server copies. it is a question of whether you can search the full text or the image only. bernard rous mentioned that this is an issue that the acm has faced as a membership organization. which services should it provide to the general public, and which enhanced services should it provide to its members in order to induce them to remain members? what is the role for government revenue? much of what we are talking aboutšscientific, technical, and medical information and scholarly researchšis information that benefits the general public either directly or indirectly, far beyond the community of scientists and scholars who are using it. there is a public interest in the dissemination of knowledge, in addition to its creation. of course, governments in most countries are one of the main, if not the main, sources of funding to create the knowledge in the first placešto fund the research. with some exceptions, however, they have not been one of the main providers or direct disseminators of that information. ultimately, at some level, governments are paying for it through subsidies to universities to pay for the libraries. but there is a question about whether the government should be directly intervening and providing access, or providing the revenues necessary to do the publishing. there is value in the content itself, as well as in the valueadded services that publishers, disseminators, aggregators, and distributors provide. there is thus a question about how different business models might succeed at supporting both aspects of the process, both content creation and distribution, getting incentives to generate the knowledge in the first place and getting reasonable incentives to provide highquality publication, dissemination, indexing, and abstracting services. having a particular business model that may address some of those needs, may not address others. finally, the overriding question for this entire symposiumšmuch less this sessionšis: what impact is the digital publishing world going to have on science itself, on the scientific enterprise? for instance, how are the different business models, the different ways that access is provided and structured, going to affect the quality and productivity of science, collaboration at a distance, access for developing countries, the professional review and career process, peer review, and other aspects of scientific research? comments by panel participants brian crawford, john wiley & sons john wiley & sons is a global, independent publisher, established in 1807. wiley has three major areas of publishing today: scientific, technical, and medical (stm) journals, higher education materials, and professional and trade information. it is a fairly diversified publishing portfolio, with about $1 billion in annual revenues. the publisher has 400 stm journals online on its interscience platform, which was established in 1997. it now contains about 2 million pages of information in the sciences, technology, and medicine. finding wiley™s content online wiley provides open access to its tables of content and abstracts, as a general principle. that is so that its information can be found easily on the web, either through secondary information services or directly by users who are browsing for such information and coming in directly to the service. each journal has its own home page. the reason for that business model is that wiley publishes on behalf of many learned societies. wiley wants even those journals that it owns to maintain their own distinct identity online. it also sees the advantage of a rich integration of links to and from related material. highwire certainly led the way with this, with its tollfree linking capability that they demonstrated with professional societies. other publishers quickly followed suit. the establishment of 28electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the crossreferencing service was quite visionary. and, of course, all publishers in the biomedical area benefit tremendously by the linkage provided by pubmed. so, the first aspect of the business model is being found on the web. content and usage statistics wiley has about 250,000 articles across those 400 journals, more than 100 of which are in biomedicine and linked to pubmed. it also publishes laboratory protocols on the web. it has about 12,000 contributions from major reference works that have been placed online. it also has been moving its books online, about 300 books per year, from a parallel path of print publication. wiley™s online platform has about 400,000 users who have chosen to register. registration is optional, and many users do not choose to identify themselves. the service gets about 3.6 million article or chapter views per month, and more than 12 million user sessions per year. the second message about wiley™s business model is that it wants very much to drive usage and to encourage pathways for that use. a customerpays model the next important aspect of wiley™s business model is that it is very much a customer or readerpays model of delivering the content. as a result, the publisher places a great emphasis on sales staff worldwidešstaff that it did not use in the conventional print environment, where it was marketing via direct promotion to scientists. libraries customarily took print subscriptions, which did not require sales representatives to meet with them to negotiate electronic licenses. it is an added cost consideration, but important in driving the kind of revenue model necessary to securing the financial success of electronic publishing. licensing considerations as wiley embarked upon the development of licenses for its electronic journals, it did so very much in consultation with its major institutional customers. this resulted in flexible sales options. it did not want to make a commitment to one preferred option early in its electronic publishing, so it provided a menu of print and online options. wiley now uses basic and enhanced access licenses. basic means titlebytitle access, with some concurrent user restrictions. this is an option that is often suitable for the smaller institution or department. its enhanced access license does not require that an institution take all wiley titles that are available electronically. the institution can choose, but it is establishing a license for a larger body of work, with no concurrent user restrictions and with additional benefits such as negotiated price caps. the publisher determined that individual and society member access privileges had to be instituted in parallel with its institutional sales models, because it also caters to an individual audience, unlike some other publishers who spoke in the first panel session. its individual member audience is actually holding steady, if not growing, particularly for some of the smaller societies it deals with. wiley has been licensing access to journals online since 1999, which has resulted in strong revenue growth. that came as a result of its sales effort worldwide, but also because of the receptivity of the customer base, primarily the institutional library base, to making that switch to electronic resources. this is not an eonly shift thoughšit is very much a parallel purchase situation. for the most part, institutions are choosing to retain their print subscriptions until there is a reliable, permanent digital archiving solution. 29electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. customer relationships the other part of the business model is an emphasis on direct relationships with customers. the business model has shifted away from selling solely through subscription agents as intermediaries. wiley now often deals directly with its customers. it views its customers as partners, rather than simply as targets. it uses librarians as a channel to end users. customer training is another mechanism to promote usage, something that all publishers need to do more. another important aspect is a focus on customer loyalty to sustain business growth. wiley also provides password access for its society members and individuals, recognizing the roaming nature of its professional customers. continuous enhancements finally, wiley constantly emphasizes investment in new features and enhancements. some of the most recent ones include content alerts to apprise its audience of what is being published; delivery of content to mobileedition platforms; and publishing online in advance of print publication. in order to move beyond a pure subscription model of access, wiley has developed a feature called article select. this is the sale of digital tokens to its licensed customers, enabling them to distribute those tokens to other users within their institutions to access content from journals that they do not subscribe to. the use of those tokens then can be examined to determine what journals might be added to that institution's collection on a subscription basis. this capability has now been expanded into a purer payperview offering. the big dealšissues in bundling licenses many notforprofit organizations are still in the early stages of offering bundled journal licenses to their institutional customers, whereas commercial publishers have undertaken much more aggressive licensing in recent years. this has caused some debate about this socalled big deal. wiley™s enhanced access licenses now include more than 1,000 institutional or consortia customers. that is not those who sign the license; it is those institutions that are part of such consortia. it constitutes about 70 percent of its elicense business. wiley™s basic access license customers indicated that there are many institutions that are not prepared to make the move to the big deal. more than 3,000 of its customers are still taking the titlebytitle electronic access option, which represents 30 percent of its elicense business. a quick review of the respective license terms is useful to help make the distinction between the business models. the basic access license tends to be to a single site, one geographically contiguous building or campus. there is activated eaccess for each subscribed title, and it is for one concurrent user per print subscription if the institution holds a print subscription. the enhanced access license tends more to be multisite, multinational, and suitable for large consortia, with unlimited concurrent users. there is access to all subscribed current content as well as back content, and privileges such as article select, roaming access, and monthly usage reports. wiley also offers license terms for individuals, what it calls a personal access license, providing individual subscribers with electronic access on a subscription basis; society member access, with members being given access to their subscriptions on terms on which they agree with their societies; and complimentary access to the publisher™s chief editors and members of its editorial boards. free access to developing countries wiley offers free access, free from the time of publication, to developing countries. for its biomedical journals, it does this through the health internetwork access to research initiative (hinari) program of the world health organization (who), in order to get content into the hands of investigators in parts of the world where they truly cannot afford such information. 30electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. wiley's participation in hinari, like that of other publishers, is viewed primarily as philanthropy, reaching out to provide information where it is very much needed in order to provide research support and medical care in struggling parts of the world, and not as way of seeding a market. the parts of the world that have been served initially by who through hinari are not seen as being markets in the near or even medium term. distinction between the paidaccess model and the authorpaid, openaccess model wiley has chosen to maintain the paidaccess model described above, and rejected what may be called the authorpaid model that is being advocated by proponents of more open access to the literature as an alternative. the reason it has not elected to go the latter route, and why most of the society partners that it publishes with are quite chary of that, is perhaps best articulated by professor hal varian, an economist at the university of california at berkeley. professor varian has pointed out that any economic system tends to favor the entity that pays. given that the bedrock of scientific communication is the peerreviewed literature for original work, wiley™s view is that any system that charges the author or a sponsor of the author in order for that author to be published is, in terms of the economic system, going to favor the author's desire to become published. there is nothing wrong with that, but one has to accept the consequences. what has evolved in scientific communication for the most part is a system where the reader pays, or an agent for the reader is paying, because that naturally introduces a selective filter that reassures any entity wanting to scrutinize the validity and the value of the work that it has passed muster. it does not mean that peer review cannot exist in and support a system where the author is paying. however, one can offer a provocative example of why this is incompatible with the sort of publishing that wiley does, certainly in biomedicine. that is, one never hears of anyone proposing that a pharmaceutical firm should fund the food and drug administration to evaluate a new drug for approval, because this would be incompatible with the system of checks and balances necessary to prevent bad drugs from getting out on the market. similarly, we have to be careful that bad science does not get out on the market. that does not mean that work that has been peer reviewed as a result of author support cannot be valid. it does mean, however, that the value of the information would be altered by such a mechanism. when we talk about value of information, we tend to debate how much the sale value of the information is worth, or about the cost of actually producing that information. in undertaking any study at the national academies that would look at the value of scientific information, however, one should not assess the value of information on the basis of the cost to produce it. instead, the value of the information is its value as a tool, as a productivity multiplier in society. one could argue that scientific information, when it is peer reviewed and selectively filtered, has tremendous value to the author, but also to society. that kind of selectively filtered information is that productivity multiplier. wiley, therefore, has very much supported a customerpays model for the business because the company believes it ultimately enhances the value of scientific information for those who should value it most. joseph esposito, sri consulting8 these are heady times for publishers. there is probably more interesting activity in the industry right now than at any time since the paperback revolution following world war ii. unlike the paperback revolution, however, which allowed publishers to grow enormously and to become highly profitable, the current state of affairs is often thought to present more challenges than opportunities for publishers. indeed, it is not uncommon to hear people say that publishers are a historical artifact, who will be disintermediated by the internet. or, we hear that copyright is robbery and that the model for scholarly communications in the future is the opensource software movement, which has given microsoft a challenger in the form of linux. when you look at some of the more intriguing projects organized around 8 joseph esposito™s current affiliation is portable ceo. 31electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. intellectual communities, such as the dspace project that is being developed by hewlettpackard at massachusetts institute of technology, even a commercial publisher has to concede that critics of traditional publishing may have a point. at the same time, commercial publishing employs a great number of people, and it is unlikely that they will all simply go away because a librarian in nebraska has learned to manage a web server. it may be useful to consider how publishers see their own activities evolving and what the environment for scholarly communications is likely to look like in 5 or 10 years. consolidation of publishers from a commercial publisher's point of view, the primary thing to be said about the academic research publishing market is that it is mature, which means that there is only modest growth in revenue, making it hard for new players to enter the market. there are conventional ways to deal with a mature market, and we should expect that publishers will try them all. we should expect, for example, to see the consolidation among publishers continue right up to the point of antitrust action. it is highly unlikely that any scientific discipline will have more than two or three information providers in the years ahead. some observers believe that there will only be two or three information providers for the entire stm market, covering all areas of study. however, this would result in fewer operating synergies across disciplines. the form that consolidation will take may change, however. in addition to the already familiar phenomenon of the big companies buying up the smaller ones, we should look for the smaller companies to link together in an attempt to become big companies. we also should expect to see big companies establish distribution arrangements with smaller companies by, for example, putting the content of the smaller companies onto the electronic platforms of the larger companies. this provides critical mass, which can be used to shoulder aside the offerings of organizations with a small number of publications. it is not necessary for reed elsevier to buy sage communications if they can cut a deal to deliver sage's publications on the elsevier platform, but the impact on the marketplace is the same. bundling a mature market will also intensify the downward pressure on prices. this is especially true in academic libraries, where the openaccess movement was created in part to put publishers under pricing pressure. publishers will likely counter this by targeting the market share of other publishers, rather than looking for significant increases in library budgets. in other words, this is essentially a struggle for market share. this is already happening. for example, reed elsevier is now offering libraries access to previously unsubscribed journals, not by charging for each journal separately, but by simply insisting in an increase in total expenditures over the prior year. previous speakers at this symposium referred to this practice as supersizing, or the big deal. in other contexts, this practice is generally called bundling or tying. bundling will have the effect of greatly increasing the number of reed elsevier publications available through particular libraries, at the expense of having less well positioned publishers lose those customers entirely. downstream value migration we also should expect commercial publishers to seek socalled downstream value migration and to target competitors that for various reasons are thought to be vulnerable. currently, publishers sit in the value chain between authors and wholesalers, and wholesalers have direct relationships with librarians. by moving downstream, more publishers will attempt to disintermediate the wholesalers and reap the wholesalers' marginal revenue. critical mass is necessary for this, which is another impetus to consolidation. disintermediation strategies that do not provide significant new value to end users are probably ineffective, but that does not mean publishers will not try it. 32electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. targeting vulnerable competitors what makes one competitor weak and another one strong is a matter of debate, but we are likely to see commercial publishers target the notforprofits more aggressively in the future. the reason for this is that, rightly or wrongly, the notforprofits are perceived to be slower to respond and less capable of defending themselves, though they often have substantial goodwill in the marketplace. there are many prospective targets, but one might name just two: the american chemical society (acs) and the online computer library center (oclc). the acs is a target because of its sheer size. the oclc became a target when it acquired the assets of netlibrary, making its strategic direction appear to be increasingly like that of a competitor. creation of metacontent metacontent is interesting in that it sidesteps the copyright issue. content about content can take the form of bibliographies, indexes, and, most important of all, search engines. publishers are keenly aware of openaccess publications and are looking for ways to make money from them. one way to do this is to create search engines for openaccess content. even better is to integrate openaccess content with proprietary content for search purposes, as this will tend to bring even users of open content to proprietary services because the breadth of materials included will be much greater. in effect, openaccess provides publishers with lower costs for content development even as it makes charging for access fees possible. from an economic point of view, copyright may not be necessary for supporting publishing profits. the shift to web services the real economic response to open access, however, is likely to be in the creation of web services, a dynamic substitute for the publication of fixed content in hard copy. in a web service, a publisher provides online software that manipulates or processes data that are uploaded to it by a user. the user creates the content and then pays the service provider for the online processing. think of what it would be like to do your taxes online and you will get the idea. copyright is irrelevant for models like this, but the economic potential is very great. diversification of customer base one final strategy, like many of the other ideas here, is already playing out in the marketplace. it is customer diversification. if the academic channel is mature, publishers will seek new sales channels. the most likely one, because of its size and creditworthiness, is sales to businesses. we should expect to see more publishers trying to modify their offerings for this channel. since businesses, unlike research universities, are profitoriented, the kinds of publications they are most likely to purchase speak to an immediate business need. this will tend to shift capital investment away from pure research publications toward applied research and engineering. looking out 5 to 10 years from now in the world of scholarly communications, we are likely to see an ample number of openaccess publications coexisting with proprietary ones, and we will witness ingenious publishing strategies designed to extract economic gain even in the absence of copyright. the toll collector may have switched positions, but he still has his hand out. wendy lougee, university of minnesota this presentation addresses journal costs and value, and the kind of tension that libraries feel as the intermediary between the publishers and the consumers of publications. these tensions may suggest 33electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. new models, new ways of looking at revenues. one aspect that has not yet been talked about, but that constitutes a behavioral constraint that should be taken into account, is the longstanding convention of the scholarly journal, going back three centuries. journals gradually accrued the function of providing a place to document discovery, to disseminate ideas, and also to codify prestige. that kind of threecentury convention is not going to be easy to change. responses of libraries to recent cost and marketing trends libraries are the intermediaries between two types of economies. they buy the content in a market economy, and users create and use content in a socalled gift economy. libraries are often caught in the middle. libraries have witnessed decades of dramatic journal price increases, with average annual increases of 8 or 9 percent for the past 5 years. it is a very inelastic market, in that as the prices have gone up, the libraries have not necessarily withdrawn or canceled the journal subscriptions. data over the 15 years ending in 2001 show that journal prices went up 215 percent, yet libraries canceled only about 5.1 percent of their subscriptions. the rates of increase are such that if you look at the higher education price index, the rate of journal price increases or library materials now outstrips utilities and health care costs. this is certainly cause for concern. michael keller earlier cited last year's report by morgan stanley on the stm market. it serves as a primer for investors, shedding light on stm publishing strategies. the report highlighted reed elsevier's profit margin of 37 percent for core titles and 22 percent for the medical titles that it recently acquired from harcourt. the morgan stanley report further discussed three trends worth future attention. first noted was the cyclical slowdown in industry growth due to research library budget cuts, coupled with a downturn in investments. a second trend noted the benefits of scale increasingly accruing to large players. it highlighted, in particular, the kind of bundled portfolio of journals that has come to be known as the big deal, as has already been mentioned. and, finally, it noted that the profit margins will expand for those publishers who have a successful online platform. interestingly, the report revealed that libraries spend an average of $1.50 on staff and operating costs for every dollar they spend on acquiring content. it speculated that the saving for libraries in moving to electroniconly would be somewhere on the order of 16 percent. the libraries have experienced a different range of models than publishers, some of which have been discussed at this symposium. these various models frequently come with a requirement that the library sustain its commitment to a certain level of spending, and they give the library very little predictability or control over costs. as the publishers are experiencing these cyclical effects, there is a tendency to stay the course with the expectation that the level of commitment by subscribers will remain stable. this past year we have seen two trends reflecting an interesting shift. the first is that the publisher strategy of the big dealšthe multiyear, alltitles approachšhas begun to unravel. we have seen a number of major libraries withdraw from that model, largely fueled by budget reductions and by the recognition that many titles in the bundle simply are not used. the libraries are beginning to push for much more finely tuned licensing models whereby they can select the content that they wantšnot only at the title level, but in terms of other kind of bundling, much like the tokens that brian crawford described. moreover, in the big deals, the libraries have encountered pricing that is very hard to defend to their user community. at the university of minnesota, the library worked with reed elsevier to move away from its bundled deal, but found it hard to explain and rationalize the end result: a reduction from 750 subscribed titles to 650 titles, and a move from two formatsšprint and electronicšto electronic only, resulted in a higher pertitle cost. a second disturbing event this year, which also has been alluded to by several speakers at this symposium, is the demise of some of the other intermediaries, the serial vendors. there was at least one major incident of a serial vendor going bankrupt, resulting in a potential loss of $73 million in library subscription payments. this raises the question of where the functions of procurement and distribution will go. these are functions that have to be sustained somewhere. the likelihood of increased revenues for libraries in the near term (particularly increases that match inflation in journal prices) is low. a recent informal survey conducted within the association of research libraries suggested that nearly half of respondents expected cuts in some areas and the prospects were high for further budget reductions in 34electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the coming fiscal year. library budgets, a major source of revenue for publishers, are obviously stressed. the volatility in the publisher marketplace will probably continue, as will the push from the library community for the more finely grained models that allow them to make some choices. implications of changes in journal format and content it is also important to look at the changes in format and content, how these changes might affect the existing service models, and the resulting costs for libraries. there are data indicating that much more intensive use is likely to come with electronic content. there are also recent studies of university users nationwide that have revealed an overwhelming preference for electronic format. nearly half of all faculty in most disciplines reported they use online materials for the majority of their work. in the biological and physical sciences and mathematics the percentage of information needs met through online resources exceeded 60 percent. yet interestingly, despite that preference, other studies looking at the perceptions of convenience and ease of use show a dramatic gap in terms of how the library performs in delivering that content. users are citing evidence of an inability to really control the content, to navigate it well, or to deal with the myriad different platforms and channels that it comes through. libraries increasingly are challenged to deal with these shortcomingsšthat is, they are challenged to develop systems to penetrate the socalled deep web and federate the content in a meaningful way. an experiment done by the university of california system looked at the differences between print and electronic use by putting the print versions in storage in order to force the use of the electronic versions and to see what would happen. they found that the relatively modest number of requests to retrieve the print versions from storage was by and large because the electronic versions failed to have some attribute or content that was solely in the print versions, or where the fidelity of some of the representation was not as strong. this result was similar to the data collected in the peak study at the university of michigan (in the late 1990s), which provided access to all the reed elsevier journals to 12 different institutions and found that 30 percent of the use was for information beyond the text of the articles. this was at a time when many of the publishers were not converting or indexing the whole journal. some of the major publishers still have been unable to migrate all aspects of a journal (e.g., the ﬁfront matterﬂ) into the electronic realm. these issues of journal integrity are important to understand, because they obviously impact how quickly we can migrate to that electroniconly nirvana. they are also important when we think about the kind of unbundling that might be possible and when we look at different parts of a journal and how they are presented. there is one last point about how content is changing. perhaps it is a subtle trend, but we are seeing a shift from our concept of publication as product to the notion of publication as process. there are a number of examples where online publications take on an ongoing, dynamic form as users interact with content and comment. for libraries, which are in the business of managing copyrighted, fixed works, that presents a real challenge. it is a challenge, too, to the stm publishing sector in terms of pricing, how to develop models that support something that is not fixed or well bounded. there is a marvelous book by anthropologist sharon traweek about going to live among the physicists. the analysis (called beamtimes and lifetimes9) sheds light on how the physics community migrated into the eprint form. it describes how the early, informal sharing of information (now represented in eprints) functions to document what is going on, as opposed to the formal publication that occurs months later, which is seen as a ledger to give credits and debits to fellow scientists. the changing role and influence of libraries as the divide between content and publication begins to occur, what is the role of the libraries? how do they manage and sustain formal publications and more informal processes? what influence do they have in affecting change with publishers, or with scholars and researchers for that matter? 9 traweek, sharon. 1992. beamtimes and lifetimes: the world of high energy physicists. harvard university press, cambridge, ma. 35electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the morgan stanley report suggests the potential for reduced operating costs for librariesšno journal checkin, no binding, no claiming lost issues. however, the counterargument might be that the necessary infrastructure to support the investment in that content, to federate it appropriately, to ensure its longevity, and to archive it is going to be an increased expenditure on the library side. what we are likely to see is a shift of any savings to supporting the additional infrastructure. it underscores how critical it is to have community attention to this issue of infrastructure, to interoperability, and the kinds of protocols that will allow that federation to happen. what influence might libraries have? it is rare that collective action is robust and sustained. a recent exception occurred a few years ago when nature proposed to embargo content for institutional subscribers. economist ted bergstrom has suggested an analogy with the economic construct of coordination games. he has a wonderful parable where he talks about the annual meeting of anarchists. they are used to going to the same hotel, but all of a sudden the hotel decides to raise its price, and they do not know how to coordinate action to change the venue for their meeting. like the anarchists, librarians and scholars may be stuck in the context where it is impossible to agree on or organize collective action. and, when they do attempt coordinated action, they are accused of collusion. libraries do have a role, however, in seeding and developing alternative, competitive publishing models. libraries have a fundamental understanding about content, its use, and users. there are a number of examples, of which highwire press is certainly one. there also is an increasing incidence of institutions, such as cornell and the university of michigan, starting incubator services, production services to help small publishers move to electronic publishing. these projects represent a move by the libraries away from their traditional role of providing access to information toward facilitating production of information, and it may help them reconceive the relative position they may have in the stm information sector. to summarize, libraries play an interesting role in the middle in a time of shifting costs and resource constraints. the nearterm focus for libraries is likely to fix on achieving a more finely tuned approach to acquisitions, while at the same time developing the necessary infrastructure for the future. patrick brown, stanford university the public library of science (plos) is a new scientific publisher based on an openaccess business model, sometimes also called a midwife model. bruce alberts and james duderstadt have already pointed out that scientific publication has traditionally been viewed as a public good by the scientists in universities, the sponsors of research, and the citizens whose tax dollars pay for much of the research that gets published. the goal of scientific publication from the perspective of these stakeholders has always been to make the results of scientific research as widely available and useable as possible. none of the important constituencies are benefited by limiting access or the freedom to use the published scientific information. this is the operating premise. reconceptualizing the stm publishing business model on the internet before the internet, the hard reality was that there was no choice but to charge the readers and users of scientific publications, because the most efficient way of distributing this information was by printing it on paper and delivering it in boats and trucks. every potential user thus represented an incremental expense for the publisher. so, of course, any business model that did not take that into account was doomed to lose money in proportion to the popularity and success of its publications. that system, however, had the unfortunate consequence of limiting distribution to individuals and institutions that were willing and able to pay, and this was accepted as a necessary evil. with the advent of the internet, that model is no longer a necessary evil. the traditional business model also had another indirect consequence that was more subtle, but equally unfortunate. it was based on selling research articles. this gave rise to the mindset that the content of scientific journals was valuable property that the publishers were somehow entitled to own, control, and sell for a profit. this is a really big psychological barrier that we now have to overcome. with the worldwide spread of the internet, something fundamental has changed about the economics of scientific publication, not just the technical means of distribution. there is an absolutely 36electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. categorical change in the economics that is wonderful, because it makes jefferson's ideal of the infinite, free dissemination of scientific ideas and discoveries a realistic possibility. what had been an impossible ideal in the preinternet erašto make the published information an open public resourcešis now possible because the cost to the publisher no longer scales with the number of copies produced or with the number of potential readers of a publication. that means that we are not restricted to a business model that charges per access or per copy. in fact, it means that a business model that restricts the distribution and use of the published work is really working against the interest of science and society. it is unnecessary, anachronistic, and inefficient. this business model now stands in the way of the ideal that we have been working for all along, and that is now potentially available to us. if we do not need to charge for access, then we should not charge for it. open access defined the plos will use an openaccess business model. an openaccess publication is one that meets the following two conditions. the first is that the copyright holder (either the author or the publisher, if the copyright has been transferred to the publisher) grants to the public a free, irrevocable, perpetual right of access to the work, and a license to copy, distribute, perform, and display the work and to make and distribute derivative works in any medium for any purpose. the second condition is that not only do they grant that license, but they actually make it possible for people to access the work. they do that by making a complete version of the article and all supplemental materials available in some suitable standard electronic format, deposited immediately upon publication in at least one internationally recognized, independent online repository that is committed to open access. one example of such an openaccess archive is pubmed central, maintained by the national library of medicine. the plos plans to make everything that it publishes immediately and freely available online, through its own web site, pubmed central, and any other distributor or library server that wishes to house and distribute its journals. under this definition, ﬁopen accessﬂ does not mean just some free peeks or downloads from the publisher's web site. it means that everything plos publishes can be downloaded in its entirety, manipulated, copied, redistributed, integrated with other data, and even resold by commercial publishers. the point is that the content is not the publisher's property. the publisher in this model plays the role of a midwife, an expert at delivery that gets paid for the service but does not expect to keep the baby. advantages of the openaccess model the advantages of this kind of true open access are already very familiar to everyone in the life sciences. they are certainly not theoretical. in the life sciences there are two longstanding, amazingly successful experiments in openaccess publication, genbank and the protein structure database. the success of the genome project, which is generally considered to be one of the great successes of recent times scientifically, and perhaps even its existence, is in no small part due to the fact that the world's entire library of published dna sequences has been an openaccess public resource for the past 20 years. if the sequences could be obtained only in the way that regular published work can be obtained, one article at a time under conditions set by the publisher, there would be no genome project. the incredible value of genome sequences would be enormously diminished. more significant is the fact that open access is available for every new sequence, and it can be compared to every other sequence that has ever been published. the fact that the entire body of sequences can be downloaded, manipulated by anyone, and used as a raw material for a creative work has meant that thousands of individual investigators on their own initiative have taken up the challenge of developing new datamining tools. it is such tools and the new databases that incorporate sequences, enriched by linking them to other information, that have really made the genome project the big success that it is. with openaccess publication, there is absolutely no reason to think that we would not see at least as great a flowering of new, investigatorinitiated research using that wealth of published material as a resource for creative work, and adding to its value. 37electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. open access supported by the authorœpays business model unlike the subscriptionbased model discussed by the previous speakers, to pay for the cost of publication the plos plans to charge the cost to authors and their sponsors. from the standpoint of business logic, this is by far the simplest and most natural model. it is natural, because the cost of online publication is scaled with the number of articles, not with the number of readers. it also makes perfect sense from the standpoint of institutions that pay for research. their mission is to promote the production and dissemination of useful knowledge. from that perspective, publication is inseparable from the research that they fund. the plos initially plans to charge about $1,500 for a published article, with no charge to authors who cannot afford to pay. that figure is based both on a groundup analysis of the publisher™s cost at steady state and also on a survey of publishing costs from a number of publishers who were kind enough to open their books to plos. research sponsors should welcome this model, because for a fraction of 1 percent of the cost of the research itself, the results can be made truly available to all, not just to the fortunate few who are at the lead research institutions like stanford that can afford to pay for site licenses. it can include the students and faculty of community colleges, high schools, and smaller institutions; scientists in poorer countries; and patientsšand there are plenty of themšwho want, on their own initiative, to read about the latest progress in research on their condition and are unable to pay exorbitant fees to read all the articles. this model therefore would greatly increase the benefits that are realized from the public research investment. in the short term, of course, there will be an incremental expense, but in the long term, once the scientific community has made the transition to openaccess publication, there will actually be savings to the major research sponsors because they are ultimately the ones who pay for most of the costs of the subscriptions in libraries and even for individual subscriptions. the howard hughes medical institute (hhmi) is one of the largest funders of scientific research in the life sciences, and it has already endorsed this model. the hhmi has agreed to provide budget supplements to its investigators, specifically to cover author charges for openaccess publications. those of you in the publishing business should take this as a carrot. the money is available only for publishing through an openaccess mechanism, as defined above. it is deliberately intended as a gesture of how important hhmi considers open access and as an incentive for the researchers to choose that system. at least some other major funders of research in the life sciences are taking similar steps, and this should be encouraged. the plos is also going to produce printed editions of its journals that it will sell to institutions or individual subscribers at a price that is intended just to recover the cost of printing and distribution, everything downstream of producing the published digital document. this is estimated to cost on the order of $160 a year, but that will depend on many things, including the size and the distribution of the journal, as well as other factors. if there is a market for the printed journal, which there very well may be, then the print journal operation is not going to be selling the content, it will just be selling a physical form of the content that some people may find convenient. there will not be any crosssubsidies between the openaccess online publication and the basically breakeven print publication operation. that makes perfect sense, because now that these two versions are being decoupled, the system is much more flexible. the plos does not care how well the print publication business does. the market will decide whether it is something people value. concluding observations in closing, it is useful to reemphasize a point that was made by the first two speakers at this symposiumšthat scientific publication is seen as a public good. this ideal of scientific publication being managed as a public good, as an open public resource, is now within our reach. if we, as scientific publishers, can work together, and work with the universities and the institutional sponsors of research, we can make the transition from the anachronistic, restrictedaccess business model that we have now, to a system that is economically sustainable, much better for science and society, and that provides free and open access to everyone. 38electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. discussion of issues relative merits of the userpays and authorpays models pat brown began with a comment for brian crawford. he thought that mr. crawford™s argument in favor of restricted access, namely, that it had an important role in maintaining the quality of publications and that an openaccess publication model would somehow undermine quality standards in scientific publications, was specious. there are two grounds for saying that. first, all the studies that he knows of, and there have been a number of them that have looked at the relationship between the price and measures of quality of scientific journalsšcitations, their assessment by peers in the field, and so forthšhave found, overall, a dramatic negative correlation between price and quality. the notion that users paying for journals somehow upholds highquality standards is not supported by the data. second, every scientist, at least in the life sciences, knows that most of the journals that are regarded as thirdtier journals of last resort, but which publish 99 percent of the articles, have no author charges but very high subscription costs. the premier journals, however, typically wind up charging authors in color charges and page charges something on the order of $1,000 or more. so, all the evidence that pat brown knows of actually tends to argue against the point mr. crawford made. brian crawford responded that professor brown may have extrapolated incorrectly from what he had said. he did not say there was a correlation between the price and the quality or value of the information. he was making the distinction between two different economic modelsšone where the customer or the user is paying, versus one where the author is paying. dr. crawford™s point was that there is an inherent bias in a system where the author pays, which ultimately would devalue that information in terms of its overall utility as a productivity multiplier. there are two reasons for this. the first is that a less selective filter would be imposed. and second, going back to hal varian's argument, is that an economic system where the author pays is naturally going to favor the author. that means that any entity wanting to make decisions about that work needs to impose yet another filter at some cost, in order to determine how that work stacks up against others. pat brown added that in his view, that second filter exists already, and there is no added cost for it. the factor that serves to maintain the quality of the work that authors submit for publication is not that they do not think they can ever get a paper published. one can always find a mediocre journal that will accept just about anything. rather, it is that the authors know that sooner or later their peers are going to read what they write, and their reputation depends on it being good. that is ultimately what determines their career advancement, their status in the field, and so forth. so, that filter is already in place. it is not at the level of whether or not a paper can find its way into print. brian crawford said he thought that most scientific publishers would say that selectivity is probably not exercised all that often by authors, because they seek to get their work out. professor brown did ask another question regarding the subvention of publishing costs by the payment of page charges or subvention for color reproduction charges. this is a good point, and an example of where some organizations have found an effective balance. they have done this even in a userpays model, implementing certain charges that are passed on to the author, where the needs of the author are seen as unique and something that the author would want to pay in order to benefit from a service. most organizations, however, do not make that a criterion for a decision of acceptance or rejection; it is merely a matter of presentation of the work. steven berry, of the university of chicago, said that hal varian's point that publishers would favor the author by charging the author is absolutely right, and that this is exactly what the scientific community would like to do. the public good that is produced by scientific research is very special. in contrast to other public goods, the value of scientific public goods increases with use, and it is specifically in the author's interest. it is very much a part of the motivation of the scientific paper author to have that paper as widespread as possible, more and more used, thereby enhancing the value to the community, as well as the interests of the author. consequently, brian crawford™s quote of hal varian is very much an argument for placing the charges to the author, rather than to the user, who does not have a particular vested interest in increasing the use of each written piece of work. brian crawford clarified his remarks further. it is absolutely correct that one needs to optimize two 39electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. variables. one variable is the dissemination, and the other is the filtering. his point was that the authorpays model moves the filter boundary, whereas publishers also have an interest in disseminating the work as broadly as possible. it is a matter of getting the balance right between those two considerations. professor berry added that despite mr. crawford™s logic, the facts already indicate, as pat brown observed, that the quality of articles in the journals is not correlated with the users' charges; if anything, it is inversely correlated. donald king noted that one of the things that he has found in his collaborative research is that it costs authors on average about $7,000 to produce an article. roughly onethird of the articles are funded by the federal agencies, about a third of them are funded exclusively within the universities, and the rest by industry. there are two problems in this context. one is with the argument that steven berry was making. how will the funders of these authors be convinced that they need to pay an additional $1,500 above the $6,000 or $7,000 that they are already paying the authors to prepare those articles? it is not an easy argument to make for all of those different sources of funds. for the government funding sources, such support can be written into the grants, although that has its own problems. about 75 percent of the readers of stm articles are outside the universities and the granting organizations, so they may not want to pay for page charges, especially if federal funding for research grants is reduced. nevertheless, some good arguments can be made here. where you are going to have even more difficulties, however, is with the people who are writing those articles, who are paid by the universities or industry. you will need to have different kinds of arguments for those three different constituencies. how are you going to get all of them to do it? pat brown said he thought that the interests are similar for all three of those constituencies, namely, whatever motivated them in the first place to support the research and to encourage the authors to publish it. the motivation to pay an increment of less than 1 percent of the total research cost to make it much more valuable to the people who are supposed to be served by it is presumably the same for all three sectors that sponsor research. donald king continued that it may be necessary to go further in trying to understand the funding priorities and budget profiles in each sector. for example, one could look at what the universities are already purchasing through their subscriptions and get some notion of what the transfers of funds are. once you have those data, you might be able to have even stronger arguments for the authorpays model. ed barnas, journals manager for cambridge university press, said he has been involved in both scientific society and commercial journal publishing for about 30 years. one thing about the discussion of business models that has been ignored thus far is the question of the mission of the organization that is doing the publishing. that does have a significant impact on the business model selected. forprofit, commercial publishers will seek to disseminate information in journals that are very attractive to the library market, the institutional market that provides them with the financial return on those journals. notforprofit, nonsociety publishers will seek to disseminate information also, but in a nonloss situation so that they can continue to stay in business and serve whatever their own mission is. society publishers, however, have a significant investment and interest in serving their members, and in that regard, some of the comments about author support, page charges, and color charges may be seen in a different light. when mr. barnas was working for the society publisher, or working with societies as a contract publisher, various societies would make use of the page charges as a way of supporting their member subscriptions, keeping the member rates low, so that they could provide more benefits to their members. a commercial publisher or a university press cannot really charge page charges, unless it is a journal they are publishing on behalf of a society because that is not viewed as appropriate. it would be seen as gouging. color page charges might be acceptable, because they are a direct cost, but author page support for articles in nonsociety publications cannot be a viable part of current business models for nonsociety publishers. brian crawford commented that he considered the points made by mr. barnas to be quite valid. one should not look at the customerpays model, as opposed to the openaccess model that has been proposed as an authorsupported model, in absolute terms. indeed, many professional societies have had a hybrid model, where they have benefited from subscriptions at the same time that they have used author page charges and other subventions to help support their publishing programs and to keep the costs to the customer down. that kind of hybrid model has been determined over time by market forces. 40electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. one could ask, why are we debating now what the right business model is? many of the business models that we have serve us well, and have been adopted over time. the effect of different publishing business models on the longterm preservation of digital journals wendy lougee raised the question of how publishers see the incorporation of the longterm preservation requirements in the context of the userpays economic model, given the unknown future costs associated with that. brian crawford noted that the focus of publishers in electronic publishing is mainly on the actual production of the work and its dissemination on the web. they have not addressed the important issues that deal with longterm preservation, including the integrity of the information and its migration to new platforms. reed elsevier has begun a very sensible project with the royal library in the netherlands, where they are looking at how to address those kinds of longterm preservation problems. they will be costly, and the costs will need to be shared between publishers and libraries, ideally with support from governments and other institutions. jeffrey mackiemason asked pat brown to address that same question from the perspective of the plos. professor brown™s definition of open access included the immediate deposition of the publication with some organization committed to longterm access. however, there is no guarantee the organization will follow through on this commitment. one of the advantages of print publication in is that with enough copies produced, very longterm preservation is more or less guaranteed, which is not always the case with electronic publications. the plos will make print copies, so does it plan to deposit print copies in archival institutions as well? pat brown responded that plos has a number of plans for providing archival stability for its information. what is important, however, is not only that somewhere there exists a permanently preserved copy of the information, but that it is permanently openly available to everyone. of course, one would be hard put to find a more trusted and trustworthy archival repository than the national library of medicine, but the fact that the information is also going to be freely and widely distributed, and will exist in many institutional servers, provides another measure of assurance. wendy lougee added that her question was focused more on the natural process involved in digital preservationšthe durability of format, whether it will migrate, whether the publisher is doing all the right things to guarantee successšand then how to incorporate those requirements and their costs into the publishing business model. will it be ensured through an escrow fund or folded into the base cost, and how will those costs be accounted for realistically when they are unknown in advance? pat brown noted that it would be a terrible mistake for institutional repositories to agree to take on the job of archiving information without requiring the publishers to grant unrestricted openaccess rights. to have them take on the burden of preserving information that the publishers still treat as their property, in his view is really an abuse. martin blume, editorinchief of the american physical society, addressed the archiving issue, because his society has put all of its content going back to 1893 online. it is linked and searchable, and pdfs are also available. current content is added as it becomes available. there will be costs in changing the format of this in the future. the society also recognizes the concerns of libraries and of the entire scientific community that this historical record might be lost. what would happen if the american physical society were to go under? there is a full mirror site of the entire archive that is already accessible and tried at cornell university, which is where the physical review originated back in 1893. the full content of that, from 1893 to 2001, is 378 linear feet of shelf space. the journals are also deposited at the library of congress. if the society is terminated, it has an agreement with the library that these holdings will be put in the public domain, freely available to anyone. this is the society™s primary effort for ensuring continued availability of its archived publications. it should be noted, however, that unlike the american physical society, cornell is not an eleemosynary institution. the society, therefore, has provided the university with servers for keeping its digital publications available and also pays a fee. 41electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. issues in transitioning to the openaccess model martin blume also noted that the american physical society has an openaccess model for one of its journals that it started five years agošphysical review special topics, accelerators and beams. it is a small journal, but it is available completely without access barriers. it is not, in fact, free because it does cost something to publish it. the costs are recovered by subscriptions through the sponsorship of 10 large particle accelerator laboratories around the world. it is also cosponsored by the european physical society's interdisciplinary group on accelerators. at this point it is only a limited experiment because the american physical society has other expenses that it must recover, and it has to see whether these things work or not. the society cannot afford to bet the whole ballgame on the openaccess approach because it still needs to recover its costs. in a published debate in nature on this topic, dr. blume did propose that the society would like to put everything online without access barriers. it could do this right now if every organization that now subscribes to its journals would make those subscriptions sponsorships, providing enough to recover its costs. then it could be opened to everyone else. the risk would be that institutions might be tempted to decide that since all journals are available, they no longer have to pay anymore. the libraries would love that, but then the publisher would go under. if the society were to attempt this, and did not recover enough sponsorship in the future, it could have a ﬁpledge week.ﬂ every time a reader tried to download an article, there would first be a solicitation for contributions. doris peter, from the medical letter, a notforprofit tertiary publisher that publishes reviews of drugs approved by the food and drug administration (fda), expressed concern about pharmaceutical influence in publishing, and industry influence on publishing generally. she wanted to know if plos is going to accept sponsorships from industry. she said that there appears to be some support for industry to be more involved in supporting stm journals. from the medical letter™s point of view, it purposely maintains its independence because it could not credibly publish independent reviews of drugs if it were to accept money from anyone that is associated with the pharmaceutical industry. in addition, brian crawford earlier cited a hypothetical situation in which the fda might accept money from the pharmaceutical industry. in fact, the pharmaceutical industry does pay the fda to review new drugs. pat brown responded that he completely agreed with ms. peter™s point of view and that it would be a bad idea to accept such sponsorships. it is not something that the plos is contemplating doing. nick cozzarelli, editor of the proceedings of the national academy of sciences (pnas), said that the idea of open access is a great one and he hopes that pnas someday will be switching to that business model. the problem is that pnas is a notforprofit journal, with no endowment. the risks of going immediately to an openaccess model would be too great. his question to pat brown was whether he saw any paths or way stations between full open access and the current subscriptionbased model. dr. cozzarelli suggested that one possible approach might be for a publisher to give the author an option for a surcharge. the author could pay extra to have immediate open access to his or her article, whereas all the other articles of authors who did not pay the surcharge would be free in pnas after six months. are there any other ideas of some sort of possible intermediate path? pat brown responded that he did not object to the surcharge idea in principle. the amount of the surcharge, however, must not be so high as to be a disincentive. it is not a crazy model, although he does not think it is the best way to publish. he realized the difficulty of making that transition. the plos had to get a large grant from the moore foundation to buffer the financial risk for its experiment. without the grant, the plos could not have done it. in addition, martin blume™s suggestion in the nature debate was not a bad idea as a transitional approach. in fact, it was something professor brown would strongly urge the pnas to consider, that is, to ask the institutional subscribers that now provide most of the revenues, mostly academic institutions that probably accept the philosophy that journals provide a public service, to continue to pay their subscription fees at the current rate for some interval of years to be specified, during which time the pnas would make the transition to open access. with such a multiyear commitment of support, pnas would have a stable revenue source that is not put at risk by making that transition. it can try to make the transition, at the end of which time it can determine whether it looks like it is going to be a selfsustaining model. 42electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. professor brown thought that would be worth doing. he would hope that the current subscribing institutions would not take the low road and try to undermine the process by saving themselves a little money. they could see that it is in their own best interest in the long run to encourage open access. he asked whether dr. blume™s society had actually proposed that to institutional subscribers, at least as a transitional approach. martin blume responded that the american physical society is having some discussions with other u.s. organizations to start this. of course, the situation is different in every country of the world. he added that this is not unlike other consortium agreements that are made, whereby welloff institutions offer to pay a little more, and then open access can be provided for all institutions. unfortunately, he has found that even the big institutions have asked, ﬁwhat's in it for us?ﬂ and have refused to go along with this approach. they are unwilling to help the smaller organizations. so, the society has made efforts along this line, but so far to no avail. such an arrangement is not unlike what ohio link does, but they came up with some extra money that enabled the society to open its journals to the institutions in the entire state of ohio. that is the kind of catalyst that is needed, a bit of extra money. pat brown noted that it might be best to view this approach as transitional, because ultimately the sensible thing would be for the research sponsors to cover the publication costs as an essential part of their mission of promoting and disseminating research. in the short term it is necessary to catalyze the process. he asked if the libraries would be open to trying an openness approach as a temporary experiment. mr. blume answered that the problem with the temporary approach is, what happens if it does not work? it is very hard to get people to resubscribe. once a library has given up the subscription and used the money somewhere else, resubscribing becomes a new acquisition. pat brown added that the sponsoring institutions could be provided an incentive to maintain support. for example, if stanford provides $5,000, or whatever the amount may be, then everyone affiliated with stanford would have openaccess publishing rights in the journal. therefore, it is a competitive advantage for stanford to offer this. if you treat it as kind of a credit pool that could be drawn on by authors from the subscribing institution, then even in the openaccess model, they are getting some special benefits beyond what the nonpaying institutions are getting. it becomes an added incentive. constraints of the print model on the electronic model bruce mchenry, of discussion systems, raised the issue that everybody is still working in the paradigm of the old paper model of publishing, where there is a lot of prepublication work, because a big print run is needed in order to economize on the costs of distribution. the questions about archival policy, editorial policy, and open access all change completely if one moves to a model of continuous improvement of the materials, or continuous publication, where all peers have an opportunity to adjust the prominence of newly developed pieces. in that model, the world changes completely; this issue is discussed in more detail in the last two sessions of the symposium. pat brown noted that a prerequisite for the kind of innovation that dr. mchenry has raised is that the materials that are being continually reevaluated are an open public resource. it is hard to see how this kind of approach would work, as a practical matter, if you still have a system in which every publisher's body of work is in a separate, restricted repository. bruce mchenry commented further that there are certain domains outside academia where that is simply not viable, because people need to be able to earn a living for doing intellectual work. in that case, they cannot make it free and open right away. however, after some period of time, as the prices go down, foundations, government, and corporations may decide to purchase the materials and make them accessible within their group or publicly. advertising revenues in electronic publishing professor mackiemason raised a question posed by a webcast participant who works for a large society publisher. nearly 50 percent of this publisher™s total costs are attributable to editorial, peerreview, and production processesšitems other than printing, paper, and distribution. advertising associated with paper journals underwrites more than 50 percent of the total cost of operation. if there 43electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. were less advertising in the electroniconly model, then although it may save on some costs, it actually would represent a major lost source of revenue. would electroniconly publishing be a viable business model if you were not to charge even more than publishers currently do for electronic access that is coupled with a print version? brian crawford responded that for journals with broad member or individual circulation, most advertisers still see print as their means of reaching that audience. they are not yet ready to move away from print advertising to onlineonly in scholarly publications, although this may not be the case with other consumeroriented publications. it would be very risky to go to an onlineonly strategy for stm publications, if the publisher currently relies on such advertising revenue. sponsorsupported openaccess model steven berry raised the issue of the sponsorsupported publication model, in contrast to the author or the author's institution supporting the publication. he believes that many researchers like the former model because it is the sponsor of the research who shares with the author the interest in having the product disseminated as widely as possible. when the national institutes of health (nih) supports research, it is because it is going to be published and made into a public good. consequently, to fulfill the goals of supporting the research at all, the sponsor assumes a responsibility to see that the material gets published. for this reason, the hhmi model that pat brown described is a very good one, and many scientists who publish research would like to see that kind of approach propagate. is there a way to persuade the less well funded agencies, particularly the national science foundation and department of energy, to take on the responsibility to pay for publication? this would be in contrast to the present model, which puts the author payment responsibility into a grant in which the author has discretion whether to spend that money for publication or to spend it for support of another graduate student or some other research cost. pat brown answered that despite his best efforts, he is not sure if there is a way to persuade them. it is an open question. however, one can make a strong argument that the publication costs amount to less than 1 percent of the research costs in biomedical research, (although he does not know the percentage in other disciplines). if that is true, if you had to take a 1 percent hit on other aspects of the budget to do it, it is a plausible argument that the return on that investment would be extremely high, as opposed to the 1 percent cut from other areas, because all the grantees and all the research that an agency is funding would be providing much freer access to a more extensive body of information. again, the purpose of funding of research is not just to serve the immediate community of the grantees, it is the wider scientific community and the general public that should be much better served by the information. at the same time, when this issue has been raised with respect to nih funding, many nih grantees have objected to such a proposal if it were to come at the expense of a 1 percent cut in research funding. they argue that there is not enough research funding to go around now; so it obviously is a controversial proposal. author selectivity in the openaccess model ted campion, senior deputy editor at the new england journal of medicine, commented that open access makes eminent sense, especially for anything of archival value, with genbank being the ultimate example. scientific information should become freely open to all. at the same time, the value and the future of any publication are going to be determined mainly by what authors want to do, where authors want to publish. the nejm is open to all authors; 4,000 scientific articles are submitted to the journal each year, with no charge for submissions and no page charges if the paper is accepted. however, the nejm does reject many submissions. the role of a biomedical or scientific journal is to be critical and selective. that is in part why authors want to publish there. if authors or their sponsors have to pay, is this selectivity going to be compromised? how is the plos going to exercise the functions of peer review and of being selective? is that part of its model? pat brown agreed that the journals most attractive to authors tend to be the ones in which they have the least chance of having their papers accepted. the plos certainly has factored that into its development strategy. it intends to be very selective from the beginningšnot selective just on the basis 44electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. of whether the article is good enough to be published somewhere else, but selective on the basis of publishing papers likely to be of interest to a very wide audience as well, precisely because the plos considers this important in terms of developing the journal identity and as a magnet for submissions. if the point was that adding author charges would be a disincentive to authors to submit to the nejm, that would be highly unlikely. professor brown has looked into author charges quite a bit and has paid plenty of them to a lot of journals that are very selective. it is not at all uncommon for such charges to be more than $2,000 per paper. there is a prototypical archival journal in the life sciences, for example, that does not charge for submissions, but it does charge for color. it is $750 for the first color figure and then $500 for the second. about 20 percent of all the papers that are published in that journal (in the several issues that professor brown has seen) had, on average, two color figures. the authors were willing to pay extra money to add color to their articles published in a completely archival journal. in the vast majority of those cases, the colors were primarily to make it look better and not because color was essential for the scientific content. the point is that an author will not balk at paying extra money after putting a great deal of work into a scientific paper for all the world to see. if we go to a model where the institutions are covering page charges as an essential part of research, it will make it even less of a disincentive to authors. in professor brown™s view, the new england journal of medicine has nothing to worry about in terms of going to author charges, and such a change would be better for the community it is supposed to be serving. bob simoni, associate editor of the journal of biological chemistry (jbc), which is a publication of the american society for biochemistry and molecular biology, commented that the jbc charges authors page charges. on average, an article in the journal costs about $1,000, which includes both page and graphics charges. this has allowed the journal to keep its subscription price quite low. the journal also considers itself to be an openaccess journal, though it does not comply fully with the current plos definition. two years ago, prompted by all of the discussion that came up with plos, the jbc decided to find some way to make all of its articles free online as soon as they are published. the journal started something that is called pips, or papers in press. on the day a manuscript is accepted, the paper is published on the web in pdf. it is free to everyone, and it stays free forever. anyone with internet access can read freely every single jbc article that has been published. it has been enormously popular with the journal™s readers and authors. the fact that it is published on the day of acceptance reduces the time to publication by eight weeks on average, which is the time usually taken for copy editing and processing; so it has served everyone well. 45electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 5. legal issues in production, dissemination, and use introductory comments jane ginsburg, columbia law school the discussion in this session is divided into three areas: the basics of copyright and ownership of rights; licensing by authors to publishers and from publishers to users; and some of the economic and noneconomic rewards to authors from scientific, technical, and medical (stm) publishing. before addressing the issues surrounding the grant of copyright from authors to publishers, it is important to review the "upstream" question of whether academic and research authors are initial copyright holders, or whether the institutions that employ them in fact hold the copyrights. the initial position matters for a variety of reasons, which need to be better understood. with regard to rights: what rights does copyright confer on authors (including the right to make available for free), and what rules govern the transfers of rights? in terms of licensing, what rights can authors realistically give to or withhold from publishers? what are some alternative forms of managing rights, such as broad license to publishers while retaining copyright, and what are the pros and cons? what are the major issues between the publishers and users of stm publications and of various models of access? institutions generally gain access to electronic information via license. what kinds of access do these licenses offer? how could access be improved? what are the advantages and disadvantages of licensed access models compared with hard copy distribution? what role might the science funding agencies or other organizations play in resolving conflicts in relationships between authors and publishers, or publishers and users? finally, what are the current and potential economic and noneconomic rewards to stm authors from publication, in both traditional and alternative venues? how else, apart from monetary compensation, can authors share in the value of their work? the social science research network (ssrn), an online publisher of abstracts and (when authors can provide it) full text for working papers or forthcoming published articles, is used as an example to explore these and other related issues in more detail. copyright basics: ownership and rights to begin this discussion, it is useful to examine three questions about copyright and scientific, technical, and medical publications. first, what rights does copyright establish, in what kinds of works? second, who owns copyright in scholarly works? that is, does the academic author own the copyright, or does the university that employs her own the copyright? and third, assuming that the academic does own the copyright, what rights does she end up having to give up in order to get published? basic rights conferred by copyright law with respect to copyright basics, first of all, copyright protects works of authorship, and that is a very broad notion. for example, it includes scientific works of all kinds, including computer software, whatever the medium of expression. whether it is on paper, on film, or in ones and zeros, if it is a work of authorship, it is protected by the copyright law. it is important to understand, however, that while copyright protects the work, it does not protect the information as such within the work. copyright does 46electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. not protect ideas or facts, such as genome sequences. it only protects the article that describes or expresses the results and analysis of that information, but not the information itself. with respect to the exclusive rights that copyright confers on the copyright owner, those include the right to reproduce the work in any medium, including digital; the right to make what the law calls a derivative work, that is, the right to adapt the work, including updating the work or making further works based on the first work; the right to distribute the work in copies, including digital copies; and a right that is becoming of increasing importance on the internet, the right to publicly display the work. when you see the work on your computer screen that you are viewing from a web site, that is a public display of the work, and the rights of the copyright owner may be implicated thereby. it may come as a surprise to a lot of people that copyright does not grant a right of attribution in the united states. there is nothing in our copyright law, unlike the copyright laws of many other countries, that gives the author the right to be recognized as the author, that gives the author name credit. this is a great failing of u.s. copyright law. to the extent that authors enjoy this right, they enjoy it either by contract with those who publish their work, or possibly as the result of other laws, but not the copyright law. while the research and academic communities respect a right of attribution in practice, such a general right does not exist under american law. the exclusive rights under copyright also are not absolute. there is the wellknown fairuse doctrine, which derogates from the exclusive rights. copyright is limited in time as well. the term of copyright in the united states is now the life of the author plus 70 years. that is a very long time, and it is the same term in the european union. ownership of copyright copyright ownership vests in the author of the work. that makes a lot of sense until one finds out that according to the copyright statute, the employer of a work for hire is considered to be the statutory author. what is a work for hire? for present purposes, a work for hire is one prepared by an employee within the scope of his or her employment. that sounds bad to most academics, since it might follow that everything they write belongs to the university, at least if you apply the statute's terms literally. in fact, there has been a long tradition predating the 1976 copyright act under which professors own the copyrights in what they writeštheir articles, books, lectures, and teaching materials. there is no indication that congress ever meant to change that tradition in the 1976 act. it just wrote an allpurpose provision on employee authorship transferring authorial status, as well as all copyright rights, to the employer. the question has not been settled under the current copyright act, whether the rule is what the statute says or whether the longstanding practice remains valid. two appellate judges in the 7th circuit, which sits in chicago, have suggested that surely congress did not intend to disturb this longstanding practice and understanding. at the same time, one has to acknowledge that the two judges who said that are judges posner and easterbrook, both of whom have been very prolific professors, as well as judges, so their statements may not have been devoid of selfinterest. as a practical matter, the question of professorial or academic copyright ownership did not even come up, apart from these speculations about the text of the 1976 act, until relatively recently, probably because most academic copyrights, as opposed to patents, were not worth much. although universities have had patent ownership policies for a long time, most universities, until recently, have not had any kind of policy on copyright ownership. that has changed in the past few years, in part because of the rising importance of software. software is protected by copyright, and it is worth something. so, some universities thought that perhaps they should lay claim to softwarešnot only software written by staff, but also software written by professors. universities traditionally had distinguished between works by administrators or staff on the one hand, which were works for hire, and works by professors, which were not works for hire. nevertheless, some universities began to lay claim to software produced by their professors. another thing that changed was the continuing escalation of journal subscription prices. some universities thought that perhaps if they owned the copyrights, rather than the actual creators owning the copyrights, they could bargain better with the publishers. that was perhaps more speculation than actual practice. finally, the most recently flurry of copyright ownership policies was precipitated by distance education on the internet. before the dotcom bust, a lot of universities began to get stars in their eyes 47electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. about the commercial potential of online distance education, and saw this as a potential profit center. they therefore wanted to make sure that the content that was being distributed via online digital distance education would belong to the universities, and not to the teachers. at least, that was their first impulse. the allure of online distance education has diminished since those policies were first being adopted, however, and many universities appear to have backed off from asserting such claims. table 51 presents a selection of copyright ownership policies that were publicly available from major public and private research universities. these are divided first into universities whose default position is that faculty own the copyrights in their works, followed by universities whose default position is that the university owns the copyright in faculty works, most notably on grounds of work for hire. for those researchers who are contemplating moving from one university to another, there could be valuable bargaining information here about faculty ownership or not of their works. what difference does this actually make? 48electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. table 51 default university ownership. source: jane ginsburg default ownership position: university ownership conditions for faculty ownership written agreements exceptions to the conditions for faculty ownership creator acknowledgment/right of withdrawal (when university owns copyright) faculty role in subsequent disposition of universityowned work transfer or license back to creator provision for revenue sharing university of arizona university owns work of employees, including faculty. "employeeexcluded" works (traditional publications in academia, artistic works, academic software) owned by creator. creator responsible for executing documentation to assign ownership and secure protection of property owned by the university. board retains paidup, nonexclusive license of "employeeexcluded" works for education, research and public service. board owns ip created by visiting faculty, subject to exceptions on a case by case basis consistent with the policy. release if in best interest of university; discretionary license to creator. release subject to nonexclusive license to u for educational, research, and public service work; minimal royalty; cannot use university resources to improve the invention. university will pay creator a share of net income, (gross revenue minus administrative fee and unreimbursed costs). percent paid to creator determined by "ip official" in accordance with university policy. revenue sharing continues beyond employment. university of chicago university owns "the intellectual property the faculty create at the university or with the substantial aid of its facilities or its financial support." individual faculty enjoy the revenue "until it is substantial," at which point, university and faculty member share. revenues will generally be too minimal for it to be "appropriate" that the university assert ownership rights. "individualfacultymembers are entitled to share in the revenues from the intellectual property they have a hand in creating." share will vary, depending on faculty member's contribution and costs incurred by university. emory university university owns all work of "emory personnel" related to normal duties (including areas of research/scholarly expertise) or using emory support. emory does not assert rights to "academic or scholarly copyrightable works," including "works for hire." emory personnel must agree in writing to be bound by policy; personnel receiving substantial support must sign specific ip rights agreement form. if "related to emory personnel's normal duties...field of research or scholarly expertise;" specifically assigned; or used substantial support. can petition for release or assignment of ownership. university retains nonexclusive, royaltyfree, perpetual license for research, clinical, and educational purposes. contributors receive 100% of gross up to $25,000; thereafter revenue is split with department, school/center, and university generally. contributor will receive his/her personal share after departure or death. 49electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. university of minnesota university is sole owner of "all intellectual property created through use of university resources or facilities–" "regular academic work product," as specifically defined, is an explicit exception and thus owned by the creator. written agreements required to effect exception to the regular academic work product rule of ownership. university has an ip policy acknowledgement form. regular academic work product assigned in writing to the university; specifically ordered/commissioned work designated in writing as such. no express provision in policy for transfer or license, but policy stipulates that nothing should affect right of creator to publish, except for delay as needed for university to secure protection. university will share revenue derived from "academic research or scholarly study," and "other intellectual property" as deemed "appropriate" after consultation with creator's supervisor. no mention of share after creator leaves university. ohio state university university asserts ownership rights in "products of university research." "university research" excludes works of "artistry, academic instruction, or traditional scholarship." necessary prior to beginning work on a "specific assignment." otherwise excluded works created as part of a sponsored program or resulting from a "specific assignment." ifuniversityexercises rights, university will pursue protection and licensing "with the assistance of the–creator." if university does not arrange, within a reasonable time, commercial development, transfer to third party, or dedication of rights to the public, creator may request transfer or waiver. subject to nontransferable, royaltyfree license for university. creator shares in royalties derived from any "product of university research." no explicit mention of rights after employment ends, but policy provides for university share if creator cannot be located within five years, which implies ongoing rights. university of texas board may assert ownership in intellectual property of all types. board "shall not assert ownership" of works by faculty, unless commissioned or hired specifically to create the work. general counsel "shall prepare and distribute... model agreements" as appropriate for implementation of policy. creators encouraged to sign an acknowledgment clarifying ownership of works they create. creator shouldhave"major role" in publication decision, but discretion rests with university regarding development and commercialization. university may choose not to assert interest; may elect to release after asserting interest, subject to possible recovery of expenses, reserved income rights, or other limitations. no share for commissioned work or work created by employee specifically hired to or required to produce it; for other universityowned work, university "recaptures" costs, then splits remainder with creator. university of virginia acknowledgment that workforhire gives the university ownership of works by employees. university "cedes" ownership and thus will not assert ownership interest in "copyrightable scholarly and academic works" created by faculty with use of generally available resources. required when university cedes ownership of a work in which, under the policy, it would otherwise retain an interest (not required for scholarly and academic works). use of university resources and work generates royalty payments or is of commercial value that can be realized by university marketing. university retains nonexclusive, royaltyfree right for noncommercial use of works in which it "cedes" ownership. universitymaycedeownership of works in which it otherwise, under the policy, would have retained an interest. retains a nonexclusive, royaltyfree right for noncommercial use. income is distributed according to guidelines developed by the vice president for research and public service. 50electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. yale university begins with workforhire premise that works created by faculty in the course of their teaching/research are university property. disclaims ownership of works by faculty, staff, postdocs, and students. assigned tasks and special circumstances of unusual commitment by university to project (discretionary). universitywillshare "net income" at its sole discretion, but generally on the same terms as patent policy. university deducts costs for securing and defending copyright, and for licensing or otherwise using the work. 51electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. about twothirds of the universities that were reviewed start out with the position that faculty own the works that they create, subject to a number of fairly typical defaultshifting conditions. not every university has all of them, but most of them show up in one form or another. first of all, if the university has invested substantial resources in the creation of the work over and above resources commonly made available to faculty, then the university will assert ownership in that work. second, many universities have a category of institutional works, which may border on administrative, or often refer to courses that are uniform throughout the university, such as the columbia contemporary civilization course, which has been taught and added onto by generations of faculty and is considered really more of a university work product than the work of any individual faculty member or faculty members. those are also the objects of university assertion of copyright. third, many universities provide that where the research was sponsored by an outside entity, if a condition of the outside entity is that the university own the copyright, then the university will own the copyright. fourth, a number of schools will assert copyright ownership if the work qualifies as "technology" or software. this can be troublesome, because most of these categories are not well defined between "traditional" academic work, which belongs to professors, and "technology" which belongs to the university. there is some potential tension in the understanding of what those categories mean. when the default position is that the university owns, there is often a provision that the university will forbear from asserting copyright in traditional faculty works. that leaves open the question: what do we mean by a traditional sort of faculty work? if it is not clear what that technology category means, and if the university's position is default university ownership, it may be that the faculty members are on the short end of that stick. regardless of who starts out owning the copyright, most policies have some kind of limitation on the exercise of copyright, whether by the faculty member or by the university. the most typical constraint on the faculty in the exercise of copyright is that the faculty member will be asked to give the university a royaltyfree license to make nonprofit university educational use of the faculty member™s work product. often that license applies even after the faculty member has left the university. on the side of university ownership, if the university owns, whether by default or because one of the conditions for transferring ownership back to the university applies, the most typical constraint on the university™s exercise of copyright is to allow faculty noncommercial use rights. some universities will not commercialize the work without obtaining the reasonable consent of the faculty member in advance. furthermore, many universities provide that with respect to course materials, even if the copyright is owned by the university, if the faculty member leaves and goes to teach somewhere else, he or she can take those materials to the next place. however, only one university, columbia, provides that even when the university owns the copyright in the work, it should respond favorably to the creator's request to the university to make the work freely available. in other words, the university in effect renounces its exercise of copyright control over the work that it owns at the behest of the creator of the work. this provision extends not only to faculty creators, but also to staff creators. the provision was added to the columbia policy largely at the request of the academic computing staff, who do not necessarily hold faculty appointments, but who felt very strongly that what they do should be made freely available for other people to build upon. the university acceded to their request. other universities may do this as well, but it is not explicitly included in their policies rights that a faculty member gives up to be published assuming that the faculty member does own the copyright, what does he or she have to give up in order to get published? table 52 presents the copyright policies for authors from a crosssection of publishers, both the notforprofit professional societies and forprofit publishers. it is rather interesting that there is not much difference in the terms between the societies and the commercial publishers. 52electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. table 52 default faculty ownership. source: jane ginsburg default ownership position: faculty ownership conditions changing default (excepting sponsored projects) other limitations on faculty copyright limitations on university ownership written agreements creator acknowledgment / right of withdrawal (when university owns copyright) faculty role in subsequent disposition of universityowned work transfer or license back to creator (if conditions for university ownership exist) provision for revenue sharing university of california scholarly/aesthetic work resulting from "independent academic effort" by a "designated academic appointee" owned by creator. contracted facilities work; commissioned work; sponsored work (unless otherwise provided); institutional work. requiredforcontracted facilities work, commissioned work, sponsored work, and project with © ownership other than as provided by the policy. discretionary,subject to thirdparty agreements and best interest of university. work may not be further developed with university resources; university is granted freeofcost, nonexclusive, license for educational/research purposes. royalty or income from thirdparty assignment or license of universityowned copyrights may be shared with creator based on "originator's contribution, university's costs," agreements with sponsors, or other applicable agreements. columbia university recognizes faculty ownership of traditional works of authorship created by faculty. substantial use of or support by university resources; created or commissioned for use by university; workforhire (including administrative duties). university may assert copyright in: institutional works; course content / courseware (possible exception); use of university name; software. in licensing work to third parties, creator shall seek to reserve university royaltyfree right to use portion of work within university for teaching, research, noncommercial use. if university holds rights, faculty members can use works for noncommercial purposes. policy constitutes binding understanding; formal agreements as necessary to implement policy. acknowledgment of creators who make "a substantial creative contribution" (including creators of workforhire). creator of a work owned by the university can request the work be made publicly available. creator has consultation regarding commercialization. transfer possible upon university determination that ownership has no commercial value. university retains irrevocable, royaltyfree license for noncommercial purposes revenue shared for works made with substantial use of university resources; sponsored project; courseware. share on a case by case basis for institutional works and worksforhire. share not altered by termination of university affiliation or death. duke university personal ownership of intellectual property rights in works of the intellect by their individual creators. computer programs; databases; extraordinary allowance, grant, or subvention; certain collaborative works. members of university community enjoy permanent, nonexclusive, royaltyfree license to make customary and traditional use of course content. moral rights of individual creator "respected to the extent practicable in every case contemplated by this policy." discretionary grant "of licenses or royalties or both–on just and reasonable terms." royalties granted at university's discretion. 53electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. harvard university except as qualified, "a member of the university is entitled to ownership of copyright and royalties or other income derived from works." work created by nonteaching staff (workforhire); use of rare university holdings (potentially); substantial university assistance; use of university name or insignia; extensive use of voice or image of students or staff. if work falls into optional exception categories, and creator continues to be owner (excluding workforhire), university may share in royalties and enjoys royaltyfree or reducedroyalty license for notforprofit educational activities. explicit agreementnecessary when there is substantial university involvement (even if agreement is that creator retains copyright). introductoryprinciples advocate consultation for creators in the publication, development, modification, or commercialization of universityowned work. share does not apply to workforhire. for other universityowned works, creator's share continues after individual leaves university. creators deriving "substantial income" encouraged to continue tradition of making a gift to university. university of illinois creators retain ownership to "traditional academic copyrightable works" unless subject to defined exceptions. creation pursuant to external agreement, as a specific requirement of employment or assigned duty; commissioned work; patentable work. if resources beyond those "customarily provided" are used, creator copyright subject to perpetual, nonexclusive, royaltyfree license for use in internal teaching and research. acceptance of policy deemed condition of employment. commissioned work must be prepared under written agreement. assignment in case of university abandonment; license upon demonstration by creator of capability to commercialize. assignment must grant university perpetual, nonexclusive, royaltyfree license for internal teaching/research; may be subject to revenue sharing. license subject to conflict of interest review. creator does not share in revenue from sponsored research agreement or grant, nor is there a share in equity comprised of real or personal property. other revenue is shared with creator, subject to deductions for "out of pocket payments or obligations" plus a reasonable reserve. johns hopkins university "copyright to, and royalty from, literary or scholarly works" produced as part of "usual teaching, service, and research" belongs to creator. university has "right to obtain title to ip developed as a result of support either directly from or channeled through the university." an appointment itself is not "university support." workusuallyexcepted from university ownership but work that is a "specified deliverable" from a universityfunded or sponsored project is owned by university. creator shares in "net revenues." personal share survives termination of affiliation with university and death. 54electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. massachusetts institute of technology author ownership subject to carved out exceptions. workforhire by operation of copyright law; works created with significant use of mit funds or facilities. (policy does not state whether faculty work is generally considered workforhire.) must sign agreement if participating in sponsored research, mitfunded research, or "significant use" work. subjecttouniversity discretion, author may request that work be placed in the public domain. discretionary license back or "stand aside"; creator must demonstrate ability to commercialize. royalties to university applicable. transfer may be terminated if effective dissemination does not occur within three years. provision for share after deduction of overhead and direct outofpocket costs. university of michigan "all faculty own and control instructional materials and scholarly works, created at their own initiative with usual university resources." work created with "unusual university resources" or "as a specific requirement of employment." university may use materials created for "ordinary teaching use" and administrative purposes; in licensing to third parties, faculty encouraged to secure permission for university community; should not use works in manner that competes with university's educational activities. nonexclusive use and distribution rights for noncommercial purposes, continuing after faculty member's departure from university. first refusal to make new versions of universityowned instructional materials (consultation for those who left university). policy states written agreements "may be necessary" to modify rights or provide greater specificity. acknowledgment and withdrawal. right of first refusal on new versions; consultation on reuse and revision of instructional materials for creators who have left university. departed creator may use work for teaching, research, and noncommercial purposes. university of north carolina, chapel hill ownership dependent on category; "traditional" or "nondirected" works are owned by the creator. ("directed works" are works that are "specifically funded or created at the direction of the university,ﬂ whether or not exceptional resources invested.) if exceptional resources are used, university owns copyright. if university is involved in commercializing traditional/nondirected work, assignment required. as a condition of employment, faculty grant university nonexclusive, nontransferable, royaltyfree license, unless such will impede scholarly publication. in case of commercialization by university of creatorowned work, an assignment agreement shall specify responsibilities and proceed shares. release possible in case of work using exceptional resources, directed work, or sponsored work. nonexclusive, nontransferable, royaltyfree license for university's own education/research; income should be shared with university. policy states that royalties for copyrighted works are usually a matter between author and publisher; acknowledges that "different treatment" may be necessary for universityowned works but does not specify terms of such treatment. likely will accord with patent policy for net revenue sharing. 55electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. pennsylvania state university ownership of "literary, scholarly, and artistic works" rests with creator. (such works do not include "survey instruments, instructional materials including videotapes, and computer software created on u time.") works generated within scope of creator's employment (literary, scholarly, and artistic works specifically excluded) or commissioned by university. all faculty must sign an intellectual property agreement. policy manual has a provision on coauthorship of scholarly reports; applies regardless of copyright ownership. provided only for patents. policy provides for sharing after recovery of costs related to prosecution, maintenance, or infringement litigation costs incurred by university. university of pennsylvania affirms academic custom of creator ownership in works resulting from research, teaching, and writing. substantial use of university resources; workforhire, defined in policy as prepared at direction of supervisor, pursuant to job description, or administrative duty. in transferring interest to others, author should attempt to secure for university royaltyfree right for traditional, customary, or reasonable academic use. university retains nonexclusive, royaltyfree license for software and courseware. universitymayrequest assignment in instances of workforhire; written agreement necessary for works making "substantial use" of university resources; will attempt to "secure acknowledgement" from authors in sponsored project. faculty maypetitionuniversity to waive ownership of work created with substantial use of university resources. revenue from sponsored research and work involving substantial use of university resources follows that of patent policy, but with different share percent; creator share continues following employment at university. princeton university no claim to work produced by faculty through "normal teaching and research efforts." "substantial resources... designated for the development of intellectual property" and by which faculty member may derive outside income. software subject to special disclosure requirements with respect to commercial potential. even with the use of substantial resources, university only considers it has "some equity" and "part" of the income should reimburse university for use of resources. policy provides for sharing of "net income." stanford university "copyright shall remain with the creator"; specific disclaimer with respect to "pedagogical, scholarly, or artistic works." institutional works (direct allocation of university funds, created at direction of university for specific purpose, or works without a discrete author). significant use of university resources requires assignment of title to the university. all faculty must sign university patent and copyright agreement. discretionaryreconveyance provided there is no real or potential conflict of interest for the creator, or conflict with university policy. reconveyance may not "limit appropriate university uses of the materials." policy states that royalty will be distributed in accordance with patent licensing policy, which provides for distribution after deductions for overhead and direct expenses. 56electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. vanderbilt university rights in "literary and artistic works" are granted "to faculty, staff, and students who are the authors." nonscholarly literary and artistic works created with use of university "funds or facilities, or [which] capitalize" on university affiliation. "technology" (computer programs, designs) created with use of university facilities, or without use of university facilities but within creator's scope of employment. "technology"thatwould otherwise be owned by university but assigned to outside entity by creator or created pursuant to independent research, consistent with university policy. technology assigned to outside entity or created pursuant to independent research must be disclosed or acknowledged in writing. anticipatedthatcreator will participate in licensing, including consultation prior. affiliated licensing may be given first negotiation. if university does not patent or transfer "technology," creator can request assignment. must be consistent with existing governmental rights. net income shared with inventor. purpose of policy is to aid inventors and creators for small discoveries, while large inventions aid inventor's school proportionately more. if inventor leaves university, inventor's school share increases by 10%. university of washington university faculty, staff, and students retain all rights in copyrightable materials they create. commissioned works (developed during regular duties, except scholarly works); "sponsored" works making extensive use of university service centers. regardless of ownership of work, use of university facilities, supplies, or staff, gives university royaltyfree use of work (other than books, etc., available through normal channels). when necessary for transfer of universityowned or universitysponsored work. required for commissioned works, for works with uncertain ownership interests prior to beginning, or to vary default status. required to dedicate author share of revenue of universitysponsored works to school/department. can request materials be withdrawn from internal use if revisions are not feasible and provided author is still employed by the university. internal: approval of author required unless waived in written agreement or implied; can request revisions prior to internal use. external: subject to written agreement between university and author. noshareincommissioned works; revenue shared in the case of "university sponsored" works, subject to recovery of costs. university of wisconsin (policy primarily pertains to instructional materials with brief disclaimer of scholarly copyright; no other policy located). university does not assert ownership in "traditional teaching, research and scholarly activities," though performed as an employee. where substantial institutional resources support development of instructional materials; work using minimal support subject to agreement. to determine ownership rights in instructional materials developed with minimal university support; also if university will own copyright because of substantial support, assigned duty or workforhire. agreements under the policy should address internal use rights and sharing, which is encouraged. right to not be presented as work of the author if use is contrary to recommendation of author. rights re: revision and withdrawal with respect to instructional materials only. includes consultation, option to assume responsibility for revision; consultation on "other" uses. author role in external distribution. revenuefrom"external distribution" shared; split based on preand postamortization schedules. author's share continues regardless of termination from university. 57electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. typically, the publishers demand a total assignment of copyright from the authors. this is pandemic. for example, the copyright assignment form of the national academy of sciences (nas) states: i, the undersigned, hereby irrevocably grant and assign exclusively to the national academy of sciences my rights, including copyright interests, in materials or work that i will contribute to the preparation of the report identified below, and in the report, under all laws, treaties, and conventions throughout the world, in all forms, languages and media, now or hereafter known or developed without limitation. many of the agreements do, nonetheless, provide for some kind of privilege back to the authors, notably for their own nonprofit, university use in further works or in teaching. where there is considerable divergence is with respect to electronic rights. almost all of these journals publish both in print and in electronic form. depending on the contract, authors will be permitted to post an abstract of the article with a link to the publisher site, or they may be allowed to post a preprint of the article on their own web page, or on some other preprint site, but they may not post the text as edited by the journal. another variation permits posting of the article, but only on restricted access sites. so, with respect to what authors can do with their own articles, it is far from open access. another fairly typical provision, also from the nas contract, paragraph four, states: in making this assignment, i understand that it is the national academies' policy to grant permission to me to use materials prepared for this report without fee upon application to the national academy press. that means that the author cannot use his or her own material without written permission from the national academies. one might think these policies of the publishers would create a lot of public opposition by the authors, but they have not. so, why not? or, at least, why not yet? the simple and completely unlegal answer to that is that professors in practice largely ignore these contractual limitations, and they boldly post full text regardless of what their contract says. a highly unscientific survey of professor ginsburg™s colleagues in the sciences at columbia university suggests that this reflects what may be called the nathan detroit model of epublication: ﬁso sue me, sue me, what can you do to me?ﬂ the authors are basically betting that the stm publishers will not actually enforce that contract against them. a number of the preceding speakers have envisioned an evolution away from the production and distribution of copies on the part of publishers and toward valueadded services such as peer review, or what wendy lougee referred to as the codification of prestige, as well as toward a variety of secondary indexing and searching services. if it is true that the commercial value will no longer attach to the actual content of the work, which may well remain on the academic's web site or on a preprint server, then joe esposito may be correct that copyright will not matter anymore to publishers. copyright nonetheless does matter to authors. one thing that was not mentioned regarding the university copyright default rules is that there is a significant psychological factor in being the owner of the copyright in your work. the universities that take the workmadeforhire positionšto the extent that their faculty know about itšrisk antagonizing those faculty members. there is something very deepseated, even if one is disposed favorably to open access, to still being considered to be the author and copyright owner of your work. whether or not it will matter to publishers in the future, it will most likely continue to matter to authors. 58electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. licensing ann okerson, yale university licenses are not entirely new, but are a more recent method for distributing stm journals and other digital information. licensing can and does work, and this presentation will discuss why. at the same time, there are many people in academia and at libraries who have expressed negative views about the licensing of journals. copyright is a longestablished regime that most participants in stm journal publishing view favorably. this is also because copyright law is arrived at through a broad national dialogue, and it takes time for that dialogue and for the resulting law to be completed. copyright is federal national law, and it tends to be very high in concept, and very low in specifics. it is also hard to change, which is probably a good thing. licenses or contracts are quite different. they are private, negotiable agreements. they are specific and very tailored, and that specificity can be very reassuring. if you are an elementary school teacher and you like to teach the kids in your class, and you want to know what you can do, a license can help delineate that in very useful ways. licenses can restrict or expand rights that would be granted by copyright law. in that sense, a license is a neutral document controlled by the parties to the agreement. licenses used to be regarded as entirely controlled by publishers, at least in the library community, but this is no longer true in all cases. libraries, working with publishers in the license environment, have in fact made an electronic market possible. why do publishers need licenses if they have copyright? people often ask why is it that we cannot just use copyright for the distribution of electronic information resources. why is it that we are now using licensing as well? the most obvious reason is that there is no artifact to work with in the electronic environment. there is nothing to hold, and there is no object to own. this is a very big factor in library licensing. the electronic distribution environment is very new, and current copyright law does not fully address copyright owners' concerns. some of those concerns are based on the ease and rapidity of distribution of copyrighted works on digital networks, but there are others. a number of vendors and aggregators have added value to publicdomain databases originally produced by government entities and have found customers willing to pay for such enhanced information products. copyright may not adequately protect such products. other vendors, aggregators, and publishers have digitized a lot of outofprint works whose copyright has expired and have added a great deal of functionality and value to those works. again, because the works they have digitized are in the public domain, copyright law would not really help them in protecting their distributed material. another reason there is value in licensing is that copyright has been in a state of flux. even though it may take a long time to change the law, in almost every session of congress in recent years there has been some copyright or database protection legislation or some other proposed statute concerning rights in digital information. licenses provide certainty to the parties regarding the specific terms and conditions of the use of the material. copyright law also cannot address the details of complicated electronic information arrangements anymore than it could in print. licenses can clarify and make less ambiguous numerous matters where practices are unclear. they consequently encourage dialogue, which may be the most valuable part of the licensing process. licensing engages the two parties in a conversation about what kind of expectations they have of each other. over the past few years in the internet information environment, this has contributed to a great improvement in understanding between librarians and publishers. finally, licenses generally are able to cross national boundaries successfully. they have enabled progress in the marketplace for electronic information without waiting for copyright or database legislation 59electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. to be revised or implemented. in the united states, licenses are governed by state law, not national law. they can be harmonized through the national conference of commissioners on uniform state laws (nccusl), although attempts to do so in this specific area of the law have failed thus far. major licensing issues for libraries not all licenses have been fair or negotiable for libraries. in fact, 10 or 15 years ago, the licenses offered to libraries were unacceptable. some problem areas have been resolved, but others persist. the most difficult areas to reach agreement on have been the terms for interlibrary loans and guaranteeing perpetual access and archiving. the interlibrary loan is a relatively long established practice whereby a user in an institution that does not have a book or article can request it from another library. articles are generally photocopied and sent to the requester by fax or by mail. now that articles are available online, one might expect that interlibrary loans should be able to take place with the supplying library transmitting the article electronically to the requesting user. in fact, very few publisher licenses permit this type of transmission. most require that articles be printed from the eversion and then forwarded, or they do not permit any interlibrary loan from the electronic version at all. when the interlibrary loan from the eversion is only permitted after it is printed, the main consequence is that delivery is more cumbersome than it otherwise would be. where the interlibrary loan for the eversion is not permitted at all, then a serious degradation of access ensues in the electronic environment, especially if libraries move to electroniconly subscriptions, which, contrary to what was said by some previous speakers, they are. in the latter instance, libraries cannot fulfill interlibrary loan requests. the requesting reader may be unable to afford a payperview solution at the price set by the publishers, or, more likely, the payperview option just does not exist. the concern about archiving and perpetual access in licenses is somewhat different from this, but it is nonetheless a great concern. research libraries have indeed attempted to hold onto their print subscriptions for their archival and preservation value and to adopt electronic subscriptions to journals for reasons of service and functionality. however, the economic climate is such that many libraries, for lack of resources to support both print and electronic subscriptions, are beginning to drop the print. this is happening even in the biggest research libraries. libraries have always assumed that the material they pay for will last indefinitely, and users will be able to have longterm access to it. the movement to electroniconly suggests two requirements. one is that repositories of electronic journals must be robust, even though currently they are less than optimal. the other is that libraries licensing journals on behalf of their users will want continued access to those eresources even after their paid access to a given journal stops for some reason. the majority of contracts for electronic journals now provide language for continued access, often in perpetuity, but not all do. furthermore, the contracts are very weak because of the inability of most publishers to follow through technologically or in a business sense on the promises that are made about the archiving provisions. these major flaws in archiving provisions will be resolved only upon significant discussions and investments by all the stakeholders about archiving responsibilities, costs, and technologies. meanwhile, as libraries are now canceling paper subscriptions, the official scientific record is at some considerable risk. licenses that serve author interests better scientists clearly want their articles to be widely available. many scientists today might like to distribute their own articles on their web sites, their university sites, their lab sites, and eprint sites, in addition to the formal peerreviewed journals. some publishers permit this, even though copyright has been transferred to them, but others do not. in most cases, publishers ask scientists to transfer their copyright, and scientists are accustomed to make such transfers and sign those papers. how can authors get out of this quandary? it is not really so difficult or impossible, and there are two ways of doing this. a good way is to sign a copyright transfer that permits the author to distribute his or her own work. the advantage of this to the author is that it keeps them out of downstream copyright management problems. 60electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the copyright transfer form of the american physical society (aps) provides one good model.10 the most important points are that the author has the nonexclusive right to give permission to third parties to republish the print version without obtaining permission from the aps. the author also has the right to use all or part of the article on his or her web home page, or for lecture or classroom purposes. the author also has the right to post and update the article on eprint servers, if certain reasonable conditions are met. an even better way, perhaps, is to maintain copyright, while licensing the publisher to deploy the work in ways that the publisher needs for effective publication and dissemination. in this case, the author will have continued responsibility for managing his or her rights, but will also have broad flexibility as owner of the work.11 economic and noneconomic rewards to authors: the social science research network example michael jensen, harvard business school the social science research network (ssrn) was not founded to make profits, but as a way to change or to make efficient the distribution of work in the social sciences. the ssrn posts abstracts to fulltext, nonrefereed working papers on its web site, as well as fulltext, refereed articles and journals of established publishers who want to have access to the community that has been created around the web site. the network provides both webbased access to this work and email journals, which carry abstracts. the announcement service is very important and highly valued. it is one of the few services for which the ssrn charges, and not very much at that. the copyright policy is very simple. the ssrn does not take copyrights. all it requests is a nonexclusive license to post the electronic versions of the abstracts or working papers on its web site, and the author can take them down at any time. the network was originally founded in 1993 to publish just abstracts in financial economics. it began to carry fulltext working papers in 1997. it outgrew financial economics very quickly and then launched different related ventures, including the economics research network, the accounting research network, the legal scholarship network, and, most recently, the management research network.12 it will probably continue to expand. the ssrn publishes more than 400 email subjectoriented abstracting journals to narrow down the torrent of material to something that is manageable for people. the emails show up on each subscriber™s desktop weekly, containing three to five abstracts, with links to the fulltext paper, full author contact information, and the ability to communicate with each other. the ssrn charges about $10 per year for the email abstracting journals. another way the ssrn gets revenues is as a subcontractor to many universities and research institutes that contract out the job of distributing their research. currently there are 92 institutionally sponsored research paper series, which have the fulltext papers. each of those institutions has its own email abstracting journal, along with the whole associated infrastructure. the network has an increasing number of established publishers that are giving it their work so that they are part of the database, part of the email abstracting service. they sell the fulltext downloads, and share the revenues. the ssrn only charges for the downloads if the partner does. the size of the network community currently is about 40,000 subscribers; 31,000 authors; 1,800 partners in publishing (both formal publishers and universities or research institutes); 716 academics worldwide who act as advisory board members on these journals and networks; 137 editors and network directors, most of whom are unpaid; and 34 paid administrative and professional staff. there are about 200 journal publishers with roughly 1,000 journals, for which ssrn has permission to carry abstracts and fulltext documents. the nonprofit publishers, which include the 10 the american physical society™s copyright transfer form can be found on their web site at http://forms.aps.org/author/copytrnsfr.pdf/. 11 several examples of such licenses can be found at www.library.yale.edu/~/license/authorsslicenses.shtml. 12 see the social science research network home page at: www.ssrn.com. 61electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. universities, research institutes, and similar organizations number about 400, with about 400 journals and research paper series. partners include mit press, yale, ucla, stanford, skadden arps, national bureau of economic research, goldman sachs, and blackwell. there are approximately 53,900 papers in their database, all with abstracts, and fulltext documents for about 33,000 of them. the ssrn gets about 3 million hits and about 120,000 unique users per month on its web site. the average length of a session on its site is 4.5 minutes, which is well above the norm in the internet context, which typically averages about a half a minute. there are 190,000 downloads of fulltext documents per month now. it has had about 4.3 million downloads to date and about 12.6 million abstract views. the ssrn has created some measures of popularity. although it does not referee anything, it will put material on the site if it is part of the worldwide scientific discourse in the field for which it is intended. it creates for each field and each journal, ﬁtop tenﬂ lists, based on use. the top paper has had 30,257 downloads. this results in an unbelievable amount of attention for papers, at least compared with what you would normally expect to get through a journal. another top 10 list is for the most popular papers published in one of the network™s abstracting journals in the past 60 days. every author in the system has an author page. wherever the author™s name shows up on the ssrn web site it is hyperlinked to a stable url that takes the user to the full contact information for the author. the author page provides a list of all of each author™s papers on the ssrn site, which are hyperlinked and available for downloading. the author page gives the total number of downloads from all of the author™s documents, the articles are ranked, and the downloads for each one are given. in addition to charging small amounts for the abstracting journals, the ssrn charges institutions that want to stop distributing the work themselves and transfer that function to the network. it also carries a professional announcement and job series, for which it charges nominal amounts. in cases where people choose to sell their work through its site, prices range anywhere from $5 per download for many fulltext journal articles, to $20 a download for a couple of the publishers, and they share those revenues with us. although the ssrn is not yet selffinancing, is expected that it will eventually cover its cost. discussion of issues the value of ssrn to authors jane ginsburg began the discussion by giving the ssrn a warm endorsement as a ssrn author. she noted that in the field of legal scholarship, it has made a tremendous impact through those abstracting journals, which arrive regularly. you can select what fields you want to know about in recent filings with ssrn, and then click on the full text. more often than not you can get the full text, and not just the abstract. then there is the author gratification element of ssrn, which is not generally available elsewhere. the brilliance of ssrn in developing those top 10 lists is that it has devised an infinite number of carefully gauged categories so that almost every ssrn author at one point in his or her career is a ﬁtop 10ﬂ ssrn author. the author then gets a nice email from michael jensen conveying congratulations that the article has been selected as one of the top 10 in some category, even if it is an extraordinarily narrowly defined category. it is very gratifying. authors can look at their web page and see how many downloads there were of their article in any given week. that is one of the reasons that the ssrn gets a high level of participation once authors know about it. even though it is not the kind of prestige credentialing that is provided by peer review, there is a certain amount of validation. if your article has been downloaded 30,000 times, it is not just your sisters and cousins who are doing that, so this has some value to it. martin blume added that the ssrn looks very much like the eprint server that is used in physics, and also like pubscience, a service that was provided by the department of energy until it was forced to be taken down. the aps mirrors the eprint archive and makes extensive use of it too, because authors can submit to that, and they subsequently submit to the society™s home journals. 62electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. authors™ assignment of nonexclusive licenses michael jensen noted that it was not clear when he started out that he could make the ssrn work without taking anything other than a nonexclusive license, which people could revoke at any time. in fact, it does work. indeed, he emphasized that he has never seen a publisher enforce a copyright in any way other than against an author. whenever he gets a standard copyright assignment form from a publisher, he substitutes ﬁnonexclusive licenseﬂ for the word "copyright," and sends it back. nobody has ever refused. digital media are making a difference, and attempts to maintain monopolies on the scientific literature are going to be very shortlived, perhaps another 10 years. in some areas it is going to be slower, and in others it will be faster, but licenses will be the future. publishing certainly will not vanish but it will take a somewhat different form. authors will have more rights. at the same time, publishers have to earn a fair return on their capital, or they cannot stay in business. ann okerson agreed that the days of exclusive distribution may be rapidly fading. she recommended as a model the rights form used by the aps and others like it, based on a nonexclusive license. she then read a question from a webcast participant who asked: if the funding or employing institutions were to insist on retaining copyright of work submitted to stm journals, how would this impact stm publishers' decisions about acceptance, and also the overall economics of their publishing business? for example, the mayo clinic has in the past insisted on retaining rights after first publication, and this has had little impact on acceptance of papers originating from mayo. martin blume responded that the liberality of the aps™s copyright form, which was mentioned, gives these rights to institutions and to authors. the society has a reason for insisting on taking copyright, as opposed to leaving it with the authors. dr. blume mentioned earlier that the society had put all of its content going back to 1893 online. if it had been taking during those years a license, and if that license had failed to mention the electronic means of distribution and publication, as it almost certainly would have, the society would have had to think quite a long time before going ahead, because it would have had to go to all the authors to collect these rights, at least if one were to believe the lawyers. it is also not known how this will work in the future, for example, whether a license that gives the publisher rights to methods of distribution as yet uninvented is valid. aps has received legal advice that in a number of countries, this may not be a conscionable contract, and the society is a very international journal. consequently, it would not be able to have the best assurance that it would be able to continue to assemble and distribute an archive in the future. this is the reason the aps asks for assignment of copyright. it does give back to authors all the rights that they would have if they had copyright, except the right to keep the society from doing what it wants to with the publication. it has not accepted a request for a nonexclusive license, and generally authors have understood this policy and complied. jane ginsburg noted that if the license states that it extends to new modes of communication, now known or later developed, even though it is a license and not an assignment, that is valid. even if you have an assignment of copyright, the author can terminate it under certain circumstances. she also noted that for those publishers who have any grant, exclusive or not, executed as of 1978, it can be terminated 35 years after the grant. even with an assignment, therefore, a publisher does not have watertight protection, particularly for old material. dr. blume then continued with the following story. i.i. rabi, a distinguished professor of physics at columbia, nobel laureate, and onetime president of the aps, told an anecdote of taking president eisenhower around the university. eisenhower said what a pleasure it was to be able to meet with the employees of columbia university. professor rabi drew himself up and replied, ﬁwe are not employees of columbia university, we are columbia university.ﬂ accordingly, perhaps the professors at columbia could keep copyright under this, since they are the university. another reason the aps wants to hold copyright and have the maximum protection that it can get is that it does not trust the law or lawyers, not because lawyers are dishonest or the law is corrupt, but because there is no certainty about how any future dispute is going to come out. the new york times v. tasini case is a perfect instance of this, where an unexpected result came about. in fact, it is one that was detrimental to archiving everything on new media. 63electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. although aps has never taken an author to court to enforce a copyright, it does deal with other publishers. it grants back all of the rights to the authors, but if other publishers are assembling collections of the society™s articles and are going to make money out of it, the society needs to have the copyright to get a piece of the action. with regard to the articles submitted by government employees, the situation is straightforward. those articles are in the public domain, and aps accepts that. in fact, it could deal much more readily with works put in the public domain than with copyright being held by someone. it would be easier, because the society would then have a greater assurance that it would be able to do what it wished with the article in the future. nick cozzarelli added that the policy of pnas is the same as that of the aps. he went on to note that, as an author, he dislikes receiving requests for reproducing his work. he is delighted when a journal will do it for him, and will be even more delighted in the future when nobody has to ask. however, in the discussion thus far about the pros and cons of maintaining control of the copyright, nobody has brought up the fact that by the author maintaining sole rights, it is more difficult when someone else is trying to reproduce it and has to get permission from the author for it, instead of from the publisher. legal issues in new types of digital publications dan atkins, of the university of michigan, noted that subsequent panels at this symposium would focus on the future of the process and the products of publishing. in the future, the nature of the document could be expanded to be a multimedia work, involving computational programs, access to data sets, video, audio, and possibly produced as a composite object by multiple authors in multiple institutions, and so forth. what kind of complications does that present to the existing models of copyright and intellectual property? jane ginsburg responded that the work that professor atkins described, a multimedia work, would probably be considered an audiovisual work under the copyright statute. the owners of that would be the creators, all of them, if it is not a work made for hire. this is a situation in which it might be useful to have contracts between all the owners before they go ahead with such a collaboration, because the administration of the rights to the work might be a little complicated. the default position in the copyright law is that all joint authors share equally in the value of the work. any joint author or coowner of the work can separately grant nonexclusive rights in the work, unless the contract provides otherwise, although they have to get together and agree on the grant of exclusive rights. then it is a question of appropriate contracting. as a matter of copyright law, however, it does not really pose any greater degree of complexity than making a movie or an audiovisual work with a lot of participants involved. it is largely a question of contracts. implications of digital rights management (drm) technologies for stm journal publishing dan atkins went on to ask about the drm technology developments and the laws making it illegal to break the coding schemes that protect intellectual property rights. do these developments have applicability to the stm realm, creating either additional opportunities or problems? jane ginsburg responded that some drm applications, such as payperview publishable or downloadable could be applied to stm journals. there are technological measures that control access and possibly additional ones that control the copying of the work. if we just look at access control, for example, you might have a subscription deal that allows you to have access for a week. copyright law prohibits you from breaking the access code, so that you could not keep it forever when you only paid to keep it for a week. business models are now being developed on variations of how much of the work you get to see, how often, how many times, for how long, and so forth. all of these are built on access control technologies. the premise is that publishers can give people more varieties of access at different price points. this probably does not apply to stm as well as it does to the music business, where maybe you do not want to buy the cd with 12 songs you hate just to get the one song you want. there you can see that the model of accessing just one song with multiple replays could be quite attractive. it has taken the record companies a very long time to acknowledge that people do not want to be tied to the entire cd 64electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. anymore. they would like to have just one or two songs, regardless of whether they have it permanently or for some limited time. the package that the record companies have imposed on the musicbuying public since the death of singles is now no longer enforceable, thanks to a variety of unlawful enterprises like napster and kazaa. similarly, one might say with respect to journal publishing, that not everybody wants the entire journal issue. why do you have to buy the entire journal, if all you want is one article? the breaking up of journal issues has already been practiced in the print medium with respect to photocopy rights. the copyright clearance center (ccc) has for years granted licenses for photocopying single journal articles. that translates perfectly well to the digital rights management situation as well. an example of a publisher who enforced copyright in this context, not against an author, but against a user, was american geophysical union v. texaco.13 the american geophysical union sued texaco because it was making systematically a large number of photocopies of works from that society and from 500 other journals, without paying for a license to do that. texaco bought one subscription, and then circulated the subscription to all of its inhouse scientists, who made and kept photocopies. consequently, the american geophysical union, together with other publishers, sued texaco, requiring the company to take the ccc license instead, and they prevailed. burdens for small publishers from licensing and restricting access leslie chan, of bioline international, said that in many ways his project is the small deal compared to the big deal. its associated publishers are all typically very small, nonprofit scholarly journals from developing countries. the major obstacle they have is making their materials visible, although by electronic means they are now able to do that. another obstacle has been getting their journals into libraries. they were advised a couple of years ago to take the licensing route and to negotiate with the libraries, and they quickly discovered that the big libraries have the advantage because they have the staff to negotiate a license. they also found the flip side to be truešthat these processes are also set up in favor of the big publishers, because they have sales people and lawyers, people who do nothing but licensing and negotiation. bioline international has a permanent staff of one and a half, so they just do not have the knowhow or the time to negotiate with libraries or to work out a system to get their material into the right places. they are interested therefore in getting advice on how best to work with the library community. dr. chan also noted that in the last year or so he realized that for small publishers with only several hundred members and very limited distribution, open access is in fact the only way to go. notforprofit publishers need to fulfill their mission to get the information and research out rather than to make a profit. there have to be other ways to recover the costs from other sources and activities, rather than getting stuck in the subscription model as a way to pay for the publication. a related lesson is that by controlling access, small publishers spend more money than they are able to recoup from the subscriptions. he would like to know from the other big publishers when they talk about the costs of it, how much money they spend on controlling access. bioline international has 24 journals right now, some from latin america, india, and africa, and a couple from southeast asia. the number is growing, but the area that is growing the fastest is in fact the openaccess journals, because, as they have been documenting in the last year and a half, the benefit of having the readership expand results in a lot more than just readership. it leads, for example, to submissions from authors outside their native countries, because the authors know that when they publish in these journals, which are peer reviewed, they will be read by their colleagues just the same. it is an aggregated database that will be open archives initiative (oai)compliant. bioline international needs to measure the return in terms of the readership and the impact, rather than the revenue that it can generate. it hopes to go back to the funding agencies that support these journals, to convince them that they are spending the money in a good way martin blume commented that the aps favors assistance to readers of the society™s publications in developing countries, providing free access to them like many of the other societies. he agrees that for developing countries, in general, open access is the best approach. moreover, there is an apparent diseconomy of scale between very small journals and very large ones, because the very large ones have all 13 see http://www.arl.org/info/frn/copy/texaco.html 65electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. of the apparatus of employment, medical benefits, a human resources group, and so on. the smaller organizations do not have that. one of the discussions that aps has had with a number of journals in developing countries is the possibility of their reprinting articles from the society™s journals that were sent in by authors from their country, so that they can give greater publicity to those authors and also greater publicity to their journals. there are not that many articles that come from the smallest of the journals, but it would be worthwhile helping them with their publications. problems encountered in the transfers of copyrights alan rapoport, of the national science foundation, noted that, based on jane ginsburg™s presentation, faculty may not even know who owns the copyright in the university. one can see the situation of faculty assigning the copyright to the publishers when they do not even own it. does that happen often and what are the legal ramifications? jane ginsburg responded that if an article is a work made for hire, then the author may in fact be selling the brooklyn bridge when signing the publishing contract. one way out of that bind is that a number of universities have their copyright policies in writing (see table 51) and require their faculty to sign a special agreement, or their employment agreement incorporates the copyright policy by reference. if the university does not assert copyright, or grants back the copyright in traditional academic scholarship to the professors, that transfers the copyright to the professors, and then the professors have something to give to the publishers. it is true, however, that the ambiguity about the actual status of faculty writing potentially affects a lot of publishing contracts as well. problems with the university workforhire approach to academic publications gordon neavill, of wayne state university, raised some more points in connection with the idea of academic work being done for hire. many academics move from one institution to another. in that case, which institution owns the copyright? if he is teaching courses at wayne state that he created earlier at the university of alabama, it would be hard to see how wayne state could claim copyright in lectures that originated elsewhere. the concept of academic freedom also suggests that the professor originates the concepts, develops them independently, and the work is not done at the behest of the university. all the university really requires is that the professor be productive, so how can the university claim copyright? jane ginsburg responded that these are all very good reasons why academic work products should not be considered work made for hire. as she noted earlier, there has not actually been a court decision (with the exception of some inconclusive decisions in the seventh circuit court of appeals) about whether congress perhaps unwittingly changed the law. the university of michigan used to take the position that everything was work for hire, but has recently changed that. 66electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 6. what is publishing in the future? introductory comments daniel atkins, university of michigan the digital revolution is disaggregating the traditional processes of many knowledgeintensive activities, but in particular the publishing or scholarly communication processes, and it is offering alternatives in both how the various stages of these processes are conducted and who does them. the various functionsšwhether metadata creation, credentialing review, or longterm stewardshipšcan be separated, disaggregated, and players different from those who traditionally have carried out these tasks can, in theory, perform them. the digital revolution is also changing the process by which knowledge is created, by which discovery takes place. this is most true within the scientific, technical, and medical (stm) arena. that is the central theme of a national science foundation (nsf) study, revolutionizing science and engineering through cyberinfrastructure, recently chaired by professor atkins.14 the report clearly documented an emergent vision and enhanced aspiration of many science communities in the use of information and computing technologies (ict) to build comprehensive information environments based on what is now being called cyberinfrastructure, such as collaboratories or grid communities. these are functionally complete, in the sense that all of the people, data, information, and instruments that one needs to do a particular activity in that particular scientific community of practice are available through the network and online. a growing number of research communities are now creating ictbased environments or cyberinfrastructurebased environments that link together these elements with relaxed constraints on distance, time, and boundaries. there is a general trend toward more interdisciplinary work and broader collaborations in many fields. publications now exist in many intermediate forms. we are moving toward more of a continuousflow model, rather than a discretebatch model. raw data, processed data, replays of experiments, and deliberations that are mediated through a collaboratory can be captured, replayed, and reexperienced. working reports, preprint manuscripts, credentialed or branded documents, or even postpeerreview annotated documents can now become available at varying times to different people with diverse terms and conditions. publications need not necessarily be precredentialed before publication on the net. as george furnas, at the university of michigan, says, their use on the net can itself be credentialing. in theory, every encounter with the document may be an opportunity to rank it in some way and to create some kind of a cumulative sense of its impact or importance. there could be alternative credentialing entities or methods, and you could pick your favorite. the raw ingredientsšthe data, the computational models, the outputs of instruments, the records of deliberationšcould be online and accessible by others, and could conceivably be used to validate or reproduce results at a deeper level than traditionally has been possible. the primary source data can be made available with a minimum set of metadata and terms and conditions. third partiesšparticularly in an openaccess, openarchives contextšcan then add value by harvesting, enriching, federating, linking, and mining selected content from such collections. 14 national science foundation. 2003. revolutionizing science and engineering through cyberinfrastructure: report of the national science foundation blueribbon advisory panel on cyberinfrastructure. national science foundation, arlington, va. 67electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the goal of this session is to try to illuminate and inform the discussion about some of these emerging technologies, the related social processes, some specific pilot projects, and the challenges and opportunities that may provide the basis for these kind of future ﬁpublishing processes.ﬂ this is put in quotes, because we may someday not actually think about it explicitly as a publishing process, but as more holistically integrated into the ﬁknowledge creation process.ﬂ implications of emerging recommender and reputation systems paul resnick, university of michigan when some people think about changing the current publication process to a more open system, they express concerns that scholarly communication will descend into chaos, and that no one will know what documents are worth reading because we will not have the current peerreview process. this idea should be turned on its head. instead of going without evaluation, there is a potential to have much more evaluation than we currently have in the peerreview process. we can look at what is happening outside of the scientific publication realm on the internet to give us some clues about where this could go. the democratization of review and feedback in today's publication system, there are reputations for publication venues. certain journals have a better reputation than others, and certain academic presses have strong reputations. there are a few designated reviewers for each article, and these serve as gatekeepers for the publication. an article either gets into this prestigious publication, or not; it is a binary decision. then afterward, we have citations as a behavioral metric of how influential the document was. we can examine some trends on the internet to see how they apply to the scientific publication and communication process. there can be a great deal of public feedback, both before and after whatever is marked as the official publication time, and we can have lots of behavior indicators, not just the citation counts. let us consider some examples of publicly visible feedback. many web sites now evaluate different types of products or services, and post reviews by individual customers. in the publishing world, many people are now familiar with the reviews at amazon.com, both text reviews and numeric ratings that any reader can put in. many of us find this quite helpful in purchasing books. we do not quite have it for individual articles in scientific publishing yet, but we do have it for books, even some scientific ones. even closer to the scientific publishing world, there is a site called merlot, which collects teaching resources and provides a peerreview process for them. there is a peerreview process before a publication is included in the collection, but even after it is included, members can add comments. typically, such comments are made by teachers who have tried using it, and they report what happened in their classroom, and so on. the member comments do not always agree exactly with the peerreview comments. these examples provide a sense of what is happening with subjective feedback that people can give on the internet, beyond the traditional peerreview approach. behavioral indicators with behavioral indicators, you do not ask people what they think about something, you watch what they do with it. that is like the citation count. for example, amazon.com, in addition to its customer reviews, has a sales rank for each book. another example is netscan, which is a project that mark smith at microsoft research has been doing to collect behavioral metrics on usenet newsgroups. it uses various types of metrics about the groups. google uses behavioral metrics of links in their page rank algorithm. many people check how they are ranked on google on various search strings. however, google is not just doing a text match; it 68electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. also takes into account how many links there are to the page from other web pages, and weights it by the rank of the other pages. so this is a ranking system that takes into account the behavioral metric of who is linking to whom. a final example is the social science research network that was described by michael jensen. it uses the download count as a behavioral metric. the main one is the alltime, ﬁtop 10ﬂ downloads, but it also has many other different top10 categories, so that more of the authors using the system have a chance to be a winner. issues in evolving credentialing processes the preceding examples provide some potential models for developing credentialing processes for scientific publication in the future. there are also some problems that require further thought. some of these already are active areas of research for people who are working on recommender and reputation systems. an obvious potential problem is gaming the system. you can make a bunch of web pages that all point to yours so that google will rank yours higher. in fact, there is a whole cottage industry that does this. you can hire consultants who will help you get higher in the google rankings. it is a little harder to do this with the amazon sales rank. it requires you to actually buy some books. no matter what the system, however, people try to figure out what the scoring metric is and game the system. this means that those who are designing these metrics need to consider it from the outset. the ideal metric would be strategyproof, meaning that the optimal behavior for the participants would be just to do their normal thing and not try to game the system, but it is not always so easy to design the metrics that way. another problem is eliciting early evaluations. in those systems where there is widespread sharing of evaluations, there really is an advantage to going second, to let somebody else figure out whether an article is a good one to read or not. of course, if we all try to go second, there is no one who goes first. yet, another problem can be herding, where the later evaluators do not really reveal what they thought of the document, but are overly influenced by what the previous evaluators thoughtšthey just go along with the herd. there are some interesting approaches that potentially would help with the herding problem. for example, you might reward evaluators for saying something that goes against the previous wisdom, but with which subsequent evaluators agree. that would be the person who finds the diamond in the rough and that person would get special rewards; the person who just gives random or tendentious reviews would get noticed and would get a bad rating. such a process also would require going back and revisiting some of the decisions that we have made about anonymity and accountability in review processes: singleblind, doubleblind, not blind at all. for different purposes different processes may be preferred. experiments to try in stm publishing some potential experiments are more radical than others. journal web sites might publish reviewer comments. the reviewers might take more care if they knew their comments were going to be published, even without their names attached. the reviews for the rejected articles could be published as well. there could be fewer really bad submissions if authors knew that they could potentially be hurting their reputation by having the reviews of their article up there. then, after publication, the web sites that publish fulltext articles or abstracts could let people put comments there that would be publicly visible. some other experiments might try to gather more metrics. projects such as citeseer in the computer science area measure citations in real time. one might also use the link and download to find out how many people are actually reading online, how many times an article is being assigned in courses, and other behavioral metrics. experiments in evaluating the evaluators are needed as well. the best place for this might be in some of the conference proceedings, where there are a number of evaluators for an individual article. one could examine the nature of the reviews in a more explicit attempt at evaluating evaluators. more attention, greater credit, and rewards need to be given to reviewers for evaluating early, 69electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. often, and well. publishers already are complaining about the difficulty of finding good reviewers. one reason is that reviewers™ services are not valued sufficiently in the system. we need some metrics that go beyond merely noting that someone was a reviewer or an associate editor at some journal, but that actually evaluate and give proper credit to such functions. we might start thinking about that as a contribution to research, rather than as purely a service line. in the promotion and tenure reviews for academics, the categories typically include teaching, research, and service, but this evaluation and commentary activity might really be thought of as contributing to the growth of knowledge. if some metrics could be developed on how much they are contributing in what way, we might think about that as a research contribution rather than just a service contribution. preprint servers and extensions to other fields richard luce, los alamos national laboratory the preprint service is something that is a wellknown, wellunderstood concept in the physics community, but not much in other communities. it is useful, at the outset, to distinguish between preprints and eprints. preprints have a ﬁbuyer bewareﬂ connotation to them in the physics community. they provide a means to obtain informal, nonpeerreview feedback that is weighted very differently in the community than a formal refereed report. they are a way to get an early version of an article out to colleagues to get some feedback, if any may come back, to help decide whether or not to publish it later. eprints, on the other hand, tend to be more polished papers deposited by authors in a public online archive in order to speed up the communication process and give authors more control over the public availability of their work. the eprint arxiv in physics back in 1991, paul ginsparg created the eprint arxiv15 at the los alamos national laboratory. that database archive, which today is at cornell, has about 30 or so fields and subfields and 244,000 papers. it has succeeded in large part because dr. ginsparg is a physicist. as a highenergy physicist, he understood well how that community works and what its needs are. his notion was to take and streamline the communication process relative to preprints. over the past decade, this approach has spread to some other fields, including mathematics, materials, nonlinear sciences, and computation. the adoption of this approach by other disciplines demonstrates that this is not a phenomenon only in highenergy physics or in the physics community, but can work in other research areas. it clearly has increased communication in the areas that it covers in physics. it is the dominant method for authors to register publicly when their idea first comes out, even though the article may not be formally published until six months later. according to dr. ginsparg, the cost is very low, and this is partially responsible for the wide acceptance of the system. there has been a continuing increase in submissions. the driver in the community clearly is speed, to make communication faster. the eprint arxiv has set an example for both society and commercial publishers to consider. it is significant to note that in 19951996, the american physical society (aps) began to accept preprint postings, and later began to link back to them. this was the beginning of a more formalized recognition that there was a role for this approachša bottom layer or first tier of scientific informationšand that we could have a twotier structure and start to link those things together. one might raise the question of the quality of the information in the eprint arxiv. these articles are not peer reviewed. if one does an analysis over a period of time of the quality of the submissions, however, what you see is a fieldspecific track record in terms of what actually gets published. in highenergy physics theory, about 73 percent of the papers in the archive turn out to be 15 for additional information on the eprint arxiv, see http://www.arxiv.org/. 70electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. published, whereas in condensed matter physics, it is only somewhere around onethird. consequently, it is fairly field specific, but it does indicate that the archive does not just contain articles that have no future or role in the formal publication system itself. what lessons can we learn from this? greater timeliness in the disclosure of research results is certainly one. another is that a few passionate people can make a difference. for a decade, while this system was at los alamos, it was typically run by three to five people, sometimes spending far too many hours, but inspired by the view that this was really going to change the world. that small number of people and the relatively small amount of money to operate the system (about a half million dollars per year) became a very dominant factor in terms of the community itself. most important is the lesson is that it addressed the common interests of a community in a sociologically compatible way, which is why the system worked for that particular community. other preprint server projects scholarly communication is a very complex ecosystem. clearly, not all fields are the same. the sociology, behavior, and traditions differ from field to field. consequently, this solution is not the correct or only solution, nor could it be expected to fit in all other fields. one needs to understand the community behavior and traditions and then look for models that meet those kinds of needs and requirements. nonetheless, there have been spinoffs into other fields organized by both universities and government agencies. cogprints, at the university of southampton in the united kingdom,16 is a preprint system that is quite well known in cognitive science. ncstrl, the networked computer science technical reference library, was an early effort to get computer science papers harvested together and then start to build a federated collection of that literature.17 ntltd, developed by ed fox at the university of virginia, provides thesis dissertations. within the federal government, the nasa national technical reports server (ntrs)18 was a pioneer in terms of trying to bring together and to make available a collection of federal reports, both in terms of metadata and the full text. pubmed central is certainly well known in the life sciences community. living reviews, (www.livingreviews.org/) is a somewhat different model in gravitational physics at one of max planck's institutes in germany, created a review that gets updated by the author over time. rather than going back to read a decadeold review that is static and wondering what has happened to the field since it was published, readers of living reviews know that it is current, because the authors have committed to a process of keeping the material they put into that online publication up to date. there are perhaps a dozen well known eprint systems with servers active today. there are around 100 other servers who claim to have some kind of an eprint system. they use a standardized protocol and allow people to come in and at least harvest some of the material that is collected there. one problem is the absence of an enabling infrastructure. the open archives initiative was not meant to be the endall, beall fix to the system. it sought a solution that would allow a disciplinespecific eprint archive to be able to talk to or communicate with other systems, so that users would have the opportunity to look at a pool of material. the problem is, how to ensure access to this variety of different systems. the protocol specifies the method by which material on the different systems can be harvested. we are just reaching the point where we are starting to see what kinds of interesting services can be put on top of that. that development perhaps has been slower than expected, but is now beginning to take off with a variety of different systems. one example is citeseer, mentioned by paul resnick. this begins to hint at the kinds of things that people might do in an open environment. implications for the future what do these developments mean beyond the physics community, and what new efforts can we 16 for additional information on cogprints cognitive sciences eprint archive, see http://cogprints.ecs.soton.ac.uk/. 17 for additional information on the ncstrl archive, see http://www.ncstrl.org/. 18 for additional information on nasa™s national technical reports server, see http://ntrs.nasa.gov/. 71electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. foresee? it is important to note that there has been very powerful opposition coming from traditional institutions that have used journal publishing as a cash cow, or from secondary publishers who see their secondary databases essentially as a birthright. for example, there was a lot of political pressure leading to the demise of the department of energy™s pubscience web portal because it was perceived as threatening to those other kinds of interests. a major question that needs to be addressed is how peer review will work in the preprint and openaccess context. how should we evaluate, recognize, and reward? another question that is related in part to the peer review issue is: what is the influence, and how do we detect it? there are a variety of methods that one needs to consider, essentially a composite approach. today we use citations as the sole indicator of influence, that is, an authorderived statement about what is important, what has influenced the research. we might look at a complementary path, which is the notion of reader behavior related to determining influence. digital libraries or service providers can provide analytical tools to generate new metrics based on user behavior, which complements or may even surpass citation ranking and impact factors. what is the problem with impact factors? to some extent, it is the lazy person's notion of how to figure out what is important in terms of journal ranking. it can be very convenient for publishers to state that their journal is ranked well in terms of impact factors. it also is relatively easy for librarians to justify why they buy one title over another title. the problem is that the citation is only an indicator of influence. there are many reasons that people might cite a paper: to show that they have read the literature in a field, to disagree with somebody and prove them wrong, to help a colleague get some visibility and enhance reputation, or to give credit to someone™s ideas. although impact factors are widely used to rank and evaluate journals they frequently may be used inappropriately. then there is a whole field of bibliometrics, which enables one to track science and to find out the interrelationships between authors, citations, journal citations, and the subjects that are covered. this is still an emerging field, but one that may become more extensively used. a better approach, however, might be to supplement the current system with a multidimensional model to balance bias. what would such a model look like? an ideal system might have the following elements: citations and cocitations to determine the proximity indicator; the semantics, or the content and the meaning of the content in articles to see how they are related; and user behavior, with regard to behavior metrics. at los alamos, about 95 percent of information resources are electroniconly, so it is possible to detect and determine communityspecific research trends and look at where those trends differ from the isi impact factors. however, this is quite complex and raises privacy concerns. finally, there is the problem of longterm curation, which has several aspects. the first element is the published literature, which is what most people think about in the context of preservation. there is also the issue of the relationships in the rich linking environment that one might want to collect and preserve over time, however, and make available in the future. institutional repositories hal abelson, massachusetts institute of technology this panel is supposed to be about this wonderful cyberinfrastructure future. one is reminded of william gibson, the outstanding cyberpunk writer, who 20 years ago gave us the word cyberspace. he said that he does not like to write about the future, because for most people the present is already quite terrifying enough. it is in that spirit that this presentation looks at the present. the changing role of universities in making their research output openly accessible the main action one should watch for is not in the new technology, but the possibility of new players finding institutional reasons related to their other primary missions to participate in a new game of disintermediate thy neighbor. in particular, do universities have institutional roles to play here other than what they have done so far, which is to be the place where the authors are? do universities have a 72electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. reason in their institutional missions to start participating in making their research output openly accessible? according to mit's mission statement (and probably that of many other universities), mit is committed not only to generating knowledge, but also to disseminating and preserving it. so how does a place like mit implement this mission? one initiative, opencourseware,19 is already getting quite famous. according to the mit president's report in 2001, the university has made an institutional commitment ﬁto provide free access to the primary materials for virtually all our courses. we are going to make our educational material available to students, faculty, and other learners, anywhere in the world, at any time, for free.ﬂ the reason mit did that is not that it was overcome by some fit of altruism; rather, it decided that given the way the world is going, it would be better for mit and indeed all universities, in terms of fulfilling their primary mission to educate students, to put their primary educational material on the web. the opencourseware web site is a prototype and has 50 courses up. there is a group at mit that is madly trying to get the first 500 courses online by september 2003. they are on a timeline to get all mit courses up by 2007. it is an institutional publication process, to which the university has committed as a permanent activity. the dspace initiative,20 which is the sister project of opencourseware, is a prepublication archive for mit's research. the difference between dspace and most other prepublication activities is that there is an institutional commitment by the mit libraries, justified by mit's mission to maintain it. opencourseware would make sense if only mit did it, but dspace cannot possibly be like that. dspace, in addition to being a preprint archive for mit, is meant to be a federation that collects together the intellectual output of the world's leading researchers. mit now has six institutional partners working with it. what is important about dspace is that there is a group of universities working out the management and sustainability processes in terms of their institutional commitments for how to set up such a federated archive. responses to the proprietization of knowledge both opencourseware and dspace are ways that mit and other universities are asking what their institutional role should be in disseminating and preserving their research output. why are these questions coming up now? why might universities start wanting to play institutional roles in the publication process, other than serving as places where the authors happen to be? the answer is that the increasing tendency to proprietize knowledge, to view the output of research as intellectual property, is hostile to traditional academic values. what are some of the challenges that universities see? they include high and increasing costs; imposition of arbitrary and inconsistent rules that restrict access and use; impediments to new tools for scholarly research; and risk of monopoly ownership and control of the scientific literature. the basic deal, as seen by universities, is that the authors, the scientists, give their property away to the journals. the journals now own this property and all rights to it forever. lifetime of the author plus 70 years is forever in science. if that regime had been in place 100 years ago, we today would be looking forward to the opportunity in 2007 to get open access to rutherford's writings on his discovery of the atomic nucleus. then the publishers take their property and magnanimously grant back to the authors some limited rights that are determined arbitrarily and totally at the discretion of the publisher. the universities, who might think they had something to do with this, generally get no specific rights at all, and the public is not even in this discussion. jane ginsburg already showed some examples of the restrictions imposed by publishers on authors, but it is most useful to mention some here. for example, some rights generously granted by reed elsevier to its authors include the right to photocopy or make electronic copies of the article for their own personal use, the right to post the final unpublished version of the article on a secure network (not accessible to the public) within the author™s institution, and the right to present the paper at a meeting or conference and to hand copies of the paper to the delegates attending the meeting. the rights 19 for additional information on mit™s opencourseware project, see http://ocw.mit.edu/index.html. 20 for additional information on the dspace federation, see http://www.dspace.org/. 73electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. generously granted to authors by the journals of the american chemical society are that authors may distribute or transmit their own paper to not more than 50 colleagues, and authors may post the title, abstract (no other text), tables, and figures from their own papers on their own web sites. the rights from the new england journal of medicine to its authors are even more limited: ﬁthe (massachusetts medical) society and its licensees have the right to use, reproduce, transmit, derive works from, publish, and distribute the contribution, in the journal or otherwise, in any form or medium. authors will not use or authorize the use of the contribution without the society™s written consent, except as may be allowed by u.s. fairuse law.ﬂ it is instructive to list some of the elements that are valuable for promoting the progress of science. they include quality publications and a publication process with integrity, certainly, but also: open, extensible indexes of publications; automatic extraction of relevant selections from publications; automatic compilation of publication fragments; static and dynamic links among publications, publication fragments, and primary data; data mining across multiple publications; automatic linking of publications to visualization tools; integration into the semantic web; and hundreds of things no one has thought of yet. will the information technology to support scholarly research be stillborn because everything is hidden behind legal and electronic fences? probably not, because it is too valuable and people are going to invest in it anyway. the more serious possibility is that the spread of these tools will be done in a way that stimulates network effects that will further concentrate and monopolize ownership of the scientific literature. if a search engine only searches the publications of one publisher, that becomes valuable enough for the publisher then to come in and do what the librarians call the big deal. as derk haank, ceo of elsevier science, noted recently: ﬁwe aim to give scientists desktop access to all the information they need, for a reasonable price, and to ensure that the value of the content and the context in which it is presented are reflected in the information provision. the information is made available to researchers under licenses accorded to their institutes, and they have all the access they wish.ﬂ does this mean that science will be restricted with monopoly ownership, or rather will we enjoy a system that participates through open standards? one impediment to openness is copyright. it turns out to be surprisingly difficultšin the wonderful legal phrasešto abandon your work to the public domain. it is even more difficult to specify some rights reserved, rather than all rights reserved, which is the default rule under copyright. the creative commons was founded recently to encourage people to allow controlled sharing of their work on the internet.21 to sum up, the world is disaggregating, and there is a big game of disintermediation going on. the place to look for access is not new technology. the places to look for the action are the new institutional players coming into the game. new technologies might enable new roles for the universities, and that will lead to the promotion of the progress of science. discussion of issues the role of peer review and other review mechanisms steven berry began by pointing out that there is one aspect of the refereeing process that paul resnick either dismissed or overlooked. reviews by and large have a lot of influence on what actually gets published. it is not a simple rejection process. of the papers that are reviewed and published, a very high percentage are revised as a result of the reviews. furthermore, we have to recognize that the review process provides only a very low threshold. it simply identifies material that is of sufficient quality for scientific discourse. so it is not necessarily a judgment of whether it is right or wrong. this function could be done in other ways, of course, but we have to recognize that in some fields, reviewing even at that low threshold is looked upon as a very important protection. the arguments between physical and biological scientists about this are instructive. physical scientists, as rick luce pointed out, are very ready to accept the archive model without review first, and use online reader review. the biological scientists are worried that without that low threshold review, information could be put online that might be dangerous to lay users. they feel that there is a large 21 see http://www.creativecommons.org. 74electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. audience of nonprofessional readers of biological, and especially biomedical, articles that simply does not have the judgment that the professionals have, and that it is simply dangerous. paul resnick responded by emphasizing that in considering these alternative review mechanisms, he views them as being in addition to peer review, not instead of it. we also could get more out of the peerreview process than we are getting now, if the reviews were made public. for example, you do not want to let bad information get out there for the general public to see without anybody from the scientific community saying it is rubbish. fine, put it out there along with the reviews of scientists who said it is rubbish. why is it better to just hide it than to disclose it with the commentary from the scientific experts? dan atkins read a question from a member of the webcast audience, who asked: might not the thread of the original version of a paper, along with reviewers' comments and authors' revisions or responses to the comment, as well as the journal editor's additional comments and interpretations, be used as educational material and enrichment for university students? this could be done either anonymously or by attribution. paul resnick said he thought it was a great idea. one of the problems for ph.d. students is that, more often than not, they do not get to see a paper through its whole process until they do it themselves and have cleared it the hard way. an adviser can show them the reviews submitted on the adviser™s papers, but this is not done routinely. making it a more public process would be helpful for education. rick luce added that there needs to be a proper balance in that discussion, whether we should let the system filter out the bad material or let the user filter it out. the role of the public press ted campion noted that one big player in scientific publishing that has barely been mentioned is the public press. scientific publication, particularly in biomedicine, is largely being judged now, at least by authors and even academe more generally, by how much press coverage it gets. it is not just studies of estrogen. zebrafish and hedgehog mutations are getting into the press. the scientific press is not only being judged now by the boring citation indices, but by whether the network news covers it. this, of course, is all being driven by the public's increasingly sophisticated understanding and interest in science, and in biomedical sciences in particular. what effect is this having on scientific and biomedical publication? hal abelson responded that he had a discussion with his daughter, who is in medical school, about mit's opencourseware, and she told him she thought it would be a terrible thing for medical schools to put their course curriculum on the web, simply because you had to be a professional medical student in order to evaluate and use that information, so it would be dangerous to have it out there. it is hard to know what to do about sensationalism in the public media. at the same time, you could do a lot worse than to have peter jennings talk about an article in the new england journal of medicine. it has been a tradition in the united states that the cure for speech is more speech. maybe if there were other channels for people to respond, things would be better, but restrictions on the press are not the answer. maybe part of the answer is that it is up to the press to worry about the novelty and up to the journals to worry about the authenticity. curation and preservation of digital materials mark doyle, of the aps, said that like mit, part of his society™s mission is explicitly the advancement in the diffusion of knowledge in physics. in addition to peer review, the society considers very important the responsibility to do the archiving. it has already gone through the transition, going to fully electronic publications. the core of its archive is a very richly marked up electronic file from which the print and the online material are all derived. what still appears to be absent in dspace or in the eprint arxiv are any efforts to build the infrastructure for curating that kind of material. it is one of the most important things to do. one other related issue is the low cost of arxiv.org. the key problem is that there is a two to three order of magnitude difference between what it costs the society or other publishers to publishš$1,500 per articlešand what it costs to publish in the arxiv. that is really where all the tension in the 75electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. economic models come from, when you have such large orders of magnitude difference. that is what puts all the pressure on people to change the way that things are done. rich luce responded that curation is an interesting question. most libraries are not going to be able to do this. there are probably one to two dozen libraries globally that see that as an important role, together with a significant capability in which to make an investment, because they are thinking about centuries of preservation, not years or decades. there are many publishers who are quite aware of the problem, but very few make the necessary investments. the vast majority of publishers are simply too small to have the wherewithal, both technologically and financially, to be able to pull it off. it is going to take some sort of a hybrid relationship between some publishers and some libraries, who see that as their role. hal abelson added that when mit designed the dspace, it was absolutely essential and deliberate that it be housed in the mit libraries. the reason was that whether or not the mit libraries will preserve something for 200 years, they are certain to preserve it for 50 years. the developers of dspace wanted to work with an organization that understood what archiving and curation meant, and that is what libraries do. to build on what he said earlier, the critical thing is not the technology for archiving, because that technology is well under control. rather, it is to find an institution that will commit, as part of its core mission, to keep and preserve the material. for example, if it is part of the core mission of the american chemical society to be the repository of all chemical literature and to have every other organization in the world be its franchisee, that is an important thing to say. it also is very important not to get trapped into the idea of building the monolithic, endtoend solution. dspace will never be that. it might be a place where people who are building peerreview and curating systems, and all kinds of other systems, can link to and build on. the trap, which was alluded to yesterday by gordon tibbitts, is that you do not want to be in a position where you build the whole system. instead, you want to have elements that are communicating through open standards, so that lots of people can come in at different places in the value chain and add value in different ways. evaluating stm literature outside academia donald king noted that in his studies of the amount and types of use of stm articles, approximately 25 percent of those articles are read by academicians, and the rest are read outside of the academic community. when you begin these feedback systems that paul resnick was describing, you need to think in terms of the enormous value that is derived from the literature outside academia. there are two purposes for doing this. one is that it is a better metric for assessing journals and authors. the other is that it also will begin to develop a means for the authors to better recognize that their larger audience is outside of their immediate peers. professor king added that he has done a lot of focus group interviews and indepth interviews of faculty, and they seem to think that they are writing only for the people they know, their immediate community. there would be some value in the assessment system, therefore, if there were some acknowledgment or recognition that there are other uses of that information outside academia. paul resnick agreed that providing such external feedback to the authors on how their works are being used would be useful. adapting tenure requirements to open source lennie rhine, university of florida, asked how universities adapt the tenure process to the opensource environment. most academics are tied to the peerreviewed journal system as a mechanism to be ranked hierarchically and to be evaluated in the tenure process. how do you incorporate this more ephemeral opensource information into that process? paul resnick responded that in the current tenure process, the department heads look at the journal rankings and count the number of journals in which you are published and cited, and different departments weight it differently. they may or may not construct a numeric score. one could actually develop a more open system for computing metrics like that. consider how u.s. news & world report does its rankings of schools; it has a particular way of weighting everything. now, imagine a more open version where we collect all the data, know what things have been cited and read, have all the reviews, 76electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. and have both the behavioral and the subjective feedback. then the teaching institution can create its own metrics based on its own tenure criteria, and the research institution can create a different metric, and you can have many different ways of using those data. hal abelson added that one of the marvelous things that the nsf did several years ago was to limit the number of papers that one can cite in preparing a grant proposal. it is not really a question of quantity or vast numbers of papers published in thirdrate journals. mit has been trying to get its teaching staff who are coming up for tenure to identify three things on which they should be evaluated. the goal is to try to get the enterprise focused on what alan kaye used to call ﬁthe metric of sistine chapel ceilings per century, rather than papers per week.ﬂ problems with metrics martin blume was happy to hear rick luce use the word indicators, rather than metrics, because there is no one number that can be used as a measure of quality. there are things wrong with all of them, including the opportunities for gaming the system. there is a dilbert cartoon that shows the human resources manager saying that metrics are very important, and a very good one is the rate of employee turnover. the manager replies that they do not have any turnover, they only hire people who could not possibly get work anywhere else. many metrics suffer from this, and they can be manipulated. you really have to look into them and use them as indicators, and it takes a fair number of them if you are going to get a fair measure of quality. open versus confidential peer review dr. blume also commented on peer review, and the concerns about public comment. although he thinks public commentary is a good thing to do, nevertheless there is a sort of gresham's law of refereeing, in that the bad referees tend to drive out the good ones. all of us who take part in listservs of one sort or another know of the loudmouth who will not be contradicted or denied, and eventually the rest of us give up and say we are not going to take part in this anymore. you need to expect something like this, and you have to have a degree of moderation in it. that is where an editor™s role in the peerreview process comes in. also, the knowledge that a paper is going to be peer reviewed does have an effect on authors. it means that they try to improve it at first so that it will pass this barrier. dr. blume then presented some statistics on peer reviews from the aps™s journals. the society looks at the first 100 articles submitted to one of its journals and tracks them through a year to see what has happened to them in the end. of the first 100 submitted in one of the years, 61 were accepted and the remaining 39 were rejected or recommended for publication elsewhere. of the 61 that were accepted, 14 were approved without any revision after one report, 22 were approved after resubmission after some modifications, 14 after a second review, and 4 after the third resubmission, all of these leading to improvements. some wags would say that the improvements for some of them are largely adding references to the referees' papers, but even that is an improvement if there is not enough citation of other work. of the rejects, 19 were rejected after one report, 13 after two, 4 after three, 2 after four, and 1 after six reports. it is much more costly and difficult to reject a paper than to accept it. this provides an indication of some of the value added in the course of traditional peer review. it is something that has to coexist with the other types of assessments. aps also tries to avoid using referees of the type that would lead to gresham's law such as one sees on the listserv. it is aware of them, and tries to select accordingly. paul resnick said that one way to evaluate the reviewers is to have an editor who chooses them or moderates. you also could come up with some system where you calibrate reviewers against each other. dr. blume noted that his society actually does this. it keeps a private file based on the reports it receives. unfortunately, this leads to an overburdening of the good reviewers, so they are punished for the good work that they do. dr. resnick pointed out that that kind of system that dr. blume is using privately and internally could be adapted to a more public version. if you go to a more public system, you do not necessarily 77electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. have to give all the lousy reviewers an equal voice. dr. blume added that his society would certainly want to pick people and would probably continue to do it anonymously. a reviewer is always free to reveal his or her identity, but the aps will not. relationship between open archives and traditional publishers fred friend asked hal abelson how he sees the future longterm relationship between open repositories and traditional publishers. one can see that repositories such as dspace have a very valuable role in shaking up the system and in helping establish or return to better priorities in scholarly publication, but what is the future role for traditional publishers in that situation? hopefully, publishers will respond in a positive way to these changes, and may come out in the end with a better role than they have at the moment. or could institutional repositories take the place of traditional publishers completely? professor abelson noted that it was yogi berra who said, ﬁit is really hard to make predictions, especially about the future.ﬂ the main point is that as you have new players, you have different kinds of roles. there is no inherent hostility between institutional archives and traditional publishers. mit, for example, has a very respectable journal operation in the mit press, and it is looking for ways to find joint projects with the dspace archives. one can imagine a university holding both the preprint through the edited version, and the journals coming in with some kind of authentication and review cycle. there may be lots of opportunities. the trick is to free up the system and allow other players to provide pieces of that process that the journals, for various reasons, have not been. the danger is for some individual player to come in and try to lock everything up in some complete system. the problem with the world wide web has always been that everybody wants to be the spider at the middle of it, and that is the outcome that we have to resist. dan atkins recalled the point made by rick luce that preprint servers are the repositories at the lower layer, and they provide a platform or an infrastructure on which a whole host of yet to be fully imagined valueadded entities could be built, some of them forprofit entities. the idea is to create a more open environment for the primary, or upstream, parts of the value chain, and then to encourage an economy of activity on top of that. one of the themes that came out loud and clear in the nsf™s recent cyberinfrastructure study was the huge latent demand for serious curation of scientific data, and mechanisms for the credentialing and validating of data and for encouraging interoperability between data in different fields as people create more comprehensive computational models. of course, there is a lot of synergy between that and some of the longterm preservation and access issues. in fact, the volume of bits that, for example, just the highenergy physics community generates in a year probably exceeds most of the scientific literature worth keeping. fred friend added that, traditionally, the formal publications have been viewed as being the record. yet we seem to be saying that longterm archiving is not for publishers. so that perhaps rules out the recordkeeping function for traditional publishers. what are they then left with? rich luce noted that an issue of deep concern to the governmental sector, and to public institutional repositories, is being able to have access to material created with public monies, and to make these materials publicly available. one can easily imagine a system again that is open at the bottom level, where there are some nuggets that perhaps publishers might look at and start to mine for opportunities to add value to on a more formal basis. these activities do not have to be competitive. in fact, they can coexist in a way that is very complementary. universal search engines for licensed proprietary content mark krellenstein, from reed elsevier, referred to hal abelson™s comments about derk haank's statement about reed elsevier producing a universal search engine for its licensed and proprietary content. that initiative came in response to the publisher™s users and the libraries. reed elsevier has found that people want as close to a universal search engine for proprietary materials as they have with google for the open materials on the internet. the idea of multiple players in the value chain, which appeals to expert researchers, frequently is 78electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. less appealing to undergraduates, in particular, who really want a single solution for all their search needs. reed elsevier does not expect to be that single solution, but it is trying to do as much as it can. it is charging for it because of the model it has for charging for content. google supports itself via advertising. that could be an option perhaps for a company like reed elsevier, but probably not in the scientific context to make the kind of revenue that is necessary, given other models discussed at this symposium. another point is that reed elsevier is also open to other players doing the same kind of thing. there is a metasearch initiative going on right now in the national information standards organization (niso), which is trying to respond to libraries' request to have a small number of services for proprietary content, rather than a long list of 60 providers. reed elsevier is working together with niso and other large publishers to develop open standards, so that any metasearch company could come in and search these proprietary services. finally, reed elsevier offers a service called scirus (www.scirus.com), which does provide access to some of that hidden content that google does not provide access to. it has scientific papers from the web, with 150 million web documents. it has all the reed elsevier proprietary content, plus whatever it can license from other publishers. most of the proprietary content is available for a fee. the abstracts are free, but if you click through to that content you go right through to the full text, if your site is licensed, and if not, there is a payperview model, at least for the reed elsevier material. what google has in fact done is to create a successful application for open content; however, something equivalent is not considered desirable, to some extent, on the licensedcontent side. improving assessments of publications donald king noted that there is another dimension that needs to be considered in assessing the materials that are published. that is the dimension of time following the publication of the articles, or the point of their availability in the preprint archives. the median age of a citation is approximately six to eight years, depending on the field of science. most of the reading, about 60 percent, that takes place is within the first year of publication, but almost all of that is for the purpose of keeping up with the literature and knowing what peers are doing. as the age of the material gets older, the usefulness and the value of that material ages as well, but about 10 percent of the articles that are read are over 15 years old. that is particularly useful information in industry, where people are assigned a new area of work that requires them to go back into the literature. such feedback on uses can help make the publications better. the issue of cultural diversity in publishing michael smolens, with a company called 3billionbooks, said that up to this point in the symposium he had not heard a term that might be referred to as cultural diversity. there are a lot of different cultures and language groups in the world that have a lot to say about many of the issues being discussed at this meeting. there was an organization founded in 1998 called the international network of cultural diversity. it was started by someone in sweden who got 30,000 artists, writers, and musicians together because they could not deal with the european union in their own language of swedish. this posed a problem for a broad range of interactions on many issues. their goal is to try to have the subject of very small cultures and language groups be heard at international meetings and consortia, so that when the world trade organization is dealing with trade issues, someone there is at least thinking about the fact that language groupings are disappearing very rapidly and cultural diversity should be maintained. the cultural diversity issue around the world is a very sensitive one that everyone needs to keep in mind when discussing publishing on the internet. 79electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 7. what constitutes a publication in the digital environment? introductory remarks clifford lynch, coalition for networked information in this session, we start taking a look at the issue of what constitutes a publication in the evolving digital world, with specific attention to the frameworks of science and journals. although we are focusing on science, there are a lot of developments in scholarship broadly beyond science, technology, and medicine. for example, the humanities have been very active and creative in exploring the use of the new media.22 as we look at this question of what constitutes a publication and how the character of publications changes, we switch our focus from the environmental questions of publication as process that the previous panel session discussed to how we author and bind together pieces of authorship into structures like journals. we can approach this from two kinds of perspectives. one is from the individual author's point of view. the practice of science is changing, as is well documented in the national science foundation (nsf) cyberinfrastructure report, for example. it is becoming much more data intensive. simulations are becoming a more important part of some scientific practices. we are seeing the development of community databases that structure and disseminate knowledge alongside traditional publishing. from that perspective, one might usefully ask questions about how people author articles, given the new opportunities technology is making possible. it is clear that articles can be much more than just paper versions by digital means. in fact, they are often printed for serious reading and engagement, and the journals still are using all of this technology around an authorship model that is strongly rooted in paper. there are trivial extensions that could be made, but there are less trivial extensions, too. we can, of course, add multimedia. moreover, not all of our readers are going to be human. software programs are going to read the things we write, and are not very bright sometimes, so you have to write differently for them. so this is one perspective, the author™s, that we can look at. the other perspective is that of the journal, of the aggregation of these articles, recognizing that the ecology in which journals exist has changed radically. it used to be that the other items in that ecology were other print journals and secondary abstract and indexing sorts of databases. now it has become very complicated. there are all kinds of data repositories. there are live linkages among journals. there is an interweaving of data and authored argument that is becoming very complex. these are the kinds of issues that we will have an opportunity to explore in this session. 22 see, e.g., roundtable on computing and the humanities, jointly sponsored by the national research council, the coalition for networked information, the national initiative for a networked cultural heritage, and the two ravens institute, which can be found at http://www7.nationalacademies.org/cstb/pubhumanities.html. 80electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the signal transduction knowledge environment monica bradford, science this presentation about science's signal transduction knowledge environment (stke)23 summarizes its history and current status, the specific issues related to defining what is a publication in the digital environment, and then how science has used its power as a more traditional publication to help move forward this less traditional project. the project was started in 1997. at the time, the staff at science thought it was a bold experiment. it was formed jointly by the american association for the advancement of science (aaas), stanford university library, and island press. the reason the three groups came together was that stanford university library was very interested in making sure that the notforprofit publishers and the smaller publishers were able to be players online as we moved into the digital environment. they also had started up highwire press. aaas had recently launched its electronic version of science and was excited about the possibility of working with stanford university press on new technology ideas. island press is a small environmental publisher, primarily of books, but they had ties to the pew charitable trust, which was interested in funding some kind of publishing experiment online. island press was helping to determine what the right area for that might be. they were particularly interested in the intersection of science and policy. of course, aaas was so excited about creating science online and all the potential that the online environment might offer, that it was eager to try something new. the goal of the knowledge environment was to move beyond the electronic fulltext journal model. the idea was to provide researchers with an online environment that linked all the different kinds of information they use, not just their journals, together so that they could move more easily among them and decrease the time that was required for gathering information, thereby giving them much more time for valuable research and increasing their productivity. why was signal transduction the first area that was chosen? the funders of the project wanted to find an area that would have the chance to become selfsustaining. therefore, science at the intersection with policy was quickly eliminated, particularly because so much of the literature in that area is actually gray literature, not digitized, and had unclear prospects. the project moved instead to an area where aaas and science, in particular, were very comfortable. signal transduction is a very interdisciplinary field within the life sciences. cell biologists, molecular biologists, developmental biologists, neuroscientists, structural biologists, immunologists, and microbiologists all come to a point in their research when they need to know something about signal transduction. there also were some business reasons, not necessarily cost or revenue, but the kinds of factors for which a publisher typically looks. it seemed there was a broad potential user base, with both industry and academia very interested in this topic. there was no primary journal at the time, with the information spread across a lot of journals, nor was there a major society. other aspects about this area of research and the kind of information in it were some of the most important reasons for the partners wanting to pursue it. the area of signal transduction is very complex and the information is widely distributed. it was important to be able to create links between these discrete pieces of information to help push knowledge forward in this area. it appeared there was the potential by making these links for substantial gains in practical and basic understanding of biological regulatory systems. in short, it was an ideal place for aaas to begin such an experiment, because it would reach across disciplines, and after all, that is what aaas is all about. the information in signal transduction had outgrown what could be done in print, and it really called out for a new way of communicating. one thing the stke partners were somewhat surprised to find out was that not only did they have to answer the question why signal transduction is important, but for business reasons they had to answer what signal transduction is. although it was very clear to researchers what it meant, in the business world, they had to explain why a library should care about a knowledge environment around this topicšthat a lot of their different researchers, schools, and departments would be interested in this. so, there had to be an education effort that went along with the marketing process. 23 for additional information about science™s stke, see http://stke.sciencemag.org/. 81electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the stke was the first of the knowledge environments hosted at highwire press. there are now five. aaas considers stke as part of the suite of products referred to as science online. one of the areas of knowledge environment that science online is moving into next is the biology of aging, because it has some of the same characteristics that were used for choosing signal transduction before: it is interdisciplinary, does not have a home base, and a need exists for scientists to talk across fields. the stke has both traditional and nontraditional types of publications and functions. in addition to providing access to the typical journal literature, science tried to create a community environment, so there is an area with tools that relate to that community, resources that scientists would use, and then the most interesting part, which is the various external connections. on the macro level, what is a publication? parts of stke are very familiar; they look like what you would think of as a publication, although it is really a collection of many items that used to be considered each as a separate publication. the stke has connected them all together in this environment, which is considered to be a publication itself. it has its own international serial standard number (issn); it is indexed by medline; and the updated reviews, protocols, and perspectives all have their own digital object identifiers. in many ways, that part of it is very familiar and very similar to a traditional publication, but then it was combined with these other aspects to make a larger publication. the scientific community will see more of this in the future, as publishers move away from just putting the content of print journals online and try to pull diverse sources together. the stke virtual journal has fulltext access to articles about signal transduction from 45 different journals, including some that are represented at this symposium. they are referred to as the participating publishers. when their journals go online at highwire press, the stke uses an algorithm that scans the content and selects the materials related to signaling, which the subscribers to the stke can access. the communityrelated functions of stke include letters, discussion forums, and directories. that has been the hardest part to develop. the initial presumption was that this would be perhaps the most exciting or interesting aspect, providing people with a place to talk across fields and to each other and with scientists that they do not normally see at their meetings, but in fact, that has been the most difficult part to really get going. other stke functions include learning services, reviews of the current literature with highlights of what is important, and custom electronic table of contents and personalizationšthe ability to put things in personalized folders, to save searches, and to relate resources. at the macro level, the stke has been accepted as a publication. it has 45,000 unique computers visiting a month, with about a quarter of those coming back more than once a month. there are 30,000 requests for full texts of original articles, 5,000 pdf reprint downloads, and 10,000 connection map pages viewed. within this larger publication there are parts than in and of themselves are a new kind of publishing and a new kind of publication. one of these is the connections map. this is basically a sybase relational database of information about the signaling pathways. the connections map is pathwaycentric, versus some efforts that are being done that are moleculecentric. in the long run, this will be an interesting way of synthesizing information and will have a lot of use. each entry within the database is created by an authority, who is solicited by the editors at stke. on top of this database there is a graphical user interface. to create this database, highwire worked with stke to create a software that is downloaded for the authority to work on to enter data into the system. as time goes on, the real value of this will be adding bioinformatic tools on top of this database to enhance the ability to find new and interesting information. the authorities that work with stke on this are willing to do so because science had existing relationships with them as authors. they trusted stke to put out quality content and to try to make sure that this kind of effort is recognized. they put this kind of effort into creating the stke because of the reputation of science and its reliability. during this time of experimenting, that is really important. at this point, the question is: is this effort moving beyond what would just be called a database entry to a true publication? for each of these pathways, the authors are supplying abstracts, and metadata are being created at the pathway level. the stke plans to submit those to pubmed and see what happens when it submits the rest of its information. 82electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. each pathway has an identified author, who is an authority, and who can work with a group. other people may be brought in because some of these pathways are so complex that more than one authority is needed. they synthesize and vet vast and sometimes conflicting literature. the stke edits the data in the database before it is released and approved, though this process is far from perfect, and it is more difficult than anticipated. these pathways are reviewed each year just before science publishes its special issue that focuses on the pathways. the review process is very hard, because one has to go through a lot of information. it is all networked. one of the things that the stke needs to develop is tools for the reviewersšhow to be able to know systematically what they looked at, how it is connected, at what point they saw it, and have some sort of printed output for them to look at, because navigating through the database is difficult. at the bottom of each information component, there is a date and time that it was updated. then, the ﬁviewpointﬂ in science provides a snapshot of the state of the knowledge in this pathway at the time of review. the pathways can be reviewed continually, however, if there are significant changes to the pathway or changes in the authorities that are involved. in summary, the stke is trying to create new knowledge and look for ways to explore the network property, the signaling systems that cannot be done in the print product. one needs to look for interpathway connections, to clear the pathways and look for networking properties, and then use this tool for modeling and integration with other bioinformatic resources. there are still many inputs that need work. the stke project plans to look for more funding to do some of these things. the goal is to make it easier for scientists to discover new knowledge and to be more productive. there have been some lessons learned in the stke experiment already. the definition of a publication is evolving. the researchers understand how this information can work for them, and they are excited. that is the best part of this and what makes it fun, because they say what they need. efforts at standardizing data input and control vocabularies have been difficult. nih tried to help with this, but the standards are not evolving. someone will have to just start doing it, and from there the standards will start to take hold. the reward system is not yet in place for those people who are doing this kind of authoring. for that reason, the stke felt that it had to link this work to the traditional mode of publication during the initial transition phase, so that the contributors get credit. finally, the ﬁviewpointsﬂ in science do get cited. they are in pubmed and they draw attention to the site. hopefully, over time, we will see the pathways themselves cited more than just by science. publishing large data sets in astronomyšthe virtual observatory alex szalay, the johns hopkins university why is the publishing of large data sets an issue? scientific data are increasing exponentially, not just in astronomy, but in science generally. astronomers currently have a few hundred terabytes derived from observations made in multiple wavelengths. very soon, however, they will start projects that will reobserve the sky every four nights, to look for variable objects in the temporal dimension. at that point, the data will increase to a few petabytes per year. astronomers are searching for new kinds of objects, for interesting ones that are atypical. they are also studying the properties of the typical objects, and for every object they detect in the sky they derive 400 different attributes. the bottom line is, the volume of data doubles every one to oneandahalf years. astronomy, as most other fields of science, operates under a flat budget, so astronomers spend as much money as they get from their funding sources to build new observational tools and computer equipment to get more data. the data in astronomy typically become public after about one year, because there is an initial proprietary period of use for the people who build the observing instruments and who schedule observing time, but after that the data are released. everybody can see pretty much the same data that are publicly available. how will astronomers deal with this? the transfer of a terabyte of data at current internet speeds takes about two days, but when we get to a petabyte, it will take years to do anything with the data in the 83electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. networked environment. this is already on the horizon, and, by the way, a petabyte currently would be on 10,000 disks. consequently, astronomers need to create databases that are searchable so they do not have to look at all the data at once, and can use indices to find what they need in the data. they also can do searches in parallel that way. the driving force for astronomers is to make discoveries. they are trying to go deeper in the universe and also trying to explore a broader range of wavelengths. there is an interesting parallel to metcalf's law. robert metcalf worked at xerox parc and is one of the inventors of the ethernet. he postulated that the utility of computer networks grows not as the number of nodes on the network, but as the number of possible connections we can make. the same thing is true with data sets. if we have n different archives, we can make order of n squared connections between those different data sets, so there are more things to discover. that is the utility of federating data sets. the current sky surveys have really proven this. astronomers have discovered many new objects, where they combine multiple surveys in different colors. there also is an increasing reuse of scientific data, so people are using each other's data for purposes that they were not necessarily originally intended. the data publishing in this exponential world is also changing very dramatically. all these big astronomy projects are undertaken by collaborations of 60 to 100 people, who work for 5 or 6 years to build an instrument that collects the data, and they then operate it for at least that long, because otherwise it would not be worth investing that much of their time to do it. once they have the data, they keep using them and eventually publish them. they put the data in a database and make them accessible on their web sites. when the project ends, the people go on to other projects, and at that point they are ready to hand over the data to some big national archive or centralized storage facility. after that, the scientists are still interacting with all the data. why are the roles changing? the exponential growth makes a fundamental difference. first of all, because these projects last six years, or more, at any one time the data at the national data facilities are only going to hold about 12 percent of the data that have been collected, and everything else still remains with the groups of scientists. this is very different from the previous linear progression of data collection. there is also more responsibility placed on the projects. the astronomers and other scientists are learning how to become publishers and curators, because they do not have a choice if they want to make their data public. this is professor szalay™s situation. he was trained as a particle physicist, who turned into a theoretical cosmologist and then became an observational cosmologist, because that was where the new breakthroughs were occurring. now he worries about data publishing and curation, because this is necessary to do the science. he spends an increasing fraction of his budget on software, and in many cases reinventing the wheel. more standards and more templates would help with this. luckily there are many emerging concepts and developments that help. one is that it is becoming easier to standardize distributed data, for example, using xml. there are web services emerging, supported on many platforms, that make it very easy to exchange data, even complex data. there also is a major trend in making computing more distributed. this is called grid computing, where the computing is distributed all across the internet on multiple sites, and people can borrow time on central processing units (cpus) whenever they need it. the people who talk about grid computing, however, tend to think only about harvesting the cpus; they do not think about the hundreds of terabytes or possibly petabytes of data behind it, because we currently lack the bandwidth and cannot move the data to the computers. if you need huge amounts of data where every byte needs a little bit of computing, it is not efficient to move the data to the computer site. it is more efficient to move the computers and the analysis to where the data are. essentially, we now have an intercommunication mechanism, and a lot of what is done in grid computing also applies in this distributed work, which is growing in an exponential way. it is also getting exponentially more complex. the threshold for starting a new project is getting lower and lower as the threshold for the hardware is getting cheaper. professor szalay got into this through the sloan digital sky survey, which is sometimes called the cosmic genome project. it is one of the first big astronomy projects that is set up in that mode, to map the sky not just to do one thing, but to try to create the ultimate map. there are two surveys being done. one is taking images in five colors and the other is trying to measure distances. there is quite a lot of software involved. at the time when this project started, which was in 1992, 40 terabytes of raw data looked like an enormous amount. today it does not seem like such a big deal. in one night the 84electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. survey takes a 24,000 by 1 million pixel image of the sky in five colors. when it is finished in 2005, there will be approximately 2.5 terapixels of data. the project is also trying to measure distances so that through the expansion of the universe, astronomers can figure out the distance of the galaxy. for about 1 percent of the objects they are trying to get very detailed information. the data flow is such that they take the data on the mountaintop in new mexico and ship the tapes via federal express to fermilab in batavia, illinois, because the networks are simply too slow to move that much data around. then they process the data and put them into an sql database, which currently has about 100 million objects in it, and will have about 300 million when the project is completed. professor szalay and jim gray, of microsoft research, have begun a project to make these complex data understandable and useable by high school students. they opened a web site in 2001, and after two years they had about 12 million page hits, and now get about 1 million page hits per month. it is used by high school students who are learning astronomy and the process of scientific discovery, using uptodate datašdata that are as good as any astronomer can get todayšand also by professional astronomers, who like the ease of use. the project identified other issues that they are just starting to appreciate. after they released the first set of data, which was only about 100 gigabytes, they realized that once they put out the data, people started writing scientific papers about them. they are putting out the next data release in the summer of 2003, close to a terabyte, but they still cannot throw away the old data set, because there are papers based on it. someone may want to go back and verify the papers, so whatever they put out once is like a publication; they cannot take it back. they have a yearly release of the data on the web site, which is like a different edition of a book, except that the book is doubling every year. the project also brings up other interesting aspects. once the project goes offline, the databases will be the only legacy. most of the technical information is going on in email communication. so they also have to capture, for example, all the email archives of the project. they should not delete them, but rather add them to the database, because this will be the only way that somebody years later can figure out what they did with some subject, or a certain part of the sky. as jim gray says, astronomy data are special because they are entirely worthless. he means this in a complimentary and good sense. he works for microsoft, so he does not have to sign disclosure agreements and bring in lawyers if he actually wants to play with some astronomy data. there are no proprietary or privacy concerns. you can take the data and give them to somebody else. they are great for experimenting with algorithms. they are real and well documented, spatially and temporally dimensional. one can do all sorts of exercises with them that one has to do with commercial data, but without being sued. astronomical observations are diverse and distributed, with many different instruments constantly observing the sky from all the continents, in different wavelengths. the questions are interesting, and there are a lot of data. this all adds up to the concept of a ﬁvirtual observatory.ﬂ szalay and gray were struggling with their data publication process, based on a relatively small survey. their colleagues at the california institute of technology, the space telescope science institute, the nasa goddard space flight center, and various places were all doing the same thing. they all decided that it is much better to try to do it together because, eventually, astronomers would ask why it does not work together. when they created the concept of the virtual observatory, the vision was to make the data integration easy by creating some standard interfaces and to federate the diverse databases without having to rebuild everything from scratch. they also wanted to provide templates for others, for the next generation of sky surveys, so astronomers could build it the right way from the beginning. this idea has taken off. about a year and a half ago, nsf funded a project for building the framework for the national virtual observatory, which involves all the major astronomy data resources in the united statesšastronomy data centers, national observatories, supercomputer centers, universities, and people from various disciplines, including statistics and computer science. this project has already developed some demos, which led to some unexpected discoveries. it is also now growing internationally. this effort is now being copied in more than 10 countries, including japan, germany, italy, france, the united kingdom, and canada. today, all these projects are operating with a funding of about $60 million, and there is really active cooperation. in late april there was a oneweeklong meeting in cambridge, england, about the standardization effortsšwhat are the common dictionaries, what are the common data exchange formats, 85electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. how to do common registry formats that are oai compatible, and so on. there is now even a formal collaborationšthe international virtual observatory alliance. publishing this much data requires a new model. it is not clear what this new model is, however, so astronomers are trying to learn. there are multiple challenges in the use of the data for different communities. there are datamining and visualization challengesšhow to visualize such a large, distributed complex set of databases. there are important educational aspects of it; students now have the ability to see the same data as professional astronomers. and there is very much more data coming, petabytes per year by 2010. astronomy is largely following the path of particle physics, with about a 15year time delay. particle physics also grew through this evolutionary phase. it will probably last for the next 10 or 15 years, until the next telescope will be so expensive that only the whole world together can afford to build one, as happened with the cern lhc accelerator today. until then, there will continue to be this data doubling. indeed, the same thing is happening in all of science. we are driven by moore™s law, whether highenergy physics, genomics, cancer research, medical imaging, oceanography, or remote sensing. this also shows that there is a new, emerging kind of science. when you think about how science began, it basically was very empirical. it started with leonardo da vinci, who did beautiful drawings of turbulence and described the phenomena as he saw them. then through kepler, through einstein, people wrote down the equations that captured in the abstract sense the theoretical concepts behind the phenomena and the natural world, and provided a simple analytical understanding of the universe around us. then a computational branch of science emerged over the past 20 or 30 years, and what we are faced with now is data exploration. we are now generating so much data, both in simulations and in real data, that we need both theory and empirical computational tools, and also information management tools to support the progress of science. genomic data curation and integration with the literature david lipman, national institutes of health/national center for biotechnology information24 david lipman recently met with jim gray and alex szalay. a stimulating discussion on the similarities and differences between biomedical research and astronomy ensued. as alex szalay noted, one of the driving forces for most scientists, certainly those in biological research, is that science is becoming more data intensive. this means that researchers are generating more data for each paper, but they are also using more data from other scientists in their own research. that has an impact on both the factual databases and the literature. many scientists believe that in order to make the most of these resources we will need to have deeper links and better integration between the literature and the factual databases. this includes improving data retrieval from both types of database, improving their actual usability, and maximizing the value that can be extracted from them. the quality of the factual data can be very much improved if tighter integration can be achieved between the literature and the databases. the growth in the number of papers in pubmed and medline is substantial, but it is basically linear. if we look at a number of factual databases, however, in most areas of biology the amount of data is increasing exponentially. for example, this is true of both the number of sequences and the total number of nucleotides in genbank. an example of postgenomic research that generates a lot of data is proteomics. if you look at the number of proteins reported as identified in earlier studies, and compare this to the number reported in more recent articles, the amount of data generated with each paper is increasing. most universities now have a variety of proteomics core services doing mass spectrometry that produces a lot of data. in the area of expression analysisšthe kind of work that pat brown has pioneeredšone can see the same kind of pattern. there are now many more labs doing this work, but also, any given lab can generate more data points per paper as the cost of doing this kind of experiment goes down. for example, an expression analysis experiment on the human genome may use a chip that monitors the 24 see http://www.ncbi.nlm.nih.gov/ for additional information about the ncbi. 86electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. expression of a few thousand to perhaps 20,000 genes over a range of conditions. hundreds of thousands of data points could be generated that are relevant not only to support the results reported in a scholarly paper, but it would also be useful to make those data available so that others can learn new things. not only are scientists producing more data, but they are also using the data generated by others for their own research. there are more than 400,000 users per weekday at the national center for biotechnology information (ncbi) web site, and this number is growing; most of these people are using data to design experiments. as well as accessing information though direct searching of databases, papers in electronic journals now have many links to, or at least cite, the identifiers of databases records that the authors used in writing their papers. supplementary data files are also increasingly submitted with research articles, and these too are frequently linked back to the source information. a typical functional genomics approach, such as gene expression analysis, generally requires a range of genomic data to set up the experiments  sequence data from a number of transcripts or genomes are needed to design microarrray chips. in proteomics, it is essential to be able to compare mass spectrometry data against a number of genomic data sets in order to interpret them fully. another important result of functional genomics processes is that the researcher will often generate a new kind of data. in proteomics, the new data might be on interacting proteins. these data may also be relevant to the very databases that were used to design the experiments. for example, a researcher doing proteomics in a microbe may get mass spectrometry data that confirm that certain proteins are actually translated and found in the cell. that information needs to be transferred back to the relevant databases so that researchers using the databases in the future will know that some previously hypothetical genes are now confirmed, and are translated or expressed. a related point is that if the researcher who generates these data keeps them only as supplementary data files on a personal server, then they are not going to be structured consistently, and therefore the data are not going to be as useful as they would be in a public database, where data are structured and normalized. for expression analysis experiments, some journals are just beginning to require submission of the data to various archives. it is very important to keep in mind this cycle of feedback between source data and publications, because it will affect the way the literature is presented in the future. at pubmed central,25 which is the national institutes of health (nih) archive for the biomedical literature, there are many links and other functions. for example, it is possible to link from the cited references in a fulltext article to the appropriate pubmed records, as well as to a variety of other databases. one can now also look computationally through the text for matches to known organism names, and if the names appear in certain places in the article, for example, a methods section, this increases confidence that the paper contains relevant information with respect to that organism. by having this fairly fine level of integration and links between the literature and factual databases, the article has a higher value: not only can the reader better understand the point that the author was trying to make, but he or she can also go beyond that point and look at newer information that may be available. discussion of issues subscription model as a restriction on use of the stke david lipman began the discussion by noting that because the stke is proprietary and requires a subscription to access it, people do not link into it as much as they might otherwise. it is a limitation when you have some of these factual resources that not everybody is able to get into. monica bradford responded that the connections database is free and anyone can use it. the other parts of the site are behind a subscription wall, those that are more your typical journal items. one of the ideas behind the stke was to see if you can support the whole combined environment while keeping the database itself free. that is what all the authorities want. the data most likely also will have value for drug discovery. one of the ideas is to see if stke 25 for additional information about pubmed central, see http://www.pubmedcentral.nih.gov/. 87electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. can license the data for use by pharmaceuticals companies so they can integrate them with their own databases on a proprietary basis, but providing enough support for the overall site to allow it to remain free for academic use. so far, most of the stke™s experts, the authorities, are comfortable with that model. it is a little early to say how much traffic stke will have, but it is expected to grow a lot. aaas also has an nsf grant for biosite ednet, which is trying to take online resources and make them available, or make it easy for instructors to find them to use them in their course work. the stke has adopted the same metadata that are being developed for that program. this has allowed stke to extend its audience from primarily researchers, who were the initial focus, to incredible use in education in undergraduate courses. the need for federal coordination and investment in the cyberinfrastructure dan atkins said that the three excellent presentations in this session were representative of the 80 or so exciting testimonies that his committee had as part of the cyberinfrastructure investigations at nsf. he would like to encourage the scientific community to get behind nsf to do something bold in this area. first, the exponential data growth is now present in many fields. it illustrates that the challenge and opportunity includes going to higher performance networks, higher speed computers, and greater capacity storage, but to do that together with another dimension that he mentioned earlier, and that is functional completeness, by having the complete range of services. the challenge involves this balance between increased capacity and increased function. a second point that the preceding talks illustrate is the exciting potential for multiuse technologiesšthe fact that the same underlying infrastructure is serving the leading edge of science, and making the learning of science more vivid, authentic, and exciting, all at the same time. although a major investment is needed to create this infrastructure, once it is created, as the astronomy example illustrates, leadingedge teams or individual amateurs can make seminal and important contributions to science provided they are given open access to these data and to the tools. finally, both the opportunities and the challenges illustrate the urgency for some leadership in this area. the various research communities are already doing this, and they are scraping together the resources. cosmologists are becoming data curators, and so on. people are putting extraordinary efforts into it, and that is very commendable. at the same time, if we do not get the right investments or the right synergy between domain scientists and librarians and information specialists, we could end up with suboptimal solutions, solutions that do not scale, and worst of all, we can end up with balkanized environments that cannot interoperate and result in enormous opportunity costs. so this is a plea for the nih, the department of energy, and the other agencies to cooperate and try to create the kind of environment that is going to allow these research opportunities to prosper on a grand scale. quality control and review of data in very large or complex databases paul resnick asked about the quality control or review process for the data that gets into the very large or complex databases. can anyone put their data in? alex szalay responded that typically in astronomy a threshold is emerging. the condition for putting contributed data online is that they are provided with adequate metadata in certain formats. this keeps a lot of people out whose data are not of high enough quality, and who have not documented them sufficiently. this issue was given a lot of consideration, but they did not want to institute a formal refereeing process. they will probably introduce an annotation system eventually, where people can feed in comments and annotations. david lipman noted that in the databases at ncbi, and typically for molecular biology in the life sciences, there are databases such as genbank, where authors directly submit sequence data into an archive. when that process of direct office submission started about 20 years ago, the sense was that people were going to put in all kinds of makebelieve data and so forth. actually, that does not happen; although some data are redundant, and some versions are of higher quality than others. that has given rise to related databases, which are curated, some by expert groups on the outside, others by the database managers themselves. so, for example, ncbi curates the human sequences along with some others, such as mouse sequences. there is a comprehensive set of curated 88electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. sequences. for the molecular biology data, there are two related sets, the archived set that represents what scientists provided at the time and the curated set that contains what is supposed to be the best version of the data. those two data sets work together well, because sometimes what some experts think is correct in the curated set turns out to be incorrect and something that was deposited originally may be more correct, so there are pointers to the older versions. monica bradford noted that the stke is more of a curated database. the pathway and all the related components are externally reviewed by people outside of the group of authorities that created it and did the data entry. that is a snapshot in time, however, and those pathways and data entries need to be constantly updated. right now, the only way that someone can comment during the period between the formal reviews is either by directly sending an email to the authority, which one can do right from the graphical interface, or by using the feedback function on the web site, in which case everyone sees it. the stke will provide some additional tools to improve the review process and also to allow for more community annotation over time, as long as it remains clear which portion is the community annotation and which is the official part that has gone through the review process. a combination of these two approaches will add the most value over time. david lipman raised one other point that relates to the difference between astronomy and biology. astronomy has an organizing principle, in that there are spacetime coordinates, which is largely agreed upon. although there are a number of different coordinate systems that are used, they can be resolved largely to one. given that there is a stronger theoretical base in astronomy from physics and there is this natural organizing principle, it is possible with a variety of computations to actually assess some aspects of how good the data are. in biology, at the level of the genome, transcripts, proteins, and, to some extent, protein structure, there are organizing principles that are natural and strong enough to allow detection of data anomalies that do not make sense. the information in a database fits together in a certain way, and as one gets more transcript information or more comparative data for proteins, one can see better how it fits together. above the level of the genotype and perhaps protein structure, however, with expression data, pathways, and proteomics there are not such natural organizing principles. one of the difficulties of a project like stke and other functional genomics projects is that it is much more difficult to use crossvalidation to fully assess the quality of the data. because there are highthroughput methods in biology that are at the level of function, it is a challenge to deal adequately with quality. monica bradford added that one of the reasons the stke took the approach it did is because all the tools are not yet available. people are trying to make connections across different disciplines. a cancer specialist may find an oncogene that turns out to be in a signaling pathway. they may be looking for information in literature or data that they have not followed before. the value of having scientists, university libraries, and the stke editorial processes associated with the curation is that it helps build some trust in the information when the more automated tools that david lipman mentioned do not exist. you can also link to other archival and curated databases elsewhere, but at a certain point, when you are pushing the edges and trying to gain new knowledge, you have to figure out what you are going to trust. that is what the stke hopes to improve. professor szalay also observed that as the data grow exponentially, over a period of time they will grow by a factor of 100. the capacity of computers will also grow by a factor of 100, so if the organizing principle is to save every bit of data collected, one can keep up. if the problem is to connect every bit of data to every other bit of data which is n squared, there will be 100 times more data and the computers will be 100 times faster, but the computational problem will be 10,000 times larger. we are starting to approach this. martin blume said he was struck by the connection between a number of things that have been discussed here and things that have happened in the past. it is useful to look at how we got here, and perhaps to extrapolate into the future. the eprint arxiv grew out of the preprint archives in print back in the 1960s that followed the xerox revolution at that time. looking back particularly from astronomy to particle physics, there were experiments done in the 1960s with bubble chambers, and there were many photographs taken. those data represented the equivalent of the current sky survey, because they could be used not just for one discovery, but for metaexperiments that were done on that. in fact, one can see the same thing now happening with the astronomical data, where one can reuse them for many different 89electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. experiments or analyses that yield new data into the future. the particle physics data were available freely after a period of proprietary use by the investigators (as are the astronomy data today) to others who could use them for new ideas and new experiments. of course, the digital technologies have changed this dramatically, but it still looked like a massive job in the 1960s. there was one paper where the omega minus was discovered, and 33 people had collaborated on it. it was a sign of things to come. datamining restrictions from proprietary stm information linda rosenstein, at the university of pennsylvania, asked how we might automatically download, create, and/or centralize a repository of identified articles if the intent is to extract data and republish subsequently extracted facts. as the extracted data would in no way resemble the original text and would also have pointers to their sources, this should not pose a significant infringement issue. the university of pennsylvania scientists want to do something that is scientifically quite new, and they think it is very important for cancer research. they know there are various restrictions on the access to the data and the use of the data that their library has licensed. how can they undertake this incredible datamining process, which presumably will have great results in science and perhaps even in medicine, when they are still subject to the proprietary model of how the scientific literature is made available to them? david lipman said that is a good question, whether the current model of fee for access fits how the literature and the factual databases need to be used. however, if the activity in question involves textmining work based on looking at papers, that will not be useful. if, instead, the researchers want to mine the organized data sets associated with the literature, and would use the two together, that is an issue. it is one of the issues with which the publishing community will have to contend. there is clearly a tension between the current model and how scientists want to work, but it is not clear where it is going to go. pubmed central is a service in which the publishers volunteer to participate. they are not necessarily providing the newest versions of their information into it, but scientists request subsets or entire sets of pubmed central data to compute on locally and to search for discoveries. some of the publishers that participate in pubmed central, upon request, allow the download of this information. as pubmed central begins to get more openaccess journals, it will be able to provide the material for download automatically, just as it does with the sequence data. the more literature that is provided under open access the more that allows for new directions in terms of how the literature and the data sets are used, and how they are developed in the first place. pubmed central recently interacted with some radiologists who were interested in creating a special database of radiological images, which would be useful for education and training. clearly, if the journals were open access, then it would be a very natural thing to put the two together. this is a challenge that both the publishers and the scientific community are going to have to face. monica bradford said that this tension is good. it helps publishers to think about what they are doing and the basic goals they have. the purpose of stke is to help researchers be more efficient. it is supposed to advance science and serve society, and stke has the potential to help do that. the tension makes publishers rethink their model and experiment with new approaches. it is an evolution. hopefully, it will be possible to come up with some creative ways that will work in the marketplace, and not be totally dependent on government control or government funds. publishing large and complex data sets for knowledge discovery participant eileen collins pointed out that, presumably, the methods for organizing and labeling the huge data sets reflect current knowledge in the field. does the availability of all these wonderful data to so many people enable researchers to make quantum leaps in their knowledge of phenomena? or, is there a risk that because the data are organized according to what we understand now, it might be tempting just to stay in that vineyard and not make big advances or face big changes in modes of thinking? how might this issue be addressed, particularly for disciplines that are not as far along as those that are putting together these huge data sets? alex szalay responded that in the sloan digital sky survey, which is now about 40 percent ready, 90electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. after two nights of operation astronomers found 6 out of 10 of the most distant quasars in the universe. this is a small telescope by astronomy standards, but it has proven to be efficient in finding new objects by organizing the data and scanning through the outliers. is it the only way to actually store the data? the answer is, clearly not. with these large data sets, the only way to make sure the data are safe is to store them at multiple sites around the world. if the mirror sites are organized slightly differently, each enabling certain types of functions, and the queries are redirected to the most appropriate place, it might improve the capabilities. david lipman said that for databases like genbank and for most of the factual databases in the life sciences, there are multiple sites. genbank, the u.s. human genome sequence database, collaborates with similar centers in japan and the united kingdom. the data are exchanged every night, but they have different retrieval systems and different ways of organizing access into them. furthermore, people can download large subsets or the entire database, and do whatever they want, including making commercial products. with the stm literature, if there were more openaccess journals and multiple sites that provided comprehensive access, one would see different capabilities. pubmed central is working with a group in france to set up a mirror archive that would organize those data in a different retrieval system. open access to the data allows for multiple archives that are comprehensive and provides different ways to get at that information. dr. collins™s question raised a deeper issue, however. in biology, gene ontology is a way to make it easier for people to see what is there, and to move the information around and to understand it. this represents a tradeoff between what we understand now and the kind of new discoveries that alex szalay refers to, which change that. there is a tension between reductionism and computing from the bottom up and learning new things, and being able to say this is what we know right now, and having that superimposed from the top down. right now, there is a huge amount of interest in ontologies in biology, and some of it may be misplaced. one of the reasons researchers focused on molecular biology was that they really did not understand enough from the top down. if you look at proteins or genes that are involved in cancer, you find those that are a part of glycolytic pathways, and so forth. it is not clear how much these ontologies assist in finding and understanding things, and how much they obscure new discoveries. as long as we maintain access to the data in a very open way, and people can download and do almost whatever they want with them, then if they want to ignore something like an ontology, they can do that. monica bradford agreed with david lipman, but also made a few observations that are not quite as global, but based on the experience with stke. the research in signaling started out looking at things very literally. a researcher followed a signal down one pathway, and soon realized these were really networks; one could not think about one pathway, but had to think about all these pathways together and how they intersected and affected each other. this could not be done in print. so the ability to build a database and be able to represent these and then query across the pathways was very useful. aaas would like to develop bioinformatic tools that will actually let researchers look for interpath connections and find new knowledge. ms. bradford hopes that eventually these tools will allow researchers to play with the stke and add in their own information to see if it changes the effect or has an effect on the pathways. the information is vast, and tools are needed to help organize it. once the organization of the information is taken care of, it gives researchers a chance to begin to think about new things and to look at the information differently. this process can be found in many other disciplines. clifford lynch noted that another striking example of that is the migration away from surrogates to full texts that allow computation. that capability is having a radical effect in many fields. it is useful to be able to find things by doing computation and searching on the full text of scholarship, as opposed to being locked into some kind of a classification structure for subject description that some particular group used in creating surrogates. that is a theme he hears from scholars in every field from history all the way through biology. transformation of archiving in the knowledge discovery processes marcus banks, of the national library of medicine, asked a question that he hoped connects sessions four and five. if we are moving from publication as product to publication as process, should we make a similar transformation in archiving? or should we still want to archive products that maybe are 91electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. extracted from that process? an example in stke would be the ﬁviewpointsﬂ from science, which are snapshots in time. it seems that archiving the best information in a separate subset would be useful, but then that might be paper bound and published the old way we have done things. monica bradford said that aaas has been talking about that. because stke is constantly changing and the database is updated on a regular basis, they assume as it scales and grows it will increase a lot. should the stke viewpoints be archived on a certain schedule, perhaps every quarter? aaas is not sure yet what would be the right frequency with which to do that, but the viewpoints do not have to be paper bound. right now stke is in a transition stage, and the scientific authorities still want to get credit and want to be recognized for the effort, because it takes a lot of effort to do these functions well. so the purpose for the viewpoints is not so much archival, although it does serve that purpose. it is more to give the authorities some recognition. dan atkins noted that rick luce touched upon the archiving issue during his presentation. there is an enormous stream of digital objects that could be created by knowledge discovery processes that are mediated through technology. one needs to be able to archive not only these objects, but the relationships between the objects and the social context in what they are created. we need to start thinking about archiving that includes these temporal streams that come through. one of the most profound ideas about all of this comes from john seeley brown, former chief scientist at xerox and chief innovation officer for 12 entrepreneur, inc., who says that perhaps the most important aspect of this technologymediated way of work is not just relaxing constraints of distance and time, enhancing access, and so forth, but actually comes from the possibility of archiving the process, not just sampling the artifacts along the way. the idea in areas of ubiquitous computing is that people could then come back and actually mine processes and extract new knowledge that otherwise has been left on the table. it is an extension of this whole notion of data mining into knowledge process mining, so it gets very abstract. we can start to see that it is not just fanciful, and it is something to think about. people who are interested in longterm preservation need to consider huge repositories that take into account not only the basic ingredients, but the social processes by which these ingredients are encountered. increasing data and lost opportunities donald king noted that over a period of 15 years, scientists have increased the amount of time that they devote to their work, by about 200 hours a year. the point is that scientists are now approaching the individual capacity with regard to how much time they can devote to their work. most of that additional 200 hours per year is devoted to communicating. the number of scientists increases about 3 percent a year, which means that the total number of scientists doubles about every 15 to 20 years or so, but the point was made earlier in the symposium that some of the information we are gathering doubles every year. it seems that one of the concerns is the limitation of the capacity of the human intellect to work with these data. the national academies, nsf, and others could focus on trying to increase the number of scientists who work with these data and the infrastructure that can help them work with these data, to begin doubling the scientists every 5 to 10 years instead of every 15 to 20 years. in response, david lipman agreed with the basic point donald king made, but noted that scientists do adapt to dealing with large data sets. if presented with the computing power and the data, scientists ask different kinds of questions. it takes a long time before more scientists within the community shift and start to think of different kinds of questions. a few pioneers start to think a new way, and then it starts to happen. he has a lot of confidence that the more data we generate, if the computers and access to the data are there, people will come up with ways to ask the questions. donald king clarified that he wanted to know if there are lost opportunities. it seems as though there must be. david lipman said that a lost opportunity does exist. nih a few years ago set up an initiative called the biomedical information science and technology initiative (bisti) to try to get more people involved in computational biology. they came up with a lot of recommendations. however, he thought the goal of bisti would be to recommend more training for scientists and more money for computational research for the kind of work that alex szalay referred to, where one analyzes other peoples' data sets, because there is a huge number of discoveries to be made. there is to some extent a lost opportunity, because there are not enough biologists researching and writing papers that get 92electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. published in biological journals. bisti should have focused more on training people to do that kind of work and on more grants for doing that sort of research. professor szalay also noted that because of this data avalanche or data revolution, if the data get properly published, it will cause another fundamental sociological change. today in science, many people train a lifetime to build up their own suite of tools that they then apply to the raw data that they can get. through these digital resources, one can get reasonably polished data, where one can think more about the discovery itself; a researcher does not have to spend so much time with the mechanics of scrubbing the raw data and converting them into useable form. people will be much less reluctant to cross boundaries, for example, if the data are all available in a ratified and documented form. the value of knowledge discovery mark doyle, of the aps, said he was amazed when listening to the presentations and comments in this session because they make what he does when he publishes simple papers look trivial in terms of the amount of data and text published. he previously mentioned the two to three orders of magnitude difference between pure dissemination and what a publisher might do in creating an archival xml and doing peer review. there is another two or three orders of magnitude increase in what researchers actually are doing with their time. that makes him hopeful that publishing is really becoming more of a minor cost, compared with the cost of doing research and related activities. he hopes that regular publishing would piggyback on these larger kinds of things, since publishing is important, but not nearly as costly as doing the research or accumulating these much more complex kinds of things. he also asked whether there is a transition to where these things become much more primary than the journal articles that come out of them now. monica bradford noted that the 200 hours of communicating is a huge cost. that is the real publishing, the communicating and getting the idea shared. the amount of time someone puts into creating a product, be it a connections map or whatever, is significant. it is a time away from doing basic research. at stke, they are happy to hear from the authorities that gives them an added value, in that they have to organize their own understanding and framework in their area. but that is a significant cost one cannot dismiss. donald king also asked about the cost for doing the traditional paper. he asked if a researcher or some granting agency would realistically be willing to pay $1500, rather than going through a subscription model. the scale of the costs that are involved in traditional publishing is much greater than that. david lipman raised the nih budget as an example, assuming that the total amount that nih spent and the number of publications that came out of it is increasing. it is probably $250,000 or $300,000. so the amount for doing the publication in some journal is a smaller part of that. there are issues in terms of economic analyses of openaccess publishing versus fee for service on the basis of that. however, monica bradford and mark doyle were referring to how much time all those other things are factoring into this as well. in a sense, the paper does represent what the scientist did; it is the knowledge part of it. that can only be done so fast. so there is a difference between organizing data sets and making them useable to other people, which is a challenge, and finding and extracting what one thinks about that data set and getting it out there. the role of journals in the successful development of databases in molecular biology bob simoni, from stanford university and the journal of biological chemistry (jbc), added that the enormous success of factual databases and our reliance upon them actually are the result of the collaborative effort the journals have made with regard to their ﬁnowallﬂ requirement that the data be deposited as a prerequisite to publication. one might think that is a natural thing, but it is not. in the area of protein structure, for example, the data that crystallographers gathered were held and not deposited. the pressure from peers to a certain extent, but more importantly from journals, resulted in those data being deposited as a condition of publication, which makes the databases in this area as robust as they are. he then asked david lipman about the status of a centralized, normalized system for deposition of arrayed data and gene expression data. 93electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. david lipman responded that people at the ncbi have been discussing this with david klein, who is active with the jbc on that issue. there are standards that people have tried to agree upon for what is the minimum amount of data necessary to be submitted to a database. unfortunately, that minimum is so high that a number of scientists are not willing to do that. gene expression omnibus (geo), which is an ncbi database, has a somewhat lower threshold in terms of the requirements for submission, but the ncbi is in discussions with the international group on that issue. geo is growing faster now. some journals require submission. he also seconded bob simoni's point about the critical role that journals have in terms of getting data in a useful form into these databases. despite the fact that databases are useful, scientists often do not want to spend their time on data submission. what they are judged by is not what they put into a database, but what they publish. the role of the journals is absolutely critical, and jbc was one of the real pioneer journals in pushing submission to the sequence databases and getting essentially 100 percent compliance. 94electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 8. symposium wrapup moderator™s overview mary waltham, publishing consultant in the context of dr. duderstadt™s opening remarks to this symposium, which referred to the ﬁchaos of concernsﬂ that currently surrounds scientific, technical, and medical (stm) publishing, this final session reflects on some key issues raised by the symposium™s participants, including publishers, university faculty, administrators, and librarians. from a publisher™s perspective, changes occurring to publishing systems driven by electronic journals have been discussed extensively during the symposium, and a range of issues that require close attention were identified and addressed. it is clear that the costs of onlineonly journals are less than print plus online. however, print seems unlikely to go away completely across all of stm publishing in the near future, so the costs of a dual system persist for most publishers. archiving was discussed and questions raised about who will be responsible for the archive journal contentšpublishers, national libraries, or other third parties? what will be archivedšall of the journal content or just the research content? who will curate the journal archive and ensure it migrates to appropriate platforms as technology evolves? who will pay? within stm journal publishing there are economies of scale that tend to favor publishers of large numbers of journals; these economies of scale are not available to small and midsized publishers. as a result, some cooperation and grouping of content have arisen within some sectors in order to mimic or replicate these economies. recent mergers and acquisitions within the stm journal market have further made smaller publishers nervous about their longer term viability as independent entities. most of the broad range of online business models are quite experimental and seem likely to diversify and hybridize over time as the publishing system develops and new advances emerge. speakers also talked about the likely development of publishing as a more disaggregated process with separate pieces of the continuum done by different groups, from content creation through dissemination. filtering and quality control of the information are central to the publishing process but, in the future with more open access to all types of information and data, who will provide reliable and consistent filtration and quality control, and who will pay for it? increased online access results in increased usage of information. the journal as a package of information is not granular enough and so further unbundling of information is clearly taking place. customers and users want more granularity in the online environment than a journal issue represents. there was discussion of who needs copyright as opposed to ﬁwants itﬂ or ﬁuses it.ﬂ copyright is very dependent on both the author and the mission of the publisher with whom they may be working. the continuous online publishing process means that documents may no longer be static but evolve through time because addition, annotation, and revision are simple. interoperability and common standards are essential to bring together and integrate information and provide a dynamic reference tool. achieving this type of integration is necessary for making the optimal use of scientific information. a key point for publishers is: where do they add value to the publishing process, and is that value added where users and customers want it? the role of publishers must continue to change to meet the needs of the research community. 95electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. comments by panel participants malcolm beasley, stanford university professor beasley commented on the symposium from the perspective of an interested practicing scientist working in the lab, with a physical science background. he shared an example of the use of print versus online journals in the physics community in stanford, where the faculty now overwhelmingly use electronic journals. only the older faculty still use print copies, and it will only be a very short time before online journals are used universally. professor beasley™s first impression was that some sort of broad open access to scientific information bases of various kinds, including journals, is inevitable. it is too important not to have them. this is his view as a practicing scientist, and, although they are younger and naive, that is also the view of the students. it will not be simple to get there, and there are a wide variety of vested interests and issues that have to be accounted for. he believes that ultimately scientists will insist on it. nonetheless, he agrees with monica bradford from science that this will be a tensionfilled transition. that tension is not necessarily a bad thing if it is creative. tension can also be destructive, however, so it will require strong and wellinformed leadership to ensure that the tension is creative for science and all who partake in that enterprise. his second impression is that ﬁpublishingﬂ will not go away because it provides added value. the breakdown of the present system is forcing an examination of what those fundamental values added are in the existing publishing modes. equally interesting are the alternatives in terms of publishing broadly defined to achieve things such as certification, formatting, and editing. most interesting to professor beasley are the new modes of value added that will be created. there are some wonderful ideas out there. maybe threequarters of them will not work, but it is the quarter that survive and provide the essential value added that will be wonderful to have and very interesting to see evolve. also, it is clear that access to information almost universally will continue to profoundly improve in speed and efficiency, and therefore have a similar effect on science. that is basically good, but it is important to deal with all that information and get to it with some sense of the quality, the various other types of value added that might be seen. hyperspeed in itself is not the ultimate good; rather, it is speeding up the process of achieving understanding, as distinct from the acquisition of facts. we ultimately have to achieve understanding in science, and it may mean different things in different fields. during the symposium, professor beasley heard ideas that might implement that, but he did not hear a focus on what might be done to improve that aspect of the scientific enterprise. he thought that aspect of the scientific enterprise should have been brought into sharper focus. people are thinking about it, but it is important to stress that we do need to do that. professor beasley thought that in addition to understanding the information we will be getting, we must find methods to improve and accelerate our ability to form good judgments on the basis of that information. there will be more information, which presumably will be better and received faster. to give a concrete example, he observed over his 30 to 40 years as a university professor that graduate students cannot read the literature. it is too abstract, has too much jargon, or has concepts they do not understand. maybe this is something more specific to physical sciences, but simple access to it is not going to be enough. as a university professor, he found some related issues as well. how will the faculty help students deal with the information? they may be more adept in using the tools, but they are not going to be adept in this essential value added. for example, there are personalized searches and virtual journals. there are in science and nature perspectives on some of the articles. he is an avid reader of those because they help him gain perspective, not only in his own field, but a bit more broadly. the condensedmatter physics community is considering creating electronic journal clubs. there is a famous journal club in this community where individuals report on a paper selected because of its quality and importance, which is then presented and critiqued. this was a tradition at bell labs, which was initiated by conyers herring, a truly great physicist, and some of the alumni of bell labs are now trying to institutionalize that by putting it online somehow. professor beasley does not know whether it will work, but believes it is an excellent thing to try. 96electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the knowledge environments and connection maps discussed during this symposium are ideas that illustrate what is possible in this area. as a university professor wearing his teaching hat, professor beasley found the implications of electronic publishing and the digital revolution interesting and not topics that have received sufficient attention either in the scientific community or in the universities. james o™donnell, georgetown university dr. o™donnell is one of the longest serving editors and publishers of an openaccess online scholarly journal anywhere. his cofounder, professor richard hamilton, and he began publishing bryn mawr classical review (bmcr), an online book review journal for classical studies in the fall of 1990. it has always been free; it has always been on the internet. they tried selling a paper version for a few years, but that did not go anywhere and ended in 1997. an archival cd was published in the mid1990s and was a complete flop. bmcr has become the leading book review journal in the classics in the united states and one of the top two or three in the world. the editors have all the worries and anxieties of an openaccess publisher, including where the next dollar is coming from, but they are confident they are in business to stay. we expect, he believes, of a new system that it will drive costs out of the old system. there is a general sense that we pay a lot for information and if we could possibly pay less, we would like to do so. he shared the dreams and imaginings of the early 1990s, when we thought that electronic publication would be somehow magically frictionless and cost free. he realizes that we know it is not, but at the same time we know there are opportunities to reduce some of the costs. the first two sessions of the symposium were particularly instructive in that regard. o'donnell treats separately the question of recovering those costs and sharing those costs in an equitable and sustainable manner. the openaccess movement, to the extent that it is a movement, tends to shift costs away from the user and toward the producer of information. o™donnell recognizes the attractiveness of this shift, and has qualms about it. the adventures in the nonopenaccess domain of journal publishing in the past 15 to 20 years have demonstrated that one of the features of highpriced paper or electronic journals is that they facilitate a form of quality control. when university libraries cut their budgets for serials acquisition and reduce the number of titles they acquire, they are exercising a form of quality control over what they will accept. if the ﬁbig dealﬂ for licensing ejournals is fraying around the edges, library resistance will transmit itself back to the publishers as news about which journals are to be sustained and which journals are not. the actual closing of real journals and cutting back of titles by publishers over the past decade has been achieved mainly through the pacmanlike gobbling up of one publisher by another and the recognition that not every title was sustainable in the economic model of the forprofit publisher. but here o'donnell identifies the focus of an important disagreement. some of the symposium™s speakers spoke up for the openaccess model as delivering superior quality. one can equally argue, he thinks, that the commercial model provides superior quality. as one argues about business models, costs, and recovery of costs, it remains an open question whether a change in the system will improve or degrade the quality of the aggregate system of scholarly communication. as long as that question remains open, he thinks there is no forcing argument to use in favor of one model or the other. a mere economic argument would not be definitive. a good recommendation from this symposium, according to dr. o™donnell, would be to continue to assess what the effects on quality of information, timeliness of access, and quality of peer review in the different models would be. o'donnell observes wryly that if, as an openaccess journal publisher, he is part of the solution, as a provost, he is undoubtedly part of the problem for everyone. commercial publishers will complain that he does not give enough money to the library, and explain that this is why librarians think the prices of the journals are too much. openaccess enthusiasts will complain that he does not underwrite their experiments in new forms of publication, because he is still paying for the old system. he would like to stop paying for the old system before paying for the new system, and not be caught paying for two different systems at once. from the viewpoint of a provost, he is struck by the emerging differentiation of the product for which the scholars, scientists, and publishers want him to pay. he was particularly impressed by many of the numbers of costs raised during the first session, especially mike keller's findings on the precipitous drop in commercial value of information over the first 12 months of its life. that seems to suggest at least 97electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. a tripartition of the kind of information objects that are under discussion at this symposium. at one end, there is the timeliest of information services, providing the linked news from the front as rapidly as possible, with no consideration for archiving or price, but simply getting the news as fast as possible so that science can proceed in its best way. at the other end of the life expectancy of that information, service has become an artifact, something to be preserved, maintained, and sustained long after its commercial life, or perhaps even its scientific life, has been exhausted. preserving the science that is done today for the historians of science 100 years from now is an important exercise, but the historians of science 100 years from now do not have their budgets yet and cannot come to the table with dollars to pay for that task. that first information service, o'donnell believes, tends to be marketbased. that last artifact service is not market based at all, but is something done out of noblesse oblige for the greater good of the community. between there is a borderland area, where the information service itself needs to be mediated to those who have limited access to the market. if a researcher has large research grants or is in a major research university, he or she is probably doing well in acquiring information. however, if one is in a developing nation or a small institution or is otherwise disadvantaged, the situation may be very different. he believes that understanding that differentiation of product, which is an increasing differentiation in an electronic environment, increasing among products and increasingly differentiated among disciplines, will be an important part of understanding what a new system of information dissemination can be like. in the end, o'donnell thinks himself more tranquil than many other observers. much of the symposium™s discussion has been focused in his view not on imminent fear of negative events, not on reasonable expectation of longerrange problems, but on still shapeless anxiety. as provost, he cannot pay to fend off every piece of unsubstantiated anxiety. every other department and office of the university comes to him with the same kind of anxiety. much progress has been made in ten years, he believes, and the discourse has changed dramatically. in that regard, he is optimistic. his question for the librarians, scientists, and colleagues he works with will be, can you make sure you define what the problem is before you need a solution? he is still not convinced that he can get a good answer to that question. it is time, he believes, to begin disaggregating the scholarly and scientific publishing crisis, if there is one, into pieces that can be addressed in rational and coherent kinds of ways, as part of the many movements that have begun to emerge in the past ten years, as ejournals have become a reality, as open access has become an astonishing reality, and as the impact on academic culture has been so great. if someone can identify persuasively the problem and say what the characteristics of a successful outcome might be, then they will find him to be a much better provost. ann wolpert, massachusetts institute of technology the views expressed during this symposium have been largely those of the authors and publishers in the scholarly communications system. ms. wolpert commented from the standpoint of the university library, which is responsible for bringing in, paying for, managing, and maintaining access to the kinds of information resources that are the subject of this symposium. to paraphrase hal abelson™s earlier comment, the progress of science requires access to raw material, evaluated judgments, and conclusions, to work that is current and previous. this is true in the university, not just in one's own discipline, but in multiple disciplines, because universities are dealing with students who have not yet settled into a discipline or who are working in the interstices between disciplines. so it is the responsibility of university libraries to think across those kinds of boundaries. it appears that the raw material and the evaluated conclusions coming out of scholarly research are bifurcating into two fairly distinct realms. one realm is a quite tightly controlled, often highly priced peerreview literature, and the other is the minimally controlled, scholarmanaged, openaccess publication and database regime. both environments present challenges to universities that are by mission dedicated to providing homes for researchers, educating the students who come to these environments, and managing the information that is created today and yesterday for the benefit of scholars and students, some of whom have not yet been born. one could generalize from the symposium™s discussion that the controlled literature is making every effort to increase that control over the content that they publish, and to expand their reach both in 98electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. time and in format. we heard that there is considerable interest on the part of publishers in controlling the archive of those publications, so that they would be responsible for the permanent record of a discipline rather than university libraries. the disciplines or the publishers themselves would thus be responsible over time for the archiving of publications. at least some publishers have also expressed an interest in adding data to that record, which would bring all of these resources under the intellectual property control of the publishers themselves. this may be a perfectly rational business model from the standpoint of someone who manages a university press and struggles on a daytoday basis with the finances of publishing. however, it is not a perfectly rational business model if one happens to be on the university library end of this formula and looking at buying these materials from publishers under the kinds of intellectual property controls that are created largely for the benefit of the entertainment industry rather than education and nonprofit research. the university perspective on the value chain of the process by which new information is created is very different from the value chain perspective of authors or publishers. in the value chain from the university perspective, the university builds the infrastructure and provides an opportunity for faculty to develop curricula and conduct research. universities also attract and admit students, especially the graduate students who conduct some of the research. universities provide access to information for their communities at a cost that must seem to them to be reasonable, as a percentage of the overall annual operating budget of the university. and they delegate that responsibility to their university libraries, which is where research libraries come in. not everyone in research libraries believes that the subscription model as a way of acquiring information is fundamentally broken, although the price structure certainly is. most do believe that the terms and conditions under which information is allowed into our campuses are substantially flawed, because licensing agreements control, in many cases, how subsets of a community can actually use that information. so it is the combination of the price and the constraints on the use of information that represent problems in the system for us. looking down this value chain of universities to the point where faculty are evaluated for publishing their work, one can see that the publication function has been outsourced to publishers for a number of valid reasons. as long as publishers handled the peer review and the publication of the work, and that work came back into the university for the next generation of education and research at a price that was reasonable as a percentage of the annual operating budget of the university, the system worked to everyone™s advantage. over the past 10 or 15 years the outsourcing enterprise has developed its own independent business model, which does not think of itself as having a symbiotic relationship with the university as much as it thinks of itself as a standalone, independent enterprise, able to create and craft its own business future, separate and distinct from what the university wants. it is in that environment that the costs have gone up and that the terms and conditions of use have changed, because there is now a very clear boundary between universities and the groups that capture the work that comes out of universities, publish it, and evaluate the quality of that work. one of the consequences of this separate and distinct business model, that is now at more than arm'slength from universities, is that some of those businesses doing the publishing and the peer reviewing have come to think of universities as patronsšthat is to say, organizations that ﬁoweﬂ the publisher a certain amount of money to support the work. others think of universities as pigeonsšthat can be plucked endlessly to support a profit margin they may or may not wish to support if they were asked. a third alternative one hears, is that universities are pirates, stealing information, trying to get for free information that was of high value and added to by the publishers. none of these scenarios makes for a particularly useful set of expectations around which to have conversations, although we keep trying. the conundrum for universities has several dimensions. for many publications, the costs are simply too high for the value received, and the licensing conditions are problematic, in terms of what we can do, particularly with digital information when it is delivered to our campuses. as described in session three, the intellectual property environment is not only incomprehensible to the average faculty member and student, but it may place universities at risk. any legal regime that varies so considerably by circumstance and format is not a particularly useful legal regime for people to operate under. on the other hand, people are afraid to seek definition through litigation; because they do not know what the outcome is going to be. now there are also new forms of data and information that clamor for attention for curation and funding on our campuses. ms. wolpert shared some concluding observations from the perspective of the university librarian. 99electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. it is apparent scholarly communication and publication are diverging. at one time, communications with colleagues was through publications; now it is clear that one can communicate outside of the formal publication record. the formal publication record is moving in many cases off to one side, which again affects the question of how much value do we put in the formal record of advances in a discipline, if in fact most of the communication is happening some other way. that is in a sense what mit™s dspace is aboutšto capture the communication, not the formal publication record. we hardly have a clue about what reasonable standards and norms might be for the cost of peerreviewed publication. those costs vary tremendously, and we do not know what drives them. we also do not know what drives the cost of print as opposed to the electronic publication. so it is very hard for us to think logically without the kinds of norms and standards that one can get from most other industries. it is clear that intellectual property law that meets the needs of the entertainment industry and the international publishing conglomerates is not particularly conducive to what goes on in the academy. we do not know what new models of peer review and recognition might be developed for opensource publication, which is an area of real attention. finally, ms. wolpert took away from this symposium a new, tongue in cheek business model for university librariesšthe offshore library model. she will advise her colleagues in the state of maine, who are dying under the current cost structure for peerreviewed literature and have had to cancel subscriptions, that all they need to do is go to one of the 130 developing countries where this information is delivered for free and open a branch library; they can have an offshore library. discussion of issues dr. o™donnell added that he was struck by the vividness of the presentations. ann wolpert's comments are quite relevant to the divergence of views and the building of collaborative enterprises. that said, he thought the speakers addressed fewer of the problems that might emerge as, for example, those huge databases age. in another ten years, 5 or 10 petabytes will be easy to carry around, but it will still take some housekeeping and maintenance, and will begin to look a lot like what publishers and libraries do. his question remains, what is it going to take to do the good science and to make sure the good science gets done? that should probably be the bottomline question and set of priorities. universities role in data curation and management ann wolpert asked malcolm beasley as a scientist in a research university what role he sees for the university, if any, in the curation and management of data that emerge from science. she asked this in light of dan atkins™s comments on the national science foundation™s cyberinfrastructure report and the question about where responsibility resides for the longterm storage and curation of data. professor beasley thought that question was more appropriate for mike keller. he did, however, add that faculty in general do not understand the costs of providing these library services in this modern sense and therefore are not necessarily informed people to make judgments about how these things ought to be paid for and what the real tradeoffs are. ann wolpert asked the question in part because she was interested in knowing how and where the discussion can go forward on university campuses. professor beasley, reflecting on his own experience, said that the faculty do need to understand these things better, because if they do not they will go and beat up on the provost. to some degree, it simply is not fair, and to another degree, it must be exasperating to be ill informed. there are a number of areas where the provost will get beaten up, but the point is that with the revolution in the way science is done that it is too important to leave to the provost alone. jim o'donnell thought that the real question was not what the provost wants to do in the abstract, but an empirical question: where is information best cared for? he did not think we know the answer to that. as a provost, he could understand an argument that would say only the scientists know what goodquality curating looks like and what maintains information in good enough form to be useable, therefore it must be done inhouse. he also could accept an argument that the scientists do not have a clue how to 100electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. do it technically, managerially, with appropriate metadata, or with appropriate access or preservation, and therefore the curating needs to be outsourced. the question then would be, does one outsource it inside the notforprofit community, the university, or the larger notforprofit community, or externally to the market? donald king commented that the scientific literature should be made available to the faculty, not just per title, but rather per article. for some journals the cost per article has actually decreased because the size of the journal has increased much more rapidly than the price. faculty should also be presented with the cost based on the price by use. the number of individual subscriptions by scientists has dropped from 5.8 to 2.2 over the past 25 years. as a consequence, all of that reading has now gone into the library. scientists have roughly doubled the reading they do in academic libraries, and in special libraries it has increased four or five times. faculty need the right indicators in a way that is realistic for them to understand. mike keller tried to respond to ann wolpert™s initial question about the role of the university in data curation and management. he noted that the underwriters who work for stanford and who want the university to spend a lot of money on insurance say that the collections that have been amassed there in the libraries, now amounting to about 8 million books and probably 30 or so miles of archives and so forth, are valued at $1.2 billion. it is one of the largest assets the university has, other than its land. in the decade since 1993, when the world wide web became massively and widely available, stanford has spent $73 million on capital projects, the most recent of which was to buy land to construct the first of what might be six or eight modules to store books and other physical artifacts, as well as perhaps to house a very large digital store off campus. in the same period of time, stanford spent about $5 million, earned almost entirely from the publisher clients at highwire, putting out about a million articles for them. the cost of information technology and epublishing today is about 5 percent of the whole budget for publishing. as we work on this problem of nonphysical, virtual artifacts, the solution lies, as it has in the past, in great libraries, which are archives not built by archivists for their own sake. they are built with the connivance and cooperation of the scholars who most directly receive benefit from them. as the scholars in science, technology, and medicine very clearly see the advantages to some of the research possibilities that the big data sets and collections of electronic resources come to offer, then there will be more impetus and more money to provide them with the research materials, which then become artifacts for preservation over a long period of time. libraries are preparing to do that, but everything that we have is in embryonic phases, without exception. martin blume noted that on behalf of publishers from learned societies, the message, if somewhat ungrammatically, to the professors, librarians, provosts, and scientists is, ﬁwe are you, and you are us.ﬂ the american physical society (aps) is run by professors and laboratory scientists, including the president, the vice president, past president, and presidentelect. it is operating with scientists, who have been professors. so the values that we have are the same values that are expressed at this symposium. the aps officers understand some of the problems of the publications perhaps a little better, because they have been thrown into it and are therefore forced to think about it. at the same time, they need the understanding of the scientific community and the universities that this is where they are coming from. aps™s publications committee is chaired by an industrial scientist. its members are university professors and laboratory scientists as well. paul ginsparg is a member of that committee, so there is representation from the electronic archives. all of this comes together at this point, so aps cannot be accused of having a very narrow point of view; it is better educated. interaction between scientists and archivists steve berry asked jim o'donnell whether it is possible to overcome the arrogance of the scientists and the humility of the archivists, so that the natural solution will come from these two groups of people learning to talk to each other, to deal with the problems of these enormous databases, and realize that each brings something, but not a complete solution. jim o'donnell thought the critical intellectual capital will be formed in the dialogue between the archivist and the information scientist on the one hand, and the working researcher on the other. he would like the institutions and the notforprofit sector to retain control of that process in order to hold on to the intellectual capital. then he would be willing to talk about who should do the dirty work and make 101electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. the things actually operate. he was brought up short once early in the days of bmcr, when an associate provost, who used to be a librarian, asked dr. o™donnell if he could do a good job of marketing to nonclassicists, and o™donnell replied, ﬁnot reallyﬂ since he knew who all their readers were. the associate provost told him that, ﬁthe one thing you know in a library is there are lots of other people who have an interest in the stuff you are doing besides you, your six friends, and the folks you think you are doing this for.ﬂ that really adds an order of magnitude of value that o'donnell thinks we always need. getting the dirty work done adds another value, but it is a very different kind. tom arrison from the national academies was interested in the idea of the dialogue between the scholars and the librarians. an instructive example is the opencourseware project at mit, where there was a process that led to this fundable vision, implemented with money from private foundations. sometimes this dialogue and reflection on a campus can lead to a vision that can bring in the resources to implement it. he asked whether in the panelists™ individual institutions that kind of dialogue is going on and is adequate. also, more broadly, he was interested in what needs to go on between institutions and among the scholarly communities to help promote that. malcolm beasley noted that in terms of dialogue, it depends on definition. at stanford, there is a faculty committee that interacts with the administration to deal with these questions. the faculty hears reports, and he has no doubt that it is a substantive discussion. but a wider discussion than that through the normal faculty senate committees is not widespread. the question is: is this an issue that is sufficiently important that one ought to try to have a wider discussion about it or not? he would argue that it is a candidate, because of its importance to the scholarly side of what faculty does. but he does not think it would be fair to say that this discussion is highly developed in any institution. jim o'donnell said that the learned societies play a stronger role than the individual campuses do. they can aggregate more resources, and they can bring more intellectual firepower to bear on disciplinespecific kinds of questions than happens in a university faculty with a library advisory committee with different disciplinary representatives. ann wolpert believed that perhaps one of the points to take away from the symposium was that there need to be vehicles to encourage that discussion, because there are conversations within institutions and groups of institutions, and there certainly are conversations within disciplines, and, to a lesser degree, among and between disciplines. but there is no easy way for the conversations to happen across the boundaries that were just described. malcolm beasley followed up on tom arrison™s comment about universities obtaining resources from foundations and other private sources. he felt it would be more appropriate to try certain experiments. it is not the intended role of any foundation to pay the continuing costs. the role of publication and communication in the sciences steve berry asked ann wolpert if she would accept that the role of publication still remains central to the process of communication in the sciences. although colleagues in many different places talk more now, when it comes to the substance on which scientists base their inferences and new ideas, they have to go back to the literature. they have to go back to the publication from last month or the last decade. the scientific publication still remains the rock on which they build, and the thing that has changed most is the communication and the way scientists collaborate with each other. ann wolpert agreed. however, she added that the challenge that confronts us now is the migration of the official version of the publication of record from print to electronic formats. for 15 years, people have been asking when the library will become entirely digital. the answer is, it depends, like the law. but as a practical matter, we are in a serious transition phase right now. she has no idea how long this is going to last, but we will know that we have successfully tipped into the electronic environment and learned how to archive material that is formatted digitally in reliable and sustainable ways when someone gets a nobel prize based entirely on an electronic publishing record. that is her standard. steve berry noted that it has sometimes been said that paper is much more permanent than any current electronic form, and there is the dilemma of transferring last year's electronic archive into next year's electronic archive. however, there was a period when physical review was printed on very high acid paper; if one opens a mid1970™s physical review now, the pages crack and come apart. aps was able to transfer those fragile paper versions to electronic form and save the records in prola. 102electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. university libraries and publishing deals robert bovenschulte asked ann wolpert if he could conclude from her remarks that she might roughly divide all library deals into three categories: good deals, acceptable deals, and bad deals. he was curious to understand better the interplay of forces within the university when a library accepts a bad deal. ann wolpert noted that there are different flavors of bad deals. some of them have to do with the cost of the deal, and some of them have to do with the terms and conditions of use that come with the deal. a bad deal for a university is a deal that over time consistently favors the needs and interests of one group within the university over others. part of the political risk that the scientific community runs right now, and where there is pushback in the university environment, is around the constant percentage increase in demand to support the scientific and technical literature out of a finite amount of money. at the end of the day, someone on the campus gets shortchanged as a consequence of the need to constantly feed a set of growing expectations about the payout from university library budgets in support of scientific and technical literature. from the standpoint of the university, ultimately the groups that are shortchanged will have their time in the spotlight. disciplines cycle through favor. around the turn of the last century it was mechanical engineering then civil engineering and then physics; right now it happens to be the biological sciences. but as a practical matter, sometime they will cycle out too, and something else will replace them. libraries struggle to maintain a balance on their campuses from the expenditure point of view between and among the disciplines. the potential for longterm damage is there, because one cannot buy a book that is not available anymore. the other worry that universities have is about the terms that licenses about who can use material and under what conditions. that sometimes disadvantages parts of the campus. a library can afford to license for a limited number of users or for one subset of its community, which disadvantages those who cannot easily get to use it. those licenses cannot be networked, or they can only be networked within a particular subset of buildings on campus. so these are the kinds of complications that libraries did not have 10 or 15 years ago, which they now confront on a regular basis. retrospective information in an electronic environment gordon neavill, of wayne state university, noted that one of the problems in the digital environment is that the economic link between current and retrospective information is broken. in the print environment, almost all the information that libraries bought was acquired because it was current. it was then simply retained and became valuable retrospective information. in the electronic environment, we pay once for current information, then we probably have to pay all over again, at a fairly high cost, to capture the same information for very important, but lowlevel, retrospective uses. in the case of electronic databases, snapshots are made, but fewer people will use the retrospective information than need the current information. to some extent, these retrospective costs can be shared. he asked if electronic systems can be designed to minimize the additional cost required to retain them for retrospective purposes. ann wolpert said that there are two ways to think about that. when only one format existed, it served a variety of needs: the current information dissemination, the nearterm research requirement, and the longterm archiving requirement. in the digital environment, people want to use the digital materials for the sake of convenience and productivity, but there are difficulties with that, in that the electronic material does not perfectly mirror what came out in print. we heard previously that a third of the use of the reed elsevier titles was for nonarticle material. so there is a lot of information in print publications that is not in electronic collections of journals. the other way to think about the electronic environment is that there are presumably costs associated with buying the retrospective collection on an annual basis. if a library had a choice of buying one year's worth at a time in terms of budget, it could do that and stop. but in the electronic environment, a library has to buy the current year plus the archive in many instances over and over again. certainly that is the model for reference books. if a library had limited funds, it bought a scientific and technical 103electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. reference book one year and then bought it on an every other year or every third year basis. if this reference book is purchased electronically, the same price is paid year after year. so it affects the economics of how a library thinks about its collections, not just in terms of the current material, but in terms of how it manages the archive. jim o'donnell added that the retrospective collection has never been free. it costs a lot of money to keep it dry and cool and to reshelve it. universities spend immense amounts of money on redundant collections. where offsite shelving is beginning to be done cooperatively among institutions, it certainly provides an alternative opportunity to think about just how much redundancy is worth paying for in terms of the use they get out of it. universities have to be rational about how much they spend on the print archivesšand it is a lot. his comment reminded ann wolpert that that there are different kinds of money in universities. capital funds are needed to build a building, which is a big onetime effort. although the cost of the materials that are stored there over the life of the building are amortized, in fact, for the university the economics work differently. costs in the electronic environment are annual operating costs as opposed to being able to move some of those costs off into a capital budget, and manage them differently. closing remarks ted shortliffe ted shortliffe closed the symposium by thanking the speakers and participants. one of the charges to the steering committee is to take the lessons of the symposium and try to crystallize them, and in particular to ask what are the key potential areas of study that the national academies might focus on for additional work to help move these issues forward. there are many potential topics for study, and he welcomed advice from symposium participants on which areas the national academies could make useful contributions in, such as areas that have not been looked at effectively by others perhaps, and where there is a need for further work. on that note, he thanked the national academies staff for assisting with the symposium and adjourned the meeting. 104electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. appendix a symposium agenda symposium on electronic scientific, technical, and medical journal publishing and its implications main auditorium the national academies 2100 c street, nw washington, dc 20418 monday, may 19 8:00 registration and continental breakfast 8:30 welcoming remarks bruce alberts, president, national academy of sciences 8:45 symposium overview edward shortliffe, professor and chair, department of medical informatics deputy vice president for information technology health sciences division, columbia university and symposium chair 9:00 keynote address james duderstadt, president emeritus and university professor of science and engineering millennium project, university of michigan panel 1: costs of publication moderator: floyd bloom, the scripps research institute 9:30 opening remarks by moderator 9:35 overview presentation michael keller, ceo, highwire press 9:55 comments by panel participants kent anderson, publishing director, new england journal of medicine robert bovenschulte, director, publications division, american chemical society bernard rous, deputy director/electronic publisher, association for computing machinery gordon tibbitts, president, blackwell publishing usa 10:25 break 10:45 discussion of issues 12:00 lunch 105electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. panel 2: publication business models and revenue moderator: jeffrey mackiemason, university of michigan 1:00 opening remarks by moderator 1:05 comments by panel participants joseph esposito, president and ceo, sri consulting wendy lougee, director, university of minnesota library brian crawford, vice president and general manager, life and medical sciences, john wiley & sons patrick brown, professor of biochemistry, stanford university 1:55 discussion of issues 3:10 break panel 3: legal issues in production, dissemination, and use moderators: ann okerson, yale university, and jane ginsburg, columbia law school 3:30 copyright basics: ownership and rights jane ginsburg, morton l. janklow professor of literary and artistic property law, columbia law school 3:50 economic and noneconomic rewards to authors michael jensen, jesse isidor straus professor of business administration, emeritus, harvard business school 4:10 licensing ann okerson, associate university librarian for collections and technical services, yale university 4:30 discussion of issues 5:45 adjourn 6:00 reception, national academy of sciences™ great hall tuesday, may 20 8:00 continental breakfast panel 4: what is publishing in the future? moderator: daniel atkins, university of michigan 8:30 opening remarks by moderator 8:35 institutional repositories hal abelson, class of 1922 professor of computer science and engineering, massachusetts institute of technology 106electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. 8:50 preprint servers and extensions to other fields richard luce, research library director, los alamos national laboratory 9:05 implications of emerging recommender and reputation systems paul resnick, associate professor, university of michigan 9:20 discussion of issues 10:35 break panel 5: what constitutes a publication in the digital environment? moderator: clifford lynch, coalition for networked information 10:55 opening remarks by moderator 11:00 signal transduction knowledge environment monica bradford, executive editor, science 11:15 publishing large data sets in astronomyšthe virtual observatory alex szalay, alumni centennial professor, department of physics and astronomy, the johns hopkins university 11:30 data curation and integration with the literature david lipman, director, national institutes of health/national center for biotechnology information 11:45 discussion of issues 1:00 lunch panel 6: wrapup session moderator: mary waltham, publishing consultant 1:55 opening remarks by moderator 2:00 symposium summaries malcolm beasley, theodore and sydney rosenberg professor of applied physics, stanford university james o™donnell, provost, georgetown university ann wolpert, director of libraries, massachusetts institute of technology 2:30 discussion of issues 3:10 closing remarks by symposium chair, edward shortliffe 3:15 adjourn 107electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. appendix b biographical information for speakers and steering committee members hal abelson is professor of electrical engineering and computer science at massachusetts institute of technology (mit) and a fellow of the institute of electrical and electronics engineers (ieee). he is winner of several teaching awards, including the ieee™s booth education award, cited for his contributions to the teaching of undergraduate computer science. professor abelson™s research at the mit artificial intelligence laboratory focuses on ﬁamorphous computing,ﬂ an effort to create programming technologies that can harness the power of the new computing substrates emerging from advances in microfabrication and molecular biology. he is also engaged in the interaction of law, policy, and technology as they relate to societal tensions sparked by the growth of the internet, and he is active in projects at mit and elsewhere to help bolster our intellectual commons. professor abelson is a founding director of the free software foundation and a founding director of creative commons. he also serves as consultant to hewlettpackard laboratories. at mit, professor abelson is codirector of the mitmicrosoft research alliance in educational technology and cohead of mit™s council on educational technology. bruce alberts, president of the national academy of sciences, is known for his work both in biochemistry and molecular biology, in particular for his extensive study of the protein complexes that allow chromosomes to be replicated. dr. alberts graduated from harvard college and earned a doctorate from harvard university in 1965. he joined the faculty of princeton university in 1966 and after 10 years moved to the department of biochemistry and biophysics at the university of california, san francisco, where he became chair. he is one of the original authors of the molecular biology of the cell, through four editions the leading advanced textbook in this important field. his most recent text, essential cell biology (1998), is intended to present this subject matter to a wider audience. dr. alberts has long been committed to the improvement of science education, dedicating much of his time to educational projects such as city science, a program that seeks to improve science teaching in san francisco elementary schools. kent r. anderson is the publishing director for the new england journal of medicine. prior to joining the journal, he was director of medical journals at the american academy of pediatrics. he has been in health care publishing for more than 15 years and has worked as a writer, editor, designer, production manager, copy editor, managing editor, and publisher. he has also worked in continuing medical education, launched a halfdozen successful new titles, and contributed to numerous online publishing initiatives. daniel e. atkins is a professor in the school of information and in the department of electrical and computer engineering at the university of michigan (um), ann arbor. he began his research career in the area of computer architecture and did pioneering work in highspeed computer arithmetic and parallel computer architecture. he has served as dean of the college of engineering and more recently as the founding dean of the school of information at um. he is now director of the alliance for community technology (act), an international partnership with philanthropy for research and development in the use of information and communication technology (ict) to further the mission of educational and other nonprofit organizations. dr. atkins does research and teaching in the area of distributed knowledge systems. he has directed several large experimental digital library projects as well as projects to explore the application of ﬁcollaboratoriesﬂ to scientific research. he has recently served as chair of the national science foundation advisory panel on cyberinfrastructure.. the panel issued a report in february 2003 recommending a major advanced cyberinfrastructure initiative intended to revolutionize science and engineering research and education. he also serves regularly on panels of the national academies exploring issues such as scholarship in the digital age, the future of scholarly communication, and the impact of information technology on the future of higher education. he is coauthor of a recent book entitled higher education in the digital age: technology issues and strategies for american colleges and universities. he serves as a consultant to industry, foundations, educational institutions, and government. malcolm r. beasley is professor of applied physics in the geballe laboratory for advanced materials at indicates member of the symposium steering committee. 108electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. stanford university. he received his b.eng. and his phd in physics from cornell university. he then went to harvard university as a research fellow and subsequently became a member of the faculty. in 1974 he joined the faculty of stanford university where he became a full professor of applied physics in 1979. he served as the chairman of the department of applied physics at stanford from 19851989. in 1990 he was named the theodore and sydney rosenberg professor of applied physics. from 1992 to 1998 he served as director of the center for materials research. and from 1998 to 2001 he served as dean of the school of humanities and sciences. professor beasley is a member of tau beta pi, the ieee, the national academy of sciences, and the american academy of arts and sciences, and a fellow of the american physical society and the american association for the advancement of science. he is the recipient of the dean™s award for superior teaching at stanford university. he has served as a consultant to the national science foundation, department of energy, defense advanced research projects agency, and various industrial laboratories. he has also served on various panels of the national research council of the national academies. he was an elected member of the board of trustees of associated universities, inc., for the period 20032005. professor beasley™s research interests are in materials physics with an emphasis on basic and applied superconductivity, in particular hightemperature superconducting materials and applications, and the development and application of advanced thinfilm deposition techniques for complex materials. floyd bloom* is chairman of the department of neuropharmacology at the scripps research institute. he previously served as director of behavioral neurobiology at the salk institute and as chief of the laboratory of neuropharmacology of national institute of mental health. dr. bloom is a member of the national academy of sciences and the institute of medicine (iom). he has received many awards, including the pasarow award in neuropsychiatry and the hermann von helmholtz award, as well as a number of honorary degrees from major universities. he served as editorinchief of science magazine from 19952001 and currently is the president of the american association for the advancement of science board of directors. robert bovenschulte is director of the publications division of the american chemical society (acs), which publishes journals, magazines, books, and electronic products. prior to joining acs in 1997, he was vice president for publishing at the massachusetts medical society, owner of the new england journal of medicine and other medical publications. his career spans scholarly, professional, trade, college, and school segments of the industry. he is a frequent speaker and moderator at conferences of publishers and librarians. mr. bovenschulte has served as chair of the executive board of the international association of scientific, technical, and medical publishers, chair of the board of directors of the copyright clearance center, chair of the executive council of the association of american publishers™ professional and scholarly publishing division, and member of the board of directors of the council on library and information resources. monica bradford is the executive editor of the international journal science (published by the american association for the advancement of science). in this position she oversees the peerreview and selection of manuscripts; the copyediting and proofreading process; and the design, production, and manufacture of the print product. over the past few years, ms. bradford has been heavily involved in the development of science online. in particular, she has helped create a new line of digital products referred to as online knowledge environments. science™s stke and sage ke, the first two products in this line, are directed at research scientists. in addition, ms. bradford administers the apbiotech & science prize for young scientists in molecular biology. prior to joining the staff of science in 1989, ms. bradford worked for the publications division of the american chemical society for nine years. she holds a bachelor™s degree in chemistry from st. mary™s college, notre dame, in, and has done graduate work in management at the university of maryland. ms. bradford was a member of the board of directors of the council of biology editors, served as vicechair of the scientific publishing board of the american heart association, and is a member of the society for scholarly publishing. patrick o. brown is a professor of biochemistry at stanford university school of medicine and a howard hughes medical institute investigator at the stanford medical school. he received his b.a. in chemistry, and m.d. and ph.d. in biochemistry from the university of chicago. he did his graduate work with nick cozzarelli, studying the basic molecular mechanisms of dna topoisomerases. following a residency in 109electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. pediatrics at children™s memorial hospital in chicago, he began studies of the mechanism of retroviral integration as a postdoctoral fellow at the university of california, san francisco, working with j. michael bishop and harold varmus. dr. brown is a member of the national academy of sciences. dr. brown™s current research uses dna microarrays and other ﬁgenomicﬂ approaches to explore a wide range of fundamental questions in gene regulation, cell biology, physiology, development, and medicine. for the past several years he has been working to promote open, unrestricted access to scientific and scholarly publications. he is a cofounder and codirector of the public library of science, a nonprofit, openaccess scientific publisher. brian crawford is vice president and publishing director for global life and medical sciences within the scientific, technical, and medical (stm) publishing operations of john wiley & sons, inc. he holds overall responsibility for coordinating the strategic development and ongoing editorial management of wiley™s international life and medical sciences publishing. experienced in subscription journal, book, and new media publishing, dr. crawford has spent nearly two decades within the commercial stm information industry, most recently having served in several management roles at wiley. before joining wiley, he held the position of vice president and editor with alan r. liss, inc. (19881989), a privately held book and journal publishing firm that merged to become the wholly owned wileyliss, inc. subsidiary. mr. crawford began his publishing career in 1985 as an acquisitions editor within the journals publishing division of academic press, inc. (at that time a part of harcourt brace jovanovich; now a part of reed elsevier). prior to entering scientific publishing, dr. crawford was active in both scientific research and teaching. he had the distinction of being the first biologist to be appointed as a j. robert oppenheimer fellow of the los alamos national laboratory, where he was a member of the scientific research staff within the genetics group of the life sciences division from 19811984, and helped to launch the department of energy (doe)sponsored human genome project. he received his ph.d. in the biochemical and biophysical sciences at the johns hopkins university school of public health, specializing in cellular and molecular aspects of cancer genetics, under sponsorship from the national cancer institute. he received his b.s. cum laude in chemistry from the university of maryland at college park. dr. crawford is a member of the board of directors of the american medical publishers association (ampa; presidentelect in 20022003) and the executive council of the professional and scholarly publishing division of the association of american publishers (psp/aap; 2001present). james j. duderstadt is president emeritus and university professor of science and engineering at the university of michigan. he received his b.eng. in electrical engineering from yale university and his ph.d. in engineering science and physics from the california institute of technology. dr. duderstadt joined the faculty of the university of michigan in 1968 as professor of nuclear engineering. he became dean of the college of engineering in 1981 and provost and vice president for academic affairs in 1986. he was appointed president of the university in 1988 and served in this role until july 1996. he currently holds a universitywide faculty appointment. dr. duderstadt™s teaching and research interests have spanned a wide range of subjects in science, mathematics, and engineering, including work in areas such as nuclear energy, lasers, computers, science policy, and higher education. during his career, dr. duderstadt has received numerous awards for his research, teaching, and service activities, including the national medal of technology for exemplary service to the nation. dr. duderstadt has served on and chaired numerous public and private boards, including the national science board and the national academy of engineering. he also serves as a director of unisys and cms energy. he currently chairs several major national study commissions in areas including federal research policy, higher education, information technology, and nuclear energy. joseph j. esposito is president and ceo of sri consulting, the leading publisher of syndicated research for the global chemical industry. over the course of his career, mr. esposito has been associated with various publishers in all segments of the industry and was involved from an early time with new media publishing. he has served as an executive at simon & schuster and random house, as president of merriamwebster, and ceo of encyclopaedia britannica, where he was responsible for the launch of the first internet service of its kind. mr. esposito has also worked extensively in the technology industry as a consultant, with such clients as microsoft and hewlettpackard, and formerly ran the internet communications company tribal voice. his primary area of concentration is the development of strategy and business models for the dissemination of digital content. he has participated in numerous trade 110electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. shows and has written extensively in trade magazines and journals (see, for example, the essay ﬁthe processed bookﬂ in the march 2003 issue of firstmonday at www.firstmonday.org). mr. esposito is currently researching new economic models for a postcopyright age. jane ginsburg* is morton l. janklow professor of literary and artistic property law at columbia law school. ms. ginsburg received her j.d. from harvard law school and her doctor of law from the universite de paris ii. she served as clerk to judge john j. gibbons, u.s. court of appeals for the third circuit and spent three years in private practice before turning to teaching. her principal areas of interest and expertise are in intellectual property, comparative law, private international law, and legal methods. she has published extensively on copyrights and intellectual property and serves on the editorial boards of several intellectual property journals in the united states and abroad. michael c. jensen is the managing director of the organizational strategy practice at the monitor company and jesse isidor straus professor of business administration emeritus of the harvard business school. professor jensen joined the faculty of the harvard business school in 1985. in 1999, he left harvard to assume his current position at the monitor company. he was laclare professor of finance and business administration at the william e. simon graduate school of business administration, university of rochester, from 19841988, professor from 19791984, associate professor from 19711979, and assistant professor from 19671971. he founded the managerial economics research center at the university of rochester in 1977 and served as its director until 1988. professor jensen earned his ph.d. in economics, finance, and accounting, and his m.b.a. in finance from the university of chicago, and an a.b. degree from macalester college. he also has been awarded several honorary degrees and served as a visiting scholar at the tuck school of business at dartmouth college from july 2001 to june 2002. professor jensen is the author of more than 50 scientific papers, in addition to numerous articles, comments, and editorials published in the popular media on a wide range of economic, finance, and businessrelated topics. he is author of foundations of organizational strategy (harvard university press, 1998) and theory of the firm: governance, residual claims, and organizational forms (harvard university press, 2000). he is editor of the modern theory of corporate finance (with clifford w. smith, jr., mcgrawhill, 1984) and studies in the theory of capital markets (praeger publishers, 1972). he founded the journal of financial economics, one of the top two scientific journals in financial economics, in 1973, serving as managing editor from 1987 to 1997, when he became founding editor. in 1990, he was named ﬁscholar of the yearﬂ by the eastern finance association and one of the ﬁyear™s 25 most fascinating business peopleﬂ by fortune magazine. he is the recipient of a 1989 mckinsey award, the 1984 joseph coolidge shaw s.j. medal by boston college, and was awarded (with william meckling) the graham and dodd plaque and first leo melamed prize for outstanding scholarship by business school teachers from the university of chicago™s graduate school of business. dr. jensen has served as consultant and board member to various corporations, foundations, and governmental agencies, and has given expert testimony before congressional and state committees and state and federal courts. he is past president of the american finance association and the western economic association international and a fellow of the american finance association, of the american academy of arts and sciences and of the european corporate governance institute. michael a. keller is the ida m. green university librarian at stanford university, director of academic information resources, publisher of highwire press, and publisher of the stanford university press. formerly he has been in library leadership positions at cornell, berkeley, and yale, most actively engaged in collection development with broad exposure to the global publication and bookselling trades. in 1995, in response to scholars™ requests for assistance to their scholarly societies, he established the highwire press as an enterprise within the stanford university libraries to provide online copublishing services initially to three scholarly journals. as of april 2003, highwire press has grown to support 361 highimpact stm journals among more than 120 major scholarly societies. it is also the site creation and host service for the revolutionary, online, third edition of the oxford english dictionary. mr. keller is now fostering development of additional information tools and services for the scholarly community based on the successful highwire model, such as the lockss network caching application. he serves on the boards of or as an adviser to several organizations, both forprofit and notforprofit, including the long now foundation, the digital library federation, the pacific neighborhood coalition, and the world economic forum. mr. keller has consulted for a variety of institutions and programs, including the city of 111electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. ferrara in italy, newsweek magazine, princeton, cornell, indiana and other universities, and several information technology companies as well as some of the numerous scholarly societies whose publishing enterprises highwire press supports. for more information see http://highwire.stanford.edu/~mkeller/. david lipman is the director of the national center for biotechnology information (ncbi), which is a division of the national library of medicine within the national institutes of health. ncbi was created by congress in 1988 to do basic research in computational biology and to develop computational tools, databases, and information systems for molecular biology. after medical training, dr. lipman joined the mathematical research branch of the national institute of diabetes, digestive, and kidney diseases as a research fellow. in his research on computational tools, he developed the most widely used methods for searching biological sequence databases. there are thousands of citations to dr. lipman™s methods in papers, which have used them to discover biological functions for unknown sequences and that have thereby advanced the understanding of the molecular basis of human disease. since 1989, dr. lipman has been the director of the ncbi, a leading research center in computational biology and one of the most heavily used sites in the world for the search and retrieval of biomedical information. wendy pradt lougee is university librarian and mcknight presidential professor at the university of minnesota (appointed june 2002). as university librarian, she is responsible for a system of 16 libraries on the twin cities campus. prior to her appointment at minnesota, ms. lougee served as associate director of libraries at the university of michigan, with responsibility for digital library development. michigan™s distinction as a premier digital library enterprise developed from a number of significant efforts launched during her tenure, including jstor, making of america, the peak (pricing electronic access to knowledge) project, oai harvesting initiative, and a number of publisher collaborations. ms. lougee holds a b.a. in english from lawrence university, an m.s. in library science from the university of wisconsin, and an m.a. in psychology from the university of minnesota. richard e. luce is the research library director at los alamos national laboratory (lanl). he was appointed project leader of lanl™s library without walls in 1994, an internationally recognized pioneering largescale digital library. mr. luce holds numerous advisory and consultative positions supporting digital library development and electronic publishing. in 1999 he cofounded the open archives initiative to develop interoperable standards for author selfarchiving systems. currently, he is the senior adviser to the max planck society™s center for information management, an executive board member of the national information standards organization, and a member of the university of california systemwide library and scholarly information advisory committee. he is the course director of the international spring school on the digital library and epublishing for science and technology in geneva and a founding member of the alliance for innovation in science and technology information. mr. luce received a 1996 los alamos distinguished performance award for his contributions ﬁintroducing technological innovations supporting science and technology.ﬂ the research library was corecipient of the 1999 federal library and information center of the year award and a 1997 and 2000 quality new mexico ﬁroadrunnerﬂ recipient for organizational performance excellence based on the malcolm baldrige criteria. clifford lynch* is the director of the coalition for networked information (cni). prior to joining cni in 1997, dr. lynch spent 18 years at the university of california office of the president, the last 10 as director of library automation. he holds a ph.d. in computer science from the university of california, berkeley, and is an adjunct professor at berkeley™s school of information management and systems. he is a past president of the american society for information science and a fellow of the american association for the advancement of science and the national information standards organization. dr. lynch currently serves on the internet2 applications council and is a member of the nrc committee to study digital archiving and the national archives and records administration. he was a member of the nrc committee that published the digital dilemma: intellectual property in the information infrastructure and served on the nrc committee on broadband lastmile technology. jeffrey mackiemason* is arthur w. burks professor of information and computer science and a professor of economics and public policy at the university of michigan. he is also the founding director of the program for research on the information economy at the university and the director of doctoral studies at the school of information at michigan. his work is primarily in information economics, especially the internet and advanced telecommunications technologies, and the economics of digital information 112electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. content. professor mackiemason received his a.b. in environmental policy from dartmouth, his master of public policy from university of michigan, and his ph.d. in economics from mit. james j. o™donnell is professor of classics and provost at georgetown university. he served in 2003 as president of the american philological association and has been elected a fellow of the medieval academy of america. in 1990, he cofounded bryn mawr classical review, the secondoldest humanities ejournal. his 1998 book, avatars of the word: from papyrus to cyberspace (harvard university press), explores the impact of technologies of writing on the shaping of culture from antiquity to the present. ann okerson* is associate university librarian for collections and technical services at yale university. she is responsible for making digital collections available to the many and varied users at yale and has become an expert in licensing digital information for academic use and on building consortia of libraries to achieve the most effective service at the best price for academic users. prior to joining yale, she served as director of the office of scientific and academic publishing at the association of research libraries. ms. okerson was named serials librarian of the year in 1993 by the american library association (ala) and is also the 1999 recipient of their lita/high tech award. she was a member of the nrc committee on information technology strategy for the library of congress. paul resnick is an associate professor at the university of michigan school of information. he previously worked as a researcher at at&t labs and at&t bell labs, and as an assistant professor at massachusetts institute of technology™s sloan school of management. he received his master™s and ph.d. degrees in electrical engineering and computer science from mit, and a bachelor™s degree in mathematics from the university of michigan. professor resnick™s research focuses on sociotechnical capital, productive social relations that are enabled by the ongoing use of information and communication technology. he was a pioneer in the field of recommender systems (sometimes called collaborative filtering or social filtering). recommender systems guide people to interesting materials based on recommendations from other people. his current research focuses on reputation systems, which apply the ideas of recommender systems to evaluating people. bernard rous is the deputy director of publications and electronic publishing program director for the association of computing machinery. he received his undergraduate education at brandeis university, and an m.a. at the new school for social research in anthropology. mr. rous has worked in publishing at acm from 1980 to the present. his responsibilities have included development and management of a database publishing system for reference publications; development of early cdrom and hypertext products; project manager for sgml publishing production system; development and direction of electronic publishing program; drafting copyright and permissions policy for the networked environment; and establishment of digital library, online bibliographic database, and computing portal with appropriate business models. edward h. shortliffe* (chair, symposium steering committee) is a professor and chair of the department of medical informatics and deputy vice president for information technology for the health sciences division of columbia university. his research interests include medical informatics; issues related to integrated decisionsupport systems and their effective implementation; clinical medicine; and medicalinformatics research and teaching. prior to his current position, he was at stanford university. dr. shortliffe provides expertise in both medicine and computer science. he received an a.b. from harvard in applied mathematics, a ph.d. from stanford in medical information sciences, and an m.d. from stanford. dr. shortliffe is a member of the institute of medicine (iom), the iom council, and the nrc committee on science, engineering, and public policy. he also served as chair of the nrc committee on enhancing the internet for biomedical applications: technical requirements and implementation strategies. alexander szalay is the alumni centennial professor of astronomy and professor of computer science at the johns hopkins university. he is a cosmologist, working on the statistical measures of the spatial distribution of galaxies and galaxy formation. he is the architect for the science archive of the sloan digital sky survey. professor szalay is project director of the national science foundation (nsf)funded national virtual observatory. gordon tibbitts is president of blackwell publishing, usa. since 1982, he has worked to integrate 113electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. electronic publishing technology into traditional publishing business. early on, he recognized the advantages of automating prepress and manufacturing work, including editorial, production, and typesetting functions, and successfully integrated the entire digital work flow at aster publishing corporation in the mid1980s. he also was responsible for automating publishing processes, creating sgml/xml/htmlencoded content, and delivering products electronically in the early 1990s for american health consultants (a thomson company). in addition to his publishing experience, mr. tibbitts worked for several years leading health care software development for two other thomson information technology holdings, dkd and the medstat group. he developed systems sold to leading hospitals and clinics in the united states, integrating clinical and financial informatics, applying clinical best practice with performance optimization concepts drawn from evidencebased, diseasestaging, case management, and jit fields of learning. throughout his career, mr. tibbitts has driven organizations toward the leading edge of technology™s application in disseminating information. with blackwell, he plays a major role in the development of all corporate technology initiatives, including webbased content development. he has worked in executive positions for aster publishing, advanstar, and the thomson corporation. mr. tibbitts received a b.s. in computer science and an m.b.a. from the university of oregon. mary waltham* is a publishing consultant. she was most recently president and publisher for nature and the nature family of journals in the united states, and formerly managing director and publisher of the lancet in the united kingdom. she founded her own consulting company four years ago. its purpose is to help international scientific, technical, and medical publishers to confront the rapid change that the networked economy poses to their traditional business models, and to develop the new opportunities to build publications that deliver outstanding scientific and economic value. ms. waltham has worked at a senior level in science and medical publishing companies across a range of media, which include textbooks, magazines, newsletters, journals, and open learning materials. she served on the nrc committee on community standards for sharing publicationrelated data and materials. ann wolpert became director of libraries for the massachusetts institute of technology in january 1996. she oversees this distributed library system, which consists of five major collections, five smaller branch libraries in specialized subject areas, a feeforservices group, and the institute archives. as of january 1999, her position expanded to include reporting responsibility for the mit press, which publishes approximately 200 new books and more than 40 journals per year in fields related to science and technology. recently, ms. wolpert also assumed oversight of technology review, mit™s magazine of innovation. ms. wolpert™s institute responsibilities include membership on the committee on copyright and patents, the council on educational technology, the campus plan steering committee, the deans™ committee, and the president™s academic council. she chairs the management board of the mit press, serves on the opencourseware interim management board, and is cochair of the internal review committee for financial systems services and information systems. prior to joining mit, ms. wolpert was executive director of library and information services at the harvard business school. her experience also includes management of the information center of arthur d. little, inc., where she additionally engaged in consulting assignments. more recent consulting assignments have taken her to adelphi university in new york, to the campuses of incae in costa rica and nicaragua, and to the malaysia university of science and technology, selangor, malaysia. ms. wolpert is active in the professional library community. she currently serves on the association of research libraries (arl) board of directors and is a member of arl™s scholarly communication committee and of its internet2 working group. she also serves on the board of directors of the boston library consortium. in addition, she is a member of the editorial boards of library & information science research and the journal of library administration. a frequent speaker and writer, she has recently contributed papers on such topics as library service to remote library users, intellectual property management in the electronic environment, and the future of research libraries in the digital age. ms. wolpert serves on the board of trustees of simmons college. in 1998 she was elected to the national network for women leaders in higher education of the american council on education. she received a b.a. from boston university and an m.l.s. from simmons college. 114electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. appendix c symposium participants laura abate himmelfarb library, the george washington university leabate@gwu.edu hal abelson massachusetts institute of technology hal@mit.edu karen albert talbot research library, fox chase cancer center kmalbert@fccc.edu bruce alberts national academy of sciences balberts@nas.edu kerri allen association of research libraries kerri@arl.org mohammad alubaydli national center for biotechnology information (ncbi) mo@mo.md kent anderson new england journal of medicine kanderson@nejm.org carl anderson drexel university libraries ca25@drexel.edu tom arrison the national academies tarrison@nas.edu susan ashworth glasgow university s.ashworth@lib.gla.ac.uk daniel atkins university of michigan atkins@umich.edu john aubry american museum of natural history library jaubry@amnh.org midori baerprice informs, the institute for operations research and the management sciences midori.baerprice@informs.org marcus banks national library of medicine banksm@mail.nlm.nih.gov lori barber scholarone lori.barber@scholarone.com edward barnas cambridge university press ebarnas@cup.org svetla baykoucheva american chemical society (acs) sbaykouchev@acs.org malcolm beasley stanford university beasley@stanford.edu philippa benson cabs, conservation international p.benson@conservation.org sandra berlin american anthropological association sberlin@aaanet.org r. steven berry the university of chicago berry@uchicago.edu iona black yale university, dept. of chemistry iona.black@yale.edu floyd bloom the scripps research institute fbloom@scripps.edu ronald bluestone library of congress rblu@loc.gov martin blume the american physical society blume@aps.org 115electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. robert bovenschulte american chemical society publications rbovenschulte@acs.org monica bradford american association for the advancement of science (aaas) mbradfor@aaas.org kerry brenner the national academies, bls kbrenner@nas.edu sarah brookhart american psychological society sbrookhart@psychologicalscience.org patrick brown stanford university pbrown@cmgm.stanford.edu david bruggeman the national academies dbruggem@nas.edu edward campion new england journal of medicine ecampion@nejm.org gene carbona the medical letter gene@medicalletter.org bonnie carroll cendi bcarroll@infointl.com mary case association of research librarians marycase@arl.org meredith cawley aiaa meredithc@aiaa.org guy chalk johns hopkins university center for communication programs gchalk@jhuccp.org leslie chan university of toronto at scarborough chan@utsc.utoronto.ca nina chang elsevier n.chang@elsevier.com bonnie chojnacki university of maryland bc128@umail.umd.edu pamela clapp hinkle marine biological laboratory pclapp@mbl.edu jeff clark james madison university clarkjc@jmu.edu william cohen the biological bulletin, hunter college cohen@genectr.hunter.cuny.edu eileen collins s&t studies el.collins@verizon.net jim comes univ. of massachusetts medical school james.comes@umassmed.edu joan comstock cadmus comstockj@cadmus.com bridget coughlin the national academies bcoughli@nas.edu c. arleen courtney american chemical society acourtney@acs.org denise covey carnegie mellon, hunt library troll@andrew.cmu.edu nicholas cozzarelli university of california, berkeley ncozzare@socrates.berkeley.edu brian crawford john wiley & sons, inc. bcrawfor@wiley.com vicki croft washington state university croft@wsu.edu 116electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. raym crow chain bridge group crow@chainbridgegroup.com bruce dancik nrc research press c/o renewable resources bruce.dancik@ualberta.ca lloyd davidson northwestern university, mudd library for science & engineering ldavids@northwestern.edu adrienne davis the national academies adavis@nas.edu bart de castro cambridge scientific abstracts bdecastro@csa.com stephanie dean american society for cell biology (ascb) sdean@ascb.org laura dean national center for biotechnology information laura@idiopathic.com masako dickinson american chemical society med96@acs.org heather dittbrenner independent consultant hdittbrenner@comcast.net lisa dittrich association of american medical colleges lrdittrich@aamc.org richard dodenhoff american society for pharmacology and experimental therapeutics rdodenhoff@aspet.org daniel dollar yale university, cushing/whitney medical library daniel.dollar@yale.edu mark doyle the american physical society doyle@aps.org guy dresser allen press, inc. gdresser@allenpress.com michael droettboom johns hopkins university mdboom@jhu.edu james duderstadt university of michigan jjd@umich.edu carol edwards american statistical association caroled@amstat.org anita eisenstadt national science foundation aeisenst@nsf.gov julie esanu the national academies, biso jesanu@nas.edu joseph esposito sri consulting jesposito@sric.sri.com walter finch national technical information service wfinch@ntis.gov henry flores copyright clearance center, inc. hflores@copyright.com martin frank american physiological society mfrank@theaps.org mark frankel aaas mfrankel@aaas.org tracie frederick georgetown university medical center tef7@georgetown.edu theodore freeman allen press, inc. tfreeman@allenpress.com amy friedlander council on library and information resources amfr@loc.gov 117electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. fred friend joint information systems committee f.friend@ucl.ac.uk sherrilynne fuller university of washington sfuller@u.washington.edu ken fulton the national academies kfulton@nas.edu john gardenier independent consultant drgarden@erols.com lorrin garson american chemical society lgarson@acs.org julia gelfand university of california, irvine libraries jgelfand@uci.edu jane ginsburg columbia university school of law ginsburg@law.columbia.edu carter glass american geophysical union cglass@agu.org erica goldstein american society for nutritional sciences goldsteine@asns.org barbara gordon american society for biochemistry & molecular biology bgordon@asbmb.faseb.org albert greco fordham university angreco@aol.com michael greenberg the whitney laboratory mjgberg@aug.com suzanne grefsheim nih libarary sg8d@nih.gov jane griffith national library of medicine jbgriffith@nlm.nih.gov anne gwynn library of congress, congressional research services agwynn@crs.loc.gov melissa hagemann open society institute mhagemann@sorosny.org joel hammond biosis jkhammond@biosis.org charles hancock american society for biochemistry and molecular biology (asbmb) chancock@asbmb.faseb.org holly harden welch medical library, johns hopkins medical institutions hharden@mail.jhmi.edu elizabeth have biosis btenhave@biosis.org michael held the rockefeller university press held@rockefeller.edu stephen heller national institute of standards and technology (nist) srheller@nist.gov robert hershey medical doctor hershey@cpcug.org derek hill national science foundation dhill@nsf.gov carol hunter university of virginia chunter@virginia.edu alvin hutchinson smithsonian institution libraries, national zoological park library hutchinsona@si.edu john inglis cold spring harbor laboratory press inglis@cshl.org 118electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. demarie jackson american psychological association djackson@apa.org beth jacoby university of maryland, baltimore, health science library bjacoby@hshsl.umaryland.edu brent jacocks american speechlanguage hearing association (asha) journals@asha.org michael jensen national academies press mjensen@nas.edu michael jensen harvard university michaeljensen@ssrn.com nels jensen blackwell publishing nels.jensen@usa.blackwellpublishing.com rick johnson sparc rick@arl.org heather joseph bioone heather@arl.org lisa junker american industrial hygiene association ljunker@aiha.org brian kahin university of michigan, ford school of public policy kahin@umich.edu neal kaske university of maryland nkaske@umd.edu cara kaufman kaufmanwills group, llc ckaufman@bellatlantic.net irena kavalek u.s. geological survey library ikavalek@usgs.gov michael keller highwire press michael.keller@stanford.edu maureen kelly consultant mckelly@ix.netcom.com donald king university of pittsburgh dwking@pitt.edu gary kittredge capital city press, inc. gkittred@capcitypress.com barbara koehler welch library, johns hopkins bmk@jhmi.edu michael koenig long island university michael.koenig@liu.edu sheldon kotzin national library of medicine/nih annbornstein@nlm.nih.gov alan kraut american psychological society akraut@psychologicalscience.org marc krellenstein reed elsevier m.krellenstein@elsevier.com thomas kuhn american college of physicians tkuhn@acponline.org catherine langrehr association of american universities catherinelangrehr@aau.edu judith lavoie va rehabilitation research & development judith@vard.org ann link american association of immunologists  the journal of immunology alink@ji.faseb.org anne linton george washington university mlbaml@gwumc.edu 119electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. david lipman national institutes of health/national center for biotechnology information dl2a@nih.gov wendy lougee university of minnesota library wlougee@umn.edu michael luby columbia university ml1047@columbia.edu richard luce los alamos national laboratory rick.luce@lanl.gov richard lucier dartmouth college richard.e.lucier@dartmouth.edu marilyn lux american college of surgeons mlux@facs.org clifford lynch coalition for networked information clifford@cni.org james macdonald the american phytopathological society jdmacdonald@ucdavis.edu jeffrey mackiemason university of michigan jmm@umich.edu amanda maguire american institute of aeronautics and astronautics amandam@aiaa.org gene major national aeronautics and space administration (nasa) major@gcmd.nasa.gov constance malpas the new york academy of medicine cmalpas@nyam.org david martinsen american chemical society dmartinsen@acs.org jan massey the american association of immunologists jmassey@aai.faseb.org joseph mazurkiewicz journal of histochemistry and cytochemistry mazurkj@mail.amc.edu luke mccabe american institute of aeronautics and astronautics lukem@aiaa.org katherine mccarter ecological society of america ksm@esa.org johanna mcentyre ncbi mcentyre@ncbi.nlm.nih.gov bruce mchenry discussion systems bruce@discussionsystems.com robert mckinney cadmus communications mckinneyr@cadmus.com peggy merryman us geological survey library mmerryma@usgs.gov henry metzger the national academies, biso metzgerh@exchange.nih.gov cynthia middleton u.s. government printing office cmiddleton@gpo.gov linda miller library of congress lmil@loc.gov kenneth miller american association of colleges of pharmacy kmiller@aacp.org lenne miller the endocrine society lmiller@endosociety.org kurt molholm defense technical information center kmolholm@dtic.mil 120electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. pat molholt columbia university health sciences division molholt@columbia.edu robert molyneux u.s. national commission on libraries and information science bmolyneux@nclis.gov marguerite montes journal of rehabilitation research & development marx@vard.org john muenning university of chicago press jmuenning@press.uchicago.edu jane murray university of maryland baltimore, health sciences & human services library jmurray@hshsl.umaryland.edu gordon neavill wayne state university aa3401@wayne.edu michael neff american society for horticultural science mwneff@ashs.org kathe obrig himmelfarb library, the george washington university mlbkso@gwumc.edu jack ochs american chemical society jochs@acs.org james o'donnell georgetown university jod@georgetown.edu ann okerson yale university ann.okerson@yale.edu jill o'neill national federation of abstracting and information services (nfais) jilloneill@nfais.org shigeharu ono kinokuniya publications service ono@kinokuniya.com david osterbur harvard university dosterbu@mcb.harvard.edu doris peter the medical letter, inc. dpeter@medicalletter.org walter peter cadmus peterw@cadmus.com stephen phelps capital city press sphelps@capcitypress.com theresa pickel alliance communications group, allen press,inc. tpickel@allenpress.com kevin pirkey dartmouth journal services kpirkey@dartmouthjournals.com barbara pope national academy press bkline@nas.edu heather price journal of bone & mineral research heather@jbmr.org roberta rand university of miami  rosenthal school of marine & atmospheric science rrand@rsmas.miami.edu alan rapoport national science foundation arapopor@nsf.gov howard ratner nature publishing group h.ratner@natureny.com tovah reis brown university tovahreis@brown.edu paul resnick university of michigan presnick@umich.edu 121electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. leonard rhine university of florida health science center libraries lenny@library.health.ufl.edu susan riedel library of congress wrie@loc.gov nancy rodnan faseb nrodnan@faseb.org linda rosenstein university of pennsylvania biomedical library rosenstl@mail.med.upenn.edu beth rosner aaas rosner@aaas.org bernard rous association for computing machinery rous@hq.acm.org kevin rowan the national academies, cosepup krowan@nas.edu lucia ruggiero world health organization ruglucia@paho.org john rumble nist john.rumble@nist.gov john sack stanford university sack@stanford.edu jennifer samuels aiaa jens@aiaa.org agnes schonbrunn university of texas  houston agnes.schonbrunn@uth.tmc.edu roger schonfeld the andrew w. mellon foundation rcs@mellon.org edward shortliffe columbia university shortliffe@dmi.columbia.edu elliot siegel national library of medicine siegel@nlm.nih.gov pamela sieving national institutes of health library ps256e@nih.gov william silberg medscape bsilberg@webmd.net natalie simonefountaine analytical sciences, inc. nfountaine@asciences.com robert simoni department of biological sciences rsimoni@asbmb.faseb.org susan skomal american anthropological association sskomal@aaanet.org eric slater american chemical society eslater@acs.org f. hill slowinski worthington international hslowinski@worthingtoninternational.com mackenzie smith mit libraries kenzie@mit.edu kent smith national library of medicine kentsmith@nlm.nih.gov michael smolens 3billionbooks michael@3billionbooks.com john sopka national technical information services wfinch@ntis.gov beth staehle american society of plant biologists beths@aspb.org robert stanley university of alabama, birmingham rstanley@uabmc.edu 122electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. diane sullenberger the national academies sullenb@nas.edu alex szalay johns hopkins university szalay@jhu.edu herbert tabor american society for biochemistry & molecular biology htabor@asbmb.faseb.org marian taliaferro association of american medical colleges mtaliaferro@aamc.org patricia thibodeau duke university medical center thibo001@mc.duke.edu gordon tibbitts blackwell publishing gtibbitts@bos.blackwellpublishing.com jeanne slater trimble american institute of aeronautics & astronautics jeannet@aiaa.org ian tuttle georgetown university tuttlei@georgetown.edu paul uhlir the national academies, biso puhlir@nas.edu victor van beuren american association of pharmaceutical scientists vanbeurenv@aaps.org pamela van hine american college of obstetricians and gynecologists pvanhine@acog.org john vaughn association of american universities johnvaughn@aau.edu jack verna analytical sciences, inc. jverna@asciences.com philip wallas ebsco information services pwallas@ebsco.com mary waltham independent consultant mary@marywaltham.com walt warnick u.s. department of energy walter.warnick@science.doe.gov donald waters the andrew w. mellon foundation pam@mellon.org judy weislogel elsevier j.weislogel@elsevier.com nanette welton university of washington nwelton@u.washington.edu louise wides wides consulting, llc widesconsult@attglobal.net sophie wilkinson american chemical society swilkinson@acs.org alma wills kaufmanwills group, llc almawills12@comcast.net bonita wilson corporation for national research initiatives (cnri) bwilson@cnri.reston.va.us nancy winchester american society of plant biologists nancyw@aspb.org terry wittig george mason university twittig@gmu.edu ann wolpert massachusetts institute of technology awolpert@mit.edu melissa yorks national library of medicine yorks@nih.gov 123electronic scientific, technical, and medical journal publishing and its implications: proceedings of a symposiumcopyright national academy of sciences. all rights reserved. nevenka zdravkovska georgetown university nevenkaz@georgetown.edu laura zimmerman fry communications, inc lzimmerm@frycomm.com eric zimmerman the research authority of barilan university zimmee@mail.biu.ac.il 124
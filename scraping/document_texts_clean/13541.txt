detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/13541big data: a workshop report34 pages | 8.5 x 11 | paperbackisbn 9780309266888 | doi 10.17226/13541committee for science and technology challenges to u.s. national securityinterests; division on engineering and physical sciences; national researchcouncilbig data: a workshop reportcopyright national academy of sciences. all rights reserved.    report of a workshop on big data   committee for science and technology challenges to u.s. national security interests    division on engineering and physical sciences           the national academies press washington, d.c. www.nap.edu big data: a workshop reportcopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, nw washington, dc 20001  notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.  this study was supported by contract hhm40210d0036 between the defense intelligence agency and the national academy of sciences. any views or observations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the organizations or agencies that provided support for the project. international standard book number: 13: 9780309266888 international standard book number: 10: 0309266882  limited copies of this report are available from the division on engineering and physical sciences, national research council, 500 fifth street, nw, washington, dc 20001; (202) 3342400.  additional copies of this report are available from the national academies press, 500 fifth street, nw, keck 360, washington, dc 20001; (800) 6246242 or (202) 3343313; http://www.nap.edu.  copyright 2012 by the national academy of sciences. all rights reserved.  printed in the united states of america  big data: a workshop reportcopyright national academy of sciences. all rights reserved. the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. ralph j. cicerone is president of the national academy of sciences. the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. charles m. vest is president of the national academy of engineering. the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v. fineberg is president of the institute of medicine. the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government.  functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. ralph j. cicerone and dr. charles m. vest are chair and vice chair, respectively, of the national research council  www.nationalacademies.org  big data: a workshop reportcopyright national academy of sciences. all rights reserved. iv committee for science and technology challenges to u.s. national security interests j. jerome holton, tauri group, chair edward m. greitzer, massachusetts institute of technology, vice chair brian ballard, apx labs kenneth i. berns, university of florida college of medicine ann n. campbell, sandia national laboratories dean r. collins, defense advanced research projects agency (retired) sharon c. glotzer, university of michigan j.c. herz, batchtags, llc kenneth a. kress, kbk consulting, inc. darrell long, university of california, santa cruz julie j.c.h. ryan, george washington university janet a. therianos, independent consultant (usaf, retired) elias towe, carnegie mellon university alfonso velosa iii, gartner, inc. eli yablonovitch, university of california, berkeley  staff  terry jaggers, lead deps board director daniel e.j. talmage, jr., study director sarah capote, research associate marguerite schneider, administrative coordinator dionna ali, senior program assistant chris jones, financial associate  big data: a workshop reportcopyright national academy of sciences. all rights reserved. v    preface  the workshop described in this report is the first in a series of three workshops, held in early 2012 to further the ongoing engagement among the national research council™s (nrc™s) technology insightšgauge, evaluate, and review (tiger) standing committee, the scientific and technical intelligence (s&ti) community, and the consumers of s&ti products. a restricted version of this report is available by contacting the public affairs office of the sponsoring agency (defense intelligence agency) directly. we express our appreciation to the members of the committee for science and technology challenges to u.s. national security interests for their contributions to the development of this report. we are also grateful for the active participation of many members of the technology community in the workshop, as well as to the sponsor for its support. the committee also expresses sincere appreciation for the support and assistance of the nrc staff, including terry jaggers, daniel talmage, sarah capote, marguerite schneider, zeida patmon, and dionna ali.   j. jerome holton, chair edward greitzer, vice chair committee for science and technology  challenges to u.s. national security interests  big data: a workshop reportcopyright national academy of sciences. all rights reserved. vi     acknowledgment of reviewers   this report has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the national research council™s (nrc™s) report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscript remain confidential to protect the integrity of the deliberative process. we wish to thank the following individuals for their review of this report:  gilman louie, alsop louie partners, david maddox (nae), consultant, alton romig (nae), lockheed martin aeronautics company, and mikhail shapiro, university of california, berkeley.  although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the views of individual participants, nor did they see the final draft of the report before its release. the review of this report was overseen by louis j. lanzerotti (nae), new jersey institute of technology. appointed by the nrc, he was responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and that all review comments were carefully considered. responsibility for the final content of this report rests entirely with the authoring committee and the institution.        big data: a workshop reportcopyright national academy of sciences. all rights reserved. vii     contents   1 motivation for the workshop 1 2 firstday presentations 3 big data analytics, 3 discussion of big data, 4 big data as too much data, 4  big data as ubiquitous sensor data, 4  big data as data fusion challenges, 5  big data as too much of a good thing, 5 big data feedsš1, 5 computational data, 6 big data feedsš2, 6 discussion of vulnerabilities, 7 data discovery, 7 social networks, 8  3 secondday discussion 9 technical, 9 temporal, 9 personnel, 9 blue process, 10 closing remarks, 10  appendixes a committee biographies 15 b workshop agenda and participants 21 c speaker biographies 23 big data: a workshop reportcopyright national academy of sciences. all rights reserved.        big data: a workshop reportcopyright national academy of sciences. all rights reserved. 1  1 motivation for the workshop in 2012, the defense intelligence agency (dia) approached the national research council™s tiger standing committee and asked it to develop a list of workshop topics to explore the impact of emerging science and technology. from the list of topics given to dia, three were chosen to be developed by the committee for science and technology challenges to u.s. national security interests. the first in a series of three workshops was held on april 2324, 2012. this report summarizes that first workshop, which explored the phenomenon known as big data. the objective for the first workshop is given in the statement of task (see box 11), which states, ﬁthe workshop will review emerging capabilities in large computational data to include speed, data fusion, use, and commodification of data used in decision making. the workshop will also review the subsequent increase in vulnerabilities over the capabilities gained and the significance to national security.ﬂ the committee devised an agenda that helped the committee, sponsors, and workshop attendees probe issues of national security related to socalled big data as well as gain understanding of potential related vulnerabilities. the workshop (see the agenda in appendix b) was used to gather data that is described in this report, which presents views expressed by individual workshop participants. although the committee is responsible for the overall quality and accuracy of the report as a record of what transpired at the workshop, the views presented are not necessarily those of all the workshop participants, the committee, or the national research council. this workshop report was not intended to provide a comprehensive review of the state of big data. chapter 2 of this report summarizes presentations made and discussions held on the first day of the workshop, april 23, 2012. chapter 3 chronicles the presentations and discussions from the second day of the workshop, april 24, 2012. the three appendixes contain, in order, the biographies of the committee members, the workshop agenda and lists of attendees, and the biographies of the presenters.          big data: a workshop reportcopyright national academy of sciences. all rights reserved.2 report of a workshop on big data                   box 11 statement of task an ad hoc committee will plan and conduct three workshops on the science and technology (s&t) fields noted below that have potential impact on u.s. national security.  ł big datašthe workshop will review emerging capabilities in large computational data to include speed, data fusion, use, and commodification of data used in decision making. the workshop will also review the subsequent increase in vulnerabilities over the capabilities gained and the significance to national security. ł future of antennasšthe workshop will review trends in advanced antenna research and design. the workshop will also review trends in commercial and military use of advanced antennas that enable improved communication, data transfer, soldier health monitoring, and other overt and covert methods of standoff data collection. ł future battlespace situational awarenessšthe workshop will review the technologies that enable battlespace situational awareness 1020 years into the future for both red and blue forces. the workshop will emphasize the capabilities within air, land, sea, space, and cyberspace.  the committee will design the workshops to address u.s. and foreign research, why s&t applications of technologies in development are important in the context of military capabilities, and what critical scientific breakthroughs are needed to achieve advances in the fields of interestšfocusing detailed attention on specific developments in the foregoing fields that might have national security implications for the united states. the workshops will each also consider methodology to track the relevant technology landscape for the future. each of the three workshops will feature invited presentations and panelists and include discussions on a selected topic including themes relating to defense warning and surprise. the committee will plan the agenda for the workshops, select and invite speakers and discussants, and moderate the discussions. each event will result in a workshop summary that will be subject to appropriate institutional review prior to release.  big data: a workshop reportcopyright national academy of sciences. all rights reserved.3  2 firstday presentations  big data analytics rod smith, emerging technologies, ibm the following is a summary of the presentation made by rod smith, who attended the big data workshop via teleconference. according to smith, ibm recognizes that with the proper tools and understanding of customer needs, there is significant business potential in gleaning information from the burgeoning information loosely termed ﬁbig data.ﬂ big data grows by the easy sharing of data on the web and with mobile applications, which now is the backbone of social interactions and many business transactions. while the sources of data are growing in both number and magnitude, the cost of storage and processing continues to decline. by utilizing early and direct customer engagement, ibm has developed business insights enabling quick, profitable, and advantageous decisions. when integrated with data from social media like twitter (7 terabytes/day) and facebook (10 terabytes/day), big data acquires a realtime dimension. the value increases even more with the addition of proprietary data. these combined sources of data yield a stronger, discernible signal that illuminates sights and events that might otherwise go unnoticed. one example of the realtime effect of social media is the reporting of the august 2011 earthquake in mineral, virginia: twitter users posted reports in about 40 seconds, whereas the u.s. geological survey issued reports, based on seismometer readings, 2 minutes after the same event. open source projects like hadoop1 and cassandra2 are common platforms for big data solutions. the ibm tools for such analyses are refined or modest modifications of rapidly evolving and widely available web technologies so that processing developments using java, linux, and xml continue without direct investments from ibm. using the open source environment is both economical and ensures that efforts continuously remain at the cutting edge of technology. an example of what ibm can do with these capabilities comes from the support it provided to the mergers and acquisition department of american express (amex), for which ibm produced critical decision information through discovery and analysis of public and private data on intellectual property. the amex business question was whether or not the innovation of a specific company was enhanced by a particular business acquisition. the analysis began with a review of the company™s patents, ranking them in value in accordance with the number of times they were cited in other patents. this analysis included all u.s. patents (1,400,000) from 2002 to 2009 and another 6,100,000 u.s. and international patents. with approximately one hour of processing time, the analysis showed that one patent had 67 citations, and 24 patents had one citation each. this search triggered an ancillary question, were these patents involved in litigation?, which resulted in the identification of 3600 cases from the federal circuit court of appeals (19932007).   1hadoop is a crossplatform distributed file system that allows massive interaction of computationally independent systems processing enormous amounts of data. it is a product of the apache software foundation. 2cassandra is a product of the apache software foundation, which is dedicated to open source products. apache cassandra is a database management system that is designed to handle massive amounts of data distributed across many systems. big data: a workshop reportcopyright national academy of sciences. all rights reserved.4 report of a workshop on big data ibm believes that such applications are only beginning, demonstrating the start of a possible next wave of business applications. further, the combination of astute data analytics from ibm and continued contact with customers is key to success. smith posited that integrating social media analytics is critical to reducing time to value.  discussion of big data darrell long and gilman louie workshop attendee gilman louie and committee member darrell long next led a discussion on big data challenges. the challenges of big data are difficult to categorize, primarily because the exact definition of "big data" varies according to the intentions of the speaker. as such, several participants noted that it is important to specify precisely which problem applies in which context and then approach the problem from that definitional space. there was a robust dialog that included four different ways to view the problem: volume3 of data (too much data), ubiquity of sensor data, data fusion challenges, and too much of a certain type of data.  big data as too much data beginning with the problem of big data as simply too much data, or overwhelming amounts of data, many participants felt that the challenge was not new. it was pointed out that too much data had always been a problem, particularly for aggressive collectors of data, as large governments tend to be, and that the result of too much data typically inspires new approaches for handling the increasing amounts of data. several participants, however, noted two elements of the current era of big data that seemed to be different from previous eras. the first element was the relationship of data to individuals, uniquely and globally; i.e., the big data challenges of this era seem to be mostly about the data associated with individual movements, preferences, sentiments, and thoughts. this situation differs from previous eras in which big data tended to be generated as a result of economic activities, wars, and science. the individualization of big data stems primarily from the social networking phenomenon but is also enabled by the credit and debit card industries and the logistics industry, particularly pointofsale applications. other workshop participants noted that the second element was the importance of algorithmic analysis of data; i.e., the use of math, machine learning, and human emotionbehavior analyses (such as sentiment analyses) seems to have made both a quantitative and a qualitative difference in how data is used and interpreted.  big data as ubiquitous sensor data it was noted by several workshop participants that part of the forcing function of the big data problem is what is called the data ingest challenge: more data is originating from more sensors. these sensors range from social network updates (facebook posts, tweets, blog posts, etc.) to embedded, distributed utility functions (wifi repeaters, cameras, financial transactions). some participants suggested that the greater variety of sources of data and data inputs requires different approaches to data integration and analysis, and also contributes to data communication and storage challenges.   3some believe that unlike in the ﬁold worldﬂ where volume was a problem, in the big data world, volume is a friend: even dirty data can increase the ﬁresolutionﬂ of an entity. in the big data world, data is processed differently. unlike in the old world where data was processed by reducing collections down to semifinished and finished intelligence (known as the ints) and then reintegrating it (allsource analysis) to produce knowledge, in the big data world, data is computed all at once and across different data types, to reveal or allow discovery of knowledge and intelligence. big data: a workshop reportcopyright national academy of sciences. all rights reserved.firstday presentations 5 big data as data fusion challenges the great variety of data types, including audio, video, text, geographic location markers, and photos, were discussed by many workshop participants as having contributed to the growing need for different approaches to data fusion. the point was made that prior to 1999, most of the data available for analysis was structured. now it is mostly unstructured and varies in terms of format and dimensionality. for example, fusing text to video is challenging, particularly if the video is not annotated in any way that allows the analyst (and the analysis) to know at what point in the video the text is pertinent. it was noted that these types of data fusion issues are of utmost concern to the intelligence community. the fusion problem, of course, is simply the ﬁtip of the iceberg.ﬂ an issue is how the data can be presented for cognitive review, i.e., how they might be visualized. a workshop participant commented that representational data can contribute to solving this issue, but that may be simply replacing one form of metadata (existing text tags) with a different sort of metadata (representational constructs of the actual data).  big data as too much of a good thing there is a point at which conventional approaches to storage (e.g., copies of files) may stop scaling. this was characterized as reflecting the difference between conventional and quantum physics: below the petabyte range, data storage is ﬁnewtonianﬂ; whereas at greater than petabyte sizes, it becomes ﬁeinsteinian.ﬂ novel approaches to storage may help, such as the use of mathematical techniques for distributing elements of data sets and then recreating them as needed. challenges related to representational data versus fully captured data include preprocessing, distribution of processing actions, reduction of communications needs, ﬁdata to decision,ﬂ targeted ads, signatures/signals identification, and ﬁanalysis at the edge.ﬂ  big data feedsš1 eldar sadikov of jetlore eldar sadikov of jetlore (formerly qwisper) was asked to present as a representative of the community that exploits largescale data for social analysis. jetlore is capable of taking in unstructured data from social networking sites and producing detailed analysis. sadikov noted that one of the challenges is in natural language processing, particularly in using context to recognize entities and relationships in unstructured texts written in less than grammatically correct language. he discussed how, in contrast to only a few years ago, there is a wealth of data available in addition to the traditional textual content sensor data; these data include geocoordinates, image and video, and data from many other sources that are not evident to the active user. he discussed using this information for nontraditional purposes such as event detection. he referred to the speed of reports generated via twitter, which are much quicker than those produced through traditional methods. much of the discussion focused on the profound changes in the amount of data that is available, and the ability to fuse such data to derive information in ways that were not possible in the past. he also discussed where the best work was being done, and he learned that some of the foundational mathematics was done in russia.    big data: a workshop reportcopyright national academy of sciences. all rights reserved.6 report of a workshop on big data computational data chris gladwin of cleversafe in his presentation on computational data, chris gladwin provided an overview on the size and scale of data as technologists move into the next 10 years and the approaches that industry is using to be able to cope with it. according to gladwin, the world™s data requirements are growing exponentially. while much of the data of the past was textual (e.g., html 1.0 web pages) or in documents, most of the new data (up to 85 percent of it) is now unstructured data in the form of media (audio, video, images) and other binary large objects (blobs). complexity arises when these data must be indexed, accessed, and distributed effectively. as an example, gladwin noted that one of his current customers, shutterfly, needs to be able to serve 1.4 million photos per hour to its customer base with little to no perceived delay between request and response. gladwin noted that the new challenges related to massive amounts of data supporting dynamic requests have given birth over the last decade to a whole new subset of mathematical techniques for storage. erasure coding for instance, offers a means for breaking data into recoverable chunks and distributing it across more than one storage device, thus hardening the data against loss. gladwin noted that cleversafe uses a 20of26 strategy in which it needs only 20 slices of data to recover the whole set (the data is stored as 26 slices). he stated, ﬁat a quadrillion bits, you can™t assume they are all right.ﬂ through errorcoding techniques, data can be protected against corruption. the final portion of the briefing addressed scalability of data storage systems. gladwin pointed out that hadoop, for instance, is limited in the number of nodes that are addressable within its name spacešlimited to a few thousand nodes. data storage and access needs in the exabyte range would require new computationally efficient ways to build systems capable of supporting these needs. gladwin noted that his company was able to prove the feasibility of a storage architecture, not yet in use, that could support 10 exabytes of datašthe single largest storage architecture in the world. an onthespot backoftheenvelope calculation shows that this highly distributed and decentralized architecture would need roughly 300 million conventional hard drives and consume approximately 2.4 gw just to operate the hard drives (not including the power demands of servers and networking infrastructure for such tasks as thermal management). the discussion then explored the extreme challenge that such a large data storage structure would present to the current ability to generate power at this scale.  big data feedsš2 john marion of logos technologies john marion of logos technologies described a persistent surveillance system that evolved from a prototype developed at lawrence livermore national laboratory into a series of systems that have been operationally deployed. the fundamental technology is a group of highresolution image sensors mounted on a gimbaled platform carried aboard aircraft. the challenges presented by such sensors are centered on limited communication bandwidth, and although various communication technologies have been proposed, none has provided the necessary bandwidth. marion stated that the current strategy is to exchange arrays of magnetic disks physically, but it is understood that techniques need to be developed for processing all the data on the platform in order to reduce the decision latency. by performing computation at the edge, for example, by using models to enable detection of change, marion said, only small amounts of data need to be transmitted over the limited communications bandwidth that is available. even though processing at the edge will be necessary, it is not anticipated that raw data will lose its value: some problems can be addressed through processing on the platform, while others will need higherperformance processing and fusion with other data that will be available only on the ground. big data: a workshop reportcopyright national academy of sciences. all rights reserved.firstday presentations 7  discussion of vulnerabilites al velosa committee member al velosa led a discussion on vulnerabilities. he mentioned three core areas of vulnerability in the big data arena: infrastructure, data and analysis, and tools and technology. infrastructure presents opportunity for vulnerabilities. for example, adversaries can and do have the same level of access to equivalent types of infrastructure as the united states does. the infrastructure tends to be built on standardized equipment that is available globally and can be installed by a large number of service providers. for those adversaries who cannot afford the infrastructure, plenty of companies offer this level of infrastructure by means of a ﬁpay as you goﬂ business model. the infrastructure does have the benefit of many data centers with redundant backups of the data, but some key facilities can be crippled by disrupting their power supply. data and analysis also present a variety of vulnerabilities. data, and lots of it, is available to opposing forces, often for free. but the proliferation of data, and the speed with which it is used and consumed, sometimes limit how much the united states verify the data. as a result, there is the possibility of false and malicious data being planted in u.s. systems (e.g., false data on stock movements that can drive capital markets or data that can start a panic about a transmittable disease or contamination of food). data vulnerabilities operate over different time frames. in an attempted manipulation of financial markets, a shorttimeframe response might involve an army of bankers immediately figuring out what is happening and then announcing that this is all misinformation. a longtimeframe response might be needed in a scenario where people are faking illness: authorities might need time to discern what is happening and to determine that misinformation has been spread and that there is no cause for alarm. the analysis and communication of the truth associated with these scenarios are also challenging, in that trust becomes a critical issue. thus data are very susceptible to issues that center on trust. tools and technology are widespread and often available on an open source basis. thus opponents often have access to the same levels of analysis that the united states does. furthermore, the large number of computer scientists graduating from both u.s. and foreign universities guarantees them a talent base that may develop opportunities and tools that opponents could deny to the u.s.  data discovery benjamin reed of yahoo! research ben reed, a research scientist at yahoo! research, gave a presentation on data discovery. one of the assumptions on which yahoo! operates is that everyone has the same kind of infrastructure, as compared to yahoo!. the secret sauce (what is kept confidential) is the code used to link data pieces. yahoo! tries to anticipate the information wants of the general online population so that when someone seeks elaboration on a particular piece of news and goes to the yahoo! website, he or she can easily find those details. for example, yahoo! kept track of the buzz surrounding the death of michael jackson so that people could find out about the details. yahoo! also embraced open source implementation (specifically hadoop). yahoo! has commoditized hardware, software, and who can use the platforms. yahoo! data analysis tools are open source (such as pig4) and have contributors from all over the world. users actually contribute, and do not just use the tools. there is also a yahoo! asia office that coordinates these contributions.   4according to wikipedia, a ﬁ‚pig™ is a highlevel platform for creating mapreduce programs used with hadoop. pig was originally developed at yahoo! research around 2006 for researchers to have an ad hoc way of creating and executing mapreduce jobs on very large data sets. in 2007, it was moved into the apache software foundation.ﬂ big data: a workshop reportcopyright national academy of sciences. all rights reserved.8 report of a workshop on big data automated, largescale datagathering agents, known as bots (short for software robots), generate a large volume of traffic to yahoo! and tend to tax yahoo! with large quantities of queries. yahoo! deals with bots by giving them a ﬁfakeﬂ version of the information they seek. because attempts to ignore the bot queries, once they are identified as such, simply result in a multiplication of even more bot queries, yahoo! simply replies with a version of what the bot asked for, minimally satisfying the query, but well enough to pacify the bot and clear bandwidth for other users. hardware to process big data is easily accessible, the software is free, and the processing models are accessible, and so big data is no longer a niche marketšthere is no barrier to the commercial market. during the workshop discussion, a question was asked about whether parallel processing is difficult across multiple nodes with highperformance computing. yahoo! does do parallel computing, with algorithms designed to solve the big data problem that are often separate and distinct from those ubiquitous to traditional highperformance computing. with big data, a whole lot of information comes in, and not much comes out. in highperformance computing, a little bit of information comes in, but the outputs are tremendous. so, a different type of tool is required for these two data environments.  social networks paul twohey of ness computing ness computing (not to be confused with ness corporation) is a small startup headquartered in los altos, california. currently with 15 employees, it embraces data analysis for commercial purposes. the firm™s likeness search engine, which draws data from various sources of information, such as social networking sites, applies machine learning techniques to tease out patterns that are then used to establish recommendations for users, generating a small profit per transaction. ness computing describes what it does in the following terms: ﬁness creates products that connect our users with new experiences.ﬂ its flagship product, ness, is an application that runs on mobile phones to provide users with restaurant recommendations. to seed the analysis, users are asked to input reviews of 10 restaurants. based on these reviews and the powerful backend analysis of data from other users and social networks, the ness ﬁappﬂ provides recommendations on what other restaurants the user can be expected, in all probability, to like. according to workshop presenter paul twohey, his firm™s approach to developing products is based on the emerging realities of electronic commerce and free social networking, whereby the user exchanges personal information for services. it requires extensive backend computing power, which is different from highperformance computing. he stated that the computing approach is that of taking an enormous amount of data, performing complex mathematical analysis (including sentiment analysis), and providing customized output per user. ness computing hires only employees with superb math and computer science skills, and a workshop participant voiced that this approach is problematic, given the low availability of individuals with such skills. several participants at the workshop noted the need for an emphasis by u.s. educational institutions on advanced math skills so that the u.s. workforce can remain competitive in the future. big data: a workshop reportcopyright national academy of sciences. all rights reserved.9  3 secondday discussion  after the first presentations of the day, workshop participants began several hours of open discourse recapping and further exploring topics raised in the morning and on the previous day. numerous anecdotes shared demonstrated that ﬁdrowning in dataﬂ was not a new problem for the intelligence community or for dod at large, and that the coming paradigm shift arises from the challenge that big data presents more than just a ﬁvolumeﬂ problem. according to many workshop participants, the big data challenge has at least three aspectsštechnical, temporal, and personnelšeach with very different implications. technical the technical aspect of big data encompasses a range of obstacles that hide under the labels of ﬁjust hardwareﬂ or ﬁjust softwareﬂ or ﬁjust human factors.ﬂ investment by both the government and the private sector is ongoing in each of these areas, with a growing understanding that the greatest progress lies in attending to all three from a unified perspective rather than treating them as independent investments. temporal discussion among the participants revealed two very different timebased challenges, realtime (an increasing rate of data production that accompanied some realtime sensor applications) and retrospective (an increasing amount of data over a larger and larger period of time). many contributed to the discussion with examples of how the increase in sensor data acquisition rates was making it more and more difficult to transmit in real time to another point for analysis. this reality has prompted a great deal of attention to methods of digesting, parsing, or triaging data, resulting in transmission of only the actionable parts. alternately, the discussion touched on the exponentially increasing size of historical data sets, which are fueling interest in inference techniques. personnel many workshop participants argued that individuals who are trained in and work at the cutting edge of big data are currently in short supply and that the supply is dwindling even as demand continues to grow. given that a large number of new advanced degrees in this area are awarded to foreign nationals who then return to their countries of origin, some argued that it would seem that efforts to recruit and retain such individuals in positions in the united states should be redoubled. some workshop participants said that machines and humans must learn to work together to exploit the burgeoning world of big data. for example, gary kasparov™s 2005 freestyle chess tournament, in which teams could be composed of any combination of humans and computers, was won by two amateur chess players running open source chess engines on simple offtheshelf laptopsšnot by grandmasters, prodigies, or chess supercomputers. big data analysis requires much human interaction and guidance, and the optimal combination is not necessarily the best machines and the brightest humans. it may instead be the right interface between human and machine.  big data: a workshop reportcopyright national academy of sciences. all rights reserved.10 report of a workshop on big data  blue process asher sinensky of palantir technologies asher sinensky of palantir started his talk by using a chess metaphor for human and machine interaction. humans have an uncanny ability to make decisions and analyze ideas. this ability is unique to humankind. he stated that for the technological enterprise it is important that humans be in the analysis process. humans are key for conceptualizing new innovations and new ideas after data analysis. see box 31 for sinensky™s full comments.  david thurman of pacific northwest national laboratory david thurman, computing strategy lead at pnnl, asserted that, in the future, computing applications will move toward computer architectures that bring together the different strengths of customized hardware and software capabilities to evaluate different types of distributed data (bringing together many types of computing). a key issue is being able to derive resultsgenerated data from across different agencies. he said that in 2005 pnnl created a new computer architecture that analyzes data where it resides rather than making copies of the data and transferring it to one central location. this architecture assumes the availability of new highly efficient algorithms tailored to distribute the heterogeneous data sets. many of these algorithms are derived from the rapidly evolving commercial offtheshelf (cots) processing applications of big data.  closing remarks at the end of the april 2012 workshop, the chair asked committee members and speakers in attendance to make any final comments on what they had heard over the two days. these comments are made as a summary for the workshop:  ken kressšﬁbig dataﬂ is more than just a change of scalešit is a more persistent threat than we have previously observed.ﬂ al velosašﬁprogress in the humanmachine interface will reduce friction and will allow capability enhancement for the individual, but we will mostly likely experience a fluidity of people more pronounced than we have ever seen.ﬂ david thurmanšﬁi am struck by how different are the threat and impact of big data versus ballistic missiles and other classical threats because of the acceleration of commercially driven offerings, none of which are as controllable as the classical threat domains.ﬂ asher sinenskyšﬁnow more than at any time in history, we must demand flexibility and adaptability in the tool sets we create for the problem at hand, because those problems are changing faster than ever before, and we don™t have time to create a new generation of inflexible tools to counter each new twist.ﬂ mikhail shapirošﬁthe highest value should be placed on the human capital, the engineer, and that asset is an asymmetric economic issue.ﬂ brian ballardšﬁthe big question is how to organize the data and make it accessible to the problem solvers. cyber is its own category, but big data is a force multiplier of massive scale, with farreaching implications. succeeding here will allow us to ‚own the net,™ delivering advantages that we posit today but even more importantly, advantages of which we are not yet even aware.ﬂ    big data: a workshop reportcopyright national academy of sciences. all rights reserved.secondday discussion 11                       box 31 chess analogy asher sinensky one of the most important years in the history of big data was 1997, the year that deep blue beat gary kasparov at chess. at first blush, this might not seem like a big data challenge; chess after all has only 64 spaces, 32 pieces, 6 different types of pieces, and only two players. however, when chess is analyzed more deeply, its true complexity emerges. claude elwood shannon, the socalled father of information theory, showed that the number of legal configurations a chess board could realize is approximately 1043. this is obviously an enormous number and is sometimes referred to as the shannon number. a study* by the university of southern california in early 2011 estimated the world™s total digital storage to be on the order of 1021. in this light, chess is clearly huge when considered against the scale of the digital world. even beyond that, a dutch computer scientist, louis victor allis, estimated the gametree of complexity of chess to be approximately 10123. that number is roughly 40 orders of magnitude greater than the estimated number of atoms in the entire universe. the act of computationally playing chess is clearly a ﬁbig dataﬂ problem, and 1997 showed us that computers can do this better than humans can. the next important year in this story is 2005. in that year, gary kasparov decided to host his own chess tournament. in light of deep blue, kasparov become extremely interested in the capabilities of computational systems but also in the ways that computers and humans approach problem solving. kasparov™s 2005 chess tournament was a freestyle tournament in which teams could be composed of any combinations of humans and computers available. grandmasters, prodigies, and chess supercomputers could team up to form super teams. by 2005, it had already been shown that a chess master teamed with a chess supercomputer was far more capable than a supercomputer alone. computers and humans have different and complementary analytic strengths: computers don™t make mistakes, they are highly precise, while humans can use intuition and lateral thinking. these skills can be combined to build truly formidable chess opponents. however, 2005 was different. the winning team, zacks, performed so well many thought it was actually kasparov™s team. however, the truth was much more intriguing. it turns out that zacks was actually two amateur chess players running open source chess engines on simple offtheshelf laptopsšno grandmasters, no prodigies, no chess supercomputers. this was a remarkable outcome that surprised everyone, including kasparov himself. kasparov drew the only conclusion he could: ﬁweak human + machine + better process was superior to a strong computer alone and, more remarkably, superior to a strong human + machine + inferior process.ﬂ this revelation points to the essential evolution of the conclusion from deep blue in 1997šthat humans working together with machines can solve big data challenges better than computers alone. tackling big data means more than just algorithms, highperformance computing, and massive storagešit means leveraging the abilities of the human mind.  *see http://www.computerworld.com/s/article/9209158/scientistscalculatetotaldatastoredtodate 295exabytes. see also zacks  http://chessbase.com/newsdetail.asp?newsid=2461; ﬁfriction in humancomputer symbiosis: kasparov on chessﬂ at http://blog.palantir.com/2010/03/08/frictioninhumancomputersymbiosiskasparovonchess/; and ﬁa rigorous friction model for humancomputer symbiosisﬂ at http://blog.palantir.com/2010/06/02/arigorousfrictionmodelforhumancomputersymbiosis/. big data: a workshop reportcopyright national academy of sciences. all rights reserved.        big data: a workshop reportcopyright national academy of sciences. all rights reserved.           appendixes   big data: a workshop reportcopyright national academy of sciences. all rights reserved.        big data: a workshop reportcopyright national academy of sciences. all rights reserved.15    appendix a committee biographies j. jerome holton, chair, is a senior systems engineer with the tauri group, where he supports the biowatch systems program office within the office of health affairs, department of homeland security (dhs). he provides analysis, advice, and counsel to senior government decision makers on policy, technology, and operations issues related to weapons of mass destruction and their effects on civilian infrastructure, first responders, military forces, and tactical operations. prior to this, he served in a variety of leadership positions for privatesector companies, spanning the gamut from scientific research startup to large management consulting firm. past clients include the office of the deputy assistant to the secretary of defense for counterproliferation and chemical/biological defense, the chemical biological defense directorate of the defense threat reduction agency, the chemical biological national security program of the department of energy, and the dhs science and technology directorate. his work extends broadly across the chemical/biological/radiological/nuclear/conventional explosives detection and countermeasures arena. for several years, he focused on the counterproliferation of, counterterrorism/domestic preparedness issues for, and the detection, identification, and decontamination of chemical and biological weapons. recent accomplishments include fielding information operations tools and enhancing the intelligence, surveillance, and reconnaissance capabilities to detect and defeat improvised explosive devices as well as the development of applique armor solutions to counter explosively formed penetrators. holton previously served on the nrc™s standing committee on defense intelligence agency technology forecasts and reviews (tiger), the committee for the symposium on avoiding technology surprise for tomorrow™s warfighter, and the committee on alternative technologies to replace antipersonnel landmines. he earned his b.s. in physics from mississippi state university and holds m.s. and ph.d. degrees in experimental physics from duke university.  edward m. greitzer (nae), vice chair, is the h.n. slater professor, department of aeronautics and astronautics at massachusetts institute of technology. he received his a.b., s.m., and ph.d. from harvard university. prior to joining mit in 1977, he was with united technologies corporation, and, more recently, he was on leave at united technologies research center as director, aeromechanical, chemical, and fluid systems. from 1984 to 1996 he was the director of mit™s gas turbine laboratory, and from 1996 to 2002 was associate head, and from 2006 to 2008 deputy head, of the department of aeronautics and astronautics. his research interests have spanned a range of topics in gas turbines, internal flow, turbomachinery, active control of fluid systems, universityindustry collaboration, and robust gas turbine engine design; he was the mit lead for the cambridgemit institute silent aircraft initiative. he teaches graduate and undergraduate courses in the fields of propulsion, fluid mechanics, thermodynamics, and energy conversion, as well as the department™s undergraduate project course. greitzer is a threetime recipient of the american society of mechanical engineers gas turbine award for outstanding gas turbine paper of the year; in addition, he received the asme freeman scholar award in fluids engineering, the international gas turbine institute scholar award, and big data: a workshop reportcopyright national academy of sciences. all rights reserved.16 report of a workshop on big data publication awards from the american institute of aeronautics and astronautics and the institution of mechanical engineers. he has also received the aircraft engine technology award from the asme international gas turbine institute, the u.s. air force exceptional civilian service award, and the asme r. tom sawyer award. he has been a member of the u.s. air force scientific advisory board and the nasa aeronautics advisory committee, and he is an honorary professor at beihang university (beijing). greitzer has published more than 70 papers and is lead author of the book internal flow: concepts and applications, published by cambridge university press. he is a fellow of aiaa and asme, a member of the national academy of engineering, and an international fellow of the royal academy of engineering.  brian ballard founded and currently serves as the ceo of apx labs, a software company focused on leading development into wearable augmented reality products at the nexus of computer vision, user experience, and seethrough displays. previously he served as the director of product development and vice president at battlefield telecommunication systems (bts), where he led the development of defenseoriented augmented reality and biometric data fusion applications. as part of his portfolio, he was also heavily engaged in developing mobile 3g and 4g networks, devices, and applications for tactical military employments. prior to joining bts, ballard served as the cto at mav6, where he was involved in the development of emerging networking and embedded systems technologies for intelligence, surveillance, and reconnaissance (isr) systems and applications in government and military. he is a highly experienced professional in the field of national intelligence systems and computer engineering. employed for more than 10 years with the national security agency, he has dealt with all forms of data collection, dissemination, processing, and visualization. ballard holds an m.s. and a b.s. in electrical and computer engineering from carnegie mellon university, and a master™s of technology management from the university of maryland. he is currently working on an mba at the university of maryland.  kenneth i. berns (nas/iom) is director of the university of florida genetics institute and distinguished professor of molecular genetics and microbiology, medicine. he has served as a member of the composite committee of the united states medical licensing examination, chairman of the association of american medical colleges, president of the association of medical school microbiology and immunology chairs, president of the american society for virology, president of the american society for microbiology, and vicepresident of the international union of microbiological societies. he is a member of the national academy of sciences and the institute of medicine. berns™s research examines the molecular basis of replication of the human parvovirus, adenoassociated virus, and the ability of an adenoassociated virus to establish latent infections and be reactivated. his work has helped provide the basis for use of this virus as a vector for gene therapy. berns™s m.d. and his ph.d. in biochemistry are from the johns hopkins university.  ann n. campbell is director, information solutions and services, at sandia national laboratories. her organization develops and stewards a broad range of software applications and information systems for both internal (enterprise) and external customers to facilitate the delivery of effective national security technologies. at sandia, she previously served as senior manager and deputy to the chief technology officer for cybersecurity science and technology (s&t). in that role she was responsible for developing and implementing an institutional strategy for cyber s&t. she was recently acting director for sandia™s cyber security strategic thrust, leading the lab™s activities to expand sandia™s cyber workforce and infrastructure, and strategies to provide increased support for sandia™s national security sponsors™ cyber missions. campbell has also served as deputy for technical programs for the defense systems and assessments strategic management unit (dsa smu). in that role she advised the dsa vice president regarding the big data: a workshop reportcopyright national academy of sciences. all rights reserved.appendix a 17 unit™s national security programs, was responsible for strategic planning and the investment strategy for the dsa, and assisted with implementation of the laboratory™s cyber strategy. from 2003 to 2007, campbell led the assessment technologies group in sandia™s information systems analysis center. she was responsible for development, coordination, and oversight of programs focusing on vulnerability assessments and development of national security solutions in information technologies for multiple government sponsors. from 1999 to 2003 she was manager of the microsystems partnerships department, which assessed and addressed microelectronics vulnerabilities for a variety of government sponsors. in that role campbell led sandia™s program to support the dod antitamper initiative. she joined the technical staff at sandia in 1985 and had assignments in the materials and process center and the microsystems science, technology, and components center. she conducted research on the microstructure and physical properties of advanced materials, the physics of microelectronics failures, and the development of advanced microelectronics failure analysis techniques. campbell serves on the national academies™ standing committee on technology insightœgauge, evaluate and review (tiger). she is a senior member of ieee and served as vice president of membership for the ieee reliability society and on the management committee and board of directors for the ieee international reliability physics symposium. she has more than 20 publications and several patents. she holds a b.s. degree in materials engineering from rensselaer polytechnic institute and m.s. and ph.d. degrees in applied physics (materials science concentration) from harvard university. dean r. collins recently retired as a deputy director of darpa™s microsystems technology office (mto); as a chief scientist he was responsible for the monitoring, analysis, and evaluation of research projects directed by mto program managers and also participated in the concept planning for leading mto into new programs beyond the current state of the art in electronics, photonics, microelectromechanical systems (mems), component architectures, and algorithms. he managed the mto program on integrated circuit cybersecurity. prior to joining darpa, collins was director for the advanced research and development activity (arda) in information technology. arda functioned as a joint activity of the intelligence community and the department of defense, addressing highrisk/highpayoff information technology problems that had broad impact across both supporting communities. collins initiated arda™s key cyber security effort. he was also a member of the intelligence community advanced research and development committee and managed the arda quantum information science effort. prior to joining arda, collins was with the national institute of standards and technology (nist), where he was chief of the high performance systems and services division, the largest division at nist. this position focused on information technology with a strong commercial bias, and the topics investigated ranged from biometrics to electronic books. previously, collins was with texas instruments, as director of the system components lab, which was responsible for all research on iiiv devices, nanoelectronics, photonics, and neural networks. prior to that, he was director of the interface technology lab, which was responsible for all sensor and display research, including lcds, dlps, and ccds. collins is a fellow of the ieee, a member of the american physical society, and a registered professional engineer. he has published more than 40 refereed articles and has 10 issued u.s. patents.  sharon c. glotzer is the stuart w. churchill collegiate professor of chemical engineering and a professor of materials science and engineering at the university of michigan (um), ann arbor, and is director of research computing in the um college of engineering. she also holds faculty appointments in physics, applied physics, and macromolecular science and engineering. she received a b.s. in physics from ucla and a ph.d. in physics from boston university. prior to joining um, she worked at the national institute of standards and technology. her research focuses on computational nanoscience and simulation of soft matter, selfassembly and materials design, and computational science and engineering and is sponsored by the dod, doe, nsf, and big data: a workshop reportcopyright national academy of sciences. all rights reserved.18 report of a workshop on big data the j.s. mcdonnell foundation. glotzer is a fellow of the american physical society and of the national security science and engineering faculty, and she was elected to the american academy of arts and sciences in 2011. she has served on the national academies™ solid state sciences committee; technology warning and surprise study committee; biomolecular materials and processes study committee; modeling, simulation, and games study committee; and technology insightœgauge, evaluate, and review (tiger) committee. she is involved in roadmapping activities for computational science and engineering, including chairing or cochairing several workshops, steering committees and panagency initiatives, and she serves on the advisory committees for the doe office of advanced scientific computing and nsf directorate for mathematical and physical sciences. glotzer is also cofounding director of the virtual school for computational science and engineering under the auspices of the nsffunded blue waters petascale computing project at the national center for supercomputing applications.  j.c. herz is chief executive officer at batchtags, llc. she is also a technologist with a background in biological systems and computer game design. her specialty is massively multiplayer systems that leverage social network effects, whether on the web, mobile devices, or more exotic highend or grubby lowend hardware. she currently serves as a white house special consultant to the office of the secretary of defense (networks and information integration). defense projects range from aerospace systems to a computergamederived interface for nextgeneration unmanned air systems. hertz is one of the three coauthors of osd™s open technology development roadmap. she serves on the federal advisory committee for the national science foundation™s education directorate. in that capacity, she is helping nsf harness emerging technologies to drive u.s. competitiveness in math and science. hertz was a member of the national research council™s committee on it and creative practice and is currently a fellow of columbia university™s american assembly, where she is on the leadership team of the assembly™s next generation project. in 2002, she was designated a global leader for tomorrow by the world economic forum. she is a member of the global business network; a founding member of the ieee task force on game technologies; a term member of the council on foreign relations; and a member of the advisory board of carnegie mellon™s etc press. hertz graduated from harvard university with a b.a. in biology and environmental studies, magna cum laude. she is the author of two books, surfing on the internet (little brown, 1994), an ethnography of cyberspace before the web, and joystick nation: how videogames ate our quarters, won our hearts, and rewired our minds (little brown, 1997), a history of videogames which traces the cultural and technological evolution of the first medium that was born digital and how it shaped the minds of a generation weaned on nintendo. her books have been translated into seven languages. as a new york times columnist, hertz published 100 essays on the grammar and syntax of game design between 1998 and 2000. she has also contributed to esther dyson™s release 1.0, rolling stone, wired, gq, and the calgary philatelist. kenneth a. kress is a senior scientist for kbk consulting, inc., an affilate of montana state university™s department of physics, and a consultant for booz allen hamilton, where he specializes in quantum information science and other technical evaluations and strategic planning for intelligence and defense applications. some of his past clients include darpa™s microsytems technology office, noblis, georgia tech research institute, mitretek systems inc., and lockheed martin™s special programs division. from 1971 to 1999 he worked in a series of positions at the central intelligence agency™s directorate of operations, office of development and engineering, and finally, office of research and development (ord); first as a research and development manager, later as a program manager, and finally as an ord office senior scientist responsible for management support, the development of technical and strategic plans, and dod interagency coordination for advanced technology. he is the inventor of the solidstate neutron big data: a workshop reportcopyright national academy of sciences. all rights reserved.appendix a 19 detector, for which he won an award in 1981. he holds a ph.d. in physics from montana state university.  darrell d.e. long is the kumar malavalli professor of computer science at the university of california, santa cruz. he holds the kumar malavalli endowed chair of storage systems research and is director of the storage systems research center. he received his b.s. in computer science from san diego state university and his m.s. and ph.d. from the university of california, san diego. his dissertation advisor was jehanfrançois pâris. he is a fellow of the institute of electrical and electronics engineers and of the american association for the advancement of science. he is a member of the ieee computer society, the association for computing machinery, the american society for engineering education, the usenix association, upsilon pi epsilon, and sigma xi. he has broad research interests in many areas of mathematics and science, and in the area of computer science including data storage systems, operating systems, distributed computing, reliability and fault tolerance, and computer security. his research has been supported by the national science foundation; the department of energy (office of science and national nuclear security administration); lawrence livermore, los alamos, and sandia national laboratories; the office of naval research; and a number of industrial sponsors that include ibm, microsoft, netapp, symantec, lsi logic, samsung, hewlettpackard, and data domain. he served as the vice chair and then chair of the university of california committee on research policy. he has served on the university of california president™s council on the national laboratories, and on the science and technology, national security, and intelligence committees. he currently serves on the science and technology committee for both los alamos and lawrence livermore national laboratories. he previously served on the national research council standing committee for technology insightœgauge, evaluate and review. he continues to serve on numerous committees and advisory panels for various federal government agencies.  julie j.c.h. ryan is an associate professor and chair of engineering management and systems engineering at george washington university. she holds a b.s. degree in humanities from the u.s. air force academy, an m.l.s. in technology from eastern michigan university, and a d.sc. in engineering management from the george washington university. ryan began her career as an intelligence officer, serving the u.s. air force and the u.s. defense intelligence agency. after leaving government service, she continued to serve u.s. national security interests through positions in industry. her areas of interest are in information security and information warfare research. she was a member of the national research council™s naval studies board from 1995 to 1998. she has conducted several research projects and has written several articles and book chapters in her focus area.  janet a. therianos, a consultant, has 30 years of military experience. she is a u.s. air force academy graduate with an undergraduate degree in aeronautical engineering; an mba from harvard business school; and a masters of arts in air and space power strategy. she was a national defense fellow and has executive education from harvard™s kennedy school of government, the center for creative leadership, and the intelligence community senior leader program. therianos has flown several military aircraft and has served as a command pilot, flight examiner, flight instructor, and functional check pilot. she also holds an faa airline transport pilot rating. her military career was grounded in operations, but she also had extensive higherheadquarters staff duties, including serving as senior military assistant to the secretary of the air force. her leadership experiences were threaded throughout her career, including several commands. her final military assignment was leading the air mobility command™s directorate of intelligence, where she was responsible for organizing, training, and equipping the air force™s big data: a workshop reportcopyright national academy of sciences. all rights reserved.20 report of a workshop on big data global mobility intelligence units. operationally she led the command™s daily threat working group, which assessed threat levels for all global mobility flight operations.  elias towe is currently a professor of electrical and computer engineering and the albert and ethel grobstein professor of materials science and engineering at carnegie mellon university. he was educated at the massachusetts institute of technology (mit), where he received b.s, m.s., and ph.d. degrees from the department of electrical engineering and computer science. towe was a vinton hayes fellow at mit. after leaving mit he became a professor of electrical and computer engineering, and engineering physics at the university of virginia. he also served as a program manager in the microsystems technology office at the defense advanced research projects agency (darpa) while he was a professor at the university of virginia. in 2001, he joined the faculty at carnegie mellon university. towe is a recipient of several awards and honors that include the national science foundation young investigator award, the young faculty teaching award, and an outstanding achievement award from the office of the secretary of defense. he is a fellow of the institute of electrical and electronics engineers (ieee), the optical society of america (osa), the american physical society (aps), and the american association for the advancement of science (aaas).  alfonso velosa iii is research director for gartner with a focus on sustainability, business ecosystems, and smart cities. he is also agenda manager for electronic equipment research at gartner, concentrating on electronics and semiconductor supply chain research, with a particular focus on global trends for manufacturing, consumption, financing, and the key vendors in the market. velosa has also written extensively about electronics, outsourcing of electronics manufacturing, electronic manufacturing services (ems), original design manufacturing (odm), and semiconductor consumption. he previously worked at or consulted for intel corporation, nasa lewis research center and nasa headquarters, mars & co., and ibm research. velosa graduated from columbia university with a b.s. in materials science engineering; from rensselaer polytechnic institute with an m.s. in materials science engineering; and from thunderbird, the garvin school of international management, with an m.i.m. in international management. eli yablonovitch (nas/nae) is an adjunct professor of electrical engineering at ucla after having served as a full faculty member until 2007. he is currently a professor of electrical and computer engineering at university of california, berkeley. he graduated with a ph.d. in applied physics from harvard university, worked for 2 years at bell telephone laboratories, and then became a professor of applied physics at harvard. in 1979 he joined exxon to do research on photovoltaic solar energy; in 1984, joined bell communications research, where he was a distinguished member of staff and also director of solidstate physics research; and in 1992, joined the university of california, los angeles, where he became the northrop grumman optoelectronics chair and a professor of electrical engineering. yablonovitch™s work has covered a broad variety of topics: nonlinear optics, laserplasma interaction, infrared laser chemistry, photovoltaic energy conversion, strainedquantumwell lasers, and chemical modification of semiconductor surfaces. yablonovitch™s research focuses on optoelectronics, highspeed optical communications, highefficiency lightemitting diodes and nanocavity lasers, photonic crystals at optical and microwave frequencies, and quantum computing and communication. big data: a workshop reportcopyright national academy of sciences. all rights reserved. 21   appendix b workshop agenda and participants  agenda  april 2324, 2012 the beckman center of the national academies irvine, california data as a commodity rod smith, vice president, emerging internet technologies ibm big data (discussion) darrell long, kumar malavalli professor of computer science, and kumar malavalli endowed chair of storage systems research center university of california, santa cruz  computational data chris gladwin, ceo cleversafe  big data feeds eldar sadikov, ceo/founder jetlore john marion, director, persistent surveillance logos technologies  data discovery benjamin reed, research scientist yahoo! research social networks paul twohey, cofounder and vice president ness computing  blue process asher sinensky, lead engineer palantir technologies   big data: a workshop reportcopyright national academy of sciences. all rights reserved.22 report of a workshop on big data  david thurman, computing strategy lead pacific northwest national laboratory  participants committee brian ballard, apx labs kenneth kress, kbk consulting, inc. darrell long, university of california, santa cruz julie ryan, george washington university alfonso velosa, gartner, inc.  speakers chris gladwin, cleversafe john marion, logos technologies benjamin reed, yahoo! research eldar sadikov, jetlore asher sinensky, palantir technologies rod smith, ibm david thurman, pacific northwest national labor paul twohey, ness computing  staff terry jaggers, lead board director daniel talmage, study director sarah capote, research associate dionna ali, senior program assistant  guest or agency represented gilman louie, alsoplouie partners mikhail shapiro, university of california, berkeley defense intelligence agency department of defense national geospatial intelligence agency united states air force  big data: a workshop reportcopyright national academy of sciences. all rights reserved.23   appendix c speaker biographies chris gladwin founded cleversafe in 2004. previously he was the creator of the first workgroup storage server at zenith data systems and was a manager of corporate storage standards at lockheed martin. gladwin also created and managed a number of successful new technology startups, including musicnow, which was acquired by circuit city. he has been the creative force behind the development of the first dispersed storage system to solve the growing global problem of big data storage. gladwin understood the growing issues surrounding unstructured data and the inability of traditional technology solutions to accommodate the explosive growth of digital assets such as audio, video, and imaging. he applied advances in dispersed information technology to storage to create a reliable, costeffective, secure solution with a limitless ability to scale. gladwin holds a degree in engineering from the massachusetts institute of technology.  john marion has a broad background in lasers and optical materials, strategic defense, and persistent surveillance. at lawrence livermore national laboratory he led the team that pioneered the field of persistent surveillance, developing hardware and imageprocessing capabilities and field testing of these new systems. at logos he led the technical effort to develop a deployable system, resulting in constant hawk, the first persistent surveillance system intheater, as well as championing the use of this new intelligence collection and exploitation paradigm in the dod and the intelligence community. currently he leads the logos team developing the kestrel persistent surveillance systems for aerostat deployment and is developing future persistent surveillance systems, including the analysis and visualization tool development for exploitation of the imagery. his group also develops novel systems for the intelligence community and is leading a cyber defense technology development effort sponsored by darpa. he provides technical analysis and support to navair, arl, nvesd, darpa and the cia.  benjamin reed is a research scientist at yahoo! research. he has worked for almost 2 decades in industry, in positions ranging from work as an intern on cad/cam systems, to shipping and receiving applications in os/2, aix, and cics, to operations, to system admininistration research and java frameworks at ibm almaden research (11 years). he arrived at yahoo! research 3 years ago to work on the largest distributedcomputing problems. his main interests now are largescale processing environments and highly available and scalable systems. he has worked largely in open source, including writing and maintaining the linux aironet wireless driver. his research project at ibm grew into osgi which is now in application servers, cars, and mobile phones. two projects for which he has led research are pig and zookeeper, which are apache software foundation projects.  eldar sadikov is on leave from the ph.d. program in computer science at stanford university. while at stanford, eldar conducted research in web search and social network mining. he also worked at google and at microsoft research in web search. he founded qwhisper, whose goal is to make exponentially growing social content more useful by automatically inferring its meaning and giving it structure. qwhisper is a highly stimulating intellectual environment in which robust big data: a workshop reportcopyright national academy of sciences. all rights reserved.24 report of a workshop on big data distributed systems are built that handle webscale data and design algorithms that challenge the published state of the art.  asher sinensky holds a ph.d. from mit in materials science and engineering and oversees product development at palantir technologies, where he is directly responsible for plotting the development roadmap for all features and functions. he has been involved in national security for a decade including having worked at sandia and lawrence livermore national laboratories on projects related to bio security and detection of chemical and biological pathogens. he has received several securityrelated awards, including the sandia national security fellowship and a department of homeland security fellowship. at mit he explored techniques in nanoscale detection of organic molecules such as anthrax dna. sinensky is involved in numerous palantir technologes deployments across the defense, intelligence community, and law enforcement spaces.  rod smith is an ibm fellow and vice president of the ibm emerging internet technologies organization, where he leads a group of highly technical innovators who are developing solutions to help organizations realize the value of big data. his early advocacy in the industry has played an important role in the adoption of technologies such as j2ee, linux, web services, xml, rich internet applications, and various wireless standards. as an ibm fellow, smith is helping lead ibm™s efforts around big data analytics and the application of ibm watsonlike technologies to business solutions, helping companies make better decisions more quickly for improved business outcomes. his early identification of emerging technologies has led to a sustained record of achievement in the global software community. smith has authored numerous invention patents and disclosures, and he is the recipient of several prestigious awards, including the tj watson design excellence award. smith is a computer science graduate of western michigan university, and holds an m.a. and a b.a. in economics with a concentration in math from western michigan university.  david thurman currently leads computing strategy development at pacific northwest national laboratory (pnnl) as well as providing oversight for activities at the seattle research center in nonproliferation policy analysis, systems engineering, and humancentered analytics. he was previously responsible for program management for pnnl™s information analysis portfolio in the national security domain, coordinating a range of research and development projects focused on delivering new analytic capability to a range of government users. with more than 25 years of professional experience in research and university settings, thurman has managed a variety of information analysis projects that developed new analytic methods and capabilities for a range of client organizations. he previously conducted research on advanced knowledge representation techniques to support intelligence analysis, led efforts to define information integration architectures for the u.s. department of homeland security, studied information analysis methods at the international atomic energy agency, and developed integrated analysis systems for a variety of government clients. internally at pnnl, he has served in leadership roles for research initiatives on dataintensive computing, threat anticipation, and signature discovery. he is currently leading the definition of a new research initiative in distributed analytics for multisource data. thurman was previously a research engineer at georgia institute of technology™s center for humanmachine systems research, developing human interfaces, training systems, and automation in the domains of satellite ground control and commercial aviation. prior to that, he worked as a software developer in pnnl™s computational sciences department, developing advanced data analysis and visualization applications. he has more than 40 peerreviewed publications and technical reports on a range of information processing and analysis topics. thurman was a presidential fellow at georgia institute of technology, where he received an m.s. in humanmachine systems engineering. he also holds b.s. degrees in mathematics and big data: a workshop reportcopyright national academy of sciences. all rights reserved.appendix c 25 computer science from the university of oregon. he is a member of ieee, and acm and was previously a fellow of the world affairs council in seattle.  paul twohey is the vice president and a cofounder of ness computing. previously, he was the cofounder of good ga.me and also worked as a software engineer for palantir technologies. ness, whose mission is to make search personal, is sometimes referred to as the ﬁpalantir for fun.ﬂ twohey gained his m.s. in computer science from stanford university and his b.s. in electrical engineering and computer sciences from the university of california, berkeley. he is also the recipient of the 2002 william everitt award for excellence.  big data: a workshop reportcopyright national academy of sciences. all rights reserved.        
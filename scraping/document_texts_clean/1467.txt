detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/1467scaling up: a research agenda for software engineering100 pages | 8.5 x 11 | paperbackisbn 9780309041317 | doi 10.17226/1467computer science and technology board, national research councilscaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.scaling up: a research agenda forsoftware engineeringcomputer science and technology boardcommission on physical sciences, mathematics, and resourcesnational research councilnational academy presswashington, d.c. 1989iscaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine.the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.this report has been reviewed by a group other than the authors according to procedures approved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific andengineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority ofthe charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientificand technical matters. dr. frank press is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallelorganization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the nationalacademy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers.dr. robert m. white is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members ofappropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibilitygiven to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. samuel o. thier is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy's purposes of furthering knowledge and advising the federal government. functioning in accordancewith general policies determined by the academy, the council has become the principal operating agency of both the national academy ofsciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineeringcommunities. the council is administered jointly by both academies and the institute of medicine. dr. frank press and dr. robert m. whiteare chairman and vice chairman, respectively, of the national research council.support for this project was provided by the following organizations and agencies: control data corporation, cray research, inc., thedefense advanced research projects agency (grant no. n0001487j1110), digital equipment corporation, the department of energy(contract no. defg0587er25029), hewlett packard, ibm corporation, the national aeronautics and space administration (grant no.cda860535), the national science foundation (grant no. cda860535), and the office of naval research (grant no. n0001487j1110).library of congress catalog card no. 8963272international standard book number 0309041317available from:national academy press 2101 constitution avenue, nw washington, dc 20418s039printed in the united states of america first printing, october 1989 second printing, june 1990iiscaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.computer science and technology boardjoseph e traub, columbia university, chairmanjohn seely brown, xerox corporationmichael l. dertouzos, massachusetts institute of technologysamuel h. fuller, digital equipment corporationjames freeman gilbert, university of california at san diegowilliam a. goddard iii, california institute of technologyjohn e. hopcroft, cornell universityrobert e. kahn, corporation for national research initiativessidney karin, general atomicsleonard kleinrock, university of california at los angelesdavid j. kuck, university of illinois at urbanachampaignrobert langridge, university of california at san franciscorobert w. lucky, at&t bell laboratoriesraj reddy, carnegie mellon universitymary shaw, carnegie mellon universitywilliam j. spencer, xerox corporationivan e. sutherland, sutherland, sproull & associatesvictor vyssotsky, digital equipment corporationshmuel winograd, ibm t. j. watson research centerirving wladawskyberger, ibm corporationmarjory s. blumenthal, staff directordamian m. saccocio, staff officermargaret a. knemeyer, staff associatec. k. gunsalus, staff consultantdonna f. allen, administrative secretarycatherine a. sparks, secretaryiiiscaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.commission on physical sciences, mathematics, and resourcesnorman hackerman, robert a. welch foundation, chairmanrobert c. beardsley, woods hole oceanographic institutionb. clark burchfiel, massachusetts institute of technologygeorge f. carrier, harvard universityralph j. cicerone, national center for atmospheric researchherbert d. doan, the dow chemical company (retired)peter s. eagleson, massachusetts institute of technologydean e. eastman, ibm t j. watson research centermarye anne fox, university of texasgerhart friedlander, brookhaven national laboratorylawrence w. funkhouser, chevron corporation (retired)phillip a. griffiths, duke universityneal f. lane, rice universitychristopher e mckee, university of california at berkeleyrichard s. nicholson, american association for the advancement of sciencejack e. oliver, cornell universityjeremiah p. ostriker, princeton university observatoryphilip a. palmer, e.i. du pont de nemours & companyfrank l. parker, vanderbilt universitydenis j. prager, macarthur foundationdavid m. raup, university of coloradoroy e schwitters, superconducting super collider laboratorylarry l. smarr, university of illinois at urbanachampaignkarl k. turekian, yale universitymyron f. uman, acting executive directorrobert m. simon, acting associate executive directorivscaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.prefacehow to produce software of sufficient quality and in sufficient quantity to meet national needs is a problemthat has been festering for some time and is getting worse. of particular concern is the need to facilitate thedevelopment of software for large and complex systems, on which the world is becoming critically dependent.this problem has concerned the national research council's computer science and technology board (cstb)since its inception in 1986.on february 13œ15, 1989, in austin, texas, the cstb sponsored a twoandonehalfday workshop oncomplex software systems research needs. a diverse group of software engineers, representing a range of industryperspectives and the academic community, participated (appendix a). the workshop was chaired by victorvyssotsky of digital equipment corporation, and the steering committee included laszlo belady ofmicroelectronics and computer technology corporation (mcc), mary shaw of carnegie mellon university, andshmuel winograd of ibm corporation's t. j. watson research center.the cstb workshop took as a starting point the notion that large and growing opportunity costs are resultingfrom the inability to produce sophisticated, reliable software in a timely manner. its objective was to identifydirections for software engineering research and potential mechanisms to improve the way software engineeringresearch builds from and contributes to practice in the field. consequently, workshop discussions focused oncharacterizing impediments perceived by software engineers, promising research directions, and options forimproving the interplay between software engineering research and practice.this report summarizes the deliberations of workshop participants, focusing on directions for research.included in appendix b are brief position statements contributed by individual participants. the report is aimed atleaders in the academic and corporate research community who should be concerned about largesystem softwareengineering. it is also directed to government funders of software engineering research, who control key levers ofchange.joseph f. traub, chairmancomputer science and technology boardprefacevscaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.prefaceviscaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.contents1 introduction and summary 12 perspective 73 engineering practice 154 research modes 195 conclusions 23 bibliography 25 appendixes a workshop participants 31b position statements 32contentsviiscaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.contentsviiiscaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.1introduction and summarybusiness, government, and technical endeavors ranging from financial transactions to space missionsincreasingly require complex software systems to function. the complexity of the software arises from stringentrequirements (e.g., for reliability in performance and integrity of the data used), the need to support a range ofinteractions with the environment in real time, and/or certain structural features (see table 1.1). these attributesmake software difficult to produce and often require largescale projects involving the efforts of several hundredševen a few thousandšprogrammers, software engineers, applications experts, and others working for one yearor more. the difficulty of developing complex software systems tends to force delays in the implementation of newapplications, compromises in what those applications can do, and uncertainties about their reliability. (see box for amore detailed characterization of the problem.) as a result, there is a perception in the field, especially in industry,that opportunity costs are large and growing.how can this situation be improved? the problem has resisted the efforts of many talented individuals overmany years. some degree of resistance to change is inevitable, reflecting the inertia that comes from the large andcumulative investment that companies have made in their software development processes, but cstb workshopparticipants expressed a widely shared frustration that options circulating within the software engineeringcommunity fall short of what is needed (or fall on deaf ears). solutions suggested in the past have ranged fromways to improve tools used by software developers to ways to improve the management of software developmentteams; they have often been couched as options for improving productivity in software development, itself aslippery and manysided concept.agenda for software engineering researchdirections for changeacknowledging those suggestions and accepting that there may be no "silver bullet" in this area (brooks,1986), cstb workshop participants reached the consensus that progress will be made if the vast array of existingand emerging knowledge can be codified, unified, distributed, and extended more systematically. softwarerequirementsintroduction and summary1scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.for large and complex systems have been outrunning understanding of their fundamental principles, and softwaredevelopment practices have not filled that gap. further, the shared framework of existing knowledge has notgrown commensurately with advances made by individual practitioners. codification of existing knowledge wouldhelp to make the process of developing routine software (which is what most software is) more routine, therebysaving time and money. it is essential for progress in the reuse of knowledge. a strategy for dissemination ofcodified knowledge should build on the concept of software engineering handbooks.table 1.1 observations on complex systemswhat makes complex systems complex? is complexity inherent when software is the focal point of the system? how canwe characterize the complexity of software systems? can we identify avoidable and unavoidable kinds of complexity?looking at systems from a broad perspective, we see several ways in which they may be complex:structuresubsystems, modules, macros, down to statements and expressionsbehavior observable activity of a systemfunction transformations on components of the stateprocess flow of controlreactivity events to which the system must respondtiming constraints on response timesstate persistent and transient data with associated consistency and correctness invariantsapplication requirements from the context in which system will be usedrecovery of state and continuation of reactivitysecurity of state from destruction or unwanted inspectionsafety from catastrophic external or internal eventsinterfaces with users and other systemsoperations that maintain the state and continuity of the systemdevelopment environment people and processes producing the codedesign and implementationdocumentation and trainingverification, validation, and certificationsource: adapted from "complexity, multiple paradigms, and analysis," position statement by susan l. gerhart, appendix b.computer science and technology board workshop participants agreed that software engineering researchcan contribute to the improvement of practice if the research community broadens its view of what constitutesgood research (and amends its reward structure accordingly). in particular, researchers need to look to practice tofind good research problems, validating results against the needs of practice as well as against more abstractstandards. the problems experienced by practitioners are serious and pressing, and they call for innovativesolutions. the promise of fruitful interactions between researchers and practitioners should not have to founderbecause of cultural differences between the two groups.the cstb workshop underscored the need for both software engineering researchers and practitioners toaccept a more realistic view of the problem. many of the problems experienced today reflect implicit assumptionsthat the flow from software system concept to implementation is smoother and more orderly than it is, as well asimplicit assumptions that a development process involving project teams is subject to the degree and kind ofcontrol that might be found if a single individual were responsible for the software. aintroduction and summary2scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.characterizing the problem: observations from the fieldthere are few human endeavors that are as difficult to grasp as a complex program or set of programs. therelations, processes, and purposes of the elements of a program are difficult to describe and thus difficult touse as construction elements. creating tools, methods, or magic to solve these difficulties is extremely hard.the worst problem is not cost; it's not unreliability; it's not schedule slips. rather. it's the fact that in the nearfuture we won't be able to produce requisite software at all!both industry and government do not take management actions which recognize the industry's inability toefficiently develop large software packages.the typical system development life cycle is so long that only a few people will have the opportunity to learn byrepetition during a typical career.the state of practice in software development and maintenance is far behind the state of [the] art.even when more 'modern' software development techniques and technologies are widespread, new andunanticipated requirements for 'ities' (e.g., usability, installability, reliability, integrity, security, recoverability,reconfigurability, serviceability, etc.) which are not yet taught in software engineering, are not yet part of themethodology being used, and are not yet 'parameters' to the code generator will necessitate rediscovery andrework of our complex systems.[my company's] most significant software problem is duplication of effort: we often write several times whatappears to be essentially the same software.all problems pale against the issue of chasing the leading edge of technology.... for instance, our stateofthepractice can finally handle serial, monolithic systems quite well but our challenge is distributed, parallel,asynchronous applications for which we have little or no practical engineering principles.industry does not collaborate effectively with the research community in creating the next generation ofsoftware capability and software production technology.a key motivator for software tools and programmer education in the 1990s will be software evolved overdecades from several thousand line, sequential programming systems into multimillion line, multitaskingcomplex systems.without clear understanding of design, developing and teaching skills and developing better tools will continueto be happenstance.what we call the problem of requirements generation is actually a poverty of tools, techniques, and languageto assist business experts and computer scientists in collaborating on a process of transformation whichadequately exploits the knowledge of all.the worst problem i must contend with is the inability of the software buyer, user, and builder to write ablueprint which quickly leads to a lowcost, correct product.a most serious problem ... is ... cost and schedule increases due to changing user requirements.too often funders, customers, and managers are willing to be 'low balled' on effort estimation. the lack ofappreciation for upfront capitalization in the software industry with consequential failures points to a seriousproblem confronting us.nonprogrammers dominate modern computer use.... computer users are interested in results, not inprogramming; software must reflect this.users of software have reliability and economic (speed, space, cost) constraints that are of little interest to thecomputer scientist; the computer scientist has solutions which, when properly engineered, could greatlyenhance products.technology to enable satisfaction of capability requirements, such as for high assurance, deadlineguarantees, and high adaptability is not being provided, despite need.we need a true pragmatic test engineering discipline. today it is a truism that we test until out of money andtime, not because we've achieved results.there is a rigorous science, just waiting to be recognized and developed, which encompasses the whole of'the software problem,' as defined, including the hardware, software, languages, devices, logic, data,knowledge, users, uses, and effectiveness, etc. for endusers, providers, enablers, commissioners, andsponsors, alike.source: quotations from position statements prepared before workshop (appendix b).introduction and summary3scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.clearer understanding of the realities of software development can lead to improvements in any of severalways. for example, it may facilitate the identification of valuable tools that do not now exist, or it may facilitatethe identification of fundamental flaws in the software development process itself. a more realistic view will alsomake clear the extent to which problems are technical or are amenable to technical solutions.table 1.2 agenda for software engineering researchrecommended actionsshort term (1œ5 years)long term (5œ10 years)perspectiveportray systems realisticallyresearch a unifying model for softwaredevelopmentšfor matching programminglanguages to applications domains and designphases view systems as systems recognize change as intrinsicstudy and preserve software artifactsstrengthen mathematical and scientificfoundationsengineering practicecodify software engineering knowledge fordissemination and reuseautomate handbook knowledge, access, andreusešand make development of routinesoftware more routinedevelop software engineering handbooksnurture collaboration among system developerand between developers and usersresearch modesfoster practitioner and researcher interactionslegitimize academic exploration of largesoftware systems in situglean insights from behavioral and managerialsciencesdevelop additional research directions andparadigmsšencourage recognition of reviewstudies, contributions to handbooksspecific shortand longterm actionsimproving the development of complex software systems requires a series of longterm (5 to 10 years ormore) and shortterm (1 to 5 years) measures. the cstb workshop reached consensus on several directions forchange in software engineering research, which fall into three interconnected areas: (1) perspective, (2)engineering practice, and (3) modes of research (see table 1.2). these improvements are outlined below anddiscussed in greater detail in the body of the report. carried out together, they will bring to software engineeringstrengths found in traditional engineering disciplines, notably the effective reuse of shared knowledge and aworking distinction between routine and innovative design. some of these directions have been advanced before;others are more novel. cstb workshop participants shared the hope that this new presentation and the consensusit represents will help to catalyze muchneeded changes.introduction and summary4scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.perspectiveportray the software development process more realisticallychanges in attitude take time to achieve, but the time is right to push for better models of the softwaredevelopment process in research, teaching, and management. today's systems are conglomerate wholes, not merecollections of parts, and change is intrinsic in such systems. a new approach to software design that accounts forsystem evolution and that aggressively links software development and application expertise could provide a basisfor advances in practice. a sampling of the software engineering leadership, cstb workshop participants notedthat outmoded models lead to unrealistic expectations among developers or managers; such models contribute todelay and diminish quality by engendering inappropriately tight schedules and complicating the process of midcourse corrections. further, new attitudes are prerequisite to other steps being satisfactorily developed andimplemented.study and preserve software artifactsa better understanding of the process of system development would provide useful perspectives and insightsfor today's researchers and practitioners. because such an understanding must be built on the study of actualsystems, itself a dynamic process, an ongoing mechanism for studying innovative systems now in existence shouldbe established. looking at snapshots of a system may be easy, but it does not provide sufficient information topermit realistic portrayal of the software development process. cstb workshop participants urged efforts topreserve and study software artifacts and their development processes in the hope that understanding theirstructure and properties would contribute to the development of better experiments, tools, and softwaredevelopment methods.develop unifying models and strengthen mathematical and scientific foundationslongterm efforts that will enhance research and education include developing unifying models for systemdevelopment and associated languages, which are the basic units of the software development process. acomprehensive and unifying view of models and languages would be a valuable analytical device that could helpresearchers develop new tools and techniques to enhance the structure and development of large software systems.also, strengthening the mathematical and scientific foundations of software development will help softwareengineers to improve the quality of software systems and to produce them more systematically. for example, thedegree of rigor in software design and analysis (including testing and other verification and validation methods)should be increased.engineering practicecodify and disseminate software engineering knowledgecodification of knowledge can help move the practice of software engineering from a craft toward a science.broad dissemination of software engineering insights and techniques that are known now will save time by makingmuch useful information broadly and conveniently available, thus reducing duplication of effort and enhancingsoftware development productivity, in particular, it will help make development of routine software more routine.this dissemination can be achieved through software engineering handbooks, providing for software engineers thereference assistance other types of engineers have benefited from for decades. and the handbooks need not belimited to conventional book form; they can be developed and accessed in electronic formats. over the long term,new technologies (e.g., advanced networking and data base systems) will enhance the delivery and use of whatshould be shared knowledge, but the codification and dissemination process should be launched now.introduction and summary5scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.nurture collaboration among system developers and between developers and usersthe development of large software systems involves many people acting in concert. progress is especiallyneeded to facilitate communication and cooperation between software developers and applications experts andother endusers; this is a longterm issue. because software engineers have an incomplete understanding of theprocess of collaboration, tools to support software development are not optimal. support for research into anddevelopment and use of computersupported tools for collaborative work would improve the softwaredevelopment process.research modesfoster practitioner and researcher interaction and legitimize academic exploration of large softwaresystemsacademic researchers lack access to large complex software systems, which are developed and used primarilyin commercial and defenserelated industries. as a result, these researchers have encountered difficulty in teachinglargesystem topics and in identifying good basic research problems related to largesystem development. newways should be explored to facilitate interaction between software engineers in academia and industry, includingexpediting academic exploration of large systems, promoting outlets for information that would reach bothacademic and corporate researchers, and augmenting current teaching of largesystem concepts. legitimizing thestudy of large systems (and their development) in situ would be a major step in the right direction.glean insights from behavioral and managerial sciencecollaboration by software engineering researchers with other types of researchers, including those from thebehavioral and management sciences, may lead to insights into the peopledriven process of software systemdevelopment. although project management and administration are only a part of the complex system problem,insights from other disciplines may enhance the planning, management, and timeliness of system projects, whichhave resisted efforts from within the software engineering field.develop new research paradigmsthe field would benefit from such new efforts as review and synthesis studies, case studies, and comparativeanalyses, which have not made the same mark on software engineering as on other fields, because such studieswould help to advance the codification and reuse of knowledge. similarly, researchers should get credit forcontributions to the electronic handbooks discussed above.organization and content of this reportthe remainder of this report of the cstb workshop on complex software systems examines each researchagenda item in turn. chapter 2 covers the area of perspective, chapter 3 discusses engineering practice, andchapter 4 addresses research modes. chapter 5 presents brief conclusions and a call to action. a bibliography isgiven in a final chapter. appendix a lists workshop participants, and appendix b contains position statementsprepared by individual participants in the workshop.determining the details of implementing the measures outlined below was beyond the scope of the cstbworkshop. it is clear, however, that implementation of the recommended measures will hinge on funding forcorresponding research projects and other incentives. selected implementation issues will be addressed in followup work by the cstb.introduction and summary6scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.2perspectivethe software research community has not kept up with the development of complex software systems inindustry and government, while in the field, managers concerned with procuring software systems often evinceidealized and unrealistic views of how software is developed. a critical reality in the field that is insufficientlyappreciated by academic researchers and systems purchasers is the extent to which existing systems tie upresources. socalled system maintenance, discussed below, may constitute up to 75 percent of a system's cost overits lifetime. the high cost of maintenance diminishes the amount of money available to replace existing systems,and it lengthens the payback period for investment in new system development. it makes designing for a system'sentire life cycle imperative, but such design is rarely if ever achieved. cstb workshop participants agreed that toprogress in system development, it is time to portray systems realistically by (1) viewing software systems assystemsšwhich has implications for optimal system design, (2) recognizing that change is intrinsic in largesystems, and (3) striving for a better and more timely blending of software engineering and applications expertise(see discussions headed ''build a unifying model,'' p. 10, and "nurture collaboration," p. 17).1 they also agreedthat a more rigorous use of mathematical techniques can help researchers to manage and diminish complexity.shortterm actionsportray systems realisticallyview systems as systems, not as collections of partswhile the computer field has helped to popularize the word "systems" and the concept of systems, it is ironicthat information systems developers have not developed formal mechanisms to understand systems and theinterrelationships among system components. software engineering researchers have been unable to provideeffective guidance to practitioners regarding the process of system definition and the concomitant implementationof functional elements. progress in developing software systems requires a fundamental appreciation that thosesystems are more than just a collection of parts and that software is embedded in larger systems with a variety ofphysical components;perspective7scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.design of such systems must deal with both of these issues. design of software systems must also take intoaccount the fact that the whole system includes people as well as hardware, software, and a wide variety ofmaterial elements.recognize change as intrinsic in large systemssoftware projects are increasingly likely to be built on top of an existing, installed base of code rather thanbuilt anew. as that installed base of software grows over time, software systems that might or might not have beendesigned to endure have been patched, modified, and "maintained," transforming them greatly from their originaldesigns and functions (belady and lehman, 1985). two factors are at work here: the first is that systems are oftennot wellenough designed to begin with, and the second is that user needs change over timešnew requirementsarise, and existing systems must be adapted to accommodate them. but commonly used conceptualizations, suchas the "waterfall model" or even the "spiral model,'' assume a more surefooted progression from requirementspecification to design to coding to testing and to delivery of software than is realistic (royce, 1970; boehm,1988). given that 40 to 60 percent or more of the effort in the development of complex software systems goes intomaintainingši, e., changingšsuch systems (boehm, 1981), the design and development processes could be mademore efficient if the reality of change were accepted explicitly.sustaining the usefulness of software systems differs from the care of other assets because it entails twodistinct activities: (1) corrective and preventive maintenance, which includes the repair of latent defects andtechnological wear and tear, and (2) enhancement, which normally introduces major transformations not only inthe form but also in the functions and objectives of the software. enhancement activities have been observed toconstitute perhaps 75 percent of the total maintenance effort.2the degree and impact of change is analogous to the evolution of an urban neighborhood: over time, old andobsolete buildings are torn down, the supply of utilities changes in both quality and delivery aspects, andtransportation routes and media change. as new needs, wants, and capabilities emerge, the structure and functionof the neighborhood evolve; the neighborhood is not thrown out wholesale and replaced because doing so wouldbe far too costly. as with changes in neighborhoods, changes in software are not always improvements; softwaresystems suffer from the tension between providing for functional flexibility and assuring structural integrity of thesystem.software developers in industry and government are increasingly aware that change occurs from the earliestdesign stages as initial expressions of customer requirements are refined. managing this change involves managing amix of old code (typically with inadequate documentation of original specifications as well as modifications madeover time), new programmers, and new technology. the process is ad hoc, and the problem grows over time; thelarger the installed base of code, the more formidable the problem. the problem is aggravated where managementdecisions, including contracting decisions, keep developers and maintainers separate.ideally, system designers leave hooks for the changes they can anticipate, but problems arise from majorchanges that result from changed circumstances or goals. also, schedule and process pressures often militateagainst providing for functional flexibility and future changes. further, the current generation of computeraidedtools for software engineers concentrates on development activities and generally neglects maintenance. as aresult, supporting information for new code and the tools to exploit it are not carried forward from development tomaintenance. more seriously, these tools do not accommodate large bodies of code developed without using thetools, althoughperspective8scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.some progress is being made in the necessary restructuring of programs to accommodate computeraided tools.just as change, per se, should be accepted as a basic factor in most large, complex systems, designing forchange should become a fundamental body of knowledge and skill. the very notion of maintenance as an activityseparate from the creation process seems to legitimize high costs, poor support, and poorly managed redesign.eliminating this notion via a move toward designing and building systems in anticipation of change would help toincrease the engineering control over postrelease modification. since software reflects both system specificationsand design decisions, changing either element will indirectly produce changes in the code. one possibility is tostrive for designing systems that are more modular or easier to replace as needs change.note that the issue of determining what the software shall do (the "requirements definition") is much broaderthan software engineering practices today would suggest; this perceptual difference contributes to the maintenanceproblem. what is needed is a thorough investigation, analysis, and synthesis of what the combined functions will,or should, be of the automated and nonautomated (human, business, or physical) elements of the system,including all "think flows," work flows, information flows, and other functionalities. a total systems approach, asdiscussed above, would be involved, with a heavy emphasis on the conceptualization of the functional role of boththe automated parts and the fully combined systems, allowing for reengineering to accommodate or exploit thechanges that are made possible by introduction of the automated system.understanding the reasons for change and the costs, impacts, and methods of change could lead to morecontrol of a major part of software development costs. creating mechanisms that allow for change and that makesystems robust while undergoing change will help to reduce opportunity costs in system development anddeployment. part of what is needed is a change in attitude. but for the long term, a theory of software systems isneeded that will build on empirical study of software system applications.study and preserve software artifacts: learn from real systems past and presentalthough systems developers work with an evolving set of goals and technologies, they can still learnvaluable lessons from existing systems, lessons about what led to success or failure and what triggeredincremental or major advances. the history of computing is replete with instances in which identifying theintellectual origins of key developments is difficult or impossible because most advances, in their time, were notthought of as intellectual issues but instead were treated as particular solutions to the problems of the day. mostsoftware specialists would be hard put to name "seven wonders of the software systems world" or to state whythose wonders are noteworthy.3 meanwhile, the artifacts of such systems are disappearing every day as olderequipment and systems are replaced with newer ones, as projects end, and as new applications emerge. becausealmost all large software systems have been built in corporate or government settings where obsolete systems areeventually replaced, and because those systems have received little academic attention, useful information may bevanishing.a concerted effort is needed to study (and in some cases preserve) systems and to develop a process for thesystematic examination of contemporary and new systems as they arise. immediate archival of major softwareartifacts, together with the software tools needed to examine them, or even to experiment with them, would enableboth contemporary and future study. systematic study of those systems would facilitate understanding of theontology of architecture and system components, provide a basisperspective9scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.for measuring what goes on in software development, and support the construction of better program generators.studies of contemporary systems would provide an understanding of the characteristics of software developedunder present techniques. such an effort would examine software entities such as requirements documentation,design representation, and testing and support tools, in addition to the actual source code itself, which hastraditionally been the focus of measurement. better mechanisms that provide quantifiable measures ofrequirements, design, and testing aspects must be developed in order to understand the quality baseline that existstoday. existing mechanisms for measuring source code must be put to more widespread use to better assess theirutility and to refine them (bowen et al., 1985; mccabe, 1976). in addition, variations in quality need to be tracedto their sources to understand how to control and improve the process. thus this effort should encompass lesssuccessful as well as exemplary artifacts, if only to show how poor design affects maintainability. the examinationof artifacts should be combined with directed interviews of the practitioners (and system users) and observation ofthe process to correlate development practices with resulting product quality.having quantifiable measurements would enable new, innovative development methods and practices to beevaluated for their impact on product quality. however, as the software industry evolves, so too must themeasurement techniques. for example, if new means of representing requirements and design are put intopractice, the measurement techniques must be updated to accommodate these new representations. in addition,efforts to automate measurement can be improved if researchers consider measurability as an objective whendeveloping new development methods and design representations.such measurement and research cannot take place in the laboratory due to the size of the actual systems beingdeveloped (the costs of experiments at this scale are prohibitive), and it is unlikely that small experiments can beextrapolated to apply to largescale projects. a cooperative effort between government, industry, and academiacould provide necessary funding, access to realworld artifacts and practitioners, and the academic research talentrequired for such an effort. such a combined effort also would provide an excellent platform for greatercollaboration in software engineering research between members of these communities. designation and fundingof one or more responsible entities are needed, and candidates include federal agencies (e.g., the national sciencefoundation and the national institute of standards and technology), federally funded research and developmentcenters, or private institutions.finally, active discussion of artifacts should be encouraged. a vehicle like the online risks forumsponsored by the association for computing machinery, which provides a periodic digest and exchange of viewsamong researchers and practitioners on risks associated with computerbased technology, should be established.also, completed case studies would provide excellent teaching materials.longterm actionsbuild a unifying model for software system developmentshortcomings in software systems often reflect an imperfect fit to the needs of particular users, and in thissituation lie the seeds for useful research. the imperfect fit results from the nature of the design and developmentprocess: developers of complex software systems seek to translate the needs of endusers, conveyed in everydaylanguage, into instructions for computer systems. they accomplish this translation by designing systems that canbe described at different conceptual levels, ranging from language comprehensible to the intended user (e.g., "plainenglish" or formal models of theperspective10scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.application domain) to machine language, which actually drives the computer. different spheres of activity arereferred to by the profession as enduser domains; these include the following:figure 2.1illustration of a unifying model for software system design. scientific computation, engineering design, modeling and visualization, transaction processing, and embedded command and control systems.these domains tend to have different types of abstraction and different language requirements arising fromdifferences in the representations of application information and associated computations. as a result, softwaredevelopers work with a variety of domain specific models. during the design process in any domain, key pieces ofinformation or insights tend to be lost or misinterpreted.how can the process of moving from a domainspecific model to a working piece of software be improved?one approach would be to develop a unifying view of the software design process and the process of abstraction, aview that would define a framework for the task of the complex software system developer. cstb workshopparticipants did not reach a consensus on this complicated issue, but to illustrate the point, they began to sketchout the parameters for such a framework (figure 2.1). for example, a system design can be thought of as asequence of models, one (mi,j) at each level. different sorts of details and design decisions are dealt with at eachlevel. the model at each level is expressed in a language (li,j). languages are not necessarily textual or symbolic;they may use graphics or even gestures. also, languages are not always formally defined.4 just as the domains ofdiscourse at each level are different, so are the languages. finally, the unifying view would distinguish domainspecific models, a multilevel set of appropriate languages (although it is possible that languages may be sharedšor largely sharedšacross domains), abstractions, and interlevel conversion mechanisms.perspective11scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.useful outcomesthe concept of a unifying model and the associated issues are emblematic of the larger problem of achievingmore complementarity between software engineering research and practice. a unifying model would notnecessarily be of immediate use to a system builder. but it would be a tool for a academic analysis that could, inturn, yield structures and tools useful to a practitioner. in particular, it could help a researcher to analyze thesoftware development process and forge improvements that might make that process more efficient in practice.for example, a unifying view could help the software engineering researcher to see the relation amongexisting mechanisms, to see what mechanisms are missing, and to devise ways to facilitate transitions from onemajor conceptual level to another (since it is necessary to be able to convert a system description at one level to asystem description at an adjacent level).5 by showing how parts are related, unification may facilitate the collapseof domainspecific models to include fewer levels than at present.6 eventually, it may be possible to moveautomatically from a description of requirements to a working product, bypassing intermediate levels. themodeling process can facilitate this progress much as the modeling of production processes in manufacturing hasfacilitated the reduction of the number of tasks in manufacturing processes and the application of manufacturingautomation. also useful, as noted above, is better coordination technology, which would support both themodeling and the development processes.research implicationswhile the theory and nature of program transformation functions, drawing on a body of knowledge aboutlanguage that crosses levels (sometimes called widespectrum language), have already been developed (balzer,1985; partsch and steinbruggen, 1983; and smith et al., 1985), the propose kind of unifying view would alsomotivate new styles of research independent from those noted above. relevant current research addressestraditional programming language (although some of this research is in eclipse), computersupported cooperativework (beyond the mere mechanical aspectsšsee discussion headed "nurture collaboration," p. 17), and efforts toraise the level at which automation can be applied. also needed are the following: research that would support the development of domainspecific models and corresponding programgeneratorsšit is critical to recognize the legitimacy of specialization to the domain at the expense ofexpressive generality. research to identify domains, levels, and commonalities across domains, since languages are needed foreach level and domain. research into the architectural level, which cuts across individual domain models. this level deals withthe gross function of modules and the ways they are put together (for procedure call, data flow,messages, data sharing, and code mingling). the aggregates defined at this level include "state machine,""objectoriented system," and "pipe/filter system." contrast this with the programming level, where theissues are algorithms and data structures and the defined entities are procedures and types. research into whether it is possible to implement a concept found in the mechanical engineeringenvironment, the quarterscale model, and if so, how. a quarterscale model, which would provide amore precise and detailed correspondence to the desired system than does a conventional prototype,would help to convey the complexity and various design attributes of a software system. it would allowpractitioners to better comprehend how well a design works, and it would allow managers to control riskby helping them to understand where problems exist and to better estimate the resourcesperspective12scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.required to solve those problems. in essence, it would make a seemingly intangible product, software,more real. investigation of the mechanisms for making the transition between and among the various levels ofabstraction. this research would involve exploration of automation aspects (e.g., compilers andgenerators) and computeraided and manually directed steps. it would also involve exploration of theorder of development of the models of a system: whereas the conventional waterfall life cycle calls forcompleting each model before translating to the next, other approaches such as rapid prototyping or thespiral model allow for simultaneous development of several models. reformulation of expressions of rigor and technical precision (sometimes referred to as "correctness"),performance given resources, traceability, cost, reliability, and integrity.strengthen the mathematical and scientific foundations of software engineeringin the absence of a stronger scientific and engineering foundation, complex software systems are oftenproduced by brute force, with managers assigning more and more people to the development effort and takingmore and more time. as software engineers begin to envision systems that require many thousands of personyears, current pragmatic or heuristic approaches begin to appear less adequate to meet application needs. in thisenvironment, software engineering leaders are beginning to call for more systematic approaches: moremathematics, science, and engineering are needed (mills, 1989).workshop participants focused on application of such approaches to software analysis; they also affirmed thevalue of mathematical foundations for better modeling and translation of realworld problems to the abstractionsof software systems. software analysis, which seeks to assure that software works as specified and as designed, isboth a significant and a critical part of the implementation of large software systems. unfortunately, analysisactivities have received too little focused attention, and what attention they have received has been largely limitedto today's main analytical approachštesting. testing techniques, moreover, are constantly being discovered andrediscovered.a more rigorous and comprehensive approach to analysis is needed, one that renders techniques explicit,teaches about them, and develops its own literature and authority. in addition to testing, such techniques asproving, modeling, and simulation should be further developed and targeted to more properties (e.g., safety andfunctional correctness). work is needed in performing measurements, establishing metrics, and finding a way tovalidate them. the understanding of what constitutes a defect and how to verify that designs or code are defectfree is today very limited.note that the ability to find defects earlier in the life cycle of a product or to prevent them from beingintroduced reduces test cost and reduces the number of defects in products delivered to endusers. this abilityinvolves quality assessment and quality assurance. research questions center on how to specify and measure theattributes (functional, behavioral, and performance) a system must possess in a manner that permits correctgeneration or proof. what aspects of a product can be assured satisfactorily only by testing as opposed toexperimentation? what are the economic tradeoffs between developing mathematical proofs and conductingtesting? how to design for testability and verifiability is also an issue here.promising directions include the application of formal methods (which involve mathematical proofs),exploration of the mechanical and civil engineering concept of a quarterscale model for previewing a design,application of the "cleanroom concept" (featuring walkthroughs of software with proofs of claims about featuresrather thanperspective13scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.checklists of flaws; mills, 1989), and statistical quality control analogous to that used in manufacturing. ahandbook of testing and/or quality assessment is desirable and will be possible with further development of thefield of analysis.notes1. these conclusions are consonant with those of the defense science board task force (1987), which focused on management aspectsbecause attitudes, policies, and practices were a major factor in defense software system acquisition.2. the national bureau of standards (now the national institute of standards and technology) drew on several studies to decomposemaintenance into corrective maintenance (20 percent), including diagnosis and fixing design, logic, or coding errors; adaptive maintenance(25 percent), which provides for responses to changes in the external environment; perfective maintenance (50 percent or more), whichincorporates enhancements; and preventive maintenance (5 percent), which improves future maintainability and reliability (martin andosborne, 1983; swanson and lientz, 1980). similarly, experience with u.s. air force weapons systems suggests that while 15 to 35percent of software maintenance corrects design errors, 25 to 50 percent adds new capability, 20 to 40 percent responds to changes in thethreat, 10 to 25 percent provides new system interfaces, 10 to 20 percent improves efficiency, 5 to 15 percent improves human factors, and 5to 10 percent deletes unneeded capability (mosemann, 1989).3. an informal query addressed to a community of several hundred software engineering specialists suggested the following candidates:the sage missile defense system, the sabre interactive system for airline reservations, the yacc compiler tool for unix, arpanetcommunications software, and the visicalc spreadsheet package, among others.4. to serve this modeldefinition role, a language must provide five essential capabilities: (1) component suitabilityšmodulelevelelements, not necessarily compilation units, with function shared by many applications; (2) operators for combining design elements; (3)abstractionšability to give names to elements for further use; (4) closurešnamed element can be used like primitives; and (5)specificationšmore properties than computational functionality, with specifications of composites derivable from specifications ofelements.5. this process of transition is sometimes accomplished manually and sometimes mechanically. mechanical transitions (e.g., using programgenerators or compilers) can enhance productivity, but they depend on more precision and understanding than are often available.6. overall, the number of levels has grown and shrunk with technology over time. for example, today few people actually code in machinelanguage, and relatively few program in assembly code.notes14scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.3engineering practicethe phrase "software engineering" was coined in 1968 as an expression of aspiration.1 it remains today morean aspiration than a description. 2 the field of software engineering lacks the strengths and structure of otherengineering disciplines, which have a more highly developed theory and firmer methodological foundations, aswell as widely shared tools and techniques. engineering disciplines are rooted in craftsmanship and evolve through acommercial stage (with emphasis on production and management) before becoming engineering as we generallyknow it (see table 3.1). what is needed is a way to define and discuss the "parts" of software engineering, thespecifications for each, and a conceptual framework within which to place them. organizing known techniquesand information to identify and describe the parts of the software enterprise and how they fit together would go along way toward enabling cleaner, more flexible design and development processes (biggerstaff and perlis, 1989).shortterm actionscodify software engineering knowledge for dissemination and reusecodifying existing software engineering knowledge and disseminating it through handbooks would helpachieve several desirable ends: increasing the amount of software that can be created routinely, contributing toknowledge reuse, and ultimately, it is hoped, helping to reduce the size of programs, the time required to developthem, the risk of unacceptable errors, and the tendency to reinvent solutions to the same problems.for software engineering to progress as a discipline, far more "routine" software development must beproduced routinely. at a time when our needs for software are beginning to outstrip our ability to produce it,efforts to reduce the number of tasks requiring human effort are one obvious way to improve the situation. practicein traditional engineering disciplines includes opportunities for both innovative design (creating things that havenot been done before) and routine design (creating yet another example of a class of things that is wellunderstood). current software practice tends to treat most designs as innovative, even when knowledge exists thatshould render them routine. there is a need to make the reuse of knowledge routine, something many observerslament is far from happening:engineering practice15scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.table 3.1 engineering evolutioncraftsmanshipcommercial practiceprofessional engineeringpractitionersvirtuosos and amateursskilled craftsmeneducated professionalspracticeintuition and brute forceestablished procedureanalysis and theoryprogresshaphazard and repetitivepragmatic refinementscientifictransmissioncasual and unreliabletraining in mechanicseducation of professionalssource: "maybe your next programming language shouldn't be a programming language." position statement by mary shaw, appendix b.indeed, if builders built buildings the way many programmers wrote programs, then most of us would still behomeless, because builders, like too many programmers, would be busy reinventing their technology every time theybuilt something new. continually having to rediscover carpentry, metallurgy, and project management, as well ashaving to write new building codes, would clearly be enormous disincentives to productivity.... (booch, 1987)codifying knowledge and making it more accessible could be an important step in moving toward a situationin which machines can do some of the routine tasks, leaving those more complex and creative tasks to humans.3this is one potent way to improve software development productivity. toward this end, academic researchers canhelp practitioners by developing a conceptual framework for software elements, routine designs, and standardcomponents, much as chemical engineers have developed a framework for the reuse of design elements at a largescale (perry et al., 1984).reuse of code, a less flexible concept than is reuse of knowledge, is the avenue for minimizing programmingeffort that has been most widely discussed in the software research and development community (biggerstaff andperlis, 1989). although theoretically attractive, there are many barriersšboth technical and sociologicalštosignificantly improving the amount of reuse actually achieved. achieving reuse involves more than buildinglibraries of programs, and it requires research on what kinds of resuse are feasible, how to index, how to representreusable elements, and how to deal with variations in the language in which a piece of reusable code is stated oreven in the wording of the specification. but socalled code libraries serve as precursors to the broader concept ofhandbooks discussed below; current work in that area provides a useful starting point.develop software engineering handbookssoftware engineering should follow the lead of other engineering fields, which codify basic knowledge anduse handbooks as carriers of common knowledge, thereby reducing the tendency for dispersed practitioners toindependently develop solutions to common problems, duplicating effort while diluting progress. handbooks forsuch disciplines as mechanical and chemical engineering allow a broad sharing of general and specific technicalknowledge, which provides a base for further progress. software engineering needs such products; referencesduring the cstb workshop to heavily used copies of don knuth's multivolume work, the art of computerprogramming (knuth, 1973), illustrate that a demand exists but remains unmet except in selected, narrowinstances.the structure and contents for software engineering handbooks cannot be determined without progress inaccomplishing the codification discussed above. what is clear,engineering practice16scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.however, is that there is a need for substantive as well as process knowledge to be conveyed in these handbooks,and it is that substantive component that distinguishes these handbooks from the manuals that individualorganizations use to standardize the software development procedures followed by their employees. thushandbooks should contain a compendium of algorithms (see, for example, cody and waite, 1980), test methods,and items pertaining to design and programming style. also, to help practitioners work within the practicalconstraints that they face, handbooks must vary for different domains; the languages, knowledge, and processesassociated with, say, transaction processing systems differ from those used for largescale scientific processing orother types of systems.given the dynamic nature of the field, a software engineering handbook should be one that can use computertechnology to deliver its contentsšan electronic handbook. the goal is to have a repository of information thatcreates a uniform organization for current knowledge, presents the information accessibly, and provides a meansfor updating its contents easily.longterm actionsautomate handbook knowledgeto maximize the effectiveness of an electronic handbook, advances in several areas that will make suchproducts easy and attractive to use will be necessary. a research initiative aimed at the development of anelectronically accessible, interactive software handbook should be inaugurated to develop the following: concepts and notations for describing designs and components; techniques for organizing and cataloging designs and components; techniques and representations for storing, searching, and retrieving designs and components; codification of routine designs and components for a large variety of types of software and applications; techniques for evaluating designs and components in terms of engineering tradeoffs; techniques for modeling and simulating systems based on routine designs and components; criteria for evaluating and accepting or rejecting handbook entries; and technology to make the handbook easily usable and easily accessible.if the technology and the electronic handbooks can be developed, it will be important to educate softwareengineers about appropriate methodologies and techniques for using the information they contain. the handbooksthemselves will facilitate the teaching of routine design as part of software engineeringšitself an important steptoward increased productivity. finally, the handbooks should not only be electronically ''recorded," but they shouldalso be built into the standard tools of software engineers, making for a truly activist incarnation.nurture collaboration among system developers and between developers and userscomplex software systems are created by the efforts of many peoplešsometimes as many as a few thousandorganized into multiple teamsšand frequently no one person has a thorough understanding of the interaction ofthe entire system. further, the softwareengineering practice17scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.developers must communicate with endusers and others to understand the application, the issues, and therequirements. system development is an exercise in collaboration, and it is necessary to maximize theeffectiveness of that collaboration. although the team management problem has captured much attention andconcernšmuch current software engineering consists of ad hoc measures or what could be called ''crowdcontrol"štoday's measures do not go far enough (mcc, 1986; acm, 1988; and bernstein and yuhas, 1987).methodologies for iterative design are necessary. specifications will always be idealized and simplified, andneither users nor designers are able to envision the full functionality of the resulting system during the traditionaldesign stages. consequently, system requirements are not so much analytically specified (contrary to appearances)as they are collaboratively evolved through an iterative process of consultation between endusers and softwaredevelopers. too many projects or designs have been completed that do not accomplish the desired end becamesubstantive information was not well conveyed or understood in the design or implementation process (curtis etal., 1988).better linkage of knowledge about application areas or domains with software engineering expertise isessential; it is an important direction for exploration. another involves developing and sustaining a common worldview of systems under development. and a third is gaining understanding about how skilled designers makearchitectural tradeoffs in the designs of systems (r. guindon, 1988; shaw, 1989).better tools to support and enhance cooperative work are necessary in order to provide productivityenhancements; the more time that programmers can spend designing and programming pieces of systems thatuniquely require their attention, as opposed to investing their time to overcome communications difficulties, themore likely it is that systems can be built in less time. various forms of "groupware," tools for computersupportedcooperative work, may prove well suited to the collaborative process of system development. also, thedevelopment of highspeed, ubiquitous computer networks, coupled with sophisticated and easytouse resourcesavailable through network access, may provide software engineers with valuable research and development tools(cstb, 1988). for example, the growth of the information services business has illustrated the market potential ofdata base searching, and handbook implementation will depend critically on network access to data base facilities.the call for improved collaboration is not new, nor are discussions about computer support for collaboration.but it may be particularly timely, since the motivation in this area is high and new tools are appearing andbecoming more economical to use.notes1. the classic reference is to the software engineering workshop sponsored by the nato science committee in garmisch, westgermany, october 7œ11, 1968.2. in gross terms, software engineering is concerned with the practical aspects of developing software, such as design under variousconstraints and economic delivery of software products. it overlaps all of the various specialties of software research, includingprogramming languages, operating systems, algorithms, data structures, data bases, and file systems, and it also addresses such crosscutting qualities as reliability, security, and efficiency.3. thus it could provide a foundation for exploration of userprogrammable application generators, which may be appropriate for smallersystems.notes18scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.4research modesto complement existing directions in software engineering research and to better address the problem ofdeveloping software for large systems, cstb workshop participants identified a need for crossfertilizationbetween academic software engineering researchers and practitioners as well as between software engineers andspecialists in the behavioral and managerial sciences. cstb workshop participants also urged universities toencourage additional topics and styles of software engineering research and to seek commensurate funding.shortterm action: foster practitioner and researcher interactionsthere is little academic investigation of the practices, techniques, or problems out in the field today. torectify this situation, greater interaction among researchers and practitioners is needed as a first step. suchinteraction has proved a boon in, for example, manufacturing engineering. industry and university collaboration inthat field has provided researchers and students access to realworld problems and constraints, while providingpractitioners with access to creative problemsolving talent and new techniques.the interaction of academia, industry, and government in software engineering has been inhibited by cultureand tradition (besemer et al., 1986). although much is known about how complex software systems are built,there are few connections among the various repositories of practical knowledge. much of the expertise incomplex software systems resides in corporations, government research centers, and other nonacademicinstitutions. it is largely inaccessible to the academic community because of considerations of product delivery,proprietary knowledge, and cultural differences between the corporate and academic communities involved insoftware research.that academic computer scientists do not often study large software systems and the process of developingthem is one reason that practitioners often feel that the issues studied by academia do not adequately address theproblems and challenges faced by builders of large systemsšdespite an apparently large body of systemsanalysis, systems design, and other university courses that do address systems issues. this is particularly so, forexample, for complex systems involving software embedded in other products orresearch modes19scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.systems (ranging from spacecraft to medical technology) and those systems that involve distributed processes inmultiple nonhomogeneous computing and storage elements.there are a number of reasons that information generated in our universities flows only slowly into thecommercial sector: academics do not study largesystems because they do not have them or have access to them,and commercial and academic software specialists tend to read and have their work published in differentjournals. on the other hand, many topflight corporate researchers and developers, to the extent that they publish atall, do not publish in archival computer science journals because their topicsšproblems of practicešare notdeemed scholarly.the disparity in perspective and exposure existing between the academic software engineering researchcommunity and the practitioners of the corporate world hinders u.s. progress in developing complex softwaresystems. reducing that disparity is imperative, and it will require a greater degree of interaction between the twogroups. special meetings like the cstb workshop are but a beginning to this process; implementing an initiativeto preserve and study major artifacts, as discussed above, and legitimizing academic exploration of large softwaresystems, discussed below, are other vehicles for interaction.longterm actionslegitimize academic exploration of large software systemsacademic investigation of research topics based on problems encountered in the "real world" by softwaredevelopers could help industrial and other practitioners in both the short and long terms. for this to happen, newattitudes and incentives must be adopted.as currently structured, most academic departments are not conducive to largesystem research. the tendencyof universities to encourage and reward narrow specializations compounds the problem of a lack of opportunity orfunding for access to large, complex systems by academic software researchers. another side of this problem is thefocus of the academic world on individual actions, whereas the corporate world is more team oriented. therealities of academic lifešfunding, tenure tracks, and other career concernsšmilitate against an individualacademic researcher making a strong commitment to largesystem research without consideration from thesurrounding environment.further, whereas industry tends to focus on a problem as it appears in production, researchers (whethercorporate or academic) need to find the underlying conceptual problems that are amenable to the development ofknowledge that transcends a particular system manifesting a problem. identification of good research problemsbased on production problems is a nontrivial problem that itself requires focused efforts. and to pursue thatresearch requires analytical advances, as discussed above, inasmuch as abstract formal models are lacking,language design issues are in eclipse, and testing and measurement have not been formalized.funding is a major consideration. funding of some considerable magnitude is needed if large systems are tobe builtšwhich is necessary to determine feasibilityšand studied in academic settings, because the artifacts beingstudied are large. also, while some universities have stateoftheart hardware resources (although many do not),universities seldom invest in software tools and tend to lag behind industry in that area. this is a problem becausethere must be a fit between hardware and software across academic and industrial environments if large artifactsare to be experimented with other than as text (code). thus it is difficult to study large systems cost effectively.solving thisresearch modes20scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.problem requires innovations in funding, the details of which were beyond the scope of the workshop but whichwould clearly involve actions by government research funders, universities, and companies (including productdevelopment as well as research entities). another direction for improvement and relief may come from enhancednetworking, such as through the proposed national research network, which would allow dispersed researchers toshare access to artifacts, other researchers, and practitioners (cstb, 1988).if software systems are to be studied in corporate settings, a number of other difficulties will need to beovercome on the industry side. resolving these difficulties will take much thought and concerted action; the cstbworkshop identified key directions for change. the insights and enhancements that software engineering managersand practitioners seek will come at a price: industry must be willing to provide supportšfinancial and humanresources, and computer resources for experimentationšas well as access to the records of the proprietary system.mechanisms would be needed to compensate industry for its efforts to produce data in a form useful to researchersor for bearing the risk of experimenting with novel development activities.perhaps the biggest concern is protecting the proprietary interests of corporations, for whom largesystemsare often a source of competitive advantage. although the academic culture is devoted to openness andinformation exchange, universities are actively grappling with the problems of protecting corporate proprietaryinformation that are presented by increasing corporate interest in research on practical problems. business schoolsappear to have solved this problem some time ago. it should be possible to extend such efforts to apply toacademic research into corporate software systems.finally, one way to get around some of the difficulties of studying largesystems in corporate settings wouldbe to facilitate the study of large systems in government settings. the federal government has been the impetus forthe development of largescale integrated systems, interaction with academic researchers is a longtime traditionfor many government organizations, and government entities are more obligated to respond to governmentprograms or mandates. however, inasmuch as federal systems are developed and/or managed by privateorganizations, limitations on access to design and development processes and personnel may have to beovercome, as in purely corporate settings. also, some peculiarities of federal systems development are notgeneralizable to commercial systems. for example, the federal procurement process is associated withspecifications that are much more detailed than those typically generated by commercial buyers. study of federalsystems may therefore be an option that is second best.glean insights from behavioral and managerial sciencesthere is a need to better understand how groups of people collaborate in large projects involving a variety ofparticipants sharing a rich but uneven distribution of knowledge and imagination among them. softwareengineering research would be enhanced by greater interaction with behavioral, managerial, and other scientiststhat could lead to increasingly effective contributions to software engineering practice, in part by accelerating thetransfer of technology into and through the software engineering community. the field has benefited in the pastfrom technology transfer; for example, configuration management practices and changecontrol techniquesdeveloped in the aircraft industry were adopted in the 1950s and 1960s.there may be particular value in augmenting the insights of computer science and electrical engineering withthe insights of behavioral and managerial sciences. since large software systems will continue to be produced byteams for the foreseeable future, insights gained in other team contexts may be useful for software engineering. toget those insights it may be necessary for software engineers to actually team up withresearch modes21scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.specialists from other disciplines; the benefits of such crossdisciplinary teams have been demonstrated, forexample, in the area of ergonomics, where cognitive and management science specialists have been brought in todetermine how best to complement human skills with automation. even within computer science, some areas otherthan software engineering have aging software platforms that need to be reimplemented to make them less brittleand more easily changed or to improve the user interface to take advantage of workstation technology advances. insuch areas software engineers could collaborate with other types of computer scientists and engineers in newdevelopments that both produce new tools and serve as the objects of study. the cstb workshop pointed to aneed for software engineers to glean insight from people with complementary expertise but did not develop theconcept.develop additional directions and paradigms for software engineering researchsoftware engineering research today follows a variety of patterns, including the following: building systems with certain properties to show their feasibility; measuring properties of one or several systems; improving the performance of systems along particular dimensions; developing abstract formal models for certain domains; showing how to describe phenomena by designing languages; and making incremental improvements on prior work.all of these activities are relevant to complex software systems. but given the nature of those systems and theproblems we face today, some new approaches to research may also be productive.computer science and technology board workshop participants recommended that the academic researchcommunity expand its notion of good research to accept review or synthesis studies, case studies, comparativeanalyses, and development of unifying models for individual or multiple domains. in particular, review orsynthesis studies, which are common in a number of other fields, would support a greater and ongoing codificationof software engineering knowledge and help to minimize the reinvention of techniques and processes. finally, ifeffective handbooks are to be developed, as recommended above, research that supports such handbooks must beencouraged and rewarded.research modes22scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.5conclusionsmodern society is increasingly dependent on large, complex computerbased systems and therefore on thesoftware that drives them. in many cases, systems designed 20 years ago still provide a foundation for largebusinessesšand such systems are becoming unmaintainable. as the ability to manipulate, analyze, and graspinformation has been magnified by information systems, so also has the appetite to process more and moreinformation. each new application has generated ever more complex sets of software systems. in the past fewyears, problems with such systems have cost millions of dollars, time, and even lives in applications ranging fromaviation to controls for medical devices. improving the quality and the trustworthiness of software systems is anational priority, but it is also a problem that seems forever to receive less attention than it deserves becausesoftware systems seem invisible, are poorly understood by laymen, and are not even adequately addressed inuniversities. managers are consistently surprised by the inability of software engineers to deliver software ontime, within budget, and with expected functionality. the nation should not have to wait for a catastrophe before ittries to enhance this critical resource.the software research community has ridden the waves of several advances; expert systems and objectoriented programming have been among the topical loci of researchers during this decade. although largesystemdevelopers benefit from these and other advances, the software systems challenge is fundamental and is notamenable to solution through single categories of advances.as discussed in this report, a necessary first step is for the software engineering community and managerswho procure and use large software systems to adopt a more realistic vision of the complex software systemdevelopment process. following on, the direction and conduct of software engineering research should be bothbroadened (in particular, by fostering interactions with practitioners) and made more systematic, throughcodification and dissemination of knowledge as well as an infusion of more mathematics, science, andengineering. good problems make good science and engineeringšand good problems in the softwaredevelopment community are being bypassed because software engineering researchers are unable to deal with themin a structured, rigorous manner.the chinese pictograph for "crisis" is composed of the characters for "danger" and "opportunity." the wisdomthis represents is worth noting as we grapple with the looming crisis in our ability to create and maintain large andcomplex softwareconclusions23scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.systems. the danger is that soon we might not be able to create the software our business and governmentapplications need. the opportunity is there for the software engineering research community to find new andfruitful directions in the problems faced by practitioners.conclusions24scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.bibliographyad hoc committee on the high cost and risk of missioncritical software, report of the usaf scientific advisory board ad hoccommittee on the high cost and risk of missioncritical software, united states air force scientific advisory board, december1983.association for computing machinery (acm) (1988) proceedings of conference on computersupported cooperative work, september 26œ28, 1988, portland, oregon. new york: acm.balzer, r. (1985) "a 15 year perspective on automatic programming." ieee transactions on software engineering, 11(11):1257œ1268.belady, l. (1989) "software is the glue in large systems." ieee communications magazine (august).belady, l., and m. lehman (1985) program evolution processes of software change. london: academic press ltd.bernstein, l., and c. yuhas (1987) "the chain of command." unix review (november).besemer, d. j., et al. (1986) "a synergy of industrial and academic education." technical information series, general electric corporateresearch and development, schenectady, n.y., (august).biggerstaff, t., and a. perlis (1989) software reusability: concepts and models (vol. 1) and software reusability: applications andexperience (vol. 2). addison wesley/acm press frontiers in science.boehm, b. w. (1981) software engineering economics. englewood cliffs, n.j.: prenticehall.boehm, b. w. (1988) "a spiral model of software development and maintenance." ieee computer 21, 5 (may 1988), pp. 61œ72.booch, grady (1987) software components with ada. menlo park, calif.: benjamin/cummings publishing, p. 571.bowen, t., g. wigle, and j. tsai (1985) specification of software quality attributes. radctr8537, volume 1, rome air developmentcenter.brooks, frederick p., jr. (1986) "no silver bulletšessence and accidents of software engineering." information processing 86, h. j. kugler(ed.), elsevier science publishers b.v. (northholland), ifip.bibliography25scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.cleaveland, j. c. (1988) "building application generators." ieee software, (july):25œ33.cody, william james, jr. and william mccastline waite (1980) software manual for the elementary functions . englewood cliffs, n.j.:prenticehall, 269 pp.computer science and technology board (cstb), national research council (1988) toward a national research network. nationalacademy press, washington, d.c.curtis, b., h. krasner, and n. iscoe (1988) "a field study of the software design process." communications of the acm, 31(11), 1268œ1287.defense science board task force on military software, report of the defense science board task force on military software, office of theunder secretary of defense for acquisition, september 1987.freeman, p. (1987) "a conceptual analysis of the draco approach to constructing software systems." ieee transactions on softwareengineering , (july):830œ844.guindon, r. (ed.) (1988) cognitive science and its applications for humancomputer interaction. hillsdale, n.j.: lawrence erlbaumassociates.knuth, don (1973) the art of computer programming (series title: addisonwesley series in computer science and information processing. 2volumes. second ed.). reading, mass.: addisonwesley.martin, roger j., and wilma m. osborne (1983) "guidance on software maintenance." nbs special publication 500œ106. computer scienceand technology, u.s. department of commerce, national bureau of standards, washington, d.c., december 1983, p. 6.mccabe, t. (december 1976) "complexity measure." ieee transactions on software engineering, se2(4):308œ320.microelectronics and computer technology corporation (mcc) conference committee for cscw '86 (1986) proceedings of conference oncomputersupported cooperative work. december 3œ5. mcc, austin, tex.mills, harlan d. (february 1989) "benefits of rigorous methods of software engineering in dod software acquisitions."mosemann, lloyd k. (1989) "software engineering and beyond: the people problem." keynote address at the sei affiliates symposium,software engineering institute, carnegie mellon university, pittsburgh, pa., may 2œ4, 1989.neighbors, j. m. (1984) "the draco approach to constructing software from reusable components." ieee transactions on softwareengineering , se10(5):564œ573.partsch, h., and r. steinbruggen (1983) "program transformation systems." acm computing surveys, 15(3):199œ236.perry, robert h., et al. (1984) perry's chemical engineers' handbook . new york: mcgraw hill.royce, w. (1970) "managing the development of large software systems." proceedings wescon, august 1970.shaw, mary (1986) "beyond programminginthelarge: the next challenges for software engineering." technical memorandum sei86tm6, software engineering institute, carnegie mellon university, pittsburgh, pa., may 1986.shaw, mary (1989) "larger scale systems require higherlevel abstractions." proceedings of the fifth international workshop on softwarespecification and design, may 19œ20, 1989, pittsburgh, pa. association for computing machinery, new york.smith, d., g. kotik, and s. westfold (1985) "research on knowledgebased software environments at kestrel institute." ieee transactionson software engineering, 11(11):1278œ1295.swanson, e. b., and b. lientz (1980) software maintenance management: a study of the maintenance of computer application software in487 data processing organizations. reading, mass.: addisonwesley.bibliography26scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.williams, b. g., c. k. mui, b. b. johnson, and v. alagappan (1988) ''software design issues: a very large information systemsperspective." center for strategic technology research, arthur andersen & co., chicago, september 28, 1988.bibliography27scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.bibliography28scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.appendixesappendixes29scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.appendixes30scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.appendix aworkshop participantsvictor vyssotsky, digital equipment corporation (workshop chair)frances e. allen,* ibm t. j. watson research centerdavid r. barstow, * schlumberger wells gerrislaszlo a. belady,* microelectronics and computer technology corporation (mcc)larry bernstein,* at&t bell laboratoriesrichard b. butler,* ibm corporationthomas a. corbi,* ibm corporationjohn d. gannon, national science foundationsusan l. gerhart,* microelectronics and computer technology corporation (mcc)barry m. horowitz,* the mitre corporationbruce b. johnson,* arthur andersen and companyanita jones,* university of virginiakenneth kochbeck, mcdonnell douglas corporationharlan d. mills,* information systemsjohn b. munson,* unisys corporationdouglas t. ross,* softech, inc.winston royce,* software firstmary shaw,* carnegie mellon universitycharles simonyi,* microsoft corporationshmuel winograd, ibm t. j. watson research centerstephen wolfram, university of illinois at urbanachampaignwilliam a. wulf,* national science foundationandres g. zellweger,* cta, inc.arthur i. zygielbaum,* jet propulsion laboratorystaffmarjory s. blumenthal, cstbc. kristina gunsalus, cstb consultantpamela r. rodgers, cstb consultantdonna f. allen, administrative secretary* position statement appears in appendix b.appendix a31scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.appendix bposition statementsprior to the workshop, participants were asked to submit position statements that responded to the followingtwo questions:1. what do you consider to be the worst problem you have with current software production, and whatsuggestions do you have for alleviating it?2. what do you see as the most critical problem that industry and the nation have with current softwareproduction, and what solutions do you suggest?some participants revised their statements as a result of workshop deliberations. these statements arepresented as submitted by the authors, with some standardization of format.appendix b32scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.frances e. allenthe worst problem i have with current software production is transferring prototyped ideas developed in acomputer science environment to product software useful to customers. technology transfer between differentgroups is frequently difficult but transferring technologies and requirements between two very different cultures isdoubly difficult. users of software have reliability and economic (speed, space, cost) constraints that are of littleinterest to the computer scientist; the computer scientist has solutions which, when properly engineered, couldgreatly enhance products.i believe there are three ways of alleviating the problem. one way is to develop a technology for measuringand evaluating the effectiveness of an approach when applied to a given problem. we have ways of evaluating thecomplexity and correctness of an algorithm; we need ways of evaluating and predicting the appropriateness ofspecific software solutions to specific problems. in other words, software engineering must become a science withaccepted and validated predictive metrics.the second way of alleviating the problems of moving ideas and prototypes to market is to build usableprototypes. (i will discuss this below.)the third way of alleviating the problem is education. computer scientists must become more relevant andmust understand the realities of the market place.the most critical problem that industry and the nation have with current software production is the number oflines of code needed to accomplish a function. many production and maintenance costs can be correlated to thenumber of klocs (thousands of lines of code) needed. programmers produce x klocs a year and y errorsoccur per z kloc. but each kloc isn't providing much function. if programs were written in much higher levellanguages then many fewer klocs would be required to do the same job. so though the x, y, z numbers mightstay the same, fewer programmers would be needed and fewer errors would occur.moving to very high level languages requires compiler technology which effectively maps the program to theunderlying system without loss of efficiency. much of that technology exists today. i recommend a concentratedeffort on expanding that technology and exploiting it in the context of very high level languages.this proposal runs counter to what is happening today with c emerging as a major systems language. c isregressive in that it was designed to allow and generally requires that the user optimize his program. hence, usersof the language are spending time and effort doing what compilers can do as well or better in many cases. whathas been gained? more klocs but not more productivity, more function or fewer errors.appendix b33scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.david r. barstowcurrently, schlumberger's most significant software problem is duplication of effort: we often write severaltimes what appears to be essentially the same software. one solution to the problem is to maintain an extensivesoftware library, but this approach is complicated by a diversity of target machines and environments. a secondsolution would be to develop sophisticated programming environments that present to the user a higher levelcomputational model, coupled with translators that automatically produce code for different targets.the most critical software problem faced by industry and the nation is the cost of maintenance and evolution:most studies of software costs indicate that over twothirds of the cost of a large system is incurred after the systemis delivered. these costs cannot be reduced completely, of course, since uses and expectations about a softwaresystem will naturally change during the system's lifetime. but much of the cost is due to the fact that aconsiderable amount of information, such as the rationale for design and implementation decisions, is lost duringdevelopment and must be reconstructed by the maintainers and evolvers of the system. one way to address thisproblem would be to develop knowledgebased techniques for explicitly representing such information so that itcould be stored during development and referenced during evolution. one good way to develop such techniqueswould be through case studies of existing large systems, perhaps through collaborative efforts between industryand academia.appendix b34scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.laszlo a. beladyworst problemšpossible solutionsince large scale software development is a labor intensive activity, look for the problem where people spendthe most time. through our field studies of industry mcc found that the predominant activity in complex systemdevelopment is the participants' teaching and instructing each other. users must teach the software engineersabout the application domain, and vice versa; designers of subsystems must describe the intricacies of their workto other designers, and later to implementors; and since the process is rather iterative, this mutual teaching happensseveral times and in several participant groupings. indeed, most of the time all project participants must be readyto transmit the knowledge they have acquired about the emerging product and to analyze together theconsequences on the total systems of local (design) decisions.even more importantly, experience gathered in the computer aided system project setting could spawn muchneeded efforts in computer aiding the training and retraining process needed everywhere to keep the nation'sworkforce attuned to changing circumstances, and thus competitive.perhaps the experience accumulated over decades in computer aided instruction (cai) must be tuned,applied and refined for the complex system development process. results from ai could also be applied to helpeliminate the ''teaching" overload for all involved.industry/national problemsoftware is the glue that holds the islands of computer applications in distributed systems. for the nextdecades this gradual integration into networks will take place in each industry, between enterprises, at the nationallevel and beyond. the resulting systems will be built out of offtheshelf software and hardware components,where each integrated subsystem is unique and must be designed individually by a team of experts: users,managers, application specialists, programmers.the design of these "heterosystems" needs fundamentally new approaches, in particular: efficient, cooperative, project teamwork augmented by computer technology (which will be applicableeverywhere where people must work tightly together, not only in the computer industry) convergence of hardwaresoftware design; in fact, a deliberate shift in basic education is also needed tocreate interdisciplinary "system designers" instead of separate hardware and software professionals.but, more importantly, experience gathered in the computer aided system project setting could spawn muchneeded efforts in computer aiding the training and retraining process.appendix b35scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.larry bernsteinworst problem facing me in software productivitysoftware architecture problems are the most difficult for me. the solution requires problem solving skillswith a heavy dose of the ability to do engineering tradeoffs. people trained in computer science do not often bringthese skills to the work place. we cannot teach them in twoto fourweek short courses, so we often suffer whilerookies learn on the job. studies of computer science curricula in the acm pointed out the lack of problemsolvingskills in the typical computer science curriculum (computing as a discipline, peter j. denning, douglas e.commer, david gries, michael c. mulder, allen tucker, a. joe turner, and paul r. young, report of the acmtask force on the core of computer science, january 1989, vol. 32, no. 1). those with a bachelor's degree areoften mechanics who know how to program, but do not know how to decide what problem needs solving, or whatalternatives there are for its solution.making four semesters of engineering science a requirement for computer scientists is a minimal solution.apprenticeships and identifying software architectures are quite useful. prototypes are helpful to make designdecisions quantitative rather than qualitative.worst problems facing the country in software productivitytoo often funders, customers, and managers are willing to be "low balled" on effort estimation. the lack ofappreciation for up front capitalization in the software industry with consequential failures points to a seriousproblem confronting us. it leads to the scattered and slow application of proven techniques to enhance productivityand fosters a climate for hucksters to sell their latest all purpose course to those ailing projects.a technology platform incorporating proven approaches would facilitate technology transfer fromuniversities to industry and between companies. changes are needed to permit and foster such cooperationbetween competitors. ties of u.s. companies to japanese companies will speed the growth of the japanese asviable software competitors, yet we discourage similar ties in the united states. we need to have joint researchwith japan and canada so as to foster a market where each benefits and contributes to the extension of softwaretechnology. various harvard business review articles have dealt with capitalization and the introduction oftechnology.recommendationa specific research recommendation is to regularize design by creating a handbook which would: organize software knowledge, provide canonical architectures, provide algorithms in a more usable way than knuth did, facilitate understanding of constraints, domains of application, and tradeoff analysis, and foster codification of routine designs that can then be taught and used by journeyman architects.a second issue is to focus on software (not just code!) reuse. specific items to tackle include: determine when structured interfaces between subsystems with different models of the problem domainare sufficient and when integration by designing to a single model of the problem domain is necessary. develop benefit models for justifying investment in making software reusable. classify architecture which will encourage reuse. determine how much reuse is possible with current prices.appendix b36scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved. develop indexing and cataloging techniques to find reusable elements.a new theory of testing is needed to design software that is testable to certify quality.on design for testabilityhow do you specify the attributes (functional and nonfunctional) a system must possess in a manner whichpermits correct generation or proof?.what attributes are only verifiable by testing?what are the economic tradeoffs between proofs and testing?on certifying qualityclassify quality certification methods and measures effective in real (large) projects for functional and nonfunctional performance. examples include the following: proof of correctness. theory of stochastic software usage. functional scenario testing is thirty times more effective than coverage testing.we need to build systems in anticipation of change by understanding the correct granularity of componentsand forcing localization of change.appendix b37scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.richard b. butler and thomas a. corbiprogram understanding: challenge for the 1990'sabstractthere are a variety of motivators1 which are continuing to encourage corporations to invest in software toolsand training to increase software productivity, including: increased demand for software; limited supply ofsoftware engineers; rising software engineer support expectations (e.g., for case tools); and reduced hardwarecosts. a key motivator for software tools and programmer education in the 1990's will be software evolved overdecades from several thousand line, sequential programming systems into multimillion line, multitaskingcomplex systems.this paper discusses the nature of maturing complex systems. next, it examines current software technologyand engineering approaches to address continuing development of these systems. program understanding isidentified as a key element which supports many development activities. lack of training and education inunderstanding programs is identified as an inhibitor. directions to encourage development of new software toolsand engineering techniques to assist the process of understanding our industry's existing complex systems aresuggested.maturing complex systemsas the programming systems written in the 1960's and 1970's continue to mature, the focus for software toolsand programmer education will shift from tools and techniques to help develop new programming projects toanalysis tools and training to help us understand and enhance maturing complex programming systems.in the 1970's, the work of belady and lehman2 3 4 strongly suggested that all large programs will undergosignificant change during the inservice phase of their lifecycle, regardless of the a priori intentions of theorganization. clearly, they were right. as an industry, we have continued to grow and change our large softwaresystems to remove defects, address new requirements, improve design and/or performance, interface to new programs, adjust to changes in data structures or formats, exploit new hardware and software features, and scale up the new architectures and processing power.as we extended the lifetimes of our systems by continuing to modify and enhance them, we also increasedour already significant data processing investments in them and continued to increase our reliance on them.complex software systems have grown to be significant assets in many companies.however, as we introduce changes and enhancements into our maturing systems, the structure of the systemsbegins to deteriorate. modifications alter originally "clean" designs. fix is made upon fix. data structures arealtered. members of the "original" programming teams disperse. once ''current'' documentation gradually becomesoutdated. system erosion takes its toll and key systems steadily become less and less maintainable andincreasingly difficult, error prone, and expensive to modify.flaherty's5 study indicates the effect on productivity of modifying product code compared to producing newcode. his data for the studied s/370 communications, control, and language software show that productivitydifferences were greater between the ratio of changed source code to total amount of code than productivitydifferences between the different kinds of product classesšproductivity was lowest when changing less than 20%of the total code in each of the products studied. the kind of software seemed to be a less important factor relatedto lowerappendix b38scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.productivity than did the attribute of changing a small percentage of the total source code of the product. does thispredict ever decreasing programmer productivity for our industry as we change small percentages of maturingcomplex systems?clearly as systems grow older, larger, and more complex, the challenges which will face tomorrow'sprogramming community will be even more difficult than today's. even the wall street journal stereotypestoday's "beeper carrying" programmer who answers the call when catastrophe strikes:he is so vital because the computer software he maintains keeps blowing up, threatening to keep paychecks frombeing issued or invoices from being mailed. he must repeatedly ride to the rescue night and day because thesoftware, altered repeatedly over the years, has become brittle. programming problems have simply gotten out ofhand.corporate computer programmers, in fact, now spend 80% of their time just repairing the software and updating it tokeep it running. developing new applications in this patchwork quilt has become so muddled that many companiescan't figure out where all the money is going.6the skills needed to do today's programming job have become much more diverse. to successfully modifysome aging programs, programmers have become part historian, part detective, and part clairvoyant. why?"software renewal" or "enhancement" programming is quite different from the kind of idealized softwareengineering programming taught in university courses:the major difference between new development and enhancement work is the enormous impact that the base systemhas on key activities. for example, while a new system might start with exploring users' requirements and then moveinto design, an enhancements project will often force the users' requirements to fit into existing data and structuralconstraints, and much of the design effort will be devoted to exploring the current programs to find out how andwhere new features can be added and what their impact will be on existing functions.the task of making functional enhancements to existing systems can be likened to the architectural work of adding anew room to an existing building. the design will be severely constrained by the existing structure, and both thearchitect and the builders must take care not to weaken the existing structure when the additions are made. althoughthe costs of the new room usually will be lower than the costs of constructing an entirely new building, the costs persquare foot may be much higher because of the need to remove existing walls, reroute plumbing and electricalcircuits and take special care to avoid disrupting the current site.7the industry is becoming increasingly mired in these kinds of application software "renovation" andmaintenance problems. parikh8 reports the magnitude of the problem: results of a survey of 149 managers of mvs installations with programming staffs ranging from 25œ800programmers indicating that maintenance tasks (program fixes/modifications) represent from 55 to 95%of their work load. estimates that $30b is spent each year on maintenance (s10b in the us) with 50% of most companies'dp budgets going to maintenance and that 50œ80% of the time of an estimated 1m programmers orprogramming managers is spent on maintenance. an mit study which indicates that for every $1 allocated for a new development project, $9 will be spenton maintenance for the life cycle of the project.whereas improved design techniques, application generators, and wider usage of reusable software parts mayhelp alleviate some aspects of the "old code" problem,9 until these approaches take widespread hold in our criticalcomplex systems, programmers will need tools and training to assist in reconstructing anti analyzing informationin previously developed and modified systems. even when more "modern" software development techniques andtechnologies are widespread, new and unanticipated requirements for ''ities'' (e.g., usability, installability,reliability, integrity, security, recoverability, reconfigurability, serviceability, etc.) which are not yet taught insoftware engineering, are not yet part of the methodology being used, and are not yet "parameters" to the codegenerator will necessitate rediscovery and rework of our complex systems.appendix b39scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.approaches to maturing complex systemsthe notion of providing tools for program understanding is not new. work in the 1970's10 11 12 13 14 whichgrew out of the program proving, automatic programming and debugging, and artificial intelligence efforts firstbroached the subject. researchers stressed how rich program descriptions (assertions, invariants, etc.) couldautomate error detection and debugging. the difficulty in modelling interesting problem domains and representingprogramming knowledge, coupled with the problems of symbolic execution, has inhibited progress. while therehas been some limited success,15 the lack of fully implemented, robust systems capable of "understanding" and/ordebugging a wide range of programs underscores the difficulty of the problem and the shortcomings of these aibased approaches.recognizing the growing "old program" problem in the applications area, entrepreneurs have transformedthis problem into a business opportunity and are marketing "code restructuring" tools. a variety of restructuringtools have emerged (see reference16 for an examination of restructuring). the restructuring approach to address"old" programs has had mixed success. while helpful in some cases to clean up some modules, in other casesrestructuring does not appear to help.one government study17 has shown positive effects which can result from restructuring include some reducedmaintenance and testing time, more consistency of style, reduced violations of local coding and structurestandards, better learning, and additional structural documentation output from restructuring tools. however, onthe negative side: the initial source may not be able to be successfully processed by some restructurers requiringmodification before restructuring; compile times, load module size, and execution time for the restructured programcan increase; human intervention may be required to provide meaningful names for structures introduced by thetool.movement and replacement of block commentary is problematic for some restructurers. and, as has beenobserved, overall system control and data structures which have eroded over time are not addressed:if you pass an unstructured, unmodular mess through one of these restructuring systems, you end up with at best, astructured, unmodular mess. i personally feel modularity is more important than structured code; i have an easiertime dealing with programs with a bunch of goto's than one with it's control logic spread out over the entireprogram.18in general, automatically recapturing a design from source code, at the present state of the art, is notconsidered feasible. but some work is underway and some success has been reported. sneed et al.19 20 have beenworking with a unique set of cobol tools which can be used to assist in rediscovering information about oldcode via static analysis, to interactively assist in remodularizing and then restructuring, and finally to generate newsource code representation of the original software. also, research carried out jointly by criai (consorziocampano di ricerca per l'informatica e l'automazione industriale) and dis (dipartimento di informatica esistemistica at the university of naples) reports21 the automatic generation of low level jackson or warnier/orrdocuments which are totally consistent with cobol source code.both sneed and criai/dis agree, however, that determining higher level design abstractions will requireadditional knowledge outside that which can be analyzed directly from the source code.the experience of ibm's federal systems division with the aging federal aviation administration'snational airspace system (nas)22 seems to indicate that the best way out is to relearn the old software relyingprimarily on the source code, to rediscover the module and datastructure design, and to use a structured approach2325 of formally recording the design in a design language which supports the data typing, abstract types, controlstructures, and data abstraction models.this often proved to be an iterative process (from very detailed design levels to more abstract), but it resultedin a uniform means of understanding and communicating about theappendix b40scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.original design. the function and state machine models then provided the designer a specification from which,subsequently, to make changes to the source code.the need to expand "traditional" software engineering techniques to encompass reverse engineering designand to address "software redevelopment" has been recognized elsewhere:the principal technical activity of software engineering is moving toward something akin to "softwareredevelopment." software redevelopment means taking an existing software description (e.g., as expressed in aprogramming or very high level language) and transforming it into an efficient, easiertomaintain realizationportable across local computing environments. this redevelopment technology would ideally be applicable to both 1)rapidly assembled system prototypes into production quality systems, and 2) old procrustean software developed 3 to20 years ago still in use and embedded in ongoing organization routines but increasingly difficult to maintain.26understanding programs: a key activitywith our aging software systems, studies indicate that "more than half of the programmer's task isunderstanding the system."27 the fjeldstathamlen study28 found that, in making an enhancement, maintenanceprogrammers studied the original program about threeandahalf times as long as they studied the documentation, and just as long as they spent implementing the enhancement.in order to work with "old" code, today's programmers are forced to spend most of their time studying theonly really accurate representation of the system.to understand a program, there are three things you can do: read about it (e.g., documentation); read it (e.g.,source code); or run it (e.g., watch execution, get trace data, examine dynamic storage, etc.). static analysis(control flow, data flow, cross reference) can augment reading the source. documentation can be excellent or itcan be misleading. studying the dynamic behavior of an executing program can be very useful and candramatically improve understanding by revealing program characteristics which cannot be assimilated fromreading the source code alone. but the source code is usually the primary source of information.while we all recognize that "understanding" a program is important, most often it goes unmentioned as anexplicit task in most programmer job or task descriptions. why? the process of understanding a piece of code isnot an explicit deliverable in a programming project. sometimes a junior programmer will have an assignment to"learn this piece of code"šoddly, as if it were a one time activity.experienced programmers who do enhancement programming realize, just as architects and builders doing amajor renovation, that they must repeatedly examine the actual existing structure. old architectural designs andblueprints may be of some use, but to be certain that a modification will be successful, they must discover orrediscover and assemble detailed pieces of information by going to the "site." in programming, regardless of the"waterfall" or "iterative" process, this kind of investigation happens at various points along the way: while requirements are being examined, lead designers or developers are typically navigating through theexisting code base to get a rough idea of the size of the job, the areas of the system which will beimpacted, and the knowledge and skills which will be needed by the programming team which does thework. as design proceeds from the high level to low level, each of the team members repeatedly examines theexisting code base to discover how the new function can be grafted onto the existing data structures andinto the general control flow and data flow of the existing system. wise designers may tour the existing code to get an idea of performance implications which theenhancement may have on various critical paths through the existing system. just before the coding begins, programmers are looking over the "neighborhood" of modules which willbe involved in the enhancement. they are doing the planning of the detailed packagingšseparating thelow level design into pieces which must be implemented by newappendix b41scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.modules or which can be fit into existing modules. often, they are building the lists of new and changedmodules and macros for the configuration management or library control team who need this informationin order to reintegrate the new and changed source code when putting the pieces of the system backtogether again. during the coding phase, programmers are immersed in the "old code". programmers are constantlymaking very detailed decisions to rewrite or restructure existing code vs. decisions to change the existingcode by deleting, moving, and adding a few lines here and a few lines there. understanding the existingprograms is also key to adding new modules: how to interface to existing functions in the old code? howto use the existing data structures properly? how not to cause unwanted side effects? a new requirement or two and a few design changes usually come into focus after the programmers haveleft the starting blocks. "new code" has just become "old code". unanticipated changes must be evaluatedas to their potential impact to the system and whether or not these proposed changes can be contained inthe current schedules and resources. the "old base" and the "new evolving" code under developmentmust be scrutinized to supplement the intuitions of the lead programmers before notifying managementof the risks. testers may delve into the code if they are using "white box" techniques. sometimes even a technicalwriter will venture into the source code to clarify something for a publication under revision. debugging, dump reading, and trace analysis constantly require long terminal sessions of "programunderstanding" where symptoms are used to postulate causes. each hypothesis causes the programmer togo exploring the existing system to find the source of the bug. and when the problem is found, then amore "bounded" exploration is usually required to gather the key information required to actually buildthe fix and insert yet another modification into the system.therefore, the program understanding process is a crucial subelement in achieving many of the projectdeliverables: sizings, high level design, low level design, build plan, actual code, debugged code, fixes, etc.the programmer attempts to understand a programming systems so he can make informed decisions about thechanges he is making. the literature refers to this "understanding process" as "program comprehension":the program comprehension task is a critical one because it is a subtask of debugging, modification, and learning.the programmer is given a program and is asked to study it. we conjecture that the programmer, with the aid of hisor her syntactic knowledge of the language, constructs a multileveled internal semantic structure to represent theprogram. at the highest level the programmer should develop an understanding of what the program does: forexample, this program sorts an input tape containing fixedlength records, prints a word frequency dictionary, orparsers an arithmetic expression. this highlevel comprehension may be accomplished even if lowlevel details arenot fully understood. at low semantic levels the programmer may recognize familiar sequences of statements oralgorithms. similarly, the programmer may comprehend lowlevel details without recognizing the overall pattern ofoperation. the central contention is that programmers develop an internal semantic structure to represent the syntaxof the program, but they do not memorize or comprehend the program in a linebyline form based on syntax.29learning to understand programswhile software engineering (e.g., applied computer science) appears as a course offering in many universityand college computer science departments, "software renewal", "program comprehension", or "enhancementprogramming" is absent. when you think in terms of the skills which are needed as our software assets grow andage, lack of academic training in "how to go about understanding programs" will be a major inhibitor toprogrammer productivity in the 1990's.... unfortunately, a review by the author of more than 50 books on programming methodologies revealed almost nocitations dealing with the productivity of functional enhancements, except a few minor observations in the context ofmaintenance.the work of functional enhancements to existing software systems is underreported in the softwareappendix b42scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.engineering curriculums, too, and very few courses exist in which this kind of programming is even discussed, muchless taught effectively. 7for other "language" disciplines, classical training includes learning to speak, read, and write. readingcomprehension is partner with composition and rhetoric. in school, we are required to read and critique variousauthors. an english education curriculum does not teach "basic language skills" (programming language syntaxand semantics), "recommended sentence structures" (structural programming), and "short stories" (algorithms),expecting students to be fully trained, productive copy editors/authors for major publications. yet, manycomputer science departments sincerely believe that they are preparing their students to be ready for theworkplace.unfortunately, most new college graduates entering today's software industry must confront a veryconsiderable "learning curve" about an existing system before they get to the point where they can begin to try todo design or coding. they have little or no training nor much tool assistance to do this. acquiring programmingcomprehension skills has been left largely to "onthejob" training while trying to learn about an existing system.30even experienced programmers can have trouble moving to a different project.the lack of training and tools to help in understanding large, "old" programming systems also has anothernegative effect on productivity. it is resulting in a kind of job stagnation throughout the industry which boehmterms the "inverse peter principle".31the inverse peter principle: "people rise to an organizational position in which they become irreplaceable, and getstuck there forever." this is most often encountered in software maintenance, where a programmer becomes souniquely expert on the inner complexities and operating rituals of a piece of software that the organization refuses tolet the person work on anything else. the usual outcome is for the programmer to leave the organization entirely,leaving an even worse situation.as a large programming system grows older and older, more and more talented programmers will "get stuck"due to the "inverse peter principle". "getting stuck" directly impacts attempts by management to maximizeproject productivity by assigning the most talented programmers to get the next job done. therefore, a lack ofprogram understanding, training, and tools is a productivity inhibitor for new programmers on a project as well as acareer inhibitor for the key project "gurus". as our programming systems grow in age, size, and complexity, theseproblems will compound, becoming increasingly more acute.directionsan industry focus on "software renewal" tools and programmer education is needed to reduce the costs tomodify and maintain large complex programming systems, to improve our understanding of our programs so wecan continue to extend their life and restructure them as needed, and to build bridges from old software to newdesign techniques and notations and reuse technologies.just as library and configuration control systems were developed when the volumes of source code and thenumbers of programmers working on a system increased, it is inevitable that new tools systems for managing theinformation about large programming systems will emerge to support long term "software renewal".just as software engineering education evolved to guide development of new programming systems, newconcepts and techniques must be developed to assist programmers in rediscovering the properties andunderstanding our longlived, complex systems.software engineering has an opportunity to develop new methods for implanting "guideposts" so that thenext generation's programmer can more easily gain insight into the rationale of past programming.appendix b43scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.references1. barry w. boehm, maria h. penedo, e. don stuckle, robert d. williams, and arthur b. pyster, "a software development environment forimproving productivity," ieee computer, 17, no. 6, 30œ44, june (1984).2. l.a. belady and m.m. lehman, "a model of large program development," ibm systems journal, 15, no. 3, 225œ252, (1976).3. m.m. lehman and f.h. parr, "program evolution and its impact on software engineering," proceedings of the 2nd international conferenceon software engineering, san francisco, october (1976).4. m. m. lehman, "laws of evolution dynamics  rules and tools for programming management," proceedings of the infotech conferenceon why software projects fail, london, april (1978).5. m.j. flaherty, "programming process measurement for the system/370," ibm systems journal, 24, no. 2, 172œ173, (1985).6. paul b. carroll, "computer glitch: patching up software occupies programmers and disables systems," wall street journal, 1, january22, (1988).7. capers jones, "how not to measure programming quality," computer world, 82, january 20 (1986).8. girish parikh, "making the immortal language work," international computer programs business software review, 33, april (1987).9. ronald a. radice and richard w. phillips, software engineering: an industrial approach, volume 1, pp. 14œ19, prentice hall, englewoodcliffs, 1988.10. i.p. goldstein, "summary of mycroft: a system for understanding simple picture programs," artificial intelligence, 6, 249œ277,(1975).11. s.m. katz and z. manna, "toward automatic debugging of programs," sigplan notices, 10, 143œ155, (1975).12. g.r. ruth, "intelligent program analysis," artificial intelligence, 7, 65œ87, (1976).713. s.m. katz and z. manna, "logical analysis of programs," communications of the acm, 19, 188œ206, (1976).14. f.j. lukey, "understanding and debugging programs," international journal of manmachine studies, 12, 189œ202, (1980).15. w.l. johnson and e. soloway, "proust: knowledge based program understanding," proceedings of the seventh internationalconference on software engineering, orlando, fl, march (1984).16. robert s. arnold, editor. tutorial on software restructuring, ieee computer society press, washington, dc, 1986.17. parallel test and evaluation of a cobol restructuring tool, u.s. general accounting office, september 1987.18. irv wendel, "software tools of the pleistocene," software maintenance news, 4, no. 10, 20, october (1986).19. h.m. sneed, "software renewal: a case study," ieee software, 1, no. 3, 56œ63, july (1984).20. h.m. sneed and g. jandrasics, "software recycling," ieee conference on software maintenance, 82œ90, austin, tx, september (1987).21. p. antonini, p. benedusi, g. cantone, and a. cimitile, "maintenance and reverse engineering: lowlevel design documents productionand improvement," ieee conference on software maintenance, 91œ100, austin, tx, september (1987).22. robert n. britcher and james j. craig, "using modern design practices to upgrade aging software systems," ieee software, 3, no. 3,16œ24, may (1986).23. a.b. ferrentino and h.d. mills," state machines and their semantics in software engineering," proceedings of compsac '77, 242œ251,(1977).appendix b44scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.24. r.c. linger, h.d. mills, and b.i. witt, structured programming theory and practice, addisonwesley, reading, ma, 1979.25. h.d. mills, d. o'neill, r.c. linger, m. dyer, and r.e. quinnan, "the management of software engineering," ibm systems journal, 19,no. 4, 414œ477, (1980).26. walt scacchi, "managing software engineering projects: a social analysis," ieee transactions on software engineering, se10, no. 1,49œ59, january (1984).27. girish parikh and nicholas zvegintzov, editor. tutorial on software maintenance, p. ix, ieee computer society press, silver spring, md,1983.28. r.k. fjeldstad and w.t. hamlen, "application program maintenance study: report to our respondents," proceedings of guide 48, theguide corporation, philadelphia, pa, (1979).29. b. shneiderman and r. mayer, "syntactic/semantic interactions in programmer behavior: a model and experimental results,"international journal of computer and information science, 8, no. 3, 219œ238, (1979).30. carolynn van dyke, "taking 'computer literacy' literally," communications of the acm, 30, no. 5, 366œ374, may (1987).31. barry w. boehm, software engineering economics, p. 671, prenticehall, inc., englewood cliffs, nj, 1981.appendix b45scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.richard b. butler and thomas a. corbijapan: nationally coordinated r&d for the information industryabstracta healthy, growing, competitive u.s. software industry is a key element to the national security and to wellbeing of the american economy. not only is software an important industry in itself, but complex system'ssoftware is key to providing leadership in computer hardware. it is also the technological enabler to the fullspectrum of energy, retail, manufacturing, transportation, construction, and financial industries and to education,government, and the military. today's complex software systems will continue to evolve to provide the supportinginformation services needed and brand new complex systems will emerge.ever improving u.s. software technology research and development and growing computer literacy arecrucial to our industry and nation, if we are to continue to compete worldwide in developing new complexsoftware systems to support future economic growth. the alternatives are either to fall behind the worldwidecompetition in the spectrum of industries dependent on complex systems technology or to become increasinglydependent on foreign nations as suppliers of this technology.this paper briefly examines japan's coordinated research and development of various software technologiesand computer literacy efforts. the pattern has been widely discussed in the literature. miti jointly develops goalswith industry and for education. western technology is closely studied. western developments are reduced topractice. incremental improvements are made. competitive product offerings emerge and are then exported.development of a u.s. national agenda to coordinate and accelerate research and development of softwaretechnologies and to improve computer literacy is suggested.miti goal settingnew haven, conn.šibm's toughest competition is not coming from u.s. companies, but from the japanese,ibm's chairman and chief executive officer, john f. akers, said monday.the japanese have created full product lines, technological capabilities, flexible alliances and worldwide distributionnetworks, akers said, in citing their accomplishments to students at yale university's school of organization andmanagement.akers listed a number of companies in europe and the united states, but he said: "the japanese are the toughest. noquestion about it." 1since 1970, the japanese government has been promoting research and development activities in the"information industry" through its ministry of international trade and industry (miti). miti has identified the"information industry" as a probable successor to steel and automobiles, has been steering legislation through thejapanese diet (like our congress), has set longterm national directions and goals, has established phased projectsto work toward those goals, and has funded forwardlooking research and promoted cooperative ventures amongthe japanese computer manufacturers (jcms).in 1970, the diet passed a law submitted by miti to promote information technology with a central body tocoordinate various actions and plans. this organization was called the information technology promotion agency(ipa). the program has received significant funding. for example, miti's government support and loans from thejapan development bank for ipa committed about $373 million for hardware (includes 5th generation project)and $566 million for software through fiscal year 1986. reportedly, the participating jcms invest 2 to 3 dollarsfor every dollar invested by miti.miti's direction and influence are widely reported in the computer trade press. miti's visions for theinformation industry have been released every 5 to 7 years for the purpose of gaining private sector consensus inorder to develop national industrial policy. significant reports haveappendix b46scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.been issued in 1976, 1981, and 1987. in developing the june 1987 report, ibm japan and at&t international(japan), ltd. representatives were included for the first time as part of a group of 30 individuals formed by thebasic policy committee of the miti's information industry structure council. the 1987 miti report, titled "avision of the information industry in the year 2000" has been commented on by the japanese press.2the report clearly states, "it is expected that the information industry will eventually become a mainstay toreplace the steel and automobile industries." this kind of a change implies a significant shift in the japaneseeconomy. it spells out japan's objectives underscoring their serious, long term commitment to such topics as: continued needs for faster, more sophisticated hardware with new function. continued push on lower costs and better priceperformance. enriched software pool and more efficient software development. trends toward the internationalization of the industry and investigation of software developmentcapabilities of developing nations centered on china and asiannics to fill supply/demand gap forsoftware. interoperability (e.g., osiopen software interconnection) worldwide and various standardization of enduser interfaces is seen as a trend. systems integration combining and adding new technology to base products of medium to small sizemanufacturers. distribution of information services (value added, beyond telecommunication and networking). multiple media terminals. "information literacy", training, and education for the general population. increasing dependence on information industry implies improvements in security, auditability, andreliability (nonstop). diversification of information services (database provider) which the information industry (databasedistributor) can provide. promotion of information technology in nonmetropolitan areas.in the u.s., government and industry have formed organizations with software development technologyactivities such as: the dod software engineering institute (sei) at carnegie mellon university; the nationalscience foundation software engineering research center (nsf serc) with purdue university and universityof florida; microelectronics and computer technology corporation (mcc); the software productivity consortium(spc); open software foundation (osf).but none of these have the kind of nationally coordinated focus, goal setting, and funding which miti andipa provide in japan.reducing to practice developments in the westyou can observe a lot by watching....šattributed to yogi berrain the private sector, the japanese computer manufacturers have been closely following the development ofprogramming, design, and testing methodologies in the west.the jcms have experimented with various methodologies and tools since the 1970's. they have beenfollowing developments in software engineering methodologies since the mid1970's (structured programming,topdown design, inspections, quality control circles, etc.). in the late 1970's and early 1980's, they began toinstitutionalize these methodologies and began building various tool sets to automate or control various softwaredevelopment tasks (library control, graphics for low level design, reusable parts, test coverage, etc.). they havemethodically applied the quality control techniques which were so successful in manufacturing settings to softwareappendix b47scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.development, employed statistical methods for analyzing costs and testing, promoted widespread usage of qualitycontrol techniques, and utilized test coverage tools.the jtech panel report on computer science, prepared by saic, for the u.s. department of commerceindicated that japan was behind in basic software research and advanced software development, but even with andgaining on the u.s. in delivering product software. however, a variety of published journal and conferencepapers,4 5 6 7 8 9 10 11 indicate that research and advanced development on a variety of software developmenttechnologies is starting to show progress. the jcms began commercially offering a variety of mainframe "case"application development support systems for cobol which use graphics, data dictionaries and support reuse andcode generation. they continue to experiment with different requirements, high level design, and low level designlanguages and notations and hypertext for documentation. many jcms use "integrated development supportenvironments" on mainframes for internal development which have graphical design and coding interfaces. someof the systems have the concept of "reusable software parts" and have facilities for building up libraries of theseparts. experimentation with artificial intelligence and knowledge based expert systems approaches are also beingreported as assists for software development and testing.in 1985, a u.s. department of commerce report summarized in ieee computer12 described the jcm focuson software development as follows:the japanese are apparently ahead of u.s. firms in making improvements to existing technologies from the u.s. andeurope, and in incorporating these into highly marketable products. the japanese have also made impressive gains inthe development of software tools and have encouraged their widespread use to boost productivity. both the japanesegovernment and the leading computing manufacturers began investing in the automation of the softwaredevelopment process nearly seven years ago.in the same year, miti and ipa embarked on the fiveyear sigma project (software industrial generatorand maintenance aids) which, if successful, will result in an industrywide standard, workstation based, softwaredevelopment platform. the published goals of sigma were to improve productivity by a factor of 4 and to build amachine independent, standardized development environment and a common network for exchanging programsand technical information. those goals have led to a 1988 prototype sigma workstation environment withinterfaces to the jcm mainframe systems. in the past three years, the jcms with miti guidance have systematizedwhat they have learned and have begun deploying "integrated" development environments on mainframes, trainingprogrammers, and beginning to standardize on and use the unixderived sigma workstation system.about 177 companies are now involved in sigma. systems overview and recent status on sigma has beenpublished by the ipa.13 14 15 common tools, business application tools, and scientific tools are finishingimplementation and undergoing prototype usage. several sigma centers for networking and database have beenidentified. the plan to begin using the sigma software and workstations in 50 companies starting in early 1988as a trial before general production usage in mid1988 has kept on schedule.while there does not appear to have been any "breakthrough" technology apparent in sigma and otherreports from japan, there has been the steady assimilation of published work in the west and exploration/adaptation in japan, followed by efforts to bring the best available techniques and tools into practical usage with aheavy emphasis on quality control.recently, miti launched a task force for "friend 21" (future information environment development forthe 21st century) aimed at developing an advanced technology with strong emphasis on user friendliness: japaneselanguage, graphics and advanced interfaces. this is a new six year effort, begun in fy 1988.similar themes are also apparent in the west: promoting various methodologies including: structured programming, topdown design, dataflowdesign, enterprise analysis, inspections and reviews, etc. concern about "up stream" requirements and design activitiesappendix b48scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved. touting higher level languages for design and programming applying tools and programming language improvements to commercial application programming developing software development environments and ''integrated'' tools using workstation and exploiting graphical notations for design and programming emphasizing reuse technologies for productivity exploring artificial intelligence (ai) for traditional software development forming national or joint venture research organizations focusing on software technologiesyet there has not been an equivalent, coordinated, national investment in technology and training with thegoals to bring advanced software development technology into widespread usage in the u.s. what we are seeingnow is the logical continuation of what was reported five years ago in the work done by university of southflorida16 and the ibmsponsored project at the university of maryland.17if the published reports are true, the japanese computer manufacturers and japanese computer scienceresearch community appear to be continuing a steady advance on a broad set of software issues and deploymentand usage of software development methodologies, tools and workstations.impressive claims for quality and significantly improved productivity based on software engineeringmethodologies, quality control, and tool environments appear throughout the published japanese literature. thewest can dispute the numbers or the counting methodology, but we cannot dispute the japanese commitment andbelief that what they are using does make a significant difference.computer science literacyjapan's basic literacy rate has been reported as high as 99.8%. the percentage of the population completingjapanese secondary school (which has higher standards than u.s. schools) is far greater than japan's closesteducational rivals, canada and the united states.18 miti is well aware of the shortage of trained japanesesoftware developers and has begun laying the groundwork for new education programs in japan with specializeduniversities.japan currently has over 430,000 programmers and projects a shortage of over 970,000 by 2000. miti'scouncil on the structure of industry expects software to grow from 1.05% of japan's gnp (approx. $23b) to 3.97%(estimated $233b) by the year 2000 with 2.15m software engineers according to published reports.19to address the projected shortage, miti is reportedly planning to establish a central university for information processing for research and development on softwaretechnology education methods and instructor training, establish local universities for information and processing, continue development of computer aided instruction (cai) systems, develop a central data base to integrate a number of previous independently built information data bases, install an online network linking all software subcontractors, and establish a council to nominate model technical colleges to introduce an advanced software engineeringcurriculum.in addition, miti is also interested in software joint ventures with china and other asiannics to fill the gapin skills at home. miti also drafted a bill aimed at delocalizing software industries which are currently clustered inthe tokyo region. the bill provides that the government would build basic facilities and provide tax and financialincentives to lure companies away from tokyo. software industrial centers would be constructed at 20 locationsacross japan.it is also reported that miti was planning to establish a "research institute of softwareappendix b49scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.engineering" where major information services companies will develop technologies to improve softwareproductivity, reliability, and technology transfer to asian countries (trainees from these countries will be part ofthe institute).directionsa u.s. national agenda to aggressively promote development of software technology and softwareengineering education has not materialized from congress, darpa, nsf or the dod software engineeringinstitute at carnegiemellon university. the u.s. has nothing comparable to the miti guided initiatives. whatactions have been taken in the computer science arena based on the recommendations of the japan technology(jtech) assessment?3 those studies were prepared for the purpose of aiding the u.s. response to japan'stechnological challenge. a much wider, coordinated, national agenda on advanced software developmenttechnology and computer science literacy is needed.both the u.s. government and the private sector must make the difficult decision to commit to invest andmodernize our software development methods, education, tools, and deployed workstation technology soon, if weexpect to be able to compete effectively in the marketplaces which jcm developed complex systems software willbe entering worldwide in the next decade.references1. larry rosenthal (associated press), "top competitor? it's japan, inc. ibm chief says," poughkeepsie journal, september 20, (1988).2.  "nihon jyohou sangyo shimbun," the japan information industry newspaper, april 7, 1987.3. science applications international corporation, japanese technology assessment: computer science, opto and microelectronics,mechatronics, biotechnology, pp. 23œ88, noyes data corporation, park ridge, n.j., 1986.4. d. tajima and t matsubara, "inside the japanese software industry," ieee computer, 17 no. 3, 34œ43, march (1984).5. , "six jcm's strategies for the 1990's," pp. 55œ144. nikkei computer, special issue, october 13, 1986.6. , "proceedings of the 32nd national conference of the information processing society of japan," information processing society ofjapan, march, 1986.7. , "proceedings of the 33rd national conference of the information processing society of japan", information processing society ofjapan, october, 1986.8. , "proceedings of the 34th national conference of the information processing society of japan," information processing society ofjapan, april, 1987.9. , "proceedings of the 35th national conference of the information processing society of japan," information processing society ofjapan, september, 1987.10. , "proceedings of the 36th national conference of the information processing society of japan," information processing society ofjapan, april, 1988.11. , "proceedings of the 37th national conference of the information processing society of japan," information processing society ofjapan, september, 1988.12. w. myers, "assessment of the competitiveness of the u.s. software industry," ieee computer, 18, no. 3, 81œ92, march (1985).13. , "second sigma symposium text," information technology promotion agency sigma system development office, june, 1986.14. , "sigma project report," information technology promotion agency sigma system development office, march, 1988.15. , "what's new sigma," information technology promotion agency sigma system development office, september, 1988.appendix b50scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.16. k.h. kim, "a look at japan's development of software engineering technology," ieee computer, 16, no. 5, 26œ37, may (1983).17. marvin v. zelkowitz, raymond t. yeh, richard g. hamlet, john d. gannon, and victor r. basili, "software engineering practices in theu.s. and japan," ieee computer, 17, no. 6, 57œ66, june (1984).18. jack baranson, the japanese challenge to u.s. industry, p. xii, lexington books, dc. heath and company, lexington, ma., 1981.19. , "asahi shimbun," asahi newspaper, april 21, 1987.appendix b51scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.susan l. gerhartcomplexity, multiple paradigms, and analysisthis position statement first proposes a theory about the origins of complexity in software systems and thendiscusses some responses to the problem: address the complexity of the world reflected in the system as well asthe internal complexity of the system; investigate multiparadigm approaches as a widesweeping strategy;establish a baseline of software engineering knowledge and build up to it; and stimulate a transition tocomputationintensive software production. for the last, i propose a novel cultural experiment: a nationalnetworkbased analysis game with the objective of demonstrating a fully understood software system within 3years, where the rules of the game drive a new production paradigm.observations on complex systemswhat makes complex systems complex? is complexity inherent when software is the focal point of thesystem? how can we characterize the complexity of software systems? can we identify avoidable and unavoidablekinds of complexity? looking at systems from a broad perspective, we see several ways in which they may becomplex: structurešsubsystems, modules, macros, down to statements and expressions behavioršobservable activity of a system functionštransformations on components of the state processšflow of control reactivityševents to which the system must respond timingšconstraints on response times statešpersistent and transient data with associated consistency and correctness invariants applicationšrequirements from the system context recovery of state and continuation of reactivity security of state from destruction or unwanted inspection safety from catastrophic external or internal events interfaces with users and other systems operations which maintain the state and continuity of the system development environmentšpeople and processes producing the code design and implementation documentation and training verification, validation, and certificationin most complex systems no one of the above is particularly simple. even a small running system may bequite complex and ultimately large when all the development, operational, and interface aspects are considered. acomplex software systems is much more than just code.software inherits the complexity of the world in which it operates, coming from hardware, users, andtechnological progress. by nature, software always will be pushed as far as possible to supply flexibility,variability, extensibility, and evolvability. thus, maybe we should not look at complexity as a property of softwaresystems but primarily as a property of their environments. the software for a tax code cannot be less simple thanthe tax code itself, a user interface is often meant to allow the user to mess with the system's state, and mostenvironments have some users and accidents that are hostile to computer systems. a software system often isattacking what rittel calls a "wicked problem", one where the advance of the system changes the environment sothat additional problems are created as the original one appears to be solved (although it often wasn't the realproblem, anyway).a simple example is an institutional calendar system, a shared list of events that everybody should knowabout. the management of calendar data is not hard, nor is a pleasant interface,appendix b52scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.though the complexity of each is influenced by the technology base, e.g., a standard relational data base. but thecalendar operations are the killer: who puts the data in (it's not fun and it's easy to forget to do it) and when anevent is canceled or changed, does the system adapt well to change? plus, users always want to extendfunctionality, e.g., to schedule meeting rooms or provide personal calendar services. everybody knows exactlywhat the requirements are for such a system: support my personal calendar habits. soon, the calendar systembecomes contentious as it intrudes on policy (who has priority for rooms?) or privacy (who stays home in themorning?). or, it becomes more and more relied upon until the day when somebody forgets to add an importantstaff meeting and half the staff miss an important announcement. the opportunity for an institutional calendarsystem was clear, implementation was reasonably easy at first, but the system isn't only the implemented software,it's also the entire operations and the changes in environment to utilize that opportunity,what techniques are known for controlling these various kinds of complexity? programming languages havecome a long way toward providing good structuring mechanisms and principles, but there are still few tools forevaluating the structural complexity of a system or presenting the structure in an intelligible graphical manner.timing and reactivity are yielding to a few good realtime case products on the market, but the theory is stillweak and the design methodology is obscure. function and state descriptions are amenable to formal techniquesthat are becoming more widely used. interfaces, especially graphical, have been stimulated by the progress inscientific visualization and the power of workstations and pcs but software engineers still lack the presentationtechniques of physical system engineers or even social scientists. though not a part of computer science, the fieldof systems definition (or "systems analysis"), i.e., automating existing systems and defining requirements for newones, is well developed with widely practiced techniques. recovery, safety, performance, and security are wellestablished, but difficult, fields of computer science, unfortunately rather insular. design theory is a newlyrecognized field and some theories about structure, capture, and use of design rationale are being explored.operations and some aspects of development are the challenge for new ideas in process programming (processesare just another form of software). development practices are slowly improving through internal companyeducation programs. the coordination of technical teamwork (computer supported cooperative work) is a wholenew field utilizing networks, social science, and distributed computing theory. documentation through hypertextand multimedia training are on the horizon. and computer networks link the research and developmentcommunities so that ideas move fast and informal communication supports new forms of learning.if the above is mainly good news, then why does the creation of a software system always end up mired indifficulty and the end system appear so unbelievably complex? i argue that this is because software reflects thecomplexity of the world around it and is naturally used as the manager of complexity, pushing beyond the limitsof experience and often beyond the analytic capabilities of the scientific base. we know we can't predict theperformance of a system or work through all the possible test cases, but those limits always tend to be disregardedas complex systems are undertaken.in addition, there's simply an awful lot to know to build a complex system. specialists may understand how tocontrol complexity of some aspects of a system, but the total knowledge is beyond any individual, beyond mostprojects, and poorly organized. the computer scientist focuses, properly, on the theoretically based aspects of thecomplex systems. software engineering branches out to incorporate process programming and system definition,but few software engineers have the specialists' knowledge for quality factors, such as performance and security.application scientists and engineers have techniques for understanding the problems being embedded in software,but they have difficulties communicating with software engineers and computer scientists. and, because systemdevelopment is still so unpredictable and the options change so frequently, managers cannot get a handle on thedevelopment or certification processes.so, what can we do? taking the definition of the problem as "opportunity lost" may be theappendix b53scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.wrong point of view. realizing that software provides an opportunity to bring complexity in the world under somecontrol may be more fruitful, e.g., eliminating ad hoc procedures, regularizing some related data base, and slowingdown change.total complexity = existing environment complexity (recognized or not)+ the internal complexity of the introduced system+ the operational complexity of procedures for using and maintaining the system+ the development environment complexity+ the complexity of inducing change in the world to utilize the opportunity.my worst problems with softwarefirst, the existence of multiple paradigms is a much bigger problem than might be expected. a paradigmšmine is logic programmingšgets a big hold on one's intellect and makes the understanding of other paradigmsharder. while i use an objectoriented window system, i cannot fully grasp the virtues of objectorientedprogramming extolled by my colleagues. they seem to see structure in a set of classes that i cannot, and i'malways looking for invariants that hold the system together that they don't miss. inheriting from all around makesme nervous, but is supposedly trustworthy. my paradigm involves expressing relations, many of which may becomputed in various inputoutput combinations and from sets of constraintsšwriting a single deterministicfunction seems so impoverished. i see computation as just a special, well controlled case of inferencing.evaluating a program symbolically through successive binding of variables to concrete expressions is the mostnatural thing in my world, but that's not objectoriented.while these are seemingly program level complaints, they begin to illustrate why research on softwareproblems may have trouble reaching the scale neededšsoftware support systems require both of these paradigmsand more. and so do large complex applications. single paradigms are the drivers of much modern computerscience, and the source of much enthusiasm and talent in the work force. the lack of a unifying philosophy (whento use each) and of good techniques for combining useful paradigms for a particular problem (rather than beingshoved into using only one for everything) is one challenge for computer science. it won't solve the totalcomplexity problem but it could smooth out much of the internal complexity difficulties (with appropriate analysiscapabilities).second, there isn't yet a baseline of knowledge possessed by either software researchers or system builders.my experience with the wang institute faculty impressed me that such a baseline does exist. this baseline is a wayof describing the bulk of what people should have read (classic articles,...), practiced (design reviews, using aconfiguration management system, thoroughly testing a program), explored (various distinctive methods such asjackson design and vienna development method), learned the theory of (grammars, logic, fsm), and witnessed(failure to understand requirements, value of a prototype). the baseline is a shared body of knowledge so that youcan explain to someone "it's like a with some parts of b". the baseline is broad, covering different views, withappreciation of the differences and complementarity of those views. the baseline provides an individual filterwhere some people gravitate to one or another point of view, not from lack of exposure to others but consciouslyfrom some individual preference or choice of problems to be solved. this baseline is represented in my mind bythe wang institute curriculum, not only the knowledge covered but the skills practiced in projects.national problemssoftware engineering education and trainingthere certainly is need for a dramatic increase in trained software engineers. unless this workshop findsradical new ways of producing complex software systems, there's still a large body of knowledge to be learned andthe better and sooner it's learned by more people, the better the situation will be. a target of 10,000 (cumulative)trained software engineers at the level of the wang institute curriculum is a suitable goal for 1995. theappendix b54scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.software engineering institute and other internal industrial curricula may be able to match that, with enoughresources and cooperation. a major problem is how to fit this type of education into the standard academicstructurešcomputer science departments are refusing to expand and many wouldn't do a good job of it anyway,independent professional schools can't withstand today's short term views, and rarely will companies give up theyear required of their workers. yet having this trained corps with a known baseline might make it possible toundertake building a complex system with a realistic view of what could be accomplished and what else wasneeded in the way of training, tools, and management.more computational support for software productionsoftware engineering practice is exceedingly noncomputational, despite the abundance of cycles on thenetworked workstations where researchers type their programs in emacs, read their mail and boards, occasionallycompile, spend hours debugging, flip off some test cases to see if the system holds together, and fool around withnew interactive interfaces. despite the occasional beneficial fallouts, this is not the path toward managing complexsystems. complex systems will never expose their complexity to graphical browsing unless there's a theory ofsystem structure, good testing is enormously laborintensive and unreliable, performance modeling is a lost art andmonitoring tools seem to be getting worse rather than improving, safety and security analyses are black arts.overall, we have almost no control over the qualities of the systems we build, let alone prediction of thosequalities as the systems are being built.what is needed is a massive infusion of analysis capability into routine software engineering. by analysis, imean the ability to evaluate and predict quality of a system as it is being developed. great strides have been madein formal methods, both the pragmatic nonmechanized specificationrefinement methods pursued in europe andthe mechanical theorem proving methods being applied to security applications and hardwareup vertical proofs.yet the primary symbolic computing capability is still lacking for using these techniques for versatile analysis, andthe same is true for other qualities. similarity extraction and merging of changes are just starting to develop asprogramming theory. systems that learn from usage patterns are on the brink, based on neural net and othermachine learning techniques. graphics provides an incentive but awaits a theory of what to display. these areasshare common problems: promising but underdeveloped theory and high performance computing demands.what's holding us back? why can't systems be thoroughly analyzed? i see three main inhibitors. first, therejust aren't very many good, well known "equations" around to compute with. we need equations for changepropagation, for similarity recognition, for more general symbolic forms of correctness statements, for timing, formultilayer system structure. the equations of software are more often expressed as algorithms, than the equationsof physics. but much the same is needed for softwarešequations that can be studied for accuracy, improved forefficiency, manipulated symbolically, approximated, solved for components, etc. i believe such equations existšan example is the clean presentation of the "implements" relation in an abstract data type theory from whichverification conditions can be generated and mechanically proved and which constrain the tobedefinedimplementations of abstract operations.second, current software engineers are hung up with interactive interfaces, and may have forgottenšor neverlearnedšhow to manage large computations. the proof of correct implementation of a toplevel of amicroprocessor may take 50 hours of computer time, which seems enormous, and six months of grueling work,which seems like a lifetime for a researcher. but these proofs are mainly just symbolic evaluation of the two levelsand might be amenable to noninteractive computation. and they can replace enormous amounts of testing, whilealso improving the semantic presentations (manuals) and subsequent use of the processors. simulation of complexfinite state machine models or specifications is possible, but not widely practiced. many errors found duringdebugging could be caught by extensive static analysis, e.g., checking degenerative cases. similarity explorationfor reuse possibilities should be routine, thoughappendix b55scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.algorithmically intensive. we need supergreps, semantic diffs, and more lints (in unix parlance) plusmathematics for symbolic logic.third, there hasn't been any incentive for researchers to change. the discipline barriers are very strongšperformance people and correctness people just don't talk much. the u.s. cultural mistrust of mathematics andinfatuation with ai and heuristics works against a broad based change. the great advances in parallel processinghave yet to be applied to software problemsšwhere would one start? yet, we certainly don't lack for computingpower to start the transition.proposal: national networkbased analysis gamegiven the cultural impasse and yet the availability of computing power, is there any way to revamp softwareengineering into a computational enterprise? yes, through our national and international network culture.usenet/arpanet/csnet offers news groups covering all the necessary topics and unites researchers withpractitioners in ways that cross cultural lines. networks are becoming the place where people go to learn, oftenprivately, what's going on in other fieldsšthey are a source of continuing onthejob, as well as preprofessional,learning. networks also serve to spread ideas and generate enthusiasm. i'd like to propose a kind of national gameplayed over networks. suppose the objective were, say in 3œ5 years, to have a totally predictable software system.you'd be able to ask any question about it (number of structures, uses hierarchy, state invariant, response time to aparticular event,...) and get a reasonable answer in qualitative if not quantitative terms. you'd be able to makenontrivial changes and trace their effects, not by running or by reading code but by reevaluating equations thatproduce different results. given a pattern of usage, you'd be able to predict throughput, or given a machine timingcharacteristic, you could predict the length of run for a given input sequence. you'd know the flaws of the systemand their exact effects.i propose a network based experiment on a given system, not necessarily one very complex, but challengingto attain a high level of predictability. a familiar commercial system such as a hospitality system or an automatedteller would be candidates. or the experiment might take a kludged up problem most people would understand, saymonopoly or some other board game, "played" in a way where the challenges of complex systems arise. forexample, the state of the system might be replicated, plays might be linked to the failures of the banking and s&lsystems or to the stock market,.... naturally, the experimental system would have interesting interfaces andquerying capabilities. the real game would be the technical aspects of analyzing the system, where theexperimental program was widely available and standardized, although private versions might exist. the main ruleof the game would be to apply mathematical techniques, not just code reading and running. the first objectivemight be complete verification and/or testing to extract the logical structure of the problem and exhibit minimaltest coverage. timing might be introduced by predicting the time of a run varying the statistical characteristics ofthe state (how many data it keeps track of) and the consideration of alternative implementation timings. a secondstage might introduce changes to a standard version requiring the players to adapt the system where the rewardswere based on analytic rather than traditional approaches. later as the program became very well understood,there might be optimization techniques appliedšwhat's the best data structure?while such a national challenge game might seem frivolous, there's the potential to change the softwareengineering culture. the hackers and the professors would be competing together or against each other. there'd beopportunity to learn about alien fields, e.g., how do those performance models work? the inertia of the researchsystems might yield a bit to curiosity and the challenge of really understanding this program. enough results andnew research might be proposed or transferred from other fields. the national game has the feeling for what needsto be done: discover and learn how to use analytic methods for software, open up the possibilities for largescalecomputation applied to software systems, and progress from evaluating equations to solving for unknown resultsthen to optimizing.appendix b56scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.barry m. horowitzthe worst problem in current software production as i view it from mitre's vantage point is that bothindustry and government do not take management actions which recognize the industry's inability to efficientlydevelop large software packages. my suggestions beyond the obvious one of not undertaking jobs which are toolarge (e.g., subdividing the job over time), relate to exploring different approaches to development (e.g., very small team size, super quality team members, super toolsand super wages, and different methods of customer/developer interaction during development to minimize the high degree ofinefficiency currently in that process.the most critical problems faced by industry and the nation vary with the generic type of system underdevelopment. the attached chart (figure 1) represents my view on the problems as they affect different systems.my view on solutions is that each element on the lists must be treated, so no easytodescribe solution exists. overthe past two or three years mitre has helped its clients to address some of the items.i believe that too little attention is paid to collecting and analyzing industry wide data which would helpcompanies to direct and focus their efforts in improving their software development capabilities. for example,what is the best mix of tools, people and management procedures for a given job? what is the best planningschedule to achieve maximum productivity? when are waterfall models of development appropriate; when arespirals more appropriate? today, relative to hardware development, the software industry lacks a proven basis fortaking confident management actions on software production efforts.how can a set of industrial data be collected and organized for use? first, such data must come from largescale production efforts where the industrial problems exist. it would be desirable to provide access to this data touniversities, so that analyses can be done from a vantage point of good knowledge of stateoftheart computerscience. however, industry must be coupled into such analysis efforts in order to assure a practical orientation tothe results. as a result, it would seem that a joint industryuniversity data collection and analysis network wouldbe desirable. issues of proprietary data would arise. perhaps, a governmentsponsored network limited togovernment software development programs could overcome this problem.i recommend the pursuit of a joint university/government/industry venture to better understand themanagement of production of large scale software packages. the effort should focus on building a data base whichwould be the basis for analysis of alternate concepts for production. definition of the data base and instrumentingongoing production efforts for collecting the data would be the initial efforts.sensor or communication systemscommand and control information systemskill baseschedule driven inefficiencyindustry management experiencerequirements (size of job)capital investment limitedknowledge of offtheshelf productsschedule driven inefficiencyindustry management experiencegovernment management experiencegovernment management experiencerequirements (size of job)skill baseknowledge of offtheshelf productscapital investment limitedfigure 1seven software development problems in order of importance.source: mitre.appendix b57scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.bruce b. johnsonbecause of the size and the nature of andersen consulting's business, many of the software problems of theindustry and the nation are directly reflected in our own practice. i have therefore used the two position statementopportunities to discuss two separate problems which affect the industry and andersen consulting alike.the problem of designaccording to simon, design is the process of relating goals, artifact characteristics, and environment. there isbroad agreement within andersen consulting that design is a major problem area. but what exactly does thismean? the problem has many facets.developing and retaining design skills is one facet of the problem. many of today's most experienced systemsdesigners grew up with the computing industry. they learned concepts incrementally as the industry became moresophisticated. today's new entrant is faced with a mountain of complexity, often in the context of very large scalesystems. how does one develop broadly based skills in this environment? the typical system development lifecycle is so long that only a few people will have the opportunity to learn by repetition during a typical career.still another facet of the design problem is the development of conceptual skills. many systems simplyautomate manual procedures. often that is adequate. but for decision making systems, abstraction is essential. weneed much better understanding of the abstraction process in order to develop conceptual skills and to devise toolswhich will assist with appropriate abstraction. in the absence of such tools and skills, designers fall back uponknown designs whether or not appropriate.under today's design processes, we have limited ability to evaluate a design until late in the developmentprocess. better, earlier evaluation will not only produce better designs, but also develop better designers.underlying all of these issues is the lack of a clear understanding of systems and design as a process. withoutclear understanding of design, developing and teaching skills and developing better tools will continue to behappenstance.the problems of reuse and integrationin today's generation of large scale systems, we have two primary means of integrating systems components:1) loose integration by passing information through well defined interfaces and 2) design integration throughsharing a common data base. the former is often unsatisfying because it erects "artificial" barriers to informationintegration and creates user frustration. the latter can lead to a complete redesign of an existing system each time anew functional component is to be added.our strongest answer to this problem today is to follow sound database design principles. yet anyexperienced software product developer understands that a major redesign every few years is unavoidable.nowhere is this seen more clearly than in software development tools. because the abstract representationsrequired to describe software specifications and designs are poorly understood today, each new tool developed islikely to have its own unique representation. integrating the new with the current or integrating tools from two ormore sources usually requires a complete redesign of both tools.the industry needs principled strategies for design and integration which rise above the level of industrystandards and competitive product strategies and appeal to the minds of system developers on first principles.object bases are probably moving in the right direction, but do not go far enough in defining design principles orin providing adequate performance.(i am not advocating that strategies for integration at the code or object level are the only means to achievereuse. indeed, codelevel reuse has many inherent limitations but clearly has a place in the world.)appendix b58scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.anita jonesmy worst software production problem is the same as that of most folks: to produce a new software artifact, istart typically with no particularly relevant building blocks, woefully general programming tools, no application specific software tools, and general, therefore weak, programming methodologies.the software construction process, as well as the later maintenance process, is one of dealing with the detailof programming, not with the nuances of my application. the focus should, of course, be on the application, ratherthan the raw material to implement it. the resulting costs that result from misdirected focus, both the opportunitycost and expended time and funds, are exceedingly high.but there are exceptions to this situation. they tend to come from the personal computer world in whicheconomics dictate that the unsophisticated user absolutely must be able to build his application without recourse toexpensive and scarce programming experts. the muchcited spreadsheets are one example. documentationpreparation and desktop publishing likewise qualify.each incorporates a set of assumptions about a problem domain and strategies for expressing information inthat domain. these tools are not general; they impose quite limiting restrictions. specificity and restrictionsšandthe set of assumptions about context as well as domainrelated, highleverage ''operators''šgo hand in hand withpermitting the user to express information in terms of the application, rather than in terms of writing a program.this class of tools has not been fully exploited, particularly by the computer science community. this representsan opportunity.appendix b59scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.harlan d. millsbenefits of rigorous methods of software engineering in dod software acquisitionsthe software quality and reliability problem faced by dod today is due primarily to the failure of contractsoftware management and technical personnel to use rigorous methods of software engineering. what is possibleand practical with rigorous methods is low or zero defect software to meet precise specifications. getting precisespecifications right is much more difficult, and can be open ended as requirements change. so rigorous methodsdon't solve everything, but they do bring the software defects to as low levels as desirable, including zero, in orderto concentrate on precise specifications for what is needed.the only effective way known to enforce the use of rigorous methods in software engineering is to prohibitprogram debugging before independent system testing and to require mathematical program verification in groupinspections in place of debugging. such independent system testing can be carried out in statistical designs toprovide scientific reliability projections for the software developed.software engineering and computer science are new subjects, only a human generation old. for example,when civil engineering was a human generation old the right triangle was yet to be discovered. but the explosionof an entire industry without time for orderly scientific or engineering development has led to deep problems forsociety, of especial importance for dod. during this industrial explosion, the initial methods of programmingwere necessarily heuristic and error prone. with that initial experience, errors have come to be expected insoftware development. since then, rigorous and error rare methods have been discovered in computer science, butare little used in industry from lack of understanding and precedent.as long as heuristic and error prone methods are considered acceptable by dod, why should they bechanged? as a result of this understanding, the typical state of practice in software development and maintenanceby dod contractors is far behind the state of art. this conclusion was reached by two major dod studies in recentyears, a dsb study headed by fred brooks (military software) and an sdi study headed by danny cohen (theeastport report). caz zraket voiced a similar conclusion as the keynote speaker at the sds conference inhuntsville, fall of 1988.a similar problem in a simpler form occurred in typewriting nearly a hundred years ago, also just a humangeneration after the introduction of the typewriter. touch typing was discovered as a more productive and reliableway of typing, as counter intuitive as not looking at the keys while touch typing was. but how many experiencedhunt and peckers of the day changed to touch typing? it was a small minority, indeed. instead, it was new youngpeople entering the work force over several years that finally brought touch typing into common use.unfortunately, it will be the same in software, but with even more difficulty because of the industrialexplosion of computers and software in dod contract work. no more than a minority of the first generationheuristic try and fixers in software today will ever learn and change to rigorous methods. it must be young peoplewith sound software engineering education entering the work force that will bring rigorous methods into industrialuse.but even here there is a considerable hurdle and deterrent when software managers have only heuristicexperience. it's not so likely that managers of typing pools would have demanded that new people "look at thekeys while you type, like i do." but software managers with only heuristic experience and large projectresponsibilities are likely to direct that "since we'll have a lot of debugging to do, i want you to start coding rightaway", and continue to be right with a serf fulfilling prophecy, while unwittingly preventing the use of rigorousmethods.as noted, there is a simple way to hasten the state of contract practice toward the state of art, that soundsmore drastic at first hearing than it really isšprohibit program debugging in contract software development, andrequire the use of rigorous methods of mathematical program verification with group inspections in place ofdebugging. by all means subject software to systemappendix b60scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.testing by independent teams against the precise specifications of user requirements before acceptance andrelease. indeed, conduct the system testing within a statistical design to permit scientific reliability projections ofthe software.program debugging is the counter productive sacred cow of this first human generation of softwaredevelopment. more time is spent in debugging programs today than in writing them. private debugging hidesmany marginal people in software development. it is false economics to "let the machines find the errors" insoftware development, even though machines are getting cheaper and people dearer. machines can find errors ofsyntax, but cannot find the errors of logic that finally persist in software developed by first generation heuristic,error prone methods.doing without program debugging is surprisingly easy for people who know how to verify programsmathematically. people still make errors of mathematical fallibility, but such errors have been found to be mucheasier to discover by testing than are errors of debugging. caz zraket mentioned in his huntsville talk thatsoftware today, after all the debugging and testing, will typically have 1œ3 errors/kloc. but mathematicalverification can provide software for statistical testing that start at 1œ3 errors/kloc, not end there, and go wellbelow zraket's goal of 0.1œ0.3 errors/kloc, with statistical testing that produces scientific reliabilitycertifications. errors of mathematical fallibility are much easier to find and fix than are errors of debugging.another sacred cow of this first human generation of software engineering is coverage testing. it is wellknown that coverage testing can reach 100% with many errors still remaining in the code. but it is not so wellknown that fixing errors found in usage representative statistical testing can be 30 times as effective in increasingtimes between failures than fixing errors from coverage testing. an appendix gives the evidence.quality must be designed, not tested, into software products. it can be no better than the qualifications of thesoftware engineers that produce it. in this first generation, people become software engineers in three day courseswithout examinations. it must become a subject of a four year university curriculum with many examinations.universities are part way there. many computer science curricula have courses in software engineering, much as aphysics curriculum might have a course in electrical engineering. but software engineering needs a curriculum,not simply a course, just as electrical engineering has today. moving from debugging to no debugging beforeindependent testing is a necessity for getting out of today's software morass, and will help define a curriculum, notjust a course, for society and the industry.the clean room software engineering process develops software with no program debugging beforestatistical system testing. it has been used to produce commercial software products, and a version of the hh60helicopter flight control program. it is described and discussed in the following papers:1. p. a. currit, m. dyer and h. d. mills, "certifying the reliability of software", ieee transactions on software engineering, vol. se12,no. 1, january 1986.2. r. c. linger and h. d. mills, "a case study in clean room software engineering: the ibm cobol structuring facility", proceedings ofieee compsac 88, october 1988.3. h. d. mills, m. dyer and r. c. linger, "clean room software engineering", ieee software, september 1987.appendix new understandings in software testing the power of usage testing over coverage testing*the insights and data of ed adams** in the analysis of software testing, and the differences* harlan d. mills, information systems institute, university of florida.** adams, e. n., minimizing cost impact of software defects, ibm research division, report rc 8228, 1980.appendix b61scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.between software errors and failures, give entirely new understandings in software testing. since adams hasdiscovered an amazingly wide spectrum in failure rates for software errors, it is no longer sensible to treat errors ashomogeneous objects to find and fix. finding and fixing errors with high failure rates produces much more reliablesoftware than finding and fixing just any errors, which may have average or low failure rates.table 1 distributions of errors (in %) among mean time to failure (mttf) classesmttf in k monthsproduct601961.90.60.190.060.019134.228.817.810.35.02.11.20.7234.228.018.29.74.53.21.50.7333.728.518.08.76.52.81.40.4434.228.518.711.94.42.00.30.1534.228.518.49.44.42.91.40.7632.028.220.111.55.02.10.80.3734.028.518.59.94.52.71.40.6831.927.118.411.16.52.71.41.1931.227.620.412.85.61.90.50.0the major surprise in adams' data is the relative power of finding and fixing errors in usage testing overcoverage testing, a factor of 30 in reducing failure rates. an analysis of adams' data shows that finding and fixingan error causing a failure in representative usage testing increases the mean time to the next failure 30 times asmuch as finding and fixing an error in coverage testing. that factor of 30 seems incredible until the facts areworked out from adams' data. but it explains many anecdotes about experiences in testing. in one suchexperience, an operating system development group, using coverage testing in a major revision, was finding meantime to abends in seconds for weeks. it reluctantly allowed users tapes in one weekend, and on fixing those errors,found the mean time to abends jumped literally from seconds to minutes.the adams data are given in table 1. they describe distributions of failure rates in 9 major ibm softwareproducts. the 9 ibm products include operating systems, programming language compilers, and data basesystems. the uniformity of the failure rate distributions across these different kinds of products is truly amazing.what adams found was that errors in the 9 major ibm software products had very similar distributions ofmean times to next failure (mttf) spread over 4 orders of magnitude from 19 months to 5000 years (60 kmonths), with about a third of the errors having an mttf of 5000 years, and about 1% having an mttf of 19months.with such a spread in the mttf spread, it is easy to see, right off, that coverage testing will find the very lowfailure rate errors a third of the time with practically no effect on the mttf from the fix, whereas usage testingwill find many more of the high failure rate errors with much greater effect. as noted, the numbers work out thatthe usage found fix will increase the mttf 30 times as much, on the average, as the coverage found fix. with thatkind of factor, the whole economics of testing is affected. it isn't enough simply to minimize the cost of findingerrors. errors found in usage testing are worth 30 times as much as errors found in coverage testing, so the kind oferrors found needs to be factored into the economics.table 2 develops the data that show the relative effectiveness of a fix in usage testing and coverage testing inincreasing the mttf in each mttf class. table 2 develops the change in failure rates for a fix in each class,became it is the failure rates of the classes that add up to the failure rate of the product.first, line 1, denoted m, is repeated from table 1, namely the mean time between failures ofappendix b62scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.the mttf class. next, line 2, denoted ed (error density), is the average of the densities of the 9 products of table 1,column by column, namely a typical software product. line 3, denoted ed/m, is the contribution of each class, onthe average, in reducing the failure rate by fixing the next error found by coverage testing (i/m is the failure rateof the class, ed the probability a member of the class will be found next in coverage testing, so their product,ed/m, is the expected reduction in the total failure rate from that class). now ed/m is also proportional to theusage failure rate in each class, since failures of that rate will be distributed by just that amount. therefore, thisline 3 is normalized to add to 100% in line 4, denoted fd (failure density). it is interesting to observe that errordensity ed and failure density fd are almost reverse distributions, error density about a third at the high end ofmttfs and failure density about a third at the low end of mttfs. finally, line 5, denoted fd/m, is thecontribution of each class, on the average, in reducing the failure rate by fixing the next error found by usagetesting.table 2 error densities and failure densities in the mttf classesm601961.90.60.190.060.019ed33.228.218.710.65.22.51.10.5ed/m0.61.53.15.68.713.318.326.3fd0.82.03.97.311.117.123.634.2fd/m001418903931800the sums of the two lines ed/m and fd/m turn out to be proportional to the decrease in failure rate from therespective fixes of errors found by coverage testing and usage testing. their sums are 77.3 and 2306, with a ratioof 30 between them. that is the basis for the statement of their relative worths in increasing mtte it seemsincredible, but that is the number!to see that in more detail, consider, first, the relative decreases in failure rate r in the two cases:fix next error from coverage testing r ® r  (sum of ed/m values)/(errors remaining) = r  77.3/efix next error from usage testing r ® r  (`sum of fd/m values)/(errors remaining) = r  2306/esecond, the increase in mttf in each case will be1/(r  77.3/e)  (1/r) = 77.3/(r(er  77.3))and1/(r  2306/e)  (l/r) = 2306/(r(er  2306))in these expressions, the numerator values 77.3 and 2306 dominate, and the denominators are nearly equalwhen er is much larger than either 77.3 or 2306 (either 77.3/er or 2306/er is the fraction of r reduced by thenext fix and is supposed to be small in this analysis). as noted above, the ratio of these numerators is 30 to 1, infavor of the fix with usage testing.appendix b63scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.john b. munsonin 1983, i was chairman of an air force scientific advisory board study into the same issues. i must observethat virtually none of the problems we identified and made recommendations for have been solved five years later.in addition, i will address your two questions at a "top" or generic level.worst problems with current software production?obviously a broad, multifaceted issue, let me pick two that are decisive today. they occur at each end of thedevelopment template and are the areas least researched, understood or supported by tools. they are:1. the inability to specify adequate software requirements and the unavailability of an adequatemethodology to compensate for this deficiency.in light of the "80/20" concept these appear to be the areas of greatest return on investment andcertainly the root cause of problems in other development phases. it has been proven that when wetruly understand the "problem" to be solved and the technology to be used in the solution, an ontime,within cost, high quality product will result (the issue of "productivity", e.g., doing it faster andcheaper is moot at this pointšour first challenge is to do it predictably). it is obvious to me that oneof the major dilemmas is that while system decomposition from operational conception to programcode is basically a continuous process, we have no tools or techniques (methodologies) to deal with itin an integrated continuous fashion. i believe the discontinuity we employ in our processes todaytotally destroys our ability to understand and control decomposition in any meaningful effective way.2. the inability to define a cost effective, controllable, comprehensive validation testing program forfinished software systems.we need a true pragmatic test engineering discipline. today it is a truism that we test until out ofmoney and time; not because we've achieved results. how do we engineer and plan (resources andtime) for the most effective test program? it has to be a science but, as in many things, the gullybetween stateoftheart and stateofthepractice is from ten years to infinity (we'll never get there).as in item 1 above, the synthesis process (code to validated, integrated system) is also continuous;yet again we deal with it using discontinuous techniques.most critical industrial/national issueall problems pale against the issue of chasing the leading edge of technology. our biggest problems areassociated with fielding systems at or near the stateoftheart in basically hardware technology. we do notanticipate the future capability in our development of software methodologies. for instance, our stateofthepractice can finally handle serial, monolithic systems quite well but our challenge is distributed, parallel,asynchronous applications for which we have little or no practical engineering principles. ada is a perfect exampleof this phenomenon. ada was perfect to solve the systems problems of the '70's and totally inadequate for thestateoftheart systems of the '90's.the basic issue is how do we identify and get research far enough in front of our most important issues to beeffective and also how do we reduce the gap between software stateoftheart and stateofthepractice.other issuesi have another pet "hobby horse" i would like to explore if the committee feels it's important. we have atremendous effort and investment going on in trying to develop software developmentappendix b64scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved."tool sets". yet, we have no concept of how this plethora of incompatible tools is going to be eventuallystandardized. it appears to me all we are doing at this point is raising the cost of doing business. today, with littleor no tooling, our only "overhead" cost is learning different programming languages ša relatively modestexpense. if tomorrow we standardize on the ada language but have a proliferation of tool sets (training on a fulltool set is far more expensive than just a language) we have significantly increased the "overhead" cost. obviouslythe maintenance costs for software systems will likewise sky rocket instead of reducing.further, there has been little or no thought towards how we keep the evolution and improvement of our toolsupward compatible, if it's even possible. so not only will we have differing tool sets, we will also have to maintainvarious versions of the same generic tool set for the life span of the software built with a specific version.i don't have an answer but i predict this frenzy of spending on developing tools will be a total waste and notthe proclaimed panacea until we deal with these issues.appendix b65scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.douglas t. rossunderstanding: the key to software˝#$%˛'()*+ conclusionplex22˙˘˘understanding the key to softwareunderstanding: the key to softwarekey,understanding:isunderstanding understanding2we must understand our understanding of the nature of nature.ˆˆ˙˚˙˘$<<&#02@a**scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.nature itself seems hard enough to understand, for it has a habit of overturning each successive and, up to then,successful theory or science. our understanding is itself a participant in nature, and certainly one of its leastunderstood aspects. why, then, set that as the primary goal? it would indeed he presumptuous and foolhardy toapproach the matter biologically, attempting to study brain and mind as a scientific exercise. but there is another pathopen š one that, when it is pursued, shows surprising promise and progress relative to the effort spent thus far.in this brief essay i sketch the opening steps along this path, in the hope that others will join in the exploration. theprimary style of approach is not to make a frontal (prefrontal?) attack on our understanding of understanding, butrather to assume, until forced to think otherwise, that the fundamental nature of nature must be simplicity itself šthat the rich complexity that is so apparent is an artifact of sheer magnitudes. the known measurements of physicsshow that there are roughly the same number of powers of ten above us (to the cosmic reach) as there are below us(to the depth of subatomic particles). we (i.e. man and his sensory world) are in the middle of a vast scale ofcomplexity. we will assume that that complexity is merely fantastically profligate simplicity. we will assume, untilshown otherwise, that if there be a "law of nature", that there is just one law, and that it operates intact and in totothroughout this vast scale. we seek to understand our understanding of that law, and if the law is to be simplicityitself, then so also must be our understanding.we must take nothing for granted. and i mean that exactly and literally. we must and do take the nonentity as ourstarting place. we adopt a posture of aggressive humility, lower our head, and step off along the path starting fromnothing at all. in no way intending to play god, and always open to changing our stance and direction when forcedto, nonetheless if simplicity it is to be š then there is nothing simpler than nothing. so that is where we start then, ifwe are indeed careful with our reasoning at each step, so that we truly do understand our understanding in toto, thenwhenever we encounter some aspect of the nature of nature that goes counter to that understanding, we can retraceour steps and know exactly what must be altered to proceed.that was issued april, 1976, referencing "some views, tested and still evolving over a twentyyear period",even then.in this current essay, i have done my best to present my most recent view of the opportunity of plex, in thehope that it can become a proper pan of the agenda for the future. it is a completely rigorous scientific philosophy,by now š a claim that i present in enough detail, with examples, to allow you to judge for yourself š and i feelan intolerable burden of responsibility to still be the only person in the world (to my knowledge) pursuing it, inspite of my efforts to enlist others even as responsive readers. i expect that the workshop organizers, as well asthe participants, also will not know what to make of my current effort, either, and will opt not to invest the timeand effort to respond. but i hope at least that the editors of the final report will include my essay as a "minorityreport" appendix, whether or not it even is referenced in the report, proper, so that some reader someplace mightaccept the challenge to join me in this work, which i think is very important.as to the seniormanager decisionmakers: the opportunity is real, sound, productive, and ready for vigorousgrowth š but only if the invitation i extend to the technical community can ultimately lead to a group project ofsome sort. realizing that any possible collaborators likely would be scattered worldwide, i went to the workshopwith the singleminded goal: to help to define a cohesive blending of our existing worldwide communicationnetworks with colorgraphics hypertext advances so that a collegial community of researchers and scholars caninteractively pursue joint endeavors on any topic of interest, including plex. i proposed that the resulting system benamed "the worknet", and that the word "worknet" have verb, adjective, and noun meanings, as in"all people, not just researchers and scholars,can either work alone on their workstationto create individual solutions,or can worknet with otherson worknet solutionsas active reader/author members of worknetsto benefit from the evolution of team understanding."the point is that it is not the network that is important, nor even the users š it is their collaborative workthat matters. for every knowledge worker, a worknet bustling with selfrenewing activity because it helps everyparticipant to grow, understand, contribute, and work better is just what is needed.there are many unfortunate technical obstacles to the realization of a truly universal worknet, but they can besolved, and should be š quickly. the worknet architecture must be completely open, so that each workgroup cansupply or procure language features best suited to their topic, without compromise. each group will need bothdocument language and comment language (perhaps the same) to be able to probe the meanings of documents,interactively through the network. the single universal design criterion that must not be violated is thatappendix b67scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.comments on a document do not affect the form or meaning of the document in any way.with this distinction rigorously supported, the meaning of reader comments on form or meaning or both canreliably be incorporated into a revision of the document by its author š for further circulation for commentary.every reader also is an author; every comment is a document; and with meaning thus guaranteed to be preserved,true team understanding can be evolved and expressed for any subject of interest.the important feature not yet adequately supported (although it could be, with careful design and use ofcolor) is to graphically portray the referntial meaning that is to be common between an arbitrary commentlanguage expression and an arbitrary documentlanguage expression to which it refers š with minimum disruptionto either document š for understanding often derives from subtle features of the form or "look" of an expressionin a language. e.g. ''''' quotes '' in a quote quoted by ''' illustrates the convention that doubling a character quotes itin a quotation quoted by it š for which (perversely) the commentary language is the dearer expression! [noticethat if ''' and '' are taken to be single characters, themselves, then the example says '' selfquotes in a '''quotation,which yields the mind'seye reading that ' selfšquotes in a ''quotation. the natural degeneration is that selfquotes in a 'quotation š which is the convention of the illustration, itself! in plex, words and characters are thesame, both beingquotations which can quote. this aside is a typical plex observation. many of our standardconventions deepen, in plex.]plexthere is a rigorous science, just waiting to be recognized and developed, which encompasses the whole of "thesoftware problem", as defined, including the hardware, software, languages, devices, logic, data, knowledge, users,uses, and effectiveness, etc. for endusers, providers, enablers, commissioners, and sponsors, alike.all of us in the research and production communities would hope such a science might be possible, and weget glimmers, now and then, that some powerful order underlies the common patterns we sense at work in oursuccessful efforts (and in the broken pieces of our failures).it was from some twenty years of study of such patterns in my own work and the work of others, that ibegan, about ten years ago, to develop modeling methods that could capture such generality. the insights camefrom my successful work in language theory, software engineering, problem structuring, structured analysis, systemdesign, and even business. since 1959 1 have called my philosophical study of the patterns of structure "plex" (toplait or weave). to my surprise, about four years ago i discovered that the reason my methods worked so well wasthat, although i hadn't realized it, i was practicing precisely the standard, accepted rigor of formal systems š butbackwards!in the accepted formal system methodology, two steps are involved: first i) a formal language, with rulesof inference (or equivalence or ..., depending on the choice) is established to cover and allow formal treatment ofthe entire intended area of discourse, and then ii) a model is attached to provide an interpretation š establishingthe intended meaning of the language. the key concept is that of providing a valuation for the linguisticexpressions, which is accomplished by providing some form of "satisfies" predicate, linking them to the structuredmodel. with this completion, linking language and model, questions of completeness and consistency can beaddressed š and that is as formal as any formal treatment gest! i do just the opposite. for plex, the model comesfirst, and the language is only and precisely the minimum necessary to express just what shows in the model. asequence of models, each related to the earlier ones, builds up a rich formal language capability and a very deepunderstanding š all supported by the models.the important point is that plex is not merely just as rigorous as the accepted level of rigor for formalsystems; plex is not informal in contrast to these formal systems (in spite of the seeminginformality of the wordplay inherent in plex). the rigor and formality of plex is the accepted formality and rigor š just viewed andcarried out backwards, or in plex terms š opposite but the same!the methodology"see and say" is the name i have given to the methodology of plex. it is related to, but different than "showand tell" which requires merely an appreciative (but passive) audience, who simply sit back and enjoy thepresentation of the performer. the roles are reversed in see and say. the seer is the performer š actively,appendix b68scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.not passively. to begin with, the presenter is passive. the difference is profound: i show, you see, i say, we tell š i.e. i merely show you the model, without comment. only after you have done your seeing (with noguidance or instruction of any sort from me; the shower), do i say what i intended you to see š and then we bothtell each other, back and forth, what we now see in what was presented, until we arrive at a mutual agreementabout what shows and how it is expressed in language.to make the method work, it is necessary that only pictures language models (plms) be formally shown.what is shown always is a picture , first of all [0dimensional, 1dimensional, ... , ndimensional, ...]. but equallyimportant, every tiniest feature or relation that shows is a language element [with meaning of its own, or (likeletters and syllables) contributing to meaning of some larger picturešlanguage construct]. finally and mostimportantly, there is no separate translation into other terms to obtain the meaning that is expressed in thepicture language, for it is what shows š modeled by the picturing, itself. [this is the ultimate in wysiwyg("wizziwig") what you see is what you get, now so popular in desktop publishing!] the definition of model iscrucial and very simple:m models a if m answers questionsabout a.the answers must, of course, be correct, but to the extent that any question about a is answered, to that(limited) extent, m models a. only with both seeing and saying matched so that meaning is modeled is a picture aplm.as to the extent of the questioning that determines the modeling of the model in the see and say methodologyš that is entirely up to the viewer, because of norule seeing:if what you see satisfies the definition of "it" then what you have found is <it>![several types of quotes are used in plex: "it" = name; it = word: <it> = meaning: it = concept; 'it' = sign.]multiple modelings (word and picture puns) are inherent and intentional, by the plex paradigm (from the littleknown alternate meaning of the word: paradigm (n) 1: example, pattern 2: an example of a conjugation ordeclension showing a word in all its inflection forms. [webster's 1961]). because of norule seeing, the scenepresented by any plm is the superposition of all possible meanings, any one of which may be selected for study.modeling anythingthe reason i assert that flex is the science that is needed for "the software problem" is that the firstdefinition of plex isnothing doesn't exist.and the first model of the sequence is blank. with no guidance, you see nothing modeled in the plm. butwhen i add my formal saying to that, your seeing, the full rangenothing ` something ` everything = anythingis modeled. that blank is paradigmatically a model of anything at all!appendix b69scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.notice first of all that the four lines of the and conjunction arise from all possibilities of combining the threedichotomies nothing/everything, relevant/irrelevant, and present/absent within the framework established by thefirst line š which expresses what we desire as that which is to be presented to be shown. "relevance" is the termused for the underlying characteristic which all meaning must enjoy. that which is irrelevant is not worthspeaking about. therefore that first line says that i present to you a plm of all that is worth speaking about. bydefinition, nothing is to be left out.the remaining three lines completely spell that out leaving nothing to chance! the first three lines,conjoined, confirm what i said, and the fourth line confirms that confirmation! there can be no question about it.as long as we stick to only relevant subjects, each is paradigmatically present in the showing. it also should beclear why the clear model, thus defined and shown, is clear! quite simply, it is because no contrast can show!only everything relevant is present, except for that "irrelevant nothing", which (being both irrelevant andnothing) can't possibly contribute to any meaningful seeing. to have a pattern requires a background. in thiscase there is no background, so no pattern can be discerned š no matter what is being modeled. their models alllook precisely the same š the clear model! it shows them all superimposed, all at once š as one!just as with other formal systems when they are put to use, logically consistent (i.e. ruleobeying) derivationsare the order of the day in picture language modeling. once agreement has been reached, the see + say =modeling cannot be retracted from the mind'seye seeing of either party, and it becomes a permanent part of theirshared mindset . this is particularly true for the clear model, for in a very real sense it is the light by which allseeing is done, in plex. each possibility is modeled by its own unique color, and superimposed all together, the white tight of clearness, itself, is seen! for any portion of this spectrum, some completely uniform color is seen.no color can not be seen!this, itself, is a wordonly plm, with paradigmatic punny meanings, left here as an exercise for the reader.like all plms, it arises from the clear model by crucial final steps, which i here will only describe verbally.[elsewhere they are plmed in detail.]the clear model becomes marked by another agreedupon formal saying which uses the idea and notation of <>quoted semantic reference. being an opposite viewing of the clear model, word order is swapped , as well. thetotal epistemology of plex is thatonly that which is known by definition is known š by definition.semantics concerns meaning, and hence relevance. in order to ensure the support of meaning, for thenew saying <irrelevant everything> is banished, by definition, in order to realize absence, and <relevantnothing> is allowed to be present, where it cancels with the <irrelevant nothing>, already present, yielding<nothing> along with the <relevant everything> š which thereby is isolated (but still looks blank, because the<nothing> doesn't show). that is just one of the modeled meanings, however. superimposed with it is anotherseeing (for the same saying) in which contrast can and does show, for with both <relevant everything> and<relevant nothing> present, <relevant something> is exactly defined [a plex technical term] between them!this is the mark, which is nonnothing and nonblank. notice that the <irrelevant nothing> also still is present inthis seeing, so the marked model shows only relevance (as all except the banished <irrelevant everything>) in abackground sea of irrelevance! š which doesn't show because it is nothing! in fact, because the <relevantnothing> that is present (defining the mark) also is nothing, this is the same isolation of <relevant everything>,made visible by the mark! that is what is different about the scene that is presented to be seen.what i have described is what i have (plm)shown to you for your mind'seye seeing. you know that thatmark is there to be seen, even though you cannot yet physically see it, in spite of its known visibility . the reasonthe marked model can only be known (but still cannot be seen) is that it is the superposition of the <isolated<relevant everything>> seeing with the <sea of irrelevance <relevant everything>> seeing š and they contradicteach other, because they compete over the <relevant nothing> whose presence arose from the relevanceonlymindset established by definition. if it is absorbed into <nothing>for the isolation, <relevant nothing> cannotexactly define the mark, and vice versa. this is why the marked model must be seen as it is known to be š asuperposition (denoted in plex by "with") rather than a composition (denoted by "and"). only in the clear model(with its fourway conjunctive completeness) do superposition and composition coincide. the marked modelmindset destroys the composition possibility, leaving only the superposition.so far only the showing that precedes the seeing of see and say has been plmed. to realize (pun!) theseeing, requires a further change of mindset, making use of your knowledge, thereby making real theappendix b70scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.meaning of all three relevants that are known to be present as nonbanished. when this is done, what you actuallywill see will be the model, itselfšnot clear, not marked, but (by definition) ready for your seeandsay seeing tosimilarly be plmed.just as the marked model was an opposite seeing of the clear model, this new model is an opposite seeingof the marked model. by a definitiononly change of mindset, but no change in the picturing of the plm, the twoseeings of the marked model are composed, contrary to the old mindset. the interesting consequence is that allmodeled meaning is forced out of the model into the mind of the seer! š for the model has only the capacity tobe seen. [its beauty is (only) in the (mind's)eye of the beholder!] while the <relevant everything> stays fixed, atriple swap of meanings establishes the new mindset, as follows:<irrelevant nothing> ® <relevant nothing> ® <relevant something> ® <mindset>so that the mindset now supplies the <relevant something>, while the model is the<irrelevant nothing> + <relevant nothing> boundary> + <relevant everything>composition of the <relevant nothing> boundary> of <relevant everything > in a sea of <irrelevant nothing>,i.e. the model is a whole having three parts in effect, the markasboundary cleaves (separates and joins) theoriginal <relevant everything> and <irrelevant nothing> parts that were present in the original clear model! soeffectively, the mark has been added to the clear model as insight, by this plming. by definition (of"boundary"), <everything relevant> is inside its <relevant nothing> boundary mark, so we know that <all ofrelevance> is what we see plmed in there. its meaning shows, but what it means must be modeled by moreplming.scope and relevance of plexpursuit of the deep foundations of plex in this fashion leads inexorably to the rigorous plming of language,meaning, and thought, itself, at one pole, through counting (which must be defined) and all of mathematics, to thephysical spacetime reality of the universe, itself, on the other. as an example of the latter, a wordonly plm whichlinks directly to the first definition and uses only ideas we have covered here is the existence plm (see nextpage). regarding the other pole, it can be proved in plex thatmeaning of meaning of meaning cannot be nothing, because of the impossibility of impossibilitywhich, in turn, is a direct consequence of the first definition.many startling insights relevant to our field result, such as this question and answer regarding the informationof information theory, which underlies communication:how many binary digits are required to encode one bit? answer: 3/2 , because the value of the halfbit is 3/4.š which ultimately results from the fact that in actuality, when you don't have something, it is not the casethat you have it but it is nothing š it is that you don't have it; whereas when you do have something, that isbecause you don't have what it isn't! here is the definition for "not it":given <it>, <it>, <not it> š only <not it> is every meaning other than its name."it" is our name for whatever currently is relevant (<all of relevance> is <it>, initially), and a consequence ofthe first definition is thatit can't not beappendix b71scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.for any <it>. when <it> is required in order for <it> to be what <it> is [loosely, when its time has come!] šthere <it> is! time progressing forward in an expanding universe is a consequence. [š literally!, for in plex, boththe root "con" (with) and "sequence" (which is nonnothing only in <reference sequence>, where "reference" issuch that ''it" references <it>) are rigorously defined (as is "to define'', itself)].the reason that the bit has value 3/2, rather than the (perhaps)expected value, 1, is the same reason that thereonly is reference sequence , rather than reference alone or sequence alone. in both cases, reality intrades. the"extra 1/2" value quantifies the contextual coupling of <it> to <it>, when we point and declare "that's it!". such areference, to be actual, must persist in spacetime [actually in what i call "thime" š the foundational level whereplacelike (there) and timelike (time) coincide], and a fact of plex is that there was no beginning, as each <now>similarly is coupled to its <before<now>>. all follows from that first definition: nothing doesn't exist, š theultimate and driving breaking of symmetry.appendix b72scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.appendix: the meaning of any wordlet <point> = <any word>, i.e. let "point" name the meaning of any one word.then the following propositions are to be proved (another result not generally known):p1) let points be such that, except for identity, they all are indistinguishable.p2) let there be only points.p3) let the world be the collection of all points.p4) then the identity of a point is the collection of all other points.p5) and every point is the whole world.note: given p1 œ p3, p4 must begin with "then", and p5 is a consequence (and hence begins "and"). thesepropositions provide the plex explanation of the platonic universals.proof that every point is the whole world. (p5)i n = 1: a world of one point is the whole world.ii assume the theorem true for n1 points. (n > l) i.e. for any collection of n1 points, every point is thewhole world.iii to prove the theorem for n points given its truth for n1 points (n > 1):a) the identity of any one point, p, in the collection is a collection of n1 points, each of which is thewhole world, by ii.b) the identity of any other point, q, i.e. a point of the identity of p, is a collection of n1 points, each ofwhich is the whole world, by ii.c) the identity of p and the identity of q are identical except that where the identity of p has q theidentity of q has p. in any case p is the whole world by b) and q is the whole world by a).d) hence both p and q are the whole world, as are all the other points (if any) in their respectiveidentifies (and shared between them).e) hence all n points are the whole world.iv for n = 2, i is used (via ii) in iiia and iiib, q.e.d.v q.e.d. by natural induction.note: in the fall of 1984, a noncredit graduate seminar on plex was offered in the m.i.t. ee/cs department, butsoon ceased for lack of student interest. that was the first public presentation of the above 1975 proof. becausecounting and the natural numbers do not exist in plex foundations, but must be derived, the preferred proof for plexuses the see and say plm methodology.appendix b73scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.winston roycethe worst problem i must contend with is the inability of the software buyer, user, and builder to write ablueprint which quickly leads to a lowcost, correct product.some suggestions for alleviating the problem are as follows:1. invent a formal language for expressing the buyer, user requirements; the language should beunderstood by the buyer, user and builder; the language should have both a procedural and adeclarative style.2. design exploration should be supported by lowcost, quickturnaround experimentation.3. high reliance on rulebased programming for most code.4. languages, whether for requirements, design, code or documentation, should have sufficient formalismthat provable correctness is possible for eliminating most errors.5. highly automated explanation of the software design, code, and user instructions.6. after the initial build changes to requirements, design, code and documentation are mostly doneautomatically without error.7. ironfisted, certifiable product control where needed.8. select hardware only after the code is completed.industry and the nation believe that software production is best managed in the same way as large,hardwaredominated, high technology aerospace systems. the management technology arose in the mid'50's as aresponse to the need for system synthesis of multitechnology systems. a one year old defense science boardreport aptly termed it a "specify, document, then build" approach.this approach consistently misjudges how difficult software production is and how best to spend projectresources for software.potential solutions include:1. reinvention of the software development process to suit the inherent nature of computer sciencerather than force fitting of software production to a proven hardware method; there is probably morethan one valid process.2. employment of adoptive process models to create needed discipline plus urgency about, andunderstanding of, the development process.3. provide economic models of the development process which help with decision making (i.e., howmuch frontend capital investment is right? what is the affect of scaling up a project in size? of whatvalue is reusability of old code?).4. figure out what sort of economic alliance between u.s. institutions is needed to crack this problemand then implement it.appendix b74scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.mary shawmaybe your next programming language shouldn't be a programming languageabstractsoftware needs now strain the design limits of traditional programming languages. modern application needsare not satisfied by traditional programming languages, which evolved in response to systems programmingneeds. current programming language research focuses on incremental improvements, not on major changes to thenature of software development. but major breakthroughs are needed in two areas: nonprogrammers dominate modem computer use. low computing costs have enabled a wide spectrumof application, and end users who are not programmers need to control their own computations. orderofmagnitude increases in service require substantial shifts of technology. computer users are interestedin results, not in programming; software must reflect this. requirements for large complex software systems exceed our production ability. growth of demand isgreater than growth of capacity, and system requirements exceed the scope of conventional languages.software lacks a true engineering base. language concepts can support a design level above thealgorithm/data structure level and contribute to an engineering discipline.programming language designers must look beyond the traditional systems programming domain and tackleproblems of specialpurpose software and systemlevel software design.software needs now strain the limits of traditional programming languagesmodern application needs are not satisfied by traditional programming languagesas hardware costs have decreased, computers have entered an everincreasing range of applications, butimprovements in hardware costeffectiveness have not been matched in software. computers are now in widespread use in many smallscale settings where it is not possible for each enduser to hire professional programmers. in this market, nonprogrammers need to describe their owncomputations and tailor systems to their own use. computers are often embedded in large complex systems where the major issues are not algorithms, datastructures and functionality, but gross system organization, performance, and integration of independentlydesigned components into reliable systems. this requires design well above the level of statements intraditional programming languages.traditional programming languages were designed by and for systems programmers, and they have notresponded to this broadened range of applications. even if these languages are augmented with facilities forconcurrency, database access, and so forth, they are not onlyšor even the bestšmedium for bringing computersto bear on specialized applications and very large systems. as a result, the enormous capabilities of moderncomputers are not being delivered effectively to the entire community of intended users.both delivery of computation to nonexperts and systemlevel design would benefit from the techniques ofprogramming language design. current programming language research, however, often seems to proceed from animplicit assumption that the only languages of interest are the programming languages of mainstream computerscience. this view ignores the computing needs of an increasingly diverse population.programming languages evolved in response to systems programming needsprogramming languages were developed in the late 1950s and 1960s to provide better notations for specifyingcomputations. their concepts, syntax, semantics, and primitive types supported algorithms and data structures.they were designed to solve the problems confronted by programmers and toappendix b75scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.exploit the available hardware. their specifications emphasized computational functionalityšthe relation betweenthe values of a program's inputs and its outputsšand they were quite specific about details such as the exact orderof computation. problems and models from application domains were sometimes supported, but except forscientific numerical computation and business data processing they received little significant support.progress in language design has often been associated with improved abstractions. periodically, languagedesigners recognize that a pattern, such as a code fragment, specification, or data representation, is being usedregularly in the same way in different contexts. the use of such a pattern may be automated via naming, macrogeneration, language syntax, or consistency checking; this is the process of developing abstractions [1]. progressof this kind has yielded higherlevel abstractions for data (e.g., abstract data types), less specific controlsequencing (e.g., types), extended domains (e.g., symbolic processing), and aggregation (e.g., modules).constructs are usually selected for abstraction because they are used regularly; thus the language designer'sfamiliarity with a domain affects the match between the language and the domain.current research activity focuses on incremental improvements, not on major changescurrent activity in mainstream programming language research is directed primarily at refining languagesupport for algorithms and data structures. major areas of interest are: functional and logic programming, which seek cleaner paradigms of computation; concurrent languages, which seek to support distributed computing; and widespectrum languages, which seek a single framework to support multiple paradigms.there are certainly worthwhile problems in these areas. but they do not address the most pressing needs ofthe users of software. the nation, and ultimately the programming language research community, would benefitfrom the identification and pursuit of good research problems motivated by the practical problems of softwareproduction.nonprogrammers dominate modern computer uselow computing costs have enabled a wide spectrum of applicationscomputers are becoming so pervasive that it is hard for anyone to avoid being at least a casual user.automatic banking machines, airline ticket machines, and public information servers are becoming increasinglycommon. office automation systems, pointofsale data collection, and database interactions are changing the styleof business. networks have enabled the distribution of information as a commodity. personal computers havemade computing broadly accessible. video games have changed the nature of recreation.computer information services represent a substantial share of computing services used by nonprogrammers. the electronic services market, which is growing at rate of 28% per year, provides such informationas realtime stock market quotes, demographic data, and currently breaking news. computer processing of thesedata gives business a competitive edge by providing more timely information at lower cost. as an index of thepotential of this market, the knightridder newspaper chain recently decided to sell eight television stations tofinance its purchase of dialog information services, one of about 350 online data services that together provide atotal of 3700 databases [2]. last year dialog realized revenues of $98 million providing 320 databases to 91,000subscribers.by now the majority of actual computer use is almost certainly by nonprogrammers. many of them can useclosed systemsšpointofsale interfaces, canned packages, and turnkey systemsšfor most tasks. somecomputations may be most effectively controlled by a nonprogramming user who is thoroughly versed in theapplication, especially when direct interaction with the computation is required. also, access to information andgeographically distributed services is often more important to these users than individual computations.appendix b76scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.even users who are not programmers need to control their own computationsclosed systems can provide good starting points for providing simple services. however, as users gain moreexperience, they often require more detailed control over the details of their computations than closed systemproducts provide. many users need computations specialized to their own problems but cannot afford to hireprofessional programmers to develop custom software. other users need to interact directly with theircomputations, for example when developing interactive models for business forecasts or refining databasesearches based on intermediate results.the need for nonprogrammers to develop their own software is not limited to the business community.scientists who perform large numerical calculations need better ways to specify their calculations for computerexecution. they want a programming language that matches the mathematical notations they already use andmakes it possible for other scientists to read, understand, and check a program. one particular complaint is thatconventional programming languages force programs to be organized primarily around the order in which thecalculations are performed which does not match the logical order for these calculations to be explained [3].partial support has emerged in packages for symbolic manipulation of equations such as macsyma (in the 1960s[4]) and more recently mathematica [5], which also emphasizes graphical presentation.the earliest significant response to this problemšindeed, the first strong evidence that the problem isseriousšwas the development of visicalc, the first spreadsheet. the program was developed in response to theneeds of accountants; it presented information in the tabular form familiar to accountants, and it automated thetedious and errorprone task of propagating changes to dependent cells. spreadsheets have had very littlecredibility in the computer science research community: they fail miserably on most of the criteria used to evaluateprogramming languages (computational model, generality, abstraction capabilities, data definition capabilities,readability, etc). however, the best measure of actual utilityšthe marketplacešgave them extremely high marksin the form of profits, enhancements, and imitators. spreadsheets made two significant departures from thesoftware mainstream: they packaged computational capability in a form that the user community already understood; theyadopted an appropriate user model. they avoided the pitfall of excess generality; instead of serving all needs obscurely, they serve a fewneeds well.the first effect was to give nonprogramming accountants direct control over their computers. in addition, themedium has proved useful to an enormous variety of users for whom writing programs had not been an effectiveway to deliver computation services.orderofmagnitude increases in service require substantial shifts of technologythis shift in the definition of the programming task mirrors shifts that have taken place in other technologies.for example, in the early part of this century automobile drivers had to be mechanics as well. many modemdrivers, however, would be hardpressed to identify a carburetor or describe its function, let alone fix one. also inthe early part of this century, telephone company estimates indicated that the use of telephonesšand hence thedemand for operatorsšwas growing at such a rate that by midcentury every person in the country would have tobe a telephone operator. now, after extensive automation, almost every person in the country is a telephoneoperator (though not fulltime): we write connection, routing and billing programs of up to 30œ40 characters inabsolute decimal machine language, without benefit of editors or even backspace commands.but software is more complicated than automobiles or telephones, and the users' need to adapt it and todevelop new applications is much greater. the major current obstacle to widespread, effective exploitation ofcomputers is the inability of end users to describe and control the computations they needšor even to appreciatethe computations they could performšwithout the intervention of software experts.appendix b77scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.computer users are interested in results, not in programming; software must reflect thisthe chief interest of most computer users is obtaining results simply and costeffectively. end users oftenunderstand what they want to do but have trouble expressing it in a form acceptable to their computational tools,or even in a form understandable by programmers. in order to serve the users whose main interest is theapplication rather than the programming task, the programming language community needs to help simplify anddemystify enduser computer use. if they are applied with the needs of these users in mind, research results fromuser interfaces and programming languages can help.improved user interface design is one of the most fertile areas for contributions to computer usability.interfaces must not only provide the correct functionality; they must also present that functionality in a way thatthe user can understand. unexpected, inconsistent, or incomprehensible system behavior can make a perfectlyfunctional system unusable.even when software systems have specialized functions, it is often useful to think of them as implementingspecialized, applicationspecific programming languages [6]. programming language design concepts can help tosimplify syntax and decompose tasks into regular, consistent elements. they can also help to identify goodconstructs for taking advantage of improved physical interfaces such as threedimensional and color graphics andhighresolution images. the chief risks of using ideas from language design are in succumbing to the temptationsof excess generality and in assuming that the user thinks like a software designer.a system must sustain a single user model; in language design terms this is preservation of semanticconsistency. for example, nonprogrammers should not need to make distinctions between the command languageand the application language, between editing and compilation, and so on. this integration is particularlyimportant for distributed systems; uniformly addressable networks and seamless flow from local to remotecomputing are especially important.the macintosh made a major step toward delivering computer power to nonexperts. it provides a single styleof interaction, data interchange compatibility among applications, and a large (and growing) set of applicationswhich share a direct manipulation model that seems natural to a wide class of users. this machine and itssoftware, derived from mainstream computer science research, has both academic credibility and academicpopularity. it was originally bundled with word processing and picturemaking software (macwrite andmacpaint) rather than with a programming language such as basic. though there was considerable skepticism inthe programming community about it at the time, this decision reflected a significant shift of attitude aboutcomputers, from devices for executing users' programs to deliverers of computing services to end users.requirements for large complex software systems exceed production abilitygrowth of demand is greater than growth of capacitysoftware costs for both development and maintenance are still largely laborrelated, yet the supply ofcomputer professionals is insufficient to meet the demand. neither the number nor the productivity ofprofessionals is increasing fast enough. programmer productivity grows at a rate of 4œ6% annually [7], butdemand growth can be as high as 20œ25% [8] (figure 1). an annual increase in the population of programmersestimated at around 4% does little to alleviate the shortfall.it is not uncommon for the development of a software system to get out of control. a recent survey of 600clients of a large accounting firm showed that at least 35% have system developments that are out of control:millions of dollars over budget, years behind schedule, and less effective than promised. for example, in 1982 amajor insurance company began an $8 million automation project scheduled for completion in 1987; the currentestimate is for completion in 1993 at a cost of $100 million [9]. in addition to the obvious expenses of systemoverruns and failures, we suffer opportunity costs because some projects are not even attempted.appendix b78scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.figure 1onboard code size for manned spacecraft and annual programmer productivity.system requirements exceed the scope of conventional programming languagesmodern software system products are no longer selfcontained systems that simply process information,delivering results that are subject to human monitoring and, if necessary, intervention. systems now involveresponses too fast to monitor, interconnections between systems and data bases, and high requirements forreliability, safety, and security. they often control machinery or physical processes with hard realtimerequirements. they must deal gracefully with incomplete and conflicting information, and their sheer scale andcomplexity may be so large that the implications of a design change are essentially impossible to understand.large complex system designs involve relations among subsystems, rather than algorithms and datastructures. they require specification and prediction of properties such as space, time, throughput, reliability,security, and synchronization with external events. subsystems often deal with longterm persistence (e.g., filesystems and data bases) or distributed responsibilities (e.g., scheduling, communication) rather than with therelation between inputs and outputs. a number of techniques for specifying system organizations have beendeveloped [10], but the programming language community has paid little more attention to them than it has tospreadsheets.not until the mid1970s did programming, language, support extend beyond the module boundary tosystemlevel tasks such as module interconnection specifications and configuration management [11,12]. even,now, module interconnection or system modeling languages focus on simple procedural interfaces, providing littlesupport for connecting subsystems by data sharing, data flow, or interleaving of code. they also offer little helpfor taking advantage of knowledge about special properties of modules, such as the fact that if a module defines anabstract data type then its specification is an algebra and it preserves certain invariants.in addition, large software system development requires coordination among many programmers.conventional programming languages were designed for programmers working as individuals. they provide waysto define algorithms and data structures but not to coordinate the efforts of multiple programmers, integratedprogramming environments support more efficient use of the existing tools, but they don't extend the range of theindividual tools.software lacks a true engineering basethus software design, unlike design in classicalappendix b79scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.engineering fields, is often anything but routine. software development lacks good predictive and analytictechniques. only in limited cases can the properties of a product be predicted from an initial design, andmeasurement techniques for software products are shaky. software lacks detailed engineering handbooks in whichdesign and implementation alternatives are cataloged along with quantitative specifications of useful properties;such handbooks are the mainstay of classical engineering practice [13,14].table 1 engineering evolutioncraftsmanshipcommercial practiceprofessional engineeringpractitionersvirtuosos and amateursskilled craftsmeneducated professionalspracticeintuition and brute forceestablished procedureanalysis and theoryprogresshaphazard and repetitivepragmatic refinementscientifictransmissioncasual and unreliabletraining in mechanicseducation of professionalsengineering disciplines evolve from craftsmanship through commercial practice to professional status. thosestages differ in significant ways (table 1). craftsmanship gives way to commercial practice when the significanceof a product is great enough that informal production doesn't satisfy demand. at this point production, financial,and marketing skills join the craft skills to form the basis of a formal market. the needs of this market usuallygenerate technological problems that feed an associated science; good science has often been driven by just suchproblems. when the science becomes mature enough to supply costeffective solutions, it provides the basis forprofessional engineering practice [15,16].software practice is now in transition from craftsmanship to commercial practice. the phrase "softwareengineering" is a statement of aspiration, not a description. a scientific basis is emerging from computer science,but it takes 10œ20 years for an idea to mature (consider data types, algorithms, and compiler developmenttechniques) and the coupling between practical problems and good research ideas is weak.language concepts can support a design level above the algorithm/data structure levelprogress in programming languages is closely coupled with the development of useful abstractions and theassociated analytic techniques. each new set of abstractions suppresses some body of detail that has become moreor less standardized and reduce the chance of simply writing it down wrong. thus symbolic assemblers eliminatedthe need to handplace code and data in memory; the usual control constructs of imperative languages codifiedsome common (and errorprone) assemblylanguage sequences; data types and the associated checking code allowthe classification of variables according to intended use; and so on. each of these eliminated some detail andincreased the conceptual size or computational leverage of the primitives used by the programmer. by doing so, itfreed up energy for programmers to deal with larger problems, and thereby to discover new patterns for which todevelop abstractions.software system design deals with higher, level constructs and different properties from conventionalprogramming languages. nevertheless, language concepts such as precise semantics for primitive elements andcombining operators, abstraction (the ability to extend the language by naming a defined construct), and closure(the ability to use a defined construct on an equal footingappendix b80scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.with a primitive element or operator) are as important for systemlevel elements as for algorithms and datastructures.the major problem with developing large complex softwareintensive systems is the overall organization ofthe system, not the implementation of the individual modules. good abstractions are needed at this level, thesoftware architecture level architectural styles and useful subsystem elements must be identified and classified.for example, unix pipe/filter organizations and data abstraction organizations are useful in different settings; theyhave different control and communication properties, and the individual elements (filters and abstract data types)are very different. what other software organizations are in common use? what other kinds of elements are used,and how are they combined? how can they be specified?programming language designers must look beyond traditional areasprogramming languages are notations for defining and controlling computations. conventional languages,including imperative, functional, logic, objectoriented, and widespectrum languages, operate at similar levels.they provide constructs to manipulate data elements and describe algorithms for that manipulation, semantics for data items, aggregates of data items, and functions to compute on these data items, abstraction tools for extending their vocabularies, type systems and other verification assistance for checking correctness, or at least consistency, and sometimes facilities for controlling concurrent execution.they are mostly rooted in mathematical notations; they mostly force quite specific specification of thesequencing of operations; and they mostly proceed from assumptions about absolute correctness, notapproximation or correctness with some tolerance for error or inconsistency.the mainstream of programming language design shows little enthusiasm for user interface specificationlanguages, constraint languages, program generators (other than compiler generators), or rulebased languagessuch as those used for expert systems. generality is such a strong criterion for evaluation that programminglanguage designers are rarely involved in command or job control languages or in specialpurpose tools (e.g.,structural analysis tools for civil engineers) that have rich functionality.programming language designers need to expand their mindset beyond classical programming, which isprimarily systems programming. the notion of "programming language" should take on a larger scope,encompassing techniques, tools, and notations for specialized applications and for architectural design that requiredifferent primitives and different operations. these areas rely on the same general concepts of semantics,abstraction, specification, and correctness as traditional languages, though they will differ in detail.semantics need to be better tuned to specific domains of discourse; this involves support for different kindsof primitive entities, for specification of properties other than computational functionality, and for computationalmodels that match the users' own models. syntax needs to be made more appropriate to each user community, andsupport is required for nonmathematical syntax, graphics, and manipulative models. specific areas that need workinclude specialized user models, systemlevel architectures, specification languages for program generators, visuallanguages, programming by successive approximation, data base interactions, and comprehensible connections todistributed systems.the bottom linethe most desperate needs for new advances in software lie outside the traditional domain of programminglanguages. new, interesting research questions will arise from bringing language concepts to bear on these needs.appendix b81scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.acknowledgmentsthe ideas presented here have been brewing for a long time. i appreciate the support of mobay corporationand cmu's computer science department and software engineering institute. the specific stimulus to write thisargument down was provided by a workshop on programming language research organized for the office ofnaval research and advance planning for a workshop on complex system software problems organized by thecomputer science and technology board of the national research council. norm gibbs, al newell, paulgleichauf, ralph london, tom lane, and jim perry provided helpful comments on various drafts.references[1] mary shaw. abstraction techniques in modern programming languages. ieee software, 1, 4, october 1984, pp. 10œ26.[2] jeff shear. knightridder's data base blitz. insight, 4, 44, october 31, 1988, pp. 44œ45.[3] gina kolata. computing in the language of science. science, 224, april 13, 1984, pp. 140œ141.[4] r.h. rand. computer algebra in applied mathematics: an introduction to macsyma. pittman 1984.[5] stephen wolfram. mathematica: a system for doing mathematics by computer. addisonwesley, 1988.[6] jon bentley. little languages. communications of the acm, august 1986.[7] barry w. boehm. software engineering economics. prenticehall 1981.[8] james e. tomayko. computers in space: the nasa experience. volume 18 of allen kent and james g. williams, eds., theencyclopedia of computer science and technology, 1987.[9] jeffrey rothfeder. it's late, costly, incompetentšbut try firing a computer system, business week, 3078, november 7, 1988.[10] david marca and clement l. mcgowan. sadt: structured analysis and design technique. mcgrawhill 1988.[11] frank deremer and hans h. kron. programminginthelarge versus programminginthesmall. ieee transactions on softwareengineering, se2, 2, june 1976, pp. 1œ13.[12] butler w. lampson and eric e. schmidt. organizing software in a distributed environment. proc. sigplan '83 symposium onprogramming language issues in software systems, pp. 1œ13.[13] l. s. marks et al. marks' standard handbook for mechanical engineers. mcgrawhill 1987.[14] r. h. perry et al. perry's chemical engineer's handbook. sixth edition, mcgrawhill, 1984.[15] james kip finch. engineering and western civilization. mcgrawhill, 1951.[16] mary shaw. software and some lessons from engineering. manuscript in preparation.appendix b82scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.charles simonyimy worst problem with current software productioni am a development manager at microsoft corporation, developing language products for the microcomputermarketplace. the worst problem affecting my activity is insufficient programming productivity. in themicrocomputer software business resources are ample, while the premiums on short timetomarket, on the''tightness'' and on the reliability of the produced code are all high. under these circumstances, adding extra peopleto projects is even less helpful that was indicated in fred brook's classic "mythical manmonth". there is also ashortage of "star" quality programmers.as to the solutions, on the personnel front we are expending a lot of effort to search out talent in the unitedstates and internationally. we also organized an internal training course for new hires. however, these measureswill just let us continue to grow but no improvement in the rate of growth can be expected.i think that the remedy to the productivity question with the greatest potential will come from a longlasting,steady, and inexorable effort of making small incremental improvements to every facet of the programmingprocess which (1) is easily mechanizable, and (2) recurs at a reasonable frequency which can be lower and loweras progress is made. i think of the japanese approach to optimizing production lines as the pattern to imitate.in hitachi's automated factory for assembling vcr chassis general purpose and special purpose robots alternate with afew manual assembly workers. the interesting point is that there was no allencompassing uniform vision: hitachiengineers thought that general purpose robots were too slow and too expensive for simpler tasks such as dressingshafts with lubricant or to slip a washer in its place. at the opposite extreme, a drive band which had to run in severalspatial planes was beat handled by human workers, even though, in principle, a robot could have been built to do thejob, but such a robot would have cost too much in terms of capital, development time, and maintenance. in themiddle of the task complexity spectrum general purpose robots were used. the product design was altered by theengineers to make the robots' work easier. for example, they made sure that the robots can firmly grasp the part, thatthe move of the robot arm through a simple arc is unimpeded, and so on. the actual product design could not besimplified because it was constrained by competitive requirements. if anything, the increase in productivity makes itpossible to elaborate the design by adding difficult to implement but desirable features. for instance, front loading invcrs is much more complex mechanically than top loading, yet consumers prefer front loading because of itsconvenience and because front loading units can be stacked with other hifi equipment. the point here is thatsimplicity is always relative to the requirements. the requirements always tend to increase in complexity, and thereis nothing wrong with that.these notions have direct parallels in software production. software complexity should be measured relative to therequirements and can be expected to increase in spite of the designer's best efforts. frequent simple tasks should besolved by specialized means (e.g., most languages have a special symbol "+" for addition) and manual methods canbe appropriate for the most complex production steps (for example hand compiling the innermost loop). this isnothing new: the issue is really the proper mix. in manufacturing, as in programming production old shops haveobvious problems with reliance on manual labor and on inflexible special purpose machines, while the most modemshops overutilize general purpose equipment and fail to integrate the other approaches properly. hitachi, in a verypragmatic way, created a new and potent cocktail from the different methods each with particular strengths andweaknesses.i do not deny that more radical treks in the problem space and in the solution space could result in majorbreakthroughs and also wonder sometimes if america has the temperament to pile up the small quantitativechanges in search of the qualitative changes. yet i believe that the payoffs from the cumulative improvements willbe very large. i also believe that the approach is only medium in cost and low in risk. while the usual rule ofthumb is that this would indicate low payoff, i am comfortable with the paradox and blame special cultural,historical, and business circumstances for the disparity.i give you a few examples to illustrate the miniscule scopes of the individual improvements to software tools whichcan be worthwhile to make. during debugging it is often useful to find all references to a variable foo in a procedure.every editor typically has some "find" command which, after the user types in "foo", will scan for and highlight theinstances one by one. an improvement, for this purpose, is the ability to point toappendix b83scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.one instance and see the highlight appear on all instances at once. this is hardly automatic programming, not evencase, just one small step.in the programming language we use (c) just as in practically all languages, a procedure can return only a single dataitem as its result. many procedures could benefit from the ability of returning more than one data item. a trivialexample is integer division which develops a remainder in addition to a quotient. of course myriad possibilities existfor implementing the desired effect: pass a pointer to the location where the second result should go ("call byreference"), return the second result in a global variable, aggregate the results in a data structure and return the singleaggregate value, and so on. an improvement can be made to the language by adding mesa's constructors andextractors which removes the asymmetry between the first and the other results returned, resolves the programmers'quandary providing a single optimal solution to the problem, and enables focus on optimizing the preferred solution.we will need on the order of a thousand such improvements to make an appreciable difference. it will be easytechnically. it has not been done before probably because of the following nontechnical problems:1. talented people do not get excited about incremental projects.2. one has to have source access to all the software which is to be modified: editor, compiler, debugger,runtime, etc. such access is rare.3. the incremental improvements are so small that when uncertain sideeffects are also considered, thenet result may well be negative instead of positive. many argue, for example, that the mere existenceof something new can be so costly, in terms of training, potential errors, conceptual load on the user,or maintenance, that only profound benefits could justify it.i consider the last argument very dangerous in that it encourages complacency and guarantees stagnation. ofcourse it has more than a grain of truth in it and that is where its potency comes from. my point is that if you lookaround in the solution space from the current state of the art and you see increasing costs in every direction, youcan only conclude that you are in some local minimum. cost hills in the solution space are obstacles to beovercome, not fenceposts for an optimal solution. one has to climb some of the cost hills to get to better solutions.i would also like to list a few areas which are not problems at least where i work: marketing, product design,implementation methodology, testing methodology, product reliability, working environment, and management.this is not to say that these activities are either easy or well understood. for instance, we still cannot schedulesoftware development with any accuracy, but we learned to manage the uncertainty and we can expect muchsmaller absolute errors (even at the same relative error) if productivity can be improved. similarly, testing is stillvery unscientific and somewhat uncertain. again, with sufficient safety factors we can achieve the requiredreliability at the cost of being very inefficient relative to some ideal. with greater productivity more builtin testscan be created and testing efficiency can improve. so i see productivity as the "cash" of software production whichcan be spent in many ways to purchase benefits where they are the most needed: faster time to market, morereliability of the product, or even greater product performance. we can get this last result by iterating and tuningthe design and implementation when the higher productivity makes that affordable.the nation's most critical problem with current software productionbefore coming to the workshop, i wrote that i did not know what the most critical problem is for theindustry. now, after the workshop, i still do not know which problem is the most critical, but i can list a numberof serious problems with promising approaches for their solutions and comment on them. this is a small subset ofthe list created by the workshop and i am in general agreement with all other items on the longer list as wellexcept that the sheer length of the list indicates to me that some of the problems may not be as critical as some ofthe others.1. make routine tasks routine. this is a very powerful slogan which covers a lot of territory. theimplication is that many tasks which "ought" to be routine are far from routine in practice.appendix b84scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.the workshop expressed a desire to promote the development of an engineering handbook tocover routine practices. i share this desire. i wonder, however, if this can be a research topic."routine" is a synonym for "dull" while the handbook would have to be written by first rate talents.the whole thing boils down to this larger issue: how does society get its best minds to focus on dullproblems which nonetheless have great value to society. historically, it has always been a packagedeal: solve the problem and win the war, as in the manhattan project; go to the moon, as in apollo;and get rich, as in publishing, startups and leveraged buyouts. i am enthusiastic about the handbookbut i would feel more sanguine about a "ziffdavis'' or a "chemical abstracts" financing andorganizing the work than nrc feeding the oxymoron ''routine research".we should also keep in mind that in some instances a routine task could be automated oreliminated altogetheršthe ultimate in "routinization". this is an area where my organization will bedoing some work.2. clean room method. i join my colleagues in believing that software reliability and debugging costs aregreat problems, and that harlan mills' "clean room method" is an exciting new way of addressing theissues. being high risk and high return, it is a proper research topic.3. reuse. here we have an obvious method with a huge potential economic return, yet very muchunderutilized. i agree that it is an important area and it is a proper subject for research. my onlycaveat is this: many of the research proposals implicitly or explicitly assume that the lack of reusemust be due either to the insufficient information flow, or to irrational decision making ("nihsyndrome"). my experience has been that software reuse was difficult even when the information wasavailable and the decision making rationally considered longterm benefits. to avoid disappointment,the research will have to extend beyond the "module library" concept to include: module generators recommended at the workshop as alternatives for large collections of discretemodules. for example, instead of multiple sine routines, publish one program which inputs therequirements ("compile time parameters") and outputs the optimal sine routine for the particular purpose. iagree with this, and would only add that we need better languages for module generation. a giant list of"printf" or format statements will not do either for the writers or the users. the module creator willobviously benefit from a better paradigm. in principle, the users should not be concerned with what isinside the generator, but in practice the program may have to be at least a part of its own documentation. a study of the economics of reuse. for example larry bernstein pointed out that the simpler productivitymeasurements can create disincentives for reuse. or, consider that the creation of reusable software ismore difficult than the oneshot custom approach. the beneficiary of this extra effort is typically adifferent organizational or economic entity. some sort of pricing or accounting mechanism must be usedto keep the books straight and allocate credit where credit is due. this brings us to: developing metrics especially for reuse. assume a module of cost x with 2 potential applications.without reuse the cost is 2x. with reuse, it is sometimes claimed, the cost will be x + 0, that is, writeonce and use it again free, a clear winwin proposition. this is not a realistic scenario. we need toresearch the cost picture: how much more expensive is it to make the code reusable? how expensive is itto use reusable code? what characterizes code for which the reuse costs are low, such as numericalroutines? when are reuse costs higher than the custom development costs?i would also like to make a secondorder point. let us call the solution to the nation's software problems the"next generation tools." it is safe to guess that these will be very complex software systems and that they will bedeveloped by using more current tools, which connects to my response to question #1. scientists had to measure alot of atomic weights before the periodic table was discovered. they had to measure an awful lot of spectralfrequencies before quantum theory could be developed. we, too, should try to make the great leaps but meanwhileassiduously do our homework.appendix b85scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.i think in the near term, the appearance of a ubiquitous programmer's individual cross development platformwould greatly enhance our ability to create more, cheaper, and better quality software. by "ubiquitous" i meanmany hundreds of thousands, that is, 386 os\2 based, rather than tens of thousands, that is, workstation basedsystems. by "cross development" i mean that the target system would be separate from, and in general differentfrom, the highly standardized and optimized development machine.it would be also nice if the ubiquitous platforms used a ubiquitous language as well. insofar as the userinterface to the editor, compiler, debugger, etc., is also a language, possibly larger than a programming language,this commonality is certainly within reach with the advent of the graphical user interfaces, and in particular of thesaa standard. the computer language is a greater problem. wide acceptance can be ensured, in general, by twomeans: by the fiat of a powerful organization, as was the case for aria, or by overwhelming success in themarketplace of business and academe. here lotus 123 comes to mind as an example for the degree of successwhich might be necessary; in languages c came the closest to this. however, c and even c++ did not make amajor effort to address a wider user base, and in fact takes justified pride in the substantial accomplishments insoftware technology which have sprung from the focus on narrow goals.i am a believer in a major heresy. i believe that pl/1 had the right idea, if at the wrong time. a leadingprogramming language should try to satisfy a very wide range of users, for example those using cobol (forms,pictures, decimal arithmetic, mass storage access), c++ (object oriented programming, c efficiency, bit levelaccess), snobol (sting processing, pattern matching), ada (type checking, interface checking, exceptionhandling), fortran (math libraries), and even assembly language (super high efficiency, complete access tofacilities). let me just respond superficially to the most obvious objections:1. pl/1 is terrible; pl/1 was designed to have everything; ergo wanting everything is terrible. answer:problems with pl/1 are real, but they can be expressed in terms of "lacks": pl/1 lacks efficientprocedure calls, pl/1 lacks many structuring or objectoriented constructs, the pl/1 compiler lacksspeed, or i lack knowledge of how to write a simple loop in pl/1. all of these problems can be solvedby having "more".2. not having features is the essence of the benefit i am seeking. how can you satisfy that? answer: icannot. most people seek the benefits which are easily (and usually) derived from not havingfeatures. among these benefits are ease of learning, efficiency, availability, low cost. however, thesame benefits can be obtained in other ways as well; typically this requires large onetime ("capital")investment which then will possibly enable new benefits. again, the point is that simplicity should bejust one means to some end, not the end in itself3. you cannot learn a monster language. answer: a large piece of software (or even the portion of alarge system known to a single person) is much more complex in terms of number of identifiers,number of operators, or the number of lines of documentation than a complex programming language.if some elaboration of the smaller partšthe languagešbenefits the bigger partšthe software beingwritten in the languagešwe have a good tradeoff.my feeling is that a small trend of enhancing the most promising languagešcšhas already started with thegrowing popularity of c++. i believe that this trend will continue with even larger supersets of c appearing andwinning in the marketplace. i think we should encourage this trend, promote a rapprochement between the cpeople and the data processing professionals, and point out the dangers to those who remain locked into ada.appendix b86scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.william a. wulfthe worst problemthe worst problem is not cost; it's not unreliability; it's not schedule slips. rather, it's the fact that in the nearfuture we won't be able to produce the requisite software at all!programmer productivity, measured in linesofcode per unit time has increased 4œ6% per year since themiddle 60's. the amount of code required to support an application measured over the same period, has beenincreasing at more than 20% per year. this disparity cannot continue. software is already the limiting factor inmany areas; the situation will only get worse unless by some magic programmer productivity increasesdramatically.the basic problemthe problems of software development are so well articulated in fred brooks' article "no silver bullets" andthe defense science board report that preceded it, that there is little that i can add. i can, however, cast them inanother way that i find useful.software production is a craft industry. for programming, just as for carpentry or basketweaving, everyproperty of the final product is the result of the craftsmanship of peoplešits cost, its timeliness, its performance,its reliability, its understandability, its usability, and its suitability for the task.viewing programming as a craft explains why i am intellectually unsatisfied with "software engineering". italso explains why the software crisis, proclaimed twenty years ago, is still with us. better management, betterenvironments, and better tools obviously help either the carpenter or the programmer, but they do not change thefundamental nature of the activity. not that current software engineering is wrongheaded; it's very, veryimportantšafter all, it's all we have. but we must look elsewhere for a fundamental solution.viewing programming as a craft also suggests the shape of that solution. we must find a way to make thebulk of programming a capitalintensive, automated activity. here the analogy with other crafts breaks downbecause programming is a creative, design activity rather than a production one. however, we have numerousexamples where we have achieved automation. no one writes parsers anymore; parsergenerators do that. fewpeople write code generators any more; code generator generators do that. no one should write structured editorsanymore; systems such as teitelbaum's synthesizer generator can do that.tools such as parser generators are fundamentally different from tools such as debuggers, editors or versionmanagement systems. they embody a model of a class of application programs and capture knowledge about howto build programs for those applications; in a loose sense they are "expert systems" for building applications in aparticular domain. the better ones are capable of producing better applications than the vast majority ofprogrammers because they embody the most advanced expertise in the field. thus they provide both enormousproductivity leverage and better quality as well.what should we do?i have three suggestionsšone general, longterm one, and two more specific, immediate ones.1. recognize that the general solution is hard! there are two implications of this: we need basic, longterm research, recognizing that it won't lead to a "quick fix" (i fear that too manyprior programs, such as stars, have been sold on the promise of quick, massive returns).appendix b87scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved. we must look for specific, special case solutionsšthat is, common situations (like parsers and codegenerators) where automation is possible now.i'll return to the second point below. 2. change government procurement policy, especially dod's! the current policy, which requires deliveryof the tools used to build a product along with the product, is a strong disincentive for a company toinvest in tooling. this is backwards; we should be doing everything possible to make it attractive for theprivate sector to invest in greater automation. 3. measure! ten years or so ago, ibm did a study of its largest customer's largest applications. theydiscovered that 60% of the code in these applications was devoted to screen management. i don't know ifthat's still true, and it doesn't matter. what does matter is that we (the field) don't know if it's true.suppose it were true, or that something else equally mundane accounts for a significant fraction of thecode. we might be able to literally double productivity overnight by automating that type of codeproduction. i doubt that anything so dramatic would emerge, but it's criminal that we don't know.of course, committing to a program of basic research is both the most important and the most difficult toachieve. i am not sure that the software research community itself is willing to admit how hard the problem reallyis, and we have lost a great deal of credibility by advocating a long series of panaceas that weren't (flowcharting,documentation, timesharing, highlevel languages, structured programming, verification, ...).it is not enough to say that we must do basic research. what research, and why? my sincere hope is that thisworkshop will at least begin the process of laying out a credible basic research agenda for software. to that end,let me mention just a few of my pet candidates for such an agenda: mathematics of computation: i am skeptical that classical mathematics is an appropriate tool for ourpurposes; witness the fact that most formal specifications are as large as, as buggy as, and usually moredifficult to understand than the programs they purport to specify. i don't think the problem is to makeprogramming "more like mathematics"; it's quite the other way around. languages: we have all but abandoned language research, which i think is a serious mistakešhistoricallynew languages have been the vehicle for carrying the latest/best programming concepts. i am especiallydisappointed that current mechanisms for abstraction are so weak. i am also disappointed in the"newspeak mentality" of current language orthodoxy (newspeak was the language big brother imposedin 1984; it omitted the concepts that would allow its speakers to utter seditious thoughts [bad programs]). parallelism: the emphasis of the last two decades has been on synchronizationšon mechanisms forreducing the problems of parallelism to better understand sequential ones. such a mindset will never leadto massively parallel algorithms or systems. the ideal should be parallel algorithms and systems with nosynchronization at all. testing and testability: both of these have been stepchildren, and even denigrated in academia ("... makeit correct in the first place...."). my experience in industry suggests that there is a great deal to be gainedby (1) making testability a first order concern during design, and (2) making testing an integral part of theimplementation process.this is by no means a complete list; hopefully it will provide some fodder for thought.appendix b88scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.andres g. zellwegerintroductionthe following two position statements provide, from my perspective, some of the motivation for therecommendations that came out of the complex software systems workshop. in general, the recommendationsfrom the workshop provide a long term attack on the problems i have outlined below. i have suggested someadditional avenues that may eventually offer us solutions to the problems most critical to me. i also believe that itis important that the software community, together with the military/industrial complex, take three actions that canprovide much needed near term relief to the "software problem". these are (1) education of users and builders oflarge software systems to teach them how to approach the joint understanding of what a system can and should do;(2) institutionalization of the current state of the art in software design and development; and (3) initiation of ajoint effort by the dod, affected civilian government agencies, and industry to solve the problem of theincompatibility between prescribed software dod development paradigms and what, in practice, we have learnedworks best today.my most serious problem with software developmentas the corporate chief engineer, i am intimately concerned with cta's ability to produce software. ourgoal in the software area is precisely the goal stated in the workshop statementšthe efficient production ofquality software. to that end, we have been concentrating on improving the support infrastructure (qa, cm,software development standards, etc.), standardizing and strengthening our engineering environment and processfor developing software, and developing a base of reusable software components.for the past year, i have been conducting periodic internal reviews of all projects in the company with theprimary objective of improving the quality of all the products we deliver to our customers. interestingly enough,based on that experience, our most serious problem does not appear to be in the production process per se, rather itis in the cost and schedule increases due to changing user requirements. despite the fact that cta uses rigorousmethods to analyze and define the computer human interface and refines the interface with rapid prototyping (bothwith extensive customer involvement) we still find that our customers "change their mind" about this interface, andthus also about the requirements for the system, as we go through preliminary and detailed design.while there are many contributing factors to this phenomenon, i suspect that the primary reason for this isthat neither we nor our customers have a very good idea of what they want a system to do at the outset of adevelopment project. there is clearly a lack of understanding, but, unfortunately, in most cases, the nature of thegovernment acquisition process and the pressures of schedules force us to begin a system design anyway. as thedesign process progresses, our customers become smarter about how their application could be solved (and aboutthe capabilities of modern computer technology) and we see the classical "requirements creep". anothercontributing factor, particularly on large projects that take several years to complete, is a legitimate change in userneeds that impacts software requirements.i see two complementary solutions to this problem. first, we must recognize that, in nearly all cases, asoftware system must change with the world around it from its conception to its demise. software must therefore,by design, be inherently capable of evolution. we are beginning to learn how to build such software (see, forexample, the faa's "advanced automation system: strategies for future air traffic control systems" in thefebruary 1987 issue of computer), but i think we are just scratching the surface of a good solution. second, just asthe american public had to become accustomed to fast foods in order to achieve the significant breakthrough inthe cost of food delivery, users of computer systems must learn that certain sacrifices need to be made to get theirsystems faster and for less money. standard user interfaces, incremental delivery of capabilities,appendix b89scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.and the use of commercial packages that may not have all the bells and whistles a user would like are just a fewexamples of such sacrifices. education, not only of the developers of software but also of users, is absolutelyessential if we are to make progress in this area. research into how special purpose systems can be built fromstandard building blocks or how tailored building blocks might be generated automatically is beginning but a greatdeal more is needed to make significant breakthroughs.the industry and nation's most critical problem with software production todaythe list of symptoms we hear every day, particularly in the aerospace/defense industry, is long: software doesn't meet user needs. software doesn't work as advertised. software fails. software is late. software cost is more than original estimate.at the same time, as technology advances, our appetites are growing. systems are getting bigger and morecomplex. the life cycle is getting longer. software safety is becoming more critical as we increase our dependenceon computer systems.the most critical near term problem that must be addressed if we are to alleviate some of these symptoms isthe replacement (and institutionalization) of the unwieldy waterfall model with a new software developmentparadigm. the waterfall model and all the dod standards (especially 1521b, 2167, and 483) served as a vehicle tolet industry build and the government specify and manage large software projects. unfortunately, the waterfallmodel no longer works and has not been replaced with a new model and a set of compatible standards,development methods, and management practices. the most recent version of 2167 has taken away many of thewaterfall constraints, but offers nothing to replace the paradigm. software developers are essentially told to"tailor". this freedom is, in some ways, an advantage for the more sophisticated developers, but places a severeburden on the government and the majority of the developers of large aerospace/defense systems. typicalquestions are, what documentation should be developed?, how should we deal with project reviews, test,documentation, cost and schedule when reusable parts or prototyping is involved?, how do we implement "build alittle, learn a little" and still get good documentation and coherent designs?, what should be my project milestonesand when should they occur?, how do i evaluate the cost of the proposed approach and quantify the impact of theapproach on risk?, and so on.over the past decade we (the software community) have learned a great deal about what works and whatdoesn't work in software development. the acceptance of ada and with it a renewed emphasis on good softwareengineering practice also needs to be factored into the software development process (e.g., more design time,different testing strategies). several paradigms that incorporate what we have learned (the barry boehm softwarespiral is perhaps the best known) have been proposed, but as a community we must now take the next step andadopt and institutionalize one or more paradigms so that the aerospace/defense industry can once again set upstandardized "factories" to build software in a manner that is compatible with government specifications,standards, deliverables (documentation), and well defined schedules and review points.the inherent need for this solution stems from the way in which the government procures its software.perhaps a longer term (and better?) solution is possible if we change the way government goes about the softwareprocurement and management process. i believe that a fruitful avenue of research would be the exploration of newways of specifying and buying software and the impact of this on the way that aerospace and defense contractorscould approach the development of large software systems.appendix b90scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.arthur i. zygielbaummy worst problems with softwaresoftware development has challenged me just like other developers and managers. we have an inability tocorrectly and accurately predict, monitor, and control cost and schedule during the development process and theprocess of sustaining engineering for significant and complex software projects. further, most software productsare tied to particular groups of people and usually to one or two "gurus."the "attacks" we have made on the problem are classic and include, in the past, creation and rigorousenforcement of software standards, the use of "standard" languages, and tools to aid in monitoring thedevelopment process. but we've discovered that the impact of any of these elements is difficult to ascertain.the jet propulsion laboratory (jpl) took strong steps four years ago to create a software resource center(sorce) to help change the practice of software engineering at the laboratory. charged with the task of trainingmanagers and engineers, evaluating tools, consulting and maintaining software standards, sorce is beginning toimprove the process. jpl supports this effort at about $2m per year of internal funding. sorce has also begun tocollect software metrics and to use them to develop a "corporate" memory of success and failure.as strong as the sorce effort is, we still suffer from significant overruns in cost and time in trying to meetour commitments. fred brooks, in his paper, "there's no silver bullet," predicted this outcome. brooks identifiedtwo types of software development difficulties. the first was that set of problems created in trying to improve theprocess. for example, a new language may reduce overall complexity over an earlier language, but will introducenew difficulties through syntax ambiguity or lack of rigor in type checking, etc. these accidental difficulties aresolvable through careful design or procedure. the second class of errors is inherent in the process. software is hardto do correctly. there are few human endeavors that are as difficult to grasp as a complex program or set ofprograms. the relations, processes, and purposes of the elements of a program are difficult to describe and thusdifficult to use as construction elements. creating tools, methods or magic to solve these difficulties is extremelyhard.another symptom of the problem is an inability to discover the initial set of requirements that lead to aspecification for software development. it has been said that the programmer becomes the system engineer of lastresort. being unable to completely specify the design in a closed, measurable form, we tend to leave designdecisions to the last possible stage in the development process. it is difficult to manage a process when the endgoal cannot be adequately described!underlying the difficulties i identify is the lack of an extensible scientific basis for the process called softwareengineering. dr. mary shaw of the software engineering institute and carnegie mellon university veryarticulately described this through analogy with other professional engineering disciplines. she describes threestages of evolution in a practice before it is really engineering. the first is "craft." bridge building was a craftwhen first practiced. there was little regard for the cost of natural resources and the practice of building was left to"gurus" who could pass the knowledge to a few others. extension or improvement tended to be through accidentrather than through development. the second stage is "commercial." here there is refinement of practice to a pointwhere economic use of resources is possible. the process knowledge is more widely known, but extensions andimprovements still tend to be from exercise and accident. during this stage a scientific basis for the disciplinebegins to evolve. for bridge building, this was the application of physics to understanding the building materialsand the structures made from those materials. eventually practice and the evolving science become joined into anengineering discipline. the scientific basis allows prediction of the process and improvement through systematicmethods. further, the scientific theories themselves are extensible which results in further process and productimprovements.appendix b91scaling up: a research agenda for software engineeringcopyright national academy of sciences. all rights reserved.in my opinion, the key for the future of software is in the development of this underlying theory. it took manyhundreds of years for civil engineering. it will take decades for software. but we need to make the commitmentnow. developing this theory requires experiments with programmers, engineering and management teams, and thegathering of metrics. these studies must be made both at the individual practitioner level and in "programminginthelarge" covering groups of engineers and managers. the effort requires the use of rigorous scientific practicein the development, testing and refinement of hypothesis leading to theory.the complex software systems workshop would be an excellent forum to provide a national focus on theneed to develop a science to underlie software engineering.industry and national problems with softwarerather than the usual views on the difficulty of producing software, i would like to look at the difficulty ofusing software and our increasing dependence on software.during a recent trip to washington, i was amused and dismayed to overhear a confrontation between anirritated customer and a clerk of a large national rentacar chain. it seems that the customer had signed a contractspecifying a particular rental rate for a car. when he returned the car, he paid for it with a credit card. it was notthe credit card identified in his "preferred customer" identification. since the agency's computer could not link thecard to the customer, the rate printed on the invoice was higher than that in the original contract. the customer,correctly, i think, argued that the method of payment should not affect the rate. the rate was tied to his frequentuse of the agency and to his corporate standing. the clerk said that she had no choice but to charge the rate quotedby the computer.i was similarly amused by a recent television show where the intrepid detective told his boss that the evidencegathered was accurate and correct because they "checked it in the computer." computers are becoming anincreasingly pervasive part of our lives. there is little that we do that is not affected or enabled by them. but asdemonstrated by my examples, the computer can do more than automate a process. it can replace that process withan inaccurate representation.naturally, the representation is defined in software. capturing requirements and understanding theimplications of a particular software architecture are difficult. hence the rentacar rules and regulations created bysoftware difficulties. and hence the artificial increase in the cost of a product.a recent study of students was reported wherein computer software was set to make predictable errors insimple calculations. most of the students accepted the answers without relying on common sense. we readilyassume the correctness of the computer. computer accuracy is largely dependent on the software implementation.errors can lead to amusement or to disaster.i am quite sure that the reader can conjure or remember similar examples. we also need to consider errorswhich are induced maliciously through worms, viruses, and direct manipulation. let me not carry this furtherexcept to note that while we are concerned with the difficulty and cost in developing software, the larger problemmay be in the cost of using the software. and this cost may be in terms of money, time, property and human life.there are several elements required to solve this problem. the first is education. that is, we must providemechanisms to assure that operators and users of information systems understand the basic processes beingautomated through the system. the second is in providing adequate means and methods for identifying therequirements for the system and in deriving a correct specification for development. the third is in providingadequate testing and prototyping. the fourth, and not final, element is in developing systems which maintain theintegrity of both software and information.appendix b92
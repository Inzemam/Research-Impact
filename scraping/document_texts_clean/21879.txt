detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/21879privacy research and best practices: summary of a workshopfor the intelligence community66 pages | 8.5 x 11 | paperbackisbn 9780309389198 | doi 10.17226/21879emily grumbling, rapporteur; committee for a workshop on privacy for theintelligence community: emerging technologies, academic and industry research,and best practices; computer science and telecommunications board; division onengineering and physical sciences; national academies of sciences, engineering,and medicineprivacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. emily grumbling, rapporteur  computer science and telecommunications board division on engineering and physical sciences  privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.the national academies press  500 fifth street, nw washington, dc 20001 this activity was supported by the office of the director of national intelligence, under contract no. 201414041100003001. any opinions, findings, conclusions, or recommendations expressed in this publication do not necessarily reflect the views of any organization or agency that provided support for the project. international standard book number13: 9780309389198 international standard book number10: 0309389194 digital object identifier: 10.17226/21879 additional copies of this workshop summary are available for sale from the national academies press, 500 fifth street, nw, keck 360, washington, dc 20001; (800) 6246242 or (202) 3343313; http://www.nap.edu. copyright 2016 by the national academy of sciences. all rights reserved. printed in the united states of america. suggested citation: national academies of sciences, engineering, and medicine. 2016. privacy research and best practices: summary of a workshop for the intelligence community. washington, dc: the national academies press. doi:10.17226/21879. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. the national academy of sciences was established in 1863 by an act of congress, signed by president lincoln, as a private, nongovernmental institution to advise the nation on issues related to science and technology. members are elected by their peers for outstanding contributions to research. dr. ralph j. cicerone is president. the national academy of engineering was established in 1964 under the charter of the national academy of sciences to bring the practices of engineering to advising the nation. members are elected by their peers for extraordinary contributions to engineering. dr. c. d. mote, jr., is president. the national academy of medicine (formerly the institute of medicine) was established in 1970 under the charter of the national academy of sciences to advise the nation on medical and health issues. members are elected by their peers for distinguished contributions to medicine and health. dr. victor j. dzau is president. the three academies work together as the national academies of sciences, engineering, and medicine to provide independent, objective analysis and advice to the nation and conduct other activities to solve complex problems and inform public policy decisions. the academies also encourage education and research, recognize outstanding contributions to knowledge, and increase public understanding in matters of science, engineering, and medicine.  learn more about the national academies of sciences, engineering, and medicine at www.nationalacademies.org. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.iv other recent reports of the computer science and telecommunications board bulk collection of signals intelligence: technical options (2015) interim report on 21st century cyberphysical systems education (2015) a review of the next generation air transportation system: implications and importance of system architecture (2015) telecommunications research and engineering at the communications technology laboratory of the department of commerce: meeting the nation™s telecommunications needs (2015)  telecommunications research and engineering at the institute for telecommunication sciences of the department of commerce: meeting the nation™s telecommunications needs (2015) at the nexus of cybersecurity and public policy: some basic concepts and issues (2014) emerging and readily available technologies and national security: a framework for addressing ethical, legal, and societal issues (2014)  future directions for nsf advanced computing infrastructure to support u.s. science and engineering in 20172020: an interim report (2014) interim report of a review of the next generation air transportation system enterprise architecture, software, safety, and human factors (2014) geotargeted alerts and warnings: report of a workshop on current knowledge and research gaps (2013) professionalizing the nation™s cybersecurity workforce? criteria for future decisionmaking (2013) public response to alerts and warnings using social media: summary of a workshop on current knowledge and research gaps (2013) computing research for sustainability (2012) continuing innovation in information technology (2012) the safety challenge and promise of automotive electronics: insights from unintended acceleration (2012, with the board on energy and environmental systems and the transportation research board) the future of computing performance: game over or next level? (2011) public response to alerts and warnings on mobile devices: summary of a workshop on current knowledge and research gaps (2011) strategies and priorities for information technology at the centers for medicare and medicaid services (2011) wireless technology prospects and policy options (2011) achieving effective acquisition of information technology in the department of defense (2010) critical code: software producibility for defense (2010) improving state voter registration databases (2010) proceedings of a workshop on deterring cyberattacks: informing strategies and developing options for u.s. policy (2010) toward better usability, security, and privacy of information technology: report of a workshop (2010) limited copies of cstb reports are available free of charge from computer science and telecommunications board national academies of sciences, engineering, and medicine keck center of the national academies 500 fifth street, nw, washington, dc 20001 (202) 3342605/cstb@nas.edu www.cstb.org privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.v committee for a workshop on privacy for the intelligence community: emerging technologies, academic and industry research, and best practices fred h. cate, indiana university, chair frederick r. chang, southern methodist university tadayoshi kohno, university of washington susan landau, worcester polytechnic institute helen nissenbaum, new york university staff emily grumbling, program officer, computer science and telecommunications board (cstb) jon eisenberg, director, cstb shenae bradley, administrative assistant, cstb elizabeth euller, program assistant, board on energy and environmental systems chris jones, financial manager, air force studies board privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. vi computer science and telecommunications board farnam jahanian, carnegie mellon university, chair luiz andré barroso, google, inc.  steven m. bellovin, columbia university robert f. brammer, brammer technology, llc edward frank, brilliant cloud & lime parity seymour e. goodman, georgia institute of technology laura haas, ibm corporation mark horowitz, stanford university michael kearns, university of pennsylvania robert kraut, carnegie mellon university  susan landau, worcester polytechnic institute peter lee, microsoft corporation  david e. liddle, us venture partners  fred b. schneider, cornell university robert f. sproull, university of massachusetts, amherst john stankovic, university of virginia john a. swainson, dell, inc. ernest j. wilson, university of southern california katherine yelick, university of california, berkeley staff jon eisenberg, director  lynette i. millett, associate director  virginia bacon talati, program officer shenae bradley, administrative assistant janel dear, senior program assistant emily grumbling, program officer renee hawkins, financial and administrative manager  herbert s. lin, chief scientist (emeritus) for more information on cstb, see its website at http://www.cstb.org,  write to cstb at national academies of sciences, engineering and medicine, 500 fifth street, nw, washington, dc 20001, call (202) 3342605, or email the cstb at cstb@nas.edu. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.vii acknowledgment of reviewers this workshop summary has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published workshop summary as sound as possible and to ensure that the workshop summary meets institutional standards for objectivity, evidence, and responsiveness to the project™s charge. the review comments and draft manuscript remain confidential to protect the integrity of the study process. we wish to thank the following individuals for their review of this workshop summary: alessandro acquisti, carnegie mellon university, fred h. cate, indiana university, jennifer glasgow, acxiom, and robert f. sproull, university of massachusetts, amherst. although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the views presented at the workshop, nor did they see the final draft of the workshop summary before its release. the review of this workshop summary was overseen by samuel h. fuller, analog devices, inc., who was responsible for making certain that an independent examination of this summary was carried out in accordance with institutional procedures and that all review comments were carefully considered. responsibility for the final content of this summary rests entirely with the author and the institution. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.ix contents  1 overview 1 recurring themes, 1 defining privacy, 2 rapidly changing technologies, 2 moving beyond legal compliance, 2 transparency and trust, 3 organizational best practices, 3 regulation and oversight, 4 privacy research results, challenges, and needs, 4 individual preferences and the privacy paradox, 5 privacy and society, 6 2 workshop introduction 7 welcome, 7 background and context from the intelligence community, 7 3 privacy implications of emerging technologies part išpanel summary 9 remarks from panelists, 9 panel discussion, 11 emerging technologies, 11 user perceptions and influence, 12 open discussion, 13 4 privacy implications of emerging technologies part iišpanel  14 summary remarks from panelists, 14 panel discussion, 16 unintended consequences of data collection and use, 16 emerging technologies with potential consequences, 16 the evolution of multiparty interaction with data, 17 collection of data about one user that reveals information about someone else, 17 industry practice as a potential for the ic, 18 defining privacy, 18 open discussion, 19 challenges around control and use frameworks, 19 best technical practices, 19 5 social science and behavioral economics of privacyšpanel summary 21 remarks from panelists, 21 panel discussion, 24 recent examples of tipping points, 24 anticipating future tipping points, 25 privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.x contents improving trust, 25 open discussion, 27 public perceptions and trust, 27 equality, discrimination, and consumer profiling, 28 privacy research, 29 6 best practices and ethical approaches for data collection  31 and usešpanel summary remarks from panelists, 31 panel discussion, 34 open discussion, 35 regulations and internal standards, 35 privacy in the context of organizational missions, 36 transparency, oversight, and trust, 36 7 wrapupšpanel summary 39 closing, 41 appendixes a workshop statement of task 45 b workshop agenda 46 c biographical sketches 48 d acronyms and abbreviations 56 privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 1 1  overview the computer science and telecommunications board (cstb) convened a workshop on july 2122, 2015, to advance dialogues on privacy between technical and policy staff of the intelligence community (ic) and outside experts from academia and the private sector. cstb is a standing board of the national academies of sciences, engineering, and medicine. the workshop was sponsored by the office of the director of national intelligence (odni). to conduct this workshop, a workshop steering committee was appointed to identify potential speakers and design the workshop agenda. the committee and academies staff worked with odni to invite staff from ic agencies. approximately 40 participants, including the steering committee, invited panelists, ic staff and officials, and academies staff, participated in the 1½day workshop held in washington, d.c. this report has been prepared by the workshop rapporteur as a factual summary of what occurred at the workshop. the steering committee™s role was limited to planning and convening the workshop. the views contained in the report are those of individual workshop participants and do not necessarily represent the views of their employers, the workshop participants as a whole, the steering committee, the academies, the sponsor, or any other affiliated organizations. the workshop was designed around the following three major areas: 1. privacy implications of emerging technologies,  2. public and individual preferences and attitudes toward privacy and the social science and behavioral economics of privacy, and  3. ethical approaches to data collection and use. two panels were devoted to the first topic, one panel was devoted to the second and one to the third.  the workshop was designed to be as interactive as possible, with an emphasis on discussion and engagement rather than lengthy presentations. opening remarks were delivered by fred h. cate, workshop steering committee chair and c. ben dutton professor of law at indiana university, and alexander w. joel, civil liberties protection officer, odni. each panel was moderated by a member of the steering committee, with the following format: panelists each presented 5 minutes of opening remarks, participated in a moderated panel discussion, and then engaged in open discussion with all workshop participants. the workshop concluded with a final wrapup panel, during which participants summarized and discussed key points and reflections from the proceedings.  recurring themes workshop discussions focused on privacy implications of various technologies and practices, individual privacy preferences and behaviors, privacy policies and practices of organizations, and the broader societal impacts of privacy. in particular, the invited speakers from academia and the private sector each provided his or her expertise and/or perspectives, with content ranging from academic research on consumer privacy privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.2 privacy research and best practices behaviors to corporate strategies for privacy assessment; lessons from academia and the private sector were often discussed in the context of the work of the ic. several themes recurred throughout the workshop. to assist the reader, recurring themes are briefly discussed below. defining privacy many participants noted that ﬁprivacyﬂ means different things to different people, and that this can vary highly with context. it was suggested that the term is often used to connote a range of associated values or principles; throughout the workshop, a range of examples emerged, such as trust, security, the right to be forgotten, freedom, and anonymity. one panelist identified a common definition of privacy as ﬁthe ability to control what happens to one™s information.ﬂ others defined privacy violations, or issues, in terms of whether a given practice, policy, or action regarding personal information might be perceived as negative by a stakeholder (for example, individuals or regulators). participants discussed different conceptions of privacy, touching on both legal and philosophical considerations. several participants suggested that it may not be possible or practical to develop a universal definition of privacy largely due to its contextual nature. someone suggested that the inability to be defined is an intrinsic characteristic of privacy, and that it is something that society must struggle over. a panelist pointed out that an inability to clearly define or quantify privacy could confound those working toward protecting it. another participant suggested that privacy might be easier to understand and address by focusing on one of its associated values or principles at a time. rapidly changing technologies two of the panels focused on the privacy implications of emerging technologies, touching on the internet of things, smart and connected vehicles, mobile communications and devices, biometrics, health information technology (it), cloud/edge computing, big data analytics (data mining, aggregation, etc.), and online advertising. it was noted that computing technologies have become ubiquitous, and that there are more and more ways and places that data are being collected. one of the panelists noted a shift in some emerging technologies from privacy by trust (where individuals must have faith that technology services will not misuse their data) to privacy by design (where privacy is considered during every phase of the design process to minimize potential privacy issues), citing the evolution of approaches to smart and connected vehicles. in general, participants noted that privacy implications of emerging technologies can be hard to anticipate. in particular, a panelist pointed out that transformative technologies are often fundamentally new, so it can be hard to predict how they will be used and what privacy implications could emerge. one of the panelists described a tool developed in her research group to help software developers identify potential privacy and fairness issues in their code. moving beyond legal compliance multiple participants suggested that organizational compliance with existing laws and regulations around data practices is not sufficient to protect privacy and/or preserve public trust in an organization that works with potentially sensitive data. laws and regulations take time to create, so they often lag behind technological advances. several participants suggested that the fact that a practice or action is not illegal does not make it acceptable; privacy is defined by values, not the laws that aim to uphold them, and people have reacted negatively to perceived as well as actual privacy violations. multiple participants suggested that privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.overview 3 organizations must develop and continuously adapt their own internal policies and practices to protect privacyšbeyond those that are legally mandatedšin order to be effective and maintain the trust of their stakeholders and the public. a participant suggested that the public wants to see evidence that their data are treated with care and respect. transparency and trust many workshop participants suggested that transparency is critical for building trust in an organization or a technology. several panelists suggested that individuals may be more likely to trust a given tool or service if they are (1) provided contextual information on how it works and how it uses individuals™ data, or (2) given more control over this use. it was also pointed out that, due to the generally secret nature of its mission, the ic likely does not have access to the same transparencyenabling tools or mechanisms available to private sector organizations. several participants noted that transparency does not necessarily require direct disclosure of an organization™s practices or the specific data that it is using. for example, an organization could provide illustrative rather than actual examples of its practices, or it could provide transparency to a trusted thirdparty or oversight body, who might then provide assurance to the public that a given practice is considered and appropriate. there was also discussion about how public perception impacts trust. a participant pointed out that transparency about a given practice can generate trust, and make individuals more likely to give an organization the benefit of the doubt in future cases.  several participants implied that trust can be justified or misplaced, and productive or undermining to privacy. at least one participant raised the issue of ﬁpseudotransparency,ﬂ when individuals falsely believe (or are led to believe) that they have an accurate understanding of (and/or control over) how their data are being used, which could lead to misguided trust and complacency. another participant suggested that publicity about pseudotransparency could potentially lead to public outcry. multiple participants suggested that building trust is not simple, and that it takes time. many suggested that an organization can build trust by being more transparentšfor example, about how data are used and the value they generate (to consumers or society), steps taken to protect privacy, and oversight mechanisms. several participants suggested that building trust requires a longterm commitment to clear and accurate communication, both within an organization and externally. organizational best practices there was much discussion about strategies that organizations have taken, or might take, to protect privacy and improve trust. several participants from the private sector noted that it can take time for an organization to develop the rigorous data management practices needed to protect privacy and build trust with users or constituents. they suggested that the process is iterative, and that an organization™s practices can develop and improve with time through sustained effort, evaluation, engagement with stakeholders, and adjustment. multiple participants pointed out various factors, such as changes in technological capabilities, legal requirements, or individuals™ privacy expectations, that make it difficult to anticipate potential privacy challenges or concerns. several participants suggested that privacy is a moving target, and that an organization must be willing to continuously revisit, evaluate, and adapt its practices to best accommodate the changing privacy landscape.  there was some discussion of how organizations might operationalize privacy decisionmaking. several participants discussed the strategy of asking whether revealing a given data practice would embarrass the organization. several participants from the private sector noted the contextual nature of privacy and suggested that each decision must be evaluated individually, with consideration for all stakeholder impacts. there was some discussion of important roles within an organization. for example, individuals can be designated to promote privacy as a core value beyond simple legal compliance, propose alternative strategies, privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.4 privacy research and best practices and anticipate future challenges. organizations can establish internal mechanisms for privacy oversight. several participants pointed out that some organizations have limited resources; a panelist suggested that a small core group of privacy professionals might be augmented by designating one or more people within each operational unit as privacy liaisons to the core. regulation and oversight there was some discussion of internal vs. external regulation and oversight of government and private sector organizations™ data privacy practices. several participants suggested that internal regulation could be more responsive, agile, and thorough than external regulation. another participant suggested that external regulations might prompt organizations to focus on compliance rather than outcomes, and also might lag behind current technologies. others suggested that internal regulation is subject to bias toward an organization™s own interest, and that external regulation is necessary for transparency. many participants noted that external input, guidance, or oversight could help to bring balance, and to build trust among those external stakeholders whose privacy is at stake.  privacy research results, challenges, and needs one of the functions of the workshop was to expose members of the ic to outside research related to privacy. multiple panelists discussed their own research, as described below.  fuming shih, senior product manager, oracle cloud, discussed his research around smart phone user privacy preferences and behaviors. steven m. bellovin, percy k. and vidal l. w. hudson professor of computer science, columbia university, noted that his research involves creating a new formal definition of privacy and the harms that result from various activities. carl gunter, professor of computer science, university of illinois, provided insights from his work on privacy and security in health it. roxana geambasu, assistant professor of computer science, columbia university, discussed her research aimed at increasing privacy online, including the development of tools to help users understand how their personal data are tracked and used, and to help programmers detect ﬁprivacy bugsﬂ while developing applications. idris adjerid, assistant professor of management, university of notre dame, discussed his research on the economics of privacy with a focus on behavioral economics. jessica staddon, associate professor of computer science, north carolina state university, discussed some of her work related to user perceptions of transparency tools. joseph turow, robert lewis shayon professor of communication at the annenberg school for communication, university of pennsylvania, discussed his survey research related to digital relationships and surveillance in the context of marketing and retailing, including results of a recent survey addressing consumer attitudes about private sector tracking and collection of their data. workshop discussions also addressed the broader privacy research landscape, and multiple participants highlighted challenges associated with work in this area. several suggested that massive private sector data sets are generally underutilized for research purposes, probably because of disincentives for such research in the private sector. one participant suggested that academic researchers tend to be limited to small data sets and generally lack access to privatesector data. several participants suggested that some studies on privacy preferences and behaviors have yielded conflicting results. it was pointed out that surveys and studies must be carefully designed, and results and privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.overview 5 individual behaviors carefully interpreted, in order to yield meaningful conclusions. a participant identified the need for a culture of repeatability, and for consistency and objectivity in measurement. several participants noted existing strategies for such design and interpretation of experiments and surveys, and reiterated the highly contextual nature of privacy. several participants called attention to areas in which little research has been done and where more would be helpful, including the following: how individuals feel about their own privacy vs. that of others, whether secrecy undermines trust, privacy preferences among different demographic groups, such as lowerincome populations and minority groups, and social (rather than individual) costs and benefits of privacy. several participants discussed the notion of a ﬁscience of privacy.ﬂ a member of the workshop steering committee suggested that, within this framework, grand challenge problems could be identified and data sets could be developed and shared to advance privacy research. another pondered whether such a formal framework might help organizations develop tools for operationalizing privacy decisionmaking. one participant suggested that the contextual nature of privacy could make this very difficult, and another pointed out that the field of ethics already offers a rigorous basis for deriving actionable principles. many suggested that more research about privacy is needed. a member of the workshop steering committee suggested that researchers might be able to make progress on some of the ic™s privacy challenges if given ﬁtoy problems,ﬂ or representative problems, that can be shared publicly but embody critical challenges. another member suggested that deeper engagement between the ic and academia could facilitate stronger communication of the ic™s commitment to compliance around privacy. individual preferences and the privacy paradox several participants pointed out the phenomenon of individuals who say they care about privacy but nonetheless seemingly act against their own interest, termed by one as the ﬁprivacy paradox.ﬂ possible reasons for such behavior were discussed. in particular, several participants suggested that individuals often do not have the time or knowledge to deduce the privacy implications of their actions or to learn what they may do to enhance privacy. they also may not understand how a given technology works, or what companies are doing with their data. a panelist noted that humans do not always behave rationally in the economic sense, especially when it comes to privacy decisions. it was also suggested that there may be tension between what an individual believes to be ﬁthe right thing,ﬂ and what he or she wants in the moment. another panelist suggested that an individual™s intuition or level of familiarity with a given app or service might play a large role in individual decisionmaking. several participants cautioned against the common assumption that people are comfortable giving up their data in exchange for the benefits of using a given technology or service, suggesting that this is a faulty assumption. one panelist suggested that fair tradeoffs between privacy and utility are not feasible, due to the limited number of options provided for how to use a technology or a service, and because the value of an individual™s data depends upon what other data exist that they might be combined with and how, and is thus always changing and difficult to pin down. the panelist also suggested that, for these reasons, any notion of a ﬁprivacy marketﬂ will fail. another panelist noted survey evidence suggesting that many individuals actually feel resigned to the fact that their data are being collected, and feel that this condition is simply beyond their control. several participants suggested that consumers may feel that the use of various technologies, such as the internet or mobile devices, is an allornothing proposition: either they get the convenience of these technologies while giving away data they would actually prefer not to share, or simply do not get to use the technology. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.6 privacy research and best practices there was also some discussion of the idea of ﬁtipping points,ﬂ that is, points at which individuals™ perspectives on a technology or practice shift, causing them to alter their behavior or attitudes, possibly by ceasing to use a given tool or by pushing back against a given practice. a participant suggested that tipping points likely occur at the individual rather than societal level, but that a series of minievents that trigger tipping points could cause a critical mass of individuals to change their perspectives. there was some discussion of visible events that might fall into this category, such as the abuses investigated by the church committee, the office of personnel management (opm) breach, the ashley madison breach, and the snowden disclosures. privacy and society many participants noted societal benefits of data collection and use, for example, to advance public health or national security. one participant suggested that the field of health it could be a valuable evolving case study on balancing the use of information for public good (such as disease prevention) with individual privacy. several participants suggested that while privacy is often considered an individual value, privacy itself can also have important, collective societal benefits that are not always taken into account. for example, private and anonymous voting can help promote robust democracy, and privacy can empower individuals to explore nonmajoritarian views and facilitate freedom of thought. several participants also suggested that certain demographic groups may be disproportionately impacted by privacy issues, and that such impacts may be undercounted. it was also pointed out that little research has been done on how privacy preferences vary between demographic groups. several participants suggested that more research is needed in these areas. several participants noted that recent discussions around big data and privacy have emphasized protection of privacy via control of how data are used rather than by limiting their collection. one participant suggested that privacy advocates are uncomfortable with this notion, because even the best use control policies can be changed in ways that open up pathways for unintended or harmful use of stored data: if data do not exist, none can be abused. another participant pointed out that the public seems to have accepted the practice of massive collection and aggregation of data even in the absence of a rigorous argument demonstrating that this is defensible, and suggested that there might be value in revisiting this acceptance. there was also some discussion about evolving societal values. one participant suggested that the principles underlying existing societal norms were honed over time from important societal values, and they should not simply be discarded as technology advances. a participant questioned whether these underlying values were being upended by the rapid evolution of technology. several participants cautioned against the notion that technology itself is an uncontrollable force, and suggested that we should focus not only on emerging technologies, but also on how they are deployed throughout society. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 7 2  workshop introduction welcome fred h. cate, chair of the workshop steering committee, opened the meeting by summarizing the goals and structure of the workshop. he noted that the workshop was organized by the national academies of sciences, engineering, and medicine and sponsored by odni in order to help advance dialogue between members of the ic and outside experts and practitioners around the topic of privacy. specifically, the workshop was designed to address (1) privacy challenges presented by new technologies, (2) ways of understanding the public™s behavior and attitudes about data collection and use, and (3) methodologies for making decisions about data collection and use that go beyond mere compliance with the law. a particular goal of the workshop was to address challenges that might impinge on trust around personal privacy, and strategies for anticipating or managing these challenges in ways that might enhance trust. background and context from the intelligence community alexander w. joel, civil liberties protection officer, office of the director of national intelligence, noted that the group had convened to help improve understanding of how to protect the security and privacy of u.s. citizens and people around the world. he pointed out that 2015 was the 40th anniversary of the church committee,1 whose examination of past abuses in the intelligence arena led to major reforms. he noted that the committee did conclude that certain intelligence activities serve proper and necessary ends of the government, and should be preserved under effective restraints. he suggested that the ic™s current governance framework is largely defined by the changes made in response to the church committee™s 1976 report, including the establishment of congressional oversight committees and strengthened legal oversight within the executive branch. he added that the foreign intelligence surveillance act (fisa) court currently provides oversight from the judicial branch as well. joel noted that, while much has since changed in the world, members of the ic still believe that the existing rules and governance framework provide effective guidance on maintaining balance between security and privacy. he noted that some in the public clearly disagree, and that this divergence of views was part of what the workshop was convened to address. he read a quote from president obama: 1 the united states senate select committee to study governmental operations with respect to intelligence activities, chaired by senator frank church, was formed in 1975 in response to ﬁgreat public concern that the congress take action to bring the intelligence agencies under the constitutional framework,ﬂ according to senator church™s letter of transmittal. the committee™s report was released in april 1976 (select committee to study governmental operations with respect to intelligence activities, 1976, intelligence activities and the rights of americans book ii: final report of the united states senate select committee to study governmental operations with respect to intelligence activities, washington, d.c.: u.s. government printing office). privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.8 privacy research and best practices those who are troubled by our existing programs are not interested in repeating the tragedy of 9/11, and those who defend these programs are not dismissive of civil liberties. the challenge is getting the details right, and that is not simple.2 he pointed out that the rapid pace of technological change makes the challenge more difficult, but that technology concerns are not new. he quoted justice brandeis™s famous 1928 dissent in olmstead v. united states:3 the progress of science in furnishing the government with means of espionage is not likely to stop with wiretapping. ways may someday be developed by which the government, without removing papers from secret drawers, can reproduce them in court, and by which it will be enabled to expose to a jury the most intimate occurrences of the home.4 a similar caution appeared in the church committee™s report: in an era where the technological capability of government relentlessly increases, we must be wary of the drift toward ﬁbig brother government.ﬂ the potential for abuse is awesome and requires special attention to fashioning restraints, which not only cure past problems, but anticipate and prevent the future misuse of technology.5 joel observed that people often focus on current problems, rather than thinking ahead. he then discussed some of the policy challenges associated with technological change. many current policies or rules were put into place years agoševen centuries ago, in the case of the united states constitution. he suggested that this does not mean such rules are bad, but just that we need to figure out how best to apply them. he suggested that rules and policies tailored to current technologies are unlikely to succeed; policy makers are not experts in technology, and the policy process is relatively slow compared to the pace of technology change. how society should manage the inevitable gap between the rules in place and current technological practices is an open question. joel noted that insights into how people value privacy in different contexts would help to enhance understanding of privacy implications of different technologies. discussion of ethics and best practices in the private sector and elsewhere in the government for big data collection, use, and analysis, would help inform the ic and others about how to fashion responses to emerging privacy challenges. joel also discussed the value of transparency and of dialog both between the ic and the public, and between members of the technology and policy communities in advancing dialogues and making progress on privacy. he noted that more transparency will enable the ic to better inform and explain its role to the public, including the rules it obeys and how, and the nature of and need for its activities. he also suggested that technologists and policy experts often share the same goals, whether or not they always speak the same language. 2 executive office of the president, ﬁremarks by the president on review of signals intelligence,ﬂ january 17, 2014, https://www.whitehouse.gov/thepressoffice/2014/01/17/remarkspresidentreviewsignalsintelligence. 3 olmstead v. united states, 277 u.s. 438 (1928). 4 mr. joel noted that, while it may sound as if justice brandeis was predicting the cloud, he was actually talking about developments in the psychic sciencesša fact about this opinion that is not commonly discussed. 5 select committee to study governmental operations with respect to intelligence activities, 1976, intelligence activities and the rights of americans book ii: final report of the united states senate select committee to study governmental operations with respect to intelligence activities, washington, d.c.: u.s. government printing office, p. 289.  privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 9 3  privacy implications of emerging technologies part išpanel summary remarks from panelists susan landau, panel moderator and professor of cybersecurity policy at worcester polytechnic institute, launched the session by observing that views around the privacy significance of metadata have changed in the past few years; people have become more aware that metadata can actually enable the extraction of sensitive information. she provided an example, noting that google captures information about how users swipe their android phones. analysis of this metadata can inform improvements to the product™s design, but it could also be used to infer mood. she observed that the many potential privacy implications of metadata illustrate the challenges posed by emerging technologies. landau, as moderator, then introduced the following panelists and gave each of them 5 minutes for opening comments: fuming shih, senior product manager, oracle cloud; tao zhang, distinguished engineer, cisco systems; mark mcgovern, ceo, mobile system 7; and, lee tien, senior staff attorney and adams chair for internet rights, electronic frontier foundation. fuming shih presented some results from his research on user privacy preferences and behaviors with smart phones, including the factors that affect individual preferences for information disclosure to mobile apps, and on ways of making privacy conflicts more visible to users. shih introduced the concept of privacy ﬁtipping pointsﬂšindividual user experiences or thresholds that precipitate a shift in thinking about a given technology and can alter user privacy preferences and behaviors. he cited his own experience with the google now feature on an android smart phone, in which the phone notified him, unprompted, that it was time to head home for the day, having ﬁlearnedﬂ his commuting habits by tracking his past behaviors. this experience caused him to rethink the amount of data he was comfortable sharing.  shih then summarized several useful themes from his research findings about individuals™ behaviors around privacy. 1. context matters. the details of a given circumstance are critical to understanding why people disclosešor do not discloseštheir information. 2. trust matters more. shih™s research findings suggest that trust outweighs context in individual decisionmaking. people often rely on their intuitive trust in a device or app when deciding whether to share their information. he also found that a user™s level of familiarity with an app influenced datasharing decisions more than the content of the app™s privacy policy, suggesting that improvements to privacy policies may not have much effect. shih also suggested that revealing more information on the context of data use may make users feel like they have more control and make them more willing to trust an app or device. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.10 privacy research and best practices 3. privacy is complicated. shih found that reasons for disclosingšor not disclosingšinformation vary between individuals. he suggested that understanding why people disclose is more important than what people disclose. tao zhang started by noting that there are currently approximately 12 billion networked ﬁthingsﬂ worldwide. analysis by cisco and others suggests that there will be more than 50 billion devices connected to the internet by 2020, in industrial systems, manufacturing plants, public services, connected vehicles, and consumer devices. there is even interest in embedding passive or active communication devices into people™s clothing. zhang expects that the world will become connected beyond what many of us can imagine today. this means that whenever we (or our devices) communicate, we will leave traces of ourselves behindšwhether knowingly or unknowingly. information will be left in many places, providing new opportunities for collection, and making it more difficult to enforce data collection and use policies. he noted that people are becoming increasingly aware of the privacy implications of these technologies; they want to do something about it, but run into the challenge of competing requirements. for example, privacy and security can sometimes conflict; providing privacy on a network means hiding a user™s identityšor concealing his tracesšwhich enables malicious actors to hide as well.  mark mcgovern began by making an analogy between security and privacy, noting that both can be difficult to define, and can mean different things to different stakeholders. he went on to discuss potential privacy implications of security technologies. in particular, enterprises are increasingly relying on mobile devices and storing data in the cloud. the increased number of access points creates uncertainty about who is using systems, increasing the need to monitor user activity. this trend could have the effect of tracking authorized users in a way they would not appreciate. mcgovern described the importance of balancing the needs of customers, investors, analysts, and team members when taking steps to improve security; security tools are often built before rules of acceptable practice have been established, necessitating a proactive approach to product support and buildout based upon the needs of investors, customers, users, and the public. he suggested that similar considerations likely apply to privacy as well. lee tien addressed three main points: (1) the nature of privacy, (2) privacy as a social product, and (3) trust and transparency. first, he identified privacy as an ﬁessentially contested topic,ﬂ1 and suggested that itšnot unlike beauty or justicešis not truly capable of being defined. he suggested that it is in fact a constitutive feature of privacy that its meaning must be debated over and developed by societyšit cannot simply be prescribed by a determined formula. he pointed out that the constitution does not say much about privacy, though it is appealed to in the first and fourth amendments, consistent with this picture. he suggested that there are many different conceptions of privacy. within the legal field, conceptions relate to the first, fourth, and fifth amendments, associational privacy, and decisions in cases such as national association for the advancement of colored people v. alabama. tien stated that all of these ideas are relevant to privacy discussions both within and outside the ic, but pointed out that people have many different worldviews and perspectives. tien then identified the notion of privacy as a social product, referencing the ideas of sociologists such as howard becker and erving goffman. he suggested that privacy is something that people create over time, based upon experiences, with the resources available. for example, an individual may create privacy by putting a letter into an envelope, or by going into a phone booth and closing the door. he identified encryption as a contemporary resource for the social production of privacy in a computerized world. he suggested that  1 w. bryce gallie, 1955, essentially contested concepts, proceedings of the aristotelian society, high wycombe, u.k.: harrison & sons. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.privacy implications of emerging technologies part i 11 the use of a resource is necessary: without the ability to mark a boundary, it is hard to achieve or demonstrate an expectation of privacy. tien echoed the sentiment that transparency is essential to any meaningful trust relationship, and suggested that transparency is important both before and after a privacy incident. he pointed out that a lack of knowledge at both the individual and societal levels makes it difficult for individuals to assess privacy impacts. for example, consumers often believe that the existence of a privacy policy means that their data are being protected against use by third parties, which is a false notion. he suggested that public knowledge of data practices is necessary for assessing and creating privacy. panel discussion landau led the panel in discussion of privacy implications of some emerging technologies, as summarized below. emerging technologies landau asked the panelists to comment on the privacy implications of technologies such as the internet of things, mobile devices, and notifications.  mcgovern pointed out that emerging technologies such as the internet of things are changing rapidly. as consumers and developers learn more about these technologies, their privacy expectations will change. the significant lag between the development and adoption of new technologies poses a large risk that developers will fail to anticipate privacy implications and user expectations around these technologies. zhang noted that concern has risen around the potential for correlating data collected from different streams (as in the internet of things) to identify information about a given user. such concerns generally prompted users to want more control over their data, and have helped spur development of enhanced user control mechanisms, such as user options for encryption and data storage schemes. shih questioned whether enhanced user control options would actually enhance privacy. he pointed out that some smart phones have hundreds of permissions or control options, resulting in a complex and poorly understood set of options. in his work on the android system, he found that privacy choices are also often presented at bad times. for example, users are unlikely to read the permissions requirements carefully after downloading an app that they want to use immediately. tien added that many people are truly surprised to learn how much can be inferred about an individual™s intimate details by analyzing seemingly innocuous data about their online purchases and other activities. he noted a huge gap between the technologically literate and the general public in their level of understanding of privacy risks, and that many have been surprised to learn of this gap, even in the corporate world. according to zhang, over the past 20 years, computing has shifted from local processing to the cloud, and most recently to ﬁedgeﬂ models, which combine local and cloud computing and storage to minimize latency and increase performance. this has resulted in more distributed computing and storage, which can have additional privacy impacts. in particular, the increase in local storage minimizes the amount of data that are shared, but also means that personal information must be protected in more locations. mcgovern addressed notifications, suggesting that they can expose some of the lessvisible functions of connected objects and devices. unexpected notifications can make people realize that systems have learned something about them. tien distinguished between notifications generated on a given device using locally stored data and notifications generated based upon data from somewhere on the cloud. while the privacy implications in each case are different, users may not be able to tell the difference. shih described results suggesting that notifications about data collection may prompt a portion of users to opt out of a service entirely, but otherwise have minimal impact on most users™ behavior. zhang noted several emerging privacy concerns about connected vehicles. the navigation systems may store data in the vehicle that could be related to driving habits, and potentially be correlated with geolocation, privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.12 privacy research and best practices time, and other information. the vehicles™ networks provide potential pathways for accessing these data. he pointed out that there has been much debate about ownership of this sort of data. some say that all of a vehicle™s data should be considered private, but others note that much of the collected data would enable car companies to improve vehicle function and maintenance. this is an open question, but much user data have been collected already. tien pointed out some emerging policy challenges with respect to connected vehicles. for example, some in california have proposed replacing the gas tax with a mileagebased tax that could potentially be determined either by tracking vehicle location over time or, alternatively, via some datafree method. he noted that perspectives about geolocation data have evolved over the past 7 years or so, owing partly to empirical studies showing how easy it can be to identify individuals based upon their physical paths. in this context, courts have found a reasonable expectation of privacy under the fourth amendment, in contrast to past decisions finding that individuals do not have a reasonable expectation of privacy if they are visible on a public road. such shifts underscore the challenge of determining what counts as private. user perceptions and influence zhang explained that privacy has traditionally been generally supported by trustšwe expect that data collectors will not abuse the information they hold. however, the public does not always consider this sufficient. the general idea of ﬁprivacy by design,ﬂ where privacy is considered at all phases of the engineering process, is becoming more common in certain contexts.2 for example, he suggested that customer concerns have been the major driver of joint work between industry and the department of transportation on designing privacypreserving security systems for cars. specifically, collision avoidance systems will require communication among a critical mass of vehicles to be effective, making it likely that such connectivity will be mandated in the future, prompting privacy concerns. efforts are now under way to build communications systems that will not exchange data that could enable tracking of individual vehicles. shih suggested that most phone users seem to have the false notion that their data is not really at riskšfor example, thinking that only celebrities will have their photos stolen and leaked. he suggested that people who do have concerns have no direct channel to voice them to phone designers. zhang suggested that the magnitude of users™ reactions to privacy issues depends on both the perceived value of a given product and the perceived user control over its functions. for example, the value of products such as smart phones is perceived to be high relative to the potential risk of information leaks; however, if significant risks become visible, or if users perceive that they do not really have choices, a tipping point can occur. users may or may not provide feedback when they have privacy concerns, but often simply abandon a product. shih added that his research suggests that both the accurate perception of control and the illusion of control make users more willing to stick with a product or situation. tien pointed out that receipt of customer concerns can prompt companies to simply reduce the visibility of the problematic functions, and postponing these solutions can result in much larger problems down the road. mcgovern pointed out that future complications also arise due to the ambition of technology developers; indeed, it can be difficult to anticipate the societal impacts of audacious technologies. he also suggested that regulated industries are very aware of privacy issues, and are often more sophisticated in their questions about the risk of storing privacysensitive information. he suggested that other sectors, such as manufacturing, pharmaceuticals, and biotech, may be more focused on security than on enduser privacy. tien added that there are many companies, such as data brokers, whose customers are not the individual end users, so individuals™ interests or privacy concerns may not be very visible to them. however, widely held public perceptions and bad publicity can create some pushback. attention from legislators can also move companies to make privacy improvements. 2 for background on this concept, see a. cavoukian, 2009, ﬁprivacy by design: the 7 foundational principles,ﬂ information and privacy commissioner of ontario, canada, https://www.ipc.on.ca/english/privacy /introductiontopbd/. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.privacy implications of emerging technologies part i 13 the panel also touched on the siloing of data as a technical control, challenges with verification of such controls, and the complement and interplay between legal and technical controls in providing accountability. open discussion a participant cautioned against accepting the notion that users are willing to give up privacy in exchange for value. one of the panelists pointed out that true tradeoffs may not even be possible because there is not a sufficient range of choices to enable a fair trade. the group further discussed notions of trust. a panelist suggested that organizations earn trust not just by following the rules at hand, but by also acting with consideration and restraint. another panelist suggested that people care most about privacy only after something bad has happened, and that explaining what happened and demonstrating how the problems are being addressed could help to strengthen trust. other participants pointed out that a user™s trust in a technology may reflect an accurate understanding of a technology™s protections, but it could also reflect positive past experiences with a technology, ignorance about its underlying functions, or resignation to a lack of other options. in that sense, trust can be positive or negative, justified or misplaced, and productive or undermining to privacy. the group went on to discuss smart vehicles. one participant noted that vehicles already contain dozens of computers and collect a significant and increasing amount of data. another also noted that the general public may not understand the implications or details of this or future collection, even if notice of such collection is given. some participants commented on incentives in the private sector. one participant pointed out that a firm may have an incentive to maximize its ability to collect data while alleviating customer concern by providing superficial elements of control and notice. another participant noted that the potential for individuals to be satisfied with superficial controls is a serious concern. one of the panelists noted that companies are focused on their business goals (often with good intentions), and that their attention to privacy will similarly be driven by economics. greater attention to privacy might be encouraged by a reduction in the cost of doing the right thing, the presence of regulatory oversight, or increasing the value of privacyrespecting products. another participant suggested that services and businesses whose competitiveness is tied strongly to their reputations are likely to be more attuned to privacy, and suggested that the companies have more technological capacity and expertise than their governance organizations. it was noted by a participant that the ic does not have at its disposal the same tools that companies have for building trust with individuals, such as notifications and the ﬁright to tinkerﬂ with technologies, and has had varying success in communicating either its privacyprotecting measures or the benefits the public derives from intelligence activities. the participant asked how the ic might best approach the question of trust with the public. a panelist agreed that the ic and law enforcement have a limited set of tools for building trust; for example, individual notification of the subject of an investigation is not an option, except after the fact. he suggested that one might consider the proxies for the citizen in different contexts, for example the congress, and work with them on enhancing disclosure and transparency. but he also suggested that the congressional oversight process itself may need reevaluation. another panelist suggested that trust could be gained by providing representative or model examples of how data are used, so that the public can understand the safeguards that are in place today, the privacy impact of different practices, and how those practices contribute to the public good. one of the panelists added that the government and the private sector play different roles in society, and one should not expect practices that are acceptable in the private sector to also be acceptable in the government sector. finally, a panelist asked how society characterizes ﬁsufficient privacy.ﬂ he suggested that, without an answer, it will be hard to know whether we are on the right path. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 14 4  privacy implications of emerging technologies part iišpanel summary remarks from panelists tadayoshi kohno, the shortdooley professor of computer science and engineering at the university of washington, began the session by noting that the next panel would also focus on emerging technologies, with an emphasis on analytics and the cloud. he encouraged the participants to prepare questions to pose to panelists during the open discussion session. kohno, as moderator, then introduced the following panelists and gave each of them 5 minutes for opening comments: carl gunter, professor of computer science, university of illinois; roxana geambasu, assistant professor of computer science, columbia university; steven m. bellovin, percy k. and vidal l. w. hudson professor of computer science, columbia university; and, james l. wayman, research administrator, san jose state university. carl gunter discussed privacy implications of the growing use and collection of digital health data. he distinguished between ﬁhealth careﬂ technologies (tools for diagnosis and treatment of disease) and ﬁhealthﬂ technologies (the quickly growing market of tools for disease prevention and encouragement of healthy habits, such as the fitbit), and suggested that these two areas may be moving toward a disruptive convergence. gunter described emerging capabilities in analysis of both structured and semistructured data, including doctor™s notes or even information from a fitbit or an apple watch, and noted that data mining of electronic health records (ehrs) has led to the identification of prescription drug risks. such capabilities could have enormous societal benefits, but they require access to large quantities of data about individuals, who may not want their records to be accessible even for such purposes. he suggested that the rapidly changing field of health it has a number of characteristics that could make it a useful laboratory for monitoring privacy trends and developments, including the following: there are many stakeholders with competing interests; regulations and rules are evolving; privacy provisions in existing laws such as the  health insurance portability and accountability act (hipaa) and the health information technology for economic and clinical health  (hitech) act were developed following much public debate and negotiation; the field is seeing increased use of distributed networks where institutions hold data to support research, but share answers to research queries on their data; analysis of health data can yield great public benefit (in the form of medical breakthroughs and advances in public health); and collection and analysis of data can pose privacy risks. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.privacy implications of emerging technologies part ii 15 in particular, the field could be a valuable, evolving case study on balancing the use of information for public good with individual rights to privacy. roxana geambasu discussed her research on increasing privacy online. her work emphasizes enabling development and design with privacy in mind, and increasing user awareness of the privacy implications of their online actions. she noted that privacy is scarce on the internet; indeed, many users are eager to share their data online and many services aggressively collect and use that information. today™s web services collect immense amounts of information, including every click and every site we visit, and mine our documents and emails. this data can be used to target ads or finetune prices, sometimes to the benefit of the user and sometimes not. users are generally unaware of how such data are used or abused by the collectors. geambasu described xray, a tool developed by her research group that can reveal how web services use personal data for targeting or personalization. it works by monitoring user inputs and outputs from these services, and identifies correlations using test accounts populated with subsets of a user™s information. she noted that the tool has proved remarkably accurate (around 8090 percent precision) with gmail, youtube, and amazon. by increasing transparency about how web services use data, tools like xray increase user awareness and, potentially, pressure on services to behave responsibly. she noted that the tool could also be of use to privacy watchdogs, such as the federal trade commission (ftc), and investigative journalists. she pointed out that the algorithms used to analyze online data can unintentionally lead to harmful, unintended, and/or unanticipated consequences, such as price discrimination. her group has also created an infrastructure called fairtest to help programmers identify privacy bugs in their applications, enabling them to avoid discriminatory or other unintended effects. geambasu suggested that the strategies embodied by these tools for enhancing online privacy could be applicable to privacy protection in other domains. steven m. bellovin noted that there are many definitions of privacy. he identified one of the most common definitions as the ability to control what happens to one™s personal information. based upon this definition, he identified the following two key types of privacy offenses: (1) using data for a purpose other than that for which they were originally collected and (2) linking data from two or more different sources. he noted that the second type of offense is a specific instance of the first. he illustrated the first offense with the example of driver™s licenses, which are intended to indicate that an individual is legally qualified to drive, but are used secondarily for boarding airplanes or entering bars. he pointed out that many bars scan driver™s licenses to verify their validity, and some actually record a patron™s name, address, and demographic data, which may itself constitute a privacy violation. bellovin went on to discuss privacy issues related to biometrics data, including fingerprints or facial patterns. he pointed out that it is difficult to control the secondary use of biometric information. for example, an individual™s image could be obtained or captured without his or her knowledge in a public place, then matched to other sources. if linked to an individual™s facebook profile, personal information about that person can be obtained, whether directly or through data analytics; for example, a student project from the massachusetts institute of technology (mit)1 showed that one can accurately infer an individual™s sexual orientation by analyzing that person™s facebook network. bellovin noted that compromised data from the recent office of personnel management (opm) breach includes information that could potentially be linked to facebook photos, leaked records from ashley madison, or other data sources to reveal sensitive information even if the other data sources contain no personally identifiable information (pii). this underscores the fact that privacy issues can arise even in the absence of pii. even without a user™s name, a web service such as netflix or amazon can build a dossier for that user. health records, even in the absence of pii, are still extremely personal and can be reidentified. 1 c. jernigan and b.f. mistree, 2009, gaydar: facebook friendships expose sexual orientation, first monday, 14(10). privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.16 privacy research and best practices he suggested that biometric data, when linked to other sources, present tremendous potential for privacy violation. he proposed that using salted hashes of biometric data might be more privacypreserving than direct use of biometric data such as a facial image or a human fingerprint. james wayman discussed two major themes: (1) meaning derived from the absence of information and (2) the privacy of members of the ic. he began by pointing out the idea from zen philosophy that the empty space between objects is just as important as the objects themselves. he carried this into the intelligence field, noting that the utility of information is often inverted from what one might expect. sometimes the absence of information reveals a lot, as when ﬁlistening in the gapsﬂ between pieces of information during the intelligence practice of traffic analysis. wayman provided a specific example: fugitive terrorists may likely be off the grid, meaning that they may be the ones for whom no communications data exists. he suggested that the ic would like to do more data reduction, but persists with data retention because it is hard to know what to throw away when the ability to recognize the absence of signal may also be important. he also suggested a need to consider not only the emerging ic technologies that could threaten privacy, but also how emerging technologies threaten the privacy of members of the ic. panel discussion unintended consequences of data collection and use bellovin suggested that unintended consequences often arise in the context of secondary use. he recalled a statistic suggesting that commercial data brokers may have more than a thousand data points on the average american. secondary use of data can lead analysts to draw spurious conclusions from observed correlations. incorporating such conclusions into hiring decisions or insurance qualifications could have unfair and detrimental consequences. geambasu suggested that unintended consequences may become increasingly significant as we move into a data exchangebased world. primary and thirdpartycollected data is obtained by fourthpartiesšdata brokersšwith whom users may never interact. data brokers hold vast quantities of data about users, the flow of which cannot be effectively tracked or managed. wayman noted that the u.s. visit2 program, through which the u.s. government collects biometric information about foreign visitors to the country, led other nations to collect biometric information from nonnational travelers. this, combined with the leak of fingerprint records from the recent opm breach, could have significant consequences for those within the intelligence community. wayman also addressed the unintended consequences of the absence of data in a datarich world. he noted that people who turn off their phones when entering an ic facility to avoid tracking might inadvertently raise a red flag. bellovin highlighted an example from world war ii, when the research of u.s. nuclear physicists ceased to be published, tipping off the soviets that their work had been taken out of the public eye. emerging technologies with potential consequences panelists identified the technologies whose privacy implications they found most worrisome. bellovin noted that he was most concerned about the potential privacy implications of remote (or involuntary) capture of biometric information, and those of machine learning, which can already arrive at sensitive correlationsšand these technologies continue to advance. gunter reiterated his concerns around the convergence between health care and health technologies. for example, in the health care sphere, the security of a wirelessly controllable defibrillator is scrutinized by  2 this program has been superseded by the office of biometric identity management program, enacted in 2013. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.privacy implications of emerging technologies part ii 17 regulators, but that of a fitness monitor like fitbit is not. he worries that the incentive to add capabilities to lowerend products could lead to a host of insecure midlevel products, such as an insulin pump that transmits information tošor is even controlled byša mobile phone, bringing with them significant security and privacy risks. wayman pointed out that iris recognition at a distance, kickstarted through the defense advanced research projects agency™s human identification at a distance program, is already commercially available and can work at a distance up to approximately 10 feet. the intelligence advanced research projects agency™s janis project is currently focusing on improving facial recognition under a variety of different conditions. the evolution of multiparty interaction with data geambasu pointed out that the online data landscape is complex, and information is tracked by many agents on a variety of websites. whether or not these trackers know a user™s name, they may have information about other sites a user has visited, or even other devices used, and may exchange cookies with others. neither users nor researchers fully understand this landscape, and all that can currently be done is to try to break the black box around such exchange. she noted that one (possibly controversial) solution would be to make such exchange of data explicitly legal and then devise an infrastructure that would ensure rigorous compliance with a set of appropriate controls. gunter added that there are similar issues around the architecture of advertising on mobile phones. specifically, on android phones, an app™s advertisers can have the same privileges as the app itself. it is thus possible for an advertiser to access a phone™s microphone or camera, the effects of which some have been trying to measure. there is a large potential for harm in this space; advertisementsupported apps are quite popular because they tend to be free, but advertisers may have access to sensitive data, such as medical information, collected by the apps themselves. bellovin reiterated the extent of online tracking, referring to a statistic that as much as 40 percent of people™s internet bandwidth goes to trackers and ads. he agreed that it is difficult even for a knowledgeable person to understand where his or her information is going. he noted that the fair information practice principles (fipps) instantiated in the privacy act of 1974 do not apply to the ic, and that u.s. law is unclear on the circumstances under which the government can purchase data from third partiesšan action that could enable circumvention of other provisions in the law. for example, under the stored communications act, communications companies cannot sell or give certain information to the government. however, there seems to be no prohibition against the government obtaining this information from a data broker.  geambasu noted that there could be value in engaging auditors to monitor and provide oversight of data practices that users cannot see. gunter asked who the auditors might be, and noted that allowing a company or the government to perform this role would raise trust issues and potential conflicts. geambasu pointed out that the financial sector has an established infrastructure, though it may not work perfectly, and suggested that a similar infrastructure could be established for auditing the web. collection of data about one user that reveals information about someone else gunter pointed out that direct consumer genomic testing results can reveal hereditary information and thus enable inference about a subject™s family members. he pointed out that medical professionals have welldefined protocols for revealing the presence of genetic markers that could have dire implications for a user™s family members, but there is no regulation or guidance on this in the directtoconsumer space. it is not uncommon for an individual to post his or her entire genetic sequence online, which could have unwanted effects on family members. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.18 privacy research and best practices gunter also discussed a study3 that addressed the notion of ownership of identity, describing one of its conclusionsšnamely, that someone who holds data about an individual actually owns that identity, whether or not it is complete or accurate. he also pointed out that an individual™s own selfidentity could be less accurate than the identity held by another party, because the individual might engage in selfdelusion. bellovin suggested that the findings of this study could be useful to workshop participants. bellovin also noted that facebook™s tagging function could enable an individual to reveal information about someone else. coincidence of location data can also be used to infer information about an individual whose behavior is linked to that of others about whom much is known. for example, machine learning correlations within a given data set have enabled identification of marital/relationship status and ethnicity of individuals about whom only location data over time had been collected. geambasu also raised the example of google glass, which can collect information on people other than its users, including through video. this and other augmented reality technologies present significant challenges to managing privacy. she pointed out that setting data access controls with mainstream technologies such as facebook is already difficult, and the management problem will likely increase significantly. industry practice as a potential model for the ic geambasu suggested that companies such as google have infrastructures for auditing access to data and maintaining data that are not in use, involving encryption, and minimization and compartmentalization of access. such strategies, along with anonymization of data moving between services, could be a good model. bellovin pointed out the value of minimization: if data do not exist, they cannot be abused. geambasu agreed, but also pointed out that some data holders keep seemingly unnecessary data in case they might be useful in the future; these parties can take other strategies to separate and sequester data that are not currently valuable to reduce the risk of misuse. gunter proposed the idea of developing abstract frameworks that could allow analogies between different sectors. for example, this could enable an understanding of how people feel about privacy with respect to smart electric meters to inform strategies for managing privacy in connected vehicles. he noted an idea, from a recent workshop related to intelligence, of creating a framework that distinguishes data collection from data usešan idea that has not been emphasized in other sectors. bellovin pointed out that a recent president™s council of advisors on science and technology report4 emphasized controls on data use rather than on collection. he noted that some privacy advocates are uncomfortable with this, because even the best use control policies can be changed in ways that open up pathways for unintended or harmful use of stored data. defining privacy wayman referred to the idea that ﬁprivacy is a concept in disarrayﬂ; people struggle to articulate its meaning.5 he shared an anecdote from past work on an international organization for standardization committee for terminology development where a representative pointed out that there is no single word for privacy in russian. wayman suggested that it could prove fruitful to focus instead on more carefully  3 national research council, 2003, who goes there: authentication through the lens of privacy, washington, d.c.: the national academies press. 4 president™s council of advisors on science and technology, 2014, big data and privacy: a technological perspective. washington, d.c., https://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast /pcastbigdataandprivacymay2014.pdf. 5 d.j. solove, 2006, a taxonomy of privacy, university of pennsylvania law review, pp. 477564. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.privacy implications of emerging technologies part ii 19 articulated rights. he pointed out a recent academic paper discussing a ﬁtheory of creepy.ﬂ6 he proposed that whether or not a practice is perceived as ﬁcreepyﬂ could be a very useful benchmark. gunter suggested that privacy, like friendship or security, will never have a precise definition, and that this should not dissuade people from respecting it or thinking about it. another participant suggested that there are many words and terms in any culture that embody facets of the values we associate with the term privacy. open discussion challenges around control and use frameworks one participant, picking up on the earlier discussion of controls on collection and use, suggested that reasonable limits on data collection could be impractical and difficult to define, partly because of the vast quantity and range of data that might be collected and partly because the appropriateness of collection depends on the ultimate usešwhich is largely unknowable. the participant wondered about the possibility of instead developing a framework for data control and use, noting that the appropriateness of control and use is situationdependent. the group discussed the idea of a mathematical framework that might enable objective and automated generation of limits on data use. several participants noted recent work attempting to develop formal models of the data use rules contained in hipaa, aiming to enable computers rather than attorneys to make datasharing decisions. one participant noted that researchers had found holes in this approach. one of the panelists suggested that there are no general concepts of use that are immediately and universally applicable; every concept of use would require its own ontology to achieve a contextspecific meaning. a participant identified some of the limitations of restrictions on use: 1. they are difficult to enforce, and enforcement depends upon generally underresourced enforcement agencies. 2. because restrictions are generally imposed only after something bad has happened, they are thus more punitive than preventative. 3. use restrictions may be subject to attack under the first amendment; if data were lawfully collected, what is the legal justification for limiting their use? a panelist raised an example of advertising targeted at an individual whose online activities displayed characteristics associated with depression, pointing out that targeted advertisements could be helpful (for example, advertisements for a support group) or detrimental (for example, providing advertisements for alcohol). someone else suggested that rather than focusing on the ethics of the outcome in this scenario, we should actually be more concerned with the ethics of conducting this level of profiling without the user™s consent in the first place, whether or not there is currently a legal mechanism to restrict such profiling. bellovin suggested that one benchmark to consider for use restriction is whether possession or analysis of the data in question is likely to result in a data holder taking an action that he or she would not have otherwise taken. weighing the benefits of a given use against its ﬁcreepinessﬂ factor could be helpful. best technical practices geambasu proposed some important privacy practices that could be deployed at the service level: 6 o. tene and jules polonetsky, 2015, a theory of creepy: technology, privacy, and shifting social norms, yale journal of law and technology 16.1:2. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.20 privacy research and best practices conduct extensive privacy testing while developing large and complex applications. privacy implications are often unintended consequences, and must be actively sought in order to be prevented. such testing could be required by companies and conducted by programmers. manage data effectively. if data are not being used, keep them separate and secureševen require permission all the way up the management chain before they can be accessed. bellovin proposed a few more practices: avoid globally unique identifiers, which make it easy to link data across time and among different applications. avoid looking at data that are not necessary. for example, the outlook mail service does not read email content when selecting ads to display. do not collect data that are not needed. if they do not exist, then they cannot be abused. a participant noted that the discussion had centered on technologies in the context of academia and the private sector. it was pointed out that the workshop was in fact meant to help expose the ic to outside research and practice around privacy, to provide new perspectives, and to help enrich thought about privacy within the ic. this was followed by some discussion of privacy in the context of the ic. wayman noted that the ic is clever about using data, suggesting that any general rules about the ic™s use of data could have minimal impact. he suggested that the ic does a good job of protecting individual privacies of members of the public, but that the privacy risks for those within the ic may be substantially higher. another participant noted that translating the fipps or other policies into concrete and substantive operational requirements is challenging across any industry, and suggested that technologies to help with this translation could be useful to those who design applications. bellovin pointed out that the fipps apply to the u.s. government, and have analogues in other developed nations™ data protection commissions; they are not, however, broadly applicable to the commercial sector, with the exception of hipaa. he noted that fipps may be rather obsolete, because they focus on identity; he reiterated that profiling and inference of sensitive information can occur whether or not a data set contains identity information. he said that part of his research centers on creating a new formal definition of privacy and the harms that result from various activities. he suggested that for the ic, privacy violations are more likely to arise when focusing on a specific personšbut much of the ic™s work is concerned with larger trends, rather than individuals. gunter added that it makes sense to think of fipps as a starting point, and ask how they should be extended, suggesting that looking beyond the fipps could be an important strategy. he also suggested that more progress could be made on privacy by drilling down to sectorspecific contexts and ontologies than by focusing on highlevel ideas. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 21 5  social science and behavioral economics of privacyšpanel summary remarks from panelists frederick chang, director of the darwin deason institute for cyber security, the bobby b. lyle endowed centennial distinguished chair in cyber security, and professor in the department of computer science and engineering in the lyle school of engineering at southern methodist university, introduced the next panel on the topics of attitudes, preferences, and behaviors as they relate to privacy. he noted that the panel would touch on attitudinal surveys, privacy behaviors, how much people pay for privacy, and the societal impacts of privacy, informed by the panelists™ backgrounds in economics, behavioral economics, computer science, psychology, media and communications, law, and information systems. chang, as moderator, introduced the following panelists and gave each of them 5 minutes for opening comments: idris adjerid, assistant professor of management, university notre dame; jessica staddon, associate professor of computer science, north carolina state university; joseph turow, robert lewis shayon professor of communication at the annenberg school for communication, university of pennsylvania; and, katherine strandburg, alfred b. engelberg professor of law, new york university. idris adjerid discussed his research on the economics of privacy with a focus on behavioral economics. through his work, he has found that privacy decision making is particularly susceptible to deviations from rational choice. this runs counter to the common assumption that people have consistent privacy preferences, make rational decisions, and act in their own best interest. he noted that there are many examples that illustrate such irrational privacy behavior. he highlighted the specific example of the control paradox, the phenomenon where the illusion of control can be comforting. experimental work has shown that giving users more controls puts them at ease, and makes them more willing to disclose informationšwhether or not the controls actually enhance benefits or reduce risk.1 this work suggests that systematic changes can be induced in people™s behavior by manipulating subtle or even insubstantial factorsšcounter to rational models of behavior. one study demonstrating this effect provided a set of substantive privacy controls under different names to participants. participants presented with the options labeled ﬁprivacy settingsﬂ were 56 percent more likely to actually use protective options compared to a group given the same options labeled ﬁsurvey settings.ﬂ a similar experiment suggested that perceived changes in the risk of data disclosure can have a more profound effect on behavior than the objective differences in risk. adjerid noted the large amount of evidence suggesting that people are bad at making privacy decisions, but also that this is not always the case. for example, a recent study out of mit by catherine tucker and alex  1 l. brandimarte, a. acquisti, and g. loewenstein, 2013, misplaced confidences: privacy and the control paradox, social psychological and personality science 4(3):340347. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.22 privacy research and best practices marthews2 found a measureable change in people™s search behavior after the recent disclosures about intelligence collection practices. in particular, people were less likely to search certain terms on google that previously had been identified as personally sensitive, suggesting a deliberate attempt to mask search interests. adjerid has examined how behavioral economics translates to consumers™ decisions about themselves, with an eye to limitations and rationality. another important topicšon which little research has been conductedšis how individuals make decisions about other people™s privacy, which was touched on in the previous panel™s discussion of tagging images on facebook. it is unclear whether or not individuals will be good at managing other people™s privacy and which factors affect that ability. he suggested that this question could be relevant to how individuals in the ic approach data decisions involving sensitive data about other people, and how appropriate mindsets might be systematically encouraged. jessica staddon described her background in industry research, most recently at google. she noted that she was about to begin an academic position at north carolina state university and was not representing google at the workshop. she focused her remarks on three main points: (1) the value of transparency, (2) privacy measurement, and (3) industry™s role in the privacy ecosystem. like many previous panelists, staddon emphasized the value of transparency, using insights pulled from her time in industry. she described google dashboard, which enables a user to see his or her own search history as tracked by google. in the face of concerns that the information revealed through such tools might cause users to recoil and use google less (or even close their accounts), she and her colleagues searched for any evidence that such transparency tools would have a negative effect. they found none. to the contrary, researchers found many positive associations between the use of these tools and feelings of trust and control, based on direct user feedback. she noted that, in general, data owners should be thinking about how to return some value or utility to the users from which the data were derived, and that transparency is one way to provide such utility. staddon then discussed challenges surrounding privacy measurement. she noted that many in the privacy research community have been pushing for the same kinds of standards that are found in the hard sciencesšmore of a culture of repeatability, with more consistency of measurement techniques and more objective measurements. staddon has found that many research findings are inconsistentšfor example, she knows of some very solid papers that find no difference in privacy concerns between genders, and some very solid papers that do. there is also a wide variation in reported rates of concern around certain privacy topics. she noted that we have a poor knowledge of historical trends. many questions will be hard to answer without some consensus and some means for documenting privacy incidents. for example: are privacy incidents becoming more common? what is the most common cause of such incidents? how do these trends vary by geography, or by demographics? she suggested one source for such data could be a crowdsourced but moderated (along the lines of wikipedia) privacy data repository where people could report and document events. finally, staddon expressed concern about the privacy ecosystemšespecially the role of industry, which is a huge player. she noted that industry holds a huge amount of data about privacy preferences and behaviors and suggested that it needs to be a part of the conversation, and also needs to work on innovations for privacy. she has heard anecdotes suggesting that many companies are becoming increasingly risk averse when it comes to research and development that could lead to privacy innovations, likely due to regulatory barriers to conducting such research and the potential that findings about the privacy implications of their own practices could result in more scrutiny from oversight bodies. 2 a. marthews and c. tucker, 2014, ﬁgovernment surveillance and internet search behavior,ﬂ march 23, available at ssrn 2412564, https://papers.ssrn.com/sol3/papers.cfm?abstractid=2412564. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.social science and behavioral economics of privacy 23 joseph turow described the two major thrusts of his research. he has long studied how companies deal with digital relationships and issues of surveillance, particularly in the marketing and retailing sectors. since 1999, he has also worked on approximately ten national surveys conducted through major research companies. he went on to summarize some of his basic findings.  first, people generally know that their online activities are being trackedšthis is apparent from a range of surveys, as well as from anecdotal reports. second, there is nonetheless a huge ignorance about the details of what goes on in the digital arena. for example, people think the government is protecting them more than it actually does. this has been observed around phenomena such as price discrimination. at least two surveys have suggested that people think it is illegal for companies to charge different people different prices for the same goods. turow suggested that such ignorance is not because people lack intelligence but because they are busy and simply do not have the time to examine and interpret the complicated information that exists about such protections. third, there is evidence that people philosophically do not like tradeoffs. in one of his recent surveys3 91 percent of respondents disagreed with the statement that receiving a discount is a fair exchange for companies collecting their data without their knowledge; 71 percent disagreed with the statement that it is fair for a store to monitor their online activity while shopping there, in exchange for free access to the store™s wifi. 51 percent disagreed with the statement that it is okay for a store to use the information it has about them to create a picture of them that would help to provide them with better service. nonetheless, many firms say that people accept tradeoffs. in their report on this survey,4 turow and his coauthors argue that it is not the case that people consent to giving away their personal information in exchange for some benefit; on the contrary, the evidence suggests that americans are simply resigned to having their data taken. he noted some specific findings from the study: 84 percent of respondents agreed that ﬁi want to have control over what marketers know about me.ﬂ 65 percent of respondents agreed that ﬁi have come to accept that i have little control about what marketers know about me.ﬂ š 58 percent agreed with both of these statements. 72 percent of respondents disagreed with the statement that ﬁwhat companies know about me online cannot hurt me.ﬂ š 41 percent of respondents fell into all three of these categories. turow said these findings imply that something serious is going on. he suggested that it™s not just people who are suspicious, but companies as well. he recalled a conversation with a contact from a major retailer who was concerned that large internet companies could share information about customer activities with the retailer™s competitors. he concluded by noting that these findings and anecdotes raise many interesting questions about harm and public perceptions. he suggested that these findings could relate directly to how people view the ic. katherine strandburg focused her remarks around two themes: (1) intrinsic failures of any online privacy market and (2) the social value of privacy. she began by arguing that any idea of an online privacy market fails because it fails to accurately reflect consumer preferences. in general, markets enhance social welfare because transactions reflect consumer preferencesšpeople are willing to pay what the product is  3 j. turow, m. hennessy, and n. draper, 2015, ﬁthe tradeoff fallacy: how marketers are misrepresenting american consumers and opening them up to exploitation,ﬂ annenberg school for communication, university of pennsylvania, june, https://www.asc.upenn.edu/sites/default/files/tradeofffallacy1.pdf. 4 ibid. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.24 privacy research and best practices worthšand this is a good thing. however, in the case of privacy, consumers do not know what price they™re paying when their personal data is being collected.  she reiterated turow™s point that the fact that consumers acquiesce to the collection of their personal data does not accurately signal their preferences. she said that there are many reasons consumers do not understand the price of giving up their own data, including (1) information asymmetry and (2) behavioral economics factors. she emphasized that the most fundamental problem with the idea of price in this context is that the incremental cost to a consumer of giving up data depends on what data about them is already out there and what data about them will be disclosed in the future by anyone. she said that the price or cost of giving up any single piece of data in any single transaction is thus effectively unknowable, so the idea of a privacy price is meaningful only in aggregate. the same is true when considering the value to an individual consumer of a given privacyprotecting alternativešthe consumer does not know whether the information could be obtained anyway through some other means. strandburg introduced the possibility, in line with the results of turow™s study, that consumers treat online activity as an allornothing proposition: they choose to be online and risk compromising their privacy, or else they choose not to be online and lose the value of the internet. she suggested that this could explain the socalled paradox that people expose their data online even though they say they care about privacy. this would mean that we are in a bad equilibrium where it is hard for an individual to switch out of a market with a host of useful services that are accessible only by giving up one™s data. such a change would require collective action; it does no good to make a partial switch to a privacy protective service. given the barriers to change, data collection persists even when consumers would prefer an alternative system. strandburg then pointed out that privacy is not only an individual value, and that online privacy markets, consentbased systems, or even democratic voting systems do not necessarily account for the social value of privacy. she went on to explain that privacy may have positive externalities, such as the social benefits of individuals exploring nonmajoritarian views. another example is the network benefits of emerging technologies such as social media that are not taken into account by individuals; surveillance can have chilling and conforming effects on people and their decisionmaking that would inhibit these network benefits. she also pointed out that technology can change the locus of social life, and the implications of a particular surveillance can change significantly as technology changes. strandburg also noted that shortterm benefits may be overestimated in comparison to longterm social costs, and that surveillance impacts might be undercounted because they are often concentrated in underrepresented or economically disadvantaged groups. she suggested that we need more empirical studies of the social costs and benefits of privacy, rather than just the costs and benefits for individuals. panel discussion chang then led the panel in discussion, which centered around several topics. recent examples of tipping points turow suggested that companies that have experienced data breaches, such as target, may have lost some business in the short term, and incurred the cost of credit protection services for those who were affected, but it is not clear that they will suffer longterm consequences. he said he used to think that people would rise up and reject online tracking for marketing purposes, but the changes are incremental, and people may be so wedded to the status quo that it could take a huge ﬁdisasterﬂ for real changes to occur. he suggested that we have not really seen such a tipping point yet in the marketing sphere. turow went on to agree with strandburg™s remarks that the real question is societal. he proposed that we should be thinking about the broader trajectory of society. he questioned what would happen in the absence of visible tipping points. chang and turow suggested that it is not so much about tipping points as it is about the changing sense of what is normal, and how that impacts society. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.social science and behavioral economics of privacy 25 adjerid agreed that there is probably not a single identifiable event that marks a tipping point for privacy concerns at the societal level. he suggested instead that there may be a series of minievents that, depending upon how they are presented, could result in individuals experiencing tipping points. he provided an example of a recent study in which privacyrelevant information was presented in a salient way to mobile users. in one instance, being told that a particular app leaked location information thousands of times in a single day was a significant point for individual users. he suggested that, rather than experiencing some exogenous societal shock, individual members of society experience their own personal tipping points. however, over time a critical mass may change their perspective, resulting in a societal push to respond to a certain practice. staddon added that the flood of reidentification of purportedly anonymous databases (e.g. of publicly released aol search logs and netflix customer rental data), has changed attitudesšbut so far seems to have produced resignation rather than calls to action. turow suggested that the ashley madison breach could be a tipping point for a subset of the population. strandburg suggested that the snowden disclosures might have been a tipping point with respect to the icšthough it may be too soon to sayšakin to the revelations that led to the church committee investigation and subsequent reforms. invoking her background in physics, strandburg suggested that we are arguably in a metastable situation, from which it can be difficult to return to an optimal situation, even through tipping points. anticipating future tipping points turow recommended caution when thinking about how attitudes and behaviors will change across time, in particular while considering generational differences. many have suggested that millennials think differently about privacy than members of older generations, but some of turow™s research shows that the thinking is not as different as one might imagine. he also suggested that an individual™s attitudes might change over time. adjerid voiced concern about the risks of relegating privacy protection to a noticeandconsent paradigm (where users are provided a sometimes overwhelmingly complex privacy policy to which they must consent in order to use a service), as it puts the burden on users while avoiding some of the hard questions around what uses are appropriate or inappropriate. he also noted that the ic does not have this luxury. he suggested it will not be easy to tell how quickly we might approach a tipping point around this model. strandburg suggested that we might start seeing tipping points as awareness and concern about equality issues increase. people may feel less resigned or have different expectations around equality. turow agreed, and noted that he is finding more and more concern about equality in the retail space. but he also noted that americans have come to accept many hierarchies and inconveniences, such as airline boarding protocols and luggage restrictions, which he referred to as the ﬁgerbilizationﬂ of life. he pointed out that, in general, no one stands up and complains, but suggested that inequality in the application of certain inconveniences could lead to a tipping point. turow also expressed concern that some parties might attempt to make people believe they can have control over their information without actually providing significant controls, a practice he referred to as ﬁpseudotransparency.ﬂ for example, an app may ask for permission to use your location data without telling you that it plans to sell the data to other companies. publicity about pseudotransparency could make people very angry. improving trust adjerid suggested that concerns about data misusešor perceived misusešcould be assuaged by demonstrating how individuals within an organization can make the right decisions in a context where there is not a clear right and wrong. he then discussed some research on how individuals make decisions about other people™s privacy describing some early but interesting findings related to the roles of reciprocity and social privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.26 privacy research and best practices norms. he has found that most individuals say they are respectful and cognizant of others™ privacy, but fewer say that other people are respectful and cognizant of their privacy. this could be due to a disconnect between the ways we perceive how considerate we are versus how considerate other people are. another possible explanation is that any visible bad actor skews an individual™s perceptions of others. adjerid is also examining whether ﬁnicenessﬂ is a predictor of cognizance of another™s privacy, by evaluating results in terms of where respondents fall on the psychopathy spectrum. his early results suggest that there is no significant difference; psychopathy did not have a moderating effect. most people, including ﬁniceﬂ people, are more willing to disclose sensitive information about other people than about themselves, which suggests that the role of individual judgment in greyareas is not straightforward. adjerid suggested that similar arguments can be made about individual decisionmaking around norms in an institutional setting. individuals may imposeševen subconsciouslyštheir own perceptions about the proper tradeoffs between security and privacy for the people whose data they are working with. this could potentially happen in the ic, and does happen in private organizations. staddon added that she sees similarities between large internet companies, such as google, and the ic. she recalled comments from previous panels that it is difficult for an outsider to really know what the ic does. she suggested that simply being more open might helpšfor example, about the utility being returned to the public, the types of things that are being studied, or even the fact that thought is going into these decisions. this would come with risks. people might try to poke holes in the chosen practices, or ask questions that the ic cannot answeršbut more openness could still have significant benefits. strandburg suggested that trust could be improved by being more transparent about the efficacy of the ic™s privacy protection practices. she also noted that her interactions, though somewhat limited, with people in the ic suggest to her that there is a very high degree of professionalism in the community. instilling certain ethical attitudes as part of the professional identity of a community can be important, in particular for the ic. turow added that he thinks the public wants to see evidence that their information is being handled with respect. he suggested that, before the recent disclosures about intelligence practices, americans gave more credit to the government than to marketers when it came to privacy but that this attitude has probably changed. he also noted that there is no simple recipe for building trust. it will be an incremental process, during which the community must demonstrate a genuine respect for the larger society. it will take a lot of time. alexander joel, civil liberties protection officer, odni, responded to some of the panel™s remarks. he noted that the ideas for building trust were consistent with the ic™s new efforts. in particular, the ic has principles of intelligence transparency that they are working hard to implement. he also recognized the need to be more transparent about the utility of the ic, because people™s attitudes about a particular service or trust relationship depend in part on the value that it provides them. it is much easier to see such value with online services (for example, a web service remembering a customer™s preferences) than with the ic. he also reiterated the idea of professionalism and ethics, and noted the published principles of professional ethics for the intelligence community,5 including sections on mission, truth, lawfulness, integrity, stewardship, excellence, and diversity. the section on lawfulness reads as follows: we support and defend the constitution, and comply with the laws of the united states, ensuring that we carry out our mission in a manner that respects privacy, civil liberties, and human rights obligations. joel pointed out that the research on how people feel about their own privacy vs. that of other people, addressed by adjerid, has obvious relevance to the community. he noted that, without a personal stake in the matter, data managers can be very rules and compliancefocused. 5 office of the director of national intelligence, ﬁprinciples of professional ethics for the intelligence community,ﬂ http://www.dni.gov/index.php/intelligencecommunity/principlesofprofessionalethics, accessed september 8, 2015. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.social science and behavioral economics of privacy 27 open discussion all participants were then invited by chang to ask questions and discuss the points and themes presented by the panel. chang began by asking if there is a privacy equivalent of critical security controls in the ic. several participants pointed out that the ic has both security controls and recently published privacy controls: the privacy overlay can be found as attachment 6 in appendix f of committee on national security systems instructions (cnssi) 1253.6 the privacy controls aim to operationalize the fair information practice principles (fipps). public perceptions and trust a participant noted that many in the public do not necessarily appreciate the distinction between different parts of the government, and thus do not believe that oversight of one government group by another is truly independent. the participant also pointed out that some information, including that which reflects the highest impacts of intelligence activities, is and must stay secret. the participant asked whether this undermines the ic™s ability to build trust. adjerid suggested that demonstration of value and generation of trust are two distinct concepts. an example of demonstrating value would be telling the american public that the activities undertaken will benefit them. he suggested that a more direct strategy for building trust, one that does not necessarily require the release of secrets, is to carefully articulate what is being done to protect data. another participant noted that being transparent does not necessarily mean revealing everything. nonetheless, it is not clear what kind of transparency is feasible that would make the public feel comfortable. by analogy, consider that most people do not really understand the details of how a car works, and do not need to. in order to develop trust, it may be important for the citizens to believe that the ic™s goals and interests are in line with their own.  participants discussed public perceptions of and concerns about the government™s ability to access privatesector data. one participant suggested that the firewall between the data held by commercial entities and that held by the government has disintegrated and wondered what effect this had on public trust. turow noted that his surveys had not considered this topic, but said that, in addition to the government having legal mechanisms for directly accessing company data, there is potential concern about the ability of the government to legally purchase information from an independent data broker that they would not otherwise have been authorized to access. another participant suggested that the challenge might actually be a lack of transparency about the meaning and interpretation of the rules and laws that govern this access. the participant noted that, while some details must remain secret, the ambiguity of oversight and governance of activities funded by taxpayers is a huge problem. one example raised was the fact that many terms, such as ﬁtargeted,ﬂ used in laws such as fisa have no statutory definition. the participant found it unclear how explaining such terms could legitimately compromise the ic™s mission. strandburg suggested that one way to understand the trust dynamic is to question whether citizens would approve of secret practices if they knew about them. one might, for instance, think about this by considering whether such practices would cause embarrassment if published in the new york times. she also suggested that disclosing general ideas about what kind of data are being collected and how they are being used would help demonstrate efficacy and enhance transparency, and that there are probably things that could be disclosed without negatively impacting the efficacy or usefulness of tools. joel responded to several of these points to provide some perspective from the ic. first, he acknowledged the general surprise at the disclosures of how laws were being interpreted and applied, noting that the ic aims  6 committee on national security systems, ﬁsecurity categorization and control selection for national security systems,ﬂ cnssi no. 1253, release date march 27, 2014, https://www.cnss.gov/cnss/issuances /instructions.cfm. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.28 privacy research and best practices to move forward with more transparency. he also noted that the internal process for accessing privately held data was not seen as easy within the ic, but was an elaborate process involving the foreign intelligence surveillance act (fisa) court and congressional oversight, as well as many people overseeing the process within the ic. he pointed out that there were also substantial restrictions on how any data obtained could be queried, used, and shared. recent statistics have shown a very narrow set of uses, though an admittedly enormous collection of data. the ic supported the usa freedom act, which has put in place a new model with statutory transparency requirements that the community is working to determine how to implement. a participant suggested that the question of whether secrecy undermines trust could be a good research question. another participant noted the difference between an action being legally permitted and being acceptable to the public, and asked whether the fact that something is perceived to have value (either to the nation, to an organization, or to the public) makes it more acceptable, even if it is outside the normal boundaries of acceptability. turow said that, in the marketing context, the public has not seen value as an acceptable justification for unwanted data practices. he had seen some evidence in the past that people would be more forgiving if the value accrued to the nation rather than to individual marketers, but was unsure of how people feel about that now. he suggested that understanding the difference between attitudes toward the government and attitudes toward corporations with respect to value and fairness are quite important, and noted that the nation might have a greater capacity for disappointing the public than a corporation might. adjerid described some of his work on organizations. early findings from qualitative discussions and interviews suggest dissonance between the perspectives of higherlevel and lowerlevel employees: higherlevel employees tended to offer the ﬁparty lineﬂ that the organization does not engage in data practices that are potentially invasive or discriminatory. lowerlevel employees, such as those actually doing the data analysis, tend to find creative ways of collecting and using information that might push boundaries, but could be potentially lucrative to the organization. equality, discrimination, and consumer profiling a participant observed that people with different backgrounds can have very different perspectives on data collection. strandburg reiterated that underrepresented groups can be affected disproportionately, and suggested that the government has a responsibility to those groups in a way that a company does not. she also highlighted the idea that lowerincome people could be the canary in the coal mine when it comes to privacy; what gets done to those who are not very powerful could eventually become normal, and happen to us all. turow noted that very little research has been done on the privacy preferences of certain demographic groups, such as lowerincome populations and minority groups, and that this is very important to understand. the group then discussed issues related to consumer pricing strategies. for example, loyalty programs often offer lower prices to members; this generally reflects the fact that companies gain value from tracking their members™ practices. it is not clear to consumers how much companies benefit from data collection, even if the consumers see the discount. strandburg reiterated that costs and benefits are somewhat unknowable, as they depend upon what is already known about a consumer, which will greatly influence the value of newly collected data. it was suggested that perceived injury on the part of a customer as a result of data collection is accounted for in the formula for the pricing differential. turow noted that loyalty programs have been increasingly used as a lure to get consumers to share data; such programs can be used in many ways. he described the current debate within the retail community about more elaborate price tailoring practices, where prices vary from individual to individual based on the profiles the company has built about each, a strategy enabled by the internet and mobile shopping. he provided several examples to illustrate the depth of complexity we are entering into around individual profiling. at least one retailer actually gives higher prices to more loyal customers, because it is less concerned about losing their business. some brickandmortar retailers have started using electronic ink, enabling them to change the price by time of day, or even tailor prices to individual shoppers. he suggested that current practices are teaching people that giving up their data is just a part of 21st century life. in 20 years, people privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.social science and behavioral economics of privacy 29 may think that it has always been this wayšindeed, no one today remembers what retailing was like in 1895. he again suggested a need to think about the consequences of these shifts for the larger society. the group then discussed the role of experience in shaping user attitudes, motives, and incentives. according to turow™s data, fewer than 5 percent of americans report having been directly harmed as a result of their data having been used. turow pointed out that we are still at the beginning of the digital age. he recalled testifying in a 1985 hearing on programlength commercials, where companies were creating shows aimed at marketing toys to children, and suggesting that action would need to be taken at that time, or else people would take such practices for granted. he pointed out that today, there is a whole channel run by hasbro. turow suggested that data collectors, especially the ic, are stewards of much of american society. he posed a question: ﬁhow do we want our grandchildren to think about the way the world is?ﬂ he suggested that we are really only at the beginningšthat the pace of data collection and use is accelerating, and things could change rapidly. privacy research the group also discussed challenges to conducting innovative privacy research. staddon pointed out that the heavy public scrutiny of internet companies creates a disincentive for companies to carry out research or make data available to researchers. in particular, if results can be interpreted as showing that a company™s priorities are at odds with consumer privacy, things will become more difficult for the company. another participant pointed out that this is a big loss, because internet companies often have access to massive data sets that could help advance our understanding, whereas academics tend to work with small data sets. another participant pointed to difficulties around the study of behavior, and noted that it is easy, even for social scientists, to come to the wrong conclusions when interpreting observed behavior. for example, a lack of action on the part of an individual could indicate resignation rather than acceptance. similarly, survey results can be off if questions are not asked in the correct way. finally, people do not necessarily behave logically, which might reflect selfcontradiction or some other factor such as resignation. the participant asked whether we are getting better at using this type of research to understand what people really care about. staddon noted that there are many survey practices that can provide confidence in results, such as phrasing survey questions in ways that will help to avoid bias. she noted that the community as a whole is doing more work on behavioral data. she referred to a recent study7,8 that examined what information users were willing to share with an anonymizing filter and without one; this approach provides a better understanding of the context in which users are comfortable disclosing information. nonetheless, we still do not know how to identify privacyconcerned users simply based upon their online behavior. a participant asked if we really know whether individuals are being honest in their responses to surveys about privacy preferences. turow described some of the steps his team took to address this issue, including hiring experienced public polling firms to conduct the surveys, and asking some of the same questions longitudinally. he noted that responses have been consistent and stable over time. he noted that other interpretations of such consistency are possiblešincluding the possibility that individuals provide the answers they think the questioners are hoping to hearšbut he finds this unlikely. he has compared past  7 s.t. peddinti et al. 2015, understanding sensitivity by analyzing anonymity [guest editor™s introduction], 36th symposium on security and privacy 13(2):1421. 8 s.t. peddinti, a. korolova, e. bursztein, and g. sampemane, 2014, cloak and swagger: understanding data sensitivity through the lens of user anonymity, proceedings of the ieee symposium on security and privacy, pp. 493508, doi:10.1109/sp.2014.38. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.30 privacy research and best practices results to related questions in other surveys, including some conducted by pew,9 and to anecdotes. for example, the findings about resignation are consistent with what is often heard anecdotally. another participant pointed out different approaches that can be illustrated using the example of attitudes and behavior around nutrition. consider an individual who is aware that he has bad eating habits. one could ask about the individual™s habits, what the individual would like his or her habits to be, or whether he or she finds another person™s habits admirable. in general, people have principles, but do not always follow them due to factors such as weakness or time constraints. learning what people think is the right thing could be useful, though these ideals could be distinct from what they personally want or enjoy. adjerid agreed that there are ways to experimentally tease out such details. one strategy is to ask participants to anticipate how they would behave in a hypothetical scenario, and then actually implement the scenario. he has found that people do not act as they had predicted they would. research suggests that individuals overestimate their ability to behave rationally in the future while underestimating their susceptibility to influence by nonrational factors. however, he suggested that the mechanism behind such dissonance has not yet been adequately elucidated. adjerid identified the tendency of individuals who care about privacy to act against these interests, which he termed the ﬁprivacy paradox.ﬂ he also suggested that consumers seem to have privacy fatigueša huge breach of a company™s data may not affect its sales or stock value. he wondered whether results from social science research, which might provide evidence of this fatigue, could paradoxically lead organizations to avoid taking action to enhance privacy. on the other hand, he also asked whether inaction on the part of companies could lead to a tipping point such that people stop disclosing information to retail, internet, or telecommunications companies, which would presumably also impact the ic™s ability to do its job. he recalled the contextual nature of privacy, and suggested that it could be helpful to learn more about the contextual nature of individual rationality in making privacy decisions. this could serve as a basis for tailoring policy around privacy in ways that could align individuals™ behavior with their best interests, and for providing more predictability, and thus make it easier for organizations to plan policies and react to concerns. 9 note a recent pew study addressing americans™ attitudes about privacy in the context of government: see mary madden and lee rainie, 2015, ﬁamericans™ attitudes about privacy, security and surveillance,ﬂ may 20, http://www.pewinternet.org/2015/05/20/americansattitudesaboutprivacysecurityandsurveillance/. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 31 6  best practices and ethical approaches  for data collection and usešpanel summary remarks from panelists fred h. cate, c. ben dutton professor of law at indiana university, welcomed the group back for the second day of the workshop. as moderator, he introduced the following panelists and gave each of them 5 minutes for opening comments, noting that they had been tasked with addressing a difficult set of issues: jennifer glasgow, chief privacy officer, acxiom; rob sherman, deputy chief privacy officer, facebook; david c. vladeck, professor of law, georgetown university law center, and former director of consumer protection, federal trade commission; and helen nissenbaum, professor of media, culture and communication, and computer science, new york university. jennifer glasgow began by providing background on her company and her work. acxiom is a 45yearold data company with two main lines of business. first, it collects data from public records and other sources, aggregating it and bringing it to the marketplace, both for marketing and for fraud and risk identification purposes. second, it hosts sophisticated data warehouses for large clients in consumerfacing industries, including 50 of the top 100 companies. in short, her company is all about data, including some of what people may perceive to be scary uses of data. she went on to discuss three key themes, as summarized below. 1. compliance is no longer enough. acxiom has a saying that has been baked into its culture: ﬁjust because you can do it doesn™t mean you should.ﬂ the challenge, however, is in determining what one should do. this takes a multitude of approaches and techniques, and the ability to make these determinations does improve over time. she suggested that many of the tools and techniques available for data collection and use are probably being deployed by both the ic and the private sector, noting a synergy between what the ic is facing in terms of scrutiny and public reactions and what some in the private sector have been dealing with for a long time. she pointed out that, while there is always the possibility of legislation, gaps in the legal framework mean that both industry and agencies need to develop their own rules and to identify and adopt best practices. the company™s brand and its customers™ brands are at stake. 2. determining appropriate uses of data. glasgow stated that acxiom is pushing hard for more selfregulation; it derives its own policies, as do many other large companies. in 2014, the company conducted more than 800 privacy impact assessments taking into account all of the different stakeholders. she noted that the efficacy of the process improves with experience, over time. she identified public relations as a key part of the process that has been underemphasized in both the commercial sector and the ic. she highlighted the importance of understanding how best to talk about privacy issuesšincluding what to say and what not to say. it is also important to understand privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.32 privacy research and best practices what your critics say about you; whether it is valid or totally offbase, you should have the ability to defend your practices and/or set the record straightšbefore an incident occurs, rather than after. 3. learning from consumers™ views. glasgow described some of what acxiom has learned from its consumerfacing website. the site allows consumers to register and look at the data that acxiom holds about them for marketing purposes, and enables them to change it, including to make corrections or to falsify information. at first, the company debated whether to allow a user to input data that were, to high certainty, inaccurate. ultimately, it decided that, since the data were used for marketing, it would be appropriate for a consumer to receive marketing materials or targeted ads intended for those with their preferred profile (e.g., a 50yearold wanting to be seen as a 30yearold might actually be more interested in ads aimed at 30yearolds). she suggested that, when it comes to data use, both facts and potential outcomes must be taken into account. rob sherman began by noting that facebook™s approach to privacy has evolved, and that it has learned a lot about privacy and its importance. today, the company™s touchstone is building the trust of the people it serves, because the business relies upon its users being comfortable sharing data with the service. this drives much of the company™s efforts to be responsible data stewards, which involve both complying with the law and going beyond it to best meet user expectations. he noted that there are times when no direct feedback about a particular data practice is available, which can complicate an organization™s missionša challenge that the ic likely also faces. facebook serves approximately 1.5 billion people globally, and the fact that they do not all have the same conception of privacy poses a major challenge. facebook has increasingly used focus groups and found individuals to have thoughtful, sophisticated, and unique perspectives on privacy. people have different concerns and reasons for wanting to protect their information. sherman sees the role of facebook™s privacy professionals as not only operating the business in a privacysensitive way, but also as empowering users to make their own choices, or ﬁputting people first.ﬂ he noted that this requires providing people the necessary tools and controls, and making them comfortable that the company is doing the right thing even if no user tool is in place. he noted that facebook has an internal privacy process similar to the one that glasgow described for acxiom. it is referred to as crossfunctional, meaning that the team comprises a broad range of stakeholders, including lawyers, engineers, security professionals, and communications experts. the engineering team is involved very early in the process, so that problems are dealt with before they are ﬁbaked in,ﬂ which has made ﬁprivacy reviewﬂ almost synonymous with ﬁproduct review.ﬂ this helps to avoid scenarios where privacy questions emerge at the end of a development process that must then either be scrapped, or have an afterthefact fix tacked on. sherman noted that facebook™s approach to external communication has also evolved. in the past, the company made its product decisions internally and communicated them externally later on. the company has since learned that broader external engagement, including with privacy experts and advocates, earlier in the process actually helps the company to make better decisions, and to have a better relationship with its customers. finally, he described facebook™s privacy process as an ongoing review. he noted that privacy challenges tend not to be static, and that expectations continue to change; even if a decision is right at the time, the company must be comfortable revisiting it if the environment or demands change. david c. vladeck commended other speakers for beginning with the premise that we now live in an era where law itself fails to provide adequate guidance on privacy issues. he suggested several reasons for this failure: (1) technological development has outstripped the ability of regulators and lawmakers to keep pace and (2) legal instruments often lack the clarity that is needed to make tough decisions. he pointed out that there are ongoing battles over the meaning of the fourth amendment and suggested that the ambiguity inherent in many legal instruments has created difficulties for everyone. he suggested that the ic has as a result been forced to rely on selfgenerated norms, and the challenge has been in generating and enforcing these norms in a credible way. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.best practices and ethical approaches for data collection and use 33 he made an analogy with the private sector, suggesting that trust and public perception can be critical to a company™s business model. he reiterated sherman™s point that facebook™s thinking about privacy has evolved, and noted that the company now has a wellearned reputation for being respectful of privacy. he suggested that facebook™s evolution on privacy (in the wake of a 2011 ftc enforcement action that called attention to its poor privacy practices) could serve as a model for the ic™s response in the wake of the snowden disclosures. for example, he suggested that norms need to be developed in a transparent way, with input from the public or some external validation process, and grounded in ethics and valuebased judgments of acceptable practices. he suggested that norms also need to be communicated broadly internally, and rigorously enforced; the norms themselves are valuable only if they are credible to the public, so violation of norms must be dealt with. he suggested that the private sector may actually know more about individuals than the ic does, although the average american may not understand this. he reiterated the idea that the ic would be wellserved to be more transparent about its practices, and suggested that having the public suspicious of the community would not help the ic in the long run. he noted that, in the aftermath of the snowden disclosures, it became clear that the public had little sense of what the ic was doing. for example, it was a surprise that bulk collection was not viewed by the ic as ﬁuseﬂ; there was a supposition that anything that was collected was used. he concluded by proposing that it is important for the ic to engage with the public about how data are being used, including the ways that could be inimical to individual welfare. helen nissenbaum focused her remarks on the ethical value of privacy, political governance, and notions of freedom. she began by reiterating that constraints on the flow of information often protect individual interests, but are also valuable for society as a whole. she provided several examples of benefits to individuals that also benefit society as a whole: the ability for patients to speak freely and privately to physicians about health matters also serves public health interests. the ability for citizens to vote autonomously in democratic elections also helps to promote legitimate democracy. confidentiality and privacy in tax records enables truthful financial disclosures and proper collection of funds. she pointed out that technology can disrupt the flow of information, and people often hold onto established norms, but they do not do so blindly. contextual norms are not arbitrary, and not made hastily; rather, they reflect a balance of needs among different stakeholders, and have been refined over time to promote different values and goals. she suggested that we should not blindly adapt our norms to accommodate new technologies, a point that also arose in previous panels. she suggested that norms emerging around new technologies must instead maintain a focus on the goals and values that inspired the preexisting ones. nissenbaum also addressed political governance. in this sphere, privacy (i.e. informational norms) protects and promotes important political values, including the various freedoms enshrined in the constitution. technologies disrupt these norms just as seeing through walls might disrupt the fourth amendment. she pointed out that many postsnowden commentaries focused on national security agency™s bulk collection practices. given that almost every action today is an informational action, some have asked how the intelligence agencies could not collect data in bulk. but critics see such collection as informational dragnets that do not reflect what we expect from government actors in the context of liberal democracies. nissenbaum noted that some previously proposed mitigations are persuasive (for example, requiring that collected data only be read by machines or be subject to strict use requirements), and they may even be ethically defensible given certain types of conditions and assurances. nonetheless, she suggested that it is important to understand what is at stake in these contexts. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.34 privacy research and best practices she also noted that privacy protects critical freedoms against government interference in various activities and aspects of our lives. freedom can be understood as noninterference; in other words, an individual is not a free citizen if government interferes with speech, association, or living in accordance with religious faith. the philosopher philip pettit defined freedom as nondomination, or security against arbitrary interference.1 in this sense, freedom means not only limiting what the powerful are permitted to do, but also reducing or eliminating their power to do it. nissenbaum suggested that this view is relevant to both commercial and government actors; collection of data enables government and dominant commercial actors the capacity to exercise power, and the threat of action can be almost as menacing and debilitating as the action itself. finally, nissenbaum noted that data collection, accretion, and analytics are disruptive. she suggested that while these practices can be used for good, it is important to understand what is at stake. panel discussion the panel discussed a number of topics, including strategies for determining appropriate practices within institutions, reuse of data, resource constraints, and how to translate ethical values into practice. glasgow described acxiom™s privacy impact assessment (pia) process. a team identifies stakeholder concerns and possible consumer risks and harms around a given project, with a proxy advocate representing consumer interests. enumeration of the possible negative impacts helps to elucidate how (or whether) a given decision would be defensible in a public space. projects should be reassessed periodically to account for shifting attitudes or laws. assessment is an iterative process that evolves and improves with time. sherman noted a few important principles to inform privacy decisionmaking.  both actual and perceived harm (including violations of privacy or trust) can be damaging to an organization™s reputation. external consultation is valuable, and can help to provide a nuanced understanding of external expectations. this can be performed in a confidential setting. vladeck identified three key types of internal participants to engage in privacyrelated decisionmaking: 1. a ﬁcassandraﬂšsomeone whose job it is to identify every possible thing that could go wrong, 2. privacy officersšwho can push privacy as a core value beyond simple legal requirements, and 3. an institutional naysayeršsomeone who truly understands the organization™s mission and is also prepared to push back on the status quo and compel alternative strategies. nissenbaum suggested that external input on privacy decisionmaking is necessary, because a company or organization has a natural bias toward its own interests. she also pointed out that ethical approaches require consideration of contextual values, and how constraints on information flow would best promote the goals of a given activity. the panel was asked to speak to decisionmaking around reuse of existing data. glasgow and sherman noted that acxiom and facebook conduct separate pias and analyses (respectively) on every proposed new use of existing data. vladeck suggested that unvetted new uses of data are ﬁthe landmine of privacy.ﬂ approximately half of the enforcement cases he saw brought while he was in law enforcement dealt with  1 philip pettit, 1997, republicanism: a theory of freedom and government, oxford university press, oxford, u.k. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.best practices and ethical approaches for data collection and use 35 companies that otherwise had good data hygiene, but neglected to examine the implications of novel new uses of data. the panel briefly discussed the personnel resources necessary for conducting privacy reviews. sherman noted that facebook has grown its personnel focused full time on privacy significantly, to about 50 fulltime staff (though others on different teams also address privacy in their work). he also described a more scalable staffing model for resourceconstrained organizations, in which a core group is responsible for coordination of privacy activities, with designated individuals embedded in different business units responsible for keeping track of possible privacy issues. glasgow noted that pias at acxiom have become procedural, and are held twice a day; this has helped to make the process relatively quick and easy. the scale is managed in part by separating and fasttracking the smaller, simpler decisions. vladeck added that some companies have a dedicated privacy officer whose job it is to handle these issues. he noted that privacy decisions are complicated, and sometimes the wrong decision is made, but someone with oversight needs to be involved. the moderator asked the panel to comment on how ethics and cultural values are or can be translated into the decisionmaking process. nissenbaum pointed out that ethics are unlike law in that they are based in foundational principles, which can lead to different analyses and strategies, rather on than fundamental rules. for example, an ethicsdriven approach could be to firmly define an explicit set of values and then evaluate whether a given action would be counter to these values. she also noted the sense in the united states that privacy is an ethical value. sherman noted that his company starts with a policy that reflects its values and works to communicate that policy and to respect users™ autonomy, often in concrete and tactical ways, such as by allowing a user to choose who gets to see a given status update. he also noted that there is an interest in making decisions more datadriven, in part to help meet the expectations of the users who do not speak the loudest. facebook also documents its privacy decisions and maintains them in a database, which can assist in answering individual users™ questions about data use. nissenbaum asked how facebook and acxiom define a ﬁprivacy issue,ﬂ pointing out that even if facebook users can control which friends see their posts they may not want facebook to use that information. mr. sherman acknowledged that such a definition is not straightforward and noted that facebook makes use of individuals™ data in order to provide services such as prioritization of newsfeed content. he identified a privacy issue as any negative outcome that could happen as a result of the way the company collects, uses, or stores people™s data. identification of potential issues begins with legal analysis and then takes into account user expectations and alternative approaches. glasgow defined the term as including anything that can be viewed as negative by a consumer, regulator, or client. to identify potential issues, acxiom has organized its business around major use areas, and attached specific ethical obligations to each area. vladeck suggested that an organization might generate its own operational set of ethical norms and rules by first considering its mission, such as protecting citizens from harm, and then using background norms (such as the bill of rights) to come up with discrete norms (such as a decision not to look at citizens™ data without a clearly articulated need to do so). he suggested that there are many foundational materials for generating discrete norms. open discussion regulations and internal standards several participants discussed drawbacks to reliance upon externally generated rules, raising the following points: privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.36 privacy research and best practices companies subject to external regulation often go into compliance mode, rather than trying to anticipate future needs and rule changes. regulators may not anticipate new technologies or uses. it may not be clear how existing rules apply to new technologies. the responsibility for setting norms is on the regulators, rather than the actors. regulators do not want to regulate too early, as this would stifle innovation, and because early business models may change. it was suggested that there will always be a lag between technological development and the ability of regulators to define an appropriate boundary, but that this can actually be a good thing. a participant recalled an earlier theme that people tend not to be good at protecting the privacy of others, suggesting that this is an argument against selfregulation or internal privacy standards. the participant suggested that making internal rules or standards of practice available to the public would enable review by those who do not share the organization™s mission. a participant suggested that organizations should always base decisions on whether a given practice would be perceived as fair or deceptive. someone asked how this might apply to the ic. it was suggested that this should be debated in a public way. privacy in the context of organizational missions a participant noted that privacy is generally secondary to the main mission of commercial companies, which is profit. though many companies are sincerely concerned about privacy, privacy considerations often also serve the primary mission by shaping customer perceptions. the participant suggested that this is not necessarily the same for the ic, whose mission can include the security of u.s. citizens in the broadest sense, read to include securing their privacy. another participant pointed out that the mission of the ic is to obtain information about bad actors that would not otherwise be provided to the government, for the purpose of informing policy decisions. by definition, practices such as interception of communications intrude on privacy. it is challenging to protect privacy while intruding on privacy. the need for public trust also poses the challenge of how to be transparent while still keeping secrets. while lessons and strategies from the private sector can be helpful and informative, the ic faces additional challenges. one of the panelists suggested thinking about privacy in terms of the ﬁappropriate flowﬂ of information: while some of the ic™s practices would be inappropriate for a private company, they might nonetheless be appropriate for preserving privacy in the context of the ic™s mission; however, this reasoning should be articulated publicly. another participant noted that some feel the ic™s mission could be undermined by articulating strategies for protecting u.s. citizens that may be less popular with the ic™s foreign partners. transparency, oversight, and trust the group discussed user privacy controls and oversight over backend data uses. it was noted that users may not have a clear understanding of how their data are and are not used. it was suggested that users may not have the timešor may not want to take the timešto understand the nuances of how data are used on the back end of any system, but that there may be other ways of communicating this to users for their own benefit. for example, the ftc and the irish data protection commissioner have audit authority over facebook™s operations, and audit reports have been made available to the public.2,3 whether or not users read these reports, knowledge of this external oversight is likely a comfort to users. 2 data protection commissioner, 2011, facebook ireland ltd: report of audit, december 21, https://www.dataprotection.ie/documents/facebook%20report/final%20report/report.pdf.  privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.best practices and ethical approaches for data collection and use 37 a participant noted that the ic is subject to outside (though still governmental) oversight, in particular through its own dedicated civil liberties and privacy offices and from the department of justice, neither of which are necessarily visible to the public. the participant asked what the community could do to build trust with the public. one of the panelists suggested that a good public relations campaign would be needed to effectively teach the public about the existing oversight mechanisms. another of the panelists agreed, and suggested that the problem is not necessarily what the ic is doing, but possibly the fact that people do not understand what the ic is doing. the panelist suggested that the ic should be commended for the processes it has undertaken, but that it also needed to address public misperceptions. it was again pointed out that perceptions can be just as damaging as reality. a participant suggested that a straightforward media appearance could help communicate the ic™s mission and challenges to the public. another participant suggested that this on its own would not be enough, but that there is a directed effort under way for enhancing transparency. a panelist agreed, suggesting that a sea change is needed, and noting (from experience in the private sector) the importance of education, communication, consistency, and awareness within an organization, along with publicizing the remediating actions taken in response to a privacy breach or incident. over time, an organization learns what to say, and who should say it, given the audience. building trust requires an ongoing commitment to a culture of both internal and external communication. another panelist noted that some in the public perceive a gap between the ic™s publicfacing commitments and its practices, noting that strategy for communicating actual practices can influence this perception. in addition, the panelist noted that the audience is diverse and could be divided up into many different subgroups, for example, u.s. and nonu.s. audiences. a participant noted that, while the laws that govern the ic™s actions are public, their meanings or interpretations are not, and some terms in the statutes are not clearly defined. in particular, many were surprised about some of the foreign intelligence surveillance court™s interpretations of law. another participant agreed, and suggested that there is a process problem, rather than a compliance problem, which could be addressed by involving adversarial viewpoints in the process, both internally and externally. it was pointed out that the ic recognizes this, and is making transparency a priority. the group discussed the private sector™s strategies for compliance with the data protection laws of different nations. sherman noted that facebook is structured around two data controllers, facebook, inc. and facebook ireland, with different regulators. the company also consults with other authorities. it aims to comply with all applicable laws in such a way that the user experience and protections are consistent across the board, regardless of what region the user lives in, although this is not always possible. for example, facebook developed a download your information tool in response to european union requirements, but has made it available to u.s. and other users as well. in the case of acxiom, products are rolled out on a countrybycountry basis, and product pias are performed independently for each country, according to glasgow. panelists concluded the session with a few comments about future strategies and the outlook for improving privacy. sherman said that it is important for organizations to stay humble and listen to the privacy concerns of those that they serve. he suggested that understanding and protecting privacy is an iterative process that requires persistent engagement and honest exchanges of view. the biggest challenge is that the landscape is not static, and organizations must be willing to revisit, adapt, and improve their practices. while current laws are a baseline, it is unlikely that they will predict upcoming technology changes, so continuous evaluation is needed. glasgow suggested that an organization should not expect perfection, especially in its early privacy strategies. regardless of the success of any individual action, persistence is necessary to make significant progress on building trust. she also suggested that the private sector may begin to face some of the same challenges that the ic has experienced as big data and machine learning continue to pervade business strategies and the concept of notice and choice breaks down. 3 data protection commissioner, 2012, facebook ireland ltd.: report of reaudit, september 21, https://dataprotection.ie/documents/press/facebookirelandauditreviewreport21sept2012.pdf. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.38 privacy research and best practices nissenbaum suggested that society is already on a path that has strayed from ethical activity; we seem to accept that massive collection and aggregation of data is the proper state of affairs, despite the absence of a rigorous argument for why this is defensible. she noted that it is hard to turn back the clock, but suggested that there might nonetheless be value in revisiting this acceptance. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 39 7  wrapupšpanel summary members of the workshop steering committee and workshop participants reflected upon key themes that had arisen during the panel discussions, and identified a few additional concepts, as summarized below. susan landau reiterated jessica staddon™s comment that the power of transparency should not be underestimated and also suggested that the accuracy of the information shared is critical to an organization™s credibility. she noted that transparency can be a challenge for the ic because many details of their operations are necessarily classified, suggesting that this makes it even more important for the ic to get the big picture right when communicating with the public. for example, she noted that inconsistency in the reported number of instances in which the use of metadata was critical for the ic had undermined the community™s credibility. landau said that her experience with the academies study on bulk collection of signals intelligence,1 showed her how seriously the ic takes the rules around data collection, access, and use. she pointed out that this is generally not clear to those on the outside, and could be better communicated through deeper engagements with the academic community. while there are complexities associated with control of data flow and use, she suggested that better funding for privacy enforcement activities in general would be beneficial. tadayoshi kohno pointed out that the absence (or perceived absence) of knowledge of ic activities among those in academia and the private sector made some of the privacy discussions challenging. he pondered the potential for developing ﬁtoy problemsﬂšscenarios designed to embody some of the challenges that the ic faces at a classified level but without actually revealing classified or restricted informationšto enable academic and industry researchers to better understand and make progress on some of the ic™s privacy challenges. kohno noted that, because time was limited, the workshop discussions of emerging technologies were not comprehensive. he pointed out that opportunities for discussing and learning about emerging technologies could also be found at other venues, such as the annual consumer electronics show or through interactions with venture capital firms. he reiterated mark mcgovern™s point that the audacity of new technologies makes it hard to anticipate their privacy implications. he pointed out that this is likely also true for any audacious new capabilities the ic might develop. he identified the recurring theme that a system™s privacy must be considered from conception to deployment, and take into account evolving uses and stakeholder needs. he identified the analogous need to continuously consider privacy throughout the ic™s activities, along with the public™s perception of this need. finally, he pointed out the recurring theme of how difficult privacy is to define. frederick r. chang expressed a hope that this workshop might serve as a tipping point and help seed important discussions, solutions, and identification of privacy challenges to be solved. he noted that privacy is a hard problem, confounded by human irrationality, uncertainty, inconsistency of research results, and a lack of resources for innovation. he suggested that advancement of a ﬁscience of privacyﬂ might help to make progress, alluding to efforts under way within the ic. in particular, workshops could identify grand challenge problems; researchers could develop data sets to be shared; students could be encouraged to study this field. chang suggested that some of the gaps between the participants from the ic and those from academia and the  1national research council, 2015, bulk collection of signals intelligence: technical options, washington, d.c.: the national academies press. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.40 privacy research and best practices private sector may have narrowed today, and that additional meetings such as this one would provide even more added value. helen nissenbaum observed that the snowden disclosures were an eyeopener for those working on privacy, in part because they clarified the importance of compliance with the law and internal policies, but also because the public™s strong reaction made it clear that people had actually expected more than just compliance. she found that this meeting helped to start teasing out the intersection between technology, intelligence practices, and all of the values that are gathered together under the term privacy. she noted that the emerging technologies that define what the ic can do also define what everyone else can do, and that understanding how technologies change the world and the flow of information will enable everyone to be more intelligent and understanding about what should and should not be allowed to take place. fred h. cate echoed and elaborated upon a number of points that arose in the panel discussions: the law is not enough for protecting privacy, and it has become less sufficient over time. industry has had to come to grips with this. privacy is hardš but not impossible. important (if imperfect) steps can be taken to make a difference. transparency makes a difference. this is difficult for the ic, but, again, not impossible. the ic can articulate its values and its commitment to accountability, including a visible commitment to firing individuals who do not live up to those values. ﬁtransparentﬂ need not mean ﬁpublic.ﬂ the use of advisory boards or councils can convey message, values, and activities in a controlled way. transparency also plays an important role in perception. if people feel comfortable with things they know you are doing, they are more likely to give you the benefit of the doubt on the things that are not made transparent. many companies have recognized this. rigorous data management is critical for protecting privacy. this requires great security, appropriate responses in the wake of a breach, and building accountability into the system. creation of a value proposition can help build trust. being clear and straightforward about the benefits provided by the existence or use of a given data set, tool, or authority is valuable. workshop participants discussed these reflections and ideas, reiterated recurring themes, and added a few final reflections. there was some discussion of the ic™s oversight and classification systems, with several participants suggesting a need for reform or improvements, in order to enable more transparency and assurance that privacy principles are being upheld. several participants discussed further the notions of an art of privacy or a science of privacy. one suggested that some sort of decisionmaking support tool, based upon fundamental principles, would be very helpful to organizations with limited resources for making privacy decisions, and wondered whether efforts toward a science of privacy framework might help to produce such a practical tool. another participant recommended against the framing of an art or science of privacy, as it would seem to place the field of ethics, which provides a rigorous basis for deriving actionable principles, into the category of ﬁart.ﬂ a third participant suggested that the notion of a science of privacy might not make sense because privacy is so contextdependent. in response to the theme that privacy is difficult to define, one participant suggested that progress might be made by addressing individual facets of privacy and its associated values, such as the following: the right to be forgotten, the concept of freedom of thought, the concept of freedom from physical intrusion, and privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.wrapup 41 avoidance of being ﬁcreepy.ﬂ2 a participant asked whether technology is upending our traditional notions of privacy. another participant cautioned against thinking of technology as an independent force to which we must adapt, suggesting that society has a significant rolešand responsibilityšin shaping how technology is used. closing cate closed the workshop by thanking all participants, academies staff, the workshop steering committee, and the panelists. he also thanked david honey, director of science and technology, odni, alexander w. joel, and their colleagues from the ic for making the workshop possible. he suggested that many from academia and the private sector would be prepared to continue to engage on privacy with the intelligence community. 2 o. tene and j. polonetsky, 2015, a theory of creepy: technology, privacy, and shifting social norms, yale journal of law and technology 16.1:2, http://digitalcommons.law.yale.edu/yjolt/vol16/iss1/2/.  privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. appendixes privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 45 a workshop statement of task an ad hoc steering committee will plan and convene a twoday workshop addressing the civil liberties and privacy implications of information and communication technologies. it will look at academic and industry research, emerging approaches, and best practices and consider these in the context of intelligence collection and analysis. the workshop will explore three main themes: (1) privacy implications of emerging technologies, (2) public and individual preferences and attitudes toward privacy and the social science and behavioral economics of privacy, and (3) ethical approaches to data collection and use. this workshop will involve and foster interaction between the intelligence community and academic and industry experts, and enable participants to share knowledge and active research work via presentations, panel discussions, and q&a sessions. a rapporteurauthored workshop summary will be prepared. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 46 b workshop agenda keck center, washington, d.c. july 2122, 2015 july 21, 2015 9:00 a.m. welcome and introduction fred h. cate, indiana university, steering committee chair 9:10  background and context from the ic alexander w. joel, civil liberties protection officer, office of the  director of national intelligence  9:30  panel i on privacy implications of emerging technologies: mobile communications and the internet of things susan landau, worcester polytechnic institute, moderator panelists: mark mcgovern, mobile system 7 fuming shih, oracle lee tien, electronic frontier foundation tao zhang, cisco 10:20  break 10:50  panel i on privacy implications of emerging technologies mobile communications and the internet of things (continued) 11:50  lunch 1:00 p.m. panel ii on privacy implications of emerging technologies: biometrics, analytics, and health it tadayoshi kohno, university of washington, moderator panelists: steven m. bellovin, columbia university roxana geambasu, columbia university carl gunter, university of illinois urbanachampagne james wayman, san jose state university 2:50  break privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.appendix b 47 3:20  panel on social science and behavioral economics of privacy frederick r. chang, southern methodist university, moderator panelists: idris adjerid, university notre dame jessica staddon, north carolina state university katherine strandburg, new york university joseph turow, university of pennsylvania 5:10  day one wrapup fred h. cate, steering committee chair 5:15  reception july 22, 2015 9:00 a.m. welcome, outline of day™s agenda fred h. cate, steering committee chair 9:05  panel on best practices and ethical approaches for data collection and use fred h. cate, indiana university, moderator panelists: jennifer glasgow, acxiom helen nissenbaum, new york university rob sherman, facebook david vladeck, georgetown university 10:55 break 11:30  wrapup panel steering committee 12:15 p.m. adjourn privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 48 c biographical sketches workshop steering committee fred h. cate, steering committee chair, is a distinguished professor and c. ben dutton professor of law at the indiana university maurer school of law. he is managing director of the center for law, ethics, and applied research in health information, and a senior fellow and former founding director of the center for applied cybersecurity research. professor cate specializes in information privacy and security law issues. he has testified before numerous congressional committees and speaks frequently before professional, industry, and government groups. he is a senior policy advisor to the centre for information policy leadership at hunton & williams llp, a member of intel™s privacy and security external advisory board, the department of homeland security data privacy and integrity committee cybersecurity subcommittee, the national security agency™s (nsa™s) privacy and civil liberties panel, the board of directors of the privacy projects, the board of directors of the international foundation for online responsibility, and the board of directors of the kinsey institute for research in sex, gender and reproduction. previously, professor cate served as a member of the committee on technical and privacy dimensions of information for terrorism prevention of the national academies of sciences, engineering, and medicine, counsel to the department of defense technology and privacy advisory committee, reporter for the third report of the markle task force on national security in the information age, and a member of the federal trade commission™s (ftc™s) advisory committee on online access and security and microsoft™s trustworthy computing academic advisory board. he chaired the international telecommunication union™s highlevel experts on electronic signatures and certification authorities. he served as the privacy editor for the institute of electrical and electronics engineers™ (ieee™s) security & privacy and is one of the founding editors of the oxford university press journal international data privacy law. he is the author of more than 150 books and articles, and he appears frequently in the popular press. professor cate attended oxford university and received his j.d. and his a.b. with honors and distinction from stanford university. he is a senator and fellow (and immediate past president) of the phi beta kappa society, an elected member of the american law institute, and a fellow of the american bar foundation. frederick r. chang is the director of the darwin deason institute for cyber security, the bobby b. lyle endowed centennial distinguished chair in cyber security, and professor in the department of computer science and engineering in the lyle school of engineering at southern methodist university (smu). he is also a senior fellow in the john goodwin tower center for political studies in smu™s dedman college, and a distinguished scholar in the robert s. strauss center for international security and law at the university of texas at austin. dr. chang™s career spans service in the private sector, in academia, and in government, including as the former director of research at the nsa. dr. chang has been awarded the nsa director™s distinguished service medal and was the 2014 information security magazine ‚security 7™ award winner for education. he has served as a member of the commission on cyber security for the 44th presidency and as a member of the computer science and telecommunications board of the academies. he has also served as a member of the academies™ committee on responding to section 5(d) of presidential policy directive 28: the feasibility of software to provide alternatives to bulk signals intelligence collection. he twice served as a cyber security expert witness at hearings convened by the u.s. house of privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.appendix c 49 representatives™ committee on science, space and technology. dr. chang received his b.a. from the university of california, san diego, and his m.a. and ph.d. from the university of oregon. he has also completed the program for senior executives at the sloan school of management at the massachusetts institute of technology (mit). tadayoshi kohno is the shortdooley professor of the computer science and engineering department at the university of washington and an adjunct associate professor in the university™s information school. his research focuses on computer security and privacy, broadly defined, with a particular focus on computer security and privacy for emerging and consumer technologies; computer security and privacy for mobile and cloud systems; the human element in computer security systems; and computer security education. originally trained in applied cryptography, his current research thrusts range from secure cyberphysical systems to cloud computing. dr. kohno is the recipient of a national science foundation (nsf) career award, an alfred p. sloan research fellowship, an mit technology review tr35 young innovator award, and multiple best paper awards. he is a member of the defense science study group and a founding member of the ieee symposium on secure design and the usenix security steering committee. he received his ph.d. in computer science from the university of california, san diego. susan landau is professor of cybersecurity policy in the department of social science and policy studies at worcester polytechnic institute. dr. landau has been a senior staff privacy analyst at google, a distinguished engineer at sun microsystems, and a faculty member at the university of massachusetts, amherst, and at wesleyan university. she has held visiting positions at harvard university, cornell university, yale university, and the mathematical sciences research institute. dr. landau is the author of surveillance or security? the risks posed by new wiretapping technologies (2011) and coauthor, with whitfield diffie, of privacy on the line: the politics of wiretapping and encryption (1998, rev. ed. 2007). she has written numerous computer science and public policy papers and opeds on cybersecurity and encryption policy and testified in congress on the security risks of wiretapping and on cybersecurity activities at the national institute of standards and technology (nist) information technology laboratory. she currently serves on the computer science telecommunications board of the academies. a 2015 inductee to the cybersecurity hall of fame and a 2012 guggenheim fellow, dr. landau was a 20102011 fellow at the radcliffe institute for advanced study, the recipient of the 2008 women of vision social impact award, and also a fellow of the american association for the advancement of science and the association for computing machinery (acm). she received her b.a. from princeton university, her m.s. from cornell university, and her ph.d. from mit. helen nissenbaum is professor of media, culture and communication, and computer science at new york university (nyu), where she is also director of the information law institute. dr. nissenbaum™s work spans social, ethical, and political dimensions of information technology and digital media. she has written and edited eight books, including privacy, big data and the public good: frameworks for engagement, with j. lane, v. stodden and s. bender (2014), values at play in digital games, with m. flanagan (2014), and privacy in context: technology, policy, and the integrity of social life (2010) and her research publications have appeared in journals of philosophy, politics, law, media studies, information studies, and computer science. nsf, the air force office of scientific research, the ford foundation, the u.s. department of homeland security, and the u.s. department of health and human services office of the national coordinator have supported her work on privacy, trust online, and security, as well as several studies of values embodied in computer system design, search engines, digital games, facial recognition technology, and health information systems. before joining the faculty at nyu, she served as associate director of the center for human values at princeton university. dr. nissenbaum received a ph.d. in philosophy from stanford university in 1983 and a b.a. (hons) from the university of the witwatersrand. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.50 privacy research and best practices invited panelists and speakers idris adjerid is an assistant professor of management at the mendoza college of business at the university of notre dame. his research focuses on the economics of personal information with a particular focus on the behavioral economics of privacy decision making and data ethics. dr. adjerid™s work has been published in management science, the ieee journal on privacy and security, and a number of conference proceedings and has been featured in the wall street journal, wired.com, and several other news outlets. he received a ph.d. in information systems from the heinz college of public policy at carnegie mellon university, and both an m.b.a. and b.s. in business information technology from the pamplin college of business at the virginia polytechnic institute and state university. steven m. bellovin is the percy k. and vidal l. w. hudson professor of computer science at columbia university, where he does research on networks, security, and especially why the two don™t get along, as well as related public policy issues. in his copious spare professional time, he does some work on the history of cryptography. dr. bellovin joined the faculty in 2005 after many years at bell labs and at&t labs research, where he was an at&t fellow. he received a b.a. from columbia university and an m.s. and ph.d. in computer science from the university of north carolina, chapel hill. while a graduate student, he helped create netnews; for this, he and the other creators were given the 1995 usenix lifetime achievement award (ﬁthe flameﬂ). dr. bellovin has served as chief technologist of the ftc. he is a member of the national academy of engineering and is serving on the computer science and telecommunications board of the academies. he is a past member of the department of homeland security™s science and technology advisory committee and the technical guidelines development committee of the election assistance commission; he also received the 2007 nist/nsa national computer systems security award and has been elected to the cybersecurity hall of fame. dr. bellovin is the coauthor of firewalls and internet security: repelling the wily hacker and holds a number of patents on cryptographic and network protocols. he has served on many academies study committees, including those on information systems trustworthiness, the privacy implications of authentication technologies, and cybersecurity research needs; he was also a member of the information technology subcommittee of a study group on science versus terrorism. he was a member of the internet architecture board from 1996 until 2002; he was codirector of the security area of the internet engineering task force from 2002 through 2004.  roxana geambasu is an assistant professor of computer science at columbia university. she joined columbia in the fall of 2011 after finishing her ph.d. at the university of washington. for her work in cloud and mobile data privacy, she received a microsoft research faculty fellowship, a ﬁbrilliant 10ﬂ popular science nomination, an nsf career award (all in 2014); an honorable mention for the inaugural dennis m. ritchie doctoral dissertation award in 2013, a william chan dissertation award in 2012, two best paper awards at top systems conferences (2009 and 2011), and the first google ph.d. fellowship in cloud computing (2009). jennifer glasgow has provided oversight of acxiom corporation™s global public policy, privacy, and information practices since 1991. she currently directs acxiom™s global information use policy, internal compliance with legal regulations and industry guidelines, consumer affairs, government affairs, and related public relations. in 2010 she was recognized by the international association of privacy professionals (iapp) as the profession™s first chief privacy officer and in 2011 was iapp™s vanguard winner, the highest recognition given by the association of over 20,000 members for her leadership, knowledge, and involvement in the profession. she is extremely active domestically and internationally consulting with clients and advising policy makers, regulators, and government agencies about the appropriate use of personal information. she has participated in numerous international efforts to influence the development of public policy, develop industry best practices, and achieve maximum harmonization across the world. ms. glasgow is also a regular speaker in a variety of industries, including financial services, retail, insurance, publishing, travel and entertainment, and government use of information. she is currently active on a variety of industry privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.appendix c 51 boards and councils and sits on the u.s. direct marketing association safe harbor ethics committee and cochairs the mobile marketing association™s privacy and advocacy committee. ms. glasgow is board member for the foundation for information accountability and governance organization and sits on the advisory board for the political and economic research council and lectures at the university of texas, austin, george mason university, and the university of arkansas on the subject of privacy. ms. glasgow joined acxiom after receiving a degree in mathematics and computer science from the university of texas, austin, and helping develop and install a criminal justice highway safety information system for the state of arkansas. she is active with her alma mater as a member of the ut chancellor™s council and the college of natural science foundation advisory council. she has also been elected to the arkansas academy of computing. carl gunter is a professor of computer science at the university of illinois, where he also serves as a professor in the college of medicine and as director for the illinois security lab and the health information technology center. his interests concern security and privacy, especially in specific application domains. his recent work has centered on the electric power grid and health care. he has also contributed to research and teaching in the areas of programming languages, formal methods, and networking. alexander w. joel is the civil liberties protection officer for the office of the director of national intelligence (odni). in that capacity, he leads the odni™s civil liberties and privacy office, and reports directly to the director of national intelligence. his responsibilities include ensuring that the protection of privacy and civil liberties is appropriately incorporated in intelligence community policies and procedures, overseeing compliance by the odni with privacy and civil liberties laws, reviewing complaints of possible abuses of privacy and civil liberties in programs and operations administered by the odni, and ensuring that the use of technology sustains, and does not erode, privacy. his appointment to this position was announced by director john negroponte on december 7, 2005. mr. joel had previously been performing the duties of that position on an interim basis. he has more than a decade of experience with privacy, technology, and national security law. he was motivated to enter public service following 9/11. he joined the central intelligence agency™s office of general counsel in october 2002, where he provided legal advice relating to intelligence activities. prior to joining the government, mr. joel served as the privacy, technology, and ecommerce attorney for marriott international, inc., where he helped establish and implement marriott™s global privacy compliance program, including the creation of marriott™s first privacy officer position. before that, he worked as a technology attorney at the law firm of shaw, pittman, potts & trowbridge in washington, d.c. (now pillsbury winthrop shaw pittman), and as a u.s. army judge advocate general corps officer, with assignments that included prosecutor and criminal defense counsel. mr. joel received his law degree from the university of michigan in 1987, magna cum laude, where he was a member of the michigan law review. he received his b.a. degree from princeton university in 1984, magna cum laude. mark mcgovern is the ceo of mobile system 7, an award winning enterprise security startup and the leader in identity behavior analytics. he is a respected security industry professional, with a 25year track record developing and deploying innovative security products for different enterprises. prior to founding mobile system 7, mr. mcgovern was vice president of technology for inqtel, where he led security investments for the u.s. intelligence community. in this role, he identified, developed, and deployed emerging security technologies to address strategic enterprise needs. mr. mcgovern™s investments included: arcsight, corestreet, silvertail systems, fireeye, red seal systems, and veracode. prior to joining inqtel, he was director of technology for cigital, inc. he led cigital™s software security group and supported fortune 100 clientele, including microsoft, mastercard international, citibank, and the federal reserve banks of richmond, new york, and boston. earlier in his career, mr. mcgovern worked as an engineer for the central intelligence agency. he holds a b.s. in electrical engineering from worcester polytechnic institute and an m.s. in systems engineering from virginia polytechnic institute and state university. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.52 privacy research and best practices rob sherman is the deputy chief privacy officer at facebook, where he is responsible for managing the company™s engagement on public policy issues surrounding privacy, security, and online trust. collaborating with facebook™s product teams, regulators, and other key stakeholders, mr. sherman works to build the company™s core commitments to transparency, control, and accountability into every aspect of the facebook service. he joined facebook from covington & burling llp, where he represented facebook and other leading technology and digital media companies on regulatory and public policy issues relating to privacy, data security, electronic marketing and communications, and digital content. while in private practice, mr. sherman was recognized by chambers usa as one of the nation™s leading media regulatory lawyers. fuming shih is a senior product manager at oracle cloud working on the cloud governance project. dr. shih graduated from the mit computer science and artificial intelligence laboratory in 2014. his research is about privacy and accountability for collection and uses of personal data, especially those from our connected personal devices. the work involves understanding human behavior in privacy, modeling people™s preferences for disclosing personal data, and developing tools to support usercentric privacy framework. part of his research is now seen in the privacy features implemented on apple™s iphone. jessica staddon is joining the computer science department of north carolina state university as associate professor and director of privacy. previously, dr. staddon was a research scientist and manager at google, an area manager at xerox parc, and a research scientist at bell labs and rsa labs. her interests include usable security and privacy tools, trends in privacyrelated attitudes, and methods for measuring and predicting privacyrelated behaviors, attitudes, and risks. she serves regularly on the program committees of acm and ieeesponsored security/privacy conferences and is on the editorial boards of the journal of computer security and the international journal of information and computer security and the advisory board of the association for women in mathematics. she holds a ph.d. in mathematics from university of california (uc), berkeley. katherine strandburg concentrates her teaching and research in information privacy law and in innovation law and policy, focusing on the interplay between social behavior and technological change. she has authored several amicus briefs to the u.s. supreme court and federal appellate courts dealing with patent law and privacy issues and was invited to speak at the privacy and civil liberty oversight board™s public meeting on executive order 12333 in may 2014. recent articles include ﬁfree fall: the online market™s consumer preference disconnect,ﬂ and ﬁmembership lists, metadata and freedom of association™s specificity requirement.ﬂ in 2014, she published the book governing knowledge commons (coedited with brett frischmann and michael madison). she coleads nyu™s interdisciplinary privacy research group with helen nissenbaum. dr. strandburg obtained her j.d. with high honor from the university of chicago law school and served as a law clerk to the honorable richard d. cudahy of the u.s. court of appeals for the seventh circuit. prior to her legal career, dr. strandburg was a physicist at argonne national laboratory, having received her ph.d. from cornell university and conducted postdoctoral research at carnegie mellon university. lee tien is a senior staff attorney and adams chair for internet rights at the electronic frontier foundation (eff). mr. tien began working in the area of cyberlaw in 1991, developing the first successful first amendment theory in the bernstein/junger crypto export control cases in the mid1990s. he joined eff in 2000, working on foia (freedom of information act) and first amendment issues. after september 11, 2001, his emphasis shifted to electronic surveillance law, including pen/trap, the wiretap act, the stored communications act, the foreign intelligence surveillance act, and fourth amendment issues. he currently focuses on privacy, surveillance, and security, managing most of eff™s legislative work in these areas. he has been part of eff™s nsa reform litigation teams since 2006. he also works on issues related to commercial privacy and security, including big data; do not track, and online behavioral advertising; electronic health records; biometrics; energy usage data and the smart grid; road usage charging, congestion pricing, and vehicletovehicle communication; and the internet of things. mr. tien received his undergraduate degree in privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.appendix c 53 psychology from stanford university, where he was very active in journalism at the stanford daily. after working as a news reporter at the tacoma news tribune for a year, he went to law school at boalt hall at university of california, berkeley. mr. tien also did graduate work in the program in jurisprudence and social policy at university of california, berkeley. joseph turow is robert lewis shayon professor of communication at the annenberg school for communication. professor turow is an elected fellow of the international communication association and was presented with a distinguished scholar award by the national communication association. in 2012, the truste internet privacymanagement organization designated him a ﬁprivacy pioneerﬂ for his research and writing on marketing and digitalprivacy. his forthcoming book with yale university press explores how retailers are using mobile devices to replicate internetlike surveillance and data gathering in physical stores. he has authored 10 books, edited 5, and written more than 150 articles on mass media industries. among his books are the daily you: how the new advertising industry is defining your identity and your worth (2012); niche envy: marketing discrimination in the digital age (2006); breaking up america: advertisers and the new media world (1997; paperback, 1999; chinese edition, 2004); and the hyperlinked society: questioning connections in the digital age (edited with lokman tsui, 2008). in 2010, the university of michigan press published playing doctor: television, storytelling, and medical power, a history of prime time tv and the sociopolitics of medicine, and in 2013 it won the mcgovern health communication award from the university of texas college of communication. mr. turow™s continuing national surveys of the american public on issues relating to marketing, new media, and society have received a great deal of attention in the popular press, as well as in the research community. he has written about media and advertising for the popular press, including american demographics magazine, the washington post, the boston globe, and the los angeles times. his research has received financial support from the john d. and catherine t. macarthur foundation, the kaiser family foundation, the robert wood johnson foundation, the federal communications commission, and the national endowment for the humanities, among others. mr. turow was awarded a lady astor lectureship by oxford university. he has received several conference paper and book awards and has lectured widely. he was invited to give the mcgovern lecture at the university of texas college of communication, the pockrass distinguished lecture at penn state university, and the chancellor™s distinguished lecture at louisiana state university. he currently serves on the editorial boards of the journal of broadcasting and electronic media, poetics and media industries. david c. vladeck is a professor of law at georgetown university law center, where he teaches federal courts, civil procedure, administrative law, and seminars on first amendment litigation and privacy. he also serves as faculty director for the law school™s center on privacy and technology. professor vladeck recently returned to the law school after serving for nearly 4 years as the director of the ftc™s bureau of consumer protection. before joining the law school faculty fulltime in 2002, he spent more than 25 years with public citizen litigation group, a national public interest law firm, supervising and handling complex litigation. he has briefed and argued a number of cases before the u.s. supreme court and more than 60 cases before federal courts of appeal and state courts of last resort. he is a senior fellow of the administrative conference of the united states, an elected member of the american law institute, and an appointed member of the academies™ committee on law, science and technology. he serves on the boards of the natural resources defense council and the national consumers law center. professor vladeck frequently testifies before congress and writes on privacy, consumer protection, administrative law, and first amendment issues.  james l. wayman received a ph.d. in engineering from the university of california, santa barbara, in 1980. in 1981, he joined the u.s. naval postgraduate school as an adjunct and research professor of mathematics. in 1986, he became a contractor to the u.s. department of defense in the areas of biometrics and technical security. in 1995, he joined san josé state university to head the biometric identification research center, which was named by the clinton administration as the ﬁu.s. national biometric test center,ﬂ from 1997 until 2000. he has been a member of two  committees of the computer science and privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.54 privacy research and best practices telecommunications board of the academies (authentication technologies and their privacy implications and whither biometrics?) and served for 4 years on the panel on information technology. he is currently vice chair of the department of justice/nist organization of scientific area committees™ subcommittee on speaker recognition, an ieee distinguished lecturer, and a fellow of the ieee and the institution of engineering and technology. tao zhang, an ieee fellow and cisco distinguished engineer, joined cisco in 2012 as the chief scientist for smart connected vehicles and has since also been leading initiatives to develop strategies, architectures, technology, and ecosystems for the internet of things and fog computing at cisco systems. prior to joining cisco, he was chief scientist and director of mobile and vehicular networking at telcordia technologies (formerly bellcore). for more than 25 years, he has been directing research and product development in broadband, mobile, and vehicular networks. his leadership and technical work have resulted in new technology, standards, and products. dr. zhang has coauthored two books, vehicle safety communications: protocols, security, and privacy (2012) and ipbased next generation wireless networks (2004). he holds 49 u.s. patents and has published more than 70 peerreviewed technical papers. he was a founding board director of the connected vehicle trade association. he has been serving on the industry advisory boards for several research organizations. he was the founding general chair and the steering committee vice chair for the international conference series collaboratecom, which has now evolved into the ieee collaboration and internet computing conference. dr. zhang is chair of the ieee communications society technical subcommittee on vehicular networks and telematics applications. he has been serving on editorial boards or as a guest editor for multiple ieee and other technical journals, including the ieee internet of things (iot) journal, ieee transactions on vehicular technology, ieee journal of selected areas in communications, and the springer journal of wireless networks. dr. zhang was also an adjunct professor at multiple universities. academies staff emily grumbling, a program officer with the computer science and telecommunications board of the academies, directed the workshop on privacy for the intelligence community. since joining cstb in 2014, dr. grumbling has also served as study director for the committee on information technology, automation, and the u.s. workforce and as staff to the academies™ forum on cyber resilience. she previously served as an aaas science and technology policy fellow in the directorate for computer and information science and engineering at nsf (20122014), and an american chemical society (acs) congressional fellow in the u.s. house of representatives (20112012). dr. grumbling currently serves as a volunteer associate of the acs committee on environmental improvement. she received her ph.d. in physical chemistry from the university of arizona in 2010 and her b.a. with a doublemajor in chemistry and film/electronic media arts from bard college in 2004. jon eisenberg is director of the computer science and telecommunications board of the academies. he has also been study director for a diverse body of work, including a series of studies exploring internet and broadband policy and networking and communications technologies. from 1995 until 1997, he was an aaas science, engineering, and diplomacy fellow at the u.s. agency for international development, where he worked on technology transfer and information and telecommunications policy issues. dr. eisenberg received his ph.d. in physics from the university of washington in 1996 and b.s. in physics with honors from the university of massachusetts, amherst, in 1988.  shenae bradley is an administrative assistant at the computer science and telecommunications board of the academies. she currently provides support for multiple projects, including continuing innovation in information technology; information technology, automation, and the u.s. workforce; and towards 21st century cyberphysical systems education, to name a few. prior to this, she served as a senior project privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved.appendix c 55 assistant with the board. prior to coming to the academies, she managed a number of apartment rental communities for edgewood management corporation in the maryland/dc/delaware metropolitan areas. liz euller is a senior program assistant for the academies™ board on energy and environmental systems. she currently provides support for studies focused on energy technology and policy assessments. she worked previously for the environmental law institute, the wilderness society, the chicago botanic garden, and the university of chicago survey lab. ms. euller has a b.a. in history from the university of chicago. privacy research and best practices: summary of a workshop for the intelligence communitycopyright national academy of sciences. all rights reserved. 56 d  acronyms and abbreviations cnssi committee on national security systems instruction cstb computer science and telecommunications board ehr electronic health record fipp fair information practice principle fisa foreign intelligence surveillance act ftc federal trade commission it information technology hipaa health insurance portability and accountability act hitech health information technology for economic and clinical health act ic intelligence community mit massachusetts institute of technology odni office of the director of national intelligence pia privacy impact assessment pii personally identifiable information 
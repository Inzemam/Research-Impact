detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/5018statistical software engineering84 pages | 8.5 x 11 | paperbackisbn 9780309053440 | doi 10.17226/5018panel on statistical methods in software engineering, national research councilstatistical software engineeringcopyright national academy of sciences. all rights reserved.statistical software engineeringpanel on statistical methods in software engineeringcommittee on applied and theoretical statisticsboard on mathematical sciencescommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1996istatistical software engineeringcopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of te of me.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of thecharter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific andtechnical matters. dr. bruce alberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academyof sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievement of engineers. dr. haroldliebowitz is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility givento the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, toidentify issues of medical care, research, and education. dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of scienceand technology with the academy's purposes of furthering knowledge and advising the federal government. functioning in accordance withgeneral policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. bruce alberts and dr. harold liebowitzare chairman and vicechairman, respectively, of the national research council.this project was supported by the advanced research projects agency, army research office, national science foundation, and department of the navy's office of the chief of naval research. any opinions, findings, and conclusions or recommendations expressed in thismaterial are those of the authors and do not necessarily reflect the views of the sponsors. furthermore, the content of the report does not necessarily reflect the position or the policy of the u.s. government, and no official endorsement should be inferred.copyright 1996 by the national academy of sciences. all rights reserved.library of congress catalog card number 9571101international standard book number 0309053447additional copies of this report are available from: national academy press, box 2852101 constitution avenue, n.w.washington, d.c. 2005580062462422023343313 (in the washington metropolitan area)b676printed in the united states of americaiistatistical software engineeringcopyright national academy of sciences. all rights reserved.panel on statistical methods in software engineeringdaryl pregibon, at&t bell laboratories, chairherman chernoff, harvard universitybill curtis, carnegie mellon universitysiddhartha r. dalal, bellcoregloria j. davis, nasaames research centerrichard a. demillo, bellcorestephen g. eick, at&t bell laboratoriesbev littlewood, city university, london, englandchitoor v. ramamoorthy, university of california, berkeleystaffjohn r. tucker, directoriiistatistical software engineeringcopyright national academy of sciences. all rights reserved.committee on applied and theoretical statisticsjon r. kettenring, bellcore, chairrichard a. berk, university of california, los angeleslawrence d. brown, university of pennsylvanianicholas p. jewell, university of california, berkeleyjames d. kuelbs, university of wisconsinjohn lehoczky, carnegie mellon universitydaryl pregibon, at&t bell laboratoriesfritz scheuren, george washington universityj. laurie snell, dartmouth collegeelizabeth thompson, university of washingtonstaffjack alexander, program officerivstatistical software engineeringcopyright national academy of sciences. all rights reserved.board on mathematical sciencesavner friedman, university of minnesota, chairlouis auslander, city university of new yorkhyman bass, columbia universitymary ellen bock, purdue universitypeter e. castro, eastman kodak companyfan r.k. chung, university of pennsylvaniar. duncan luce, university of california, irvinesusan montgomery, university of southern californiageorge nemhauser, georgia institute of technologyanil nerode, cornell universityimgram olkin, stanford universityronald f. peierls, brookhaven national laboratorydonald st. p. richards, university of virginiamary f. wheeler, rice universitywilliam p. ziemer, indiana universityex officio memberjon r. kettenring, bellcore chair, committee on applied and theoretical statisticsstaffjohn r. tucker, directorjack alexander, program officerruth e. o'brien, staff associatebarbara w. wright, administrative assistantvstatistical software engineeringcopyright national academy of sciences. all rights reserved.commission on physical sciences, mathematics, and applicationsrobert j. hermann, united technologies corporation, chairstephen l. adler, institute for advanced studypeter m. banks, environmental research institute of michigansylvia t. ceyer, massachusetts institute of technologyl. louis hegedus, w.r. grace and companyjohn e. hopcroft, cornell universityrhonda j. hughes, bryn mawr collegeshirley a. jackson, u.s. nuclear regulatory commissionkenneth i. kellermann, national radio astronomy observatoryken kennedy, rice universitythomas a. prince, california institute of technologyjerome sacks, national institute of statistical sciencesl.e. scriven, university of minnesotaleon t. silver, california institute of technologycharles p. slichter, university of illinois at urbanachampaignalvin w. trivelpiece, oak ridge national laboratoryshmuel winograd, ibm t.j. watson research centercharles a. zraket, mitre corporation (retired)norman metzger, executive directorvistatistical software engineeringcopyright national academy of sciences. all rights reserved.prefacethe development and the production of highquality, reliable, complex computer software have becomecritical issues in the enormous worldwide computer technology market. the capability to efficiently engineercomputer software development and production processes is central to the future economic strength,competitiveness, and national security of the united states. however, problems related to software quality,reliability, and safety persist, a prominent example being the failure on several occasions of major local andnational telecommunications networks. it is now acknowledged that the costs of producing and maintainingsoftware greatly exceed the costs of developing, producing, and maintaining hardware. thus the development andapplication of costsaving tools, along with techniques for ensuring quality and reliability in software engineering,are primary goals in today's software industry. the enormity of this software production and maintenance activityis such that any tools contributing to serious cost savings will yield a tremendous payoff in absolute terms.at a meeting of the committee on applied and theoretical statistics (cats) of the national researchcouncil (nrc), participants identified software engineering as an area presenting numerous opportunities forfruitful contributions from statistics and offering excellent potential for beneficial interactions betweenstatisticians and software engineers that might promote improved software engineering practice and cost savings.to delineate these opportunities and focus attention on contexts promising useful interactions, cats convened astudy panel to gather information and produce a report that would (1) exhibit improved methods for assessingsoftware productivity, quality, reliability, associated risk, and safety and for managing software developmentprocesses, (2) outline a program of research in the statistical sciences and their applications to softwareengineering with the aim of motivating and attracting new researchers from the mathematical sciences, statistics,and software engineering fields t o tackle these important and pressing problem areas, and (3) emphasize therelevance of using rigorous statistical and probabilistic techniques in software engineering contexts and suggestopportunities for further research in this direction.to help identify important issues and obtain a broad range of perspectives on them, the panel organized aninformationgathering forum on october 1112, 1993, at which 12 invited speakers addressed how statisticalmethods impinge on the software development process, software metrics, software dependability and testing, andsoftware visualization. the forum also included consideration of nonstandard methods and select case studies (seethe forum program in the appendix). the panel hopes that its report, which is based on the panel's expertise aswell as information presented at the forum, will contribute to positive advances in software engineering and, as asubsidiary benefit, be a stimulus for other closely related disciplines, e.g., applied mathematics, operationsresearch, computer science, and systems and industrial engineering. the panel is, in fact, very enthusiastic aboutthe opportunities facing the statistical community and hopes to convey this enthusiasm in this report.the panel gratefully acknowledges the assistance and information provided by a number of individuals,including the 12 forum speakersšt.w. keller, d. card, v.r. basili, j.c. munson, j.c. knight, r. lipton, t.yamaura, s. zweben, m.s. phadke, e.e. sumne r, jr., w. hill, and j. staskošfour anonymous reviewers, thenrc staff of the board on mathematical sciences who supported the various facets of this project, and susanmaurizi for her work in editing the manuscript.prefaceviistatistical software engineeringcopyright national academy of sciences. all rights reserved.prefaceviiistatistical software engineeringcopyright national academy of sciences. all rights reserved.contents executive summary 11 introduction 52 case study: nasa space shuttle flight control software 9 overview of requirements 9 the operational life cycle 10 a statistical approach to managing the software production process 10 fault detection 11 safety certification 123 a software production model 13 problem formulation and specification of requirements 14 design 14 implementation 16 testing 184 critique of some current applications of statistics in software engineering 27 cost estimation 27 statistical inadequacies in estimating 29 process volatility 30 maturity and data granularity 30 reliability of model inputs 31 managing to estimates 32 assessment and reliability 32 reliability growth modeling 32 influence of the development process on software dependability 36 influence of the operational environment on software dependability 37 safetycritical software and the problem of assuring ultrahigh dependability 38 design diversity, fault tolerance, and general issues of dependence 38 judgment and decisionmaking framework 39 structural modeling issues 40 experimentation, data collection, and general statistical techniques 40 software measurement and metrics 415 statistical challenges 43 software engineering experimental issues 43 combining information 46 visualization in software engineering 48contentsixstatistical software engineeringcopyright national academy of sciences. all rights reserved. configuration management data 49 function call graphs 50 test code coverage 50 code metrics 50 challenges for visualization 52 opportunities for visualization 52 orthogonal defect classification 596 summary and conclusions 61 institutional model for research 62 model for data collection and analysis 62 issues in education 64 references 67 appendix: forum program 72contentsxstatistical software engineeringcopyright national academy of sciences. all rights reserved.executive summarysoftware, a critical core industry that is essential to u.s. interests in science, technology, and defense, isubiquitous in today's society. software coexists with hardware in our transportation, communication, financial, andmedical systems. as these systems grow in size and complexity and our dependence on them increases, the need toensure software reliability and safety, fault tolerance, and dependability becomes paramount. building software isnow viewed as an engineering discipline, software engineering, which aims to develop methodologies andprocedures to control the whole software development process. besides the issue of controlling and improvingsoftware quality, the issue of improving the productivity of the software development pro cess is also becomingimportant from the industrial perspective.purpose and scope of this studyalthough statistical methods have a long history of contributing to improved practices in manufacturing andin traditional areas of science, technology, and medicine, they have up to now had little impact on softwaredevelopment processes. this report attempts to bridge the islands of knowledge and experience between statisticsand software engineering by enunciating a new interdisciplinary field: statistical software engineering. it is hopedthat the report will help seed the field of statistic al software engineering by indicating opportunities for statisticalthinking to contribute to increased understanding of software and software production, and thereby enhance thequality and productivity of both.this report is the result of a study by a panel convened by the committee on applied and theoreticalstatistics (cats), a standing committee of the board on mathematical sciences of the national researchcouncil, to identify challenges and opportunities in the development and implementation of software involvingsignificant statistical content. in addition to pointing out the relevance of rigorous statistical and probabilistictechniques to pressing software engineering concerns, the panel outlines opp ortunities for further research in thestatistical sciences and their applications to software engineering. the aim is to motivate new researchers fromstatistics and the mathematical sciences to tackle problems with relevance for software development, as well as tosuggest a statistical approach to software engineering concerns that the panel hopes software engineers will findrefreshing and stimulating. this report also touches on important issues in training and education for softwareengineers in the statistical sciences and for statisticians with an interest in software engineering.central to this report's theme, and essential to statistical software engineering, is the role of data: whereverdata are used or can be generated in the software life cycle, statistical methods can be brought to bear fordescription, estimation, an d prediction. nevertheless, the major obstacle to applying statistical methods tosoftware engineering is the lack of consistent, highquality data in the resourceallocation, design, review,implementation, and test stages of software development. statisticians interested in conducting research insoftware engineeringexecutive summary1statistical software engineeringcopyright national academy of sciences. all rights reserved.must play a leadership role in justifying that resources are needed to acquire and maintain highquality andrelevant data.the panel conjectures that the use of adequate metrics and data of good quality is the primary differentiatorbetween successful, productive software development organizations and those that are struggling. although thesingle largest area of overlap between statistics and software engineering currently concerns softwaredevelopment and production, it is the panel's view that the largest contributions of statistics to softwareengineering will be those affecting the quality and productivity of fronten d processes, that is, processes thatprecede code generation. one of the biggest impacts that the statistical community can make in softwareengineering is to combine information across software engineering projects as a means of evaluating effects oftechnology, language, organization, and process.contents of this reportfollowing an introductory opening chapter intended to familiarize readers with basic statistical softwareengineering concepts and concerns, a case study of the national aeronautics and space administration (nasa)space shuttle flight control software is presented in chapter 2 to illustrate some of the statistical issues in softwareengineering. chapter 3 describes a wellknown general software production model and associated statistical issuesand approaches. a critique of some current applications of statistics and software engineering is presented inchapter 4. chapter 5 discusses a number of statistical challenges arising in software engineering, and the panel'sclosing summary and conclusions appear in chapter 6.statistical challengesin comparison with other engineering disciplines, software engineering is still in the definition stage.characteristics of established disciplines include having defined, tested, credible methodologies for practice,assessment, and predictability. software engineering combines application domain knowledge, computer science,statistics, behavioral science, and human factors issues. statistical challenges in software engineering discussed inthis report include the following:ł generalizing particular statistical software engineering experimental results to other settings and projects, obtained in academic studies to industrial settings, across software engineering projects and studies, data analysis and visualization techniques,the software engineering community regarding statistical approaches and data issues,methods of analysis to cope with qualitative variables,executive summary2statistical software engineeringcopyright national academy of sciences. all rights reserved. providing models with the appropriate error distributions for software engineering applications, and enhancing accelerated life testing.summary and conclusionsin the 1990s, complex hardwarebased functionality is being replaced by more flexible, softwarebasedfunctionality, and massive software systems containing millions of lines of code are being created by manyprogrammers with different backgrounds, training, and skills.the challenge is to build huge, highqualitysystems in a costeffective manner. the panel expects this challenge to preoccupy the field of softwareengineering for the rest of the decade. any set of methodologies that can help in this task will be invaluable. moreimportantly, the use of such methodologies will likely determine the competitive positions of organizations andnations involved in software production. what is needed is a detailed understanding by statisticians of the software engineering process, as well as an appreciation by software engineers of what statisticians can andcannot do.catalysts essential for this productive interaction between statisticians and software engineers, and some ofthe interdisciplinary research opportunities for software engineers and statisticians, include the following: a model for statistical research in software engineering that is collaborative in nature. the idealcollaboration partners statisticians, software engineers, and a real software process or product.barriers to academic reward and recognition barriers, as well as obstacles to the funding of crossdisciplinary research, can be expected to decrease over time; in the interim, industry can play aleadership role in nurturing collaborations between software engineers and statisticians and canreduce its own set of barriers (for instance, those related to proprietary and intellectual propertyinterests). a model for data collection and analysis that ensures the availability of highquality data forstatistical approaches to issues in software engineering. careful attention to data issues ranging fromdefinition of metrics to feedback/forward loops, including exploratory data analysis, statisticalmodeling, defect analysis, and so on, is essential if statistical methods are to have any appreciable impacton a given software project under study. for this reason it is crucial that the software industry take a leadposition in research on statistical software engineering. attention to relevant issues in education. enormous opportunities and many potential benefits arepossible if the software engineering community learns about relevant statistical methods and ifstatisticians contribute to and cooperate in the education of future software engineers. some relevantareas include:executive summary3statistical software engineeringcopyright national academy of sciences. all rights reserved.š designed experiments. software engineering is inherently experimental, yet relatively few designedexperiments have been conducted. software engineering education programs must stress the desirability,where feasible, of validating new techniques using statistically valid designed experiments.š exploratory data analysis. exploratory data analysis methods are essentially ''model free," whereby theinvestigator hopes to be surprised by unexpected behavior rather than having thinking constrained towhat is expected.š modeling. recent advances in the statistical community in the past decade have effectively relaxed thelinearity assumptions of nearly all classical techniques. there should be an emphasis on educationalinformation exchange leading to more and wider use of these recently developed techniques.š risk analysis. a paradigm for managing risk for the space shuttle program, discussed in chapter 2 ofthis report, and the corresponding statistical methods can play a crucial role in identifying riskproneparts of software systems and of combined hardware and software systems.š attitude toward assumptions. software engineers should be aware that violating assumptions is not asimportant as thoroughly understanding the violation's effects on conclusions. statistics textbooks,courses, and consulting activities should convey the statistician's level of understanding about andperspective on the importance and implications of assumptions for statistical inference methods.š visualization. graphics is important in exploratory stages in helping to ascertain how complex a modelthe data ought to support; in the analysis stage, by which residuals are displayed to examine what thecurrently entertained model has failed to account for; and in the presentation stage, in which graphics canprovide succinct and convincing summaries of the statistical analysis and the associated uncertainty.visualization can help software engineers cope with, and understand, the huge quantities of data collectedas part of the software development process.š tools. it is important to identify good statistical computing tools for software engineers. an overview ofstatistical computing, languages, systems, and packages should be done that is focused specifically for thebenefit of software engineers.executive summary4statistical software engineeringcopyright national academy of sciences. all rights reserved.1introductionstatistics . the mathematics of the collection, organization, and interpretation of numerical data, especially theanalysis of population characteristics by inference from sampling.1software engineering . (1) the application of a systematic, disciplined, quantifiable approach to the development,operation, and maintenance of software; that is, the application of engineering to software. (2) the study ofapproaches as in (1).2statistical software engineering . the interdisciplinary field of statistics and software engineering specializing in theuse of statistical methods for controlling and improving the quality and productivity of the practices used in creatingsoftware.the above definitions describe the islands of knowledge and experience that this report attempts to bridge.software is a critical core industry that is essential to u.s. national interests in science, technology, and defense. itis ubiquitous in today's society, coexisting with hardware (microelectronic circuitry) in our transportation,communication, financial, and medical systems. the software in a modern cardiac pacemaker, for example,consists of approximately onehalf megabyte of code that helps control the pulse rate of patients with heartdisorders. in this and other applications, issues such as reliability and safety, fault tolerance, and dependability areobviously important. from the industrial perspective, so also are issues concerned with improving the quality andproductivity of the software development process. yet statistical methods, despite the long history of their impactin manufacturing as well as in traditional areas of science, technology, and medicine, have as yet had little impacton either hardware or software development.this report is the product of a panel convened by the board on mathematical sciences' committee onapplied and theoretical statistics (cats) to identify challenges and opportunities in software development andimplementation that have a significant statistical component. in attempting to identify interrelated aspects ofstatistics and software engineering, it enunciates a new interdisciplinary field: statistical software engineering .while emphasizing the relevance of applying rigorous statistical and probabilistic techniques to problems insoftware engineering, the panel also points out opportunities for further research in the statistical sciences andtheir applications to software engineering. its hope is that new researchers from statistics and the mathematicalsciences will thus be motivated to address relevant and pressing problems of1 see the american heritage dictionary of the english language (1981)2 see institute of electrical and electronics engineers(1990)introduction5statistical software engineeringcopyright national academy of sciences. all rights reserved.software development and also that software engineers will find the statistical emphasis refreshing andstimulating. this report also addresses the important issues of training and education of software engineers in thestatistical sciences and of statisticians with an interest in software engineering.at the panel's informationgathering forum in october 1993, 12 invited speakers described their views ontopics that are considered in detail in chapters 2 through 6 of this report. one of the speakers, john knight,pointed out that the date of the forum coincided nearly to the day with the 25th anniversary of the garmischconference (randell and naur, 1968), a natosponsored workshop at which the term "software engineering" isgenerally accepted to have originated. the particular irony of this coincidence is that it is also generally acceptedthat although much more ambitious software systems are now being built, little has changed in the relative abilityto produce software with predictable quality, costs, and dependability. one of the original garmisch participants,a.g. fraser, now associate vice president in the information sciences research division at at&t belllaboratories, defends the apparent lack of progress by the reminder that prior to garmisch, there was no"collective realization" that the problems individual organizations were facing were shared across the industryšthus garmisch was a critical first step toward addressing issues in software production. it is hoped that this reportwill play a similar role in seeding the field of statistical software engineering by indicating opportunities forstatistical thinking to help increase understanding, as well as the productivity and quality, of software and softwareproduction.in preparing this report, the panel struggled with the problem of providing the "big picture" of the softwareproduction process, while simultaneously attempting to highlight opportunities for related research on statisticalmethods. the problems facing the software engineering field are indeed broad, and nonstatistical approaches (e.g.,formal methods for verifying program specifications) are at least as relevant as statistical ones. thus this reporttends to emphasize the larger context in which statistical methods must be developed, based on the understandingthat recognition of the scope and the boundaries of problems is essential to characterizing the problems andcontributing to their solution. it must be noted at the outset, for example, that software engineering is concernedwith more than the end product, namely, code. the production process that results in code is a central concern andthus is described in detail in the report. to a large extent, the presentation of material mirrors the steps in thesoftware development process. although currently the single largest area of overlap between statistics andsoftware engineering concerns software testing (which implies that the code exists), it is the panel's view that thelargest contributions to the software engineering field will be those affecting the quality and productivity of theprocesses that precede code generation.the panel also emphasizes that the process and methods described in this report pertain to the case of newsoftware projects, as well as to the more ordinary circumstance of evolving software projects or "legacy systems."for instance, the software that controls the space shuttle flight systems or that runs modern telecommunicationnetworks has been evolving for several decades. these two cases are referred to frequently to illustrate softwaredevelopment concepts and current practice, and although the software systems may be uncharacteristically large,they are arguably forerunners of what lies ahead in many applications. for example, laser printer software iswitnessing an orderofmagnitude (base10) increase in size with each new release.introduction6statistical software engineeringcopyright national academy of sciences. all rights reserved.similar increases in size and complexity are expected in all consumer electronic products as increasedfunctionality is introduced.central to this report's theme, and essential to statistical software engineering, is the role of data, the realmwhere opportunities lie and difficulties begin. the opportunities are clear: whenever data are used or can begenerated in the software life cycle, statistical methods can be brought to bear for description, estimation, andprediction . this report highlights such areas and gives examples of how statistical methods have been and can beused.nevertheless, the major obstacle to applying statistical methods to software engineering is the lack ofconsistent, highquality data in the resourceallocation, design, review, implementation, and test stages of softwaredevelopment. statisticians interested in conducting research in software engineering must acknowledge this factand play a leadership role in providing adequate grounds for the resources needed to acquire and maintain highquality, relevant data. a statement by one of the forum participants, david card, captures the serious problem thatstatisticians face in demonstrating the value of good data and good data analysis: "it may not be that effective to beable to rigorously demonstrate a 10% or 15% or 20% improvement (in quality or productivity) when with no dataand no analysis, you can claim 50% or even 100%."the cost of collecting and maintaining highquality information to support software development isunfortunately high, but arguably essentialšas the nasa case study presented in chapter 2 makes clear. thepanel conjectures that use of adequate metrics and data of good quality is, in general, the primary differentiatorbetween successful, productive software development organizations and those that are struggling. traditionalmanufacturers have learned the value of investing in an information system to support product development;software development organizations must take heed. all too often, as a release date approaches, all availableresources are dedicated to moving a software product out the door, with the result that few or no resources areexpended on collecting data during these crucial periods. subsequent attempts at retrospective analysisšto helpforecast costs for a new product or identify root causes of faults found during product testingšare inconclusivewhen speculation rather than hard data is all that is available to work with. but even software developmentorganizations that realize the importance of historical data can get caught in a downward spiral: effort is expendedon collection of data that initially are insufficient to support inferences. when data are not being used, efforts tomaintain their quality decrease. but then when the data are needed, their quality is insufficient to allow drawingconclusions. the spiral has begun.as one means of capturing valuable historical data, efforts are under way to create repositories of data onsoftware development experiments and projects. there is much apprehension in the software engineeringcommunity that such data will not be helpful because the relevant metadata (data about the data) are not likely tobe included. the panel shares this concern because the exclusion of metadata not only encourages sometimesthoughtless analyses, but also makes it too easy for statisticians to conduct isolated research in softwareengineering. the panel believes that truly collaborative research must be undertaken and that it must be done with akeen eye to solving the particular problems faced by the software industry. nevertheless, the panel recognizesbenefits to collecting data or experimentation in software development. as is pointed out in more detail in chapter 5,one of the largest impacts the statistical communityintroduction7statistical software engineeringcopyright national academy of sciences. all rights reserved.can have in software engineering concerns efforts to combine information (nrc, 1992) across softwareengineering projects as a means of evaluating the effects of technology, language, organization, and thedevelopment process itself. although difficult issues are posed by the need to adjust appropriately for differencesin projects, the inconsistency of metrics, and varying degrees of data quality, the availability of a data repository atleast allows for such research to begin.although this report serves as a review of the software production process and related research to date, it isnecessarily incomplete. limitations on the scope of the panel's efforts precluded a fuller treatment of somematerial and topics as well as inclusion of case studies from a wider variety of business and commercial sectors.the panel resisted the temptation to draw on analogies between software development and the converging area ofcomputer hardware development (which for the most part is initially represented in software). the one approach itis confident of not reflecting is oversimplification of the problem domain itself.introduction8statistical software engineeringcopyright national academy of sciences. all rights reserved.2case study: nasa space shuttle flight control softwarethe national aeronautics and space administration leads the world in research in aeronautics and spacerelated activities. the space shuttle program, begun in the late 1970s, was designed to support exploration ofearth's atmosphere and to lead the nation back into human exploration of space.ibm's federal systems division (now loral), which was contracted to support nasa's shuttle program bydeveloping and maintaining the safetycritical software that controls flight activities, has gained much experienceand insight in the development and safe operation of critical software. throughout the program, the prevailingmanagement philosophy has been that quality must be built into software by using software reliability engineeringmethodologies. these methodologies are necessarily dependent on the ability to manage, control, measure, andanalyze the software using descriptive data collected specifically for tracking and statistical analysis. based on apresentation by keller (1993) at the panel's informationgathering forum, the following case study describes spaceshuttle flight software functionality as well as the software development process that has evolved for the spaceshuttle program over the past 15 years.overview of requirementsthe primary avionics software system (pass) is the missioncritical onboard data processing system fornasa's space shuttle fleet. in flight, all shuttle control activitiesšincluding main engine throttling, directingcontrol jets to turn the vehicle in a different orientation, firing the engines, or providing guidance commands forlandingšare performed manually or automatically with this software. in the event of a pass failure, there is abackup system. as indicated in the space shuttle flight log history, the backup system has never been invoked.to ensure high reliability and safety, ibm has designed the space shuttle computer system to have fourredundant, synchronized computers, each of which is loaded with an identical version of the pass. every 3 to 4milliseconds, the four computers check with one another to assure that they are in lock step and are doing the samething, seeing the same input, sending the same output, and so forth. the operating system is designed toinstantaneously deselect a failed computer.the pass is safetycritical software that must be designed for quality and safety at the outset. it consists ofapproximately 420,000 lines of source code developed in hal, an engineering language for realtime systems, andis hosted on flight computers with very limited memory. software is integrated within the flight control system inthe form of overlaysšonly the small amount of code necessary for a particular phase of the flight (e.g., ascent,onorbit, or entry activities) is loaded in computer memory at any one time. at quiescent points in thecase study: nasa space shuttle flight control software9statistical software engineeringcopyright national academy of sciences. all rights reserved.mission, the memory contents are "swapped out" for program applications that are needed for the next phase of themission.in support of the development of this safetycritical flight code, there are another 1.4 million lines of code.this additional software is used to build, develop, and test the system as well as to provide simulation capabilityand perform configuration control. this support software must have the same high quality as the onboardsoftware, given that flawed ground software can mask errors, introduce errors into the flight software, or providean incorrect configuration of software to be loaded aboard the shuttle.in short, ibm/loral maintains approximately 2 million lines of code for nasa's space shuttle flight controlsystem. the continually evolving requirements of nasa's spaceflight program result in an evolving softwaresystem: the software for each shuttle mission flown is a composite of code that has been implementedincrementally over 15 years. at any given time, there is a subset of the original code that has never been changed,code that was sequentially added in each update, and new code pertaining to the current release. approximately275 people support the space shuttle software development effort.the operational life cycleoriginally the pass was developed to provide a basic flight capability of the space shuttle. the first flownversion was developed and supported for flights in 1981 through 1982. however, the requirements of the flightmissions evolved to include increased operational capability and maintenance flexibility. among the shuttleprogram enhancements that changed the flight control system requirements were changes in payload manifestcapabilities and main engine control design, crew enhancements, addition of an experimental autopilot fororbiting, system improvements, abort enhancements, provisions for extended landing sites, and hardware platformchanges. following the challenger accident, which was not related to software, many new safety features wereadded and the software was changed accordingly.for each release of flight software (called an operational increment), a nominal 6 to 9 month period elapsesbetween delivery to nasa and actual flight. during this time, nasa performs system verification (to assure thatthe delivered system correctly performs as required) and validation (to assure that the operation is correct for theintended domain). this phase of the software life cycle is critical to assuring safety before a safetycriticaloperation occurs. it is a time for a complete integrated system test (flight software with flight hardware inoperational domain scenarios). crew training for mission practices is also performed at this time.a statistical approach to managing the software production processto manage the software production process for space shuttle flight control, descriptive data are systematicallycollected, maintained, and analyzed. at the beginning of the space shuttle program, global measurements weretaken to track schedules and costs. but as softwarecase study: nasa space shuttle flight control software10statistical software engineeringcopyright national academy of sciences. all rights reserved.development commenced, it became necessary to retain much more productspecific information, owing to thecritical nature of space shuttle flight as well as the need for complete accountability for the shuttle's operation. thedetail and granularity of data dictate not only the type but also the level of analysis that can be done. data relatedto failures have been specifically accumulated in a database along with all the other corollary informationavailable, and a procedure has been established for reliability modeling, statistical analysis, and processimprovement based on this information.a composite description of all space shuttle software of various ages is maintained through a configurationmanagement (cm) system. the cm data include not only a change itself, but also the lines of code affected,reasons for the change, and the date and time of change. in addition, the cm system includes data detailingscenarios for possible failures and the probability of their occurrence, user response procedures, the severity of thefailures, the explicit software version and specific lines of code involved, the reasons for no previous detection,how long the fault had existed, and the repair or resolution. although these data seem abundant, it is important toacknowledge their time dependence, because the software system they describe is subject to constant "churn."over the years, the cm system for the space shuttle program has evolved into a common, minimum set ofdata that must be retained regarding every fault that is recognized anywhere in the life cycle, including faults foundby inspections before software is actually built. this evolutionary development is amenable to evaluation bystatistical methods. trend analysis and predictions regarding testing, allocation of resources, and estimation ofprobabilities of failure are examples of the many activities that draw on the database. this database also continuesto be the basis for defining and developing sophisticated, insightful estimation techniques such as those describedby munson (1993).fault detectionmanagement philosophy prescribes that process improvement is part of the process. such proactive processimprovement includes inspection at every step of the process, detailed documentation of the process, and analysisof the process itself.the critical implications of an illtimed failure in space shuttle flight control software require that remediesbe decisive and aggressive. when a fault is identified, a feedback process involving detailed information on thefault enforces a search for similar faults in the existing system and changes the process to guard actively againstsuch faults in flight control software development. the characteristics of a single fault are actively documented inthe following fourstep reactive processimprovement protocol:1. remove the fault,2. identify the root cause of the fault,3. eliminate the process deficiency that let the fault escape earlier detection, and4. analyze the product for other, similar faults.case study: nasa space shuttle flight control software11statistical software engineeringcopyright national academy of sciences. all rights reserved.further scrutiny of what occurred in the process between introduction and detection of a fault is aimed atdetermining why downstream process elements failed to detect and remove the fault. such introspective analysis isdesigned to improve the process and specific process elements so that if a similar fault is introduced again, theseprocess elements will detect it before it gets too far along in the product life cycle. this fourstep processimprovement is achievable because of the maturity of the overall ibm/loral software management process. thecomplete recording of project events in the cm system (phase of the process, change history of involved line(s) ofcode, the line of code that included an error, the individuals involved, and so on) allows hindsight so that thedevelopment team can approach the occurrence of an error not as a failure but rather as an opportunity to improvethe process and to find other, similar errors.safety certificationthe dependability of safetycritical software cannot be based merely on testing the software, counting andrepairing the faults, and conducting "live tests" on shuttle missions. testing of software for many, many years,much longer than its life cycle, would be required in order to demonstrate software failure probability levels of107 or 109 per operational hour. a process must be established, and it must be demonstrated statistically that ifthat process is followed and maintained under statistical control, then software of known quality will result. oneresult is the ability to predict a particular level of fault density, in the sense that fault density is proportional tofailure intensity, and so provide a confidence level regarding software quality. this approach is designed to ensurethat quality is built into the software at a measurable level. ibm's historical data demonstrate a constantlyimproving process for comfort of space shuttle flight. the use of software engineering methodologies thatincorporate statistical analysis methods generally allows the establishment of a benchmark for obtaining a validmeasure of how well a product meets a specified level of quality.case study: nasa space shuttle flight control software12statistical software engineeringcopyright national academy of sciences. all rights reserved.3a software production modelthe software development process spans the life cycle of a given project, from the first idea, toimplementation, through completion. many process models found in the literature describe what is basically aproblemsolving effort. the one discussed in detail below, as a convenient way to organize the presentation, isoften described as the waterfall model. it is the basis for nearly all the major software products in use today. but aswith all great workhorses, it is beginning to show its age. new models in current use include those with design andimplementation occurring in parallel (e.g., rapid prototyping environments) and those adopting a more integrated,less linear, view of a process (e.g., the spiral model referred to in chapter 6). although the discussion in thischapter is specific to a particular model, that in subsequent chapters cuts across all models and emphasizes theneed to incorporate statistical insight into the measurement, data collection, and analysis aspects of softwareproduction.the first step of the software life cycle (boehm, 1981) is the generation of system requirements wherebyfunctionality, interactions, and performance of the software product are specified in (usually) numerousdocuments. in the design step, system requirements are refined into a complete product design, an overallhardware and software architecture, and detailed descriptions of the system control, data, and interfaces. theresult of the design step is (usually) a set of documents laying out the system's structure in sufficient detail toensure that the software will meet system requirements. most often, both requirements and design documents areformally reviewed prior to coding in order to avoid errors caused by incorrectly stated requirements or poordesign. the coding stage commences once these reviews are successfully completed. sometimes schedulingconsiderations lead to parallel review and coding activities. normally individuals or small teams are assignedspecific modules to code. code inspections help ensure that module quality, functionality, and schedule aremaintained.once modules are coded, the testing step begins. (this topic is discussed in some detail in chapter 3.) testingis done incrementally on individual modules (unit testing), on sets of modules (integration testing), and finally onall modules (system testing). inevitably, faults are uncovered in testing and are formally documented asmodification requests (mrs). once all mrs are resolved, or more usually as schedules dictate, the software isreleased. field experience is relayed back to the developer as the software is "burned in" in a productionenvironment. patches or rereleases follow based on customer response. backward compatibility tests (regressiontesting) are conducted to ensure that correct functionality is maintained when new versions of the software areproduced.the above overview is noticeably nonquantitative. indeed, this nonquantitative characteristic is the moststriking difference between software engineering and more traditional (hardware) engineering disciplines.measurement of software is critical for characterizing both the process and the product, and yet such measurementhas proven to be elusive and controversial. as argued in chapter 1, the application of statistical methods ispredicated on the existence of relevant data, and the issue of software measurements and metrics is discusseda software production model13statistical software engineeringcopyright national academy of sciences. all rights reserved.prominently throughout the report. this is not to imply that measurements have never been made or that data aretotally lacking. unfortunately metrics tend to describe properties and conditions for which it is easy to gather datarather than those that are useful for characterizing software content, complexity, and form.problem formulation and specification of requirementswithin the context of system development, specifications for required software functions are derived from thelarger system requirements, which are the primary source for determining what the delivered software productwill do and how it will do it. these requirements are translated by the designer or design team into a finishedproduct that delivers all that is explicitly stated and does not include anything explicitly forbidden. some commonreferences regarding requirements specification are mentioned in ieee standard for software productivitymetrics (ieee, 1993).requirementsšthe first formal tangible product obtained in the development of a systemare subjectivestatements specifying the system's various desired operational characteristics. errors in requirements arise for anumber of reasons, including ambiguous statements, inconsistent information, unclear user requirements, andincomplete requests. projects that have illdefined or unstated requirements are subject to constant iteration, and alack of precise requirements is a key source of subsequent software faults. in general, the longer a fault resides in asystem before it is detected, the greater is the cost of removing it or recovering from related failures. thiscondition is a primary driver of the review process throughout software development.the formulation requirements start with customers requesting a new functionality. systems engineers collectinformation describing the new functionality and develop a customer specification description (csd) describingthe customer's view of the feature. the csd is used internally by software development organizations to formulatecost estimates for bidding. after the feature is committed (sold), systems engineers write a feature specificationdescription (fsd) describing the internal view of the feature. the fsd is commonly referred to as ''requirements."both the csd and fsd are carefully reviewed and must meet formal criteria for approval.designthe heart of the software development cycle is the translation and refinement of the requirements into code.software architects transform the requirements for each specified feature into a highlevel design. as part of thisprocess, they determine which subsystems (e.g., databases) and modules are required and how they interact orcommunicate. the broad, highlevel design is then refined into a detailed lowlevel design. this transformationinvolves much information gathering and detective work. the software architects are often the most experiencedand knowledgeable of the software engineers.the sequence of continual refinements ultimately results in a mapping of highlevel functions into modulesand code. part of this design process is selecting an appropriatea software production model14statistical software engineeringcopyright national academy of sciences. all rights reserved.representation, which in most cases is a specific programming language. selection of a representation involvesfactors such as operational domain, system performance, and function, among others. when completed, the highlevel design is reviewed by all, including those concerned with the affected subsystems and the organizationresponsible for development.the human element is a critical issue in the early stages of a software project. quantitative data are potentiallyavailable following document reviews. specifically, early in the development cycle of software systems, (paper)documents are prepared describing feature requirements or feature design. prior to a formal document review, thereviewers individually read the document, noting issues that they believe should be resolved before the documentis approved and feature development is begun. at the review meeting, a single list of issues is prepared thatincludes the issues noted by the reviewers as well as the ones discovered during the meeting itself. this processthus generates data consisting of a tabulation of issues found by each reviewer. the degree of overlap providesinformation regarding the number of remaining issues, that is, those yet to be identified. if this number isacceptably small, the process can proceed to the next step; if not, further document refinement is necessary inorder to avoid costly fixes later in the process. the problem as stated bears a certain resemblance to capturerecapture models in wildlife studies, and so appropriate statistical methods can be devised for analyzing the reviewdata, as illustrated in the following example.example. table 1 contains data on issues identified for a particular feature for the at&t 5ess switch (eicket al., 1992a). six reviewers found a total of 47 distinct issues. a common capturerecapture model assumes thateach issue has the same probability of being captured (detected) and that reviewers work independently with theirown chance of capturing an issue, or detection probability. under such a model, likelihood methods yield anestimate of n = 65, implying that approximately 20 issues remain to be identified in the document. an upper 95%confidence bound for n under this model is 94 issues.such a model is natural but simplistic. the software development environment is not conducive toindependence among reviewers (so that some degree of collusion is unavoidable), and reviewers also are selectedto cover certain areas of specialization. in either case, the cornerstone of capturerecapture models, the binomialdistribution, is no longer appropriate for the total number of issues. it is possible to develop a likelihoodbased testfor pairwise collusion of reviewers and reviewerspecific tests of specialization. in the example above, there is noevidence of collusion among reviewers, but reviewer c exhibits a significantly greater degree of specializationthan do the other reviewers. when this reviewer is treated as a specialist, the maximum likelihood estimate (mle)of the number of issues is reduced to 53, implying that only a half dozen issues remain to be discovered in thedocument.other mismatches between the data arising in software review and those in capturerecapture wildlifepopulation studies induce bias in the mle. another possible estimator for this problem is the jackknife estimator(burnham and overton, 1978). but this estimator seems in fact to be more biased than the mle (vander wiel andvotta, 1993). both are rescued to a large extent by their categorization of faults into classes (e.g., "easy to find"versus "hard to find"). in any givena software production model15statistical software engineeringcopyright national academy of sciences. all rights reserved.table 1. issue discovery. the rows of the table represent 47 issues noted by six reviewers prior to review meetings. anentry in cell i,j of the table indicates that issue i (i = 1,...,47) was noted by reviewer j (j = a,...,f). rows with no entries(i.e., column sums of zero) correspond to issues discovered at the meeting.issueabcdefsumissueabcdefsum111251122112261123112711411281150291126113011711311181132119033111011341113111135112121136111303711141123811151139111604011171124111181142112191124311201124411211111154511221124611231147112411sum2534139660application, it is necessary to verify that the "easy to find" and "hard to find" classification is meaningful, orto determine that it is merely partitioning the distribution of difficulty in an arbitrary manner. a relevant point inthis and other applications of statistical methods in software engineering is that addressing aspects of the problemthat induce study bias is important and valuedtheoretical work addressing aspects of statistical bias is not likely tobe as highly valued.implementationthe phase in the software development process that is often referred to interchangeably as coding,development, or implementation is the actual transformation of the requirements into executable form."implementation in the small" refers to coding, and "implementation in the large" refers to designing an entiresystem in a topdown fashion while maintaining a perspective on the final integrated system.a software production model16statistical software engineeringcopyright national academy of sciences. all rights reserved.lowlevel designs, or coding units, are created from the highlevel design for each subsystem and modulethat needs to be changed. each coding unit specifies the changes to be made to the existing files, new or modifiedentry points, and any file that must be added, as well as other changes. after document reviews and approvals, thecoding may begin. using private copies of the code, developers make the changes and add the files specified in thecoding unit. coding is delicate work, and great care is taken so that unwanted side effects do not break any of theexisting code. after completion, the code is tested by the developer and carefully reviewed by other experts. thechanges are submitted to a public load (code from all programmers that is merged and loaded simultaneously)using an mr number. the mr is tied back to the feature to establish a correspondence between the code and thefunctionality that it provides.mrs are associated with the system version management system, which maintains a complete history ofevery change to the software and can recreate the code as it existed at any point in time. for production softwaresystems, version management systems are required to ensure code integrity, to support multiple simultaneousreleases, and to facilitate maintenance. if there is a problem, it may be necessary to back out changes. besides arecord of the affected lines, other information is kept, such as the name of the programmer making the changes,the associated feature number, whether a change fixes a fault or adds new functionality, the date of a change, andso on.the configuration management database contains the record of code changes, or change history of the code.eick et al. (1992b) describe a visualization technique for displaying the change history of source code. thegraphical technique represents each file as a vertical column and each line of code as a colorcoded row within thecolumn. the row indentation and length track the corresponding text, and the row color is tied to a statistic. if therow tracking is literal as with computer source code, the display looks as if the text had been printed in color andthen photoreduced for viewing as a single figure. the spatial pattern of color shows the distribution of the statisticwithin the text.example. developing large software systems is a problem of scale. in multimillionline systems there maybe hundreds of thousands of files and tens of thousands of modules, worked on by thousands of programmers formultiyear periods. just discovering what the existing code does is a major technical problem consumingsignificant amounts of time. a continuing and significant problem is that of code discovery, whereby programmerstry to understand how unfamiliar code works. it may take several weeks of detailed study to change a few lines ofcode without causing unwanted side effects. indeed, much of the effort in maintenance involves changing codewritten by another programmer. because of variation in programmer staff sizes and inevitable turnover, trainingnew programmers is important. visualization techniques, described further in chapter 5, can improve productivitydramatically.figure 1 displays a module composed of 20 source code files containing 9,365 lines of code. the height ofeach column indicates the size of the file. files longer than one column are continued over to the next. the rowcolor indicates the age of each line of code using a rainbow color scale with the newest lines in red and the oldestin blue. on the left is an interactive color scale showing a color for each of the 324 changes by the 126programmers modifying this codea software production model17statistical software engineeringcopyright national academy of sciences. all rights reserved.over the last 10 years. the visual impression is that of a miniature picture of all of the source code, with theindentation showing the usual c language control structure.the perception of colors is blurred, but there are clear patterns. files in approximately the same hue werewritten at about the same time and are related. rainbow files with many different hues are unstable and are likelyto be trouble spots because of all the changes. the biggest file has about 1,300 lines of code and takes a columnand a half.changes from many coding units are periodically combined together into a socalled common load of thesoftware system. the load is compiled, made available to developers for testing, and installed in the laboratorymachines. bringing the changes together is necessary so that developers working on different coding units of acommon feature can ensure that their code works together properly and does not break any other functionality.developers also use the public load to test their code on laboratory machines.after all coding units associated with a feature are complete and it has been tested by the developers in thelaboratory, the feature is turned over to the integration group for independent testing. the integration group runstests of the feature according to a feature test plan that was prepared in parallel with the fsd. eventually the newcode is released as part of an upgrade or sent out directly if it fixes a critical fault. at this stage, maintenance onthe code begins. if customers have problems, developers will need to submit fault modification requests.testingmany software systems in use today are very large. for example, the software that supports moderntelecommunications networks, or processes banking transactions, or checks individual tax returns for the internalrevenue service has millions of lines of code. the development of such largescale software systems is a complexand expensive process. because a single simple fault in a system may cripple the whole system and result in asignificant loss (e.g., loss of telephone service in an entire city), great care is needed to assure that the system isflawlessly constructed. because a fault can occur in only a small part of a system, it is necessary to assure thateven small programs are working as intended. such checking for conformance is accomplished by testing thesoftware.specifically, the purpose of software testing is to detect errors in a program and, in the absence of errors, gainconfidence in the correctness of the program or the system under test. although testing is no substitute forimproving a process, it does play a crucial role in the overall software development process. testing is importantbecause it is effective, if costly. it is variously estimated that the total cost of testing is approximately 20 to 33% ofthe total software budget for software development (humphrey, 1989). this fraction amounts to billions of dollarsin the u.s. software industry alone. further, software testing is very time consuming, because the time for testingis typically greater than that for coding. thus, efforts to reduce the costs and improve the effectiveness of testingcan yield substantial gains in software quality and productivity.a software production model18statistical software engineeringcopyright national academy of sciences. all rights reserved.figure 1. a seesofttm display showing a module with 20 files and 9,365 lines of code. each file is represented as acolumn and each line of code as a colored row. the newest rows are in red and the oldest in blue, with a colorspectrum in between. this overview highlights the largest files and program control structures, while the color showsrelationships between files, as well as unstable, frequently changed code. eick et al. (1992b).a software production model19statistical software engineeringcopyright national academy of sciences. all rights reserved.a software production model20statistical software engineeringcopyright national academy of sciences. all rights reserved.much of the difficulty of software testing is in the management of the testing process (producing reports,entering mrs, documenting mrs cleared, and so on), the management of the objects of the testing process (testcases, test drivers, scripts, and so on), and the management of the costs and time of testing.typically, software testing refers to the phase of testing carried out after parts of code are written so thatindividual programs or modules can be compiled. this phase includes unit, integration, system, product,customer, and regression testing. unit testing occurs when programmers test their own programs, and integrationtesting is the testing of previously separate parts of the software when they are put together. system testing is thetesting of a functional part of the software to determine whether it performs its expected function. product testingis meant to test the functionality of the final system. customer testing is often product testing performed by theintended user of the system. regression testing is meant to assure that a new version of a system faithfullyreproduces the desirable behavior of the previous system.besides the stages of testing, there are many different testing methods. in white box testing, tests are designedon the basis of detailed architectural knowledge of the software under test. in black box testing, only knowledge ofthe functionality of the software is used for testing; knowledge of the detailed architectural structure or of theprocedures used in coding is not used. white box testing is typically used during unit testing, in which the tester(who is usually the developer who created the code) knows the internal structure and tries to exercise it based ondetailed knowledge of the code. black box testing is used during integration and system testing, which emphasizesthe user perspective more than the internal workings of the software. thus, black box testing tries to test thefunctionality of software by subjecting the system under test to various usercontrolled inputs and by assessing itsresulting performance and behavior.since the number of possible inputs or test cases is almost limitless, testers need to select a sample, a suite oftest cases, based on their effectiveness and adequacy. herein lie significant opportunities for statisticalapproaches, especially as applied to black box testing. ad hoc black box testing can be done when testers, perhapsbased on their knowledge of the system under test and its users, decide specific inputs. another approach, based onstatistical sampling ideas, is to generate test cases randomly. the results of this testing can be analyzed by usingvarious types of reliability growth curve models (see "assessment and reliability" in chapter 4 ). randomgeneration requires a statistical distribution. since the purpose of black box testing is to simulate actual usage, ahighly recommended technique is to generate test cases randomly from the statistical distribution needed by users,often referred to as the operational profile of a system.there are several advantages and disadvantages to statistical operational profile testing. a key advantage isthat if one takes a large enough sample, then the system under test will be tested in all the ways that a user mayneed it and thus should experience fewer field faults. another advantage of this method is the possibility ofbringing the full force of statistical techniques to bear on inferential problems; that is, the results obtained duringtesting can be generalized to make inferences about the field behavior of the system under test, includinginferences about the number of faults remaining, the failure rate in the field, and so on.in spite of all these advantages, statistical operational profile testing in its purest form is rarely used. thereare many difficulties; some are operational and others are more basic. for example, one can never be certain aboutthe operational profile in terms of inputs, and especiallya software production model21statistical software engineeringcopyright national academy of sciences. all rights reserved.in terms of their probabilities of occurrence. also, for large systems, the input space is highdimensional. thus,another problem is how to sample from this highdimensional space. further, the distribution is not static; it will,in all likelihood, change over time as new users exercise the system in unanticipated ways. even if this possibilitycan be discounted, questions remain about the efficiency of statistical operational profile testing, which can bevery inefficient, because most often the system under test will be used in routine ways, and thus a randomly drawnsample will be highly weighted by routine operations. this high weighting may be fine if the number of test casesis very large. but then testing would be very expensive, perhaps even prohibitively so. therefore, testers oftenadopt some variant of drawing a random sample; for example, testers give more weight to boundary valuesšthosevalues around which the system is expected to change its behavior and therefore where faults are likely to befound. this and other clever strategies adopted by testers typically result in a testing distribution that is quitedifferent from the operational profile. of course, in such a case the results of the testing laboratory will not begeneralizable unless the relationships between the two distributions are taken into account.thus, to take advantage of the attractiveness of operational profile testing, some key problems have to besolved:1. how to obtain the operational profile,2. how to sample according to a statistical distribution in highdimensional space, and3. how to generalize results obtained in the testing laboratory to the field when the testing distribution is avariant of the operational profile distribution.all of these questions can be dealt with conceptually using statistical approaches.for (1), a bayesian elicitation procedure can be envisioned to derive the operational profile. this elicitation isdone routinely in bayesian applications, but because the space is very high dimensional, techniques are needed forbayesian elicitation in very high dimensional spaces.concerning (2), if the joint distribution corresponding to the operational profile is known, schemes can beused that are more efficient than simple random sampling schemes. simple random sampling is inefficient becauseit typically gives higher probability to the middle of a distribution than to its tails, especially in high dimensions. amore efficient scheme would sample the tails quickly. this can be accomplished by stratifying the support of thedistribution.mckay et al. (1979) formalized this idea using latin hyper cube sampling. suppose we have a kdimensional random vector x = (x1,...,xk ) and we want to get a sample of size n from the joint distribution ofx.if the components of x are independent, then the scheme is simple, namely: divide the range of each component random variable in n intervals of equal probability, randomly sample one observation for each component random variable in each of the corresponding nintervals, and finally randomly combine the components to create x .stein (1987) showed that this sampling scheme can be substantially better than simple random sampling.iman and conover (1982) and stein (1987) both discussed extensions fora software production model22statistical software engineeringcopyright national academy of sciences. all rights reserved.nonindependent component variables. of course, if specifying homogenous strata is possible, it should be doneprior to applying the latin hyper cube sampling method to increase the overall effectiveness of the samplingscheme.example : consider a software system controlling the state of an airtoground missile. the key inputs for thesoftware are altitude, attack and bank angles, speed, pitch, roll, and yaw. typically, these variables areindependently controlled. to test this software system, combinations of all these inputs must be provided and theoutput from the software system checked against the corresponding physics. one would like to generate test casesthat include inputs over a broad range of permissible values. to test all the valid possibilities, it would bereasonable to try uniform distributions for each input. suppose we decide upon a sample of size 6. thecorresponding latin hyper cube design is easily constructed by dividing each variable into six equal probabilityintervals and sampling randomly from each interval. because we have independent random variables here, thefinal step consists of randomly coupling these samples. the design is difficult to visualize in more than twodimensions, but one such sample for attack and bank angles is depicted in figure 2 . note that there is exactly oneobservation in each column and in each row, thus the name "latin hyper cube."figure 2. latin hyper cube. n = 6 and k = 2.a software production model23statistical software engineeringcopyright national academy of sciences. all rights reserved.finally, concerning (3), to make inferences about field performance, the issue of the discrepancy between thestatistical operational profile and the testing distribution must be addressed. at this point, a distinction can bemade between two types of extrapolation to field performance of the system under test. it is clear that even if thetrue operational profile distribution is not available, to the extent that the testing distribution has the same supportas the operational profile distribution, statistical inferences can be made about the number of remaining faults. onthe other hand, to extrapolate the failure intensity from the testing laboratory to the field, it is not enough to havethe same support; rather, identical distributions are needed. of course, it is unlikely that after spending much timeand money on testing, one would again test with the statistical operational profile. what is needed is a way ofreusing the information generated in the testing laboratory, perhaps by a transformation in which some statisticaltechniques based on reweighting can help. there are two basic ideas, both relying heavily on the assumption thatthe testing and the fielduse distributions have the same support. one idea is to use all the data from the testinglaboratory, but with added weights to change the sample to resemble a random sample from the operationalprofile. the approach is similar to reweighting in importance sampling. another idea is to accept or reject theinputs used in testing with a probability distribution based on the operational profile. for a description of both ofthese techniques, see beckman and mckay (1987).in his presentation at the panel's forum, phadke (1993) suggested another set of statistical techniques, basedon orthogonal arrays, for parsimonious testing of software. the example described above proves useful in anelaboration.example. for the software system that determines the state of an attack plane, let us assume that interestcenters on testing only two conditions for each input variable. this situation arises, for example, when the primaryinterest lies in boundary value testing. let the lower value be input state 0 and the upper value be input state 1 foreach of the variables. then in the language of statistical experimental design, we have seven factors, a,...,g(altitude, attack angle, bank angle, speed, pitch, roll, and yaw), each at two levels (0,1). to test all of the possiblecombinations, one would need a complete factorial experiment, which would have 27 = 128 test cases consistingof all possible sequences of 0's and 1's. for a statistical experiment intended to address only main effects, a highlyfractionated factorial design would be sufficient. however, in the case of software testing, there is no statisticalvariability and little or no interest in estimating various effects. rather, the interest is in covering the test space asmuch as possible and checking whether the test cases pass or fail. even in this case, it is still possible to usestatistical design ideas. for example, consider the sequence of test cases given in table 2. this design requires 8test cases instead of 128. in this case, since there is no statistical variation, main effects do not have any practicalmeaning. however, looking at the pattern in the table, it is clear that all possible combinations of any two pairs arecovered in a balanced way. thus, testing according to this design will protect against any incorrect implementationof the code involving a pairwise interaction.a software production model24statistical software engineeringcopyright national academy of sciences. all rights reserved.table 2a. orthogonal array. test cases in rows. test factors in columns.abcdefg100 00 0 00 200 01 1 11 301 10 0 11 411 11 1 00 510 10 1 01 610 11 0 10 711 00 1 10 811 01001table 2b. combinatorial design. test cases in rows. test factors in columns.abcdefg100000 00211111 10300111 01410001 11511100 01601010 11in general, following taguchi, phadke (1993) suggests orthogonal array designs of strength two. thesedesigns (a specific instance of which is given in the above example) guarantee that all possible pairwisecombinations will be tried out in a balanced way. another approach based on combinatorial designs was proposedby cohen et al. (1994). their designs do not consider balance to be an overriding design criterion, and accordinglythey produce designs with smaller numbers of required test cases. for example, table 2b contains a combinatorialdesign with complete pairwise coverage in six runs instead of the eight required by orthogonal arrays (table 2a).this notion has been extended to notions of higherorder coverage as well. the efficacy of these and other typesof designs has to be evaluated in the testing context.besides the types of testing discussed above, there are other statistical strategies that can be used. forexample, demillo et al. (1988) have suggested the use of fault insertion techniques. the basic idea is akin tocapturerecapture sampling in which sampled units of a population (usually wildlife) are released and inversesampling is done to estimate the unknown population size. the mothra system built by demillo and hiscolleagues implements such a scheme. while there are many possible sampling schemes (nayak, 1988), thedifficulty with fault insertion is that the faults inserted ought to be subtle enough so that the system can becompiled and tested; no two inserted faults should interact with each other; and while it may be possible at theunit testing level, it is prohibitively expensive for integration testing. it should be pointed out that the use ofcapturerecapture sampling, outlined in this chapter's subsection titled ''design," for quantifying document reviewsdoes not require fault seeding and, accordingly, is not subject to the above difficulties.another key problem in testing is determining when there has been enough testing. for unit testing wheremuch of the testing is white box and the modules are small, one can attempt to check whether all the paths havebeen covered by the test cases, an idea extended substantially by horgan and london (1992). however, forintegration and system testing, this particular approach, coverage testing, is not possible because of the size andthe number of possible paths through the system. here is another opportunity for using statistical approaches todevelop a theory of statistical coverage. coverage testing relates to deriving methods and algorithms fora software production model25statistical software engineeringcopyright national academy of sciences. all rights reserved.generating test cases so that one can state, with a very high probability, that one has checked most of the importantpaths of the software. this kind of methodology has been used with probabilistic algorithms in protocol testing,where the structure of the program can be described in great detail. (a protocol is a very precise description of theinterface between two diverse systems.) lee and yanakakis (1992) have proposed algorithms whereby one isguaranteed, with a high degree of probability, that all the states of the protocols are checked. the difficulty withthis approach is that the number of states becomes large very quickly, and except for a small part of the systemunder test, it is not clear that such a technique would be practical (under current computing technology). theseideas have been mathematically formalized in the vibrant area of theorem checking and proving (blum et al.,1990). the key idea is to take transforms of programs such that the results are invariant under these transforms ifthe software is correct. thus, any variation in the results suggests possible faults in the software. blum et al.(1989) and lipton (1989), among others, have developed a number of algorithms to give probabilistic bounds onthe correctness of software based on the number of different transformations.in all of the several approaches to testing discussed above, the number of test cases can be extraordinarilylarge. because of the cost of testing and the need to supply software in a reasonable period of time, it is necessaryto formulate rules about when to stop testing. herein lies another set of interesting problems in sequential analysisand statistical decision theory. as pointed out by dalal and mallows (1988, 1990, 1992), singpurwalla (1991), andothers, the key issue is to explicitly incorporate the economic tradeoff between the decision to stop testing (andabsorb the cost of fixing subsequent field faults) and the decision to continue testing (and incur ongoing costs tofind and fix faults before release of a software product). since the testing process is not deterministic, the faultfinding process is modeled by a stochastic reliability model (see chapter 4 for further discussion). the opportunemoment for release is decided using sequential decision theory. the rules are simple to implement and have beenused in a number of projects. this framework has been extended to the problem of buying software with some sortof probabilistic guarantee on the number of faults remaining (dalal and mallows, 1992). another extension withpractical importance (dalal and mcintosh, 1994) deals with the issue of a system under test not having beencompletely delivered at the start of testing. this situation is a common occurrence for large systems, where inorder to meet scheduling milestones, testing begins immediately on modules and sets of modules as they arecompleted.a software production model26statistical software engineeringcopyright national academy of sciences. all rights reserved.4critique of some current applications of statistics in softwareengineeringcost estimationone of software engineering's longstanding problems is the considerable inaccuracy of the cost, resource,and schedule estimates developed for projects. these estimates often differ from the final costs by a factor of twoor more. such inaccuracies have a severe impact on process integrity and ultimately on final software quality. fivefactors contribute to this continuing problem:1. most cost estimates have little statistical basis and have not been validated;2. the value of historical data in developing predictive models is limited, since no consistent softwaredevelopment process has been adopted by an organization;3. the maturity of an organization's process changes the granularity of the data that can be usedeffectively in project cost estimation;4. the reliability of inputs to cost estimation models varies widely; and5. managers attempt to manage to the estimates, reducing the validity of historical data as a basis forvalidation.certain of the above issues center on the socalled maturity of an organization (humphrey, 1988). from apurely statistical research perspective, (5) may be the most interesting area, but the major challenge facing thesoftware community is finding the right metrics to measure in the first place.example. the data plotted in figure 3 pertain to the productivity of a conventional cobol developmentenvironment (kitchenham, 1992). for each of 46 different products, size (number of entities and transactions) andeffort (in personhours) were measured. from figure 3, it is apparent that despite substantial variability, a strong(loglog) linear relationship exists between program size and program effort.a simple model relating effort to size islog10 (effort ) =  + ß log10 (size ) + noise.critique of some current applications of statistics in software engineering27statistical software engineeringcopyright national academy of sciences. all rights reserved.figure 3. data on the relationship between development effort and product size in a cobol developmentorganization.a least squares fit to these data yieldscoeff. se tintercept 1.120 0.3024 3.702log10(size) 1.049 0.1250 8.397rms 0.194these fitted coefficients suggest that development effort is proportional to product size; a formal test of thehypothesis, h: ß = 1, gives a t value at the .65 significance level.the estimated intercept after fixing ß = 1 is 1.24; the resulting fit and a 95% prediction interval are overlaidon the data in figure 3. this model predicts that it requires approximately 17 hours (= 101.24) to implement eachunit of size.such models are used for prediction and tool validation. consider an additional observation made of aproduct developed using a fourthgeneration language and relational databases. under the experimentaldevelopment process, it took 710 hours to implement the product of size 183 (this point is denoted by x infigure 3). the fitted model predicts that this product would havecritique of some current applications of statistics in software engineering28statistical software engineeringcopyright national academy of sciences. all rights reserved.taken approximately 3,000 hours to complete using the conventional development environment. the 95%prediction interval at x = 183 ranges from approximately 1,000 to 9,000 hours; thus, assuming that other factorsare not contributing to the apparent short development cycle of this product, the use of those new fourthgenerationtools has demonstrably decreased the development effort (and hence the cost).statistical inadequacies in estimatingmost cost estimation methods develop an initial relationship between the estimated size of a system (in linesof code, for instance) and the resources required to develop it. such equations are often of the form illustrated inthe above example: effort is proportional to size raised to the ß power. this initial estimate is then adjusted by anumber of factors that are thought to affect the productivity of the specific project, such as the experience of theassigned staff, the available tools, the requirements for reliability, and the complexity of the interaction with thecustomer. thus the estimating equation assumes the log linear form:effort  size ß × a i a j a k a l a m ...a z ,where the a's are the coefficients for the adjustment factors. unfortunately, these adjustment factors are nottreated as variables in a regression equation; rather, each has a set of fixed coefficients (termed "weightingfactors") associated with each level of the variable. these are independently applied as if the variables wereuncorrelated (an assumption known to be incorrect). these weighting schemes have been developed based onintuition about each variable's potential impact rather than on a statistical model fitting using historical data. thus,although the relationship between effort and size is often recalibrated for different organizations, the weightingfactors are not.exacerbating the problems with existing cost estimation models is the lack of rigorous validation of theequations. for instance, boehm (1981) has acknowledged that his wellknown cocomo estimating model wasnot developed using statistical methods. many individuals marketing cost estimation modeling tools denigrate thevalue of statistical approaches compared to clever intuition. to the extent that analytical methods are used in thedevelopment or validation of these models, they are often performed on data sets that contain as many predictorvariables (productivity factors) as projects. thus determination of the separate or individual contributions of thevariables almost certainly depends too much on chance and can be distorted by collinear relationships. thesemodels are rarely subjected to independent validation studies. further, little research has been done that attempts torestrict these models to including only those productivity factors that really matter (i.e., subset selection).because of the lack of statistical rigor in most cost estimation models, software development organizationsusually handcraft weighting schemes to fit their historical results. thus, the specific instantiation of most costestimation models differs across organizations. under these conditions, crossvalidation of the weighting schemesis very difficult, if not impossible. a newcritique of some current applications of statistics in software engineering29statistical software engineeringcopyright national academy of sciences. all rights reserved.approach to developing cost estimation models would be beneficial, one that invokes sound statistical principles infitting such equations to historical data and to validating their applicability across organizations. if the instantiationof such models is found to be domain specific, statistically valid methods should be sought for regeneratingaccurate models in different domains.process volatilityin immature software development organizations, the processes used differ across projects because they arebased on the experiences and preferences of the individuals assigned to each project, rather than on commonorganizational practice. thus, in such organizations cost estimation models must attempt to predict the results of aprocess that varies widely across projects. in poorly run projects the signaltonoise ratio is low, in that there islittle consistent practice that can be used as the basis for dependable prediction. in such projects, neither the sizenor the productivity factors provide any consistent insight into the resources required, since they are notsystematically related to the processes that will be used.the historical data collected from projects in immature software development organizations are difficult tointerpret because they reflect widely divergent practices. such data sets do not provide an adequate basis forvalidation, since process variation can mask underlying relationships. in fact, because the relationships amongindependent variables may change with variations in the process, different projects may require different values ofthe parameters in the cost estimation models. as organizations mature and stabilize their processes, the accuracyof the estimating models they use usually increases.maturity and data granularityin mature organizations the software development process is well defined and is applied consistently acrossprojects. the more carefully defined the process, the finer the granularity of the processes that can be measured.thus, as software organizations mature, the entire basis for their cost estimation models can change. immatureorganizations have data only at the level of overall project size, number of personyears required, and overall cost.with increasing organizational maturity, it becomes possible to obtain data on process details such as how manyreviews must be conducted at each life cycle stage based on the size of the system, how many test cases must berun, and how many defects must be fixed based on the defect removal efficiency of each stage of the verificationprocess. thus, estimation in fully developed organizations can be based on a bottomup analysis in which thehistorical data can be more accurate because the objects of estimation, and the effort they require, are more easilycharacterized.as organizations mature, the structure of relevant cost estimation models can change. when process modelsare not defined in detail, models must take the form of regression equations based on variables that describe thetotal impact of a predictor variable on a project'scritique of some current applications of statistics in software engineering30statistical software engineeringcopyright national academy of sciences. all rights reserved.development cycle. there is little notion in these models of the detailed practices that make up the totality. inmature organizations such practices are defined and can be analyzed individually and built up into a total estimate.normally the errors in estimating these smaller components are smaller than the corresponding error at the totalproject level, and it is assumed that the summary effect of aggregating these smaller errors is still smaller than theerror in the estimate at the total project level.reliability of model inputseven if a cost estimation model is statistically sound, the data on which it is based can have low validity.often, managers do not have sufficient knowledge of crucial variables that must be entered into a model, such asthe estimated size of various individual components of a system. in such instances, processes exist for increasingthe accuracy of these data. for instance, delphi techniques can be used by software engineers who have previousexperience in developing various system components. the less experience an organization has with a particularcomponent of a system, the less reliable is the size estimate for that component. typically, component sizes areunderestimated, with ruinous effects on the resources and schedule estimated for a project. sometimes historical"fudge factors" are applied to account for underestimation, although a more rigorous databased approach isrecommended. to aid in identifying the potential risks in a software development project, it would also bebeneficial to have reliable confidence bounds for different components of the estimated size or effort.statistical methods can be applied to develop prior probabilities (e.g., for bayesian estimation models) fromknowledgeable software engineers and to adjust these using historical data. these methods should be used not onlyto suggest the confidence that can be placed in an estimate, but also to indicate the components within a systemthat contribute most to inaccuracies in an estimate.as projects progress during their life cycle from specifications of requirements to design to generation ofcode, the information on which estimates can be based grows more reliable: there is thus greater certainty inestimating from the architectural design of a system or the detailed design of each module than in estimating fromtextual statements. in short, the sources from which estimates can be developed change as the project continuesthrough its development cycle. each succeeding level of input is a more reliable indicator of the ultimate systemsize than are the inputs available in earlier stages of development. thus the overall estimate of size, resources, andschedule potentially becomes more accurate in succeeding phases of a project. yet it is important to determine themost accurate indicators of crucial parameters such as size, effort, and schedule very early in a project, when theleast reliable data are available. as such, there is a need for statistically valid ways of developing model inputsfrom less reliable forms of data (these inputs must reliably estimate later measures that will be more valid inputs)and of estimating how much error is introduced into an estimate based on the reliability of the inputs.critique of some current applications of statistics in software engineering31statistical software engineeringcopyright national academy of sciences. all rights reserved.managing to estimatescomplicating the ability to validate cost estimation models from historical data is the fact that projectmanagers try to manage their projects to meet received estimates for cost, effort, schedule, and other suchvariables. thus, an estimate affects the subsequent process, and historical data are made artificially more accurateby management decisions and other factors that are often masked in project data. for instance, projects whoserequired level of effort has been underestimated often survive on large amounts of unreported overtime put in bythe development staff. moreover, many managers are quite skilled at cutting functionality from a system in orderto meet a delivery date. in the worst cases, engineers shortcut their ordinary engineering processes to meet anunrealistic schedule, usually with disastrous results. techniques for modeling systems dynamics provide one wayto characterize some of the interactions that occur between an estimate and the subsequent process that isgenerated by the estimate (abdelhamid, 1991).the validation of cost estimation models must be conducted with an understanding of such interactionsbetween estimates and a project manager's decisions. some of these dynamics may be usefully described bystatistical models or by techniques developed in psychological decision theory (kahneman et al., 1982). thus, itmay be possible to develop a statistical dynamic model (e.g., a multistage linear model) that characterizes thereliability of inputs to an estimate, the estimate itself, decisions made based on the estimate, the resultingperformance of the project, measures that emerge later in the project, subsequent decision making based on theselater measures, and the ultimate performance of the project. such models would be valuable in helping projectmanagers to understand the ramifications of decisions based on an initial estimate and also on subsequent periodicupdates.assessment and reliabilityreliability growth modelingmany reliability models of varying degrees of plausibility are available to software engineers. these modelsare applied at either the testing stage or the fieldmonitoring stage. most of the models take as input either failuretime or failure count data and fit a stochastic process model to reflect reliability growth. the differences among themodels lie principally in assumptions made based on the underlying stochastic process generating the data. a briefsurvey of some of the wellknown models and their assumptions and efficacy is given in abdelghaly et al.(1986).although many software reliability growth models are described in the literature, the evidence suggests thatthey cannot be trusted to give accurate predictions in all cases and also that it is not possible to identify a prioriwhich model (if any) will be trustworthy in a particularcritique of some current applications of statistics in software engineering32statistical software engineeringcopyright national academy of sciences. all rights reserved.context. no doubt work will continue in refining these models and introducing "improved" ones. although suchwork is of some interest, the panel does not believe that it merits extensive research by the statistical community,but thinks rather that statistical research could be directed more fruitfully to providing insight to the users of themodels that currently exist.the problem is validation of such models with respect to a particular data source, to allow users to decidewhich, if any, prediction scheme is producing accurate results for the actual software failure process underexamination. some work has been done on this problem (abdelghaly et al., 1986; brocklehurst and littlewood,1992), using a combination of probability forecasting and sequential prediction, the socalled prequential approachdeveloped by dawid (1984), but this work has so far been rather informal. it would be helpful to have moreprocedures for assessing the accuracy of competing prediction systems that could then be used routinely byindustrial software engineers without advanced statistical training.statistical inference in the area of reliability tends almost invariably to be of a classical frequentist kind, eventhough many of the models originate from a subjective bayesian probability viewpoint. this unsatisfactory stateof affairs arises from the sheer difficulty of performing the computations necessary for a proper bayesian analysis.it seems likely that there would be profit in trying to overcome these problems, perhaps via the gibbs samplingapproach (see, e.g., smith and roberts, 1993).another fruitful avenue for research concerns the introduction of explanatory variables, socalled covariates,into software reliability growth models. most existing models assume that no explanatory variables are available.this assumption is assuredly simplistic concerning testing for all but small systems involving short developmentand life cycles. for large systems(i.e., those with more than 100,000 lines of code) there are variables, other thantime, that are very relevant. for example, it is typically assumed that the number of faults (found and unfound) in asystem under test remains stableši.e., that the code remains frozenšduring testing. however, this is rarely thecase for large systems, since aggressive delivery cycles force the final phases of development to overlap with theinitial stages of system testing. thus, the size of code and, consequently, the number of faults in a large system canvary widely during testing. if these changes in code size are not considered, the result, at best, is likely to be anincrease in variability and a loss in predictive performance, and at worst, a poorly fitting model with unstableparameter estimates. taking this logic one step further suggests the need to distinguish between new lines of code(new faults) and code coming from previous releases (old faults), and possibly the age of different parts of code.of course, one can carry this logic to an extreme and have unwieldy models with many covariates. in practice,what is required is a compromise between the two extremes of having no covariates and having hundreds of them.this is where opportunities abound for applying stateoftheart statistical modeling techniques. described brieflybelow is a case study reported by dalal and mcintosh (1994) dealing with reliability modeling when code ischanging.critique of some current applications of statistics in software engineering33statistical software engineeringcopyright national academy of sciences. all rights reserved.example. consider a new release of a large telecommunications system with approximately 7 millionnoncommentary source lines (ncsls) and 400,000 lines of noncommentary new or changed source lines(ncncsls). for a faster delivery cycle, the source code used for system test was updated every night throughoutthe test period. at the end of each of 198 calendar days in the test cycle, the number of faults found, ncncsls,and the staff time spent on testing were collected. figure 4 (top) portrays growth of the system as a function ofstaff time. the data are provided in table 3.figure 4. plots of module size (ncncsls) versus staff time (days) for a large telecommunications software system(top). observed and fitted cumulative faults versus staff time (bottom). the dotted line (barely visible) represents thefitted model, the solid line represents the observed data, and the dashed line (also difficult to see) is the extrapolationof the fitted model.critique of some current applications of statistics in software engineering34statistical software engineeringcopyright national academy of sciences. all rights reserved.table 3. data on cumulative size (ncncsls), cumulative staff time (days), and cumulative faults for a largetelecommunications system on 198 consecutive calendar days (with duplicate lines representing weekends or holidays).source: dalal and mcintosh (1994).critique of some current applications of statistics in software engineering35statistical software engineeringcopyright national academy of sciences. all rights reserved.assume that the testing process is observed at time t i , i = 0 , ... , h , , and at any given time, the amount oftime it takes to find a specific ''bug" is exponential with rate m . at time , the total number of faults remaining inthe system is poisson with mean l i +1, and ncncsl is increased by an amount . this change adds a poissonnumber of faults with mean proportional to c, say qc i . these assumptions lead to the mass balance equation,namely, that the expected number of faults in the system at ti (after possible modification) is the expected numberof faults in the system atti 1 adjusted by the expected number found in the interval (t i 1, t i ) plus the faultsintroduced by the changes made at t i :l i+1 = liem( tit i1)+qci,for i =1,...h . note that represents the number of new faults entering the system per additional ncncsl, andrepresents the number of faults in the code at the start of system test. both of these parameters make it possible todifferentiate between the new code added in the current release and the older code. for the data at hand, theestimated parameters are q = 0.025 ,m = 0.002, and l 1 = 41 . the fitted and the observed data are plotted againststaff time in figure 4 (bottom). the fit is evidently very good. of course assessing the model on independent ornew data is required for proper validation.the efficacy of creating a statistical model is now examined. the estimate of q is highly significant, bothstatistically and practically, showing the need for incorporating changes in ncncsls as a covariate. its numericalvalue implies that for every additional 10,000 ncncsls added to the system, 25 faults are being added as well.for these data, the predicted number of faults at the end of the test period is poisson distributed with mean 145.dividing this quantity by the total ncncsls gives 4.2 per 10,000 ncncsls as an estimated field fault density.these estimates of the incoming and outgoing quality are very valuable in judging the efficacy of system testingand for deciding where resources should be allocated to improve the quality. here, for example, system testingwas effective in that it removed 21 of every 25 faults. however, it raises another issue: 25 faults per 10,000ncncsls entering system test may be too high and a plan ought to be considered to improve the incomingquality.none of the above conclusions could have been made without using a statistical model. these conclusions arevaluable for controlling and improving the reliability testing process. further, for this analysis it was essential tohave a covariate other than time.influence of the development process on software dependabilityas noted above, surprisingly little use has been made of explanatory variable models, such as proportionalhazards regression, in the modeling of software dependability. a major reason, the panel believes, is the difficultythat software engineers have in identifying variables that cancritique of some current applications of statistics in software engineering36statistical software engineeringcopyright national academy of sciences. all rights reserved.play a genuinely explanatory role. another difficulty is the comparative paucity of data owing to the difficulties ofreplication. thus, for example, for purposes of identifying those attributes of the software development processthat are drivers of the final product's dependability, it is very difficult to obtain something akin to a "randomsample" of "similar" subject programs. those issues are not unlike the ones faced in other contexts where thesetechniques are used, for example, in medical trials, but they seem particularly acute for evaluation of softwaredependability.a further problem is that the observable in this software development application is a realization of astochastic process, and not merely of a lifetime random variable. thus there seems to be an opportunity forresearch into models that, on the one hand, capture current understanding of the nature of the growth in reliabilitythat takes place as a result of debugging and, on the other hand, allow input about the nature of the developmentprocess or the architecture of the product.influence of the operational environment on software dependabilityit can be misleading to talk of the reliability of a program: as is the case for the reliability of hardware, thereliability of a program depends on the nature of its use. for software, however, one does not have the simplenotions of stress that are sometimes plausible in the hardware context. it is thus not possible to infer the reliabilityof a program in one environment from evidence of the program's failure behavior in another. this is a seriousdifficulty for several reasons.first, one would like to be able to predict the operational reliability of a program from test data. the simplestapproach at present is to ensure that the test environment, that is, the type of usage, is exactly similar to, or differsin known proportions for specified strata from, the operational environment. real software testing regimes areoften deliberately made to be different from operational ones, since it is claimed that in this way reliability can beachieved more efficiently: this argument is similar to that for hardware stress testing but is much less convincing inthe software context.a further reason to be interested in this problem of inferring program reliability is that most software getsbroadly distributed to diverse locations and is used very differently by different users: there is great disparity in thepopulation of user environments. vendors would like to be able to predict different users' perceptions of aproduct's reliability, but it is clearly impractical to replicate in a test every different possible operationalenvironment. vendors would also like to be able to predict the characteristics of a population of users. thus itmight be expected that a less disparate population of users would be preferable to a more disparate one: in theformer case, for example, problems reported at different sites might be similar and thus be less expensive to fix.explanatory variable modeling may play a useful role if suitably informative, measurable attributes ofoperational usage can be identified. there may be other ways of forming stochastic characterizations ofoperational environments. markov models of the successive activation of modules, or of functions, have beenproposed (littlewood, 1979; siegrist, 1988a,b) but have notcritique of some current applications of statistics in software engineering37statistical software engineeringcopyright national academy of sciences. all rights reserved.been widely used. further work on such approaches, and on the problems of statistical inference associated withthem, could be promising.safetycritical software and the problem of assuring ultrahigh dependabilityit seems clear that computers will play increasingly critical roles in systems upon which human lives depend.already, systems are being built that require extremely high dependabilityša figure of 109 probability of failureper hour of flight has been stated as the requirement for recent flybywire systems in civil aircraft. there are clearlimitations to the levels of dependability that can be achieved when we are building systems of a complexity thatprecludes claims that they are free of design faults. more importantly, even if we were able to build a system tomeet a requirement for ultrahigh dependability, we could have only low confidence that we had achieved thatgoal, because the problem of assessing these levels is such that it would be impractical to acquire sufficientsupporting evidence (littlewood and strigini, 1993).although a complete solution to the problem of assessing ultrahigh dependability is not anticipated, there iscertainly room for improving on what can be done currently. probabilistic and statistical problems abound in thisarea, and it is necessary to squeeze as much as possible from relatively small amounts of often disparate evidence.the following are some of the areas that could benefit from investigation.design diversity, fault tolerance, and general issues of dependenceone promising approach to the problem of achieving high dependability (here reliability and/or safety) isdesign diversity: building two or more versions of the required program and allowing an adjudication mechanism(e.g., a voter) to operate at runtime. although such systems have been built and are in operation in safetycriticalcontexts, there is little theoretical understanding of their behavior in operation. in particular, the reliability andsafety models are quite poor.for example, there is ample evidence (knight and leveson, 1986) that, in the presence of design faults, onecannot simply assume that different versions will fail independently of one another. thus the simple hardwarereliability models that involve mere redundancy, and assume independence of component failures, cannot be used.it is only quite recently that probability modeling has started to address this problem seriously (eckhardt and lee,1985; littlewood and miller, 1989). these models provide a formal conceptual framework within which it ispossible to reason about the subtle issues of conditional independence involved in the failure processes of designdiverse systems. however, they provide little quantitative practical assistance to a software designer or evaluator.further probabilistic modeling is needed to elucidate some of the complex issues. for example, little attentionhas been paid to modeling the full fault tolerant system, involving diversity and adjudication. in particular, theproperties of the stochastic process of failures ofcritique of some current applications of statistics in software engineering38statistical software engineeringcopyright national academy of sciences. all rights reserved.such systems are not understood. if, as seems likely, individual versions of a program in a realtime control systemexhibit clusters of failures in time, how does the cluster process of the system relate to the cluster processes of theindividual versions? although such issues seem narrowly technical, they are vitally important in the design of realsystems, whose physical integrity may be sufficient to survive one or two failed input cycles, but not many.another area that has had little work is probabilistic modeling of different possible adjudication mechanismsand their failure processes.judgment and decisionmaking frameworkalthough probability seems to be the most appropriate mechanism for representing uncertainty about systemdependability, other candidates such as shaferdempster and possibility theories might be plausible alternatives insafetycritical contexts where quantitative measures are required in the absence of datašfor example, when one isforced to rely on the engineering judgment of an expert. further work is needed to elucidate the relativeadvantages and disadvantages of the different approaches applicable in the software engineering domain.there is evidence that human judgment, even in "hard" sciences such as physics, can be seriously in error(henrion and fischhoff, 1986): people seem to make consistent errors and tend to be optimistic in their ownjudgment regarding their likely error. it is likely that software engineering judgments are similarly fallible, and sothis area calls for some statistical experimentation. in addition, it would be beneficial to have formal mechanismsfor assessing whether judgments are well calibrated and for recalibrating judgment and prediction schemes (ofhumans or models) that have been shown to be inaccurate. this problem has some similarity to the problems ofvalidating software reliability models, already mentioned, in which prequential likelihood plays a vital role. it alsobears on more general applications of bayesian modeling where elicitation of a priori probability values isrequired.it seems inevitable that reasoning and judgment about the fitness of safetycritical systems will depend onevidence that is disparate in nature. such evidence could include data on failures, as in reliability growth models;human expert judgment; results regarding the efficacy of development processes; information about thearchitecture of a system; or evidence from formal verification. if the required judgment depends on a numericalassessment of a system's dependability, there are clearly important issues concerning the composition of verydifferent kinds of evidence from different sources. these issues may, indeed, be overriding when it comes tochoosing among the different ways of representing uncertainty. the bayes theorem, for example, may provide aneasier way than does possibility theory to combine information from different sources of uncertainty.a particularly important problem concerns the way in which deterministic reasoning can be incorporated intothe final assessment of a system. formal methods of achieving dependability are becoming increasinglyimportant. such methods range from formal notations, which assist in the elicitation and expression ofrequirements, to full mathematical verification of the correspondence between a formal specification and animplementation. one view is that these approaches incorporating deterministic reasoning to system developmentremove a particularcritique of some current applications of statistics in software engineering39statistical software engineeringcopyright national academy of sciences. all rights reserved.type of uncertainty, leaving others untouched (uncertainty about the completeness of a formal specification, thepossibility of incorrect proof, and so on). one should factor into the final assessment of a system's dependabilitythe contribution from such deterministic, logical evidence, nevertheless keeping in mind that there is an irreducibleuncertainty in one's possible knowledge of the failure behavior of a system.structural modeling issuesconcerns about the safety and reliability of softwarebased systems necessarily arise from their inherentcomplexity and novelty. systems now being built are so complex that they cannot be guaranteed to be free fromdesign faults. the extent to which confidence can be carried over from the building of previous systems is muchmore limited in software engineering than in "real" engineering, because softwarebased systems tend to becharacterized by a great deal of novelty.designers need help in making decisions throughout the design process, especially at the very highest level.real systems are often difficult to assess because of early decisions regarding how much system control willdepend on computers, hardware, and humans. for the airbus a320, for example, the early decision to place a highlevel of trust in the computerized flybywire system meant that this system (and thus its software) needed to have abetter than probability of failure in a typical flight. stochastic modeling might aid in such highlevel designdecisions so that designers can make "what if" calculations at an early stage.experimentation, data collection, and general statistical techniquesa dearth of data has been a problem in much of safetycritical software engineering since its inception. only ahandful of published data sets exists even for the software reliability growth problem, which is by far the mostextensively developed aspect of software dependability assessment. when the lack of data arises from the need forconfidentialityšindustrial companies are often reluctant to allow access to data on software failures because of thepossibility that people may think less highly of their productsšlittle can be done beyond making efforts to resolveconfidentiality problems. however, in some cases the available data are sparse because there is no statisticalexpertise on hand to advise on ways in which data can be collected costeffectively. it may be worthwhile toattempt to produce general guidelines for data collection that address the specific difficulties of the softwareengineering problem domain.with notable exceptions (eckhardt et al., 1991; knight and leveson, 1986), experimentation has so far played alowkey role in software engineering research. somewhat surprisingly, in view of its difficulty and cost, the mostextensive experimentation has investigated the efficacy of design diversity. other areas where experimentalapproaches seem feasible and should be encouraged include the obvious and general question of which softwaredevelopment methods are most costeffective in producing software products with desirable attributes such asdependability. statistical advice on the design of such experiments would be essential; it mightcritique of some current applications of statistics in software engineering40statistical software engineeringcopyright national academy of sciences. all rights reserved.also be the case that innovation in the design of experiments could make feasible some investigations that currentlyseem too expensive to contemplate: the main problem arises from the need for replication over many softwareproducts.on the other hand, areas where experiments can be conducted without the replication problem beingoverwhelming involve the investigation of quite restricted hypotheses about the effectiveness of specifictechniques. for example, experimentation could address whether the techniques that are claimed to be effectivefor achieving reliability (i.e., effectiveness of debugging) are significantly better than those, such as operationaltesting, that will allow reliability to be measured.software measurement and metricsmeasurement is at the foundation of science and engineering. an important goal shared by softwareengineers and statisticians is to derive reliable, reproducible, and accurate measures of software products andprocesses. measurements are important for assessing the effects of proposed "improvements" in softwareproduction, whether they be technological or process oriented. measurements serve an equally important role inscheduling, planning, resource allocation, and cost estimation (see the first section in this chapter).early pioneering work by mccabe (1976) and halstead (1977) seeded the field of software metrics; anoverview is provided by zuse (1991). much of the attention in this area has focused on static measurements ofcode. less attention has been paid to dynamic measurements of software (e.g., measuring the connectivity ofsoftware modules under operating conditions) and aspects of the software production process such as softwarereuse, especially in systems employing objectoriented languages.the most widely used code metric, the ncsl (noncommentary source line), is often used as a surrogate forfunctionality. surprisingly, since software is now nearly 50 years old, standards for counting ncsls remainelusive in practice. for example, should a single, twoline statement in c language count as one ncsl or two?counts of tokens (operators or operands), delimiters, and branching statements are used as other staticmetrics. although some of these are clearly measures of software size, others purport to measure more subtlenotions of software complexity and structure. it has been observed that all such metrics are highly correlated withsize. at the panel's informationgathering forum, munson (1993) concluded that current software metrics captureapproximately three "independent" features of a software module: program control, program size, and datastructure. a statistical (principalcomponents) analysis of 13 metrics on hal programs in the space shuttleprogram was the key to this finding. while one might argue that performing a common statistical decompositionof multivariate data is hardly novel, it most certainly is in software engineering. the important implication of thatfinding is that there are features of software that are not being captured by the existing battery of software metrics(e.g., cohesion and coupling)šand if these are key differentiators of potentially high and lowfault programs,there is no way that an analysis of the available metrics will highlight this condition. on the other side of theledger, the statistical costs of including "noisy" versions of the same (latent) variable in models and analysiscritique of some current applications of statistics in software engineering41statistical software engineeringcopyright national academy of sciences. all rights reserved.methods that are based on these metrics, such as cost estimation, seem not to have been appreciated. subsetselection methods (e.g., mallows, 1973) provide one way to assess variable redundancy and the effect on fittedmodels, but other approaches that use judgment composites, or composites based on other bodies of data (tukey,1991), will often be more effective than discarding metrics.metrics typically involve processes or products, are subjective or objective, and involve different types ofmeasurement scales, for example, nominal, ordinal, interval, or ratio. an objective metric is a measurement takenon a product or process, usually on an interval or ratio scale. some examples include the number of lines of code,development time, number of software faults, or number of changes. a subjective metric may involve aclassification or qualification based on experience. examples include the quality of use of a method or theexperience of the programmers in the application or process.one standard for software measurement is the basili and weiss (1984) goal/question/ metric paradigm,which has five parameters:1. an object of the studyša process, product, or any other experience model;2. a focusšwhat information is of interest;3. a point of viewšthe perspective of the person needing the information;4. a purposešhow the information will be used; and5. a determination of what measurements will provide the information that is needed.the results are studied relative to a particular environment.critique of some current applications of statistics in software engineering42statistical software engineeringcopyright national academy of sciences. all rights reserved.5statistical challengesin comparison with other engineering disciplines, software engineering is still in the definition stage.characteristics of established disciplines include having defined, timetested, credible methodologies fordisciplinary practice, assessment, and predictability. software engineering combines application domainknowledge, computer science, statistics, behavioral science, and human factors issues. statistical research andeducation challenges in software engineering involve the following: generalizing particular experimental results to other settings and projects, scaling up results obtained in academic studies to industrial settings, combining information across software engineering projects and studies, adopting exploratory data analysis and visualization techniques, educating the software engineering community as to statistical approaches and data issues, developing analysis methods to cope with qualitative variables, providing models with the appropriate error distributions for software engineering applications, and improving accelerated life testing.the following sections elaborate on certain of these challenges.software engineering experimental issuessoftware engineering is an evolutionary and experimental discipline. as argued forcefully by basili (1993), itis a laboratory or experimental science. the term "experimental science" has different meanings for engineers andstatisticians. for engineers, software is experimental because systems are built, studied, and evaluated based ontheory. each system investigates new ideas and advances the state of the art. for statisticians, the purpose ofexperiments is to gather statistically valid evidence about the effects of some factor, perhaps involving theprocess, methodology, or code in a system.there are three classes of experiments in software engineering: case studies, academic experiments, and industrial experiments.case studies are perhaps the most common and involve an "experiment" on a single largescale project.academic experiments usually involve a smallscale experiment, often on a program orstatistical challenges43statistical software engineeringcopyright national academy of sciences. all rights reserved.methodology, typically using students as the experimental subjects. industrial experiments fall somewherebetween case studies and academic experiments. because of the expense and difficulty of performing extensivecontrolled experiments on software, case studies are often resorted to. the ideal situation is to be able to takeadvantage of realworld industrial operations while having as much control as is feasible. much of the presentwork in this area is at best anecdotal and would benefit greatly from more rigorous statistical advice and control.the panel foresees an opportunity for innovative work on combining information (see below) from relativelydisparate experiences.conducting statistically valid software experiments is challenging for several reasons: the software production process is often chaotic and uncontrolled (i.e., immature); human variability is a complicating factor; and industrial experiments are very costly and therefore must produce something useful.many variables in the software production process are not well understood and are difficult to control for. forsoftware engineering experiments, the factors of interest include the following: "people" factors: number, level, organization, process experience; problem factors: application domain, constraints, susceptibility to change; process factors: life cycle model, methods, tools, programming language; product factors: deliverables, system size, system reliability, portability; and resource factors: target and development machines, calendar time, budget, existing software, and so on.each of these characteristics must be modeled or controls done for the experiment to be valid.human variability is particularly challenging, given that the difference in quality and productivity betweenthe best and worst programmers may be 20 to 1. for example, in an experiment comparing batch versus interactivecomputing, sackman (1970) observed differences in ability of up to 28 to 1 in programmers performing the sametask. this variation can overwhelm the effects of a change in methodology that may account for a 10% to 15%difference in quality or productivity.the human factor is so strongly integrated with every aspect of the subjective discipline of softwareengineering that it alone is the prime driver of issues to be addressed. the human factor creates issues in theprocess, the product, and the user environment. measurements of the objects (the product and the process) areobscured when qualified by the attributes (ambiguous requirements and productivity issues are key examples).recognizing and characterizing the human attributes within the context of the software process are key tounderstanding how to include them in system and statistical models.the capabilities of individuals strongly influence the metrics collected throughout the software productionprocess. capabilities include experience, intelligence, familiarity with the application domain, ability tocommunicate with others, ability to envision the problem spatially, and ability to verbally describe that spatialunderstanding. although not scientifically founded, anecdotal information supports the incidence of thesecapabilities (curtis, 1988).statistical challenges44statistical software engineeringcopyright national academy of sciences. all rights reserved.for software engineering experiments, the key problems involve small sample sizes, high variability, manyuncontrolled factors, and extreme difficulty in collecting experimental data. traditional statistical experimentaldesigns, originally developed for agricultural experiments, are not well suited for software engineering. at thepanel's forum, zweben (1993) discussed an interesting example of an experiment from objectorientedprogramming, involving a fairly complex design and analysis. objectoriented programming is an approach that issweeping the software industry, but for which much of the supporting evidence is anecdotal.example. the purpose of the software design and analysis experiment was to gather statistically validevidence about the effectšon effort and qualityšof using the principles of abstraction, encapsulation, andlayering to enhance components of software systems. the experiment was divided into two types of tasks:1. enhancing an existing component to provide additional functionality, and2. modifying a component to provide different functionality.the experimental subjects were students in graduate classes on software component design and development.the two approaches for this maintenance problem are "white box," which involves modifying the old code to getthe new functionality, and "black box," which involves layering on the new functionality. the experiments weredesigned to detect, for each task, differences between the two approaches in the time required to make themodification and in the number of associated faults uncovered. three experiments were conducted. experiment ainvolved an unbounded queue component. the subjects were given a basic ada package implementing enque,deque, and is empty, and the task was to implement the operators add, copy, clear, append, and reverse. thesubject was instructed to keep track of the time spent in designing, coding, testing, and debugging each operator,and also the associated number of bugs uncovered in each task. the tasks were completed in two ways: by directlyimplementing new operations using the representation of the queue, and by layering on the new operators ascapabilities. experiment b involved a partial map component, and experiment c involved an almost constant mapcomponent. given that in experiments involving students, the results may be invalidated by problems with dataintegrity, for this experiment the student participants were told that the results of the experiment would have noeffect on course grades. the code was validated by an instructor to ensure that there were no lingering defects. theexperimental plan was conducted using a crossover design. each subject implemented the enhancements twice,using both the white box and the black box methods. this particular experimental design could test for thetreatment (layering or not) effect and treatment by sequence interaction. the subject differences were nestedwithin the sequences, and the sequences were counterbalanced based on experience level. the carryover effect ofthe first treatment influences the choice regarding the correct way of testing for treatment effects.the statistical model used to represent the behavior in the number of bugs was sophisticated as well, anoverdispersed log linear model. the use of this model allowed for an analysis of nonnormal response data whilealso preventing invalid inferences that would have occurred hadstatistical challenges45statistical software engineeringcopyright national academy of sciences. all rights reserved.overdispersion not been taken into account. indeed, only experiment b displayed a significant treatment effectafter adjustment for overdispersion.combining informationthe results of many diverse software projects and studies tend to lead to more confusion than insight. thesoftware engineering community would benefit if more value were gained from the work that is being done. to theextent that projects and studies focus on the same end point, statistics can help to fuse the independent results into aconsistent and analytically justifiable story.the statistical methodology that addresses the topic of how to fuse such independent results is relatively newand is termed ''combining information"; a related set of tools is provided by metaanalysis. an excellent overviewof this methodology was produced by a cats panel and documented in an nrc report (nrc, 1992) that is nowavailable as an american statistical association publication (asa, 1993). the report documents variousapproaches to the problem of how to combine information and describes numerous specific applications. one ofthe recommendations made in it (p. 182) is crucial to achieving advances in software engineering:the panel urges that authors and journal editors attempt to raise the level of quantitative explicitness in the reportingof research findings, by publishing summaries of appropriate quantitative measures on which the researchconclusions are based (e.g., at a minimum: sample sizes, means, and standard deviations for all variables, andrelevant correlation matrices).it is not sensible to merely combine pvalues from independent studies. it is clearly better to take weightedaverages of effects when the weights account for differences in size and sensitivity across the studies to becombined.example. kitchenham (1991) discusses an issue in cost estimation that involves looking across 10 differentsources consisting of 17 different software projects. the issue is whether the exponent ß in the basic costestimation model, effort size ß , is significantly different from 1. the usual interpretation of ßis the "overheadintroduced by product size," so that a value greater than 1 implies that relatively more effort is required to producelarge software systems than to produce smaller ones. many cite such "diseconomies of scale" in softwareproduction as evidence in support of their models and tools.the 17 software projects are listed in table 4. fortunately, the cited sources contain both point estimates (b)of the exponent and its estimated standard error. these summary statistics can be used to estimate a commonexponent and ultimately test the hypothesis that it is different from 1.statistical challenges46statistical software engineeringcopyright national academy of sciences. all rights reserved.table 4. reported and derived data on 17 projects concerned with cost estimation.studybse (b)var (b)wbaibas0.9510.0680.00462421.240belleh1.0620.1010.01020018.990your0.7160.2300.05290010.490wing1.0590.2940.0864407.758kemr0.8560.1770.03133013.550boehm.org0.8330.1840.03386013.100boehm.semi0.9760.1330.01769016.630boehm.emb1.0700.1040.01082018.770kittay.icl0.4720.3230.1043006.813kittay.btsx1.2020.3000.0900007.550kittay.btsw0.4950.1850.03422013.040ds1.11.0490.1250.01563017.220ds1.21.0780.1050.01102018.700ds1.31.0860.2890.0835207.938ds2.new0.1780.1340.01796016.550ds2.ext1.0250.1580.02496014.830ds31.1410.0770.00592920.670source: reprinted, with permission, from kitchenham (1992). (c)1992 by national computing centre, ltd.following the nrc recommendations on combining information across studies (nrc, 1992), the appropriatemodel (the socalled random effects model in metaanalysis) allows for a systematic difference between projects(e.g., bias in data reporting, management style, and so on) that averages to zero. under this model, the overallexponent is estimated as a weighted average of the individual exponents where the weights have the form wi = var(bi ) + 2 and the common betweenproject component of variance is estimated bywhere . the statistic q is itself a test of the homogeneity of projects and under a normalityassumption is distributed as x2k1 . for these data one obtains q = 55.19, which strongly indicates heterogeneityacross projects. although the random effects model anticipates such heterogeneity, other approaches that modelthe differences between projects (e.g.,statistical challenges47statistical software engineeringcopyright national academy of sciences. all rights reserved.regression models) may be more informative. since no explanatory variables are available, this discussionproceeds using the simpler model.the estimated betweenproject component of variance is t2 = 0.0425, which is surprisingly large and isperhaps highly influenced by two projects with b's less than 0.5. combining this estimate with the individualwithinproject variances leads to the weights given in the final column of table 4. thus the overall estimatedexponent is with estimated standard error . combining these two estimatesleads readily to a 95% confidence interval for ß of (0.78, 1.04). thus the data in these studies do not support thediseconomiesofscale argument.even better than published summaries would be a central repository of the data arising from a study. thisinformation would allow assessment of various determinations of similarities between studies, as well as potentialbiases. the panel is aware of several initiatives to build such data repositories. the proposed national softwarecouncil has as one of its primary responsibilities the construction and maintenance of a national softwaremeasurements database. at the panel's forum, a specialized database on software projects in the aeronauticsindustry was also discussed (keller, 1993).an issue related to combining information from diverse sources concerns the translation to industry of smallexperimental studies and/or published case studies done in an academic environment. serious doubts exist inindustry as to the upward scalability of most of these studies because populations, project sizes, and environmentsare all different. expectations differ regarding quality, and it is unclear whether variables measured in a smallstudy are the variables in which industry has an interest. the statistical community should develop stochasticmodels to propagate uncertainty (including variability assessment) on different control factors so that adjustmentsand predictions applicable to industrylevel environments can be made.visualization in software engineeringscientific visualization is an emerging technology that is driven by everdecreasing hardware prices and theassociated increasing sophistication of visualization software. visualization involves the interactive pictorialdisplay of data using graphics, animation, and sound. much of the recent progress in visualization has come fromthe application of computer graphics to threedimensional image analysis and rendering. data visualization, asubset of scientific visualization, focuses on the display and analysis of abstract data. some of the earliest andbestknown examples of data visualization involve statistical data displays.the motivation for applying visualization to software engineering is to understand the complexity,multidimensionality, and structure embodied in software systems. much of the original research in softwarevisualizationšthe use of typography, graphic design, animation, and cinematography to facilitate theunderstanding and enhancement of software systemswas performed by computer scientists interested inunderstanding algorithms, particularly in thestatistical challenges48statistical software engineeringcopyright national academy of sciences. all rights reserved.context of education. applying the quantitative focus of statistical graphics methods to currently popular scientificvisualization techniques is a fertile area for research.visualizing software engineering data is challenging because of the diversity of data sets associated withsoftware projects. for data sets involving software faults, times to failure, cost and effort predictions, and so on,there is a clear statistical relationship of interest. software fault density may be related to code complexity and toother software metrics. traditional techniques for visualizing statistical data are designed to extract quantitativerelationships between variables. other software engineering data sets such as the execution trace of a program (thesequence of statements executed during a test run) or the change history of a file are not easily visualized usingconventional data visualization techniques. the need for relevant techniques has led to the development ofspecialized domainspecific visualization capabilities peculiar to software systems. applications include thefollowing: configuration management data (eick et al., 1992b), function call graphs (ganser et al., 1993), code coverage, code metrics, algorithm animation (brown and hershberger, 1992; stasko, 1993), sophisticated typesetting of computer programs (baecker and marcus, 1988), software development process, software metrics (ebert, 1992), and software reliability models and data.some of these applications are discussed below.configuration management dataa rich software database suitable for visualization involves the code itself. in production systems, the sourcecode is stored in configuration management databases. these databases contain a complete history of the code withevery source code change recorded as a modification request. along with the affected lines, the source codedatabase usually contains other information such as the identity of the programmer making the changes, date thechanges were submitted, reason for the change, and whether the change was meant to add functionality or fix abug. the variables associated with source code may be continuous, categorical, or binary. for a line in a computerprogram, when it was written is (essentially) continuous, who wrote it is categorical, and whether or not the linewas executed during a regression test is binary.example. figure 1 (see "implementation" in chapter 3) shows production code written in c language from amodule in at&t's 5ess switch (eick, 1994). in the display, row color is tied to the code's age: the most recentlyadded lines are in red and the oldest in blue, with a color spectrum in between. dynamic graphics techniques areemployed for increasing the effectiveness of the display. there are five interactive views of data in figure 1:statistical challenges49statistical software engineeringcopyright national academy of sciences. all rights reserved.1. the rows corresponding to the text lines,2. the values on the color scale,3. the file names above the columns,4. the browser windows, and5. the bar chart beneath the color scale.each of the views is linked, united through the use of color, and activated by using a mouse pointer. thismode of manipulating the display, called brushing by becker and cleveland (1987) and by becker et al. (1987), isparticularly effective for exploring software development data.function call graphsperhaps the most common visualization of software is a function call graph as shown in figure 5. functioncall graphs are a widely used, visual, treelike display of the function calls in a piece of code. they show callingrelationships between modules in a system and are one representation of software structure. a problem withfunction call graphs is that they become overloaded with too much information for all but the smallest systems.one approach to improving the usefulness of function call graphs might involve the use of dynamic graphicstechniques to focus the display on the visually informative regions.test code coverageanother interesting example of source code visualization involves showing test suite code coverage. figure 6shows the statement coverage and execution "hot spots" for a program that has been run through its regressiontest. the row indentation and line length have been turned off so that each line receives the same amount of visualspace. the most frequently executed lines are shown in red and the least frequently in blue, with a color spectrumin between. there are two special colors: the black lines correspond to nonexecutable lines of c code such ascomments, variable declarations, and functions, and the gray lines correspond to the executable lines of code thatwere not executed. these are the lines that the regression test missed.code metricsas discussed in chapter 4 (in the section "software measurement and metrics"), static code metrics attemptto quantify and measure the complexity of code. these metrics are used to identify portions of programs that areparticularly difficult and are likely to be subject to defects. one visualization method for displaying codecomplexity metrics uses a spacefilling representation (baker and eick, 1995). taking advantage of thehierarchical structure of code, each subsystem, module, and file is tiled on the display, which shows them asnested, spacefilling rectangles with area, color, and fill encoding software metrics. this technique can displaystatistical challenges50statistical software engineeringcopyright national academy of sciences. all rights reserved.the relative sizes of a system's components, the relative stability of the components, the location of newfunctionality, the location of errorprone code with many fixes to identified faults, and, using animation, thehistorical evolution of the code.example. figure 7 displays the at&t 5ess switching code using the seesys( system, a dynamic graphicsmetrics visualization system. interactive controls enable the user to manipulate the display, reset the colors, andzoom in on particular modules and files, providing an interactive software data analysis environment. the spacefilling representation: shows modules, files, and subsystems in context; provides an overview of a complete software system; and applies statistical dynamic graphics techniques to the problem of visualizing metrics.a major difference in the use of graphics in scientific visualization and statistics is that for the former, graphsare the end, whereas for the latter, they are more often the means to an end. thus visualizations of software arecrucial to statistical software engineering to the extent that they facilitate description and modeling of softwareengineering data. discussed below are some possibilities related to the examples described in this chapter.the rainbow files in figure 1 suggest that certain code is changed frequently. frequently changed code isoften errorprone, difficult to maintain, and problematic. software engineers often claim that code, or people'sunderstanding of it, decays with age. eventually the code becomes unmaintainable and must be rewritten (reengineered). statistical models are needed to characterize the normal rate of change and therefore determinewhether the current files are unusual. such models need to take account of the number of changes, locations offaults, type of functionality, past development patterns, and future trends. for example, a common software designinvolves having a simple main routine that calls on several other procedures to invoke needed functionality. themain routine may be changed frequently as programmers modify small snippets of code to access large chunks ofnew code that is put into other files. for this code, many simple, small changes are normal and do not indicatemaintenance problems. if models existed, then it would be possible to make quantitative comparisons betweenfiles rather than the qualitative comparisons that are currently made.figure 5 suggests some natural covariates and models for improving the efficiency of software testing.current compiler technology can easily analyze code to obtain the functions, lines, and even the paths executed bycode in test suites. for certain classes of programming errors such as typographical errors, the incremental codecoverage is an ideal covariate for estimating the probability of detecting an error. the execution frequency ofblocks of code or functions is clearly related to the probability of error detection. figure 5 shows clearly that smallportions of the program are heavily exercised but that most of the code is not touched. in an indirect wayoperational profile testing attempts to capture this idea by testing the features, and therefore the code, in relation tohow often they will be used. this notion suggests that statistical techniques involving covariates can improve theefficiency of software testing.figure 7 suggests novel ways of displaying software metrics. the current practice is to identify overlycomplex files for special care and management attention. the procedures forstatistical challenges51statistical software engineeringcopyright national academy of sciences. all rights reserved.identifying complex code are often based on very clever and sophisticated arguments, but not on data. a statisticalapproach might attempt to correlate the complexity of code with the locations of past faults and investigate theirpredictive power. statistical models that can relate complexity metrics to actual faults will increase the models'practical efficiency for reallife systems. these models should not be developed in the absence of data about thecode. simple ways of presenting such data, such as an ordered list of fault density, file by file, can be veryeffective in guiding the selection of an appropriate model. in other cases, microanalysis, often driven by graphicalbrowsers, might suggest a richer class of models that the data could support. for example, software fault rates areoften quoted in terms of the number of faults per 1,000 lines of ncsl. the lines in figure 1 can be colorcoded toshow the historical locations of past faults. in other representations (not shown), clear spatial patterns with faultsare concentrated in particular files and in particular regions of the files, suggesting that spatial models of faultdensity might work very well in helping to identify faultprone code.challenges for visualizationthe research opportunities and challenges in visualizing software data are similar to those for visualizingother large abstract databases:1. software data are abstract; there is no natural twodimensional or threedimensional representation ofthe data. a research challenge is to discover meaningful representations of the data that enable ananalyst to understand the data in context.2. much software data are nontraditional statistical data such as the change history of source code,duplication in manuals, or the structure of a relational database. new metaphors must be discoveredfor harmonious transfer information.3. the database associated with large software systems may be huge, potentially containing millions ofobservations. effective statistical graphics techniques must be able to cope with the volume of datafound in modern software systems.4. the lack of easytouse software tools makes the development of highquality custom visualizationsparticularly difficult. currently, visualizations must be handcoded in lowlevel languages such as cor c++. this is a timeconsuming task that can be carried out only by the most sophisticatedprogrammers.opportunities for visualizationvisualizations associated with software involve the code itself, data associated with the system, the executionof the program, and the process for creating the system. opportunities include the following:1. objects/patterns. objectoriented programming is rapidly becoming standard for development of newsystems and is being retrofitted into existing systems. effectivestatistical challenges52statistical software engineeringcopyright national academy of sciences. all rights reserved.figure 5. function call graphs showing the calling pattern between procedures. the toppanel shows an interpretable, easytocomprehend display, whereas the bottom panel isoverly busy and visually confusing.statistical challenges53statistical software engineeringcopyright national academy of sciences. all rights reserved.statistical challenges54statistical software engineeringcopyright national academy of sciences. all rights reserved.figure 6. a seesofttm display showing code coverage for a program executing itsregression test. the color of each line is determined by the number of times that itexecuted. the colors range from red (the "hot spots") to deep blue (for code executedonly once) using a redgreenblue color spectrum. there are two special colors: the blacklines are nonexecutable lines of code such as variable declarations and comments, andthe gray lines are the nonexecuted (not covered) lines. the figure shows that generatingregression tests with high coverage is quite difficult. source: eick(1994).statistical challenges55statistical software engineeringcopyright national academy of sciences. all rights reserved.statistical challenges56statistical software engineeringcopyright national academy of sciences. all rights reserved.figure 7. a display of software metrics for a millionline system. the rectangle formingthe outermost boundary represents the entire system. the rectangles contained within theboundary represent the size (in ncsls) of individual subsystems (each labeled with asingle character az, at), and modules within the subsystems. color is used here toredundantly encode size according to the color scheme in the slider at the bottom of thescreen.statistical challenges57statistical software engineeringcopyright national academy of sciences. all rights reserved.statistical challenges58statistical software engineeringcopyright national academy of sciences. all rights reserved.displays need to be developed for understanding the inheritance (or dependency) structure, semantic relationshipsamong objects, and the runtime life cycle of objects.2. performance. software systems inevitably run too slowly, making runtime performance animportant consideration. host systems often collect large volumes of finegrain (that is, lowlevel)performance data including function calling patterns, line execution counts, operating system pagefaults, heap usage, and stack space, as well as disk usage. novel techniques to understand and digestdynamic program execution data would be immediately useful.3. parallelism. recently, massively parallel computers with tens to thousands of cooperating processorshave started to become widely available. programming these computers involves developing newdistributed algorithms that divide important computations among the processors. most often anessential aspect of the computation involves communicating interim results between processors andsynchronizing the computations. visualization techniques are a crucial tool for enabling programmersto model and debug subtle computations.4. threedimensional. workstations capable of rendering realistic threedimensional displays arerapidly becoming widely available at reasonable prices. new visualization techniques leveragingthreedimensional capabilities should be developed to enable software engineers to cope with theeverincreasing complexity of modern software systems.orthogonal defect classificationthe primary focus of software engineering is to monitor a software development process with a view towardimproving quality and productivity. for improving quality, there have been two distinct approaches. the firstconsiders each defect as unique and tries to identify a cause. the second considers a defect as a sample from anensemble to which a formal statistical reliability model is fitted. chillarege et al. (1992) proposed a newmethodology that strikes a balance between these two ends of spectrum. this method, called orthogonal defectclassification, is based on exploratory data analysis techniques and has been found to be quite useful at ibm. itrecognizes that the key to improving a process is to quantify various causeandeffect relationships involvingdefects.the basic approach is as follows. first, classify defects into various types. then, obtain a distribution of thetypes across different development phases. finally, having created these reference distributions and therelationships among them, compare them with the distributions observed in a new product or release. if there arediscrepancies, take corrective action.operationally, the defects are classified according to eight ''orthogonal" (mutually exclusive) defect types:functional, assignment, interface, checking, timing, build/package/merge, data structures and algorithms, anddocumentation. further, development phases are divided into four basic stages (where defects can be observed):design, unit test, function test, and system test. for each stage and each defect type, a range of acceptable baselinedefect rates is defined by experience. this information is used to improve the quality of a new product or release.towardstatistical challenges59statistical software engineeringcopyright national academy of sciences. all rights reserved.this end, for a given defect type, defect distributions across development stages are compared with the baselinerates. for each chain of resultsšsay, too high early on, lower later, and high at the endšan implication isderived. for example, the implication may be that function testing should be revamped.this methodology has been extended to a study of the distribution of triggers, that is, the conditions that allow adefect to surface. first, it is implicit in this approach that there is no substitute for a good data analysis. second,assumptions clearly are being made about the stationarity of reference distributions, an approach that may beappropriate for a stable environment with similar projects. thus, it may be necessary to create classes of referencedistributions and classes of similar projects. perhaps some clustering techniques may be valuable in this context.third, although the defect types are mutually exclusive, it is possible that a fault may result in many defects, andvice versa. this multiplespawning may cause serious implementation difficulties. proper measurement protocolsmay diminish such multipropagation. finally, given goodquality data, it may be possible to extend orthogonaldefect classification to efforts to identify risks in the production of software, perhaps using data to provide earlyindicators of product quality and potential problems concerning scheduling. the potential of this line of inquiryshould be carefully investigated, since it could open up an exciting new area in software engineering.statistical challenges60statistical software engineeringcopyright national academy of sciences. all rights reserved.6summary and conclusionsin the 1950s, as the production line was becoming the standard for hardware manufacturing, deming showedthat statistical process control techniques, invented originally by shewhart, were essential to controlling andimproving the production process. deming's crusade has had a lasting impact in japan and has changed itsworldwide competitive position. it has also had a global impact on the use of statistical methods, the training ofstatisticians, and so forth.in the 1990s the emphasis is on software, as complex hardwarebased functionality is being replaced by moreflexible, softwarebased functionality. small programs created by a few programmers are being superseded bymassive software systems containing millions of lines of code created by many programmers with differentbackgrounds, training, and skills. this is the world of socalled software factories. these factories at present donot fit the traditional model of (hardware) factories and more closely resemble the development effort that goesinto designing new products. however, with the spread of software reuse, the increasing availability of tools forautomatically capturing requirements, generating code and test cases, and providing user documentation, and thegrowing reliance on standardized tuning and installation processes and standardized procedures for analysis, themodel is moving closer to that of a traditional factory. the economy of scale that is achievable by consideringsoftware development as a manufacturing process, a factory, rather than a handcrafting process, is essential forpreserving u.s. competitive leadership. the challenge is to build these huge systems in a costeffective manner.the panel expects this challenge to concern the field of software engineering for the rest of the decade. hence, anyset of methodologies that can help in meeting this challenge will be invaluable. more importantly, the use ofsuch methodologies will likely determine the competitive positions of organizations and nations involved in software production.with the amount of variability involved in the software production process and its many subprocesses, aswell as the diversity of developers, users, and uses, it is unlikely that a deterministic control system will helpimprove the software production process. as in statistical physics, only a technology based on statisticalmodeling, something akin to statistical control, will work. the panel believes that the juncture at hand is not verydifferent from the one reached by deming in the 1950s when he began to popularize the concept of statisticalprocess control. what is needed now is a detailed understanding by statisticians of the software engineeringprocess, as well as an appreciation by software engineers of what statisticians can and cannot do. ifcollaborative interactions and the building of this mutual understanding can be cultivated, then there likely willoccur a major impact of the same order of magnitude as deming's introduction of statistical process controltechniques in hardware manufacturing.of course, this is not to say that all software problems are going to be solved by statistical means, just as notall automobile manufacturing problems can be solved by statistical means. on the contrary, the software industryhas been technology driven, and the bulk of future gains in productivity will come from new, creative ideas. forexample, much of the gain in productivitysummary and conclusions61statistical software engineeringcopyright national academy of sciences. all rights reserved.between 1950 and 1970 occurred because of the replacement of assembler coding by highlevel languages.nevertheless, as the panel attempts to point out in this report, increased collaboration between softwareengineers and statisticians holds much promise for resolving problems in software development. some of thecatalysts that are essential for this interaction to be productive, as well as some of the related researchopportunities for software engineers and statisticians, are discussed below.institutional model for researchthe panel strongly believes that the right model for statistical research in software development iscollaborative in nature. it is essential to avoid solving the "wrong" problems. it is equally important that theproblems identified in this report not be "solved" by statisticians in isolation. statisticians need to attain a degreeof credibility in software engineering, and such credibility will not be achieved by developing n new reliabilitymodels with highpower asymptotics. the ideal collaboration partners statisticians and software engineers inwork aimed at improving a real software process or product.this conclusion assumes not only that statisticians and software engineers have a mutual desire to worktogether to solve software engineering problems, but also that funding and reward mechanisms are in place tostimulate the technical collaboration. up to now, such incentives have not been the norm in academic institutions,given that, for example, coauthored papers have been generally discounted by promotion evaluation committees.moreover, at funding agencies, proposals for collaborative work have tended to "fall through the cracks" becauseof a lack of interdisciplinary expertise to evaluate their merits. the panel expects such barriers to be reduced in thecoming years, but in the interim, industry can play a leadership role in nurturing collaborations betweensoftware engineers and statisticians and can reduce its own set of barriers (for instance, those related toproprietary and intellectual property interests).model for data collection and analysisas discussed above in this report, for statistical approaches to be useful, it is essential that highquality databe available. quality includes measuring the right things at the right timešspecifically, adopted software metricsmust be relevant for each of the important stages of the development life cycle, and the protocol of metrics forcollecting data must be well defined and well executed. without careful preparation that takes account of allof these data issues, it is unlikely that statistical methods will have any impact on a given software projectunder study. for this reason, it is crucial to have the software industry take a lead position in research on statistical software engineering.figure 8, a model for the interaction between researchers and the software development process, displays ahighlevel spiral view of the software development process offered by dalalsummary and conclusions62statistical software engineeringcopyright national academy of sciences. all rights reserved.et al. (1994). figure 9 gives a more detailed view of the statistical software engineering module (ssem) at thecenter of figure 8.figure 8. spiral software development process model. ssem, statistical software engineering module.figure 9. statistical software engineering module at stage n.the ssem has several components. one of its major functions is to act as the central repository for allrelevant project data (statistical or nonstatistical). thus this module serves as a resource for the entire project,interfacing with every stage, typically at its review or conclusion. for example, the ssem would be used at therequirement review stage, when data on inspection, faults, times, effort, and coverage are available. for testing,information would be gathered at the end of each stage of testing (unit, integration, system, alpha, beta, . . .) aboutthe number of open faults, closed faults, types of problems, severity, changes, and effort. such data would comefrom test case management systems, change management systems, and configuration management systems.summary and conclusions63statistical software engineeringcopyright national academy of sciences. all rights reserved.additional elements of the ssem include collection protocols, metrics, exploratory data analysis (eda),modeling, confirmatory analysis, and conclusions. a critical part of the ssem would be related to rootcauseanalysis. analysis could be as simple as ishikawa's fish bone diagram (ishikawa, 1976), or more complex, such asorthogonal defect classification (described in chapter 5). this capability accords with the belief that a carefulanalysis of root cause is essential to improving the software development process. central placement of the ssemensures that the results of various analyses will be communicated at all relevant stages. for example, at the codereview stage, the ssem can suggest ways of improving the requirement process as well as point out potentiallyerrorprone parts of the software for testing.issues in educationenormous opportunities and many potential benefits are possible if the software engineeringcommunity learns about relevant statistical methods and if statisticians contribute to and cooperate in theeducation of future software engineers. the areas outlined below are those that are relevant today. as thecommunity matures in its statistical sophistication, the areas themselves should evolve to reflect the maturationprocess. designed experiments. software engineering is inherently experimental, yet relatively few designedexperiments have been conducted. software engineering education programs must stress the desirability,wherever feasible, of validating new techniques through the use of statistically valid, designedexperiments. part of the reason for the lack of experimentation in software engineering may involve thelarge variability in human programming capabilities. as pointed out in chapter 5, the most talentedprogrammer may be 20 times more productive than the least talented. this disparity makes it difficult toconduct experiments because the betweensubject variability tends to overwhelm the treatment effects.experimental designs that address broad variability in subjects should be emphasized in the softwareengineering curriculum. a similar emphasis should be given to random and fixedeffects models withhierarchical structure and to distinguishing within and betweenexperiment variability.there is also a role for the statistics profession in the development of guidelines for experiments insoftware engineering akin to those mandated by the food and drug administration for clinical trials.these guidelines will require reformulation in the software engineering context with the possibleinvolvement of various industry and academic forums, including the institute of electrical andelectronics engineers, the american statistical association, and the software engineering institute. exploratory data analysis. it is important to appreciate the strengths and the limitations of available databy challenging the data with a battery of numerical, tabular, and graphical methods. exploratory dataanalysis methods (e.g., tukey, 1977; mosteller and tukey, 1977) are essentially "model free," so thatinvestigators can be surprised bysummary and conclusions64statistical software engineeringcopyright national academy of sciences. all rights reserved.unexpected behavior rather than have their thinking constrained by what is expected. one of the attitudestoward statistical analysis that is important to convey is that ofdata = fit + residual.the iterative nature of improving the model fit by removing structure from the residuals must bestressed in discussions of statistical modeling. modeling. the models used by statisticians differ dramatically from those used by nonstatisticians. thedifferences stem from advances in the statistical community in the past decade that effectively relaxassumptions of linearity for nearly all classical techniques. this relaxation is obtained by assuming onlylocal linearity and using smoothing techniques (e.g., splines) to regularize the solutions (hastie andtibshirani, 1990). the result is quite flexible but interpretable models that are relatively unknown outsidethe statistics community. arguably these more recent methods lack the wellstudied inferential propertiesof classical techniques, but that drawback is expected to be remedied in coming years. educationalinformation exchanges should be conducted to stimulate more frequent and wider use of suchcomparatively recent techniques. risk analysis. software systems are often used in conjunction with other software and hardware systems.for example, in telecommunications, an originating call is connected by switching software; however, theactual connection is made by physical cables, transmission cells, and other components. the megasystems thus created run our nation's telephone systems, stock markets, and nuclear power plants.failures can be very expensive, if not catastrophic. thus, it is essential to have software and hardwaresystems built in such a way that they can tolerate faults and provide minimal functionality, whileprecluding a catastrophic failure. this type of system robustness is related to socalled faulttolerantdesign of software (leveson, 1986).risk analysis has played a key role in identifying faultprone components of hardware systems and hashelped in managing the risks associated with very complex hardwaresoftware systems. a paradigmsuggested by dalal et al. (1989) for risk management for the space shuttle program and correspondingstatistical methods are important in this context. for software systems, risk analysis typically begins withidentifying programming styles, characteristics of the modules responsible for most software faults, andso on. statistical analysis of rootcause data leads to a risk profile for a system and can be useful in riskreduction. risk management also involves consideration of the probability of occurrence of variousfailure scenarios. such probabilities are obtained either by using the delphi method (e.g., dalkey, 1972;pill, 1971) or by analyzing historical data. one of the key requirements in failurescenario analysis is todynamically update information about the scenarios as new data on system behavior become available,such as a changing user profile.summary and conclusions65statistical software engineeringcopyright national academy of sciences. all rights reserved. attitude toward assumptions. as software engineers are aware, a major difference between statisticsand mathematics is that for the latter, it matters only that assumptions be correctly stated, whereas for theformer, it is essential that the prevailing assumptions be supported by the data. this distinction isimportant, but unfortunately it is often taken too literally by many who use statistical techniques. tukeyhas long argued that what is important is not so much that assumptions are violated but rather that theireffect on conclusions is well understood. thus for a linear model, where the standard assumptions includenormality, homoscedasticity, and independence, their importance to statements of inference is exactly inthe opposite order. statistics textbooks, courses, and consulting activities should convey the statistician'slevel of understanding of and perspective on the importance of assumptions for statistical inferencemethods. visualization. the importance of plotting data in all aspects of statistical work cannot beoveremphasized. graphics is important in exploratory stages to ascertain how complex a model the datacan support; in the analysis stage for display of residuals to examine what a currently entertained modelhas failed to account for; and in the presentation stage where graphics can provide succinct andconvincing summaries of the statistical analysis and associated uncertainty. visualization can also helpsoftware engineers cope with, and understand, the huge quantities of data collected in the softwaredevelopment process. tools. software engineers tend to think of statisticians as people who know how to run a regressionsoftware package. although statisticians prefer to think of themselves more as problem solvers, it is stillimportant that they point out good statistical computing toolsfor instance, s, sas, glim, rs1, and soonto software engineers. a cats report (nrc, 1991) attempts to provide an overview of statisticalcomputing languages, systems, and packages, but for such material to be useful to software engineers, amore focused overview will be required.summary and conclusions66statistical software engineeringcopyright national academy of sciences. all rights reserved.referencesabdelghaly, a.a., p.y. chan, and b. littlewood. 1986. evaluation of competing software reliability predictions. ieee trans. software eng.se12(9):950967.abdelhamid, t. 1991. software project dynamics: an integrated approach . englewood cliffs, n.j.: prenticehall.american heritage dictionary of the english language, the. 1981. boston: houghton mifflin.american statistical association (asa). 1993. combining information: statistical issues and opportunities for research, contemporarystatistics series, no. 1. alexandria, va.: american statistical association.baecker, r.m. and a. marcus. 1988. human factors and typography for more readable programs. reading, mass.: addison wesley.baker, m.j. and s.g. eick. 1995. spacefilling software displays. j. visual languages comput. 6(2). in press.basili, v. 1993. measurement, analysis and modeling, and experimentation in software engineering. unpublished paper presented at forum onstatistical methods in software engineering, october 1112, 1993, national research council, washington, d.c.basili, v. and d. weiss. 1984. a methodology for collecting valid software engineering data. ieee trans. software eng. se10:6.becker, r.a. and w.s. cleveland. 1987. brushing scatterplots. technometrics 29:127142.becker, r.a., w.s. cleveland, and a.r. wilks. 1987. dynamic graphics for data analysis. statistical science 2:355383.beckman, r.j. and m.d. mckay. 1987. monte carlo estimation under different distributions using the same simulation. technometrics29:153160.blum, m., m. luby, and r. rubinfeld. 1989. program result checking against adaptive programs and in cryptographic settings . pp. 107118 indistributed computing and cryptography, j. feigenbaum and m. merritt, eds. dimacs: series in discrete mathematics andtheoretical computer science, vol. 2. providence, r.i.: american mathematical society.blum, m., m. luby, and r. rubinfeld. 1990. selftesting/correcting with applications to numerical problems. stoc 22:7383.boehm, b.w. 1981. software engineering economics. engelwood cliffs, n.j.: prentice hall.brocklehurst, s. and b. littlewood. 1992. new ways to get accurate reliability measures. ieee software 9(4):3442.brown, m.h. and j. hershberger. 1992. color and sound in algorithm animation. ieee computer 25(12):5263.burnham, k.p. and w.s. overton. 1978. estimation of the size of a closed population when capture probabilities vary among animals.biometrika 45:343359.chillarege, r., i. bhandari, j. chaar, m. halliday, d. moebus, b. ray, and m. wong. 1992. orthogonal defect classificationa concept for inprocess measurements. ieee trans. software. eng. se18:943955.cohen, d.m., s.r. dalal, a. kaija, and g. patton. 1994. the automatic efficient test generator (aetg) system. pp. 303309 in proceedings ofthe 5th international symposium on software reliability engineering. los alamitos, calif.: ieee computer society press.references67statistical software engineeringcopyright national academy of sciences. all rights reserved.curtis, w. 1988. the impact of individual differences in programmers. pp. 279294 in working with computers: theory versus outcome, g.c. van der veer et al., eds. san diego, calif.: academic press.dalal, s.r. and c.l. mallows. 1988. when should one stop software testing? j. am. statist. assoc. 83:872879.dalal, s.r. and c.l. mallows. 1990. some graphical aids for deciding when to stop testing software. ieee j. selected areas incommunications 8:169175. (special issue on software quality and productivity.)dalal, s.r. and c.l. mallows. 1992. buying with exact confidence. ann. appl. probab. 2:752765.dalal, s.r. and a.m. mcintosh. 1994. when to stop testing for large software systems with changing code. ieee trans. software eng. se20:318323.dalal, s.r., e.b. fowlkes, and a.b. hoadley. 1989. risk analysis of the space shuttle: prechallenger prediction of failure. j. am. stat. assoc.84:945957.dalal, s.r., j.r. horgan, and j.r. kettenring. 1994. reliable software and communication ii: controlling the software development process.ieee j. selected areas in communications 12:3339.dalkey, n.c. 1972. studies in the quality of lifedelphi and decisionmaking. lexington, mass.: d.c. heath & co.dawid, a.p. 1984. statistical theory: the prequential approach. j. r. stat. soc. london a 147:278292.demillo, r.a., d.s. guindi, k.s. king, w.m. mccracken, and a.j. offutt. 1988. an extended overview of the mothra mutation system.pp. 142151 in proceedings of the second workshop on software testing, verification and analysis. alberta, canada: banff.ebert, c. 1992. visualization techniques for analyzing and evaluating software measures. ieee trans. software eng. 11(18):10291034.eckhardt, d.e. and l.d. lee. 1985. a theoretical basis of multiversion software subject to coincident errors. ieee trans. software eng.se11:15111517.eckhardt, d.e., a.k. caglayan, j.c. knight, l.d. lee, d.f. mcallister, m.a. vouk, and j.p. kelly. 1991. an experimental evaluation ofsoftware redundancy as a strategy for improving reliability. ieee trans. software eng. se17(7):692702.eick, s.g. 1994. graphically displaying text. j. comput. graphical stat. 3(2):127142.eick, s.g., c.r. loader, m.d. long, s.a. vander wiel, and l.g. votta. 1992a. estimating software fault content before coding. pp. 5965 inproceedings of the 14th international conference on software engineering (melbourne, australia). los alamitos, calif.: ieeecomputer society press.eick, s.g., j.l. steffen, and e.e. sumner. 1992b. a tool for visualizing line oriented software. ieee trans. software eng. 11(18):957968.ganser, e.r., e.e. koutsofios, s.c. north, and k.p. vo. 1993. a technique for drawing directed graphs. ieee trans. software eng. se19(3):214230.halstead, m.h. 1977. elements of software science. new york: elsevier.hastie, t.j. and r.j. tibshirani. 1990. generalized additive models . london: chapman & hall.references68statistical software engineeringcopyright national academy of sciences. all rights reserved.henrion, m. and b. fischhoff. 1986. assessing uncertainty in physical constants. am. j. phys. 54(9):791798.horgan, j.r. and s. london. 1992. atac: a data flow testing tool for c. pp. 210 in proceedings of the second symposium on assessment ofquality software development tools (may 2729, 1992, new orleans, la.), e. nahouraii, ed. los alamitos, calif.: ieee computersociety press.humphrey, w.s. 1988. characterizing the software process: a maturity framework. ieee software 5:7379.humphrey, w.s. 1989. managing the software process. reading, mass.: addison wesley.iman, r.l. and w.j. conover. 1982. a distribution free approach to inducing rank correlations among input variables. commun. stat., part b11:311334.institute of electrical and electronics engineers (ieee). 1990. ieee standard glossary of software engineering terminology. ieee std.610.121990. new york: ieee, inc.institute of electrical and electronics engineers (ieee). 1993. ieee standard for software productivity metrics. ieee computer society,ieee std. 10451992, january 11, 1993. new york: ieee, inc.ishikawa, k. 1976. guide to quality control. tokyo, japan: asian productivity organization.kahneman, d., p. slovic, and a. tversky, eds. 1982. judgment under uncertainty: heuristics and biases. new york: cambridge universitypress.keller, t.w. 1993. maintenance process metrics for space shuttle flight software . unpublished paper presented at forum on statisticalmethods in software engineering, october 1112, 1993, national research council, washington, d.c.kitchenham, b. 1991. never mind the metrics; what about the numbers! pp. 2837 in formal aspects of measurement, t. denvir, r. herman,and r.w. whitty, eds. proceedings of the bcsfacs workshop, may 5, 1991, south bank university, london. new york:springerverlag.kitchenham, b. 1992. analyzing software data. metrics club report. manchester, england: national computing centre, ltd.knight, j.c. and n.g. leveson. 1986. experimental evaluation of the assumption of independence in multiversion software. ieee trans. software eng. se12(1):96109.lee, d. and m. yanakakis. 1992. online minimization of transition systems. pp. 264274 in proceedings of the 24th annual acmsymposium on theory of computing. new york: association for computing machinery.leveson, n.g. 1986. software safety: why, what, and how. acm comput. surveys 8:125163.lipton, r. 1989. new directions in testing. pp. 191202 in distributed computing and cryptography, j. feigenbaum and m. merritt, eds.dimacs: series in discrete mathematics and theoretical computer science, vol. 2. providence, r.i.: american mathematicalsociety.littlewood, b. 1979. software reliability model for modular program structure. ieee trans. reliability r28(3):241246.littlewood, b. and d.r. miller. 1989. conceptual modeling of coincident failures in multiversion software. ieee trans. software eng. se15(12):15961614.references69statistical software engineeringcopyright national academy of sciences. all rights reserved.littlewood, b. and l. strigini. 1993. validation of ultrahigh dependability for softwarebased systems. communications of the associationfor computing machinery 36(11):6980.mallows, c.l. 1973. some comments on cp. technometrics 15:661667.mccabe, t.j. 1976. a complexity measure. ieee trans. software eng. se1(3):312327.mckay, m.d., w.j. conover, and r.j. beckman. 1979. a comparison of three methods for selecting values of input variables in the analysis ofoutput from a computer code. technometrics 21:239245.mosteller, f. and j.w. tukey. 1977. data analysis and regression: a second course in statistics. reading, mass.: addison wesley.munson, j.c. 1993. the relationship between software metrics and quality metrics. unpublished paper presented at forum on statisticalmethods in software engineering, october 1112, 1993, national research council, washington, d.c.national research council (nrc). 1991. the future of statistical software. committee on applied and theoretical statistics, board onmathematical sciences. washington, d.c.: national academy press.national research council (nrc). 1992. combining information: statistical issues and opportunities for research. committee on appliedand theoretical statistics, board on mathematical sciences. washington, d.c.: national academy press. (reprinted in 1993 by theamerican statistical association as volume 1 in the asa contemporary statistics series.)nayak, t.k. 1988. estimating population size by recapture sampling. biometrika 75:113120.phadke, m.s. 1993. robust design method for software engineering. unpublished paper presented at forum on statistical methods in softwareengineering, october 1112, 1993, national research council, washington, d.c.pill, j. 1971. the delphi method: substance, context, a critique and an annotated bibliography . socioeconomic planning science 5:5771.randell, b. and p. naur, eds. 1968. software engineering concepts and techniques. nato science committee, proceedings of the natoconferences, october 711, 1968, garmisch, germany. new york: petrocelli/charter.sackman, h. 1970. mancomputer problemsolving: experimental evaluation of timesharing and batch processing. new york: auerbach.siegrist, k. 1988a. reliability of systems with markov transfers of control. ieee trans. software eng. se14(7):10491053.siegrist, k. 1988b. reliability of systems with markov transfers of control, ii. ieee trans. software eng. se14(10):14781480.singpurwalla, n.d. 1991. determining an optimal time interval for testing and debugging software. ieee trans. software eng. 17(4):313319.smith, a.f.m. and g.o. roberts. 1993. bayesian computation via the gibbs sampler and related markov chain monte carlo methods. j. r. stat. soc. london b 55(1):323.stasko, j. 1993. software visualization. unpublished paper presented at forum on statistical methods in software engineering, october1112, 1993 , national research council, washington, d.c.references70statistical software engineeringcopyright national academy of sciences. all rights reserved.stein, m. 1987. large sample properties of simulations using latin hypercube sampling. technometrics 29:143151.tukey, j.w. 1977. exploratory data analysis. reading, mass.: addison wesley.tukey, j.w. 1991. use of many covariates in clinical trials. int. stat. rev. 59(2):123128.vander wiel, s.a. and l.g. votta. 1993. assessing software designs using capturerecapture methods. ieee trans. software eng. se19(11):10451054.zuse, h. 1991. software complexity: measures and methods. berlin: de gruyter.zweben, s. 1993. statistical methods in a study of software reuse principles. unpublished paper presented at forum on statistical methods insoftware engineering, october 1112, 1993, national research council, washington, d.c.references71statistical software engineeringcopyright national academy of sciences. all rights reserved.appendix: forum programmonday, october 11, 19938:00 amwelcome and introductions8:05 amsession on software processsession chair: gloria j. davis (nasaames research center)invited speakers: ted w. keller (ibm corporation)david card (computer sciences corporation)9:45 ambreak10:15 amsession on software metricssession chair: bill curtis (carnegie mellon university)invited speakers: victor r. basili (university of maryland),john c. munson (university of florida)noonbreak1:00 pmsession on software dependability and testingsession chair: richard a. demillo (purdue university)invited speakers: john c. knight (university of virginia),richard lipton (princeton university)2:25 pmbreak3:15 pmsession on case studies session chair: daryl pregibon (at&t bell laboratories, murray hill)invited speakers: tsuneo yamaura (hitachi computer productsamerica, inc.),stuart zweben (ohio state university)5:00 pmadjournappendix: forum program72statistical software engineeringcopyright national academy of sciences. all rights reserved.tuesday, october 12, 19938:00 amsession on nonstandard methodssession chair: siddhartha r. dalal (bellcore)invited speakers: madhav s. phadke (phadke associates)eric e. sumner, jr. (at&t bell laboratoriesnaperville)9:45 ambreak10:15 amsession on software visualizationsession chair: stephen g. eick (at&t bell laboratoriesnaperville)invited speakers: william hill (bellcore)john stasko (georgia institute of technology)noonadjournappendix: forum program73
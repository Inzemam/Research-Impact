detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/6062the unpredictable certainty: white papers632 pages | 8.5 x 11 | paperbackisbn 9780309060363 | doi 10.17226/6062nii 2000 steering committee, commission on physical sciences, mathematics, andapplications, national research councilthe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the unpredictable certaintywhite papersinformation infrastructure through 2000nii 2000 steering committeecomputer science and telecommunications boardcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1997ithe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine.the members of the steering committee responsible for the report were chosen for their special competences and with regard for appropriatebalance.this report has been reviewed by a group other than the authors according to procedures approved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific andengineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority ofthe charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientificand technical matters. dr. bruce alberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallelorganization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the nationalacademy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers.dr. william a. wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members ofappropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibilitygiven to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy's purposes of furthering knowledge and advising the federal government. functioning in accordancewith general policies determined by the academy, the council has become the principal operating agency of both the national academy ofsciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineeringcommunities. the council is administered jointly by both academies and the institute of medicine. dr. bruce alberts and dr. william a.wulf are chairman and vice chairman, respectively, of the national research council.support for this project was provided by the national science foundation under grant no. iri9529473. any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national sciencefoundation.international standard book number 0309060362additional copies of this report are available from the computer science and telecommunications board, 2101 constitution avenue, n.w.,washington, d.c. 20418; cstb@nas.edu or http://www2.nas.edu/cstbweb.copyright 1997 by the national academy of sciences. all rights reserved.printed in the united states of americaiithe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.nii 2000 steering committeelewis m. branscomb, harvard university, chaircynthia h. braddon, the mcgrawhill companiesjames a. chiddix, time warner cabledavid d. clark, massachusetts institute of technologyjoseph a. flaherty, cbs incorporatedpaul e. green, jr., ibm t.j. watson research centerirene greif, lotus development corporationrichard t. liebhaber, mci communications (retired)robert w. lucky, bell communications researchlloyd n. morrisett, john and mary markle foundationdonald w. simborg, knowmed systemsleslie l. vadasz, intel corporationstaffmarjory s. blumenthal, directoriiithe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.computer science and telecommunications boarddavid d. clark, massachusetts institute of technology, chairfrances e. allen, ibm t.j. watson research centerjames chiddix, time warner cablejeff dozier, university of california at santa barbaraa.g. fraser, at&t corporationsusan l. graham, university of california at berkeleyjames gray, microsoft corporationbarbara j. grosz, harvard universitypatrick hanrahan, stanford universityjudith hempel, university of california at san franciscodeborah a. joseph, university of wisconsinbutler w. lampson, microsoft corporationedward d. lazowska, university of washingtonmichael lesk, bell communications researchdavid liddle, interval researchbarbara h. liskov, massachusetts institute of technologyjohn major, qualcomm inc.david g. messerschmitt, university of california at berkeleydonald norman, hewlettpackard companyraymond ozzie, rhythmix corporationdonald simborg, knowmed systems inc.leslie l. vadasz, intel corporationmarjory s. blumenthal, directorherbert s. lin, senior staff officerjerry r. sheehan, program officeralan s. inouye, program officerjon eisenberg, program officerjanet d. briscoe, administrative associatemark balkovich, research associatesynod p. boyd, project assistantlisa l. shum, project assistantivthe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.commission on physical sciences, mathematics, and applicationsrobert j. hermann, united technologies corporation, cochairw. carl lineberger, university of colorado, cochairpeter m. banks, environmental research institute of michigan (erim)william browder, princeton universitylawrence d. brown, university of pennsylvaniaronald g. douglas, texas a&m universityjohn e. estes, university of california at santa barbaramartha p. haynes, cornell universityl. louis hegedus, elf atochem north america, inc.john e. hopcroft, cornell universitycarol m. jantzen, westinghouse savannah river companypaul g. kaminski, technovation, inc.kenneth h. keller, council on foreign relations and the university of minnesotakenneth i. kellermann, national radio astronomy observatorymargaret g. kivelson, university of california at los angelesdaniel kleppner, massachusetts institute of technologyjohn kreick, sanders, a lockheed martin companymarsha i. lester, university of pennsylvanianicholas p. samios, brookhaven national laboratorychanglin tien, university of california at berkeleynorman metzger, executive directorvthe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.vithe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.prefacethis book contains a key component of the nii 2000 project of the computer science andtelecommunications board, a set of white papers that contributed to and complements the project's final report,the unpredictable certainty: information infrastructure through 2000, which was published in the spring of1996. that report was disseminated widely and was well received by its sponsors and a variety of audiences ingovernment, industry, and academia. constraints on staff time and availability delayed the publication of thesewhite papers, which offer details on a number of issues and positions relating to the deployment of informationinfrastructure. the remainder of this preface is taken from the original preface of the unpredictable certainty. itprovides more detail on the context in which the white papers were developed.in october 1994, at the request of the technology policy working group (tpwg) of the informationinfrastructure task force, cstb convened a steering committee to assess mediumterm deployment of facilitiesand services to advance the nation's information infrastructure. the project was designated "nii 2000" by thesteering committee, and its tasks were the following:ł to reach out to a broad range of industries with a stake in the future of u.s. information infrastructurešthose industries expected to be major market drivers as well as those expected to be major service providersšto explore their expectations and motivations for technology deployment in the next 5 to 7 years; to infer from this exploration the extent to which there is a shared vision of the importance of commonfeatures of system architecture, such as interoperability or open system interfaces, and the alternativelikelihood that major parts of the system will develop along proprietary, incompatible lines; and to conclude with suggestions to the u.s. government on public policy choices that might serve both therapid, orderly, and successful development of information infrastructure and its satisfaction of importantpublic interests.to achieve these goals, the steering committee was asked by the tpwg to undertake a specific series ofactivities: convene a workshop of professionals and scholars to discuss and identify key issues related totechnology deployment, call for white papers to gain further information on these issues, organize a forum todiscuss the white papers and other key ideas, and write a synthesis report of its findings.following the workshop, the steering committee released a call for white papers on issues related toarchitecture and facilities, enabling technologies, recovery of costs, middleware technologies and capabilities,applications, equitable access and public service obligations, and research and development. the call wasdistributed through various media (the internet, press advisories, direct mail, and so on) to producers ofcommunications, computer, and software systems goods and services; internet access and other networkbasedservice providers; scholars specializing in relevant technical, economic, and public policy research and analysis;and project liaisons and other representatives of industries and sectors believed likely to become major users ofadvanced information infrastructure (such as the arts, banking and finance, education, health care, governmentagencies, libraries, manufacturing, and transportation). the white papers wereprefaceviithe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.distributed to participants at the spring forum and to interested federal agencies. their content, representing abroad spectrum of views from knowledgeable participants in the evolution of information infrastructure, was amajor component in the development of the steering committee's report, which quotes from and refersspecifically to several of them.prefaceviiithe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.contents the national information infrastructure and the earth sciences: possibilities and challengesmark r. abbott (oregon state university) 1 government services information infrastructure managementrobert j. aiken and john s. cavallini (u.s. department of energy) 10 cutting the gordian knot: providing the american public with advanced universal access in a fully competitive marketplace at the lowest possible costallan j. arlow (telecommunications consultant, annapolis, md.) 18 the role of cable television in the niiwendell bailey (national cable television association) and jim chiddix (time warner cable) 26 competing definitions of "openness" on the giijonathan band (morrison and foerster, washington, d.c.) 31 communications for people on the move: a look into the futurerichard c. barth (motorola incorporated) 38 building the nii: will the shareholders come? (and if they don't, will anyone really care?)robert t. blau (bellsouth corporation) 44 the electronic universe: network delivery of data, science, and discoverygregory bothun (university of oregon), jim elias (us west communications), randolph g. foldvik (us westcommunications), and oliver mcbryan (university of colorado) 57 an sdtv decoder with hdtv capability: an allformat atv decoderjill boyce, john henderson, and larry pearlstein (hitachi america ltd.) 67 nii and intelligent transport systemslewis m. branscomb and jim keller (harvard university) 76 postnsfnet statistics collectionhanswerner braun and kimberly claffy (san diego supercomputer center) 85 nii road map: residential broadbandcharles n. brownstein (crossindustry working team, corporation for national research initiatives) 97 the nii in the home: a consumer servicevito brugliera (zenith electronics), james a. chiddix (time warner cable), d. joseph donahue (thomsonconsumer electronics), joseph a. flaherty (cbs inc.), richard r. green (cable television laboratories),james c. mckinney (atsc), richard e. ottinger (pbs), and rupert stow (rupert stow associates) 101 internetwork infrastructure requirements for virtual environmentsdonald p. brutzman, michael r. macedonia, and michael j. zyda (naval postgraduate school, monterey, california) 110 electric utilities and the nii: issues and opportunitiesjohn s. cavallini and mary anne scott (u.s. department of energy) and robert j. aiken (u.s. department ofenergy/lawrence livermore national laboratory) 123 interoperation, open interfaces, and protocol architecturedavid d. clark (massachusetts institute of technology) 133contentsixthe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. service provider interoperability and the national information infrastructuretim clifford (dyncorp advanced technology services) 145 funding the national information infrastructure: advertising, subscription, and usage chargesrobert w. crandall (brookings institution) 156 the nii in the homed. joseph donahue (thomson consumer electronics) 165 the evolution of the analog settop terminal to a digital interactive home communications terminalh. allen ecker and j. graham mobley (scientificatlanta inc.) 168 spread aloha wireless multiple access: the lowcost way for ubiquitous, tetherless access to the information infrastructuredennis w. elliott and norman abramson (aloha networks inc.) 178 plans for ubiquitous broadband access to the national information infrastructure in the ameritech regionjoel s. engel (ameritech) 185 how do traditional legal, commercial, social, and political structures, when confronted with a new service,react and interact?maria farnon (fletcher school of law and diplomacy, tufts university) 190 the internet, the world wide web, and open information services: how to build the global information infrastructurecharles h. ferguson (vermeer technologies inc.) 201 organizing the issuesfrances dummer fisher (university of texas at austin) 205 the argument for universal access to the health care information infrastructure: the particular needs of ruralareas, the poor, and the underservedrichard friedman and sean thomas (university of wisconsin) 209 toward a national data network: architectural issues and the role of governmentdavid a. garbin (mitre corporation) 217 statement on national information infrastructure issuesoscar garcia (for the ieee computer society) 228 proposal for an evaluation of health care applications on the niijoseph gitlin (johns hopkins university) 233 the internetša model: thoughts on the fiveyear outlookross glatzer (prodigy services [retired]) 237 the economics of layered networksjiong gong and padmanabhan srinagesh (bell communications research inc.) 241 the fiberoptic challenge of information infrastructuresp.e. green, jr. (ibm t.j. watson research center) 248 cable television technology deploymentrichard r. green (cable television laboratories inc.) 256 privacy, access and equity, democracy, and networked interactive mediamichael d. greenbaum (bell atlantic) and david ticoll (alliance for converging technologies) 271 as we may work: an approach toward collaboration on the niimarjorie greene (first washington associates) 280 the use of the social security number as the basis for a national citizen identifierw. ed hammond (duke university medical center) 286contentsxthe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. estimating the costs of telecommunications regulationpeter w. huber (manhattan institute), boban mathew (yale university), and john thorne (bell atlantic) 292 residential pc access: issues with bandwidth availabilitykevin c. kahn (intel corporation) 304 the national information infrastructure: a highperformance computing and communications perspectiverandy h. katz (university of california at berkeley), william l. scherlis (carnegie mellon university), andstephen l. squires (advanced research projects agency) 315 nomadic computing and communicationsleonard kleinrock (university of california at los angeles) 335 nii 2000: the wireless perspectivemary madigan (personal communications industry association) 342 small manufacturing enterprises and the national information infrastructurerobert m. mason, chester bowling, and robert j. niemi (case western reserve university) 351 architecture for an emergency lane on the nii: crisis information managementlois clark mccoy and douglas gillies (national institute for urban search and rescue) and john harrald(niusr and george washington university) 364 aspects of integrity in the niijohn c. mcdonald (mbx inc.) 374 what the nii could be: a user perspectivedavid g. messerschmitt (university of california at berkeley) 378 role of the pc in emerging information infrastructuresavram miller and ogden perry (intel corporation) 388 nii evolutionštechnology deployment plans, challenges, and opportunities: at&t perspectivemahal mohan (at&t corporation) 397 enabling petabyte computingreagan w. moore (san diego supercomputer center) 405 private investment and federal national information infrastructure policyorganization for the protection and advancement of small telephone companies (opastco) 412 thoughts on security and the niitom perrine (san diego supercomputer center) 416 trends in deployments of new telecommunications services by local exchange carriers in support of anadvanced national information infrastructurestewart d. personick (bell communications research inc.) 422 the future nii/gii: views of interexchange carriersrobert s. powers (mci telecommunications inc.), tim clifford (sprint, government systems division), andjames m. smith (competitive telecommunications association) 434 technology in the local networkj.c. redmond, c.d. decker, and w.g. griffin (gte laboratories inc.) 447 recognizing what the nii is, what it needs, and how to get itrobert f. roche (cellular telecommunications industry association) 462contentsxithe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. electronic integrated product development as enabled by a global information environment: a requirement forsuccess in the twentyfirst centurythomas c. rochow, george e. scarborough, and frank david utterback (mcdonnell douglas corporation) 469 interoperability, standards, and security: will the nii be based on market principles?quincy rodgers (general instrument corporation) 479 technology and cost models for connecting k12 schools to the national information infrastructurerussell i. rothstein and lee mcknight (massachusetts institute of technology) 492 geodata interoperability: a key nii requirementdavid schell, lance mckee, and kurt buehler (open gis consortium) 511 electronic commercedan schutzer (citibank corporation) 521 prospects and prerequisites for local telecommunications competition: public policy issues for the niigail garfield schwartz and paul e. cain (teleport communications group) 538 the awakening 3.0: pcs, tsbs, or dtmftvšwhich telecomputer architecture is right for the next generation's public network?john w. thompson, jr. (gnostech incorporated) 546 effective information transfer for health care: quality versus quantitygio wiederhold (stanford university) 553 integrating technology with practice: a technologyenhanced, fieldbased teacher preparation programronald d. zellner, jon denton, and luana zellner (texas a&m university) 560 regnet: an npr regulatory reform initiative toward nii/gii collaboratoriesjohn p. ziebarth (national center for supercomputing applications), w. neil thompson (u.s. nuclear regulatory commission), j.d. nyhart, kenneth kaplan (massachusetts institute of technology), bill ribarsky (georgia institute of technology), gio wiederhold, michael r. genesereth (stanford university), kenneth gilpatric(national performance review netresults.regnet and administrative conference of the united states [formerly]), tim e. roxey (national performance review regnet. industry, baltimore gas and electric, andcouncil for excellence in government), william j. olmstead (u.s. nuclear regulatory commission), benslone (finite matters ltd.), jim acklin (regulatory information alliance) 576 electronic document interchange and distribution based on the portable document format, an open interchangeformatstephen n. zilles and richard cohn (adobe systems incorporated) 605contentsxiithe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.white papers the unpredictable certainty informationinfrastructure through 2000 xiiithe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. xivthe unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.1the national information infrastructure and the earthsciences: possibilities and challengesmark r. abbottoregon state universityas with most areas of science, computer networks (especially wide area networks) have becomeindispensable components of the infrastructure required to conduct environmental research. from transport ofdata to support for collaboration, to access to remote information processing resources, the network (which i willuse as synonymous with the internet) provides many essential services. in this paper i discuss the varioustechnical challenges in the use of information networks for the earth sciences. however, the technical issues,though important, are not the essential point. the network is, after all, only a collection of wires, switches,routers, and other pieces of hardware and software. the most serious issue is the content carried across thesenetworks and how it engenders changes in the way earth scientists relate to data, to each other, and to the publicat large. these changes have impacts that are far more profound than access to bandwidth or new networkprotocols.most of the discussions of the national information infrastructure (nii) have focused on technical details(such as protocols) and implementation (e.g., provision of universal access), with little discussion of the impactsof an nii on the scientific process. instead, discussions of the interactions between technology and humanactivities focus almost exclusively on the positive aspects of networks and social interactions. for example,networks have been extolled as tools for an expanding sense of community and participatory democracy.however, technology does not have only positive effects; the impacts are instead far more subtle and often moreextensive than they first appear. they may not appear for decades. in this paper i neither extol nor condemn theimpacts of computer networks on the conduct of science. rather, it is essential that we become aware of theseimpacts, both positive and negative. i show that networks do far more than simply move bits; they fundamentallyalter the way we think about science.earth science and networksdata and earth sciencebefore exploring the role of networks in earth science, i first briefly discuss the role of data inenvironmental research. as my background is in oceanography, my comments focus on the ocean sciences, butthese observations are generally applicable to the earth sciences.unlike experimental sciences such as chemistry or physics, most earth sciences cannot conduct controlledexperiments to test hypotheses. in some cases, though, manipulations of limited areas such as lakes or smallforest plots can be done. other branches of science that depend heavily on observations, such as astronomy, canobserve many independent samples to draw general conclusions. for example, astronomers can measure theproperties of dozens of blue dwarf stars. however, earth science (particularly those fields that focus on largescale or globalscale processes such as oceanography) must rely on many observations collected under a varietyof conditions to develop ideas and models of broad applicability. there is only one earth.the national information infrastructure and the earth sciences: possibilities and challenges1the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.earth science thus is driven strongly by developments in observing technology. for example, theavailability of satellite remote sensing has transformed our view of upper ocean biology. the spring bloom in thenorth atlantic, the sudden ''flowering" of phytoplankton (the microscopic plants of the ocean) that occurs over aperiod of a few weeks, was thought to be primarily a local phenomenon. however, satellite imagery of oceancolor (which is used to infer phytoplankton abundance) has shown that this event covers the entire north atlanticover a period of a few weeks. here, a new observing technique provided an improved understanding of what wasthought to be a wellknown process.there are instances where new observing systems have transformed our understanding of the earth. forover 40 years, the state of california has supported regular sampling of its coastal waters to understand therelationship between ocean circulation and fisheries production. a sampling grid was designed based on ourunderstanding of ocean processes at the time. when satellite images of sea surface temperature (sst) andphytoplankton abundance became available in the early 1980s, they revealed a complex system of "filaments"that were oriented perpendicular to the coast and sometimes extended several hundred miles offshore. furtherstudies showed that these filaments are the dominant feature of circulation and productivity of the californiacurrent, yet they were never detected in the 40year record. the original sampling grid had been too widelyspaced. this example slide as can sometimes lead us to design observing systems that miss critical processes.the interaction between ideas and observations occasionally results in more subtle failures, which may befurther obscured by computing systems. a notable example occurred during the 1982œ1983 el niño/southernoscillation (enso) event. enso events are characterized by a weakening of the trade winds in the tropicalpacific, which results in a warming of the eastern pacific ocean. this shift in ocean circulation has dramaticimpacts on atmospheric circulation, such as severe droughts in australia and the pacific northwest and floods inwestern south america and southern california. the 1982œ1983 enso was the most dramatic event of thiscentury, with ocean temperatures 5°œ6°f warmer than normal off southern california. this physical eventstrongly influenced ocean biology as well. lower than normal salmon runs in the pacific northwest areassociated with this major shift in ocean circulation.the national oceanic and atmospheric administration (noaa) produces regular maps of sst based onsatellite, buoy, and ship observations. these sst maps can be used to detect enso warming events. because ofthe enormous volume of satellite data, procedures to produce sst maps were automated. when sst valuesproduced by the satellites were higher than a fixed amount above the longterm average sst for a region, thecomputer processing system would ignore them and would use the longterm average value instead (i.e., theprocessing system assumed that the satellite measurements were in error). as there was no human intervention inthis automated system, the sst fields continued to show "normal" sst values in the eastern tropical pacific in1982. however, when a noaa ship went to the area in late 1982 on a routine cruise, the ocean was found to besignificantly warmer than had ever been observed. an alarm was raised, and the satellite data were reprocessedwith a revised error detection algorithm. the enormous rise in sst over much of the eastern pacific wasrevealed. the largest enso event of the century had been hidden for several months while it was confidentlypredicted that there would be no enso in 1982.this episode reveals that the relationship between data and ideas has become more complex with the arrivalof computers. the increasing volume and complexity of the data available for earth science research have forcedus to rely more heavily on automated procedures. although this capability allows us to cope with the volume, italso relies on precise specification of various filters and models that we use to sort data in the computer. thesefilters may reflect our preconceived notions about what the data should actually look like. although computersand networks apparently place more data into our hands more rapidly, the paradox is that there is increasingdistance between the scientist and the actual physical process. this "handsoff" approach can lead to significantfailures in the overall observing system.as noted by theodor roszak 1, raw data are of little value without an underlying framework. that is, ideascome before data. there must be a context for observations before they can make sense. a simple stream oftemperature readings will not advance science unless their context is defined. part of this framework includes theability to repeat the measurements or experiment. such repeatability strengthens the claim of the scientist that theprocess under study is a general phenomenon with broad applicability. this framework also includes athe national information infrastructure and the earth sciences: possibilities and challenges2the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.historical context as well. because of the strong observational nature of earth science, it depends on earlier fieldprograms to develop new theories and understanding.with the availability of networks and computers, the task of obtaining, managing, and distributing data hasbecome critical. the information system has become part of the observing system. the way we collect, store,and retrieve our data becomes another set of filters, just like the sampling strategy or measurement technique.often technical problems obscure the underlying science. that is, the information science issues can dominatethe earth science issues. i explore these technical issues in the next section.earth science and information systemsthe network and computational requirements for earth science focus on the more obvious problems ofbandwidth, accessibility, ease of use, and so on. i argue that although these issues are important, the profoundshift in networks and computational systems has exacerbated the fundamental conflicts between informationsystems and the conduct of science while simultaneously obscuring these conflicts. analogous to the analysis oftelevision by mark crispin miller 2, information technology has both revealed these problems and hidden themwithin the medium itself.technical requirementsearth science relies heavily on close interactions between many researchers from many disciplines. a singlescientist cannot reserve an entire oceanographic research vessel for a cruise. such expeditions require the workof many scientists. the study of problems such as the impacts of climate change on ocean productivity require anunderstanding of the physical dynamics of both the atmosphere and ocean, besides knowledge of ocean biology.earth scientists must therefore develop effective mechanisms for sharing data.along with the need to share data and expertise among widely dispersed investigators, the characteristics ofthe data sets impose their own requirements. as earth science moves toward an integrated, global perspective,the volumes of data have increased substantially. although the dominant data sources continue to be earthobserving satellites, field observations have also grown significantly. sensors can now be deployed for longertime periods and can sample more rapidly. the number of variables that can be measured has increased as well.a decade ago, a researcher would come back from a cruise with a few kilobytes of data; today, a researcher willreturn with tens of gigabytes. however, these numbers are dwarfed by the data volumes collected by satellites orproduced by numerical models. the earth observing system (eos), which is planned by the nationalaeronautics and space administration (nasa), will return over 1 terabyte per day of raw data.the demands for nearrealtime access to data have also appeared. satellitebased communications toremote sampling sites have opened up the possibilities of having rapid access to observations, rather than waitingseveral months to recover environmental sensors. with more capable database servers, data can be loaded andmade accessible over the network in hours to days after collection. this is in contrast to an earlier era when datawere closely held by an individual investigator for months or even years. although realtime access does opennew areas for environmental monitoring and prediction, it does not necessarily address the need to accumulatethe long, consistent, highquality time series that are necessary for climate research. the pressures to meeteveryday demands for data often distract scientists from the slower retrospective analyses of climate research.as data become more accessible faster, public interest increases. as with the comet impact on jupiter in1994, public interest in science and the environment can often far exceed the anticipated demand. the eos dataand information system (eosdis) was originally designed to meet the needs of several thousand earth scienceresearchers. now it is being tasked with meeting the undefined needs of the much larger general public 3. thispresents many technical challenges to an agency that has little experience dealing with a potentially enormousnumber of inexperienced users.the national information infrastructure and the earth sciences: possibilities and challenges3the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.technical challengesagainst this backdrop of new technical requirements, earth science is facing a new set of technicalchallenges in addition to the continuing challenge of network bandwidth. the structure of the internet isundergoing massive changes. with the departure of national science foundation funding for the networkbackbone and for the regional providers as well in the near future, there will be increasing emphasis oncommercial customers. interoperability and functionality of the network access points remain problematic. thepossibility of balkanization of the network is real and not insignificant. the number of users has also expandedrapidly, and with the appearance of new applications such as the world wide web (www), the effectivebandwidth has dropped dramatically.as the science community becomes a smaller and smaller fraction of the network user community, networkproviders focus less on meeting scientific needs and more on meeting commercial needs. the search forbandwidth has become more intense. telecommunication companies claim that new protocols (such asasynchronous transfer mode [atm]) and new hardware(fiber optics) will usher in a new era of unlimitedbandwidth. some researchers claim that software "agents" will reduce the need for bandwidth by relying onintelligence at each node to eliminate the need for bulk transfers of data. besides the technical hurdles tobringing such technologies to market, there are economic forces that work against such developments. in aprocess that is well known to freeway designers and transportation planners, bandwidth is always used tocapacity when the direct costs of the bandwidth are not borne by the users. funding for the network is hiddenfrom the user so that any increase in personal use of network capacity is spread over every user. in reality,bandwidth is not free, though it is essentially free to the individual user. even in the case of a totally commercialnetwork, it is likely that the actual costs will be amortized over a broad customer base so that an individual userwill have little incentive to use bandwidth efficiently. although i pay a certain amount directly for the interstatehighway system through my gasoline bills, the actual costs are hidden in many other fees and spread over abroad range of the population, many of whom may never even use the highway system.with the rise of commercial internet providers such as america online and netcom, will this situationchange? will users pay the true costs of using the network as opposed to paying only a marginal cost? i wouldargue that this is unlikely on several grounds. first, many users currently have access to virtually free internetservices through universities and other public institutions; it will be difficult to break this habit. second, thegovernment, through its emphasis on universal access, is unlikely to completely deregulate the system so thatrural users (who truly are more expensive to service) will pay significantly more than urban users. third, andperhaps more compelling, network bandwidth is no different from any other commodity. every second that thenetwork is not used, revenue is lost. thus it is in the network providers' interest to establish a pricing structurethat ensures that at least some revenue is generated all the time, even if it is at a loss. some revenue is alwaysgreater than no revenue, and the losses can be made up elsewhere in the system. this is a wellestablishedpractice in the longdistance telephone industry as well as in the airline industry. offpeak prices are not lower toshift traffic from peak periods to less congested periods; they are designed to encourage usage during offpeakperiods, not reduce congestion during peak periods. they raise the "valleys" rather than lowering the "peaks."the final threat to network bandwidth is the proliferation of new services. there is no doubt that as networkbandwidth increases, new services become available. early networks were suitable for electronic mail and otherlowbandwidth activities. this was followed by file transfers and remote logins. the www dramaticallyincreased the capabilities for data location and transfer. just as the interstate highway system fosters thedevelopment of new industries (such as fast food franchises and overnight package delivery systems), so also hasthe internet. as with highways, new services create new demand for bandwidth. although considerable effort isbeing devoted within the nii to develop rational pricing strategies, it is more likely that the search for bandwidthwill continue. it appears to be a law of networks that spare bandwidth leads to frivolous traffic.as services and users proliferate, it has become more difficult for users to locate the data of interest. searchtools are extremely primitive, especially when compared with the tools available in any wellrun library. various"web crawlers" will locate far more irrelevant information than relevant information. for a user who does notknow exactly where to find a specific data source, much time will be spent chasing down dead ends, or circlingback to the beginning of the search. although the relative chaos of the network allows anyone to easilythe national information infrastructure and the earth sciences: possibilities and challenges4the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.set up a data server, this chaos confounds all but the most determined user. there are no standard methods fordefining the contents of a server and without "truth in advertising" rules, servers, that may superficially appear tobe relevant may contain totally irrelevant information. the only users who have time to "surf" the network arethose who have nothing else to do, as noted by negroponte 4. once the appropriate server has been located, thereis no consistent method for indexing and archiving data. although data may be online, it often still requires aphone call to locate and access the relevant data sets.with the increase in computer processor speeds and desktop mass storage, it has become increasinglyimportant to match network speeds with these other components of the system. following amdahl's rule where1 bit of input/output requires one instruction cycle, this implies that a 1,000mips (million instructions persecond) processor will require a 1gigabitpersecond network interface. assuming that processor performancecontinues at a rate of 50 to 100 percent per year, we will have 1,000mips machines in about 1 to 2 years. it isbecoming commonplace for researchers to have 10to 20gigabyte disk storage systems on their desktop; in 2 to3 years it will not be unusual for scientists to have 100 gigabytes of disk subsystems. network speeds areincreasing at a far slower rate, and the present internet appears to many users to be running slower than it did 3to 4 years ago.this imbalance between processor speed, disk storage, and network throughput has accelerated a trend thatbegan many years ago: the decentralization of information. in an earlier era, a researcher might copy only smallsubsets of data to his or her local machine because of limited local capacity. most data were archived in centralfacilities. now it is more efficient for the individual scientist to develop private "libraries" of data, even if theyare rarely used. although the decentralization process has some advantages, it does increase the "confusion"level. where do other researchers go to acquire the most accurate version of the data? how do researchersmaintain consistent data sets? on top of these issues, the benefits of the rapid decrease in price/performance aremore easily acquired by these small facilities at the "fringes'' of the network. large, central facilities followmainframe price/performance curves, and they are generally constrained by strict bureaucratic rules for operationand procurement. they are also chartered to meet the needs of every user and usually focus on longtermmaintenance of data sets. in contrast, private holdings can evolve more rapidly and are not required to serviceevery user or maintain every data set. they focus on their own particular research needs and interests, not on theneeds of longterm data continuity or access by the broader community.since small, distributed systems appear to be more efficient and provide more "niche" services than docentral systems, it has become harder to fund such centers adequately (thus further reducing their services.) thissituation is similar to the "commons" issue described by hardin nearly 30 years ago 5. for the centralizedarchive, each individual user realizes great personal benefit by establishing a private library while the cost isspread evenly over all of the users. it is therefore in the interest of the individual user to maximize individualbenefit at the cost of the common facility. not until the common facility collapses do we realize the total cost.the situation is similar in the area of network bandwidth.the final two technical challenges facing earth science and networks encroach into the social arena as well.the first is the area of copyrights and intellectual property. although scientists are generally not paid for the useof their data sets or their algorithms, there is a strong set of unwritten rules governing use and acknowledgmentof other scientists' data sets. with the network and its rapid dissemination of information and more emphasis onthe development of web browsers, this set of rules is being challenged. data are being moved rapidly fromsystem to system with little thought of asking permission or making an acknowledgment. copying machineshave created a similar problem, and the publishing industry has reacted vigorously. however, the rate at which acopying machine can replicate information is far slower than that for a computer network, and its reach is farsmaller. as new data servers appear on the network, data are rapidly extracted and copied into the new systems.the user has no idea of what the original source of the data is nor any information concerning the data's integrityand quality.discussions over copyrights have become increasingly heated in the past few years, but the fundamentalissues are quite simple. first, how do i as a scientist receive "payment" for my contributions? in this case,payment can be as simple as an acknowledgment. second, how do i ensure that my contributions are not usedincorrectly? for example, will my work be used out of context to support an argument that is false? with global,nearly instantaneous dissemination of information, it is difficult to prevent either nonpayment or improper use.after a few bad experiences, what incentive is there for a scientist to provide unfettered access?the national information infrastructure and the earth sciences: possibilities and challenges5the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the last technical challenge involves the allocation of resources. network management is not for amateurs.the internet requires experts for its operation. web browsers are not easy to design, build, and maintain. thusprogramming talent that was originally hired to engage in scientific analysis is spending a larger fraction of itstime engaged in nonscientific activities. although developments in the commercial field are simplifying theseand associated tasks, they do cost money. with declining resources for science, one can ask whether this is areasonable use of scarce resources. the pace of technological change is also increasing, so that the intellectualand technical infrastructure that was assembled 5 to 10 years ago is largely irrelevant. for example, it isbecoming harder to hire fortran programmers. experts on atm have not yet appeared.the computer industry and earth science researchers have generally focused on these challenges from atechnological point of view. that is, bandwidth will increase through the provision of atm services. datalocation will be enhanced through new www services. copyrights (if they are not considered absolutelyirrelevant or even malevolent) can be preserved through special identification tags.the technology optimists have decided that the problems of earth science (and science in general) can besolved through the appropriate application of technology. that is, the fundamental problems of science will beaddressed by "better, faster, cheaper" technical tools. on the science side of the issue, it appears that thescientific community has largely accepted this argument. as information systems approach commodity pricing,scientists acquire the new technology more rapidly in an attempt to remain competitive in an era of decliningfederal support. as put forth by neil postman 6, this is the argument of "technology." that is, the technology hasbecome an end in itself. the fundamental problem of science is understanding, not the more rapid movement ofdata. although we have seen that the link between understanding and observations is perhaps more closelyentwined in the earth sciences, we must be aware of the implications of our information systems for how weconduct science. it is to this point that i now turn.the hidden impactsa recent book by clifford stoll 7 provided a critical examination of the information infrastructure andwhere we are headed as a society. although he makes many valid points, stoll does not provide an analysis as tohow we arrived at this predicament. i draw on analyses of the media industries (especially television) bypostman and miller that show some parallels between information systems and mass media. i do not argue thatwe should return to some pristine, precomputer era. there is no doubt about the many positive aspects ofinformation technology in the earth sciences. however, it is worth examining all of its impacts, not just thepositive ones.information regulationpostman postulates that culture can be described as a set of "information control" processes 8. that is, wehave established mechanisms to separate important knowledge from unimportant knowledge (such as a schoolcurriculum) and the sacred from the profane (such as religion). even in the world of art and literature, we areconstantly making judgments as to the value of a particular work of art. the judgments may change over time,but the process remains.we are constantly inundated with information about our world, either from the natural world or by ourcreations. somehow we must develop systems to winnow this information down to its essential elements. in thescientific world, we have established an elaborate set of rules and editorial procedures, most of which are notwritten down. for example, experiments that cannot be repeated are viewed as less valuable than those that canbe. ideas that cannot be traced to previous studies are viewed with more skepticism than those that can be tracedthrough earlier research. occasionally, a new idea will spring forth, but it, too, must go through a series of teststo be evaluated by the scientific community. this "editorial" process essentially adds value to the information.the second point is that the scientific community believes that there is an objective reality that is amenableto observation and testing. although clearly science is littered with ideas that rested on an individual's biases andprocesses that were missed because they did not fit our preconceived notions, we still believe that thethe national information infrastructure and the earth sciences: possibilities and challenges6the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.natural world can be measured and understood in a predictable manner. assumptions and biases can at least bedescribed and perhaps quantified. scientific knowledge is more than a set of opinions.through the scientific process, researchers add value to data. useful data are separated from useless dataand interpreted within a framework of ideas. data are therefore placed in a structure that in turn can bedescribed. raw information that is presented out of context, without any sense of its historical origins, is of littleuse. thus earth science is not limited by lack of observations; rather, the correct types of observations are oftenmissing (e.g., long, calibrated time series of temperature).the process of adding value has arisen over the last several centuries. publishing in peerreviewed journalsis but one example of how valid data are separated from invalid data, and how observations are placed within alarger framework for interpretation. although data reports are often published, they are perceived to be of lessimportance than journal articles. even "data" produced by numerical models (which are the products of ourassumptions) are viewed as less valuable than direct measurements.the network and sciencethere is no doubt that networks simplify many tasks for earth science. however, there are many obviousproblems, such as the separation of information from its underlying context, the difficulty in locating informationof interest, and the lack of responsibility for the quality and value of a particular data set. much as withtelevision, it has become difficult to separate advertising from reality on the network. a recent discussion on thefuture of research universities in physics today 9 highlighted some troublesome issues associated with networks.graduate students have become increasingly unwilling to use the library. if reference materials are not availableonline in digital form, then students deem them to be irrelevant. although electronic searches can be helpful, it isclear that this attitude is misguided. most scientific documents will never be placed online because of theassociated costs. second, digital storage is highly ephemeral and can never be considered a true archive. therewill always be a machine and software between the reader and the data, and these tools are always becomingobsolete. third, digital searching techniques follow rigid rules. present search methods are quite sparse andlimited. thus far, no one has shown the ability to find material that truly matches what the reader wants althoughthe search was incorrectly specified. such serendipitous discoveries are common in libraries.the network is becoming a place of advertising, with little true content and little personal responsibility.home pages are proliferating that merely test what the network is capable of doing instead of using the networkto accomplish a useful task. we have fixated on the technology, on the delivery system for information, ratherthan on the "understanding system" 10. speed, volume, and other aspects of the technology have become thegoals of the system. although these are useful, they do not necessarily solve the problems of collaboration, dataaccess, and so on. in fact, they can distract us from the real problems. the issue of scientific collaboration mayrequire changes in the promotion and tenure process that are far more difficult than a new software widget.the emphasis on speed arises in part from the need to have short "return on investment." in such anenvironment, market forces work well in the development of flexible, responsive systems. for earth science, thisis a useful ability for some aspects of research. for example, development of new processing algorithms forsatellite sensors clearly benefits in such an environment. however, this shortterm focus is not sufficient. longterm climate analysis, where the data must be collected for decades (if not centuries) and careful attention mustbe paid to calibration, will not show any return on investment in the short run. these activities will "lose" moneyfor decades before one begins to see a return on the investment. in a sense, long time series are "commonfacilities," much like network bandwidth and central archives. they are the infrastructure of the science.the network and televisionthe early days of television were filled with predictions about increased access to cultural activities,improved "distance" learning, and increased understanding between people. the global village was predicted tobe just around the corner. however, the reality is quite different. television may fill our screens withthe national information infrastructure and the earth sciences: possibilities and challenges7the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.information, but the nature of the medium has not increased our understanding 11. science programming is usefulas a public relations tool (and such contacts with the public should be encouraged), but the medium is notsuitable for scientific research.as discussed above, one of the key activities of science is the structuring of information to sort valid frominvalid. but computer networks increasingly encourage disjointed, contextfree searches for information. ourmetaphors resemble those of television and advertising, rather than those of science and literature. home pagesencourage rapid browsing through captivating graphics; long pages of text are viewed as a hindrance.in the early days of television, the message was in front of the viewer, as discussed by miller 12. it was clearwhat product was being advertised. today, both the medium and its aesthetics are in front. the smooth graphics,the rapidly shifting imagery, the compelling soundtrack are the primary elements. the advertising and theproduct are in the background. feelings about products rather than rational evaluations are transmitted to theviewer. this sense of the aesthetic has permeated other media, including film, and to some extent, newspapers.presentation is more important than content.the early days of computers were characterized by cumbersome, isolated, intimidating machines that weresafely locked away in glass rooms. the computer was viewed as a tool whose role was clearly understood.today, computers are ubiquitous. most earth scientists have at least one if not two computers in their offices,plus a computer at home. this demystification of computers has been accompanied by much emphasis on theinterface. computers are now friendly, not intimidating. their design now focuses on smoothness, exuding an airof control and calm. as described in a biography of steve jobs 13, design of the next workstation focusedalmost exclusively on the appearance of the machine. most of the advances in software technology have focusedon aesthetics, not on doing new tasks. these new software tools require more technical capability (graphics,memory, and so on) to support their sophisticated form. this emphasis on form (both hardware and software)violates the bauhaus principle of form following function. the computer industry appears to focus more onselling a concept of information processing as opposed to selling a tool.postman 14 has described print as emphasizing logic, sequence, history, and objectivity. in contrast,television emphasizes imagery, narrative, presentation, and quick response. the question is, where do computernetworks sit with regard to print versus television? there is no doubt that networks are beginning to resembletelevision more than print. the process of surfing and grazing on information as though it were just anothercommodity reduces the need for thoughtful argument and analysis. networks encourage the exchange ofattitudes, not ideas. the vast proliferation of data has become a veritable glut. unlike television, anyone can be abroadcaster; but to rise above the background noise, one must advertise in a more compelling manner than one'scompetitors.conclusions and recommendationsnetworks will continue to play an important role in the conduct of earth science. their fundamental roles ofdata transport and access cannot be denied. however, there are other effects as well that are the result of aconfluence of several streams. first, the nextgeneration networks will be driven by commercial needs, not bythe needs of the research and education community. second, the sharp decrease in price/performance of mostcomputer hardware and the shortened product life cycles have required the science community to acquire newequipment at a more rapid pace. third, expected decreases in federal funding for science have resulted in greateremphasis on competitiveness. this confluence has caused scientists to aim for rapid delivery of information overthe network. without the regulations and impedance of traditional paper publishing, scientists can now argue innear real time about the meaning of particular results. "flame" wars over electronic mail are not far behind. thecommunity now spends nearly as much time arguing about the technical aspects of the information deliverysystem as it does in carrying out scientific research.networks allow us to pull information out of context without consideration of the framework used to collectand interpret the data. everincreasing network speeds emphasize the delivery of volume before content. if allinformation is equally accessible and of apparently equal value, then all information is trivial. science is at riskof becoming another "consumable" on the network where advertising and popularity are the main virtues.the national information infrastructure and the earth sciences: possibilities and challenges8the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.long, thoughtful analyses and small, unpopular data sets are often overwhelmed in such a system. similarprocesses are at work in television; the metaphors of the tv world are rapidly appearing in the network world.one can successfully argue that earth science is currently limited by the lack of data (or at least the correctdata), but an equally serious problem is the inability to synthesize large, complex data sets. this is a problemwithout a technological solution. while information systems can help, they will not overcome this hurdle.delivering more data at a faster rate to the scientist will obscure this fundamental problem. indeed, technologymay give the appearance of solving the problem when in reality it exacerbates it. as stated by jacob bronowski,this is the paradox of imagination in science, that it has for its aim the impoverishment of imagination. by thatoutrageous phrase, i mean that the highest flight of scientific imagination is to weed out the proliferation of newideas. in science, the grand view is a miserly view, and a rich model of the universe is one which is as poor aspossible in hypotheses.networks are useful. but as scientists, we must be aware of the fundamental changes that networks bring tothe scientific process. if our students rely only on networks to locate data as opposed to making realworldobservations, if they cannot use a library to search for historical information, if they are not accountable forinformation that appears on the network, if they cannot form reasoned, logical arguments, then we have donethem a great disservice.the balance between market forces with their emphasis on shortterm returns for individuals andinfrastructure forces with their emphasis on longterm returns for the common good must be maintained. there isa role for both the private sector and the public sector in this balance. at present, the balance appears to be tiltedtoward the short term, and somehow we must restore a dynamic equilibrium.notes[1] roszak, theodore. 1994. the cult of information: a neoluddite treatise on hightech, artificial intelligence, and the true art ofthinking. university of california press.[2] miller, mark crispin. 1988. boxed in: the culture of tv. northwestern university press.[3] u.s. government accounting office. 1995. "earth observing system: concentration on nearterm eosdis development mayjeopardize longterm success," testimony before the house subcommittee on space and aeronautics, march 16.[4] negroponte, nicholas. 1995. "000 000 111šdouble agents," wired, march.[5] hardin, garrett. 1968. "the tragedy of the commons," science 162:1243œ1248.[6] postman, neil. 1992. technopoly: the surrender of culture to technology. knopf, new york.[7] stoll, clifford. 1995. silicon snake oil: second thoughts on the information superhighway. doubleday, new york.[8] postman, technopoly, 1992.[9] physics today. 1995. "roundtable: whither now our research universities?" march, pp. 42œ52.[10] roszak, the cult of information, 1994.[11] postman, technopoly, 1992.[12] miller, boxed in, 1988.[13] stross, randall. 1993. steve jobs and the next big thing. atheneum, new york.[14] postman, technopoly, 1992.the national information infrastructure and the earth sciences: possibilities and challenges9the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.2government services information infrastructure managementrobert j. aiken and john s. cavalliniu.s. department of energyabstractthe growth and deployment of the government services information infrastructure (gsii), and itsrelationship with the national information infrastructure (nii), will require the federal government to drasticallychange the way it acquires and deploys telecommunications for support of the government's businesses. thegsii is an enlightened attempt by the clinton/gore administration to form a "virtual government" crossingagency boundaries to interact more closely with industry and with the public, as well as with state and localgovernment, to greatly improve the delivery of government services. the gsii is that portion of the nii used tolink government and its services, enable virtual agency concepts, protect privacy, and support emergencypreparedness needs. the gsii will have to be an integral component of the nii in order to do so. the gsii andother private sector efforts will have a significant impact on the design, development, and deployment of the nii,even if only through the procurement of such services.this paper concludes that the federal government must adopt new mechanisms and new paradigms for themanagement of the gsii, including improved acquisition and operation of gsii components in order tomaximize taxpayer benefits, to optimize the delivery of government services, to ensure that the gsii is anintegral component of the nii, and to adopt industry standards rather than setting them. governmentrequirements and applications, as well as the available technologies to address those requirements, will continueto evolve; therefore so must the government's use of technologies and services. the requirements from federalgovernment services and the users of these services logically form affinity groups that more accurately andeffectively define these common requirements, that drive the adoption and use of industry standards, and thatprovide a significant technology marketplace to capture the vision of the nii both now and in the future.it is critically important that the federal government adopt a management scheme that improves its ability towork with u.s. industry to ride the coming third wave,1 as opposed to being wiped out by it. a newmanagement scheme is also needed to improve cooperation between the government and its partners (i.e., theprivate sector, academia, and state and local governments) and to improve the chances for a successfuldeployment of both the gsii and nii. this new management scheme must be built upon a modular andevolutionary approach for the gsii as well as for other large systems developments to greatly improve thesuccessful use of information technology (it) by the government, especially to provide service to the privatesector and to the u.s. citizenry.note: this paper was commissioned by the government information technology services (gits) subcommittee of thecommittee on applications and technology of the information infrastructure task force (iitf). it was drafted for the gitsby john s. cavallini and robert j. aiken of the department of energy. it was reviewed and endorsed for submission to thenii 2000 forum by the gits membership.government services information infrastructure management10the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.backgroundtoday, the private sector is developing and deploying information infrastructure. at the same time, thegovernment is concurrently developing and deploying information infrastructure, mostly through contracts withthe private sector. under the lead of and at the encouragement of the clinton/gore administration, the federalgovernment has placed increased emphasis on the development and deployment of an nii as a strategic priority.this emphasis results from the understanding that properly leveraged information and information technologyare among the nation's most critical economic resources, for manufacturing industries as well as for moremodern services industries for economic security and for national security.the clinton/gore administration has made a commitment to work with business, labor, academia, publicinterest groups, congress, and both state and local government to ensure the development of an nii that enablesall americans to access information and communicate with each other using combinations of voice, data,images, or video at anytime, anywhere.2 this commitment was articulated very well by the nationalperformance review (npr) through its emphasis on using it as a key element in creating a government thatworks better and costs less.3 the president and vice president recognize the need to use it to improveamericans' quality of life and to reinvigorate the economy. to this end, they outlined a threepart agenda forspreading it's benefits to the federal government: (1) strengthen leadership in it, (2) implement electronicgovernment, and (3) establish support mechanisms for electronic government. thirteen major it areas wereidentified for accomplishing the threepart agenda:1. provide clear, strong leadership to integrate it into the business of government;2. implement nationwide, integrated electronic benefit transfer;3. develop integrated electronic access to government information and services;4. establish a national law enforcement/public safety network;5. provide intergovernmental tax filing, reporting, and payments processing;6. establish an international trade data system;7. create a national environmental data index;8. plan, demonstrate, and provide governmentwide electronic mail;9. improve government's information infrastructure;10. develop systems and mechanisms to ensure privacy and security;11. improve methods of it acquisition;12. provide incentives for innovation; and13. provide training and technical assistance in it to federal employees.development of an nii is not an end goal in or of itself. government requires an infrastructure to conductits business more effectively and to deliver services to the american citizenry at lower cost to the taxpayers. anumber of suitable nationalscale applications or uses of the nii have been identified and documented by theiitf's committee on applications and technology.4 these uses of the nii, in addition to nationwide humanisticapplications such as health care and education, include the fundamental businesses or enterprises of the federalgovernment such as law enforcement, electronic commerce (including benefits), basic research, environment,health care, and national security, and as such represent a significant set of driving requirements for niideployment.in recognizing this fact, the npr concluded that the government use of it and development of informationinfrastructure should be improved and better coordinated in order to effectively address government businessrequirements. the npr made approximately 60 recommendations for action in this regard, including thedevelopment of a plan for a gsii to electronically deliver government services and to integrate electronic accessto governmentprovided information and services. the gsii is that portion of the nii used to link governmentand its services, enable virtual agency concepts, protect privacy, and support emergency preparedness needs. itwas also recognized that better integration and coordination were required not only across federal governmentgovernment services information infrastructure management11the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.agencies, but also across federal and state governments, and local and tribal governments, to the private sectorand to the public at large.to achieve these goals, the vice president established the gits working group in december 1993. themission of the gits working group is to promote the improvement of agency performance through the use of it,accelerate the deployment of advanced networking technologies, and in conjunction with the office ofmanagement and budget (omb) and general services administration (gsa), establish procurement andimplementation policies, practices, and directives designed to improve productivity and reduce costs. theworking group's responsibilities include implementing the npr's recommendations, coordinating federal agencyand white house initiatives for evolving the nii, and enhancing crossagency it collaborations as well asgovernment and industry it activities.the gsii: analysisthe gsii is that portion of the nii used to link government and its services, enable "virtual agency"concepts, protect privacy, and support emergency preparedness needs. the "virtual agency concept" allowsservices and functions to cross traditional organizational and geographic boundaries. for example, a u.s. citizencould, in this fashion, receive multiple services such as social security, food stamps, tax preparation assistance,information on the national park system, and so on through a single interaction with one agency.the attributes of an effective gsii must include timely infusion of appropriate it to meet requirements;coordination of infrastructure across government requirements and with the private sector; effectiveness, whilealso allowing for innovation and flexibility to meet specific needs; information surety to guard citizens' rightsand privacy while also providing for national security needs; costeffectiveness of it acquisition anddevelopment while also encouraging a competitive technological environment; ubiquitous accessibility to allcitizens, including those with disabilities; ease of use for locating government information or for servicedelivery; transparent (to the user) integration of the gsii portion of the nii with the rest of the internet, nii, andthe global information infrastructure; shared resource use across government entities; and the mechanisms,processes, and procedures to appropriately manage such a diverse infrastructure.as opposed to the open system interconnect sevenlayer model, information infrastructure is more oftencurrently viewed as having three or four layers; a recent report of the national research council (realizing theinformation future) proposes a standard open bearer service to promote, and perhaps maximize, interoperabilityacross the nii.5 this is a significant feature that the gsii must adopt and integrate as it evolves. nevertheless,merely focusing on the nii layer concept and other technical models can lead one to neglect two of the mostimportant components of the gsii and the nii. they are people, both using and providing services over the nii,and the coordination mechanisms needed to effectively manage all gsii activities and efforts.standards are often cited as the only way to achieve interoperability across the nii or gsii. however,standards can often be too much of a good thing.6 the technology life cycle averages 12 to 18 months, with newhardware and software being introduced at a dizzying pace. the current standards processes have not been ableto keep up. competition among many vendors is crucial for reducing cost as well as ensuring a viable"technological gene pool" for both current and future requirements. innovation requires new concepts. flexibilityis required to meet a wide and diverse set of special requirements and can enable the incremental evolution of thegsii and nii in a stepwise manner. large systems development must be modular and better coordinated.today's infrastructure has an enormous amount of different components composed of various technologies andstandards. therefore, a modular, seamless integration and evolution of the multicomponent gsii into theevolving nii will need to be based primarily on voluntary processes and proven interoperability solutions ratherthan on mandated standards. these and many more facts lead us to the conclusion that standards alone are notsufficient.furthermore, although the internet protocol suite was originally developed as a military specification, it wasnot adopted as a federal procurement standard or an industry standard. nevertheless, through versatility andcapability to meet the research community's requirements, while supporting a productive range ofinteroperability, the internet transmission control protocol (tcp)/internet protocol (ip) became the most widelyusedgovernment services information infrastructure management12the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.internetworking protocol. therefore, common requirements and mutually beneficial, modular, evolutionarytechnical solution(s) will provide the best gauge of the range of interoperability that is ultimately needed.current federal coordination mechanisms are a result of the brooks adp act of 1965 which was passed inan era characterized by mainframe computers when timeshared processing was the primary technique formanaging scarce it resources. reflecting the centralized technology, the decisional and oversight responsibilityfor planning, acquiring, and managing it was taken from the heads of the various executive agencies and vestedin the administrator for general services. the theory was that investments in it must be centrally managed andthat only a dedicated cadre of it professionals could perform the function.over the years, the original model eroded and technology became decentralized. today, agencies plan andmanage their own it investments; the authority to contract for or to procure it solutions is "delegated" to theagency heads. as a condition of the delegation, agencies are subject to an additional, centralized acquisition andmanagement oversight. that oversight, however, is generally redundant in nature (i.e., oversight checkersoutside an agency are checking internal oversight checkers) and neither set of checkers possesses the institutionalexpertise or the resources to adequately understand, let alone manage, the government's increasinglydecentralized and diverse it infrastructure. ultimately, the centralized control model of the brooks act, whichreduced both the responsibility and the authority of the heads of federal agencies, contributed to significantfailures in it management and seriously misspent resources.the old centralized approach could achieve interoperability neither across the government infrastructure norwith the private sector, and it was even counterproductive in efforts to do so. in the future, an nii willundoubtedly incorporate multiple paradigms of internetworking, interoperability, and communications, thusmaking the task of coordination and interoperability even more difficult. logically then, the question is how topromote interoperability. we propose starting with what the gsii will be used foršthe business functions thatcan be considered gsii applicationsšand with those people who share common interests in both using andproviding the gsii. the applications should determine what standards and technologies are required and willprovide interoperability among their own constituency as well as with other groups, if properly coordinated.it should be noted, however, that gsa has taken steps to try to improve the government's acquisition of itand to recognize this new paradigm of distributed it functionality and management. two notable examples arethe time out program that tries to get faltering largesystems efforts back on track and the governmentwideacquisitions contracts program that empowers lead agencies to conduct multiagency contracts.applications and affinity groupsgovernment services and uses of the nii for law enforcement, benefits, and health care touch everycommunity. requirements for research, the environment, and national security go beyond the geographicrequirements. the latter also require advances in the stateoftheart, leadingedge it capabilities. hence,government applications provide significant financial leverage and incentive for nii deployment, as well as fornew it development. if one adds applications for education, generic electronic commerce, and energy deliveryand management to this set of federal nii applications, the cost leveraging for deploying the nii becomessignificant. for example, in energy, "[e]lectric utilities already serve over 95 percent of american homes (apercentage point above telephone companies)" with the "likely requirement that all these homes will need accessto advanced telecommunications to manage energy" consumption.7 it is unlikely that telecommunications forenergy demand management will replace conventional access mechanisms, especially for the last mile toresidences. however, the energy utilities telecommunications infrastructure can augment, leverage, and enhancethe cable and telecommunications industry infrastructure and facilitate access to the nii, and hence the gsii, foralmost all americans, including those in remote rural areas.in addition to the impact the energy utilities may have on the nii, fts 2000 and its successor will have amore direct impact on both the nii and gsii, since it will be a major vehicle for supplying telecommunicationsand services to the federal government. today's fts 2000 "currently provides intercity telecommunications for1.7 million federal government users."8 postfts 2000 is expected to go beyond the current fts 2000 bydelivering intercity telecommunications as well as valueadded services such as electronic mail, key management,government services information infrastructure management13the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.teleconferencing, and more. this deployment will be an integral component of the gsii and will use industryprovided services almost exclusively, thereby leveraging the products of industry rather than competing withthem. however, this reliance on and use of industry products and services will not lessen the impact that thefederal government, via procurement vehicles, will have on the nii and gsii; therefore, the proper coordinationof the deployment and management of the gsii with respect to the nii is even more critical.the focus should be on achieving an effective, consultative, and collegial interagency process for managingthe gsii. this will require creating a methodology for managing evolving it policies, priorities, and programsthat provides and coordinates the framework within which the mission, as well as the administrative, itrequirements, and activities of individual federal departments and agencies are conducted. the principal focus ofthe current toplevel it management is on what today are perceived as the common goalsšmainly theadministrative requirements for finance, personnel, facilities, and so onšof the overall enterprise called thefederal government. implementation of a new enterprise model, based on an understanding of gsii requirementsdriven by these other mission applications as well as those that represent government services to its customersand their nongovernment requirements, is essential to making the government work better and cost less.the federal internetworking requirements panel (firp), created by the national institute of standards andtechnology at the request of the department of energy's office of scientific computing and the highperformance computing and communications community, issued a report that recommends increasedresponsibility for this shared infrastructure, such as a gsii, for the mission areas of federal agencies or theirlogical affinity groups in as compatible a way as practicable with the common vision of the federal government.9the term "affinity group" means government agencies, or functional interest groups therein, that shareinformation electronically and have common it requirements. the npr emphasized the need for improvedinfrastructure for crossagency groups. in addition, the firp report recommended the development of affinitygroups to enhance crossagency collaborations. taken together, these factors make it reasonable to require thateach agency explicitly ensure that gsii issues, including interoperability, are addressed not only within a givenfederal agency, but also with external affinity groups, industry, and the public, in which these agency mission or"enterprise" activities take place.the federal research community is an excellent example of an affinity group. during the past decade, thefederal research community has placed a very high priority on the application of advanced informationtechnologies to enable advances in many science disciplines. as experimentation becomes too expensive, toounsafe, or too environmentally unsound, there is an increase in the importance and value of computationalexperiments, collaborative virtual reality, and distributed computing infrastructure technologies for remoteaccess to oneofakind facilities, shared data, and dispersed collaborations. correspondingly, a sound andcapable nii is needed, since the research community crosses many organizational boundaries, large geographicaldistances, and multiple capability needs. by virtue of its use of advanced capabilities, the research community oraffinity group has mobilized to coordinate requirements and to cooperate in their solution.interagency, cooperative activities in the mid1980s, prior to the start of the high performance computingand communications initiative (hpcci), included studies to examine the need for common infrastructure. thesestudies resulted in the development of the national research and education network component of thehpcci,10 which proposed, and subsequently implemented, internet technologies to support the internetworkingneeds of the research programs of the federal agencies. in one notable case, the energy sciences network(esnet) extended these common affinity group requirements to include other administrative requirements byimplementing a multiprotocol network, installing gateways for multiple email systems, and so on. and althoughtechnically different, the esnet, the national aeronautics and space administration's science internet, thensfnet, and the arpanet all were able to interoperate within an acceptable range for the researchcommunity as separate, but integral, modules in a network of networks. these activities and studies drew uponthe expertise and the involvement of many academic and industrial researchers as well as organizations externalto the federal agencies, constituting a large affinity group and adding a large user base to the internet andestablishing its technologies as a viable and productive technology paradigm.this success notwithstanding, the research community of each agency also needs to interact electronicallywith other parts of its own agency (e.g., using esnet solutions for administration and information resourcemanagement), which are not normally a part of the research affinity group, as well as with its affinity groupgovernment services information infrastructure management14the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.colleagues. these requirements of the research community did not always converge within the group (althoughconvergence was generally discussed and studied); however, convergence (and ultimately interoperability) waseven more problematic outside of the research community.affinity groups do, however, represent a very powerful method for identifying common requirements,coordinating infrastructure needs, and promoting and maximizing interoperability of applications and servicesacross agency boundaries, extending these ''functional" areas as virtual agencies. affinity groups can also extendto industry and the private sector. affinity groups could establish a common perspective for evaluating newtechnologies, eliminating unwarranted redundancies, interacting with various affinity subgroups or workinggroups, and sharing data, information, and knowledge about their enterprise or business area and how itpromotes effectiveness. by focusing on common requirements and solutions in this manner, affinity groupactivities can result in applicationdriven standardization for supporting important common functions, for settingpriorities for new tasks, and for understanding the minimum capabilities needed to perform common businessfunctions. they can also enhance the overall coordination for multiorganizational, distributed enterprises, as wellas other attributes needed to maximize coordination for multiagency, distributed government services (i.e., thevirtual agency) in the information future through the gsii.optimizing gsii and nii compatibilityfederal it management reform is needed to correct the current problems with regard to managing it in thefederal government to achieve a management scheme that works better. a new management scheme should setup mechanisms to aid agencies in carrying out their responsibilities, to evaluate agency it investments viaperformance measures of programmatic results and products, and to promote both compatibility andinteroperability across recognized affinity groups. we propose the establishment of a highlevel "leadershipcouncil" that brings together recognized leaders of larger or more critical missiondriven affinity groups alongwith government and private sector it services providers to:1. promote cooperation among agencies by empowering lead agencies to conduct more multiagencyprocurements, by coordinating across affinity groups, and by seeking opportunities for consolidation andcooperation, where appropriate;2. set strategic direction and priorities for common infrastructure services and to identify lead or executiveagencies, when appropriate, for procuring or providing common services;3. oversee a governmentwide it innovation fund;4. evaluate the work of agency activities through "independent" or external technology review panels orcommittees; and5. make policy recommendations to omb to improve overall gsii effectiveness and to enhancecoordination, such as changes to the federal advisory committee act to increase private sectorinvolvement in gsii planning and decision making.it should be noted that administrative functions for personnel, finance, procurements, facilities management,and so on logically combine the traditional federal information resource management (irm) organizationstogether into an affinity group. one could conclude that this grouping results in a federal government affinitygroup based on electronic commerce. individual agencies would still require policy and oversight function forirm or it management activities; however, it is not envisioned that multiple large centralized irmorganizations would or could promote gsii interoperability goals (e.g., large redundant organizations both in thegsa and in the agencies) or adequately serve the gsii goals and objectives well into the information future.government services information infrastructure management15the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.gsii planpromoting interoperability, coordination, and information exchange across agencies and affinity groups isof paramount importance for achieving the goals of the nii and for creating an evolving and successful gsii.the best means of achieving this goal is through the use of the very same it that will make the gsii successfulšin other words, use of the technologies that we advocate for all agencies' businesses to further enhance themanagement and oversight of the gsii. as an example, a very important it for furthering this goal is the worldwide web (www), which is a product of both the federal and international highenergy physics community andalso of the highperformance computing and communications research communities.the gits working group has endorsed the use of the www to create and maintain a "living" and"evolving" document accessible to all over the net.11 the gits working group has the responsibility forimplementing the npr it recommendations, one of which is to develop a gsii plan. understanding thechanging nature and the wide variety of gsii components, elements, and layers, the gits decided to create acollaborative, online document on the www.12 the document consists of summarized reports of variousaffinity groups, agencies, panels, committees, and so on presenting the most current thinking with regard to gsiitechnology direction, issues, applications and management. it allows for interactive feedback and comment. allcontributions will be referred to a gits subgroup and/or be addressed by other expert groups.online implementation plans, updates, and dialogue for the gsii, as well as its committees, activities,documents, and plans, will help to promote common understanding of issues and their status as well as toestablish the foundation for the discussion of new ideas and/or requirements by government, industry, and thepublic on both a national and international basis.federal versus nonfederal issues for the gsiisome issues that need to be resolved for the deployment of the nii result from the differences betweenpolicies, regulations, and practices of the federal government versus those in the private sector. it is timely thatthe congress and the administration are now committed to telecommunications reform, as this will help lay thefoundation for resolving some of the gsii versus nii issues in the future.key issues that need to be addressed for successful gsii and nii integration include procurement reform,key and certificate management infrastructure, electronic signature, intellectual property, common carrier statusand open access for the network and information service providers, standards setting, and cost recovery forshared infrastructures.recommendationsfirst, federal it management reform is needed to deal with third wave13 (i.e., truly information age)organizations and business so that the federal government can, in fact, achieve its npr goal of creating agovernment that works better and costs less.14second, the necessary features and mechanisms for achieving a successful gsii should, at a minimum,include:1. federal agency flexibility to meet mission requirements in a costeffective manner;2. accountability based on technical success leading to programmatic outcomes and enforced through thebudget process;3. support for easytouse mechanisms for interagency sharing of services and operations, includingfranchising, crossservicing, and multiple agency contracts;4. provision of services/infrastructure as required by communities of interest (e.g., affinity groups forgovernment business areas), by agencies, and by their customers;government services information infrastructure management16the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.5. standards selected by affinity groups with technical assistance and possible facilitation by the leadagency or by the gits working group;6. an interagency fund for innovative it programs;7. a ready source of expert, objective advice for complex system acquisitions;8. central collection of information, as needed, to support governmentwide priority setting, informationsharing, crossagency and crossaffinitygroup coordination, and infrastructure investment optimization;and9. a fast, equitable way of handling contract disputes.third, improvement in government and private sector cooperation (e.g., via extended affinity groups) isrequired to achieve a more timely acquisitions process; a more responsive standards process; openaccessinteroperability between nii and gsii services providers; and revision of the federal advisory committee act(faca) to provide relief for some activities with regard to the gsii and nii to encourage rather than todiscourage such cooperation. the faca can often discourage government activities from utilizing academia,industry, and other private sector organizations in developing priorities and goals for designing andimplementing the gsii and ensuring that the gsii is an integral component of the nii.lastly, continued dialogue on the direction of development and deployment of the gsiišespecially relativeto its superset, the niišvia the www implementation of the gsii plan, is needed to ensure convergence ofthese two very important national resources and to achieve the optimum "range of interoperability" and themaximum benefit that one could expect from such a complex and diverse infrastructure.notes1. toffler, alvin. 1980. the third wave. william morrow & company, new york, pp. 233, 262, and 404œ415.2. information infrastructure task force. 1993. the national information infrastructure: agenda for action. information infrastructure taskforce, washington, d.c., september 15.3. office of the vice president. from red tape to results: creating a government that works better and costs less. u.s. governmentprinting office, washington, d.c., september.4. committee on applications and technology (cat), information infrastructure task force committee. 1994a. putting the informationinfrastructure to work. information infrastructure task force, u.s. department of commerce, washington, d.c. also, committee onapplications and technology (cat), information infrastructure task force. 1994b. the information infrastructure: reaching society'sgoals. u.s. government printing office, washington, d.c., september.5. computer science and telecommunications board, national research council. 1994. realizing the information future: the internet andbeyond. national academy press, washington, d.c., may.6. aiken, r.j., and j.s. cavallini. 1994. "standards: too much of a good thing?," connexionsšthe interoperability report 8(8) and acmstandardview 2(2).7. u.s. department of energy. 1993. positioning the electric utility to build information infrastructure. doe/er0638, department ofenergy, washington, d.c., november.8. acquisition working group. 1994. analysis of postfts2000 acquisition alternatives. interagency management council, washington,d.c., september.9. federal internetworking requirements panel. 1994. report of the federal internetworking requirements panel. national institute ofstandards and technology, washington, d.c., may 31.10. federal coordinating council for science, engineering, and technology, office of science and technology policy. 1991. grandchallenges: high performance computing and communications, the fy 92 u.s. research and development program. committee onphysical, mathematical, and engineering sciences, office of science and technology policy, washington, d.c., february 5.11. gits working group. 1994. vision for government information technology services and the nii. gits working group, washington,d.c., july.12. see http://www.er.doe.gov/production/osc/gsiiplan.13. toffler, the third wave, 1980.14. gore, albert. 1993. creating a government that works better & costs less: reengineering through information technology. u.s.government printing office, washington, d.c., september 1993.government services information infrastructure management17the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.3cutting the gordian knot: providing the american publicwith advanced universal access in a fully competitivemarketplace at the lowest possible costallan j. arlowtelecommunications consultant, annapolis, md.backgroundthe advanced universal access dilemmaa common vision is shared among both the business community and the federal political establishment asto what the broad characteristics of the national information infrastructure (nii) should be. both the democraticadministration1 and the republican congress2 share the view with the private sector that the nii must be builtby private business in a competitive marketplace, rather than directed and regulated by government. moreover, itshould be a "network of networks," consisting of interconnected but competing systems, not just constructed butalso operated in accordance with the competitive market model.yet neither the congress nor the administration is content with laissezfaire, purely marketdrivendeployment. a sense of national urgency, not only to ensure our ability to compete with other nations in theglobal economy, but also to help address major domestic concerns, has prompted other, not easily reconcilablegoals. the first is to have the advanced infrastructure deployed as widely and rapidly as possible, with particularemphasis on early access by education and health care providers and users.3 there is also the second, broadersocial concern about preventing the potential disparity between "information haves" and "information havenots"for critical advanced access to the nii at reasonable and affordable rates. the current senate proposal is toprovide subsidies to designated carriers of last resortša continuation of the existing narrowband mechanismsand an extension of those mechanisms into the realm of new and as yet unknown services.4this public vision of the nii is not just internally inconsistent. it relies on a foreknowledge of the eventsthat will define future technology deployment. yet the actual composition of the anticipated advanced physicalarchitecture and technology that will link schools, hospitals, government, businesses, and residences and willultimately constitute the nii is unknown today. although it is a given that advances in technology should benefitcertain broad areas of our society, such as education and health care, there is no consensus as to what specificapplications will be most valuable to the economy and the social fabric of the country.the magnitude of current basic service subsidies and proposed broadband servicesubsidies under current operating arrangementsthe dimensions of the problem are well documented. the nationwide local exchange carrier (lec)collection of revenues from customers and interconnecting carriers in 1992 exceeded $91.5 billion. subsidy levelestimates from various sources range from $4 billion to $20 billion.5 in a study of data from tier 1 lecs whichdiscussed only the averaging effects within individual lecs (excluding the complex subsidy flows among largeand small lecs and interexchange carriers), costs for providing service to rural customers exceeded ruralrevenues by 35 percentš$5 billionšor more than $19.00 per line per month.6cutting the gordian knot: providing the american public with advanced universal access in afully competitive marketplace at the lowest possible cost18the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.a national policy to create a fully capable broadband network universally available in rural areas andprovided by regulated carriers using traditional costing methods would require far greater subsidies: total loopand nonloop costs per line in rural areas would range from $154 to $168 per month per line, depending uponwhether deployment completion was targeted for a 10 or 20year period. since rural telephony per line revenuesaverage approximately $54 per month, customer revenues or subsidies would have to bridge a $100 per monthgap. these estimates exclude the incremental cost of any customer premises equipment that would be necessary.7the regional bell companies and several of the largest cable multiple system operators are seeking or haveentered into agreements with settop box manufacturers, such as digital and scientificatlanta, to developproprietary consumer premises equipment with an eye toward bringing the cost down to well below $1,000 perunit.8 even if the nation chose to afford it, providing subsidies under a businessasusual model wouldeffectively prevent competitive entry and keep costs high. since there is currently no twoway universalbroadband network in place, there is no rationale for selecting and subsidizing one potential provider over another.this paper does not propose to chart a hybrid middle ground between regulation and competition. it alsoneither evaluates existing and proposed narrowband transition plans nor offers any new transition mechanisms toend the current subsidy scheme for those universally available services. what this paper offers instead is amechanism to provide universal twoway broadband service to highcost areas in an industry and technologyneutral way, through the use of a fully competitive marketplace, and at the lowest possible cost.the need for a separate broadband paradigmas noted above, the current senate bill, s. 652, calls for an evolving definition of universal service. thelaw's effect would be to pull new broadband services into the subsidized and regulated carrieroflastresortmandatory provisioning scheme as such technology gained popular acceptance.9 as has already been discoveredin narrowband communications markets supplied by multiple competing providers, the efficiencies ofcompetition cannot be sustained if one or more parties' market behavior and obligations are being mandated bygovernment. in an environment where there is a "network of networks" that both interconnect and compete witheach other, intraindustry subsidies undermine and distort the marketplace while providing no direct end userbenefit.although complaints about pricing and customer service led to the reregulation of cable television, cablecompanies are not monopoly suppliers of broadband services. a residential customer in search of broadbandservices has the technical capability to receive broadcast television, cable, multichannel multipoint distributionservice, lowpower satellite, and direct broadcast satellite signals. portions of the entertainment and informationthat are received via such media are also available from newspapers, broadcast radio, online computer services,video cassettes, laser disks, computer diskettes, and cdroms. the first principles of a new broadbandparadigm rest on a free market foundation. although robust narrowband local exchange competition willeventually arrive and enable the deregulation of that market, advanced infrastructure investment must go forwardnow in a "clean state" environment, in which government dictates neither the financial nor technical terms for theoffering of new services. a distinct broadband deploymentfriendly and regulationfree environment, possessingthe following characteristics, is necessary: lec broadband investments that are neither subsidized nor regulated. it appears to be a political certaintythat some price and service regulation of local exchange service will continue far into the future either inrate of return or price cap form. given this fact, shareholders, not telephone ratepayers, should take the risksand rewards of broadband deployment. whether in the form of video dialtone or more vertically integratedarrangements, broadband assets and expenses should be unregulated, belowtheline allocations or shouldreside in separate subsidiaries or affiliates. unregulated cable system broadband and telephony offerings. mandatory interconnection. all providers of telecommunications services, whether or not they have beenpreviously regulated, should be required to interconnect with each other,10 but there should be no other daytoday regulatory oversight or standards to which providers are obliged to adhere.cutting the gordian knot: providing the american public with advanced universal access in afully competitive marketplace at the lowest possible cost19the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.a great deal of capital is currently being invested with the hope of creating the technologies, products, andservices that will ultimately prove victorious in the marketplace and constitute the nii. any participation bygovernment for the purpose of ensuring universal access to those advanced broadband services by the generalpublic must, therefore, be technology and service neutral and must have the smallest possible impact on themarketplace.11the development of the competitive market for broadband servicestelephone, cable, computer, software, and telecommunications equipment vendors have made massivecommitments with each other to upgrade their products and services for a twoway broadband communicationsworld.12 at the same time, legal barriers have been struck down in several federal court cases,13 and many statelegislatures have either granted or are considering granting basic service price regulation instead of rateofreturnregulation, in return for lower prices, interconnection with competitors, and advanced infrastructure investments.14as with other industries, it will be the combined effect of individual companies' strategies, and the public'sreaction to the products and services offered pursuant to those strategies, that will determine how the total marketfor access to advanced twoway broadband services will actually develop.15 obviously, in an unregulatedmarketplace, deployment will take place at differing speeds throughout the country. different suppliers, onlysome of whom have ever been under federal communications commission (fcc) jurisdiction, may besupplying access to advanced services. many businesses in major markets today already have multiple suppliersof twoway, highspeed narrowband access. however, there is one certainty: variations in public acceptance ofparticular means of access will cause higher or lower rates of market penetration and differing market sharesamong competing access suppliers; winners and losers will emerge, and along with them, certain technologieswill become more ubiquitous and others will fall by the wayside.it will be the marketplace winners in this initial deployment phase that will help define, de facto, thestandard for which it means to have advanced access to the national information infrastructure and thus become"advanced access interfaces" (aais). the role of government in this environment should be to augment theforces of the marketplace, rather than interfere with them, so as to bring about the availability of advanced accesson a universal basis. in order for such access to be universal, it will require legislation enabling the fcc, inpartnership with the states, via a joint board, to administer a plan that provides the incentives for universaldeployment at reasonable rates within a competitive market environment. the mechanisms of that plan, theiroperation, and their impact are described below.advanced universal access deployment planphase 1: fccstate joint board "advanced access interface (aai) recognitionrulemaking proceeding"the marketplace success of various types of advanced network access interfaces will, as noted above, createa de facto definition of what it means to be an "information have" in american society. this definition wouldbecome de jure when penetration of potential aais reached a specific level, either set by congress or delegatedto the fcc or the joint board. the plan's purpose is to prevent the creation of a longterm "have not" communityof sparsely populated or untenably highcost service areas, while preserving the operation of the competitivemarketplace.pending legislation uses an imprecise standard for the circumstances under which subsidies should beprovided for new services.16 a specific, easily recognized benchmark is far more desirable, because investmentis augmented by reducing the risk that comes from regulatory uncertainty. the benchmark could be, for example,that 80 percent of institutions and businesses and 50 percent of residences in the top 25 metropolitan serviceareas have advanced access interfaces that have, as a minimum, the technical capability sufficient tosimultaneously receive five channels and send one channel of national television system committee qualityvideo (assuming thatcutting the gordian knot: providing the american public with advanced universal access in afully competitive marketplace at the lowest possible cost20the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.this reflects the abilities of the most popular residential interfaces). it is important that access be viewed in termsof capabilities and not technologies, in order to ensure that any subsidies that assist deployment are technologyneutral. for example, requiring a dedicated residential 440mhz downlink and 6mhz uplink is not meaningfulif advanced access can also be in the form of a settop box that couples digital cellular radio linked to massiveregional servers with a halfmeter dish of a coventuring direct broadcast satellite system. what is mostimportant is that a particular advanced access interface has succeeded in the market.once this plateau has been reachedši.e., when the level of market penetration has become sufficient to findthat a core group of "information haves" has evolvedšthe implementation of the plan can begin. the first phaseof the plan is an "advanced access interface recognition rulemaking proceeding." in such a proceeding, theaais with a defined minimum market share whose specifications have by this time been well documented, willbe described with particularity and declared by the fccstate joint board to be critical to the national interest.companies, standards groups, associations, and others may submit their aais for recognition, because it will bethese interfaces, and only these, whose deployment will be eligible for subsidies.17in order to ensure the availability of access throughout the country via equipment that individual users maypurchase, all of the accepted interfaces for which subsidies will be made available must, in return, be open andnonproprietary. any proprietary rights in the aai identified by the rulemaking proceeding become subject tostrict public policy limitations; in that way, no provider of technology can hold hostage to its licensing fee orproduct purchase demands those customers for whom a highly competitive marketplace is unavailable.18phase 2: joint board "inverse auction" proceedingonce the aais have been officially recognized, the joint board undertakes to implement universal accessthrough a proceeding with the following elements: by public notice the board divides the country into geographic market areas, much as the fcc has done forcellular and personal communication service (pcs) licensing, in which parties may compete for the right tobe the subsidy recipient. if not previously set forth by enabling statute, national market penetration targets are adopted; for example,aai nii access available to 100 percent of institutions by 2005 and to 90 percent of businesses and 80percent of residences by 2015. if not previously set forth by the enabling statute, the proceeding would specify the service standards andadministrative procedures that an applicant would be obligated to meet. in order to ensure the affordability of the service, the proceeding would also specify the "top rate" at whichaai nii access could be charged.19 rules for parties wishing to participate in the inverse auction would allow any party to apply for as many oras few markets as it wishes upon showing proof of general qualifications. a bid would consist of a response to the following question: what is the smallest dollar amount (the"performance payment") the bidder will accept to shoulder the supplieroflastresort responsibility forbuilding the infrastructure to meet the aai national market penetration target for the market in question? the party submitting the lowest bid in each market would be designated the universal access supplier(uas) for that market. the uas would have many of the familiar service standards requirements nowimposed upon common carriers (readiness to serve, time from request to time of installation, operatorservices, blocking rates, e911, and so on), but they would be voluntarily assumed a part of the biddingprocess.20as was widely noted at the recent auction of pcs spectrum, it is highly unlikely that strategy and gametheory will play a major role in the bidding process. some of these strategies may lead to later problems if notrecognized early on. for example, an applicant who already has a significant market presence and believes thateconomies of scale are critical to longterm profitability may "lowball" a bid to discourage potential competitorsfrom entering the market altogether. an applicant who has historically had protection from competition may alsobe tempted to lowball if it believes that it can bluff the government into providing a bailout or relaxing orcutting the gordian knot: providing the american public with advanced universal access in afully competitive marketplace at the lowest possible cost21the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.changing its rules with respect to crosssubsidies, pricing, or other requirements, as an unmet deadlineapproaches. as a consequence, it will be necessary to have penalties for failure to perform and, perhaps,performance bonds to protect the public from parties who underbid recklessly and then seek to abandon marketobligations.21the postaward competitive marketplacethe award of uas status would have little immediate impact on the marketplace. unlike radio licenses orexclusive service territories, no scarce franchises are granted. all parties, uas and nonuas alike, would still befree to compete in deploying the infrastructure, selling to any customer on a competitive basis. only the uaswould be required, upon request, to provide access to the requesting customer at no more than the top ratešanumbrella price for both its own and competitors' comparable access offeringsšand annually report on itsprogress toward meeting the deployment target date. neither federal nor state agencies would oversee the daytoday construction or offering of aais, either by the uas or other suppliers, until the uas filed to receive itsperformance payment. this competitive marketplace coupled with the uas performance payment incentiveswould encourage the most rapid deployment and meet the national deployment schedule targets.although the competitive marketplace debate has centered on visions of "last mile" competition amonglocal exchange carriers, cable systems, and interexchange carriers, advances in technology, auctioning ofspectrum, and the lessening of regulation will open the door to many potential participants, especially consortiausing combinations of diverse technologies. among possible providers: direct broadcast satellite systems with broadband downstream capabilities may find synergies with cellularcompanies that will have tremendous excesses of capacity in rural markets. using digital radio technologies,the cellular carriers may not only be able to handle full narrowband transmission demands but also will havesufficient reserve bandwidth to loadmanage twoway broadband transmissions from business andresidential customers as well. uhf (and even vhf) television spectrum is largely vacant or underutilized in most rural areas and issufficient to enable an entrepreneur to provide lowcost broadband digital access over wide spaces of sparsepopulation and, with lowpower cellular technology, over more densely inhabited areas. electric utilities, the "third wire," will be able to participate in the market much as telephone and cablecompanies; since electricity is likely to remain largely a regulated monopoly service, accounting or separatesubsidiary requirements will keep the unregulated, competitive services free of improper crosssubsidy.distribution of the performance payment and market growth beyond the initialdeployment periodat the end of the deployment target period (or prior thereto if the uas has met the goal ahead of schedule),the uas would petition for its performance payment under a "uas completion proceeding." the distributionmechanism could be administered at either the state or federal level: at the state level, the public service commission (psc) or other authorized body would review the uas'saffidavits of satisfaction of the access requirement and dispense the performance payment accordingly. if a federal proceeding is the policy route of choice, the fcc could review the uas's affidavits ofsatisfaction of the access requirement. the psc of the state in which the uas market is located would bethe designated intervenor in the performance payment approval process.22in either case, the subsidy would be simple to administer and should provide incentives for the desiredbehavior. drawing rights could be made available to each state psc or to the fcc by the treasury for theamount of subsidies maturing in markets within their jurisdiction during each year. since the state psc or thecutting the gordian knot: providing the american public with advanced universal access in afully competitive marketplace at the lowest possible cost22the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.fcc could withhold certification and thereby delay the performance payment, the uas would have an incentiveto make the strongest case and be responsive to customer complaints and government inquiries. however, inorder to be sure that the subsidies are directed to the purposes for which they were originally bid, neither thestate psc nor the fcc should be permitted to use the proceeding as an opportunity to negotiate quid pro quoforgiveness of missed targets in return for lower prices or regulatory oversight of operations, and so on.the resulting high market penetration of advanced access would probably also reflect a competitivemarketplace. ideally, it should render unnecessary both continuing subsidy of remaining and growthinfrastructure investment and subsidies for reasonably priced access. nevertheless, although these mechanismswill have enabled the initial deployment of advanced access, there may be some markets where there is acontinuing need to assure the availability of service out into the future or to allow for circumstances wherecompetition may not be adequate to ensure affordable rates. if such circumstances were to occur, the state psccould petition the fcc to provide for subsequent rounds of uas bidding for a designated market in time blocks,i.e., bidding to be the provider of aai access at no more than the top rate to all requesting parties on a lastresort basis for a designated number of years.funding the performance paymentthere is already an abundance of proposals on how and from whom subsidies for narrowband universalservice should be collected.23 for those required to participate in providing the subsidy, the issue is not only theparticipation itself, but also the annual amount and openended duration of the subsidy. this plan addresses thoseissues and limits their ability to disrupt both the marketplace and the participants' efforts at business planning.the first principle with respect to imposing any subsidy should be to spread the burden as broadly and fairlyas possible across market participants. this is especially true in this instance because there will be a variety ofcommunications technologies and varying levels of distribution of computing power (i.e., some systems ofaccess may rely on large regional servers connecting to comparatively simple consumer premises equipment,while others may have settop boxes whose capabilities rival those of today's workstations). excise taxes on thevalue of shipments of computers and peripherals, telecommunications equipment, and satellite ground andmobile equipment, along with taxes on lec, cellular, ixc, and catv service revenues, will provide that base.ordinarily, subsidies distort markets because transfer payments flow from the pockets of companiesdesignated efficient and profitable into the pockets of the designated inefficient and needy. this plan neitherdesignates nor offers preferences to any potential provider.24 consequently, these proposed subsidies would flowonly to parties providing access via interfaces that have already proven themselves successful in the marketplace.as a result, the distortion of transfer payments is minimized; the funds will flow back to the same industryparticipants who contributed to the fund and, potentially, in roughly similar proportion, preserving marketefficiency as well as equity.the second principle with respect to imposing a support burden is that it be predictably measured and finite.under this plan, the national liability and maturity date for each and every market will be known well in advanceof the time that any payment comes due, because the bidding and selection will occur years before theperformance payment will be distributed. the process can be further finetuned by holding auctions on astaggered basis, using an accurate cross section of markets rather than the largest markets first. such a procedurewould enable the establishment of an approximate market price for each type of service area as the process wentforward. the industry tax could then be set in a manner that would ensure proper levels of funding.political considerationsthe collection of new taxes is always politically unpopular, but the fact that these funds would be collectedfrom a discreet group and segregated and ultimately paid out to members of that same class of taxpayers, whilesimultaneously benefiting the general public in every congressional district, should alleviate much of the concernabout government involvement. since there is no daytoday administration of a highcostcutting the gordian knot: providing the american public with advanced universal access in afully competitive marketplace at the lowest possible cost23the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.service fund or other regulatory oversight of the potential recipients, there is no need for an independent entity tohandle the funds in order to avoid a debate on the application of federalist principles. opposition would alsolikely come from independent telephone companies, who have been both recipients of subsidies and protectedfrom competition within their own service territories in the provision of narrowband services. yet they will bepositioned better than almost any other competitor to take on the role of uas and will likely be the low biddersin their local markets; they are also likely to form partnership or consortia with other independent companies andbid successfully for the regional market.although this plan envisions a major role for state agencies, the overall operation of the proposal willpreempt state jurisdiction over the daytoday local activities, such as pricing, service standards, and so on,traditionally regulated at the state level. there is already, however, a recognition within many states, as notedabove, that new services will be offered in a competitive, marketdriven environment, where regulation does notneed to be substituted for competition. with respect to the uas itself, the state will be able, at its option, toparticipate in or control key aspects of the approval process. finally, it should be noted that the plan, which willenable more rapid deployment of infrastructure, will by its operation not only improve the quality of life withinthe jurisdiction, but also increase commerce for both cpe and ancillary services, thereby increasing local andstate sales and income tax receipts.conclusionthe ideal of relying solely on unfettered competition among multiple suppliers of broadband access to reacha goal of affordable, universally available advanced services carries with risk of failure that policymakers withinthe united states are unwilling to take. at the same time there is a realization that neither regulation normanaged competition are viable longterm options. the proposals set forth in this paper offer one possibleavenue to achieve advanced universal access in a fully competitive environment with the least economic andsocial costs and minimal regulation.notes1. information infrastructure task force (iitf). 1993. the national information infrastructure: agenda for action. iitf, u.s. department ofcommerce, washington, d.c., september 15.2. s. 652, telecommunications competition and deregulation act of 1995, 104th congress, 1st session, report no. 10423, sec. 4œ5, p. 3.3. ibid. at sec. 103(d), pp. 304œ310.4. ibid.5. weinhaus, carol, and the telecommunications industries analysis project work group (weinhaus et al.). 1994. getting from here tothere: transitions for restructuring subsidies, p. 16.6. weinhaus et al. 1994. redefining universal service: the cost of mandating the deployment of new technology in rural areas, pp. 8œ9.7. ibid. at p. 12.8. telecommunications reports. 1995. "ameritech to deploy scientificatlanta technology," 61(8), february 27, p. 7; and "rfp aims tolower settop box manufacturing costs," 61(9), march 6, p. 27.9. s. 652, report no. 10423, sec. 103(d), pp. 39œ40.10. this is arguably not new regulation. indeed, if there were no prior history of telecommunications regulation, it might well be argued thatnew legislation to ensure such interconnection would not be necessary. application of sec. 3 of the clayton act might reasonably beinterpreted to prevent telecommunications service suppliers from denying other service providers access to its customers. see datagate v.hewlettpackard, 914 f.2d 864 (9th cir. 1991); xeta inc. v. atex inc., 852 f.2d 1280 (d.c. cir. 1988); and digidyne corp. v. datageneral corp., 734 f.2d 1336 (9th cir. 1984).11. this paper does not discuss programs to fund or provide broadband services to those individuals or institutions determined to be in needof special subsidies. rather, it is limited to the issue of how best to make access to advanced broadband services universally available to thegeneral public. nevertheless, certain principles that further the overall goal of advanced universal access do apply: in a competitively neutralmarketplace, public support, whether targeted recipients are poor individuals or distressed education, health, and public institutions, mustprovide the recipientscutting the gordian knot: providing the american public with advanced universal access in afully competitive marketplace at the lowest possible cost24the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.with the flexibility to obtain services from competing vendors rather than require lastresort carriers to offer services at noncompensatoryrates. armed with the ability to select services from among several vendors, the needy will make choices that will reflect the efficiencies ofthe marketplace.12. a recent example is pactel's $150 million headend and settop box purchase agreement as part of an interactive video network. see wallstreet journal. 1994. ''scientificatlanta inc.šconcern is awarded contract to equip pactel's network," april 24, p. b4. yet even thisappears modest by current standards; cf. ameritech's $700 million in contract commitments to scientificatlanta, note 8, supra. a great dealof publicity has surrounded announcements by regional bell operating companies on the formation of programming consortia (bell atlanta,pactel, and nynex; ameritech, bellsouth, sbc communications, and walt disney), as well as speculation with regard to us west'sfuture intentions regarding its investment in time warner.13. c&p telephone co. of virginia et al. v. u.s. et al., 42 f.3d 181 (4th cir. 1994); nynex v. u.s., 1994 wl 779761 (me.); us west v.u.s., 48f.3d 1092 (9th cir. 1994).14. states where bills were introduced in 1995, and which currently appear to be under active consideration, include colorado (hb 1335),texas (hb 2128), and tennessee (rival bills sb 891 and 827, hb 695 and 721). the wyoming telecommunications act of 1995 (hb 176)was signed into law in march, and georgia's telecommunications and competition development act of 1995 (sb 137) became law in april.in north carolina, as of this writing, hb 161 awaits the governor's signature.15. telecommunications reports. 1995. "competitors and allies: cable tv companies, telcos, broadcasters jointly will create newmarkets," 61(14), april 10, pp. 16œ20.16. s. 652, report no. 10423, sec. 103(d), p. 40.17. first in the market, especially in new technology, does not guarantee dominance, or even success (e.g., 8track cassettes, commodorecomputers, atari and magnavox video game systems, bowmar calculators) even if the technology is superior (betamax, digital audiotape).commercial success is a reflection of market acceptance and manufacturing efficiency and yields a reduction in incremental risk per unitmanufactured. if the public is required to subsidize the initial build out of longterm infrastructure, there must be a high level of confidencethat the access interfaces installed under subsidy will have a long useful life and that individuals who purchase equipment and later relocateor need to access nonlocal information sources will not find their investments rendered worthless.18. any proprietor is free not to participate in the proceeding so that there is no state taking of intellectual property.19. it would be reasonable to expect that the rate would be in the top quartile of rates charged nationally, i.e., still affordable by definition,but not a windfall rate when compared to those paid by the majority of customers nationwide.20. in calculating its bid, the applicant would, of course, factor in such items as the costs of meeting the service standards obligations, thetop rate, current and projected levels of competitive deployment in the market, demographics, terrain, and so on. the development ofcompetitive markets will provide vast historical data on costs, price elasticity of demand, and the like. obviously, within that pricedemandelasticity curve, the higher the top rate, the lower the performance payment necessary to attract the successful bidder.21. the uas could be required to post a performance bond at least 5 years prior to the target date. if it advises the fcc that it is abandoningits uas status, qualified but previously unsuccessful bidders in the market may then rebid for the right to be the uas. the treasury wouldcollect on the bond to the extent that the second bid series produced a result that was higher than the original uas's bid plus a penalty andadministrative premium.22. in multistate markets, pscs would designate members to sit on a panel that would serve as the sole intervenor.23. see weinhaus et al., supra, getting from here to there: transitions for restructuring subsidies, 1994, for a review of the mostcommonly discussed alternatives. although based on recent prices paid for pcs spectrum, an auction of unused uhf and vhf spectrumcould well provide all of the funds necessary to finance advanced universal access; for the purposes of this paper, it is assumed that the fundsmust be raised from market participants.24. this is particularly important if, by operation of the program, a uas receives what could be considered a windfall profit: if the marketdevelops more rapidly and efficiently than anticipated and the performance targets are met with little or no investment that is less than fullycompensatory, the uas would receive the performance payment merely for having been "on call."cutting the gordian knot: providing the american public with advanced universal access in afully competitive marketplace at the lowest possible cost25the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.4the role of cable television in the niiwendell bailey, national cable television associationjim chiddix, time warner cablethere are over 11,000 cable television systems in the united states. they, along with the equipmentvendors that have traditionally worked with them, represent a springboard to an extremely high capacity, nearlyubiquitous broadband twoway information infrastructure serving both the business and residential communities.this existing resource passes over 97 percent of the homes and businesses in the united states and is connectedto nearly 65 percent of the residences. the cable industry has made significant improvements in the quality andreliability of its networks over the past few years and has begun to put in place the structures needed to transformits systems into highly flexible multiple service vehicles. yet, though we have continually expressed awillingness to make the investment necessary to bring about this transformation of our facilities, there aresignificant impediments to that goal. the primary barrier to the realization of our full participation in thisnational information infrastructure is posed by excessive regulation of the cable industry. this federally imposedburden, which undermines our financial capabilities, is coupled with other levels of regulation and barriers to theprovision of communication services posed by state regulators and local franchise authorities. when theserestrictions are linked with such problems as denial of switch interconnection access and usurious poleattachments and duct rental contract provisions by potentially competitive local exchange carriers, it becomesclear that the most capable and flexible network available today can contribute its full resources to theachievement of a national information infrastructure/global information infrastructure (nii/gii) only if thegovernment promulgates a rational set of regulations that make it possible for full and complete competition toflourish.the basis for today's cable television networks is coaxial cable, a radio transmission medium capable oftransporting a large number of separate radio carriers at different frequencies, each modulated with analog ordigital information. it is common practice to use filters to segregate the high and lowfrequency portions of thespectrum to allow simultaneous transmission of information in both directions. this ability to transportcoexisting carriers carrying different kinds of information (essentially separate "networks") provides anenormous amount of flexibility and capacity. today's coaxial systems are, however, arranged architecturally in abroadcast topology, delivering the same spectrum of informationbearing radio carriers (commonly called"channels") to every customer in the community.the advent of fiberoptic transmission technologies optimized for broadband use during the last decadeallows a costeffective upgrade of existing broadcast coaxial networks to a hybrid fiber coax (hfc) architecture,with fiber trunks providing transmission to and from small neighborhoods of a few hundred homes. thisarrangement of fiber and coax segments the traditional coaxialonly transmission plant into many localized areas,each providing a localized assortment of information. when combined with the economics of highspeed digitalswitching technologies, this architecture allows the simultaneous delivery of multichannel televisiontransmissions and switched video, voice, and data services to and from individual homes within these smallserving areas. the upgrade of existing coaxial cable tv networks to a hybrid fiber coax architecture as describedabove costs less than $150 per home passed and can be accomplished over the period of a few years.the current design and economic considerations of a hybrid fiber coax cable network call for an initialpassing of 500 homes (of which about 300 currently subscribe to cable), but it can be further segmented intosmaller and smaller coaxialserving areas. its potential digital capacity is at least 1.5 gigabits per seconddownstream (to the home), and 500 megabits per second upstream. this is in addition to the transmission of 80channels of broadcast national television system committee and highdefinition television signals. networksthe role of cable television in the nii26the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.configured this way can provide for extremely highspeed access to both symmetrical and asymmetrical dataservices, as well as the ability to evolve to even higher speeds and higher usage levels through furthersegmentation of the coaxial "last mile" of the network. further, because analog and digital carriers usingcompletely different modulation schemes and protocols can coexist, an evolutionary path is kept open forcontinuing advances in digital services and technologies, while, at the same time, allowing the pragmaticadoption of technology options to build costeffective systems today.in building a hybrid fiber coax plant, the cable television industry is providing a costeffective, reliable,ubiquitous transmission system that can simultaneously support many separate networks, the sum of whichmakes the necessary upgrade investment a sustainable one for any company in our industry. time warner, forexample, currently has plans to build at least four separate networks on the foundation of its fiberupgradedplant. cable television laboratories, an industry r&d consortium, is working with a group of broadbandequipment manufacturers to develop an open standard that is called the spectrum management application, toallow the coexistence of these multiple networks and the maximization of the efficiency with which the radiofrequency spectrum within the transmission plant is used.in time warner's upgraded plant, for example, there are plans for the coexistence, and separate operation,of networks that will continue the broadcast of scores of analog tv channels and will begin the delivery of highquality digital telephone service, the provision of highspeed personal computer interconnection service, andaccess to a wide range of interactive video services. an additional network designed for interconnecting pcsradio microcell sites may be integrated into that residential network, or may be operated independently.time warner's personal computer interconnection services will incorporate support of the internet protocol(ip), and they are in the process of working with a number of companies that are designing the necessaryhardware and software systems needed to provide a service of this type. many companies in the cable televisionindustry envision initially offering access to a variety of online service providers, as well as email and directconnection to the internet at speeds currently unavailable to almost anyone, anywhere. we do not pretend toknow how the internet will evolve; there are those who claim that it will one day provide video, shoppingservices, and all the rest. it is far from that today and has many shortcomings, as recent security problems havedemonstrated. but regardless of whether pc interconnection ultimately flows through a number of competingnational online services or through the laissezfaire anarchy of the internet, cable intends to offer a highlycompetitive avenue for local residential and business access to any viable service provider.on a separate front, the industry is just beginning to understand interactive television services throughprojects like time warner's full service network in orlando, and while this and other trials are beginning toteach us a few hardwon lessons, it is far too early to set the standards for technologies that are still in their earlyinnovative phase. we are, however incorporating standards wherever we can, particularly for content (mpeg,for example.) there will still be public policy questions to be dealt with in interactive tv, but they should waituntil the technology and business mature at least to the point that such issues can be intelligently debated basedon information and experience that we will gain from the trials that are under way.an example of the harm that government regulation can do is evidenced by the socalled rate regulationsthat the federal communications commission (fcc) enforces on the cable television industry. theseregulations are over 600 pages in length and are so complicated that the commission has issued corrections andclarifications that total several hundred additional pages. while many people think that these rules are just aboutthe subscription fees that operators may charge to a customer, they do not understand that these regulations alsodirectly affect whether or not we can include the cost of a new piece of equipment or a mile of fiberoptic cablein the cost of "doing business" and adjust our rates to recover the expense. in fact, the top fcc official recentlyreplied, in response to a question about "upgrade incentives," that upgrades are just a code word for rate hikes.any premature attempts to set either mandated technological standards or regulations shaping business structurehave the real potential to slow innovation in a field that may have great future value to american business andsociety. such standards may be called for by industries that have a stake in the status quo and wish to limit orconfine the directions that innovation may take, but these calls must be resisted if the country is to benefit fullyfrom the fruits of the process of invention and exploration.the role of cable television in the nii27the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in the meantime, the pc networks that the industry is building will follow a much better understood path,since they will use an existing installed base of home computers and their operating software, and an existingcommunications model as a starting point. although companies such as time warner will be one of manycontent providers, these will be networks that success in a competitive world will demand be kept open, and theywill be as symmetrical as actual usage demands.the cable industry views the competitive provision of wireless and wired telephone service as an immediateopportunity. the benefits of competition to the american public can only be realized, however, if four legislativeand regulatory principles are observed: the elimination of historic state and local barriers to competition in telecommunications; the creation of requirements for interconnection, access, compensation, unbundling, collocation, pole andconduit sharing, and number portability and dialing parity by the incumbent telephone monopoly; the prevention of interference by local authority in the growth of competing telecommunications services; and the recognition that to enhance telephone competition, debilitating cable rate regulation must be reformed.the industry strongly supports the concept of universal service and agrees that each telecommunicationsprovider should carry its fair share of the burden to ensure that universal service remains a reality. however,universal service should not be maintained through subsidies flowing directly to particular telecommunicationsproviders. rather, such subsidies should flow directly to the consumers in need of such support, if the concept oftruly competitive service is to be maintained in a multipleprovider environment.in summary, the existence of a ubiquitous broadband cable television system in this country affords analmost unique opportunity to see the rapid realization of a series of extremely powerful digital networks. somecan offer competition to the existing telecommunications monopolies; some can interconnect computers andcomputerbased interactive television terminals in ways that can lead to an explosion of new and highlyinnovative services. in order to open these benefits to the american public, however, government will have totake an active role in lowering barriers posed by incumbent telephone companies and by state and localgovernments, as well as by current federal cable television regulation. government will also have to exerciserestraint in order to allow innovative and entrepreneurial forces to chart the way into a digital future without theimposition of mandated standards designed to protect a variety of existing interests.the cable television industry has much to offer in helping this nation to realize the potential of the nii. theother network providers are seeking alliances with us and we with them. if the network of the future is to have achance to be what its promoters believe it can be, government needs to provide guidance, not hindrance. if it cando that, we can do the rest.addendum11. for your company or industry, over the next 10 years, please project your best estimate of scheduledconstruction of new broadband facilities to the total residential and small business customer base, in twoyearincrements.over the next 10 years, we project that the cable industry will upgrade its plant to hybrid fiber/coaxarchitecture with relatively small node areas (approximately 500 homes passed on the average) to virtually theentire cabled universe. since small businesses are for the most part mixed with residences, these sameprojections would apply to both bases. currently, there are approximately 97 million residences in the country,95 percent of which are passed by cable plant. (sources: a.c. nielsen and paul kagan.)2. over the same period, please project dates when each family of service will first become available, andits subsequent penetration of the total base, again in twoyear increments.the role of cable television in the nii28the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the four major families of service which we foresee are multichannel broadcast signals; digital telephony;personal computer network services; and switched interactive multimedia services. multichannel video servicesare already available, of course, to all homes currently passed by cable tv plantš96 percent of u.s. households.cable telephony services are beginning to be offered in certain markets in 1995. pc modem services will becomeavailable in 1996 and interactive multimedia services are expected to become commercially viable in 1997.projected penetration of all u.s. households and estimated market share are shown in table 1 (availabilityof these services would track the percentages in #1, from 1999 on):table 1 projected penetration of advanced cable services in u.s. households percentageyearcable telephonypc networkinteractive multimedia199510019972401999564200110810200315121520052018253. please outline the architecture(s) which will be used to build this broadband plant.the architecture which will be used to build this plant will be hybrid fiber/coax architecture, as outlinedearlier in our paper, with fiber nodes located in neighborhood with (on the average) 500 homes passed. based ontoday's cable penetration, this would mean 300 cable subscribers per node, on average.4. please outline the peak switched digital bandwidth (in kbps or mbps) available to an individualresidential or small business user when you launch broadband service, and how that bandwidth can evolve torespond to increased peak traffic and to new, high capacity services (which may not now exist).peak digital bandwidth to and from the customer can be examined in the aggregate or on a servicebyservice basis. in the aggregate, with hybrid fiber/coax architecture to 500 passings, we must make somepenetration assumptions to have a meaningful answer. if we assume that cable maintains its current 60 percentpenetration, this results in 300 customers per node. if we assume that 40 percent of those customers availthemselves of digital services of one kind of another, that results in 120 homes. if we further assume that peakusage of digital services is 33 percent (a conservatively high assumption), then the maximum number ofsimultaneous users being served by a given fiber node at any one time would be 40.in cable systems as currently being upgraded, the spectrum from 50 mhz to 750 mhz is reserved foroutgoing transmissions, but 50 mhz to 550 mhz is assumed to be reserved for broadcast analog services, with200 mhz remaining for digital services. assuming 356 qam digital modulation, with an efficiency of 7 bits/hz,this results in a total outgoing or "downstream" capacity of 1.4 gigabits/sec. this, when divided by 40 users atpeak usage time, yields approximately 35 megabits/sec of outgoing bandwidth available per customer. evengiven the expected volume of interactive video delivery, this number is ample for outgoing transmissions. iteasily accommodates telephony, pc modem, and interactive multimedia applications.incoming bandwidth currently spans the spectrum from 5 to 40 mhz, a total of 35 mhz. because of noiseaddition problems in this portion of the spectrum, modulation is probably limited to qpsk with an efficiency ofabout 2 bits/hz, yielding 70 megabits/second. if this is divided by the 40 peak users to be fed from a fiber node,this yields 1.75 megabits/second available to each user. this is more than sufficient for telecommunications,including video telephony, and is sufficient as well for easily foreseen applications in pc network and interactivemultimedia services.both the downstream bandwidth and upstream bandwidth can be increased in several ways. first, thenumber of customers per fiber node can be reduced on a neighborhoodbyneighborhood basis, based on usage,through the use of spare fibers (which are being installed) to subdivide nodes into smaller neighborhoods. inaddition to spare fibers, wavelength division multiplexing can be used on a single fiber to the same end. it is easyto foresee average node sizes of 125 homes passed per node or less, resulting in at least a fourfold increase in thenumbers cited above.there is also the ability to dramatically increase the return spectrum. time warner's project in orlandosuccessfully makes use of 100 mhz of spectrum, from 900 mhz to 1 ghz, in the return direction. this spectrumhas fewer noise problems than the low frequencies cited above, so higher modulation efficiencies are possible.however, assuming the same kind of qpsk modulation used at the low frequencies, this would yield anadditional 200the role of cable television in the nii29the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.megabits/second of transmission capability, or 5 megabits/second per peak user. again, this number can bemultiplied through segmentation as outlined above.it is possible to push these numbers significantly further. if very high speed, truly symmetrical capacity isrequired, frequencies above 1 ghz can be used. the cutoff frequency of the coaxial cable employed is close to 2ghz, allowing for very significant expansion of capacity for high speed symmetrical services.5. please project the capital investment you or your industry plan to make on a per homepassed basis toinstall broadband infrastructure, and on a per subscriber basis to install specific services.our experience to date indicates that an investment of between $125 and $135 per home passed is requiredto upgrade existing coaxial cable television plan to the hybrid fiber/coax architecture referenced earlier.assuming a 15 percent penetration rate, we expect the incremental costs per customer moving intotelephony to be no more than $1,000 per customer. this investment is largely variable in nature, is madeincrementally as telephony customers are added.it is estimated that pc modem services will cost between $400 and $600 per customer, again, incrementallyagainst only those customers taking the service. this covers the cost of the pc modem, as well as central routers,servers, gateways, and support systems.it is estimated that interactive multimedia servers will cost between $700 and $800 per incrementalsubscriber, again accounting for terminal equipment in the home as well as switches, servers, and associatedcentral investments.6. please respond to the concerns raised in vice president gore's letter (copy of letter attached) regardingthe ability of users of your network to original content for delivery to any or all other users, versus the control ofall content by the network operator.the concerns outlined by vice president gore are largely addressed in our original paper. we expect tosupport several different coexisting networks on our broadband transmission system. these range from regulatedcommon carriertype symmetrical telecommunications services, like telephony, to highly experimentalasymmetrical interactive entertainment services. in the middle ground will be a pc network, with great capacity.this network will be as symmetrical as it needs to be, given marketplace demand. as outlined above, we havethe ability to expand network capacity in pursuit of the amount of symmetry that makes sense. however,premature installation of capacity and symmetry, in advance of demand, will be prohibitively expensive and, webelieve, will not be supported by private investment.7. please specifically enumerate the actions which you or your industry believe that the federal governmentshould take to encourage and accelerate the widespread availability of a competitive digital informationinfrastructure in this country.we specifically address these points in our paper. to reiterate, they are: the elimination of historic state and local barriers to competition in telecommunications; the creation of requirements for interconnection, access, compensation, unbundling, collocation, pole andconduit sharing, and number portability and dialing parity by the incumbent telephony monopoly; the prevention of interference by local authority in the growth of competing telecommunications services; and the recognition that to enhance telephone competition, debilitating cable rate regulation must be reformed.note1. all projections (unless noted) are the estimates of the authors and do not represent an official position of the national cable televisionassociation or time warner cable.the role of cable television in the nii30the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.5competing definitions of "openness" on the giijonathan band1morrison and foerster, washington, d.c.the dozens of privatesector and governmental position papers concerning the emerging global informationinfrastructure (gii) all agree that it must be an "open" system. as the clinton administration recognized in its1993 agenda for action, the information infrastructurewill be of maximum value to users if it is sufficiently "open" and interactive so that users can develop new servicesand applications or exchange information among themselves, without waiting for services to be offered by the firmsthat operate the nii [national information infrastructure].2the position papers similarly agree that the desired openness could be achieved only through standardizedpoints of interconnection (in technical parlance, interface specifications). in the words of the agenda for action:to assure interoperability and openness of the many components of an efficient, highcapacity nii, standards forvoice, video, data, and multimedia services must be developed. those standards also must be compatible with thelarge installed base of communications technologies, and flexible and adaptable enough to meet user needs ataffordable costs.3further, the position papers all agree that governments should not impose the standards; rather, the privatesector should develop them. all concur that standards organizations will have a primary role in establishing thegii standards (de jure standards), but some acknowledge that many of the standards inevitably will be set by themarket (de facto standards).at this juncture, the position papers begin to diverge. the divergence arises over the issue of proprietarycontrol of the standards: how much control, if any, should the inventor of the standard be able to exercise overthe practice of the standard by others? the amount of control that can be exercised over a standard is inverselyrelated to how "open" that standard really is. during the course of 1994, different firms and trade associationsarticulated disparate views on the issue of proprietary control of standards, reflecting their narrow commercialinterests. during 1995, governments and international bodies began to assert positions as well. although allproclaim fealty to the goal of openness, a given entity's definition of openness turns on the extent of proprietarycontrol it would allow over gii standards. this paper examines the different definitions of openness that haveemerged over the past 2 years.u.s. private sector definitions of opennessduring 1994, many u.s. companies and trade associations began to express their opinions on "openness" onthe information infrastructure. these opinions can be classified into four definitions ranging from the restrictivemicrosoft definition to the expansive sun microsystems definition.microsoftthe microsoft position starts from the assumption that hardware interfaces specifications are patentable andthat software interface specifications can receive both copyright and patent protection. microsoft furthercompeting definitions of "openness" on the gii31the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.believes that there is no need to create special rules for the gii; rather, the current laws applying to the softwareindustry (which have served microsoft extremely well) are adequate for the gii environment.microsoft will achieve "openness" by voluntarily licensing its application program interface to all thirdparty application developers. thus, anyone who wishes to develop a product that attaches to a microsoftoperating system will be able to do so. microsoft, however, has not agree to license its interface specifications tofirms seeking to develop operating systems that compete directly with microsoft operating systems.4microsoft's narrow definition of opennessšpermitting attachmentšflows from its business plan. microsofthopes to dominate the market for the operating system of the "settop box"šthe entry point of the informationinfrastructure into individual homes or businesses. by controlling the standard for the settop box operatingsystem, microsoft will be able to exercise control over access to the entire infrastructure. microsoft wants toencourage third party vendors to develop applications that will run on its operating system; the moreapplications, the more desirable the operating system becomes and the more likely that the market will adopt itas a de facto standard. at the same time, microsoft wants to prevent the development of a competing settop boxoperating system that is compatible with all the microsoftcompatible applications.computer systems policy projectthe computer systems policy project (cspp), whose members include computer systems vendors such asapple and ibm, shares many of the same intellectual property assumptions as microsoft. thus, it believes thathardware interfaces specifications are patentable and software interfaces specifications are both patentable andcopyrightable. the cspp differs from microsoft in that it believes that special rules should apply in theinformation infrastructure environment. specifically, the cspp believes that the owner of an interface that isadopted as an infrastructure standard should be required to license it on reasonable and nondiscriminatory terms,not only to developers of attaching products but also to developers of competing products. that is, the interfacespecifications should be readily available to all vendors so that they could "build products that are compatiblewith both sides of the interface."5 further, the proprietor of an interface standard should be able to revise it onlywith timely notice or public process.the cspp position represents an underlying fear of microsoft's market power. by requiring the licensing ofthe interface standards to the developers of competing as well as attaching products, cspp hopes to prevent amicrosoft settop box operating system monopoly even if the microsoft interfaces emerge as the industrystandard. moreover, by permitting revision of standard interfaces only with timely notice, cspp hopes to preventthe lead time advantages microsoft's applications developers would otherwise have over third party developers.(these advantages are one of the subjects of the ongoing litigation over the microsoft consent decree.)the cspp approach may work well enough for de jure standards set by a standards body. the standardsbody may, through negotiations, extract significant concessions from the proprietor. but what if the market, asopposed to a standards body, adopts microsoft's settop box operating system as a de facto standard? pursuant towhat authority will microsoft be forced to license its interface specifications to competitors, or provide timelynotice of upcoming revisions? moreover, who will determine the "reasonableness" of the license fees demandedby microsoft? indeed, cspp itself recognizes the shortcomings of its approach. it has conceded that in giimarkets that are not competitive, the government may need to intervene "to ensure that critical interfaces areopen."6 nonetheless, the cspp continues to insist that the government "refrain from implementing compulsorylicensing related to standards."7the american committee for interoperable systemsthe american committee for interoperable systems (acis), whose members include storage technology,at&t global information solutions, amdahl, and broderbund software, starts from a somewhat differentintellectual property assumption than microsoft and cspp. while it agrees that hardware and software interfacespecifications are patentable, it doubts that many such specifications will meet the rigorous statutorycompeting definitions of "openness" on the gii32the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.requirements for patentability. moreover, unlike microsoft and cspp, acis believes that copyright cannotprotect software interface specifications. it reaches this conclusion in reliance on the recent appellate courtdecisions in computer associates v. altai8 and lotus v. borland.9 further, acis believes that any incidentalcopying that occurs during software reverse engineering is lawful pursuant to sega v. accolade .10acis, accordingly, believes that existing intellectual property law, as understood by acis, permitssufficient openness in the gii. because few interface specifications would receive patent protection, and nosoftware interface specifications would receive copyright protection, the firms that develop both de facto and dejure gii standards would rarely be able to exercise proprietary control over them. additionally, the ability toreverse engineer the interfaces reduces the necessity of mandatory disclosure.sun microsystemssun, like acis, believes that copyright does not extend to software interface specifications. nonetheless,sun does not conclude that the current intellectual property laws provide sufficient openness on the gii. it fearsthe ability of the de facto standard inventor to obtain patents, to keep certain interfaces hidden, and to change thespecifications without notice. because of the importance of the information infrastructure to the world economy,sun believes that the government should overcome these obstacles to openness by designating criticalinfrastructure interface specifications as "barrierfree." other developers would have complete access to thesespecifications for nominal consideration.incentives and opennessmicrosoft and cspp argue that the acis and sun positionsšwhich at a minimum deny copyrightprotection for software interface specificationsšwould eliminate an important incentive for innovation.according to microsoft,without the incentive offered by the ability to license intellectual property, the information infrastructure would notget built. r&d of the type needed to develop complex products like interactive television requires the investment ofhundred of millions of dollars. companies must be able to recoup those investments by licensing the rights to usethe fruits of those investments. in addition, public domain standards give international competitors free ride ontechnology and intellectual property developed here in the u.s.similarly, cspp states that "[d]evelopers of specifications for interfaces must be able to retain ownership ofand benefit from the intellectual property that goes into the specifications, in order to maintain incentives todevelop new technologies."12sun and acis reply to this criticism by drawing a distinction between interface specifications and interfaceimplementations: "interface specifications are pieces of paper; implementations are actual products orservices."13 removing protection from an interface specification does not lead to removal of protection from aparticular implementation of that specification. indeed, proprietary implementations of nonproprietaryspecifications provide the bases for rigorous competition between providers: "this combination ofnonproprietary interface specifications and proprietary implementations meets the imperative of balancing therequirement of providing incentives to developers of new technology with the societal need for interoperabilityalong the information infrastructure."14competing definitions of "openness" on the gii33the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.governmental and international definitions of opennessgovernments and international organizations began to adopt positions on gii openness in the weeks leadingup to the g7 ministerial conference on the information society at the end of february 1995. these positions aresomewhat vaguer than those articulated by the u.s. private sector.eurobititicjeidain january 1995, information technology trade associations from europe (eurobit), the united states(information technology industry council; itic) and japan (jeida) met to create a joint position paper theyhoped would influence the g7 meeting the following month. the first section of the joint paper dealt withinteroperability and openness:15the key to interoperability is the development and implementation of open interfaces. an interface is open if itsspecifications are readily and nondiscriminatorily available to all vendors, service providers, and users, and if suchspecifications are revised only with timely notice and public process.the joint paper stresses that those open interfaces should be the product of "privatesectorled voluntaryconsensus standards development processes."16 it also draws a distinction between interface specifications andimplementations: "interface specifications provide the information and technical parameters for how systems,products and services communicate with each other, but should be limited to that information necessary toachieve interoperability, allowing suppliers to develop different implementations with distinguishingcharacteristics."17 nonetheless, the joint paper recognizes a role for proprietary technologies in gii standards:"when a proprietary technology is incorporated into a standard, the developer must voluntarily agree to licensethe technology on reasonable terms and conditions, demonstrably free of discrimination."18the joint paper's views in general run parallel to those of cspp: intellectual property rights can reside intechnology included in a gii standard, but the proprietor must license the technology on reasonable andnondiscriminatory terms and revise the specifications only with timely notice. the joint paper unfortunately alsoshares the infirmity of the cspp paper of not explaining how the proprietor of a de facto standard will berequired to make its interface specifications available to competitors. the joint paper, however, does notexplicitly state that copyright protects software interface specifications. thus, the references to proprietarytechnology could be interpreted as referring to patented hardware and software interface specifications. indeed,the joint paper was probably intentionally left ambiguous on this point. while many itic and cspp membershistorically have supported copyright protection for software interface specifications, many jeida membershave opposed it. by discussing "proprietary technology" in the abstract, the joint paper could satisfy bothconstituencies.united statesin february 1995, the clinton administration's information infrastructure task force issued its agenda forcooperation amplifying on themes vice president gore had articulated in a speech in march 1994, to theinternational telecommunications union in buenos aires. one of the five core principles for the gii recognizedby vice president gore and the agenda for cooperation is providing open access. the agenda for cooperationfurther recognizes that open access can be achieved only through interoperability and standardization:19an essential technical element of the open access concept is interoperability, i.e., the ability to connect applications,services, and/or network components so that they can be used together to accomplish tasks. as the gii will bebased on many different existing and emerging components at local, national, and global levels, it is imperative thatthese components be interoperable. the key to interoperability is global standards.competing definitions of "openness" on the gii34the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in listing recommended government action to achieve the goal of open access through global standards, theagenda for cooperation states that the u.s. should join with other countries to ''[e]ncourage an open, voluntarystandardssetting process that does not denigrate intellectual property rights.–"20 like the eurobititijeida formulation, the u.s. government appears to recognize intellectual property rights in standards adoptedby standards bodies without taking a specific position on the copyrightability of software interface specifications.the discussion in the agenda for cooperation is so general, however, that it does not contain the protectionsincluded in both the joint paper and the cspp paper: licensing on reasonable and nondiscriminatory terms, andrevision with timely notice and public process.interestingly, a separate section of the agenda for cooperation appears to address this issue, as well as theproblem of proprietary rights in de facto standards. when describing the need to create a flexible regulatoryframework that fosters competition, the agenda for cooperation states that such a regulatory framework shouldclearly indicate:21 the means by which new entrants can gain market access, e.g., – licensing requirements –; the nondiscriminatory terms and conditions of interconnection to an incumbent operator's network and ofsupplying information services over the network.–here, the u.s. government appears to go beyond the joint paper's and cspp's call for voluntary licensing;the quoted passage reflects an intent to mandate licensing by law. in other words, the agenda for cooperationtakes the problem of proprietary rights in de facto standards seriously.european unionthe european commission in september 1994, issued an "initial theme paper" for the g7 meeting. thetheme paper recognizes the importance of interoperability in permitting competition in the development of theinfrastructure, which in turn will ensure that users receive the widest range of services at the lowest possible cost.to this end, "g7 governments are invited to express their support for a consensual standardization processwhich is open, voluntary, and private sectorled."22 the commission notes that such a process "would help avoidtwo pitfalls. on the one hand, the unilateral imposition of mandatory standards by public authorities and, on theother, the emergence of de facto standards from monopolistic market positions."23although it is properly concerned about de facto standards and resultant monopolization, the commission isunrealistic in its belief that standards bodies alone will solve the problem. standards bodies tend to work slowly,and thus probably will not keep pace with the highly complex, rapidly evolving gii. accordingly, de factostandards will emerge notwithstanding the best intentions of government and industry.g7 nationsat the conclusion of the g7 ministerial conference, the g7 parties issued a statement strongly supportingopenness. the current regulatory framework must evolve to "put the user first."24 the framework "must bedesigned to allow choice, high quality services and affordable prices."25 this consumer welfare will flow from"dynamic competition,"26 which in turn will result from "interconnectivity and interoperability."27the g7 parties specifically commit themselves to "[p]ursue the interconnectivity of networks andinteroperability of services."28 this goal will "be achieved through the promotion of a consensual standardizationprocess which is marketled and which encourages open interfaces."29 the g7 partners recognize the need toaccelerate the standardization process so that it can develop "timely and marketresponsive standards."30the statement does not explicitly discuss de facto standards. nonetheless, it reflects a concern with thepossible monopolization of the gii. accordingly, the statement indicates that competition rules need to beinterpreted and applied so as to encourage new entrants and promote global competition. further, competitionauthorities must "shield[] against – risks of abuse of market dominance."31competing definitions of "openness" on the gii35the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.moreover, the g7 statement contains no reference to the incorporation of proprietary technology instandards. the silence on this topic appears to be a result of a compromise between the united states and theeuropean union.32conclusionthe united states, the european union, and the g7 have all rejected the microsoft definition of openness;in their view, openness requires that the standard interface specifications be available to all vendors, not onlythose developing attaching products. the united states and the g7, at least implicitly, have recognized theshortcomings of the cspp/eurobititijeida approach; they understand that standards bodies alone cannoteliminate the danger posed by proprietary control of de facto standards. the g7 solution appears to be vigilantantitrust enforcement. in contrast, the u.s. solution appears to be statutorily imposed compulsory licensing.it is unlikely that congress would require compulsory licensing for critical gii interface specifications.even if it did, such licensing might serve as an effective remedy only if the government remained activelyinvolved to arbitrate disputes concerning the reasonableness of license fees and the timeliness of disclosure.further, active government participation might not succeed in preventing costly and timeconsuming litigationover these issues. vigilant antitrust enforcement has the same deficiency: it is cumbersome and resolves onlyone narrow dispute at a time.for these reasons, the most effective approach may well be that of acis. if governments clearly affirmedthe recent u.s. case law denying copyright protection for software interface specifications, much of the problemposed by de facto interface standards would vanish. to the extent problems remained on the marginsše.g., theoccasional patent over an interface specificationšcompulsory licensing or antitrust enforcement could addressthem.in any event, governments need to focus more attention on this issue immediately. the private sector hasforged forward with the gii, already outpacing the standards bodies. as a practical matter, it will be much easierto solve the problem of de facto standards before the standards emerge on the gii. once they do emerge, theywill create vested interests willing to spend significant sums to resist any change to the status quo.notes1. jonathan band is a partner in the washington, d.c., office of morrison & foerster. sections of this article appear in band and katoh.1995. interfaces on trial. westview press, and the forthcoming japanese language version of this book.2. information infrastructure task force. the national information infrastructure: agenda for action. information infrastructure task force,washington, d.c., september 15, p. 9.3. id.4. see myhrvold, nathan p. 1994. "interactive video systems," statement before the house subcommittee on telecommunications andfinance of the committee on energy and commerce, february 1.5. computer systems policy project. 1994. perspectives on the national information infrastructure: ensuring interoperability. computersystems policy project, washington, d.c., february.6. kerkeslager, ellwood r. 1994. "electronic commerce and interoperability in the national information infrastructure," statement beforethe house subcommittee on technology, environment and aviation of the committee on science, space and technology, may 26, p. 8.7. computer systems policy project. 1995. perspectives on the global information infrastructure. computer systems policy project,washington, d.c., february, p. 6.8. 982 f.2d 693 (2nd cir. 1992).9. 49 f.3d 807 (1st cir. 1995), petition for cert. filed (u.s. june 7, 1995).10. 977 f.2d 1510 (9th cir. 1992).11. myhrvold statement, p. 23.12. kerkeslager statement, p. 3.competing definitions of "openness" on the gii36the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.13. see rosing, wayne. 1994. "interactive video systems," statement before the house subcommittee on telecommunications and financeof the committee on energy and commerce, february 1, p. 2.14. american committee for interoperable systems. 1994. comment on international aspects of the nii, august, p. 4 n.2.15. eurobititijeida paper (jan. 1995), p. 5.16. id., p. 7.17. id., p. 5.18. id., p. 7.19. information infrastructure task force. 1995. global information infrastructure: agenda for cooperation. information infrastructuretask force, washington, d.c., february, pp. 14œ15.20. id., p. 16.21. id., p. 17.22. european commission. 1994. initial theme paper. september.23. id. canada also seems to rely entirely on standards bodies to achieve the goal of interoperability. see the canadian information highway(april 1994), pp. 13œ14 and 23œ24.24. documents resulting from the g7 ministerial conference on the information society, issued feb. 26, 1995, reprinted in dailyexecutive report (bureau for national affairs, washington, d.c.), feb. 28, 1995, at m5.25. id.26. id.27. id.28. id.29. id.30. id.31. id.32. see hudson, richard l., and james pressley. 1995. "g7 nations make gains in facilitating access to information superhighway," wallstreet journal, february 27, p. a7a.competing definitions of "openness" on the gii37the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.6communications for people on the move: a look into thefuturerichard c. barthmotorola incorporatedstatement of the issuewhen americans think of wireless communications systems, they often think only of cellular phonesbecause of the explosive growth of this particular technology in many parts of the world. while that form ofwireless communication is only one of manyšpaging, television broadcasting, police twoway radios, and manyothers come to mindša closer look at wireless, and especially cellular technology and systems, is instructiveregarding the growth overall of wireless communications, not just in the united states but around the world.while seemingly ubiquitous in some business settings, the use of cellular phone and data communicationssystems is only in its infancy. despite its dramatic growth, penetration rates for cellular technologybasedsystems in the united states are still running at less than 10 percent of american households. compare this topenetration rates for another key component of the global information infrastructure, computersšpcsšwhichare estimated to be in 30 percent of american households. for many reasons the growth of cellular technologybased systems will likely continue to increase dramatically. this paper seeks to highlight the reasons for growthof wireless systems generally and cellular systems specifically. all of these changes represent a significant partof the evolving national information infrastructure (nii).backgroundthere is no shortage of open issues in the nii debate. the categories of security and privacy,interoperability, spectrum availability, information access, ease of use, portability, ubiquity, network availabilityand manageability, applications development, multimedia, and network components provide just a partial list ofthe open nii issues. for our purposes here, the focus is on just two of these: portability and ubiquity. this focusis deliberate. without satisfying these two requirements, the convenience, services, and applications that arevisualized cannot be delivered, and the nii, rather than being a bold step forward, will in fact be a step backward.after some 100 years of technological progress in communications, we live today in a world where voicecommunications are virtually ubiquitous. that means that today almost anyone can callši.e., have a voiceconversationšwith almost anyone else anywhere at anytime. to do this with cellular and cordless technologies,the phones are locally or regionally wireless, and the wireless network that supports them is implemented by aparallel wired network that is highly complementary. there are some limitations in terms of access, costs, andcompetition, but recent private and publicsector activities to implement personal communication services (pcs)šan extension of cellular technologyšwill go a very long way toward improving these limitations. thus,holding private voice conversations will completely meet the anytime, anywhere standard of service.in contrast to voice communications are visual or videobased services, which in many cases are stillcomparatively expensive and tightly controlled. whereas anyone can make a phone call, only those designatedcan broadcast a television show or movie. further, access to information and the role of computing incommunications among people have been slower to develop. only recently, with the advent of easy to use onlineservices, are computers in the home, at schools, and in the workplace being used to supplement the papers,books, and access to learning services that remain pretty much as they were at the turn of the centuryšin newsstands,communications for people on the move: a look into the future38the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.libraries, and schools. radical changes in how people access data through computers are projected fairlyconfidently, at least in the developing countries.the development of the technologies and networks that is best described by the nii is dramaticallychanging all of this, and by doing so is empowering all citizens with the conveniences and opportunities that willresult from making all of the services personally accessible. with an nii, the world of "voice and imagery" aremerging, along with more ready transfer of data, to meet the anytime, anywhere standard of service. individualswill have full access not just to voice services as they do today, but also to imagebased services and informationservices that are now only being imagined. this nii will have full mobility and connectivity that will be madepossible by completing secondgeneration systems and bringing on the thirdgeneration wireless systems thatwill become part of the nii.before getting into what this represents in terms of new functionalityšefficiencies and servicesšit isappropriate to discuss why this vision could be at riskšthat is, what could easily happen if vision and actiondon't match with the opportunity for portability that wireless technologies offer to the nii concept.the promise of the nii lies in three synergistic forcesšthe availability of bandwidth brought on bydevelopments in fiber and signaling, the availability of computing brought on by the microprocessor and themarch of the semiconductor industry, and the emergence of competition and choice brought on by new telecompolicies worldwide. the wireless component of these forces of technology is critical, especially next generationpaging, cellular pcs, and dedicated systems used by public safety and critical industries.until recently, everything you could receive on your home wallattached television, you could receive onyour portable television, whether you chose to use it in another room, or on a camp out or while at a sportingevent. that started to change with cable when the delivered wired bandwidth for television services waseffectively increased by two orders of magnitude beyond that available in the radio frequency allocations fortelevision. a similar shift occurred in computing over roughly the same time period. early on, what you could dowith a portable computer, or what we then called a portable computer, was pretty much what you could do withyour office or home computer. that changed when local area networks (lans) and computer networks cameinto being. with that transition, the portable computer became a comparative weakling to its lanbasedequivalent. these changes initially went unnoticedšafter all, at least the new portable computer was portable, ifa little out of touch, and who really needed 100 channels of television in any event?let us hold this perspective and move forward in time as the nii begins to deliver on its promise. peoplecan talk face to face, and so groups can interact and decisions are made more quickly; families are united thoughthey live miles apart; highspeed computing and information access are available in the home and office, and as aresult people are more productive and better informed. telecommuting becomes a reality, lowering energyconsumption. but whereas in today's world most of the communications services that are available to a worker ata desk are available to a worker on the move, that is no longer necessarily true in the futurešunless, that is,broadband wireless services are brought into line with broadband wired services.this scenario prompts two questions: does it matter what is lost and what is gained, and, if it does, can it bedone with the technology that is available and the other constraints that are likely to apply? the answer to bothquestions is yes.analysis and forecastlet us start with the first question, does it matter? broadly, we have already seen the high value people puton mobility. that value has generated vast new highgrowth industries that not only have made the u.s. citizenrysafer and more personally in touch, but also have made u.s. industry more efficient while driving substantialnew export markets as well. but it is what happens in specific circumstances and industries that is perhaps moreimportant. in other words, the applications must be carefully examined.many of the most interesting applications of wireless technology require the availability and dependabilityof private landmobile communicationsšthat is, the system dedicated to provide bestfit solutions to thecommunications needs and critical industries and protection of the public. these systems are a primary factorthat has allowed the united states to establish and maintain its position as the world's leading producer of goodscommunications for people on the move: a look into the future39the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.and services. private landmobile radio is used by all segments of the industrial, business, public safety, publicservice, and land transportation mobile work force. private landmobile systems have become an indispensablepart of the operation of this work force. the continued growth of this nation's commercial and public serviceactivities demands additional communication capabilities. it is imperative that the industrial and public safetysectors have access to new imaging and decision processing/remote file access technologies. even with theavailability of some personal communication services offered by private and common carriers, public safety,public service, and industrial users will continue to satisfy their specialized communication requirements throughprivate systems.the private land mobile radio user community is a necessary ingredient in maintaining globalcompetitiveness. motivated by the constant need of the private sector to improve productivity and services,private users will invariably migrate to the specific communications solutions that provide the greatest advantageto their operations. an additional allocation of radio spectrum is essential if these users and their industries are tocontinue to flourish in increasingly competitive global markets.unique communication services requiredsome of the unique services anticipated as being required to serve the critical daytoday operational needsof critical industries and of public safety and public service organizations include the following.crime control mobile transmission of fingerprints, mug shots, warrants, and other images to and from law enforcementfield personnel; mobile transmission of maps, floor layouts, and architectural drawings for control of crimeinprogressoperations; tactical use of live mobile video for hostage, arrest, and surveillance operations; highresolution graphics and electronic transfer of maps and other graphic information to police vehicles; vehicle and personnel tracking systems; locator service to address personnel security utilizing wearable devices containing wireless transmitters("wireless dog tags"); and onboard information and security systems for mass transit vehicles.energy conservation and management advanced distribution automation (remote monitoring, coordination, and operation of distribution andtransmission components from centralized locations, including load management, advanced metering, andsystem control functions); demand side management ("dsm") systems (e.g., managing the consumption of electric power and naturalgas); transmissions to monitor and record pipeline flow and pipeline pressure indicators; and realtime monitoring, alerting, and control in situations involving handling of hazardous materials.health care and fire/emergency management systems remote monitoring of patients' vital signs in health care facilities to provide continuous patient monitoringand immediate response in the event of a patient crisis;communications for people on the move: a look into the future40the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. mobile transmission of maps, floor layouts, and architectural drawings to assist firefighters and otherresponse personnel in the rescue of individuals involved in emergency situations; transmission of visual signals and physician instructions in support of rescue operations; highspeed transmission of highresolution medical imagery and data from paramedics to hospitals; and automated inventory control.pollution control highresolution graphics and electronic transfer of maps and other graphic information to mobile users; management and remediation operations following spills or other crises; realtime monitoring, alerting, and control in situations involving handling of hazardous materials; and visual inspection of pipes and cables exposed during excavation projects.improving industrial productivity automatic transmission of messages advising of impending shortages of parts in a manufacturingenvironment; vehicle and personnel tracking systems; locator service to address personnel security utilizing wearable devices containing wireless transmitters("wireless dog tags"); remote safety and security inspection of inaccessible locations; automation of process and quality control functions; transmission of scheduling and cost updates, job site inspection results, and performance assessmentsrelating to construction projects; and wireless facetoface conferences between inhouse production and sales personnel.many of these applications can be satisfied through the application of wireless technologies developedinitially for the cellular market. there will also be a variety of special "niche" requirements that, by virtue oftheir highly specialized environment and exacting reliability requirements, will tend to be incompatible withconsumeroriented, carrierprovided pcs services that are evolving from the cellular technologies.for example, a variety of advanced technology services will be required to ensure the safety and effectivefunctioning of both underground and elevated transit and rapid rail transportation systems. in addition, there willbe very specialized requirements for other critical industrial and public safety operations conducted inunderground environments. further, there will be a requirement for special broadband video and data systemsdesigned to provide highly reliable communications networks in inherently dangerous settings. private useremerging technology systems will fulfill a critical role in ensuring the safe and efficient functioning ofmaintenance crews and fuel and other service personnel working on highly congested flight lines.allocation of spectrumcellular and pcs spectrum allocations over the past several years have been critical to the introduction ofsome of these technologies. however, the expected rapid growth of wireless systems, based on user demand,requires that government policymakers assure a continued reallocation of spectrum to these needs. the recentspectrum allocation for pcs will not satisfy the need for spectrum for private emerging technologies. theregulatory scheme adopted for pcs makes it impractical, if not impossible, for private users to obtain and usetheir own pcs licenses for the new telecommunications technologies they need. moreover, pcs carrierlicenseesare inherently unlikely to offer the specialized solutions needed by public safety and critical industries.another factor that requires full analysis is the mobility of these new systems that are so critical to theevolution of the nii. while the overall agenda is for full mobility for the nii, its implementation fortunatelycommunications for people on the move: a look into the future41the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.divides into two phases. the first requires immediate attention, and the second requires specific actions towardthe turn of the century. the details are as follows.phase i for nii mobility spectrum for second generation low earth orbit (leo) satellite systems. while the first generation ofleo technology is only just now being brought to market, it is not too early to plan for additional spectrumin anticipation of its success. we support the federal communications commission (fcc) proposal toallocate an additional 70 mhz to allow for expansion of existing systems and the emergence of anticipatedcompetitive systems. spectrum for industrial and public safety digital systems with broadband capability. it has always been apriority of the fcc to ensure that all needed spectrum for public safety and critical industry support is madeavailable. as such, the tradition of support and forwardlooking solutions for public safety and privateindustry is a long one that has been marked by the continued leadership of the united states. to prepare forthe next series of needed changes, it is estimated that 75 mhz of spectrum is needed to deliver digitalsystems with broadband capability. these systems will not support continuous full motion video, but theywill support selected slow scan video, image transmissions, file searches, building layouts, hazardouschemical mapping, and finger prints.phase ii for nii mobilityanalog cellular, paging, and private systems provided the first generation. digital systems that "remined"the existing spectrum, pcs, and the first phase of the nii mobility initiatives make up the second generation.phase ii for nii mobility makes up the third generation. thirdgeneration systems for private or public use andfor data, paging, image, or voice provide similar functionality with flexible broadband capability, increasedcapacity, satellite system interconnectivity, and global roaming. these systems allow voice, but they also providevideo. they support data, but they also support data at lan rates. they deliver the full capability of nii to themobile worker and the mobile person. clearly, substantial spectrum will be necessary to support competingpublic systems, wireless cable access, and needed private systems with this capability. efforts are just beginningto access how much spectrum may be needed and where that spectrum will be found in each of the world'sregions and countries.growth of wireless systemsfinally, let us focus on the key issue of the growth of wireless systems based on cellular technologies. as apercent of the world population, users of cellular technologies account for less than 1 percent. while growthrates for cellular systems have been running at 40 to 50 percent per year, that growth may well increase or atleast continue for many years because of the large market opportunities that remain. by the end of this yearalone, there will be over 70 million users of cellular technology worldwide.currently, the overall voice and telephony usage of telephony services is estimated at somewhat more than4 trillion minutes, both wired and wireless. that will grow to nearly 8 trillion minutes by the year 2003.correspondingly, the wireless component is now about 70 billion minutes at present, but that is expected to growto 2 trillion minutes in 2003. so while it now amounts to a little over 1 percent of the total, predictions are that itwill increase to 25 percent of the total. while that represents a tremendous opportunity, it is also a tremendouscause for concern if, as noted in the introduction of this paper, vision and action do not match this demand forspectrum and technology.communications for people on the move: a look into the future42the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.recommendationsthe role of governmentthe analysis presented above leads to the question, how can government help? first and foremost,government needs to accept what history has taught, that is, that mobility is essential. wireless solutions need tobe an explicit part of the nii agenda. an initial 165 mhz of spectrum if we leave in its, and 145 mhz if not,needs to be allocated for industrial and public safety services, ivhs and satellite services. substantial additionalspectrum will be required beyond that for thirdgeneration systems. in fact the government itself has projected aneed for approximately 250 mhz over the next 10 years for wireless terrestrial and satellite services.government assistance needs to be focused on making spectrum available. "remining" of existing broadcasttelevision and broadcast auxiliary spectrum should be considered in light of the capability of phase ii systems todeliver both broadband data and video. clearing the spectrum is not just a regulatory challenge. solutions needto be developed to migrate existing services to either wireline or new spectrum.the united states has led the world with its communications and computing visions in the past, and withmobility as part of the nii agenda it will do so again well into the next century.the role of standardsthe network architecture of the nii must support its goal to facilitate true national mobility andconnectivity. open standards and interfaces should be adopted to assure a competitive supply of equipment tothe operators and users of the nii. a level playing field and fair competition between independent devicemanufacturers will ensure affordable pricing and continued technological development. naturally, the standardsshould address the need for privacy while allowing room for the innovative use of technology to provide accessand security.summaryin conclusion, the nii features of portability and ubiquity are key to its success. but these aspects of the niican be realized only if u.s. government regulators free up appropriate spectrum resources so that the privatesector can develop these new markets.communications for people on the move: a look into the future43the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.7building the nii: will the shareholders come? (and if theydon't, will anyone really care?)robert t. blaubellsouth corporationmore often than not, public policy debates concerning the national information infrastructure (nii) beginwith a presumption that the proverbial "information superhighway" will be built regardless of what thegovernment does. the only thing public policymakers really have to worry about, the reasoning goes, is toensure that users, rival service vendors, and equipment vendors have affordable access to the nation'sinteroperable network of networks.many knowledgeable observers further assume that the telecommunications and cable television industrieswill move aggressively to upgrade their respective networks over the next 5 to 10 years. they presumably willdo this to take advantage of new market opportunities spawned by interactive multimedia services, and torespond to competition. still others, including several key federal officials, contend that the government will playa positive and constructive role in facilitating significant amounts of capital investment needed to extendbroadband networks to households, businesses, schools, hospitals, and other public institutions throughout thecountry.1this paper examines these expectations from the perspective of telephone company shareholders. severalkey issues are addressed. how do returns on bell company investment in local network facilities compare withreturns on investment opportunities outside local telephone markets? have shareholders been rewarded orpenalized by bell company decisionsšgiven the prevailing regulatory environmentšto upgrade their respectivewireline telephone networks in recent years? on balance, do shareholder returns matter much to anyone otherthan the shareholder and telephone company managers, and if so, to whom, how, and why?backgroundcustomer expectationsin a recent delphi survey, the consulting firm of deloitte & touche questioned 120 executives in theinformation, communications, and entertainment industries about how soon they expect various newcommunications products and services to arrive on the scene, and how rapidly markets for these products andnote: robert t. blau, ph.d., cfa, is executive director of policy analysis for bellsouth corporation. he would like tothank stephen barreca, manager, infrastructure planning at bellsouth telecommunications, for his valuable comments andassistance in developing aspects of this analysis that concern bell company deployment of advanced network technologies.views expressed in this paper are solely those of the author and are not necessarily shared by bellsouth or its subsidiaries.building the nii: will the shareholders come? (and if they don't, will anyone really care?)44the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.services will develop.2 with regard to the commercialization of interactive multimedia networks and devices, thepanel came to the following conclusions: during the next 5 years, both local telephone companies and cable television operators will make significantprogress in building new ''infostructure." a majority (54 percent) expect that advanced network technologywill be available to 10 to 25 percent of telephone company residential customers, while a plurality of thosesurveyed thought 25 to 45 percent of all cable television customers would be served by upgraded networksby the end of the decade. using census bureau population projections for the year 2000, this translates intoa possible range of 10 to 25 million homes for telephone companies and 25 to 45 million homes for cabletelevision operators. a majority (57 percent) of the executives surveyed believe that over a quarter of all schools, libraries,hospitals, and clinics will be connected to a fiber optic network by the end of the decade, and 23 percentbelieve that over 45 percent of these public institutions and agencies will have such a link. a majority (54 percent) of the participants believe that by 1998œ2000 more than onequarter of all u.s.households will have at least one computer with online capability, and 39 percent believe the penetrationrate will be in the range of 25 to 45 percent of all households.interestingly, information industry executives surveyed by deloitte & touche also had a generally favorableview about the role of government in promoting investment in the nii. approximately 62 percent thought the neteffect of government actions by 1998œ2000 will be positive (41 percent) or neutral (21 percent) "in terms ofencouraging investment, fostering research and development, and promoting the rapid spread of advancedinformation, communications, and entertainment offerings."3satisfying user expectationsare expectations for building the information superhighway realistic? in several key respects, the answersto that question will rest with those individuals who will be asked to put up the significant sums of capitalneeded to upgrade the nii, and particularly ubiquitous local telecommunications networks where the lion's shareof these costs will be incurred.if shareholders believe that riskadjusted returns on investment in advanced network technologies willremain competitive with returns on alternative investment opportunities, then those technologies will bedeployed and the new service features they make possible will be brought to the market in a timely manner. if,on the other hand, shareholders do not regard prospective returns on network investment to be high enough tocompensate for risk incurred, then lesser amounts of discretionary capital spending will be committed to newnetwork technologies. in that event, telephone company deployment of new technologies will slow, possibly tothe point of lagging user expectations and needs. this could be especially problematic for developers and usersof new multimedia service applications requiring substantially more bandwidth than is readily available over thepublic switchedtelephone network (pstn).the balance of this paper analyzes relationships between shareholder returns, network investment, and thedeployment of the types of advanced network technologies that will make up the nii. it begins with a discussionof recent trends in each and their implications for future investment. the paper concludes with a brief discussionof steps that telecommunications policymakers must take to create a more technologyfriendly marketenvironment.building the nii: will the shareholders come? (and if they don't, will anyone really care?)45the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.analysisrelationships between network investment and shareholder returnsfigure 1 highlights relationships between regional bell company (rbc) investment in regulated wirelinenetworks and total shareholder returns between 1988 and 1994. network investment is expressed as thepercentage of bell telephone company operating cash flow used to acquire wireline network plant andequipment. unlike reported earnings, which are subject to the vagaries of different financial and regulatoryaccounting practices, operating cash flow provides an accurate measure of cash that a business generates after itpays its outofpocket operating expenses, taxes, and interest on its debt.4besides financing the acquisition of new plant and equipment for their regulated wireline networks, localbell telephone companies principally use their operating cash flow in one of two ways: to pay dividends to theirregional holding company shareholders, or to finance investment opportunities outside local networks. the ratioof wireline network investment to cash flow from local telephone operations, therefore, is a good comparativemeasure of how the individual companiesšand their shareholdersšview the relative attractiveness of usingavailable cash flow to upgrade their local network facilities.5 this is particularly true of the rbcs, since theycurrently finance nearly all capital expenditures (and pay all dividends) with internally generated funds (e.g.,operating cash flow).figure 1 also depicts cumulative changes in total shareholder returns for each of the regional bellcompanies between 1988 and 1994. total shareholder returns include the percentage change in the price of anindividual stock plus its dividend yield (i.e., dividends paid divided by the price of the stock at the time). forpurposes of this analysis, cumulative shareholder returns are based on monthly returns and assume that alldividends paid on a particular stock are immediately reinvested in that stock.figure 1 highlights a definite inverse relationship between the ratio of network investment to operating cashflow from local telephone operations and total shareholder return. differences in shareholder returns between theindividual regional companies over the 1988œ94 period also were quite substantial. if a shareholder had invested $1,000 in a market weighted portfolio containing all seven rbc stocks onjanuary 1, 1988, and reinvested all dividends paid, the portfolio would have increased in value to $2,407 ondecember 31, 1994. this represents a gain of 141 percent, as compared with a cumulative return of 132percent on the s&p 500. during this period, the seven rbcs reinvested 65.6 percent of their combined cashflow from their local telephone companies operations back into their regulated wireline networks. between 1988 and 1994, three of the seven regional bell companiesšus west, bellsouth, and nynexšreinvested 71.3 percent of their local telephone companies' combined operating cash flow in wirelinenetwork plant and equipment. had the same shareholder invested $1,000 in these three stocks on january 1,1988, this market weighted portfolio would have increased in value (assuming dividend reinvestment) to$2,055, for a gain of 105 percent. during the same sevenyear period, three other rbcsšameritech, pacific telesis, and southwestern bellšreinvested only 58.7 percent of their combined cash flow from local telephone operations in their respectiveregulated wireline networks. had $1,000 been invested in these three stocks on january 1, 1988, the value ofthis market weighted portfolio would have increased to $3,019 by december 31, 1994, for a gain of 202percent.given the size of the rbcs and the capital intensity of their local telephone operations, these differences inshareholder returnsšand the emergence of an inverse relationship between capital spending on wireline networkplant and equipment and shareholder returnsšcould have an important bearing on future investment in localnetwork facilities. if the recent past is prologue, the level of discretionary capital spending on local wirelinenetworks (i.e., capital expenditures over and above those required to maintain the quality of local telephoneservice at current levels) also will determine how rapidly broadband multimedia and other advanced servicefeatures are brought to the market.building the nii: will the shareholders come? (and if they don't, will anyone really care?)46the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.relationships between capital spending on local telephone plant and equipment and thedeployment of advanced network technologiestables 1 and 2 highlight the degree to which differences in wireline network investment among the sevenrbcs are reflected in how rapidly each of the regional companies upgraded their respective local wirelinenetworks. the tables depict penetration rates and substitution ratios, respectively, for ten advanced networktechnologies that the federal communications commission routinely tracks through its armis infrastructurereports.6 the rbcs and gte file these statistics with the commission annually, and they are currently availablefor 1989œ93.figure 1 (top) rhc network investment. percent of bell telephone company operating cash flow reinvested inlocal wireline networks between 1988 and 1994. (bottom) rhc shareholder return index, 1988œ94.source: compustat and one source.penetration rates shown in table 1 represent the percentage of a company's total number of switches, accesslines, and communications channels equipped with a particular technology (e.g., isdn, signaling system 7,digital stored program control, fiber optics, etc.). substitution ratios are based on the fisherpry modelcommonly used to project ongoing increases in the penetration of new technologies expressed as a percentage oftotal usage.7technologies depicted in table 2 also are categorized in three broad groupings designed to providecomparative measures of the following: digital connectivity, which includes digital stored program control access lines, isdn access lines, digitalinteroffice links, and fiberequipped channels; deployment of fiber optic capacity, which includes interoffice fiber links and fiberequipped channels; andbuilding the nii: will the shareholders come? (and if they don't, will anyone really care?)47the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. overall network modernization, which reflects the deployment of all digital connectivity and fiberoptictechnologies plus signaling system 7 equipped access lines.substitution ratios for each of these groups were calculated by averaging the substitution ratios for theindividual technologies that make up that group. these composite measures of technology deployment arepresented along with the ratio of network investment to operating cash flow, and cumulative shareholder returnsfor the 1988œ94 period.building the nii: will the shareholders come? (and if they don't, will anyone really care?)48the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.finally, tables 1 and 2 rank order the seven rbcs by penetration and substitution rates for each of theindividual technologies and the three composite technology groups described above, and by the ratio of networkinvestment to operating cash flow and total shareholder returns.as expected, those bell companies that reinvested larger portions of cash flow from their local telephoneoperations in wireline network plant and equipment generally deployed advanced network technologies morerapidly. figure 2 shows, however, that decisions by bell company managers to accelerate the introduction ofadvanced network gear did not have a positive effect on shareholder returns. if anything, the opposite has beentrue. between 1989 and 1993, for instance, bellsouth ranked first in overall network modernization, but onlyfifth in cumulative shareholder returns for the 1988œ94 period.8 southwestern bell, on the other hand, ranked lastamong the bell companies in overall network modernization but first in cumulative shareholder returns.figure 2 (top) network modernization index, 1988 to 1993. (bottom) cumulative shareholder index, 1988 to1994. note: substitution rates measure how rapidly new network technologies replace old ones. networkmodernization index is the average of substitution rates for the following technologies: digitalspc access lines,ss7317 access lines, isdn accessline capacity, fiber and digital links, and fiberequipment channels.source: fcc armis reports, compustat, and one source.these relationships, of course, do not imply that shareholders have some inherent aversion to the bellcompanies upgrading their portion of the nation's information infrastructure. what the data suggest, however, isthat new or incremental revenue and earnings opportunities that investors expected to result from the deploymentof wideband and broadband technologies have not been large enough, at least in recent years, to compensate forthe capital cost and financial risk of installing these facilities sooner rather than later. this has been true eventhough the rbcs' local telephone operations accounted for roughly 86 percent of their total revenues between1988 and 1994, 89 percent of their combined operating cash flow, 95 percent of their total earnings beforeextraordinary charges (e.g., writeoffs of obsolete telephone plant and equipment), and 94 percent of their netincome. (see table 3.)building the nii: will the shareholders come? (and if they don't, will anyone really care?)49the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.implications for upgrading the niiin view of the bell telephone company contributions to the regional holding companies' overall earnings,why do shareholders seem at all concerned about using internally generated funds from local telephoneoperations to upgrade their wireline networks? to understand why this is so and what it means for futureinvestment in advanced network technologies requires at least some appreciation of factors that investorsconsider in valuing common stocks. besides current earnings and dividends, these factors commonly includeearnings and dividend growth, and the degree of financial risk associated with owning a given company'sshares.9 and while the bell telephone companies' current aftertax income might seem reasonable, if not ampletodayšeither in absolute terms or as a percentage of the holding companies' net incomešinvestor attitudesabout their future earnings growth and business risk are decidedly less positive. this is due largely to anunsettled regulatory environment.table 3 compares recent growth in average revenues, operating cash flow, earnings before extraordinarycharges, and net income for the seven regional bell holding companies, their local telephone companies, andtheir unregulated business operations. for comparative purposes, these same data are provided for the companiesthat currently make up the s&p 500, a broad market average. the data show that growth of the bell telephonecompanies' aftertax earnings, whether before or after extraordinary charges, was negative and well belowaverage earnings growth for the s&p 500 companies. by contrast, aftertax earnings from the regional bellcompanies' unregulated businesses grew at an average annual rate of 28 percent over the 1988œ94 period. thisbuilding the nii: will the shareholders come? (and if they don't, will anyone really care?)50the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.explains why securities analysts now attribute as much as 36 percent of the regional holding companies' totalmarket value to their unregulated, nonlocal telephone company operations.10while lackluster earnings growth among the bell telephone companies are attributable to several factors,including slow growth in the general economy between 1988 and 1994, two industryspecific developmentsstand out. one relates to competitive entry into the most lucrative segments of local telephone markets (e.g.,exchange access and intralata toll). competition has limited growth in traffic carried over the bell companynetworks while forcing the local telephone companies to reduce prices. the combination of modest traffic andaccess line growth and lower rates, in turn, has kept local telephone company revenues, earnings, and operatingcash flow flat.at the same time, legal and regulatory barriers to market entry like the modified final judgement (mfj)interlata restriction, the video program prohibition, and until recently the mfj information services restrainthave walled off the bell telephone companies from new revenue streams that could substantially hasten therecovery of capital invested in advanced network technologies. investors understand this and are cautious aboutthe bell companies spending billions of dollars on network capacity that legal and regulatory barriers to marketentry may prevent them from using.their concerns are compounded by expectations that deploying broadband technology in an increasinglycompetitive market environment will change network economics in ways that render telephone companyearnings more volatile than they have been in the past, thereby increasing the risk of owning their stock.11 unliketoday's copper based, narrow band telephone networks, tomorrow's wideband and broadband architectures willgreatly increase bandwidth available to end users. quantum increases in network capacity also will reduce themarginal or incremental cost of transporting communications traffic both in absolute terms and relative to thefixed cost of building and maintaining network capacity. similarly, as telephone company operating leverage(i.e., the ratio of a firm's fixed costs to its total cost) increases, the addition, or loss, of traffic will have anincreasingly pronounced impact on earnings since large portions of those gains or losses in marginal revenue willflow directly to the bottom line.prospects that local telephone companies could lose a significant portion of their local telephone revenuesto rival vendors have been made more apparent in recent years by growth in demand for wirelesscommunications services and the internet. the wefa group recently forecast, for instance, that over the next 10years, increases in network capacity available on commercial wireless communications systems (e.g., cellulartelephone, personal communication services, and so on) will be large enough to accommodate not only naturalgrowth in demand for mobile telephone services, but also nearly all narrowband voice and data traffic carriedover wireline networks, as illustrated in figure 3.12 should these forecasts pan out, wireless operators will nodoubt attempt to leverage their "excess capacity" by integrating into traditional wireline telephone markets. ifthey do, price competition between wireless and wireline carriers will intensify, and wireless systems willcapture voice and data traffic traditionally carried over wireline telephone networks.the same could be said of the internet. as christopher anderson of the economist magazine recentlyobserved.13if the internet does become a "data dialtone," regular dialtone might find itself out in the cold. it is already possibleto make telephone calls on the internet from specially equipped computers; the spread of multimedia pcs andfaster internet connections could make this commonplace. at the same time companies are turning an increasingamount of their telephone traffic into digital data and sending it through private data networks, saving up to halftheir telecom costs. regulation permitting, this traffic could eventually move to the internet. for the telephonecompanies, the only decision is whether they participate in the cannibalization of their revenues or watch it happen.in light of prospects for increased competition, low earnings growth, and increased business risk, it isunderstandable why shareholders appear to have rewarded the bell companies for minimizing discretionarycapital spending on advanced network technologies in recent years. overall, bell company investmentopportunities outside their regulated wireline networks have simply been more attractive in recent yearsprimarily because these investments offer better earnings potential, often with less risk.building the nii: will the shareholders come? (and if they don't, will anyone really care?)51the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.though certainly of interest to investors and bell company managers, the question remains whether inverserelationships between network investment and shareholder returns really matter much to anyone else. theanswer, it turns out, will likely depend on growth in demand for access to the internet, and the types ofmultimedia services that are just beginning to emerge on the world wide web.the internet community's stake in local network investmentaccording to the internet business center, demand for access to the world wide web (www) increased atannual rates of 443,931 percent in 1993 and 1,713 percent in 1994, bringing the total number of www users inthe united states to an estimated 4 million. the internet society further predicts that between june 1995 andjanuary 2000 monthly traffic on the www (measured in bytes) will increase by a factor of 50,000! although noone knows whether such forecasts will prove to be at all accurate, there is little question that the internet willcontinue to expand at unprecedented rates for the foreseeable future.figure 3 bandwidth available on wireline and wireless networks, 1995 to 2005source: the wefa group.internet growth is of particular interest to local telephone companies for three key reasons. the first, asreferenced above, has to do with prospects that significant amounts of voice and data traffic that traditionallyhave been carried over the pstn could conceivably be routed over the internet. because use of the internettoday is effectively free, this shift would enable consumers to bypass the system of usagebased pricingarrangements and subsidies that the telecommunications industry has long relied on to recover its costs whilekeeping basic residential telephone rates at universally affordable levels (i.e., below cost).at the same time, internet traffic rides on network capacity leased from facilitiesbased telecommunicationscarriers. as such, growth in demand for services like the www also represents new market opportunities forlocal telephone companies. routing internet traffic should result in more intensive and more efficient use ofembedded network facilities. if priced properly, it also could help recoup the cost of deployingbuilding the nii: will the shareholders come? (and if they don't, will anyone really care?)52the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.wider band technologies that internet users will need if multimedia service applications available on the wwware to continue to evolve.third, and perhaps most important, because multimedia applications on the www consume far morebandwidth than do electronic mail or other textbased services on the internet, accommodating rapid growth indemand for access to the www could prompt local telephone companies to expand the capacity of theirrespective wireline networks significantly. the need for additional capacity should become all the more apparentas information resources available on the www continue to proliferate.as new resources come online, demand for access to the www will increase along with its value to usersas well as information service providers. similarly, as the value of the www increases (e.g., by the square of thenumber of new users added to it during any given period), online sessions also should increase in duration (e.g.,from current levels of 25 minutes per session versus an average of 5 minutes for local voice telephone call) forthe simple reason that there will be more users and services to interact with. the combination of more businessesand residents spending more time online, accessing increasingly sophisticated multimedia services that requiresubstantially larger amounts of bandwidth to transport, could press the limits of many local telephone networkswithin a relatively short period of time.near term, this demand for added capacity on the pstn will be handled through the deployment of isdn,asymmetrical digital subscriber loop (adsl) equipment, and other advanced technologies that enable localtelephone companies to increase bandwidth that can be made available to individual users over existing copperphone lines. longer term, as demand for interactive full motion video applications develops, narrowband andwideband network technologies will need to give way to fiber optics or architectures that integrate fiber opticsand coaxial cable capacity. either way, the capital cost of accommodating www and other multimediaapplications could be substantial.rendering a local telephone line isdn capable, for instance, typically costs of $100 and $200 for linesalready served by a digital switch, and between $300 to $500 if digital switch capacity has to be installed.14 atyear end 1993, southwestern bell had only 11.2 percent of its access lines equipped with isdn, while 45.5percent of its lines were served by digital switches. at these levels of penetration (and assuming isdninstallation costs fall at the midpoints of the latter two ranges), the total cost of making the company's remaining88.8 percent of access lines isdn ready would be roughly $3.4 billion. as $3.4 billion represents 42 percent ofsouthwestern bell's total capital expenditures during the 1988œ93 period, a commitment by the company tomake isdn widely available, say in the next 5 years, would constitute a major capital spending decision. unlessprospective earnings from its local telephone operations improve significantly, making such a commitment alsowould arguably be at odds with the company's fiduciary responsibility to maximize shareholder value.public policy recommendationsthere is no question that the government will play a key role in balancing shareholders' interest in realizingreasonable returns on network investment, and consumer interests in seeing the pstn upgraded. how publicpolicymakers play their part also will determine when advanced technologies will be deployed, where, and onwhat scale. and though there are several steps they could take to ensure that the capabilities of the pstn keeppace with user needs, three changes in telecommunications policies should prove especially helpful in this respect.immediately replace all legal and regulatory barriers to market entry with userfriendly interconnection requirementsgiven the lessthanfavorable relationships between network investment and shareholder returns, coupledwith substantial increases in bandwidth that the deployment of fiber optics, isdn, adsl, and other digitalnetwork technologies could bring about, it is imperative that federal and state officials immediately abandon anyand all regulations that restrict how the pstn is used. notwithstanding the value of abundant bandwidth to theinternet community, other users, and growing numbers of information intensive industry groups that rely onbuilding the nii: will the shareholders come? (and if they don't, will anyone really care?)53the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.telecommunications networks to produce, distribute or add value to their wares, local telephone companies willnot invest in advanced network technologies and capacity that they may be precluded from using by law orregulation.policymakers should recognize this fact and focus on developing a set of userfriendly networkinterconnection requirements that will encourage the development of a wider array of new service applicationsover which the capital cost of upgrading local telephone networks can be spread. telecommunications legislationrecently adopted by the u.s. senate represents an important and much needed step in this direction. if enacted,the bill would allow the rbcs to provide interlata long distance and video program services and tomanufacture telecommunications equipment provided they comply with a 14point checklist of interconnectionrequirements that will open up local telephone markets to more competition.as frank governali, a leading telecommunications securities analyst with first boston, put it:15we'd love to see a bill passed. one, we think it would be good for the companies and the stocks to get this uncertainprocess behind us. second, selfishly, we're tired of having to observe the inane activities of washington so closeup. although there is no chance that the current senatebill is exactly the same as the one that may ultimately get[enacted into law], we think passage of a reasonably similar bill would be viewed positively by the market andcause many of the telecommunications companies' stocks to rise. by having a bill passed, investors could thenobserve the new operating environment and try to pick those companies that could do well in the new environment.without a bill, the selection process becomes more difficult and the stocks more volatile, which is what we've seenover the past twelve months.complete the transition from cost plus rateofreturn regulation to price regulationin addition to replacing legal and regulatory barriers to network utilization with procompetitive, userfriendly interconnection requirements, federal and state officials need to complete the transition from rateofreturn regulation to a pure form of price regulation for noncompetitive telecommunications services. moving to apure form of price regulation also should have a favorable impact on efficient network investment in twoimportant respects.first, like telecommunications legislation, price regulation would eliminate a considerable amount ofregulatory uncertainty that has discouraged network investment in recent years. price regulation wouldaccomplish this by eliminating any need for regulators to concern themselves with the types of capitalexpenditures regulated telephone companies are allowed to undertake, how local network costs are allocatedbetween different services for ratemaking purposes, or how rapidly new network plant and equipment isdepreciated. because these types of regulatory requirements are especially susceptible to being "gamed" by rivalservice providers for purely anticompetitive ends, their removal would make for a much more predictable andinvestmentfriendly market environment.equally important, price regulation would give local telephone companies a muchneeded opportunity toincrease returns on network investment, while affording consumers much better protection from having to paythe cost of bad investment decisions by regulated carriers. under price regulation, a local telephone company'sshareholders would bear the brunt of illconceived or poorly timed investment in fiberoptic feeder plant or someother advanced technology, since price constraints would preclude the company from automatically passingthese costs through to the ratepayer. at the same time, however, if a company's investment in advanced networktechnologies and capabilities succeeded in raising earnings, then those benefits would accrue to its shareholders,much as they would in any other business. incentives to invest in new technologies would then improve.shifting investment risk from the local telephone companies' customers to their shareholders is especiallyimportant in today's market environment. this is primarily because consumer demand for many new multimediaservices that telephone companies will rely on to help recoup the cost of deploying wideband and broadbandnetwork capacity will be far less predictable and, in all likelihood, far more volatile than demand for plain oldbuilding the nii: will the shareholders come? (and if they don't, will anyone really care?)54the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.voice grade telephone service. policymakers need to recognize that the emergence of a less certain and morevolatile market environment will necessarily raise the risk of investing in new telecommunications technologies.and that unless regulatory pricing policies are adjusted in ways that compensate shareholders for bearing thatadded risk, capital expenditures on network facilities will slow, and network capabilities will likely lag behinduser needs.allow local telephone companies to fashion new business models for accommodatinginternet usersfinally, if rapid growth in demand for internet services continues to reshape the face of electroniccommunications, as many believe it will, local telephone companies and their regulators will need to experimentwith new models for providing and pricing bandwidth that internet users will no doubt want. what these modelsmight entail is, at this point, unclear. suffice it to say, however, that they will need to balance the telephonecompanies' need to recoup the cost of deploying advanced network technologies with the internet community'sdesire to keep the internet free of toll or other usagebased charges.how these needs and interests ultimately get worked out will likely be accomplished through a considerableamount of trial and error. in some instances, regulators may need to give residents as well as business customersthe latitude to pay part of the front end cost of installing advanced service features such as isdn or adsl inexchange for getting access to those features sooner and at lower monthly rates. in other areas, local telephonecompanies may want to provide customers free access to the internet in exchange for agreeing to subscribe to asecond telephone line for some specified amount of time.in any case, new business models that telephone companies follow in responding to internet userrequirements may differ somewhat from ways local telephone service traditionally has been packaged andpriced. federal and state officials should recognize the need for flexibility in this area and allow any reasonabletrials to go forward, preferably with minimal regulatory delay.notes1. the estimated cost of deploying a ubiquitous broadband network in the united states ranges from $1,000 to $2,000 per business andresidential access line depending on how much fiber and associated optoelectronics is deployed in telephone company feeder plant. see, forexample, egan, bruce l. 1994. ''building value through telecommunications: regulatory roadblocks on the information superhighway,"telecommunications policy 18(8):580œ583.2. of the 120 executives surveyed by deloitte & touche, 25 percent were with telecommunications companies; 24 percent were withbroadcast or cable television firms; 21 percent were with consumer electronics or personal computer manufacturers; 14 percent were withpublishing/advertising firms; and 16 percent were with entertainment companies. fiftyfive percent of the respondents were chairs,presidents, or chief executive officers, 30 percent were executive vice presidents or vice presidents, and the remaining 30 percent were seniormanagers. see deloitte & touche. 1995. interactive multimedia age ii: report on the 12question update survey. deloitte & touche, may1995.3. ibid., pp. 2œ3.4. see rappaport, alfred. 1986. creating shareholder value: the new standard for business performance. free press, new york, pp. 19œ45.5. see hackel, kenneth s., and joshua livnat. 1992. cash flow and security analysis. business one, irwin, new york, pp. 138œ214.6. see kraushauer, jonathan m. 1995. infrastructure of the local operating companies aggregated to the holding company level. federalcommunications commission, washington, d.c., april.7. see fisher, john c., and robert h. pry. 1971. technology forecasting and social change. american elsevier publishing company, newyork.8. because there are time lags between capital expenditures on various technologies depicted in table 1 and their actual deployment, andadditional lags between the deployment of those technologies and incremental sales/earnings growth that should result from their availability,it was appropriate to compare network investment/shareholder returns forbuilding the nii: will the shareholders come? (and if they don't, will anyone really care?)55the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.1988œ94 with the federal communications commission's technology deployment data, which is currently available for the 1989œ93 periodonly.9. the discounted dividend model holds that p = e(d)/(k  g) where p = price of the stock; e(d) = expected dividend to be paid in the next year (which is a function of earnings multiplied by thepercentage of net income that the firm pays out as dividends); k = the firm's cost of equity capital, which includes interest rates on riskfree treasury bonds plus a riskpremium that compensates investors for risk incurred by holding that companies stock; and g = growth in dividends, which is a function of the firm's return on equity multiplied by the percentage offuture net income that the firm is expected to retain and reinvest.see bodie, zvi, alex kane, and alan j. marcus. 1989. investments. irwin, homewood, ill., pp. 473œ480.10. see yanis, steven r., and thomas j. lee. 1995. "the regional holding companies are more than plain old telephone companies,"telecommunications services, oppenheimer & company inc., january 26, p. 8.11. increased earnings volatility adds to the risk of owning a share of stock, because it raises the probability that the company in questionmay not be able to pay future dividends or, for that matter, sustain daytoday business operations. see bodie, zvi, alex kane, and alan j.marcus. 1989. investments. irwin, homewood, ill., pp. 130œ143.12. wefa group. 1995. economic impact of deregulating u.s. communications industries. wefa group, burlington, mass., february, p.29.13. anderson, christopher. 1995. "the internet survey," the economist, july 1, p. 18.14. see egan, bruce l. 1994. "building value through telecommunications: regulatory roadblocks on the information superhighway,"telecommunications policy 18(8):580œ583.15. see governali, frank. 1995. weekly industry and valuation report, first boston, june 16.building the nii: will the shareholders come? (and if they don't, will anyone really care?)56the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.8the electronic universe: network delivery of data, science,and discoverygregory bothun, university of oregonjim elias, us west communicationsrandolph g. foldvik, us west communicationsoliver mcbryan, university of coloradoabstractwhen technologists discuss the "information highway," their conversations usually revolve around highendtechnologies such as "broadband" and ''asynchronous transfer mode" (atm). unfortunately, this focus on theexclusive use of very high bandwidth solutions fails to reckon with the limitations confronting many learninginstitutions, and especially most of the k12 environment, in gaining access to the "highway" or even exploringtheir options.this paper describes a set of advanced networking tests that were conducted in oregon and colorado during1994 and early 1995. highspeed atm capabilities were effectively merged with readily available t1basedconnectivity to deliver affordable information highway capabilities to k12 educational institutions. innovativecollaborations between the university and k12 level served to accelerate the introduction of the new technologyto the k12 level. a range of application capabilities were effectively delivered over the integrated atm/t1based infrastructure. multimedia workstations were used for video to the desktop, desktop videoteleconferencing, and access to multimedia servers via mosaic and the world wide web.the tests that were conducted, and the results and trends, are covered in detail in this paper. these findingsare then used to suggest technology deployment models for the next 5 to 7 years.statement of the problemthe concept of "network everywhere" is a goal that is achievable through cooperation between education,industry, telecom carriers, and the government. but why is this important? its importance lies in a paradigm shiftin how the individual interacts with and views the world. almost 300 years ago, the philosopher john amoscomenius penned these words: "it is lamentable, utterly unjust and insulting that while all men are admitted togod's theatre, all are not given the chance of looking at everything."this is a description of an inclusive philosophy of education that has never really been implemented.instead, education has focused intensely upon the exclusive theme of "educating each according to his or herneeds." ironically, k12 education is now in a position of reviving this old idea through the use of newtechnology: the highspeed network. highspeed network connectivity will provide the k12 classroom with avast array of resources in all disciplines.is access to highspeed networking a reasonable goal for the k12 classroom? unfortunately, the highmedia profile of fiber optics, high bandwidth applications, and advanced communications technologies hasdistracted many institutions from exploring other options for access. this is the case with most of the k12environment. there may be other ways for them to deal with some of the limitations confronting them. this isparticularly true in most of the western states, where low population densities and correspondingly highdeployment costs prevail. these challenges created an opportunity to explore the limits and possibilities ofdefining a highspeed atm trial network for the educational community that could be integrated with the existingthe electronic universe: network delivery of data, science, and discovery57the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.geographically dispersed public network. this trial environment offered an opportunity to explore existing biasesabout fiber optic and highspeed networks, overcome the constraints of distance and dispersed populationcenters, and bridge the ever widening educational gap between technological "haves" and "have nots."education and lifelong learning are primary keys to societal success on both a personal and a national level.as our informationoriented society continues to move rapidly forward, tremendous pressures are exerted onschools at every level of learning. the rate of information acquisition in learning and scientific inquiry isastounding, making it extremely difficult for teachers and school districts to keep up. the problem is particularlydifficult for both rural and urban schools with limited enrollments, diverse student populations, and constrainedbudgets. many students in these types of schools are being taught advanced topics by minimally qualifiedteachers with outdated textbooks. these students are placed at a decided disadvantage if they advance to thecollege level and are forced to compete with students coming from schools with advanced curricula based onmore current information. worse yet, certain subjects may not even be taught in some schools due to lack ofresources or expertise.the trial activities described in this paper support the premise that these shortcomings can be addressed bymeans of a highspeed network to integrate communications transport at a variety of speeds and bandwidths andeffectively partner universitylevel experts with teachers and students in the k12 system. such a network, bothtechnological and human, can allow for a more efficient delivery of information and curricular material at boththe university and k12 levels.beginning in april 1994 and extending through march 1995, us west engaged in a set of technical trials ofasynchronous transfer mode (atm) technology in western oregon and in boulder, colorado. partnering withseveral leading universities and a number of other organizations, these trials explored issues surrounding the useof advanced networking technologies in combination with existing network services. issues that were addressedincluded network platforms and architectures, along with an understanding of the types of applications thatwould use such an advanced mix of networking technology. many planners assumed that highend nicheapplications associated with such things as supercomputers would predominate. however, we soon realized thattrial participants were placing a strong emphasis on the extension of advanced technologies to secondary schools.even as that trend began to emerge, some felt that while extension of advanced technologies to secondaryschools might be technically feasible, economic and social factors would make such applications unworkable.innovative work conducted separately by the university of oregon and the university of colorado provedthe skeptics wrong. advanced capabilities were in fact extended to a number of secondary schools in bothoregon and colorado with encouraging results. this paper discusses the experiments that were performed, thetrends observed, and subsequent plans made for followon activities. insights gained are used to project abaseline of technologies that could be deployed over the next 5 to 7 years. we believe that these results shouldinfluence the deployment of advanced technology to secondary classrooms and could serve as a model foreffective future cooperation between universities and k12 schools.backgroundus west announced a multiphase atm strategy in october 1993.1 key elements of this strategy includedsmall, scalable atm switches flexibly and economically deployed in a distributed architecture, as was done inthe western oregon and boulder, colorado, trials. the results were positive, and us west subsequentlyannounced availability of an atmbased cell relay service offering in its 14state region in january 1995.2experimentation in oregon was conducted in conjunction with the oregon joint graduate schools ofengineering as part of "project nero" (network for education and research in oregon).3,4 five widelydispersed graduatelevel engineering schools (oregon state university, oregon graduate institute, university oforegon, portland state university, and oregon health sciences university) and several state office buildingswere linked together via a network of atm switches and associated oc3c and ds3 lines in several major citiesin western oregon. in addition, connectivity was also extended to a teacher training workshop at school districtheadquarters in springfield, oregon. experimentation in boulder, colorado,5 was conducted in conjunction withthe electronic universe: network delivery of data, science, and discovery58the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the university of colorado6 and included three government laboratories, four industrial laboratories, a publiclibrary, four public schools, and even a private residence.analysis of primary trendslans at all locationsall major trial participants had an extensive imbedded base of legacy lans. for most large organizations,the lan environment has expanded rapidly over the past 10 years. for example, less than 10 years ago thecampus of the university of oregon in eugene had only one ethernet lan serving about 50 host computers.today the campus has more than 80 lans spread across 70 buildings supporting 6,000 host computers. acomplex infrastructure of lans, bridges, and routers has evolved over the intervening years to support thesenetworks, which have become an integral part of the university of oregon environment.the level of sophistication of these campus lans continued to increase as the trial progressed. typicalcampus configurations eventually included a variety of atm switches providing gateways to remote locationsand connectivity between central routers. direct atm connectivity was extended to workstations and hosts toallow them to communicate directly over atm using native ip per rfc 15777 as well as to distributed atmhubs supporting 10 baset and 100 mbps ethernet for devices using lan emulation.8based on this high level of sophistication, one might make the mistaken assumption that lans are thereforetoo sophisticated for school districts to deploy. however, we found that the trial school district and librarylocations had numerous lans already in place. for example, the school district headquarters in springfield,oregon, has an ethernet lan supporting a number of highend workstations. a macintosh lab in a school inalbany, oregon, has a mix of local talk and ethernet lans supporting 65 macintosh devices. the fourparticipating middle schools and high schools in boulder, colorado, all have ethernet lans supporting a mix ofdevices. even the boulder public library has an ethernet lan with a sophisticated set of routers and serverssupporting local and remote access to online card catalogues and banks of cdroms. while the existence oflans may not be typical for all k12 schools, their presence at our trial participant locations is an importantindicator of future direction. local area networks are quickly becoming a plugandplay item that routinelycomes with offtheshelf workstations and can be installed by the local school administrator, the local librarystaff, the home user, or (in the case of the issaquah school district near seattle) by the students.ip networking and mixandmatch telecomin addition to the universal presence of local area networks, all trial participants accessed the network viarouters and tcp/ip. typical highend configurations utilized a router to access the public wide area network viaeither a ds3based (45 mbps) or an oc3cbased (155 mbps) atm user network interface (uni). the routeroften worked in conjunction with local atm switches as previously discussed.existing distributed internet infrastructures in both oregon and colorado are typically based on tcp/iprouters that are interconnected with t1 lines running at 1.5 mbps. the university trial customers used atm toconstruct new internet infrastructures still based upon tcp/ip routers, but with 155 mbps rather than 1.5 mbpsinterconnectivity. connectivity at 155 mbps was then extended to a central campus hub where it was distributedin turn to a few individual desktop computers. by using atm with routers and tcp/ip, the trial participantswere able to gain a 100fold increase in raw speed and aggregate carrying capacity without changing their basicprotocol structure. one might assume that highend transport options would preclude the participation ofsecondary schools; however, this did not prove to be the case. the use of ipbased networking provided theability to cost effectively bring secondary schools into the networked environment.routers emerged as the universal anchors to enable the new atm transport option without affectingexisting applications and infrastructures. routers support most types of network connectivity, providing theability to transparently mix and match various types of networking options in different parts of the network. withthe electronic universe: network delivery of data, science, and discovery59the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the central dominance of ipbased routers, telecommunications transport became a transparent commodity to bemixed and matched based on costperformance analyses for each individual location.using this extreme level of flexibility, the trial participants proceeded to deploy atm in their backbonenetworks and their central locations where high aggregations of traffic were important, while extending trialconnectivity to outlying schools and libraries via less expensive t1 or frame relay links. all of the secondaryschools were in fact brought into the new environment via lowercost t1based telecommunications links in theexisting copperbased public switched network. the universal use of ipbased routers to support all types oftelecommunications transport allowed the creation of sophisticated "internets" employing the complete spectrumof telecommunications options from low speed to high speed. this is an extremely important trend to consider aswe move forward into the next 5 to 7 years.multimedia workstationsall trial participants made extensive use of multimedia workstations. these workstations provided theability to receive audio/video broadcasts at the desktop, to engage in desktop video teleconferencing, and toaccess multimedia servers. initial testing was primarily with highend sgi or sun workstations with multicastrouting capabilities built into their unix kernels. one could easily assume that using highend workstationswould preclude the participation of secondary schools. however, this was not the case. in some situations theexpedient option of loaning highend workstations to the individual schools was adopted so that proofofconcept work could proceed. as the trial proceeded, other less expensive terminals became available, such as xwindow terminals and avcapable macintoshes. the use of multimedia workstations from the secondaryschools to the university environments was an extremely important trend observed during the trial, along withthe increasing availability of affordable multimediacapable workstations.broadcast video to the desktopthe broadcast of educational video directly to the desktop was accomplished via mbone (multicastbackbone) capabilities.9,10,11 mbone originated from an effort to multicast audio and video from internetengineering task force (ietf) meetings.12 by late 1993 mbone was in use by several hundred researcherslocated at approximately 400 member sites worldwide. the mbone is a virtual multicast13,14,15 network that islayered on top of portions of the physical internet. the network is composed of islands that can directly supportip multicast (such as ethernet lans), linked by virtual pointtopoint links called "tunnels." more recently,several vendors have supplied native ip multicast routing protocols. multicast routing information wasdistributed using pim (protocol independent multicast).16,17 desktop video delivery was initially accomplishedvia the mbone tool "nv," with later migration to a newer video tool called "vic" that supported not only nv butalso other video encodings and standards such as h.261 and jpeg.mbone has been routinely used for some time to broadcast various events to individuals' desks via theinternet. examples include "nasa select," the nasa inhouse cable channel broadcast during space shuttlemissions, and "radio free vat," a community radio station. "internet talk radio" is a variation on thistechnique to conserve network bandwidth by making prerecorded programs available as data files to be retrievedvia ftp and then played back on the local workstation or other appropriate devices as desired. this can have apowerful impact on the inclusion of current, nearly uptotheminute information in curriculum development.programming as varied as talks by larry king and the "geek of the week" are made available via thistechnique.18these broadcast services have been significantly expanded recently via the internet multicasting service(ims), which broadcasts live multicast channels on the internet such as satellitebased programming likemonitor radio and world radio network, networkbased programming such as cbc news and soundbytes,and live links into the national press club, the u.s. congress, and the kennedy center for the performing arts.19the electronic universe: network delivery of data, science, and discovery60the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the oregon universities used these capabilities to broadcast computer science colloquia out over theinternet. they found that bandwidth as low as 56 kbps was adequate for broadcast of the image of an instructor,while 500 to 750 kbps was required for proper transmission of text images with handpointing motions. theyalso extended experimentation to the actual teaching of an engineering statistics class to several registeredstudents as an alternative to attendance, using specially equipped video classrooms.broadcast of various events via mbone proved to be very popular at the four secondary schools incolorado. every time the space shuttle flew they tapped into the 24houraday mbone broadcasts of the livespace shuttle activities and monitored the shuttle flight with great interest. a window was brought up on theworkstation screen on which the students could watch live pictures of the space shuttle as the mission proceeded.in november 1994 the students monitored the "prince charles video event" from los angeles via mbone. inaddition, movies precompressed via mpeg were broadcast out over the network. in december, an mpegcompressed portion of a star trek movie was successfully broadcast out at 30 frames/s via schools' t1connectivity and was then decompressed at the receiving workstations. based on this success, it has beenproposed that mpegcompressed university courses could be delivered to desktop workstations at secondaryschools with minimal t1based access capabilities. experiences in colorado showed that only 128 kbps wasgenerally needed for typical mbone broadcasts, while fullframe 30 frame/s vhs quality video functioned verywell via mpeg compression at t1 speeds. one of the more creative uses of mbone broadcast capabilities wasan "end of boulder trial party" broadcast out over the internet from a residence directly connected to an atmswitch. a 3weekold baby at the party was given the distinction of being the "youngest person to ever do aninternet broadcast."desktop video teleconferencingclosely related to the delivery of video to the desktop is the use of desktop workstations for videoteleconferencing. desktop video teleconferencing emerged as a strong trend in both the oregon and the bouldertrials. mbone proved itself to be extremely useful and versatile for desktop teleconferencing and collaboration,as did inperson software from sgi and cuseeme software for macintosh computers from cornell university.other similar software packages are also becoming more readily available on the market. in general, this type ofsoftware supports simultaneous conferencing with as many as four or five other people, along with the ability toshare application and whiteboard windows.these capabilities were used extensively and successfully in colorado on a daily basis for four or fivewayconferences. t1based connectivity was adequate for this type of conferencing, although more efficientcompression by the associated software would be helpful. dr. mcbryan used the inperson software to teachclasses at the four boulder secondary schools. he networked with the four public schools, each of which had ansgi indy workstation in a classroom. the students then clustered around the indy workstation while dr.mcbryan taught the class from his home. he taught classes on how to write world wide web (www) homepages so that the schools could come online with their own servers. all four schools subsequently came onlinewith their own servers based on what they had learned in these online classes. this was an effectivedemonstration of cooperative work between universities and secondary schools via the network. the capabilitywas so successful that the schools wanted to obtain a largescreen projection capability to make it easier forstudents to participate and to see the screen. it is significant to note again that this was done entirely with t1based connectivity on the existing copperbased public infrastructure as well as fiber optics.in october 1994 a similar experiment was conducted in oregon at a demonstration of the "21st centuryclassroom" for the itec expo's "computer office and automation show" in the portland convention center. agroup of students from grant high school in portland participated in a live audio/video exchange with faculty inthe physics department of the university of oregon. during this exchange the students were shown how to use asoftware package called mosaic and were then able to "sit in" on a physics lecture delivered at the university oforegon. the students had full twoway audio/video capabilities. they could see the lecturer and the otherstudents and could ask questions during the lecture.the electronic universe: network delivery of data, science, and discovery61the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.other colorado applications included the ability for a student ill at home to interact with teachers and otherstudents. similar capabilities were used at the college level for offhours consultation between instructor andstudents, with video consultation as an enhanced alternative to the current electronic mail procedure. all of theseapplications have direct value in delivering more timely and customized learning to students in dispersedlocations.multimedia servers, mosaic, and the world wide webthe final trend is closely related to the use of desktop workstations for receipt of teleconferencing and videobroadcasts. desktop access to multimedia servers was an extremely important area of emphasis in both trials andwas successfully extended to the secondary schools. the national center for supercomputing applications(ncsa) at the university of illinois developed an environment called "ncsa mosaic"20,21 that enables widearea networkbased information discovery and retrieval using a blend of distributed hypermedia, hierarchicalorganization, and search functionality. ncsa has developed mosaic client software for x window systems,macintosh, and microsoft windows. ncsa mosaic's communications support is provided courtesy of the cernworld wide web (www) project's common client library.22 no longer isolated within the academiccommunity, the www has emerged as one of the internet's most popular technologies. commercial companiesnow advertise the availability of their "home pages," associated software is readily available, and commercialbookstores are stocked with numerous basic howto books on mosaic and the www.as used in the oregon and boulder trials, the www servers were accessed via the internet and controlled aset of selfpaced courses that contained a mixture of video, audio, and text. a student would take a particularcourse via a multimedia workstation, with the course consisting of multiple windows on the screen, includingfull motion film clips with audio, along with associated interactive text. the multimedia courseware is deliveredover the www and is received and viewed using a www reader such as netscape or mosaic.examples of technology usesthe combination of ip networking capabilities, multimedia workstations, video to the desktop, videoteleconferencing, and multimedia servers was used in creative and effective ways in both trials. during thesummer of 1994 a teacher training workshop was held for springfield (oregon) school district science teacherson the use of multimedia xterminals as network information retrieval machines. as previously mentioned, t1connectivity was extended to the springfield school district headquarters as part of the atm trial in oregon.two xterminals were located there, connected to the university hosts and used by the teachers to practiceinformation access as well as the preparation of their own multimedia courseware.during the 1994 fall and 1995 winter term at the university of oregon, three physics classes were doneentirely using mosaic. the students in those classes were able to retrieve the class notes offline and hence wereno longer compelled to incessantly take notes during the lecture. as judged by improved performance on exams,this situation seems to have led to better student comprehension of the material. the mosaic browser allowed thestudents to personally annotate each lecture page and thus "take notes on the notes." the use of class newsgroupsand email allowed the students to work collaboratively on some class assignments as well as provide moreuseful feedback to the instructor than from the traditional lecture format. normal lectures were still given, but thelecture material itself was in the form of mosaic pages projected to the class on an lcd projector. in the winter1995 term, in a course on alternative energy, students working together in selforganized teams successfullyconstructed www pages of their own in response to a homework assignment to create a virtual energy companyand advertise it to the world. this effort and the course can be accessed at http://zebu.uoregon.edu/phys162.html.in midnovember 1994, dr. bothun went to the cerrotololo interamerican observatory in chile to makeobservations with the telescopes located there. even though the telescope and classroom were separated by10,000 miles, successful interactive classroom sessions did occur and the class was able to follow along with theobservations as the data were reduced and then placed on the www in quasireal time. these data can be foundthe electronic universe: network delivery of data, science, and discovery62the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.at http://zebu.uoregon.edu/fp.html. the delivery of scientific data, in this case digital imaging from a telescope,to the k12 classroom through the mosaic interface is perhaps the most powerful educational application ontoday's internet. as part of the nero project, the university of oregon is hoping to get its pine mountainobservatory (located at an elevation of 7,000 feet, 26 miles se of bend, oregon) on a t1 link to the internet forpurposes of delivering access to a scientific instrument and data acquisition directly to a networked k12classroom. another example of this kind of activity is provided by the recent space shuttle mission. dr. bothunwas part of the scientific experiment on board the endeavor and kept several www pages updated aboutmission progress and the scientific data being obtained. digital optical photos of the targets that were imagedwith the ultraviolet telescope were made available to the internet audience so that interested followers of themission could get some idea of the kind of object currently being imaged by the shuttle astronauts. this effortcan be seen at http://zebu.uoregon.edu/uit.html.as previously mentioned, a class on how to create www home pages was taught via teleconference to fourboulder, colorado, secondary schools. subsequent to receiving this remote training, each school set up its ownwww server on the internet. the students then proceeded to set up their own www home pages, learned howto scan pictures into the computer, and then made their pictures available online via the internet. the studentsused the network heavily as part of school projects. as part of an oceanography project, they used www"search engines" to research material and make personal contacts with researchers around the country. individualstudents made personal contacts with professionals at scripps institute in la jolla, california, and withresearchers at sea world. they also successfully accessed supercomputer centers at the national center foratmospheric research and at noaa. in addition, a teacher at one of the high schools developed a chemistryclass for remote delivery to students at middle schools. in conjunction with this class, some of the high schoolstudents mentored middle school students via the network. it is precisely this kind of new, dynamic interactionbetween experts, information resources, and students at various levels that precipitates the kind of quantum leapthe experience of learning can undergo.finally, a number of demonstrations were held in conjunction with efforts associated with the council ofgreat city schools, a nonprofit organization representing 50 of the nation's largest urban public school systems.these demonstrations included deployment of a technical model for a highperformance classroom at thecouncil's annual conference of technology directors in denver, colorado, in may 1994, and the design anddeployment of a "21st century classroom" for a computer office and automation show at the portlandconvention center in october 1994.analysis and forecast: a look to the futurea number of followon projects are either proposed or under way as a result of these successfulexperimental activities. these followon activities, when taken in conjunction with the various experiments, areexcellent examples to look to for help in forecasting networking trends for educational institutions over the next5 to 7 years.the schools in boulder, colorado, used their workstations and associated connectivity in an impressive andsuccessful way. the teachers and students adapted very well to the advanced technology, perhaps even moresuccessfully than anticipated. in 1993 the voters approved a bond issue to allocate funds for provision of a t1line to every school in the boulder valley school district by early 1996. within a year the schools will berunning their own 51school network, routinely doing the things that were on the cutting edge during the trialperiod. the ability of schools to actually know and experience the value of these networked services facilitatessolicitation of taxpayer support for efforts of this type. when the community sees direct value, then the resourcesand synergies required to build these networks (both technological and human) emerge more easily.the schools have a talented pool of teachers and students who will continue with the work begun during thetrial. dr. mcbryan has continued to work with the schools and the public library and is currently testing deliveryof highly compressed, 30 frame/s, fullframe, vhsquality video to the schools. the work conducted in bouldercould serve as a model of cooperation between secondary schools and local universities, extending universityexpertise into the secondary schools and "bootstrapping" them into the information age.the electronic universe: network delivery of data, science, and discovery63the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in october 1994 the university of colorado was awarded a department of commerce nationaltelecommunications and information administration grant to develop the boulder community network (bcn).bcn collects information resources from all available sources in boulder county and makes them available tothe whole population through the world wide web. many residents have access through work or school,including schools at the elementary and secondary levels. to widen access, information kiosks have beendeployed at strategic locations throughout the county, including libraries and public buildings. the seniorcitizens center and several similar institutions were provided with both computer platforms and network access.over 50 community organizations now supply and update information on a regular basis and participate in bcnmanagement and development. commercial information resources are also available on bcn, including acountywide restaurant menu guide and shopping information. bcn may be accessed at http//bcn.boulder.co.us/.dr. mcbryan was one of the founding members of bcn, is principal investigator on the ntia grant, andhas been bcn technical director from the outset. as a result the bcn information resources were rapidly madeavailable to the t1connected schools, resulting in interesting curriculum developments. as examples, spanishclasses are translating some bcn home pages into spanish and a history class plans an online archive of tapingsof early boulder residents, both available to all residents via bcn. the marriage of community network andschool internet connectivity represents a new paradigm for outreach in education.looking forward into the future and taking a 5 to 7year view, the boulder participants see a number of keytrends developing. in today's environment, students must leave their classrooms and go to a computer lab. in thefuture the computers must be located directly in the classrooms. cdroms will be installed on every desktop,allowing individualized instruction programs to be developed with each student having his or her own programsand subjects. even today schools can acquire a performa 630 with a cdrom for $1,200 or less. videobroadcast and video on demand will mix freely with cdrom capabilities. all three media and their supportingnetwork topologies will bring highquality educational and current affairs materials into the classroom.in october 1994 the university of oregon received one of 100 ntia grants from the u.s. department ofcommerce. the university in partnership with 15 other lane county agencies will use the $525,000 grant tocreate the lane education network. this highspeed computer network will connect the participating agencies'information systems in a seamless, fully accessible community network. the grant will finance demonstrationprojects, electronic classrooms, and distancelearning, offcampus classes. agencies in the partnership includethree local school districts. in one of those school districts, voters recently passed a $37.5 million bond levy, $3million of which is designated for instructional technology.in conjunction with these associated activities in oregon a cooperative effort is currently being proposedcalled "the online universe: network delivery of multimedia resources in science to the k12 classroom andthe general public." building on the successful experiments outlined above, the project will focus in severalareas. multimedia educational materials will be developed in the areas of astronomy and the environmentalsciences. these materials will be delivered via internet connectivity at the t1 level between the university oforegon, the oregon museum of science and industry (omsi), and individual science classrooms at grant highschool in portland and at springfield and thurston high schools in springfield, oregon. all materials will beorganized and accessible through the mosaic interface. in addition, mbone connectivity will be used at thesesites for desktop conferencing and live interactions between students, university professors, k12 teachers, andmuseum educational staff. the existing multimedia internet classroom on the university of oregon campus willbe used to develop a new classroom at omsi to serve as a training area for k12 science teachers in the eugeneand portland areas.along the same lines but on a larger geographic scale, the council of great city schools is proposing an"urban learning network" to link urban high schools in 14 states to deliver useful information and practicaleducational services to teachers and students in the realworld environments of urban community high schools.the proposed list of services and applications to be provided should sound familiar at this point: video ondemand, interactive multimedia courses, distance instruction or teleconferencing, electronic field trips, and fullyequipped smart classrooms.these various activities show that electronic networks offer the possibility of forming learning communitiesof educators and learners who can share and develop courseware. the process can be highly interactive, andfeedback from the students can be integrated into improving the content. most importantly, athe electronic universe: network delivery of data, science, and discovery64the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.network of this type facilitates educational outreach activities between higher education, k12 school systems,libraries, and science museums. these new, networkbased partnerships will allow any expert's resources to bedelivered to a larger audience with the aim of improving education through the delivery of real data in aninterface that promotes learning as a discovery process. such capabilities will place vast amounts of informationat the fingertips of educators, students, and ordinary citizens.summary and recommendationsthis trial and our collective observations support the expectation that traditional barriers between student,teacher, and professional researcher will erode and be replaced by a learning community of individuals who arecollectively interested in a particular subject. herein lies the paradigm shift. the keeper of the knowledge will nolonger be the individual k12 teacher or the professor. rather, students will now be able to access and interactwith a diverse and distributed knowledge base. professional researchers can make their real data available viainteractive networks to this learning community, and science can be taught as a discovery process rather than acollection of facts "known" about the physical world. this approach duplicates what the professional scientistdoes and can go a long way toward improving science education.these technological breakthroughs and educational links between universities and k12 schools underscorethe value of education at every level. the electronic superhighway will increasingly allow experts to share theirresearch and foster an excitement for learning among interested students, and in this process they can also helpk12 teachers navigate quickly through the vast and often times confusing information highway. thus, throughthis partnership, the "looking at everything" ideal becomes increasingly real with seemingly endless possibilitiesfor exploration, dialogue, and learning.through the activities of this trial two unexpected insights were gained relative to the communicationstechnologies: (1) the power and pervasiveness of tcp/ip as a common denominator for educational networkingwas dramatically underscored; and (2) the scalability, robustness, and aggregation strengths of atm switchingmake it an ideal fabric for a public switched network to support educational and developing multimediaapplications. also, the fact that the ip networking of education communities is already in place and expandable,and that applications were deliverable at t1 bandwidths (allowing for the use of existing copperbasedinfrastructures), means that k12 and learning communities can begin accelerated access to the informationhighway today, migrating to other exclusively fiber networks over time. although ds3 and other higher speednetwork capabilities are often desirable, it is currently neither feasible nor financially possible to extend thesecapabilities universally to all schools. until the value of these networks is comprehended, "affordability" remainsa circular argument. the network demonstrated in this trial comes within the reach of anyone when properlyscaled, and its incredible value can be experienced by learners and educators.society's challenge will be to provide schools and universities with the resources and support they need tohave full and equal electronic access to information in this new era. such a partnership extends also to businessand industry, whose success ultimately depends on qualified personnel entering the work force. this trialhighlighted the possibilities that will emerge when a learning community discovers the value that can be derivedfrom the effective application of technology and communications networking. communities of interest existeverywhere in human society. each community must individually shape the vision and define the value thatthese networks can deliver. there is no "one size fits all solution." the trial demonstrated that the technologiesare very flexible, with many of them already in place to deliver the "value" the users want. value has a directrelationship to "affordability." obtaining funding for these technological advancements and communicationsnetworks will require creativity from the communities that build them.notes1. wallace, b. 1993. "us west plots atm course with planned expansion," network world, october 18, pp. 28 and 31.the electronic universe: network delivery of data, science, and discovery65the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.2. greene, t. 1995. "us west places its atm services cards on the table," network world, february 6, p. 23.3. owen, s. 1993. "network for engineering and research in oregon," proposal submitted to education division of national aeronauticsand space administration, september 23.4. foldvik, r., d. meyer, and d. taylor. 1995. "atm network experimentation in the state of oregon," proceedings of the ieeeinternational conference on communications, june.5. foldvik, r., and o. mcbryan. 1995. "experiences with atm: the boulder perspective," proceedings of the ieee internationalconference on communications, june.6. research supported in part by nsf grand challenges application group grant asc9217394 and by nasa hpcc group grantnag52218.7. laubach, m. 1994. "ietf draft rfc: classical ip and arp over atm," rfc 1577, january.8. "lan emulation over atm: draft specification," atm forum contribution 940035.9. macedonia, m., and d. brutzman. 1993. "mbone, the multicast backbone," naval postgraduate school, december 17,taurus.cs.nps.navy.mil:pub/mbmg/mbone.hottopic.ps.10. macedonia, m., and d. brutzman. 1994. "mbone provides audio and video across the internet," computer 27(4):30œ36.11. casner, s. 1993. "frequently asked questions (faq) on the multicast backbone (mbone)," may 6, available via anonymous ftp fromvenera.isi.edu:/mbone/faq.txt.12. casner, s., and s. deering. 1992. "first ietf internet audiocast," acm sigcomm computer communication review, july, pp. 92œ97.13. deering, s. 1989. "host extensions for ip multicasting," rfc 1112, august.14. moy, j. 1993. "multicast extensions to ospf," ietf draft, july.15. deering, s. 1988. "multicast routing in internetworks and extended lans," proceedings of the acm sigcomm 1988.16. deering, s., d. estrin, d. farinacci, v. jacobson, c. liu, and l. wei. 1995. "protocol independent multicast (pim): motivation andarchitecture," internet draft, draftietfidmrpimarch00.txt, november.17. deering, s., d. estrin, d. farinacci, v. jacobson, c. liu, and l. wei. 1995. "protocol independent multicast (pim): protocolspecification," internet draft, draftietfidmrpimspec01.ps, january 11.18. release 1.0. 1994. "the internet multicasting service: ted turner, watch out!," 94(2):10.19. refer to http://town.hall.org/radio/live.html.20. andreessen, m. 1993. "ncsa mosaic technical summary," may 8, marca@ncsa.uiuc.edu.21. valauskas, e. 1993. "onestop internet shopping: ncsa mosaic on the macintosh," online, september, pp. 99œ101.22. powell, j. 1994. "adventures with the world wide webs, creating a hypertext library information system," database, february, pp.59œ66.the electronic universe: network delivery of data, science, and discovery66the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.9an sdtv decoder with hdtv capability: an allformatatv decoderjill boyce, john henderson, and larry pearlsteinhitachi america ltd.this paper describes techniques for implementing a video decoder that can decode mpeg2 highdefinition(hd) bit streams at a significantly lower cost than that for previously described highdefinition video decoders.the subjective quality of the pictures produced by this ''hdcapable" decoder is roughly comparable to currentdbs delivered standarddefinition (sd) digital television pictures. the hdcapable decoder can decode sd bitstreams with precisely the same results as a conventional standarddefinition decoder. the mpeg term mainprofile at main level (mp@ml) is also used to refer to standarddefinition video in the sequel.the decoder makes use of a preparser circuit that examines the incoming bit stream in a bitserial fashionand selectively discards coded symbols that are not important for reconstruction of pictures at reduced resolution.this preparsing process is performed so that the required channel buffer size and bandwidth are bothsignificantly reduced. the preparser also allows the syntax parser (sp) and variablelength decoder (vld)circuitry to be designed for lower performance levels.the hdcapable decoder "downsamples" decoded picture data before storage in the frame memory, therebypermitting reduction of the memory size. this downsampling can be performed adaptively on a field or framebasis to maximize picture quality. experiments have been carried out using different methods for downsamplingwith varying results. the combination of the preparser and picture downsampling enables the use of the sameamount of memory as used in standard definition video decoders.the decoder selects a subset of the 64 dct coefficients of each block for processing and treats theremaining coefficients as having the value zero. this leads to simplified inverse quantization (iq) and inversediscrete cosine transform (idct) circuits. a novel idct is described whereby the onedimensional 8pointidct used for decoding standard definition pictures is used as the basis for performing a reduced complexityidct when processing highdefinition bit streams.a decoder employing the above techniques has been simulated using "c" with hdtv bit streams, and theresults are described. normal hdtv encoding practices were used in these experiments. the bit streams weredecoded according to the concepts described herein, including preparsing, the effects of reduced memory sizes,simplified idct processing, and the various associated filtering steps. the preparser and resampling result in acertain amount of prediction "drift" in the decoder that depends on a number of factors, some of which are underthe control of the encoder. those who have viewed the resulting images agree that the decoding discussed in thispaper produces images that meet performances expectations of sdtv quality.the hdcapable video decoder, as simulated, can be expected to be implementable at a cost onlymarginally higher than that of a standard definition video decoder. the techniques described here could beapplied to produce hdcapable decoders at many different price/performance points. by producing a range ofconsumer products that can all decode hdtv bit streams, a migration path to full hdtv is preserved whileallowing a flexible mix of video formats to be transmitted at the initiation of digital television service.there is an ongoing policy debate about sdtv and hdtv standards, about a broadcast mix of bothformats, and about how a full range of digital television might evolve from a beginning that includes eithersdtv or hdtv or both. this paper offers technical input to that debate, specifically regarding consumerreceivers that could decode both sdtv and hdtv digital signals at a cost only marginally higher than that ofsdtv alone.there are at least two areas of policy debate in which these issues are relevant:an sdtv decoder with hdtv capability: an allformat atv decoder67the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.1. what is the right mix of hdtv and sdtv as the digital service evolves over time? there are a variety ofintroduction scenarios for digital television, ranging from hdtv only, to sdtv only, to various mixesof the two. to preserve the hdtv broadcast option no matter how digital television service isintroduced, sdtv receivers must be able to decode the hdtv signal. it is assumed here that sdtvreceivers with such hdtvdecoding capability are both practical and cost effective. it is thus entirelypractical to preclude sdtvonly receivers. therefore, the introduction of sdtv would not prevent laterintroduction of hdtv because fully capable digital receivers would already be in use.2. how quickly can national television system committee (ntsc) broadcasting be discontinued? thereceiver design approach described herein can be applied to lowcost settop boxes that permit ntscreceivers to be used to view digital television broadcasts. the existence of such decoders at low cost isimplicit in any scenario that terminates ntsc broadcast.cost and complexity of fullresolution hdtv decoder componentsthe single most expensive element of a video decoder is the picture storage memory. a fully compliantvideo decoder for u.s. hdtv will require a minimum of 9 mbytes of ram for picture storage. an hdtvdecoder will also require at least 1 mbyte of ram for channel buffer memory to provide temporary storage ofthe compressed bit stream. it can be expected that practical hdtv video decoders will employ 12 to 16 mbytesof specialty dram, which will probably cost at least $300 to $400 for the next few years and may be expectedto cost more than $100 for the foreseeable future.the idct section performs a large number of arithmetic computations at a high rate and represents asignificant portion of the decoder chip area. the inverse quantizer (iq) performs a smaller number ofcomputations at a high rate, but it may also represent significant complexity.the sp and vld logic may also represent a significant portion of the decoder chip area. at the speeds anddata rates specified for u.s. hdtv, multiple sp/vld logic units operating in parallel may be required in a fullhdtv decoder.cost reductions of hdtv decoderthis section describes several techniques that can be applied to reduce the cost of an hdcapable decoder.the following decoder subunits are considered: picture storage memory, preparser and channel buffer, sp andvld, inverse quantizer and inverse discrete cosine transform, and motion compensated prediction. thediscussion refers to figure 1, which is a block diagram of a conventional sdtv decoder; and figure 2, which isa block diagram of an hdcapable decoder. the blocks, which appear in figure 2 but not in figure 1, have beenshaded to highlight the differences between an hdcapable decoder and a conventional sd decoder.picturestorage memoryas described in ng (1993), the amount of picturestorage memory needed in a decoder can be reduced bydownsampling (i.e., subsampling horizontally and vertically) each picture within the decoding loop. note infigure 2 that residual or intradata downsampling takes place after the idct block and prediction downsamplingis done following halfpel interpolation blocks. the upsample operation shown in figure 2 serves to restore thesampling lattice to its original scale, thus allowing the motion vectors to be applied at their original resolution.although this view is functionally accurate, in actual hardware implementations the residual/intra downsamplingoperation would be merged with the idct operation, and the prediction downsample operation would be mergedwith the upsample and halfpel interpolation. in an efficient implementation the upsamplešhalfpel interpolationšdownsample operation is implemented by appropriately weighting each of the reference samples extractedfrom the (reduced resolution) anchor frame buffers to form reduced resolution prediction references.an sdtv decoder with hdtv capability: an allformat atv decoder68the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 block diagram of a conventional sdtv video decoder.the weights used in this operation depend on the fullprecision motion vectors extracted from the coded bitstream.experiments have shown that it is important that the prediction downsampling process is near the inverse ofthe upsampling process, since even small differences are made noticeable after many generations of predictions(i.e., after an unusually long gop that also contained many pframes). there are two simple methods: downsample without filtering (subsample), and upsample using bilinear interpolation; and downsample by averaging and upsample without filtering (sample and hold).for both of these methods the concatenated upsampledownsample operation is identity when motionvectors are zero. both methods have been shown to provide reasonable image quality.for the residual/intra downsampling process it is possible to use frequency domain filtering in lieu of spatialfiltering to control aliasing. frequency domain filtering is naturally accomplished by "zeroing" the dctcoefficients that correspond to high spatial frequencies. note that the prediction filtering may introduce a spatialshiftšthis can be accomodated by introducing a matching shift in the residual/intra downsampling process, or byappropriately biasing the motion vectors before use.when processing interfaced pictures, the question arises as to whether upsampling and downsamplingshould be done on a field basis or on a frame basis. fieldbased processing preserves the greatest degree oftemporal resolution, whereas framebased processing potentially preserves the greatest degree of spatialresolution. a bruteforce approach would be to choose a single mode (either field or frame) for all downsampling.a more elaborate scheme involves deciding whether to upsample or downsample each macroblock on afield basis or frame basis, depending on the amount of local motion and the highfrequency content. field basedprocessing is most appropriate when there is not much highfrequency content and/or a great deal of motion.framebased processing is most appropriate when there is significant highfrequency content and/or little motion.one especially simple way of making this decision is to follow the choice made by the encoder for eachmacroblock in the area of field or frame dct and/or field or framemotion compensation, since the same criteriaan sdtv decoder with hdtv capability: an allformat atv decoder69the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2 block diagram of a lowcost hdtv video decoder.may apply to both types of decisions. although field conversion is not optimal in areas of great detail, suchas horizontal lines, simulations show that if a single mode is used, field is probably the better choice.in mpeg parlance, sdtv corresponds to main level, which is limited to 720 × 480 pixels at 60 hz, for atotal of 345,600 pixels. u.s. atv allows pictures as large as 1920 × 1080 pixels. sequences received in thisformat can be conveniently downsampled by a factor of 3 horizontally and a factor of 2 vertically to yield amaximum resolution of 640 × 540, a total of 345,600 pixels. thus the memory provided for sdtv would beadequate for the reducedresolution hd decoder as well. it would be possible to use the same techniques with asmaller amount of downsampling for less memory savings.in a costeffective video decoder, the channel buffer and picturestorage buffers are typically combined intoa single memory subsystem. the amount of storage available for the channel buffer is the difference between thememory size and the amount of memory needed for picture storage. table 1 shows the amount of picturestoragememory required to decode the two highdefinition formats with downsampling. the last column shows theamount of free memory when a single 16mbit memory unit is used for all of the decoder storage requirements.this is important since costeffective sdtv decoders use an integrated 15mbit memory architecture. thememory not needed for picture storage can be used for buffering the compressed video bit stream.table 1 reduced resolution decoder memory usageactivehorizontalactiveverticalhscalefactorvscalefactordownsampledhorizontaldownsampledvertical$framesstoreddownsampledmemoryrequiredfreememorywith 16mbitsdram19201,08032640540312,441,6004,335,61612807202264036025,529,60011,247,616720480šššš312,441,6004,336,616note: shaded row reflects sdtv format for reference.an sdtv decoder with hdtv capability: an allformat atv decoder70the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.as indicated in table 1, the 1920 × 1080 format is downsampled by 3 horizontally and 2 vertically. thisresults in efficient use of memory (exactly the same storage requirements as mp@ml) and leaves a reasonableamount of free memory for use as a channel buffer.the natural approach for the 1280 × 720 format would be to downsample by 2 vertically and horizontally.this leaves sufficient free memory that the downconverter would never need to consider channel buffer fullnesswhen deciding which data to discard.after decoding of a given macroblock, it might be immediately downsampled for storage or retained in asmall buffer that contains several scan lines of fullresolution video to allow for filtering before downsampling.the exact method of upsampling and downsampling is discussed below; it can greatly affect image quality, sinceeven small differences are made noticeable after many generations of predictions.1 the upsampling anddownsampling functions are additional costs beyond that for an sd decoder.the general concept of reducing memory storage requirements for a lowercost hdtv decoder is known inthe literature. this paper adds preparsing and new techniques for performing downsampling and upsampling.preparser and channel buffera fully compliant hdtv decoder requires at least 8 mbits of highspeed ram, with peak output bandwidthof 140 mbytes/sec for the channel buffer. with the use of a preparser to discard some of the incoming databefore buffering, the output bandwidth can be reduced to a peak of 23 mbytes/sec and the size of the channelbuffer can be reduced to 1.8 to 4.3 mbits. (the lower number is required for mp@ml and the higher number isthe amount left over in the sdtv 16mbit memory after a 1080 × 1920 image is downsampled by 3 horizontallyand 2 vertically, including the required 3 frames of storage.)the preparser examines the incoming bit stream and discards less important coding elements, specificallyhighfrequency dct coefficients. it may perform this data selection while the dct coefficients are still in therunlength/amplitude domain (i.e., while still variablelength encoded). the preparser thus serves two functions: it discards data to allow a smaller channel buffer to be used without overflow and to allow reduction of thechannelbuffer bandwidth requirements. it discards runlength/amplitude symbols, which allows for simpler realtime sp and vld units.the preparser only discards full mpeg code words, creating a compliant but reduced data rate and reducedquality bit stream. the picture degradation caused by the preparsing operation is generally minimal whendownsampled for display at reduced resolution. the goal of the preparser is to reduce peak requirements in laterfunctions rather than to significantly reduce average data rates. the overall reduction of the data rate through thepreparser is generally small; for example, 18 mbps may be reduced to approximately 12 to 14 mbps.the channel buffer in a fully hdtv decoder must have highoutput bandwidth because it must output a fullmacroblock's data in the time it takes to process a macroblock. the preparser limits the maximum number ofbits per macroblock to reduce the worstcase channel buffer output requirement. the peak number of bitsallowed per macroblock in u.s. hdtv is 4608; this requires an output bandwidth of 140 mbytes/sec eventhough the average number of bits per macroblock is only 74. the preparser retains no more than 768 bits foreach coded macroblock, thereby lowering the maximum output bandwidth to 23 mbytes/sec, the same as formp@ml.the preparser also removes highfrequency information (i.e., it does not retain any nonzero dctcoefficients outside of a predetermined lowfrequency region). preparsing could remove coefficients after a prespecified coefficient position in the coded scan pattern, or it could remove only those coefficients that will not beretained for use in the idct. this reduces the total number of bits to be stored in the channel buffer.in addition to discarding data to limit bits per coded macroblock and highfrequency coefficients, the preparser also alters its behavior based on the channel buffer fullness. the preparser keeps a model of bufferoccupancy and removes coefficients as needed to ensure that the decreased size channel buffer will neveran sdtv decoder with hdtv capability: an allformat atv decoder71the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.overflow. as this buffer increases in occupancy, the preprocessor becomes more aggressive about the amount ofhighfrequency dct coefficient information to be discarded.this decoder management of its own buffer is a key difference between the enhanced sdtv decoder and a"normal" sdtv decoder. in a "normal" encoder/decoder combination, the encoder limits the peak data rate tomatch the specifications of the decoder buffer; it is the responsibility of the encoder to assure that the decoderbuffer does not overflow. in the enhanced sdtv decoder outlined in this paper, the decoder can accept bitstreams intended for a much larger buffer (i.e., an hdtv bit stream) and can perform its own triage on theincoming bit stream to maintain correct buffer occupancy.2this preparser is an additional cost over a standalone sd decoder, but the cost and complexity are lowsince it can run at the relatively low average incomingbit rate. the preparser is significantly less complex thana fullrate sp and vld because of its slower speed requirement and because it parses but does not have toactually decode values from all of the variable length codes.3syntax parser and variablelength decoderthe computational requirements for the sp and vld units of the downconverter are substantially reducedby implementing a simplified bitserial preparser as described above. the preparser limits the maximumnumber of bits per macroblock. it also operates to limit the number of dct coefficients in a block by discardingcoefficients after a certain number, thus reducing the speed requirements of the sp and vld units.at the speeds and data rates specified for u.s. hdtv, multiple sp/vld logic units operating in parallelmay be required. the preparser limits the processing speed requirements for the hd downconverter to sdtvlevels. thus the only additional requirement on the sp/vld block for decoding hdtv is the need for slightlylarger registers for storing the larger picture sizes and other related information, as shown in figure 2.inverse quantization and inverse discrete cosine transformreduced complexity inverse quantizer (iq) and inverse discrete cosine transform (idct) units could bedesigned by forcing some predetermined set of high frequency coefficients to zero. mpeg mp@ml allows forpixel rates of up to 10.4 million per second. u.s. atv allows pixel rates of up to 62.2 million per second. it istherefore possible to use sdtvlevel iq circuitry for hdtv decoding by ignoring all but the 10 or 11 mostcritical coefficients. some of the ignored coefficients (the 8 × 8 coefficients other than the 10 or 11 criticalcoefficients) will probably have already been discarded by the preparser. however, the preparser is notrequired to discard all of the coefficients to be ignored. the preparser may discard coefficients according tocoded scan pattern order, which will not, in general, result in deleting all of the coefficients that should beignored by later processing stages.processing only 11 of 64 coefficients reduces the iq computational requirement and significantly decreasesthe complexity of the idct. the complexity of the idct can be further reduced by combining the zeroing ofcoefficients with the picture downsampling described above.idct circuitry for performing 8 × 8 idct is required for decoding sd bit streams. a common architecturefor computing the twodimensional idct is to use an engine that is capable of a fast, onedimensional, 8pointidct. if the sc idct engine were used when decoding hd bit streams, it could perform about three 8pointidcts in the time of an hdtv block. thus the sd idct can be used to compute the idct of the first threecolumns of coefficients. the remaining columns of coefficients would be treated as zero and thus require idctresources.a specialpurpose idct engine would be implemented to do the row idcts. it would be especially simplesince five of the eight coefficients would always be zero, and only two or three output points would have to becomputed for each transform. note that only four rows would have to be transformed if no additional filteringwere to be performed prior to downsampling.an sdtv decoder with hdtv capability: an allformat atv decoder72the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.for blocks in progressive frames, or that use field idcts, coefficients might be selected according to thefollowing pattern (retained coefficients are represented by "x"):for blocks that use frame dcts on an interfaced picture, we might discard coefficients with the followingpattern:this pattern of retained coefficients maintains temporal resolution represented by differences between thetwo fields of the frame in moving images.motion compensated prediction (mcp)assume that the anchor pictures4 have been downsampled, as described above. the data bandwidth to themotion compensation circuitry is thus reduced by the same factor as the storage requirement. as describedabove, motion compensation is accomplished by appropriately interpolating the reduced resolution picturereference data according to the values of the motion vectors. the weights of this interpolation operation arechosen to correspond to the concatenation of an antiimaging upsampling filter, bilinear halfpel interpolationoperation (depending on the motion vectors), and optional downsampling filter.complexity comparisonsa block diagram of the hdcapable video decoder is shown in figure 2. this can be compared withfigure 1, "sdtv video decoder block diagram," to identify the additional processing required over an sddecoder. complexity comparisons between a fullresolution hd decoder, sd decoder, prior art hddownconverter,5 and the hdcapable decoder described in this paper are shown in table 2. the total costs of thehd downconverter/sd decoder are not significantly greater than the cost of the sd decoder alone.prediction driftin mpeg video coding a significant portion of the coding gain is achieved by having the decoder constructa prediction of the current frame based on previously transmitted frames. in the most common case, theprediction process is initialized by periodic transmission of all intracoded (iframes). predicted frames (pframes) are coded with respect to the most recently transmitted i or pframes. bidirectionally predicted framesan sdtv decoder with hdtv capability: an allformat atv decoder73the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 2 complexities of new hdcapable decoder and prior decodersfunctionatsc standardhdtvprior art hddownconverternew hdcapabledecodermp@ml (sdtv)preparser< 10,000 gates 19mbits/sec (partialdecode)channel buffer sizebandwidth8 mbits140 mbytes/sec bw8 mbits140 mbytes/sec bw1.8 to 4.3 mbits23 mbytes/sec1.8 mbits23 mbytes/sectotal offchipmemoryrequirements96 mbits specialtydram16 mbits dram + 8mbits specialty dram16 mbits dram16 mbits dramsp/vld93m coefficients/sec93 m coefficients/sec15.5 m coefficients/sec15.5 m coefficients/secidct1.5 m blocks/sec1.5 m simple blocks/sec (hd) +240 k full 8 × 8 blocks/sec (sd)1.5 m halfcomplexity simpleblocks/sec (hd) +240 k full 8 × 8blocks/sec (sd)240 k full 8 × 8blocks/sec (sd)upsample/downsampleš1,000 to 2,000 gates?1,000 to 2,000gates?šdecodes hdtvyesyesyesnodecodes sdtvyesnoyesyes(bframes) are coded with respect to the two most recently transmitted frames of types i or p.let the first pframe following some particular iframe be labeled p1. recall that the decoder describedabove downsamples the decoded frames before storage. thus, when p1 is to be decoded, the stored iframe usedfor constructing the prediction differs from the corresponding fullresolution prediction maintained in thedecoder. the version of p1 produced by the hdcapable decoder will thus be degraded by the use of animperfect prediction reference, as well as by the preparsing and downsampling directly applied to p1. the nextdecoded pframe suffers from two generations of this distortion. in this way the decoder prediction "drifts" awayfrom the prediction maintained by the encoder, as pframes are successively predicted from one another. notethat the coding of bframes are successively predicted from one another. note that the coding of bframes doesnot contribute to this drift, since bframes are never used as the basis for predictions.prediction drift can cause visible distortion that changes cyclically at the rate of recurrence of iframes. theeffect of prediction drift can be reduced by reducing the number of pframes between iframes. this can be doneby increasing the ratio of bframes to pframes, decreasing the number of frames between iframes, or both. as apractical matter, however, special encoding practices are neither needed nor recommended. experiments haveshown that reasonable hd videoencoding practices lead to acceptable quality from the hdcapable decoderdescribed here.simulation resultstest images were chosen from material judged to be challenging for both hdtv and sdtv encoding anddecoding. progressive sequences were in the 1280 × 720 format; interlaced sequences were 1920 × 1024 and1920 × 1035 (we did not have access to 1920 × 1080 material). the images contained significant motion andincluded large areas of complex detail.some of the bit streams used for testing were encoded by the authors using their mpeg2 mp@hlsoftware; others were provided by thomson consumer electronics. decoding at hdtv was done using theauthors' mpeg2 mp@hl software. the hdcapable decoder algorithms described above were simulated in"c" and tested with the hdtv bit streams.an sdtv decoder with hdtv capability: an allformat atv decoder74the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the simulation included accurate modeling of all the processes described here. the bit stream was preparsed; channel buffer and picture memory sizes, idct processing, and idct coefficient selection were all inaccord with these explanations; upsampling and downsampling were applied to use the original motion vectors.the resulting hdtv and downconverted images were examined and compared. although thedownconverted images were of discernibly lower quality than the decoded fullhdtv images, observers agreedthat the downconversion process met performance expectations of "sdtv quality."conclusionsthis paper describes an enhanced sdtv receiver that can decode an hdtv bit stream. the enhancementsneeded to add hd decoding capability to an sd decoder are modest, even by consumer electronics standards. ifall receivers included the capabilities described here, an introduction of sdtv would not preclude laterintroduction of hdtv because fully capable digital receivers would already be in use. the techniques describedin this paper also permit design of lowcost, settop boxes that would permit reception of the new digital signalsfor display on ntsc sets. the existence of such boxes at low cost is essential to the eventual termination ofntsc service.referenceslee, d.h., et al. goldstar, "hdtv video decoder which can be implemented with low complexity," proceedings of the 1994 ieeeinternational conference on consumer electronics, tuam 1.3, pp. 6œ7.ng, s., thomson consumer electronics, "lower resolution hdtv receivers," u.s. patent 5,262,854, november 16, 1993.notes1. the "predictions" mentioned here are the pframes within the gop sequence. the downsampling and preparsing processes alter the imagedata somewhat, so that small errors may accumulate if unusually long gop sequences contain many pframes. bframes do not cause thiskind of small error accumulation, and so good practice would be to increase the proportion of bframes in long gop sequences or to usegops of modest length. receiver processing techniques can also reduce any visible effects, although they are probably unnecessary.2. in this paragraph, the buffering operation and its associated memory are treated as distinct from the picturestorage memory. thisdistinction is useful for tutorial purposes, even though the two functions may actually share the same physical, 16mbit memory module.3. note that the macroblock type, coded block pattern, and runlength information must be decoded.4. anchor pictures are i and pframes in the mpeg gop sequence. the downsampling that has been applied to them by the decodingtechniques described here means that the motion vectors computed by the encoder can no longer be directly applied.5. the term "downconverter" as used here applies to hardware that reduces the full hdtv image data to form an sdtvresolution picture.the appropriately enhanced sdtv decoder described here inherently includes such an hdtv "downconverter."an sdtv decoder with hdtv capability: an allformat atv decoder75the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.10nii and intelligent transport systemslewis m. branscomb and jim kellerharvard universityjust as the highway metaphor has driven the vision of advanced communications infrastructure, so also areadvances in communications pushing the vision of our transportation systems future. how do these two systemsinterrelate? what policy issues does this relationship raise?over the past halfcentury, the u.s. highway system has advanced regional and national economicdevelopment by enhancing access to markets for goods, services, and people. it has also provided direct qualityoflife benefits by providing easier access to both work and leisure. now the traditional model for surfacetransportation is reaching its limit. in many areas systems are at or beyond capacity. building new or biggerroads is not the answer because of space and budget constraints and environmental concerns. instead, the focusof transportation experts is now on promoting more efficient use of existing capacity. the central theme of theseefforts is more efficient integration of existing transportation components through the use of informationtechnology.the department of transportation's intelligent transportation systems (its) program is the focal point forcoordinating the development of a national its system. the current program was funded in 1991 under theintermodal surface transportation efficiency act of 1991. in the federal its program user services are brokendown into 7 areas consisting of 29 applications (table 1). anticipated benefits of its include reduced travel timeand pollution and increased traveler safety and convenience. although there are clear publicinterest benefits toits services, the publicsector role in its development will in many respects be indirect. consistent withactivity in other components of the national information infrastructure (nii), development will be a complex mixof public and private interactions.the federal role in the development of its will be primarily that of an enabler, just as it is for the nii. itssystems involving fixed facilities will initially develop locally and regionally, whereas commercial products foruse in vehicles and in trip planning will be sold nationally and internationally. the department of transportationwill need to take a leadership role in a variety of system architecture and standards development, deployment,and coordination issues to ensure that all of these systems will come together smoothly into a coherent nationalsystem. standards will be needed to address interoperability at different layers in the its architecture and issuesof data compatibility across systems. data collected locally must integrate smoothly into national systems,requiring broad agreement on data definitions and coding. local system developers will have to agree on whatinformation is important, what it will be called, and how it will be recorded.the standardization issues are complex relative to the traditional telecommunications environment, as theyspan a broader array of technologies and systems. at the same time, the environment for standardization isrelatively weak. telecom standards evolved with a common platform and a stable (indeed regulated) competitiveenvironment, but its will consist of heterogeneous systems and a relatively independent set of players. as withother areas of computing and communications, players will face a conflicting set of incentives to seekstandardization, including incentives to differentiate (for competitive advantage and to lock in market share) andto seek interoperability (to expand the market for services and to lower the costs for customers to migrate to theirsystem).nii and intelligent transport systems76the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 1 its user servicesuser service areaapplicationtravel and transportation managementenroute driver informationroute guidancetraveler services informationtraffic controlincident managementemissions testing and mitigationtravel demand managementdemand management and operationspretrip travel informationride matching and reservationpublic transportation operationspublic transportation managementenroute transit informationpersonalized public transitpublic travel securityelectronic paymentelectronic payment servicescommercial vehicle operationscommercial vehicle electronic clearanceautomated roadside safety inspectiononboard safety monitoringcommercial vehicle administrative processeshazardous material incident responsefreight mobilityemergency managementemergency notification and personal securityemergency vehicle managementadvanced vehicle control and safety systemslongitudinal collision avoidancelateral collision avoidanceintersection collision avoidancevision enhancement for crash avoidancesafety readinessprecrash restraint deploymentautomated vehicle operationnote: user services as defined by the national its program plan, its america, march 1995.like standards development, many other aspects of its development and deployment will involve new andcomplex coordination issues. the technologies for many its applications have already been developed, but avariety of nontechnical issues will determine how and when these applications become widely available.liability, intellectual property, security, privacy, and data ownership issues will all influence the commitment ofprivate firms to deploying its services and the interest of users in adopting them.those in government and in industry responsible for planning and coordinating its developmentsrecognize that some of the communications systems, such as those supporting traffic flow control and addressingemergency situations, will require realtime, quick response capability with high reliability. such systems willprobably have to be dedicated to its applications. at the other extreme, trip planning and many other offlineapplications can surely be supported by the nation's general purpose data networksšthe nii. it is unclear,however, where the boundaries lie and what combination of architecture, commercial strategies, public services,and regulatory constraints will serve to define this relationship. in short, is the its a domainspecific applicationof the nii? or is its a special environment whose information systems support is specialized and only looselycoupled, through data sharing, with the nii?intelligent transportation systemsthe mission of the its program is to improve the safety, efficiency, and capacity of the country's surfacetransportation system through the use of information technology. the its program is coordinated within dot,nii and intelligent transport systems77the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.bringing together each of the major program areas within the department. the systems integration and realtimeinformation requirements of its include not only managing the flow of realtime information to and fromindividual vehicles, but also the seamless coupling of different modes of transportation. in addition, manycommercial applications for consumer motorists are anticipated. these include traffic management, vehicletracking, electronic toll collection, augmentation of driver perception, automated emergency intervention, realtime traffic and travel information, trip planning, and eventually automation of at least parts of the drivingprocess, such as collision avoidance.eventually the its will call for a highintegrity, realtime network system. this system will take inputsfrom highway sensors, from vehicle global positioning system (gps) systems, and from other informationgathering systems. this information will be continuously compiled in a system of databases with a dynamicmodel of the local, regional, and national highway system, and will be used to provide realtime information onoptimum routes, based on such factors as least time and fuel economy. although there is a higher degree ofhomogeneity among its applications and data requirements relative to the nii, the realtime requirements,security, and scale of some parts of the its make it one of the most challenging nii applications.the government rolelike the nii initiative of which it is a component, its is at the forefront of changes in how the federalgovernment will relate to the states and the private sector. in the postcold war economy, agency initiatives arebeing driven by a new set of requirements. these include an active role for industry in project design, selection,and execution; reliance on private investment; mechanisms to ensure commercial adoption; complexmanagement structures; and a federal role in consensus building.1 its fits well into this model. the itsprogram, and in particular the definition of its technical requirements, have been developed with activeparticipation from its america, a broadbased industry consortium, and with the understanding that its willrely heavily on private investment and state and local deployment.the its program was established by congress through the intelligent vehicle highway systems (ivhs)act,2 part of the intermodal surface transportation efficiency act (istea) of 1991. this legislation authorizesthe secretary of transportation to conduct a program to research, develop, operationally test, and promoteimplementation of its systems.3 though the secretary is authorized in these areas, it is clearly indicated in thelegislation that this role is intended to be cooperative and facilitatory. the secretary is directed to seek transfer offederally owned or patented technology to the states and the private sector. the secretary is also directed toconsult with the heads of the commerce department, the environmental protection agency, the nationalscience foundation, and other agencies, as well as to maximize the role the private sector, universities, and stateand local governments in all aspects of the program.4the management of its is conducted by the joint program office (jpo) in the secretary of transportation'soffice. the jpo manages its activities in all areas of the department, as well as working actively to coordinateand build consensus among its stakeholders. as directed by istea, dot has produced a planning document,the ''ivhs strategic planšreport to congress" (december 1992), outlining the program activities, roles, andresponsibilities. dot is also funding the development of a national its architecture. this development is beingconducted by private consortia under the direction of dot and is still in process. currently, the architecture isloosely defined, identifying its system elements and estimating the communication and informationrequirements of each. it does not specify the technical characteristics of component interfaces that will berequired for interoperability.to examine federal policy in the development of its, it is perhaps easiest to break down the areas forfederal activity. as its is broad in scope, this covers quite a bit of ground and will vary between different itsapplication areas. for example, in the case of emergency fleet management, technology development will occurunder a more traditional procurement model. many traveler information services may be provided bycommercial information services, and onboard capabilities will require the participation of auto manufacturersand wireless telecommunications companies. this participation could, in principle, be achieved either throughmarket incentives or by mandating compliance.nii and intelligent transport systems78the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.opportunities for federal activity in the development of its can be broken down into the following areas: promoting the development and testing of its technologies and applications. this is an area in which dothas been actively engaged. a number of demonstration projects are now under way, most addressingmultiple its applications. these projects should go beyond traditional r&d to include a diffusioncomponent aimed at assisting future implementors. the projects should focus not only on feasibility, butalso on measuring benefits and costeffectiveness. fostering an environment that will engage state and local agencies and private firms in the deployment ofits services. the availability of its services depends largely on the efforts of state and local agencies andprivate firms to implement them. some its services will come about on their own, based on theircommercial viability. in other cases, regulatory mandates or publicsector procurement will be needed tostimulate product development and availability. dot must clearly articulate its its goals to allow systemsdevelopers time to anticipate these requirements. promoting the development and adoption of standards and a national its architecture that will ensure thatlocal and regional systems coalesce into a coherent national system and allow integration of u.s.components into international systems. facilitating the deployment of an interoperable set of itsimplementations represents many challenges. its will likely be deployed as a set of semiautonomous, localimplementations and will involve stakeholders in many branches of the computing, communications, andtransportation industries. for these systems to coalesce smoothly into a national system, broad consensuswill need to be achieved early on among product and service developers and local implementors. it will alsorequire a robust architecture that can expand in both scope and scale to accommodate unanticipated servicerequirements.in pursuit of compatibility between local and regional systems, dot has initiated the national itsarchitecture development program. this program was established in september 1993 and is requirementsdriven, based on the 29 its user services (see table 1). the architecture seeks to define the systems componentsand component interactions for a national its. given the breadth of interoperability issues related to its, such ahigh level of coordination will be necessary, but it is not without risk. key challenges facing planners will beensuring acceptance and conformance and designing a system that will be able to support unanticipatedrequirements and applications. to ensure flexibility, the architecture should be based on informationrequirements and interfaces, not specific technologies or the particular way in which the service is provided. forexample, when placing a phone call, a user is typically indifferent to whether the call goes over copper, fiber, orany other medium, as long as basic performance standards are met.fulfilling these architectural guidelines will require political as well as technological prowess. it will bedifficult to achieve consensus in the diverse environment of its developers and implementors. the mostimportant means will be to keep the process open and participatory. this is the approach dot has taken so far,and privatesector participation has been strong. another opportunity to achieve conformance will be theprocurement lever. to some degree, dot will also be able to tie conformance to federal funding. however, notall local and regional implementations will receive federal funds, and while it is possible to mandateconformance as a requirement for general dot highway funding, these funds may be shrinking and yield lessinfluence.such a topdown approach is interesting to consider in contrast to other infrastructure development efforts.part of the success of the internet, for example, has been its ability to evolve from the middle out, to be able torun over unanticipated types of networking technology and to support unanticipated applications. the itsapplication areas are highly defined at present, but technology planning is historically an inexact science, andmuch will rest on how willing users are to pay.to the extent that the highway trust fund constitutes a major source of federal investment that can be tiedto its objectives, the its is different from the larger information infrastructure of which it is a partšthenational information infrastructure. however, the states have a much bigger role in its than in the nii, wherestate roles are largely confined to telecommunications regulationsšauthority that will likely be substantiallycurtailed in the future. so whereas the nii requires more sensitive relationships between the federal governmentand private industry, the its requires a threeway collaboration between a federal department with significantnii and intelligent transport systems79the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.financial resources; states with the major responsibility for both investment and operations; and private industry,without which neither level of government can realize its objectives. identifying and supporting those its application areas that provide clear benefit to the public but would nototherwise be deployed by the private sector. its represents a broad array of application areas. some of theseare consumeroriented conveniences, while others are public goods that will extend transportation andnontransportation benefits to the public at large. federal its investment should be made on an applicationbyapplication basis, based on the anticipated service benefits and the potential for private investment (withand without federal support). clarifying areas of law that may inhibit the adoption of its services, including product and service liability,data ownership, privacy, and security. the move toward its services brings up a number of yettobedefined areas of law. some of these issues are specific to transportation, and others are more broadlyrelevant in the emerging information society. one area is liability. as transportation systems become morecomplex and new systems, technologies, and organizations influence the movement of people and materials,liability concerns begin to touch a larger array of players. it is currently unclear to what extent informationservice and systems providers will be responsible for failures ranging from inconvenience to catastrophe. dopublicsector service providers risk liability? these ambiguities are a potential deterrent to the developmentand availability of its products.its has also produced concerns about the abuse of personal privacy. the information gathering potential ofits is tremendous. its systems may be able to identify where individuals (or at least their vehicles) are andwhere they have been. who owns this information, and who will have access to it? will there be limits on itsuse? will service providers be able to sell this information to marketers?the dot role in its will differ fundamentally from the earlier dot role in developing the federal highwaysystem, and it anticipates challenges federal policymakers will face in other sectors in the future. other areas offederal activity, such as environmental management, housing, health care, education, and social services, arebecoming more information intensive. its offers a proving ground for federal efforts to coordinate thedevelopment of intelligent infrastructure. a critical factor in managing this effort will be to maximize the extentto which systems can be leveraged across sectors. if its can be leveraged in ways that will decrease the marginalcost of developing infrastructure for other areas, it can help to jumpstart these efforts and offset or reduce itscosts.its infrastructure requirementsmost its applications have an inherent communications component, and in the minds of many, its bringsa vision of dedicated communications networks. however, in looking at the infrastructural elements, thecommunications element, while pervasive, is generally not beyond the scope of anticipated nii capabilities.instead, it appears that its communications requirements may be met largely through generalpurposeinfrastructure. specialpurpose infrastructure needed to support its can be broken down into five general areas: dedicated networks. this category is intended to identify those its applications that may not be supportedby generalpurpose nii infrastructure. the communications requirements of its applications can begrouped into three categories. the first is autonomous standalone systems that will not be part of a largernetwork, for example, intersection collision avoidance systems, which will communicate betweenapproaching vehicles and intersectionbased sensors. the next category is applications that will likely besupported by commercially available communications services, for example, pretrip or enroute travelinformation that will be a lowbandwidth, "bursty" application and that will likely be served by otherwiseavailable circuit or packetbased wireless or wireline services. the third includes applications that willrequire the support of a dedicated, specialpurpose network. the dedicated network category will includeonly this last set of applications.nii and intelligent transport systems80the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. roadside element deployment. sensors and broadcast devices that will be required along roadways to senseor communicate with vehicles. broadcast devices may signal vehicles concerning road conditions, speedlimit, or other factors. roadside elements may also communicate with central servers managing traffic orfreight information, but this will not necessarily require a specialpurpose network. information/database systems. these systems will be the heart of its. shared access will be provided tothese systems to manage information storage and retrieval and processing of its information. service points. these are sitespecific locations at which its services will be administered. examplesinclude electronic payment, weigh inmotion, and emissions testing sites. onboard vehicle components. displays, sensors, and communication devices will be installed in vehicles forthe purpose of supporting its. this includes both autonomous systems (route guidance using gps andonboard cdrom maps) and communicationsdependent systems (collision avoidance).table 2 maps the 29 its applications against these five elements of required infrastructure. this chart is notperfect, as it does not recognize the changing requirements within application areas between early and advancedapplications. it also does not recognize that some applications may be enhanced by access to data gathered by thetraffic control application. despite these limitations, table 2 does point out that a significant component of itsservices may be provided through generalpurpose communications infrastructure. only two applications areidentified as having a dedicated network requirementštraffic control and automated vehicle operation. inparticular, the primary need for a dedicated its network will be for automated vehicle operation, perhaps thefurthest out on the time horizon of the its applications. traffic control is also identified as requiring a dedicatednetwork because of the large proliferation of sensors that will be required in some areas. of course, thesededicated networks may be either owned or leased by transportation agencies.table 2 offers a simplified but practical overview of the systems that will be required to support its. theinformation infrastructure project in harvard's science, technology, and public policy program is currentlyexploring this area in its project on intelligent transportation systems and the national information infrastructure.the project will lead to a workshop in july 1995 that will investigate opportunities for linkages between theseinitiatives, including infrastructure requirements.the conclusion supported by table 2 that most its services will not require a dedicated network is basedon anticipated rather than available nii capabilities. for many applications there are a variety of potentialtechnological solutions, including public atm, frame relay, or smds networks and the internet. the internet isa desirable solution from cost and ubiquity perspectives but is currently lacking in speed, reliability, and security.it is now a "bestefforts" network, with no guarantee of delivery. the internet technical community recognizesthe need to move beyond best efforts service to provide a variety of qualityofservice levels consistent with theneeds of different groups of users and applications.5 efforts are under way to explore the development of qualityofservice parameters, including a reserved bandwidth service. similarly, a variety of systems of encryption arenow being developed and tested to ensure secure communications.in terms of communications services available today, mobile communications are virtually ubiquitous andare becoming more so. mobile phones are virtually free today, and it would be reasonable to expect that they willsoon be bundled with most new cars. exploring the provision of basic its services through mobile cellulartechnologies may offer a lowcost means of developing and testing their marketability.the trend so far in the development of its services has been dedicated private network solutions. costcomparisons made by these early implementors have heavily favored the use of owned versus leasedinfrastructure. however, these estimates may not provide a balanced perspective. analysis of owning versusoutsourcing needs to consider customized rates available for multiyear contracts, as opposed to full tariffs, andanticipate the availability of advanced virtual network services. the cost differential is also being driven by thelower cost of capital faced by public agencies. one means of leveling the playing field in this area is industrialbonds, which would allow private corporations to float taxfree bonds to raise money for public sector projects.nii and intelligent transport systems81the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.it appears that there is considerable room for a larger open network role in provision of its services.realization of this role will require active dialogue between the telecommunications community and the itscommunity. a shared infrastructure approach can offer benefits beyond reduced cost. from the perspective of anational its program, expanding the role of public communications service providers can provide a more stableenvironment for the deployment of its services. the case for owning versus outsourcing will of course varyfrom project to project, depending on the proximity of the site to existing infrastructure and its value as right ofway.nii and intelligent transport systems82the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the debate between dot officials and communications service providers has often painted a blackandwhite picture of the ownership issue. there are possible winwin solutions that recognize the value of roadsiderightofway and explore options in trading access for bandwidth and sharing revenue from excess capacity.in most areas, its networks have been procured based on a predefined architecture. alternatively, statescould purchase services rather than networks. this would allow communications service providers to determinewhat network architecture could most cost effectively meet the service requirements in the context of theiroverall business. this might also place expertise in its communications where it would more easily migrate toserve other areas. if the larger role in its network planning is undertaken by publicsector officials, the resultingexpertise and learning curve benefits will remain local. if this responsibility falls more heavily on the privatesector, other regions may benefit from the providers' expertise. it may also allow services to roll out consistentwith underlying economics, rather than arbitrary politically defined geographic areas.assuring the maximum benefit from shared infrastructure opportunities will require an active role on thepart of dot and other its constituents in articulating its requirements and in participating in nii and internetdevelopment activities. its has been largely absent in reports coming out of the clinton administration'sinformation infrastructure task force (ittf), the focal point for federal coordination on nii activities. the iitfoffers a highlevel platform for dot to articulate its its vision and seek integration with both federal andprivatesector nii development activities.referring back to table 2, the information component is a dominant infrastructural element. people,vehicles, and freight will all move across regions and its deployment areas and will require the integration ofits information across these regions. standardization of data structures will be a critical element in the successof its and needs to be addressed.conclusioninterest in advanced infrastructure is at an unprecedented level. the internet and the nii have beenembraced by both parties and are virtually unavoidable in the media. this offers fertile ground for theadvancement of its on both technical and programmatic fronts. to date, its activity has occurred almostexclusively in the transportation domain. the conclusion that much if not most of the network datacommunications needs of the its can be met by the nii, if it develops into fully accessible services that caninteroperate with all the specialized its subsystems, suggests an important dependency of its on nii outcomes.yet there is no formal mechanism, other than participation in the its america discussions, through whichfederal interests in the nii can be brought together with state interests. such a mechanism needs to be put in place.a task force launched on april 25, 1995, by governors richard celeste and dick thornburgh is respondingto a request by jack gibbons, the director of the office of science and technology policy, to examine howfederalstate relationships might be strengthened or restructured in the area of science and technology. sponsoredby the carnegie commission on science, technology and government, the national governors' association,and the national council of state legislators, the task force will be looking at a number of specific cases,including the intelligent transportation system. out of this work, or perhaps out of the president's nii taskforce, a mechanism needs to emerge for engaging state governments in the architecture and policies for the nii.if this linkage is made through the nii task force, it follows that the department of transportation, togetherwith other departments and agencies concerned with the nation's transportation systems, needs to participateactively in the work of the task force, to ensure that interoperability, security, and other features of the nii areappropriate for the national transportation system's needs.notes1. branscomb, lewis m. "new policies, old bottles," business and the contemporary world.2. in the fall of 1994 the ivhs program was officially renamed the intelligent transportation systems program in recognition of theprogram's multimodal scope.3. intermodal surface transportation efficiency act of 1991, part b, section 6051.nii and intelligent transport systems83the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.4. intermodal surface transportation efficiency act of 1991, part b, section 6053.5. computer science and telecommunications board, national research council. 1994. realizing the information future. nationalacademy press, washington, d.c., may, p. 6.nii and intelligent transport systems84the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.11postnsfnet statistics collectionhanswerner braun and kimberly claffysan diego supercomputer centerabstractas the nsfnet backbone service migrates into a commercial environment, so will access to the only set ofpublicly available statistics for a large national u.s. backbone. the transition to the new nsfnet program, withcommercially operated services providing both regional service as well as crossservice provider switchingpoints, or network access points (naps), will render statistics collection a much more difficult endeavor. in thispaper we discuss issues and complexities of statistics collection at recently deployed global network accesspoints such as the u.s. federal naps.backgroundthe u.s. national science foundation (nsf) is making a transition away from supporting internetbackbone services for the general research and education community. one component of these services, thensfnet backbone, was dismantled as of the end of april 1995. to facilitate a smooth transition to amultiprovider environment and hopefully forestall the potential for network partitioning in the process, the nsfis sponsoring several official network access points (naps) and providing regional service providers withincentives to connect to all three points[]naps are envisioned to provide a neutral, acceptable use policy (aup)free meeting point for networkservice providers to exchange routing and traffic. the three nsfsponsored priority naps are sprint nap, in pennsauken, nj; pacific bell nap, in oakland/palo alto, ca; and ameritech nap, in chicago, il.nsf also sponsors a nonpriority nap in washington, d.c., operated by metropolitan fiber systems.the sprint nap was operational as of november 1994, but the other naps were not yet ready untilmid1995, mainly because sprint was the only nap that did not try to start off with switched asynchronoustransfer mode (atm) technology, but rather began with a fiber distributed data interface (fddi)implementation. in addition, nsf is sponsoring a veryhighspeed backbone service (vbns), based on atmtechnology, to support meritorious research requiring high bandwidth network resources. the vbns represents atestbed for the emerging broadband internet service infrastructure in which all parts of the network will beexperimented with: switches, protocols, software, etc., as well as applications. it will be a unique resource fornetwork and application researchers nationwide to explore performance issues with the new technologies (e.g.,how host systems and interfaces interact with atm components of the wide area network) [1,2].note: this research is supported by a grant from the national science foundation (ncr9119473). this paper has beenaccepted by inet '95 for publication.postnsfnet statistics collection85,the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.box 1 merit notification of the cessation of nsfnet statistics collectionnsfnet performance statistics have been collected, processed, stored, and reported by the meritnetwork since 1988, in the early stages of the nsfnet project. during december 1994, the numberscontained in merit's statistical reports began to decrease, as nsfnet traffic began to migrate to the newnsf network architecture.in the new architecture, traffic is exchanged at interconnection points called network access points(naps). each nap provides a neutral interconnection point for u.s.based and international networkservice providers. once the new architecture is in place, merit will be unable to collect the data needed tocontinue these trafficbased reports. the reports will be discontinued by spring 1995.source: nic.merit.edu/nsfnet/statistics/read.me, january 9, 1995.as the nsfnet era comes to a close, we will no longer be able to rely on what was the only set of publiclyavailable statistics for a large national u.s. backbone (box 1). the transition to the new nsfnet program, withcommercially operated services providing both regional service as well as crossservice provider networkswitching points (nsps), will render statistics collection a much more difficult task. there are severaldimensions of the problem, each with a costbenefit tradeoff. we examine them in turn.dimension of the problemcontractual and logistical issuesin the cooperative agreement with the naps, the national science foundation has made a fairly vaguerequest for statistics reporting. as with the nsfnet program, nsf was not in a position to specify in detailwhat statistics the network manager should collect since nsf did not know as much about the technology as theproviders themselves did. the situation is similar with other emerging network service providers, whoseunderstanding of the technology and what statistics collection is possible is likely to exceed that of nsf. thenaps and nsps, however, at least in early 1995, were having enough of a challenge in getting and keeping theirinfrastructure operational; statistics have not been a top priority. nor do the naps really have a good sense ofwhat to collect, as all of the new technology involved is quite new to them as well.a concern is that the naps will likely wait for more specific requirements from nsf, while nsf waits forthem to develop models on their own. scheduled meetings of community interest groups (e.g., nanog, iepg,fepg, eowg, farnet)3 that might develop statistics standards have hardly enough time for more critical itemson the agenda, e.g., switch testing and instability of routing. the issue is not whether traffic analysis would help,even with equipment and routing problems, but that traffic analysis is perceived as a secondary issue, and there isno real mechanism (or spare time) for collaborative development of an acceptable model.costbenefit tradeoff: fulfill the deliverables of the cooperative agreement with nsf, at the least costin terms of time and effort taken away from more critical engineering and customer service activities.academic and fiscal issuesmany emerging internet services are offered by companies whose primary business thus far has beentelecommunications rather than internet protocol (ip) connectivity. the nap providers (as well as the vbnsprovider) are good examples. traditionally phone companies, they find themselves accustomed to havingreasonable tools to model telephony workload and performance (e.g., erlang distributions). unfortunately, theliterature in internet traffic characterization, both in the analytical and performance measurement domains,indicates that widearea networking technology has advanced at a far faster rate than has the analytical andtheoretical understanding of internet traffic behavior.postnsfnet statistics collection86the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the slower and more containable realms of years ago were amenable to characterization with closedformmathematical expressions, which allowed reasonably accurate prediction of performance metrics such as queuelengths and network delays. but traditional mathematical modeling techniques, e.g., queuing theory, have metwith little success in today's internet environments.for example, the assumption of poisson arrivals was acceptable for the purposes of characterizing smalllans years ago. as a theory of network behavior, however, the tenacity of the use of poisson arrivals, whetherin terms of packet arrivals within a connection, connection arrivals within an aggregated stream of traffic, orpacket arrivals across multiple connections, has been quite remarkable in the face of its egregious inconsistencywith any collected data3,4. leland et al.5 and paxson and floyd6 investigate alternatives to poissonmodeling, specifically the use of selfsimilarity (fractal) mathematics to model ip traffic.there is still no clear consensus on how statistics can support research in ip traffic modeling, and there isskepticism within the community regarding the utility of empirical studies that rely on collecting real data fromthe internet; i.e., some claim that since the environment is changing so quickly, any collected data are only ofhistorical interest within weeks. there are those whose research is better served by tractable mathematicalmodels than by empirical data that represent at most only one stage in network traffic evolution.a further contributing factor to the lag of internet traffic modeling behind that of telephony traffic is theearly financial structure of the internet. a few u.s. government agencies assumed the financial burden ofbuilding and maintaining the transit network infrastructure, leaving little need to trace network usage for thepurposes of cost allocation. as a result internet customers did not have much leverage with their service providerregarding the quality of service.many of the studies for modeling telephony traffic came largely out of bell labs, which had severaladvantages: no competition to force slim profit margins, and therefore the financial resources to devote toresearch, and a strong incentive to fund research that could ensure the integrity of the networks for which theycharge. the result is a situation today where telephone company tables of "acceptable blocking probability" (e.g.,inability to get a dialtone when you pick up the phone) reveal standards that are significantly higher than ourtypical expectations of the internet.we do not have the same situation for the developing internet marketplace. instead we have dozens ofinternet providers, many on shoestring budgets in low margin competition, who view statistics collection as aluxury that has never proven its utility in internet operations. how will statistics really help keep the nap aliveand robust since traffic seems to change as fast as they could analyze it anyway?we are not implying that the monopoly provider paradigm is better, only observing aspects of the situationthat got us where we are today: we have no way to predict, verify, or in some cases even measure internet servicequality in real time.there is some hope that some of the larger telecommunication companies entering the marketplace willeventually devote more attention to this area. the pressure to do so may not occur until the system breaks, atwhich point billed customers will demand, and be willing to pay for, better guarantees and data integrity.costbenefit tradeoff: undertake enough network research to secure a better understanding of the productthe naps sell, without draining operations of resources to keep the network alive. failing that, fund enoughresearch to be able to show that the naps are being good community members, contributing to the"advancement of internet technology and research" with respect to understanding traffic behavior, at the leastcost in terms of time and effort taken away from more critical engineering and customer service activities.technical issueswith deployment of the vbns and naps, the situation grows even more disturbing. the nationalinformation infrastructure continues to drive funding into hardware, pipes, and multimediacapable tools, withvery little attention to any kind of underlying infrastructural sanity checks.and until now, the primary obstacles to accessing traffic data in order to investigate such models have beenpolitical, legal (privacy), logistic, or proprietary.postnsfnet statistics collection87the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.with the transition to atm and highspeed switches, it will no longer even be technically feasible to accessip layer data in order to do traffic flow profiling, certainly not at switches within commercial atm clouds. thenaps were chartered as layer 2 entities, that is, to provide a service at the link layer without regard for higherlayers. because most of the nsfnet statistics reflected information at and above layer 3 (i.e., the ip layer), thenaps cannot use the nsfnet statistics collection architecture 7 as a model upon which to base their ownoperational collection. many newer layer 2 switches (e.g., dec gigaswitch, atm switches) have little if anycapability for performing layer 3 statistics collection, or even looking at traffic in the manner allowed on abroadcast medium (e.g., fddi, ethernet), where a dedicated machine can collect statistics without interferingwith switching. statistics collection functionality in newer switches takes resources directly away fromforwarding of frames/cells, driving customers toward switches from competing vendors who sacrifice suchfunctionality in exchange for speed.costbenefit tradeoff: (minimalist approach) collect at least cost what is necessary to switchingperformance.ethical issuesprivacy has always been a serious issue in network traffic analysis, and the gravity of the situation onlyincreases at naps. regardless of whether a nap is a layer 2 entity or a broadcast medium, internet traffic is aprivate matter among internet clients. most nap providers have agreements with their customers not to revealinformation about individual customer traffic. collecting and using more than basic aggregate traffic counts willrequire precise agreements with customers regarding what may be collected and how it will be used. behavingout of line with customers' expectations or ethical standards, even for the most noble of research intentions, doesnot bode well for the longevity of a service provider.an extreme position is to not look at network header data (which incidentally is very different from userdata, which we do not propose examining) at all because it violates privacy. analogous to unplugging one'smachine from the ethernet in order to make it secure, this approach is effective in the short term but has someundesirable side effects. we need to find ways to minimize exposure rather than surrendering the ability tounderstand network behavior. it seems that no one has determined an ''optimal" operating point in terms of whatto collect, along any of the dimensions we discuss. indeed, the optimal choice often depends on the serviceprovider and changes with time and new technologies.we acknowledge the difficulty for the naps, as well as any internet service provider, to deal with statisticscollection at an already very turbulent period of internet evolution. however, it is at just such a time, markedironically with the cessation of the nsfnet statistics, that a baseline architectural model for statistics collectionis most critical, so that customers can trust the performance and integrity of the services they procure from theirnetwork service providers, and so that service providers do not tie their hands behind their backs in terms ofbeing able to preserve robustness, or forfend demise, of their own clouds.costbenefit tradeoff: accurately characterize the workload on the network so that naps and nspscan optimize (read, maintain) their networks, but at the least cost to these privacy ethics we hold so dear.utility of analysis: examplesthere is no centralized control over all the providers in the internet. the providers do not always coordinate theirefforts with each other, and quite often are in competition with each other. despite all the diversity among theproviders, the internetwide ip connectivity is realized via internetwide distributed routing, which involvesmultiple providers, and thus implies a certain degree of cooperation and coordination. therefore, there is a need tobalance the providers' goals and objectives against the public interest of internetwide connectivity and subscribers'choices. further work is needed to understand how to reach the balance.8šy. rekhterpostnhsfnet statistics collection88the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.as the underlying network becomes commoditized, many in the community begin to focus on higher layerissues, such as the facilities, information, and opportunities for collaboration available on the network. in thiscontext, facilities include supercomputers and workstations, information includes world wide web and gopherservers, and opportunities for collaboration include email and multiuser domains (muds). one could think ofthese three categories as corresponding to three types of communication: machinetomachine, peopletomachine, and peopletopeople.within each category, multiple dimensions emerge: context: static topical, such as cognitive neuroscience research or geographically topical, such as local news; temporal: from permanent resources to dynamically created communication resources used for briefperiods, e.g., distributed classroom lecture or seminar; and geographic distribution: which may require transparency at times and boundary visibility at another.as we build this higher layer infrastructure taking the lower layers increasingly for granted, the need forstatistics collection and analysis is not diminished. on the contrary, it is even more critical to maintain closetrack of traffic growth and behavior in order to secure the integrity of the network. in this section we highlightseveral examples of the benefits of traffic characterization to the higher layers that end users care about.longrange internet growth trackingfew studies on national backbone traffic characteristics exist9, limiting our insight into the nature of widearea internet traffic. we must rely on wan traffic characterization studies that focus on a single or a fewattachment points to transit networks to investigate shorterterm aspects of certain kinds of internet traffic, e.g.,tcp10, tcp and udp11, and dns12.the authors have devoted much attention in the last 2 years to investigating the usefulness, relevance, andpracticality of a wide variety of operationally collected statistics for widearea backbone networks. in particular,we have undertaken several studies on the extent to which the statistics that the nsfnet project has collectedover the life of the nsfnet backbone are useful for a variety of workload characterization efforts. as thensfnet service agreement ends, leaving the r&e portion of internet connectivity in the hands of thecommercial marketplace, we are without a common set of statistics, accessible to the entire community, that canallow even a rough gauge of internet growth. we consider it important to establish a communitywide effort tosupport the aggregation of such network statistics data from multiple service providers, such as that beingdeveloped in the ietf's opstat group 13. we view a consensus on some baseline statistics collection architectureas critical to internet longterm stability.service guarantees and resource reservationanother ramification of the transition of the r&e portion of internet connectivity from the nsfnetservice into a commercial marketplace is the need for a mechanism to compare the quality of service providers.rather than procured via collaborative undertakings between the federal government and academia and industry,services in today's internetworking environments will be market commodities. effective provision of thosecommodities will require the ability to describe internet workload using metrics that will enable customers andservice providers to agree on a definition of a given grade of service. furthermore, metrics for describing thequality of connectivity will be important to market efficiency since they will allow customers to compare thequality of service providers when making procurement decisions.in addition, users will want to be able to reserve bandwidth resources, which will require that the networkprovider have an understanding of the current traffic behavior in order to efficiently allocate reservations withoutleaving unused reserved bandwidth unnecessarily idle.postnsfnet statistics collection89the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.however, the research community has yet to determine such metrics, and the issue requires immediateattention. a precursor to developing common metrics of traffic workload is a greater understanding of networkphenomena and characteristics. insight into workload requires tools for effective visualization, such asrepresenting the flow of traffic between service providers, or across national boundaries. without such tools formonitoring traffic behavior, it is difficult for an internet service provider to do capacity planning, much lessservice guarantees.in addition to our studies of operational statistics, we have also undertaken several studies that collect morecomprehensive internet traffic flow statistics, and we have developed a methodology for describing those flowsin terms of their impact on an aggregate internet workload. we have developed a methodology for profilinginternet traffic flows which draws on previous flow models 14 and have developed a variety of tools to analyzetraffic based on this methodology. we think that nap and other network service providers would gain greatinsight and engineering advantage by using similar tools to assess and track their own workloads.accountingthe ability to specify or reserve the services one needs from the network will in turn require mechanismsfor accounting and pricing (else there is no incentive not to reserve all one can or not to use the highest priority).many fear pricing will stifle the open, vibrant nature of the internet community; we suggest that it may rathermotivate the constructive exploration of more efficient network implementation of innovative networkingapplications over the internet. understanding the possibilities for resource accounting will also aid commercialnetwork providers in the process of setting cost functions for network services.web traffic and cachingthe world wide web is another domain in which operational collection and analysis of statistics are vitalto support of services. similar to our nsfnet analysis work, we have explored the utility of operationallycollected web statistics, generally in the form of http logs. we analyzed two days of queries to the popularmosaic server at ncsa to assess the geographic distribution of transaction requests. the wide geographicdiversity of query sources and the popularity of a relatively small portion of the web server file set present astrong case for deployment of geographically distributed caching mechanisms to improve server and networkefficiency.we analyzed the impact of caching the results of queries within the geographic zone from which the requestwas sourced, in terms of reduction of transactions with and bandwidth volume from the main server 15. wefound that a cache document timeout even as low as 1,024 seconds (about 17 minutes) during the two days thatwe analyzed would have saved between 40 percent and 70 percent of the bytes transferred from the centralserver. we investigated a range of timeouts for flushing documents from the cache, outlining the tradeoffbetween bandwidth savings and memory/cache management costs. further exploration is needed of theimplications of this tradeoff in the face of possible future usagebased pricing of backbone services that mayconnect several cache sites.other issues that caching inevitably poses include how to redirect queries initially destined for a centralserver to a preferred cache site. the preference of a cache site may be a function of not only geographicproximity, but also current load on nearby servers or network links. such refinements in the web architecturewill be essential to the stability of the network as the web continues to grow, and operational geographicanalysis of queries to archive and library servers will be fundamental to its effective evolution.for very heavily accessed servers, one must evaluate the relative benefit of establishing mirror sites, whichcould provide easier access but at the cost of extra (and distributed) maintenance of equipment and software.however, arbitrarily scattered mirror sites will not be sufficient. the internet's sustained explosive growth callsfor an architected solution to the problem of scalable wide area information dissemination. while increasingnetwork bandwidths help, the rapidly growing populace will continue to outstrip network and serverpostnsfnet statistics collection90the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.capacity as people attempt to access widely popular pools of data throughout the network. the need for moreefficient bandwidth and server utilization transcends any single protocol such as ftp, http, or whatever protocolnext becomes popular.we have proposed to develop and prototype wide area information provisioning mechanisms that supportboth caching and replication, using the nsf supercomputer centers as "root" caches. the goal is to facilitate theevolution of u.s. information provisioning with an efficient national architecture for handling highly popularinformation. a nationally sanctioned and sponsored hierarchical caching and replicating architecture would beideally aligned with nsf's mission, serving the community by offering a basic support structure and setting anexample that would encourage other service providers to maintain such operations. analysis of web trafficpatterns is critical to effective cache and mirror placement, and ongoing measurement of how these tools affect,or are affected by, web traffic behavior is an integral part of making them an effective internet resource.regulationalthough in previous years we have investigated telecommunications regulation issues in both the federaland state arenas, we prefer instead to invest, and see others invest, in research that might diminish the need forregulatory attention to the internet industry. for example, bohn et al.16 have proposed a policy for ip trafficprecedence that can enable a graceful transition into a more competitively supplied internet market that mightreduce the need for federal interest in regulation. their paper discusses how taking advantage of existing ipfunctionality to use multiple levels of service precedence can begin to address disparities between therequirements of conventional and newer and more highly demanding applications.our approach thus far been based on the belief that at least so far, the less regulation of the internetšand infact the more progress in removing regulatory barriers for existing telecommunications companies so they canparticipate more effectively in the internet marketšthe better. however, regulation may be necessary in order tofoster a healthy competitive network environment where consumers can make educated decisions regardingwhich network service providers to patronize. a key requirement in analyzing the relative performance of andcustomer satisfaction with network service providers is public availability of statistics that measure theircapacity, reliability, security, integrity, and performance.recommendationsin this section we highlight several examples of recommended statistics collection objects and tools.reaching consensus on the definition of a communitywide statistics collection architecture will requirecooperation between the private and public sectors. we hope that key federally sponsored networks such as thensf very high speed backbone network service (vbns)17 and the federally sponsored naps can serve as a rolemodel in providing an initial set of statistics to the community and interacting with the research community torefine metrics as research reveals the relative utility of various ones.existing workthe last communitywide document of which we are aware was stockman's rfc 140418, "a model forcommon operational statistics." since that time the internet environment has changed considerably, as have theunderlying technologies for many service providers such as the naps. as a result these specific metrics are notwholly applicable to every service provider, but they serve as a valuable starting point. we emphasize that theexact metrics used are not a critical decision at this point, since refinements are inevitable as we benefit fromexperience with engineering the technologies; what is essential is that we start with some baseline and create acommunity facility for access and development in the future.postnsfnet statistics collection91the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.from stockman19:the metrics used in evaluating network traffic could be classified into (at least) four major categories: utilization metrics (input and output packets and bytes, peak metrics, per protocol and per applicationvolume metrics) performance metrics (round trip time [rtt] on different protocol layers, collision count on a bus network,icmp source quench message count, count of packets dropped) availability metrics (longer term) (line, route, or application availability) stability metrics (shortterm fluctuations that degrade performance) (line status transitions, fast routechanges (flapping), routes per interface in the tables, next hop count stability, shortterm icmp anomalousbehavior).some of these objects are part of standard simple network management protocol (snmp) mibs; others, ofprivate mibs. others are not possible to retrieve at all due to technical limitations; i.e., measurement of a shortterm problematic network situation only exacerbates it or takes longer to perform than the problem persists. forexample, counts of packets and bytes, for nonunicast and unicast, for both input and output are fairly standardsnmp variables. less standard but still often supported in private mibs are counts of packet discards,congestion events, interface resets, or other errors. technically difficult variables to collect, due to the highresolution polling required, include queue lengths and route changes. although such variables would be usefulfor many research topics in internet traffic characterization, operationally collected statistics will likely not beable to support them. for example, one characteristic of network workload is "burstiness," which reflectsvariance in traffic rate. network behavioral patterns of burstiness are important for defining, evaluating, andverifying service specifications, but there is not yet agreement in the internet community on the best metrics todefine burstiness. several researchers20 have explored the failure of poisson models to adequately characterizethe burstiness of both local and widearea internet traffic. this task relies critically on accurate packet arrivaltime stamps, and thus on tools adequate for packet tracing of the arrivals of packets at high rates with accurate(microsecond) time granularities. vendors may still find incentive in providing products that can perform suchstatistics collection, for customers that need finegrained examination of workloads.the minimal set of metrics recommended for ip providers in stockman21 were packets and bytes in andout (unicast and nonunicast) of each interface, discards in and out of each interface, interface status, ip forwardsper node, ip input discards per node, and system uptime. all of the recommended metrics were available in theinternet standard mib. the suggested polling frequency was 60 seconds for unicast packet and byte counters,and an unspecified multiple of 60 seconds for the others. stockman also suggested aggregation periods forpresenting the data by interval: over 24hour, 1month, and 1year periods, aggregate by 15 minutes, 1 hour, and1 day, respectively. aggregation includes calculating and storting the average and maximum values for eachperiod.switched environmentsin switched environments, where there is no ip layer information, the above statistics are not completelyapplicable. without demand from their customer base, many switch vendors have put collection of statistics at asecond priority since it tends to detract from forwarding performance anyway. some atm switches can collectpervc (virtual circuit) statistics such as those described in the atm mib22.one alternative for providers that support ip over an internal atm infrastructure is to collect the ipstatistics described above, and in fact use objects such as hosthost matrices to plan what number and quality ofatm virtual circuits might be necessary to support the ip traffic workload. for switched fddi environments,the provider could collect statistics on each lan coming into the switch, and collapse it during analysis todetermine possible compound effects within the switch, in addition segmenting traffic by interface, customer,and perhaps by protocol/application. if there is no access to network layer information, such as at nsf naps orcertain atm switches, the network service provider will still have an interest in these statistics, since sorting thepostnsfnet statistics collection92the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.resulting arrays would give the nsp an indication of what fraction of traffic comes from what number of users,which may be critical for planning switched or permanent virtual circuit configuration, and by the same token foraccounting and capacity planning purposes. however, converting virtual circuits to end customers, most likely ipcustomers for the near future, requires maintaining mappings to higher layers.dedicated studies of ip workloadseven with a solid set of operational statistics there are times when one wants dedicated collection to gaingreater insight into shortterm dynamics of workloads. for example, there are limitations of the operationallycollected statistics for the nsfnet for describing flows in terms of their impact on an aggregate internetworkload23. we have developed tools for supporting operational flow assessment, to gain insight into bothindividual traffic signatures as well as heavy aggregations of end users. we have tested our methodology usingpacket header traces from a variety of internet locations, yielding insight far beyond a simple aggregatedutilization assessment, into details of the composition of traffic aggregation, e.g., what components of the trafficdominate in terms of flow frequency, durations, or volumes. we have shown that shifts in traffic signatures as aresult of evolving technologies, e.g., toward multimedia applications, will require a different approach innetwork architectures and operational procedures. in particular, the much higher demands of some of these newapplications will interfere with the ability of the network to aggregate the thousands of simultaneous butrelatively short and lowvolume flows that we observe in current environments.the methodology24 defines a flow based on actual traffic activity from, to, or between entities, rather thanusing the explicit setup and teardown mechanisms of transport protocols such as tcp. our flow metrics fall intotwo categories: metrics of individual flows and metrics of the aggregate traffic flow. metrics of individual flowsinclude flow type, packet and byte volume, and duration. metrics of the aggregate flow, or workloadcharacteristics seen from the network perspective, include counts of the number of active, new, and timed outflows per time interval; flow interarrival and arrival processes; and flow locality metrics. understanding howindividual flows and the aggregate flow profile influence each other is essential to securing internet stability, andrequires ongoing flow assessment to track changes in internet workload in a given environment.because flow assessment requires comprehensive and detailed statistics collection, we recognize that thenaps and other service providers may not be able to afford to continuously monitor flow characteristics on anoperational basis. nonetheless we imagine that nap operators will find it useful to undertake traffic flowassessment at least periodically to obtain a more accurate picture of the workload their infrastructure mustsupport. the methodology and tools that implement it4 will be increasingly applicable, even on a continuousbasis, for nap tasks such as atm circuit management, usagebased accounting, routing table management,establishing benchmarks by which to shop for equipment from vendors, and load balancing in future internetcomponents.the methodology can form a complementary component to other existing operational statistics collection,yielding insights into larger issues of internet evolution, e.g., how environments of different aggregation cancope with contention for resources by an everchanging composition and volume of flows. internet traffic crosssection and flow characteristics are a moving target, and we intend that our methodology serve as a tool for thosewho wish to track and keep pace with its trajectory. for example, as video and audio flows and even singlestreams combining voice and audio become more popular, internet service providers will need to parametrizethem to determine how many such end user streams they will be able to support and how many more resourceseach new such stream would require. multicast flows will also likely constitute an increasingly significantcomponent of internet traffic, and applying our methodology to multicast flows would be an important steptoward coping with their impact on the infrastructure.postnsfnet statistics collection93the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.community exchangea key requirement in analyzing the relative performance of and customer satisfaction with network serviceproviders is public availability of statistics that measure their capacity, reliability, security, integrity, andperformance. one vehicle conducive to the development of an effective marketplace is a client/serverbasedarchitecture for providing access to statistics of various nsps. each nsp would support a server that providesquery support for collected statistics for its clients or potential customers. mci provides this functionality forstatistics collected on the nsfsponsored vbns network. communitywide participation in such an open forumwould foster a healthy competitive network environment where consumers could make educated decisionsregarding which network service providers to patronize.conclusionwhen we get through we won't be done.šsteve wolff, then director, dncri, nsf on the nsfnet transition (one among many since 1983) at 1994farnet meetingwe conclude by mentioning an issue of recent popular interest as well as symbolic of the larger problem:internet security and prevention of criminal behavior. ironically, workload and performance characterizationissues are inextricably intertwined with security and privacy. much of the talk about the internet's inherentinsecurity due to the inability to track traffic at the required granularity is misleading. it is not an inability toexamine traffic operationally that has prevented it thus far, whether for security or workload characterization(and the same tools could do both), but rather its priority relative to the rest of the community research agenda.as a result, the gap has grown large between the unambiguous results of confined experiments that targetisolated environments, and the largely unknown characteristics of the extensive internet infrastructure that isheading toward global ubiquity.empirical investigation and improved methodology for doing so can improve current operational statisticscollection architectures, allowing service providers to prepare for more demanding use of the infrastructure andallowing network analysts to develop more accurate internet models. in short, we can contribute to a greaterunderstanding of real computer networks of pervasive scale by reducing the gaps among (1) what networkservice providers need; (2) what statistics service providers can provide; and (3) what indepth network analysisrequires.we encourage discussion as soon as possible within the community on developing a policy on statisticscollection/exchange/posting of available nap/internet service provider statistics, with supporting tools to allowgreater understanding of customer requirements and service models, equitable cost allocation models for internetservices, verification that a given level of service was actually rendered, and evolution toward a level of internetperformance that matches or surpasses that of most telecommunication systems today.author informationhanswerner braun is a principal scientist at the san diego supercomputer center. current researchinterests include network performance and traffic characterization, and working with nsf on nren engineeringissues. he also participates in activities fostering the evolution of the national and international networkingagenda. san diego supercomputer center, p.o. box 85608, san diego, ca 921869784; email address:hwb@sdsc.edu.kimberly claffy received her doctoral degree from the department of computer science and engineering atthe university of california, san diego in june 1994, and is currently an associate staff scientist at the san diegosupercomputer center. her research focuses on establishing and improving the efficacy of traffic andperformance characterization methodologies on widearea communication networks, in particular to cope with thepostnsfnet statistics collection94the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.changing traffic workload, financial structure, and underlying technologies of the internet. san diegosupercomputer center, p.o. box 85608, san diego, ca 921869784; email address: kc@sdsc.edu.references[1] braun, h.w., c.e. catlett, and k. claffy. 1995. "http://vedana.sdsc.edu/," tech. rep., sdsc and ncsa, march. san diegosupercomputer center.[2] braun, h.w., c.e. catlett, and k. claffy. 1995. "national laboratory for applied network research," tech. rep., sdsc and ncsa,march. san diego supercomputer center.[3] caceres, r., p. danzig, s. jamin, and d. mitzel. 1991. "characteristics of widearea tcp/ip conversations," proceedings of acmsigcomm '91, pp. 101œ112, september.[4] paxson, v. 1994. "empiricallyderived analytic models of wide area tcp connections," ieee/acm transactions on networking 2(4),august .[5] leland, w., m. taqqu, w. willinger, and d. wilson. 1994. "on the selfsimilar nature of ethernet traffic (extended version)," ieee/acm transactions on networking, february.[6] paxson, v., and s. floyd. 1994. "widearea traffic: the failure of poisson modeling," proceedings of acm sigcomm '94, february.[7] claffy, k., h.w. braun, and g.c. polyzos. 1993. "longterm traffic aspects of the nsfnet," proceedings of inet '93, pp. cbaš1:10, august.[8] rekhter, y. 1995. "routing in a multiprovider internet," internet request for comments series rfc 1787, april.[9] see claffy, k., h.w. braun, and g.c. polyzos. 1993. "longterm traffic aspects of the nsfnet," proceedings of inet '93, pp. cbaš1:10, august; davis, m. 1988. "analysis and optimization of computer network routing," unpublished master's thesis, universityof delaware; heimlich, s. 1988. "traffic characterization of the nsfnet national backbone," proceedings of the 1990 winterusenix conference, december; claffy, k., g.c. polyzos, and h.w. braun. 1993. "traffic characteristics of the t1 nsfnetbackbone," proceedings of ieee infocom 93, pp. 885œ892, march; claffy, k. 1994. ''internet workload characterization," ph.d.thesis, university of california, san diego, june; and claffy, k., h.w. braun, and g.c. polyzos. 1994. "tracking longtermgrowth of the nsfnet backbone," communications of the acm, august, pp. 34œ45.[10] caceres, r., p. danzig, s. jamin, and d. mitzel. 1991. "characteristics of widearea tcp/ip conversations," proceedings of acmsigcomm '91, pp. 101œ112, september.[11] schmidt, a., and r. campbell. 1993. "internet protocol traffic analysis with applications for atm switch design," computercommunications review, april, pp. 39œ52.[12] danzig, p.b., k. obraczka, and a. kumar. 1992. "an analysis of widearea name server traffic," proceedings of acm sigcomm'92, august.[13] wolff, h., and the ietf opstat working group. 1995. "the opstat clientserver model for statistics retrieval," april, draftietfopstatclientserver03.txt.[14] see caceres, r., p. danzig, s. jamin, and d. mitzel. 1991. "characteristics of widearea tcp/ip conversations," proceedings of acmsigcomm '91, pp. 101œ112, september; and jain, r., and s.a. routhier. 1986. "packet trainsšmeasurement and a new modelfor computer network traffic," ieee journal on selected areas in communications, vol. 4, september, pp. 986œ995.[15] braun, h.w., and k. claffy. 1994. "web traffic characterization: an assessment of the impact of caching documents from ncsa'sweb server," second international world wide web conference, october.[16] bohn, r., h.w. braun, k. claffy, and s. wolff. 1994. "mitigating the coming internet crunch: multiple service levels viaprecedence," journal of high speed networks, forthcoming. available by anonymous ftp from ftp.sdsc.edu:pub/sdsc/anr/papers/and http://www.sdsc.edu/0/sdsc/research/anr/kc/precedence/precedence.html.[17] braun, h.w., c.e. catlett, and k. claffy. 1995. "http://vedana.sdsc.edu/," tech. rep., sdsc and ncsa, march. san diegosupercomputer center.[18] stockman, b. 1993. "a model for common operational statistics," rfc 1404, january.[19] stockman, b. 1993. "a model for common operational statistics," rfc 1404, january.[20] see willinger, w. 1994. "selfsimilarity in highspeed packet traffic: analysis and modeling of ethernet traffic measurements,"statistical science; garrett, m.w., and w. willinger. 1994. "analysis, modeling and generation of selfsimilar vbr videotraffic," proceedings of acm sigcomm '94, september; and paxson, v., and s. floyd. 1994. "widearea traffic: the failure ofpoisson modeling," proceedings of acm sigcomm '94, february.[21] stockman, b. 1993. "a model for common operational statistics," rfc 1404, january.postnsfnet statistics collection95the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.[22] ahmed, m., and k. tesink. 1994. "definitions of managed objects for atm management, version 7.0," internet request forcomments series rfc 1695, august.[23] claffy, k., g.c. polyzos, and h.w. braun. 1995. "internet traffic flow profiling," ieee journal on selected areas incommunications, forthcoming.[24] claffy, k., g.c. polyzos, and h.w. braun. 1995. "internet traffic flow profiling," ieee journal on selected areas incommunications, forthcoming.notes1. see http://www.merit.edu for more information on the nsfnet transition.2. specifically, nsfsponsored regional providers, i.e., those that received funding from the nsf throughout the life of the nsfnet, willonly continue to receive funding if they connect to all three nsf naps. even so, the funding will gradually diminish within 4 years, aninterval of time in which regional providers will have to become fully selfsustaining within the marketplace.3. north american network operators group, international engineering planning group, federal engineering and planning group,engineering and operations working group, federal association of regional networks.4. the software we used is available at ftp://ftp.sdsc.edu/pub/sdsc/anr/software/flows.tar.z.postnsfnet statistics collection96the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.12nii road map: residential broadbandcharles n. brownsteincrossindustry working teamcorporation for national research initiativesthe crossindustry working team (xiwt) is in the process of examining how today's national informationinfrastructure (nii) can evolve into the future nii envisioned in the xiwt white paper, "an architecturalframework for the national information infrastructure." this paper presents the methodology the xiwt isemploying in this effort and some of the results obtained in residential broadband. in this context residentialbroadband refers to communicationsbased services using large amounts of access bandwidth to provide niiservices to residential users.methodologyto understand nii evolution, it is instructive to examine the various overlapping and interdependenttechnology and business segments making up the nii. examples of these segments include residentialbroadband, satellitebased communications, intelligent transportation system, personal communications services,enterprise networking, home automation, public telephony, electronic commerce, and the internet. each of thesesegments will undergo important evolutionary changes during the next decade. by piecing together theevolutionary directions of these segments, a vision of the overall nii evolution will emerge.the initial portion of this study has not been to determine how the nii should evolve, but rather tounderstand the current technical and business directions of the various nii segments. no a priori assumption wasmade that these segments would evolve in a consistent fashion or that the result would meet the clintonadministration's or industry's vision for the nii. indeed, although certain segments of the nii are overlapping andinterdependent, others are evolving with apparently little interaction. one of the goals of this study is to identifywhere inconsistencies exist between the evolutionary directions of the nii segments and to identify the degree towhich the combined segments miss the vision put forward for the future nii. based on the results of this study,specific recommendations will be developed for reshaping evolutionary directions toward the target vision.to understand how these segments are likely to evolve, industry experts from the relevant segments of thenii were invited to a series of nii evolution workshop meetings of the xiwt. these industry experts were askedto address three questions: (1) how would you characterize today's nii with regard to this industry segment? (2)what changes do you forecast for this industry segment over the next 3 years? and (3) what changes do youforecast will occur over the next 6 years?the 3 and 6year time horizons were found to be convenient milestones. most industry planners have wellarticulated views of how their industry will change during the next 3 years based on existing exploratory effortsand announced business plans. changes likely to occur in 6 years are harder to predict; still, this is within theplanning horizon of most companies.these industry experts were asked to present their most likely scenariosšthe one they would bet onoccurring, not a fanciful view of what could be possible given dedicated industry and government efforts. theseforecasts are predicated on whatever regulatory and policy changes these industry experts believe will probablyoccur during this planning period.nii road map: residential broadband97the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.residential broadband evolutionmany services envisioned for the nii will demand significant amounts of communications bandwidth. toextend these services to the general public, broadband access networks to private residences and apartments willplay a critical role in the nii. residential broadband (rbb) networks are already widely deployed forentertainment video distribution services. future residential broadband networks will provide increasedbandwidth and twoway interactive capabilities supporting a wide variety of applications.to characterize today's rbb networks and to understand how they are evolving to provide the capabilitiesneeded in the future nii, the xiwt invited industry experts representing catv companies, local exchangecarriers, rbb equipment manufacturers, and satellite communications service providers to discuss current andfuture rbb networks.the following is a summary of the views of these industry experts. it does not necessarily represent theviews or positions of the xiwt or its member companies.residential broadband todayaccess architecturetoday's residential broadband (rbb) is composed of overtheair broadcast networks, catv networks,microwave access networks, and direct reception from home satellite antennas. with the exception of emergingsatellitebased delivery systems, today's rbb access networks are based on 6mhz analog channels. in a recentstudy of catv networks conducted by cablelabs, typical downstream capacities were as follows: 22 percent have less than 30 channels; 64 percent have 30 to 53 channels; and 14 percent have 54 channels.although the amplifier housings employed in current catv networks are designed to accommodate areturn path amplifier (i.e., they are twoway ready), most of today's catv systems have unactivated returnchannels. roughly 20 percent of today's catv systems use some fiberoptic links to bypass long amplifierchains in the trunk portion of the network. currently a mix of 300, 400, 450, and 550mhz amplifiers is used.service is typically provided to residences and apartments, with relatively few business locations connected tocatv networks. there is usually only a single catv operator in a given service area, with nascent competitionfrom microwave and direct broadcast satellite service providers. tvro (television receive only) backgroundantennas that are 1 to 2 meters in diameter are used by a small fraction of residential customers.services available over today's rbb networks typically consist of the following core set: basic video; subscription pay; payperview; special events; and shopping channels.in addition, the following emerging services have been deployed on a limited basis: near video on demand; electronic video guides; lowspeed and highspeed data access; digital video services via highpower satellites; and highspeed, downlinkonly data via highpower satellites.nii road map: residential broadband98the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.residential broadband: 3year view of access architecturethree years from now most catv networks are expected to be 750mhz systems providing 77 analogchannels in the 54 to 550mhz band and hundreds of digital channels in the 550 to 750mhz band. mostcatv networks in that time frame will have fully activated return paths using the 5 to 40mhz band. thesesystems will have a maximum chain of four to five amplifiers providing improved quality and reliability. therewill be a clustering of cable systems in many metropolitan areas to provide connectivity to business andresidential customers throughout these metropolitan areas.local exchange carriers will provide video dialtone (vdt) service in many metropolitan areas using eitherhybrid fibercoax access or fibertothecurb architectures. these vdt networks will include a level1 gatewayfor selecting the video information programmer. the video information programmer will provide a level2gateway for selection of the specific video channel. most vdt networks will include three to five level2providers, including the lec itself.highpower direct broadcast satellites allowing reception from small antennas (those less than two feet indiameter) will be widely available for supporting video and data services to residences. terrestrial radiofrequency access networks using cellular principles in the 30 to 40ghz band will be deployed in a number ofareas.by 1998, many of today's emerging rbb services will be considered core services of most rbb networks.the following core rbb services are expected to be available: basic video; subscription pay; payperview; special events; shopping channels; near video on demand; electronic video guides; lowspeed and highspeed data access; digital video services via highpower satellites; and highspeed, downlinkonly data via highpower satellites.in addition, the following emerging services are expected to be deployed on a limited basis in 1998: telephony for second and third lines; video telephony; entrepreneurs providing niche information services (e.g., travel services); interactive games; greater variety of shopping services; and more education applications.residential broadband: 6year view of architecturefrom 1998 to 2001, fiber will migrate closer to residences, allowing available bandwidth to be sharedamong fewer customers and providing increased upstream bandwidth.despite current experimental deployments of 1ghz cable systems, catv networks in 2001 are expectedto be predominantly composed of 750mhz fiber serving area systems. fiber node sizes will be reduced to 125to 250 homes, with a maximum of 1 or 2 amplifier cascades with a move toward entirely passive networks. toprovide additional upstream capacity, the analog portion of the spectrum could be reduced to 88 to 400 mhz andnii road map: residential broadband99the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the return path expanded to 5 to 70 mhz. satellites with onboard atm switching will be deployed supportingtwoway bandwidth on demand.by 2001, the following are expected to be core services of rbb networks: basic video; subscription pay; payperview; special events; shopping channels; community bulletin boards; electronic program guides; near video on demand; high and lowspeed data services; video shopping malls; competitive access systems; video conferencing; overwhelming merchandising including electronic coupons; telemedicine to the home; and doityourself guides (e.g., auto repair).summarythis paper presents the methodology being used by the xiwt in its nii evolution study and summarizesinformation collected on rbb evolution. the xiwt continues to collect information on industry evolution plansin other nii segments. this information will be included in a forthcoming white paper on nii evolution.nii road map: residential broadband100the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.13the nii in the home: a consumer servicevito brugliera, zenith electronicsjames a. chiddix, time warner cabled. joseph donahue, thomson consumer electronicsjoseph a. flaherty, cbs inc.richard r. green, cable television laboratoriesjames c. mckinney, atscrichard e. ottinger, pbsrupert stow, rupert stow associatesintroductionthe present infrastructure for terrestrial broadcast and cable delivery to the home is analog based and is thusincompatible and noninteroperable with the concept of the national information infrastructure (nii), which isbased on digital signal processing and transmission. however, the potential economic and technical advantagesof digital transmission are so appealing that the transition to digital delivery techniques is inevitable.the central problems associated with the transition to digital broadcast transmission to the home areconsumer based. first, unlike all other services to the home, broadcasting is a mature industry with nearuniversal access to the american home, and with some 170 million television receivers installed. each householdhas on average 1.9 sets. thus, even if the total present annual output of 28 million televisions was converted todigital receivers, 9 years would elapse before the analog service could be abandoned and digital service take itsplace. if universal access is a defining condition of the nii, then the present broadcast and cabletv servicestogether constitute the national information infrastructure.second, the terrestrial broadcast service that the consumer now enjoys is free, and the public expects it toremain so. cable tv is not free, but 63 percent of television households (tvhh) are willing to pay $300 perannum to receive over 40 additional channels of programming in addition to the delivery by cable of all localbroadcast stations. in fact about 60 percent of all tvhhs view broadcast programs via cable, often because thequality of the broadcast signal delivered in metropolitan areas is deficient.it is therefore clear that the consumer will be willing to make an investment in digital receiving equipmentonly if a service is provided that offers uniquely attractive features and a superior quality of picture and sound.this paper, then, explores the means by which the nii can help to provide a significantly improved serviceto the present broadcast and cable universe, ideally with no increase in cost to the consumer. new services,functions, and means of delivery will be considered, but only in the context of providing them for all consumersat a price that they are willing and able to pay, and at a cost that is economically viable for both the program andservice providers.the primary service provided by broadcast and cable is passive entertainment with a wide range of programchoice. the issue to be addressed is whether and how the nii can be adapted to continue this service whileimproving both its quality and diversity.backgroundthe broadcast and cable industries today share three major features: universal access, an entertainmentservice, and localism.the nii in the home: a consumer service101the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.universal accessthe existing infrastructure serves almost all citizens. ninetyeight percent of all households havetelevisions, including more than 15 million households rated as below the poverty level of income. ninetysixpercent have telephone service, and 63 percent subscribe to cabletv service, which is available to 95 percent oftvhhs. in those areas where broadcast television reception is poor, the delivery of the broadcast signal viacable is an important complementary function of value to the consumer. in addition, cable tv provides over 40cable networks on the average system, together with some distant and outofmarket broadcast stations. thus thehome consumer has a wide choice of programs.the telephone system provides fully interactive voice communication and, with the addition of a faxmachine, enables interactive communication of text and graphics. for other interactive services, such as homeshopping programs delivered by cable tv, the consumer can use the telephone to place orders for the productsselected.for all these consumer services, the broadcast, cable, and telephone media have essentially met the goal ofuniversal access. a historical account of the growth of the penetration of these media into the home is presentedin figure 1. the number of tvhhs increases at a rate of about 1 million per year, generally keeping pace withthe growth in the number of households.the measure of the "television of abundance" is seen in table 1, where the services listed are the average ofthose provided by each of the top 60 television markets, a grouping that encompasses 73 percent of all tvhhs.in this average market there are eight local television stations.cabletv systems provide 42 basic cable networks catering to a diversity of special interests, commonlycalled niche markets. these include typically a 24hour national news service, local news, sports, talk shows,music video, documentaries, history, natural history, the arts, cooking, home improvement instruction,congressional proceedings, weather, and home shopping.figure 1 penetration of consumer devices and servicesšvideo cassette recorders, compact disk players, basiccable, and pay cablešinto u.s. households, 1975 to 1993.table 1 the television of abundance (top 60 markets)serviceaverage number provided by each top marketcurrentbroadcast8local stationscable42basic cable networks10paycable networks8payperview channels5outofmarket broadcast stations3public access channelsdirect broadcast satellite150channelstotal226channelscoming40nearvideoondemand channelstotal266channelsthe nii in the home: a consumer service102the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in addition to carrying the eight local broadcast stations, five distant outofmarket broadcast stations arecarried. public access channels are also available to anyone wishing to address the audience on any subject theyplease.the cable system offers also 10 paycable networks, to which the consumer may subscribe for a fee ofapproximately $100 per annum per network. these paycable networks offer recent feature films, in addition tothe production and origination of over 60 movies per year, at a production cost of $3 million to $5 million each.the paytv networks also acquire the transmission rights to other programs produced elsewhere.eight payperview channels are offered, giving access to recent feature films, sporting events, and musicspectaculars, for which a charge of about $5 is levied for each program selected.direct broadcast satellite (dbs) service is now becoming generally available, offering some 150 channels,and while the market penetration is less than 1 percent of tvhhs, it is growing steadily. the programs offeredare mainly those offered by paytv together with many payperview channels. dbs is available and is beinginstalled by a broad cross section of consumers, including many who have cable service, and among many of the37 percent of tvhhs that do not or cannot subscribe to cable services, including the 5 percent of homes that arenot passed by cable.the existing infrastructure thus offers a total of 226 channels for the consumer's selection. when thisinfrastructure is converted to digital transmission and the use of digital compression, this number may beincreased severalfold, and the socalled "500channel universe" will have arrived, with perhaps hundreds ofchannels available for nearvideoondemand, or nvod.for the practical purposes of the home consumer, the services of terrestrial broadcast, cable, and thetelephone together constitute a national information infrastructure with almost universal access.an entertainment servicethe successful and deep market penetration of terrestrial broadcasting and cable stems in part from theirsatisfying the consumer's primary desire for passive entertainment, and not interactivity. in addition to the massmarket entertainment, nichemarket entertainment is provided by cabletv networks.the second category of programming delivered by broadcast and cable is national and local news andinformation. the consumer's desire for this service is demonstrated by the fact that a broadcaster's local newsservice is the single most profitable type of programming. indeed, television news is perceived by the consumeras the most trusted source of news, according to surveys conducted over the last decade.though the home television receiver is on for an average of 7 hours per day, it is in the evening primetimehours that the audience is greatest, when 71 percent of the tvhhs, or 58 million people, are viewing the screen.the vast majority of this audience watches passive entertainment and sports.the share of this active audience viewing each of the main delivery services is shown in figure 2. themarket demonstrates that entertainment remains the most desired service and that broadcasting and cable meetthis need.while the digital revolution may offer new services that enhance the entertainment value of programsdelivered to the home, entertainment will remain the key element of such services if they are to succeed in themarketplace.figure 2 share of viewing audience during prime time for each of the main categories of program providers, 1994.the nii in the home: a consumer service103the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.localismthe ability of broadcasting and cable to deliver local news, public affairs, information, and sports hasproven to be critical to the profitability of local broadcast stations. these services, supported by localcommercial advertising, have proved vital in cementing the social fabric of a community. in times of crisis orlocal disaster, local broadcasters and cable tv provide a vital service to the populace.in planning the development of the nii as a universal service, it is therefore essential that the infrastructurebe compatible with the delivery of local content for each market and community.analysis and projectionthe plethora of new services and functions postulated for the home consumer, all enabled by applications ofdigital technology and their integration into the new infrastructure, should be examined in the context of threefactors: cost to the consumer and the desires of the consumer, technical and economic feasibility, and regulatoryissues.cost to and desires of the consumerwhatever new services and functions the nii can deliver to the consumer, his willingness to acquire themwill be a balance between desire and cost. the current market trials of new services are intended to demonstrateboth consumer acceptance and willingness to pay.in the existing infrastructure serving the home consumer, commercial television broadcasting is unique inthat it is a "free" service, essentially paid for by advertising. the real cost to the consumer is the hidden cost heor she must pay for advertised products, a price that covers the additional cost of advertising.by contrast, basic cable service costs the consumer about $300 per annum, a price that 63 percent oftvhhs are willing to pay for the better reception of broadcast signals and for the additional programs providedby the cable networks and the local cable systems.the declining rate of growth of subscribers to basic cable tv (see figure 1) may well be influenced by theprice barrier of $300 per annum, with one out of every three households where cabletv service is availabledeclining to subscribe. it is unlikely to be due to any lack of variety or the entertainment value of the programsoffered.paycable or "premium" channels are available to the consumer for an additional fee. generally free ofcommercial advertising, each paycable channel costs about $100 per annum. despite the convenience andvariety offered, this additional cost appears to be limiting the growth of this market. figure 1 demonstrates a lowrate of penetration and growth for paycable networks. over the last several years, the penetration of paycablehas stabilized at 26 percent of tvhhs, or 41 percent of basic cable subscribers, having fallen from 57 percentover the last 10 years.while the number of paycable networks offered has more than doubled in the past 10 years, the averagenumber of paycable networks subscribed to by the consumer has risen only from 1.5 to 2.0.from the above, it would appear that, for the types of programming and service now offered the consumer,the willingness or ability to pay is reaching its limit. any further outlays by the consumer must therefore bebased on more attractive programming or innovative services, perhaps made possible and economically viable bythe nii. however, the nii only facilitates the delivery of information to and from the home, while new andattractive programs and services must still be made by the production and creative community.for any new services to succeed in the home market, the cost to the consumer will be of paramountimportance. if the new services are attractive enough, the consumer may redirect current disposable income usedfor other services to acquire the new services.the nii in the home: a consumer service104the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 3 shows the growth in consumer spending on the major entertainment services. overall, these haveincreased by 90 percent over the last 10 years in constant dollars. the main use of the consumer's discretionaryfunds (71 percent) has been for basic cable and home video services, while the growth in paycable, the cinema,and payperview has been minimal. in addition to the obvious value of basic cable to the consumer, the appealof home video or video rental is interesting, because it meets a consumer demand for a direct choice ofentertainment programming at a specific time. this suggests that if paycable, payperview, and nvod can beoffered at a price competitive with the video store (about $2 per program), much of the video rental spendingcould be diverted to these services. further, this diversion of funds can be effective only if a comparableinventory of programsšperhaps 40 at any one timešis made available.given these important conditions, an nvod or payperview service could enjoy spectacular growth at theexpense of home video. for the program or content provider, significant economies in the number of cassettesmanufactured and distributed are possible, because only a relatively small number will be required fordistribution to the consumer. however, the consumer will still have to bear the cost of the cable system's right totransmit the program, together with the operational costs of storing and delivering the program.in summary, any considerations of a different infrastructure for the delivery of entertainment to the homeshould be judged on the basis of the cost to the consumer, however great the convenience of using the servicemay be.figure 3 consumer spending on major entertainment services, 1980 to 1993.source: p. kagan associates.technical and economic feasibilityit is a common mantra that ''people do not watch and listen to technology; they watch and listen toprograms." while this is a selfevident truth, all systems for watching and listening to programs must passthrough the critical funnel of technology. experience has shown that the consumer is willing to pay for thoseapplications of technology that offer superior performance and quality. the classic example is the market successof the compact disk. the music may be the same as that recorded on vinyl or audiocassette, but the quality of thesound is markedly improved, and cd penetration rates have been phenomenal.we discuss below the economic feasibility of some of the proposed services that the nii might deliver interms of the cost to the consumer and the cost of delivering the services.digital hdtvin pursuit of improved visual quality, the television industry has, since its inception, continuously strived toachieve better quality in the transmitted signal. if the desired gain in quality has not reached the consumer, it isbecause of the fundamental constraints of an aged analog transmission standard.the transition to digital transmission offers the prospect of higherquality service to the home. in fact,digital transmission permits the selection of a quality of transmission appropriate to the requirements of the signalthe nii in the home: a consumer service105the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.being transmitted and its intended use. to this end, a basic digital transmission format has been developed,containing a hierarchy of related and scalable transmission formats, each with a particular data bit rateappropriate to the requirements of the signal to be transmitted and to the bandwidth of the system underconsideration.the top member of this hierarchy is the highdefinition television format developed by the grand alliance,a consortium of seven leading organizations under the aegis of the fcc's advisory committee on advancedtelevision service, or acats. this standard has been adopted by the advanced television systemscommittee, or atsc.the original objective of japanese research in 1970 was to develop a signal origination and transmissionsystem that would provide the full quality of the cinema experience in the living room, complete with cd qualitysound. when, in 1987, the fcc formed the acats, its mandate was to study and evaluate proposed systems,and to recommend a proposed terrestrial transmission standard for the delivery to the home of a greatly improvedand advanced television service, intended to replace the present standard, all "in the public interest."the development work and the construction of a prototype system have now been completed by the grandalliance, and, following final tests of the complete system, the acats will be in a position to formallyrecommend a standard to the fcc by the end of 1995. it is already known that the system works, and works well.this first alldigital transmission system has been designed to be compatible and interoperable with the niiconcept, as will be the lower orders of the hierarchy that may be used for the transmission of lowdefinitiontelevision, graphics, data, and text.the development work and the testing have enjoyed the active participation of cablelabs, a cable industryresearch organization representing all the major cable system companies in the united states and in canada. thecable industry has researched the means by which hdtv can be delivered to cabletv subscribers, and, intechnological terms, is ready to implement such a service.although representing a revolutionary advance in television technology, hdtv is not a new consumerservice but rather a greatly improved and advanced television service, serving the same constituency of tvhhsand offering the same or similar types of programs as those transmitted today. however, with five times theamount of information on the display, with a color rendition at least the equal of film, with a widescreen displaysimilar to that of the cinema, and with multichannel digital sound, the sense of presence given the viewer hasbeen likened to "a window on the world."while this paper is concerned only with the home consumer's interests, it may be remarked that thedevelopment of hdtv is a vital enabling application for a wide range of other services, including education,distance learning, medicine, printing and publishing, and indeed the whole field of video telecommunications.all these applications are compatible with, and complementary to, the intended use of the nii. further,educational and medical services can be of great value to the consumer at home.returning to the interests of the home consumer, audience research on the demand for hdtv is sparse, buta recent poll of 500 randomly selected adults is useful. when asked to rate in order of preference the value of 21new and proposed digitalbased services to the home, ranking each on a scale of 1 to 10, 51 percent placedhdtv in the top three rankings, followed by nvod with 49 percent. all other video services received a muchlower ranking.it thus appears that, once again, the consumer cares greatly about quality and foresees a market for hdtvin the home. by what means hdtv can be delivered most effectively and economically to the home, and at anacceptable price to the consumer, is discussed below.interactive servicesthe perceived consumer demand for interactive services will clearly influence the future infrastructureserving the home. apart from the present telephone service that employs symmetric information flow, nearly allother proposed services will require a much smaller information flow from the consumer than that delivered tothe home. such asymmetry of data flow is important to the economic design of the interactive service.the nii in the home: a consumer service106the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in the multichannel universe now developing, an interactive device is needed to enable the consumer tonavigate with ease and speed among the options presented. while individual viewers have different and diversesets of interests in programs, they all share a common characteristic. it is established that people, when facedwith a large number of options, cannot make a choice directly but instead reduce the number intellectually tobetween seven and nine. they are then able to make a selection by choosing among this smaller number.television audience analysis bears this out and shows that the average viewer selects a small group ofprogram sources, typically four broadcast networks, perhaps a local independent broadcast station, and two nichemarket cable networks that respond to particular interests. in the future it will be important for the consumer torecord and store favorite types of programs in the electronic channel navigator. following this interaction, thenavigator device may display only those upcoming programs that match the consumer's tastes.electronic games require interactive responses at a low data rate. when games are played online against theprogram provider or other remote players, the infrastructure must provide for this communication.home shopping services on cable require consumer responses made upstream. ideally, these responses maytake the form of individual and specialized questions, formatted as an upstream data flow on the cable system,rather than through the use of a telephone.provision must be made also for demanding various and specific sets of textual data, whether it be particularstock prices, sports statistics, particular news items, or additional data relating to a program that the consumer iswatching. it should be possible to record with the program provider particular types of information to bedelivered on a regular basis and stored in the consumer's home for later retrieval. in all these cases, only a lowbandwidth upstream data flow is required. upstream commands for vod and nvod programs and forinformation about the programs should be accommodated by the cable system.overall, cable and dbs are well placed to support asymmetric interactive applications, and unlike thetelephone system, they do not have to be switched.the clinton administration would like to see an nii that would allow individuals to be producers as well asconsumers of information. certainly such a facility would be valuable for the consumer who wishes to invest indistance learning or to access remote medical services or electronic publishing services.however, the great majority of consumers are interested in receiving a broad selection of mostly passiveentertainment programs with the highest possible quality and at the lowest possible cost. even with new servicessuch as the internet, which offers a remarkable range of information services at a rapid rate of growth, it isreported that the home consumer frequently uses it as an entertainment medium, spending much time surfing theworld wide web for amusement rather than edification.the consumer's equipment in the homesome 30 percent of tvhhs now have a computer that is used for nonbusiness purposes.telecommunications, broadcasting, cable, and the computer are converging toward the use of a common digitaltechnology. as television becomes a purely digital medium, it will also become a digital data stream. at the topend of the spectrum hierarchy there will be digital hdtv, and at the bottom end, a telephone twisted pairoccupying the smallest bandwidth.in the home, however, the convergence will not be complete. the large television display will remain theappropriate device for the viewing of pictures and for entertainment however the signal is delivered to the home.the highresolution computer, on the other hand, is designed for reading text and graphics, and for their creationand manipulation, with the viewer seated close to the smaller screen. given their different functions and features,the two displays will remain separate devices, although they will, for some applications, have to beinterconnected.given the several digital data delivery media entering the homešincluding cellular, broadcast, cable,telephone, and dbsšpracticality, economy, and convenience demand a single settop box for the decryption,decoding, and processing of the signals for display. furthermore, the increasing need for interactive upstreamcommunication suggests that, if it should ever achieve universal penetration, the computer, with its associatedtelephone modem, is the most efficient location for the multisignal converter functions. the convenience of thethe nii in the home: a consumer service107the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.armchair remote control facing the television display will remain for the viewer of entertainment programs, butsome of the switching functions it originates would be performed in the computerbased signal converter.in this concept, the leads from the terrestrial antenna, the satellite dish, and the cable feed would allterminate at the computer box, where appropriate circuits would process the signals for display at, in many cases,multiple viewing locations in the home, each requiring different signals for display. this aspect of convergencewill minimize the cost of equipment to be acquired by the consumer.given the rising rate of technological advance, it is inevitable that, in the future, incoming signals will beprocessed by a continuing stream of new or improved circuits and storage media. it is unrealistic to expect thatthe consumer will be able to replace the television set with the frequency necessary to accommodate the latestadvances in digital technology. the display and cabinet in the tv set constitute 75 percent of the total cost of theequipment. while the display must be replaced once to accept the 16:9 aspect ratio of picture to be provided bydigital broadcast and cable services, it then should remain in service for several years.the changes in signal processing equipment that will occur may be accommodated by some form of settopbox, equipped with modular components and circuit cards and provided by the program carrier. the consumerdoes not have to buy them, but in effect pays for them through a monthly subscription to the service, as is nowthe case with the cable services.this situation will obtain only during the introductory period of service, but as volume builds, the mostpopular services will be received by television sets or computers with integrated signal processing facilities.delivery mediaprima facie, in the interests of overall economy, it would seem sensible to minimize the number of separatedata delivery media entering the consumer's home, but the monopolistic dangers of a single carrier delivering allsignals to the home are considerable, absent a strong regulatory overview. on balance, the merits of marketcompetition for delivery of information to the home are compelling.such competition, now developing between dbs and cable tv, where the programs delivered are similar,can lead only to lower costs for the consumer. what temporary advantage dbs may gain in quality of pictureand sound is balanced by the local broadcasting availability, the number of programs offered by cable, and itsinteractive capability.terrestrial broadcast service has the important advantage of being free of charge and has the vital attributeof localism, but it cannot match the variety of programs offered by cable.developments planned by the telephone companies can lead to direct competition with basic and paycableservices. however, the phone companies have recently shown a reluctance to quickly enter the televisiondelivery business.a cellular video transmission system may also serve as a carrier of programs nominally broadcast by localtelevision stations. the interactive capabilities of such systems would enable upstream commands to switchbetween program sources at a local switching node. in addition to providing a full range of all broadcast andcable feeds, the cellular system would enable the transmission of data, graphics, and compressed video to othersubscribers, and enable access to many nonentertainment services conceived for the nii. in this scenario, thecurrent local broadcaster would continue originating all local programming but would no longer endure the costof terrestrial broadcasting.some elements of a national infrastructure will remain common. the program creators and their productsare sought by all. some of them will remain independent, selling their programs on an open market to the highestbidder. the delivery media may acquire some program creators and thus gain from the vertical integration oftheir business. this has occurred in television broadcasting and cable but has not impaired the competitivemarket, because the consumer can always select another program carrier for personal entertainment needs.it is asserted that, in the interests of the consumer, an open and competitive marketplace for the delivery ofprograms to the home is essential. this is best assured by the existence of multiple content carriers.the nii in the home: a consumer service108the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.regulatory issuesas proposed above, we believe that the future infrastructure for communication to the home will develop inthe consumer's best interests if competing information carriers vie for this business. regulatory interventionshould have the sole purpose of ensuring that such competition exists. only where no competing carrier exists israte regulation required. where a new service to the consumer is uniquely proposed by a single organization, theregulatory authorities should act to encourage the entry of a competing entity offering the same service.it is further arguable that regulation should work to inhibit or even prohibit any single entity from owningor controlling both program production and program delivery operations, if such control results in the consumerbeing denied access to any available program source. consumer access should be a prime consideration.recommendationswe believe that the services required most by the consumer include entertainment and local and nationalnews. the consumer wants the highest technical quality of picture and sound and a wide range of choices ofprogramming and expects these services to be available at minimal cost.the infrastructure necessary to meet these needs should have the following features:1. there should be multiple competing delivery systems entering the home, where it is economicallyfeasible. a governmental role in ensuring a level playing field may be appropriate.2. the delivery systems should be able to carry all services now provided by broadcast, cable, dbs, andcomputer data networks.3. the delivery systems should be able to carry a range of digital data rates, including that necessary for thefcc's initiative for hdtv service.4. the delivery systems, as an element of the nii, following a period of innovation and standardsdevelopment, should be interoperable with other elements of the nii, enabling access tononentertainment and information services.5. the delivery systems should permit interactive operation by the consumer, through the use of a simpleinterface.6. the cost of delivering all services to the consumer should be such that universal access is practicable andaffordable. in particular, the delivery of current terrestrially broadcast programs should continue to befree to the consumer.7. the existing cable infrastructure, which is broadly available to almost all television households, shouldcontinue to offer a costeffective path to a broadband component of the nii, which builds on its existingbroadband architecture.8. with the exception of recommendation 1 above, no governmental role is foreseen as desirable ornecessary.the nii in the home: a consumer service109the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.14internetwork infrastructure requirements for virtualenvironmentsdonald p. brutzman, michael r. macedonia, and michael j. zydanaval postgraduate school, monterey, californiaabstractvirtual environments constitute a broad multidisciplinary research area that includes all aspects of computerscience, virtual reality, virtual worlds, teleoperation, and telepresence. we examine the various network elementsrequired to scale up virtual environments to arbitrarily large sizes, connecting thousands of interactive playersand all kinds of information objects. four key communications components for virtual environments are foundwithin the internet protocol (ip) suite: lightweight messages, network pointers, heavyweight objects, and realtime streams. we examine both software and hardware shortfalls for internetworked virtual environments,making specific research conclusions and recommendations. since largescale networked virtual environmentsare intended to include all possible types of content and interaction, they are expected to enable new classes ofsophisticated applications in the emerging national information infrastructure (nii).conclusions virtual environments are an allinclusive superset of nii 2000. any workable solution must address scaling up to arbitrarily large sizes. lower and middlenetwork layer problems are basically solved. four key communications components are used in virtual environments. virtual reality modeling language (vrml) will take the world wide web (www) into three dimensions.recommendations upper application layers relating to entity interactions need funded research. a virtual environment network testbed will enable research and testing. behaviors must be added to vrml for interaction with 3d graphics objects. virtual reality transport protocol (vrtp) will soon be needed. compelling applications, not protocols, will drive progress.overviewvirtual environments (ves) and virtual reality applications are characterized by human operators interactingwith dynamic world models of increasing sophistication and complexity (zyda et al., 1993) (durlach and mavor,1995). current research in largescale virtual environments can link hundreds of people and artificial agents withinteractive threedimensional (3d) graphics, massive terrain databases, and global hypermedia and scientificdatasets. related work on teleoperation of robots and devices in remote or hazardousinternetwork infrastructure requirements for virtual environments110the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.locations further extends the capabilities of humanmachine interaction in synthetic computergeneratedenvironments. the variety of desired connections between people, artificial entities, and information can besummarized by the slogan "connecting everything to everything." the scope of virtual environment developmentis so broad that it can be seen as an inclusive superset of all other global information infrastructure applications.as the diversity and detail of virtual environments increase without bound, network requirements become theprimary bottleneck.the most noticeable characteristic of virtual environments is interactive 3d graphics, which are ordinarilyconcerned with coordinating a handful of input devices while placing realistic renderings at fast frame rates on asingle screen. networking permits connecting virtual worlds with realistic distributed models and diverse inputs/outputs on a truly global scale. graphics and virtual world designers interested in largescale interactions cannow consider the worldwide internet as a direct extension of their computer. we show that a variety ofnetworking techniques can be combined with traditional interactive 3d graphics to collectively provide almostunlimited connectivity. in particular, the following services are essential for virtual world communications:reliable pointtopoint communications, interaction protocols such as the ieee standard distributed interactivesimulation (dis) protocol, www connectivity, and multicast communications.existing infrastructure technologieslayered modelsthe integration of networks with largescale virtual environments occurs by invoking underlying networkfunctions from within applications. figure 1 shows how the seven layers of the wellknown open systemsinterconnection (osi) standard network model generally correspond to the effective layers of the ip standard.functional characteristic definitions of the ip layers follow in box 1.figure correspondence between osi and ip protocol layer models, and objects passed between correspondinglayers on separate hosts.these diagrams and definitions are merely an overview but help illustrate the logical relationship andrelative expense of different network interactions. in general, network operations consume proportionately moreprocessor cycles at the higher layers. minimizing this computational burden is important for minimizing latencyand maintaining virtual world responsiveness.internetwork infrastructure requirements for virtual environments111the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.box 1 summary of internet protocol (ip) suite layer functionalitył process/application layer. applications invoke tcp/ip services, sending and receiving messages orstreams with other hosts. delivery can be intermittent or continuous.transport layer. provides hosthost packetized communication between applications, using eitherreliable delivery connectionoriented tcp or unreliable delivery connectionless udp. exchangespackets end to end with other hosts.internet/network layer. encapsulates packets with an ip datagram that contains routing information;receives or ignores incoming datagrams as appropriate from other hosts. checks datagram validity,handles network error and control messages.data link/physical layer. includes physical media signaling and lowest level hardware functions;exchanges networkspecific data frames with other devices. includes capability to screen multicastpackets by port number at the hardware level.methods chosen for transfer of information must use either reliable connectionoriented transport controlprotocol (tcp) or nonguaranteed delivery connectionless user datagram protocol (udp). each of thesecomplementary protocols is part of the transport layer. one of the two protocols is used as appropriate for thecriticality, timeliness, and cost of imposing reliable delivery on the particular stream being distributed.understanding the precise characteristics of tcp, udp, and other protocols helps the virtual world designerunderstand the strengths and weaknesses of each network tool employed. since internetworking considerationsaffect all components in a large virtual environment, additional study of network protocols and applications ishighly recommended for virtual world designers. suggested references include internet nic (1994), stallings(1994), and comer (1991).internet protocolalthough the variety of protocols associated with internetworking is very diverse, there are some unifyingconcepts. foremost is "ip on everything," or the principle that every protocol coexists compatibly within theinternet protocol suite. the global reach and collective momentum of iprelated protocols make their useessential and also make incompatible exceptions relatively uninteresting. ip and ip next generation (ipng)protocols include a variety of electrical, radiofrequency, and optical physical media.examination of protocol layers helps clarify current network issues. the lowest layers are reasonably stablewith a huge installed base of ethernet and fiber distributed data interface (fddi) systems, augmented by therapid development of wireless and broadband integrated services digital network (isdn) solutions (such asasynchronous transfer mode [atm]). compatibility with the ip suite is assumed. the middle transportrelatedlayers are a busy research and development area. addition of realtime reliability, quality of service, and othercapabilities can all be made to work. middlelayer transport considerations are being resolved by a variety ofworking protocols and the competition of intellectual market forces. from the perspective of the year 2000,lower and middlelayer problems are essentially solved.distributed interactive simulationthe dis protocol is an ieee standard for logical communication among entities in distributed simulations(ieee, 1993). although initial development was driven by the needs of military users, the protocol formallyspecifies the communication of physical interactions by any type of physical entity and is adaptable for generaluse. information is exchanged via protocol data units (pdus), which are defined for a large number ofinteraction types.the principal pdu type is the entity state pdu. this pdu encapsulates the position and posture of a givenentity at a given time, along with linear and angular velocities and accelerations. special components of an entity(such as the orientation of moving parts) can also be included in the pdu as articulated parameters. a fullinternetwork infrastructure requirements for virtual environments112the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.set of identifying characteristics uniquely specifies the originating entity. a variety of dead reckoning algorithmspermits computationally efficient projection of entity posture by listening hosts. dozens of additional pdu typesare defined for simulation management, sensor or weapon interaction, signals, radio communications, collisiondetection, and logistics support.of particular interest to virtual world designers is an open format message pdu. message pdus enableuserdefined extensions to the dis standard. such flexibility coupled with the efficiency of internetwidemulticast delivery permits extension of the objectoriented messagepassing paradigm to a distributed system ofessentially unlimited scale. it is reasonable to expect that freeformat dis message pdus might also provideremote distributed connectivity resembling that of "tuples" to any information site on the internet, furtherextended by use of network pointer mechanisms that already exist for the world wide web. this is a promisingarea for future work.world wide webthe world wide web (www or web) project has been defined as a "widearea hypermedia informationretrieval initiative aiming to give universal access to a large universe of documents" (hughes, 1994).fundamentally the web combines a name space consisting of any information store available on the internetwith a broad set of retrieval clients and servers, all of which can be connected by easily defined hypertextmarkup language (html) hypermedia links. this globally accessible combination of media, client programs,servers, and hyperlinks can be conveniently used by humans or autonomous entities. the web has fundamentallyshifted the nature of information storage, access, and retrieval (bernerslee et al., 1994). current webcapabilities are easily used despite rapid growth and change. directions for future research related to the web arediscussed in (foley and pitkow, 1994). nevertheless, despite tremendous variety and originality, webbasedinteractions are essentially clientserver: a user can push on a web resource and get a response, but a webapplication can't independently push back at the user.multicastip multicasting is the transmission of ip datagrams to an unlimited number of multicastcapable hosts thatare connected by multicastcapable routers. multicast groups are specified by unique ip class d addresses,which are identified by 11102 in the highorder bits and correspond to internet addresses 224.0.0.0 through239.255.255.255. hosts choose to join or leave multicast groups and subsequently inform routers of theirmembership status. of great significance is the fact that individual hosts can control which multicast groups theymonitor by reconfiguring their network interface hardware at the data link layer. since datagrams fromunsubscribed groups are ignored at the hardware interface, host computers can solely monitor and processpackets from groups of interest, remaining unburdened by other network traffic (comer, 1991; deering, 1989).multicasting has existed for several years on local area networks such as ethernet and fddi. however, withip multicast addressing at the network layer, group communication can be established across the internet. sincemulticast streams are typically connectionless udp datagrams, there is no guaranteed delivery and lost packetsstay lost. this besteffort unreliable delivery behavior is actually desirable when streams are high bandwidth andfrequently recurring, in order to minimize network congestion and packet collisions. example multicast streamsinclude video, graphics, audio, and dis. the ability of a single multicast packet to connect with every host on alocal area network is good since it minimizes the overall bandwidth needed for largescale communication. note,however, that the same multicast packet is ordinarily prevented from crossing network boundaries such asrouters. if a multicast stream that can touch every workstation were able to jump from network to networkwithout restriction, topological loops might cause the entire internet to become saturated by such streams.routing controls are necessary to prevent such a disaster and are provided by the recommended multicaststandard (deering, 1989) and other experimental standards. collectively the resulting internetwork ofcommunicating multicast networks is called the multicast backbone (mbone) (macedonia and brutzman, 1994).internetwork infrastructure requirements for virtual environments113the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.improved realtime delivery schemes are also being evaluated using the realtime transport protocol (rtp),which is eventually expected to work independently of tcp and udp (schulzrinne and casner, 1993). otherrealtime protocols are also under development. the end result available today is that even with a timecriticalapplication such as an audio tool, participants normally perceive conversations as if they are in ordinary realtime. this behavior is possible because there is actually a small buffering delay to synchronize and resequencethe arriving voice packets. research efforts on realtime protocols and numerous related issues are ongoing,since every bottleneck conquered results in a new bottleneck revealed.the mbone community must manage the mbone topology and the scheduling of multicast sessions tominimize congestion. currently over 1,800 subnets are connected worldwide, with a corresponding host countequivalent to the size of the internet in 1990. topology changes for new nodes are added by consensus: a newsite announces itself to the mbone mail list, and the nearest potential providers decide who can establish themost logical connection path to minimize regional internet loading. scheduling mbone events is handledsimilarly. special programs are announced in advance on an electronic mail list and a formsfed schedule homepage. advance announcements usually prevent overloaded scheduling of internetwide events and alert potentialparticipants. cooperation is key. newcomers are often surprised to learn that no single person or authority is "incharge" of either topology changes or event scheduling.software infrastructure needswe believe that the "grand challenges" of computing today are not large static gridded simulations such ascomputational fluid dynamics or finite element modeling. we also believe that traditional supercomputers arenot the most powerful or significant platforms. adding hardware and dollars to incrementally improve existingexpensive computer designs is a wellunderstood exercise. what is more challenging and potentially morerewarding is the interconnection of all computers in ways that support global interaction of people and processes.in this respect, the internet is the ultimate supercomputer, the web is the ultimate database, and any networkedequipment in the world is a potential input/output device. largescale virtual environments attempt tosimultaneously connect many of these computing resources in order to recreate the functionality of the real worldin meaningful ways. network software is the key to solving virtual environment grand challenges.four key communication methodslargescale virtual world internetworking is possible through the application of appropriate networkprotocols. both bandwidth and latency must be carefully considered. distribution of virtual world componentsusing pointtopoint sockets can be used for tight coupling and realtime response of physicsbased models. thedis protocol enables efficient live interaction between multiple entities in multiple virtual worlds. thecoordinated use of hypermedia servers and embedded web browsers allows virtual worlds global input/outputaccess to pertinent archived images, papers, datasets, software, sound clips, text, or any other computerstorablemedia. multicast protocols permit moderately large realtime bandwidths to be efficiently shared by anunconstrained number of hosts. applications developed for multicast permit open distribution of graphics, video,audio, dis, and other streams worldwide in real time. together these example components provide thefunctionality of lightweight messages, network pointers, heavyweight objects, and realtime streams (box 2).integrating these network tools in virtual worlds produces realistic, interactive, and interconnected 3d graphicsthat can be simultaneously available anywhere (brutzman, 1994a,b; macedonia, 1995; macedonia et al., 1995).application layer interactivityit is application layer networking that needs the greatest attention in preparing for the informationinfrastructure of the year 2000. dis combined with multicast transport provides solutions for manyinternetwork infrastructure requirements for virtual environments114the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.box 2 four key communications components used in virtual environments lightweight interactions. messages composed of state, event, and control information as used in disentity state pdus. implemented using multicast. complete message semantics is included in a singlepacket encapsulation without fragmentation. lightweight interactions are received completely or not at all. network pointers. lightweight network resource references, multicast to receiving groups. can becached so that repeated queries are answered by group members instead of servers. pointers do notcontain a complete object as lightweight interactions do, instead containing only a reference to an object. heavyweight objects. large data objects requiring reliable connectionoriented transmission. typicallyprovided as a www query response to a network pointer request. realtime streams. live video, audio, dis, 3d graphics images, or other continuous stream traffic thatrequires realtime delivery, sequencing, and synchronization. implemented using multicast channels.source: macedonia (1995).applicationtoapplication communications requirements. nevertheless dis is insufficiently broad and notadaptable enough to meet general virtual environment requirements. to date, most of the money spent onnetworked virtual environments has been by, for, and about the military. most of the remaining work has been in(poorly) networked games. neither is reality. there is a real danger that specialized highend militaryapplications and chaotic lowend game ''hacks" will dominate entity interaction models. such a situation mightwell prevent networked virtual environments from enjoying the sustainable and compatible exponential growthneeded to keep pace with other cornerstones of the information infrastructure.nextgeneration diswe believe that a successor to dis is needed that is simpler, open, extensible, and dynamically modifiable.dis has proven capabilities in dealing with position and posture dead reckoning updates, physically basedmodeling, hostile entity interactions, and variable latency over widearea networks. dis also has severaldifficulties: awkward extendibility, requiring nontrivial computations to decipher bit patterns, and being a very"big" standard. dis protocol development continues through a large and active standards community. however,the urgent military requirements driving the dis standard remain narrower than general virtual environmentnetworking requirements.a common theme that runs through all network protocol development is that realistic testing and evaluationare essential, because the initial performance of distributed applications never matches expectations or theory. anextgeneration dis research project ought to develop a "dialaprotocol" capability, permitting dynamicmodifications to the dis specification to be transmitted to all hosts during an exercise. such a dynamicallyadjustable protocol is a necessity for interactively testing and evaluating both the global and local efficiency ofdistributed entity interactions.other interaction modelsmany other techniques for entity interaction are being investigated, although not always in relation tovirtual environments. intelligent agent interactions are an active area of research being driven by artificialintelligence and user interface communities. rulebased agents typically communicate via a messagepassingparadigm that is a natural extension of objectoriented programming methods. common gateway interface (cgi)scripts function similarly, usually using hypertext transfer protocol (http) (bernerslee et al., 1994) queryextensions as inputs. ongoing research by the linda project uses "tuples" as the communications unit for logicalentity interaction, with particular emphasis on scaling up (gelernter, 1992). muds (multiuser dungeons) andmoos (muds objectoriented) provide a powerful server architecture and textbased interaction paradigm that isinternetwork infrastructure requirements for virtual environments115the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.well suited to support a variety of virtual environment scenarios (curtis and nichols, 1994). passing scripts andinterpretable source code over the network for automatic client use has been widely demonstrated for themultiplatform tool control language (tcl) (ousterhout, 1994). recently the java language has provoked interestover the possibility of simple and secure passing of precompiled program object files for multiplatformexecution (sun, 1995).virtual reality modeling languagethe web is being extended to three spatial dimensions thanks to virtual reality modeling language (vrml),a specification based on silicon graphics inc. open inventor scene description language (wernicke, 1994). keycontributions of the vrml 1.0 standard are a core set of objectoriented graphics constructs augmented byhypermedia links, all suitable for scene generation by browsers on pcs, macintoshes, and unix workstations.the current interaction model for vrml browsers is clientserver, similar to most other web browsers.specification development has been effectively coordinated by mail list, enabling consensus by a large, active,and open membership (pesce and behlendorf, 1994; pesce and behlendorf, 1994œ1995; and bell et al., 1994).discussion has already begun on incorporating interaction, coordination, and entity behaviors into vrml2.0. a great number of issues are involved. we expect that in order to scale to arbitrarily large sizes, peertopeerinteractions will be possible in addition to clientserver queryresponse. although behaviors are not yet formallyspecified, the following possible view of behaviors extends the syntax of existing "engine" functionality in openinventor (figure 2). two key points in this representation follow. first, engine outputs only operate on thevirtual scene graph, and so behaviors do not have any explicit control over the host machine (unlike cgi scripts).second, behaviors are engine drivers, while engines are scene graph interfaces. this means that a wide variety ofbehavior mechanisms might stimulate engine inputs, including open inventor sensors and calculators, scriptedactions, message passing, command line parameters, or dis. thus it appears that forthcoming vrml behaviorsmight simultaneously provide simplicity, security, scalability, generality, and open extensions. finally, weexpect that as the demanding bandwidth and latency requirements of virtual environments begin to be exercisedby vrml, the clientserver design assumptions of the hypertext transfer protocol (http) will no longer bevalid. a virtual reality transfer protocol (vrtp) will be needed once we better understand how to practicallydeal with the new dynamic requirements of diverse interentity virtual environment communications.vertical interoperabilitya striking trend in public domain and commercial software tools for dis, mbone, and the web is that theycan seamlessly operate on a variety of software architectures. the hardware side of vertical interoperability forvirtual environments is simple: access to ip/internet and the ability to render realtime 3d graphics. the softwareside is that information content and even applications can be found that run equivalently under pc, macintosh,and a wide variety of unix architectures. one important goal for any virtual environment is that human users,artificial entities, information streams, and content sources can interoperate over a range that includes highestperformance machines to leastcommondenominator machines. here are some success metrics for verticalinteroperability: "will it run on my supercomputer?" yes. "will it run on my unix workstation?" yes. ''will italso run on my macintosh or pc?" yes. this approach has been shown to be a practical (and even preferable)software requirement. vertical interoperability is typically supported by open nonproprietary specificationsdeveloped by standardization groups such as the internet engineering task force (ietf).internetwork infrastructure requirements for virtual environments116the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2 proposed behavior interaction model for vrml.hardware infrastructure needsresearch testbedthe national research council report on virtual reality (durlach and mavor, 1995) made fewrecommendations for funding virtual environment hardware research due to active commercial development inmost critical technologies. we agree with that assessment. however, the report also has a notable hardwarerelated recommendation regarding networks:recommendation: the committee recommends that the federal government provide funding for a program(to be conducted with industry and academia in collaboration) aimed at developing network standards that supportthe requirements for implementing distributed ves [virtual environments] on a large scale. furthermore, werecommended funding of an open ve network that can be used by researchers, at a reasonable cost, to experimentwith various ve network software developments and applications. (durlach and mavor, 1995, p. 83)the cost of highspeed network connections has precluded most academic institutions from conductingbasic research in highperformance network applications. those sites with highperformance connections arerarely free from the reliability requirements of daytoday network operations. a national ve network testbedfor academia and industry is proposed as a feasible collaboration mechanism. if rapid progress is expected before2000, it is clearly necessary to decouple experimental network research from campus electronic mail and otheressential services. the international widearea year (iway) project is a proposed experimental nationalnetwork that is applicationsdriven and atmbased (iway 95). it will connect a number of highperformancecomputing centers and supercomputers together. iway may well serve as a first step in the direction of anational testbed, but additional efforts will be needed to connect institutions with lesser research budgets.finally, it must be noted that design progress and market competition are bringing the startup costs of highspeedlocalinternetwork infrastructure requirements for virtual environments117the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.area networks (e.g., fddi, atm) within reach of institutional budgets. at most schools, it is the offcampuslinks to the internet that need upgrading and funding for sustained use.other problemsin order to achieve broad vertical integration, it is recommended that proprietary and vendorspecifichardware be avoided. video teleconferencing (vtc) systems are an example of a market fragmented bycompeting proprietary specifications. broad interoperability and internet compatibility are essential. closedsolutions are dead ends. in the area of new network services such as asynchronous transfer mode (atm) andintegrated services digital network (isdn), some disturbing trends are commonplace. supposedly standardizedprotocol implementations often do not work as advertised, particularly when run between hardware fromdifferent vendors. effective throughput is often far less than maximum bit rate. latency performance is highlytouted and rarely tested. working applications are difficult to find. network operating costs are often hidden orignored. application developers are advised to plan and budget for lengthy delays and costly troubleshootingwhen working with these new services.applicationswe believe that working applicationsšnot theories and not hypešwill drive progress. in this section wepresent feasible applications that are exciting possibilities or existing works in progress. many new projects arepossible and likely to occur by the year 2000 if virtual environment requirements are adequately supported in theinformation infrastructure.sports: live 3d stadium with instrumented playersimagine that all of the ballplayers in a sports stadium wear a small device that senses location (through theglobal positioning system or local electrical field sensing) and transmits dis packets over a wireless network.similar sensors are embedded in gloves, balls, bats, and even shoes. a computer server in the stadium feedstelemetry inputs into a physically based, articulated human model that extrapolates individual body and limbmotions. the server also maintains a scene database for the stadium complete with textured images of theedifice, current weather, and representative pictures of fans in the stands. meanwhile, internet users havebrowsers that can navigate and view the stadium from any perspective. users can also tune to multicast channelsproviding updated player positions and postures along with live audio and video. statistics, backgroundinformation, and multimedia home pages are available for each player. online fan clubs and electronic mail listslet fans trade opinions and even send messages to the players. thus any number of remote fans mightsupplement traditional television coverage with a live interactive computergenerated view. perhaps the mostsurprising aspect of this scenario is that all component software and hardware technologies exist today.military: 100,000player problem"exploiting reality with multicast groups" describes groundbreaking research on increasing the number ofactive entities within a virtual environment by several orders of magnitude (macedonia, 1995; macedonia et al.,1995). multicast addressing and the dis protocol are used to logically partition network traffic according tospatial, temporal, and functionally related entity classes. "exploiting reality" further explains virtualenvironment network concepts and includes experimental results. this work has fundamentally changed thedistributed simulation community, showing that very large numbers of live and simulated networked players inrealworld exercises are feasible.internetwork infrastructure requirements for virtual environments118the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.science: virtual worlds as experimental laboratories for robots and peoplein separate work, we have shown how an underwater virtual world can comprehensively model all salientfunctional characteristics of the real world for an autonomous underwater vehicle (auv) in real time. thisvirtual world is designed from the perspective of the robot, enabling realistic auv evaluation and testing in thelaboratory. realtime 3d computer graphics are our window into that virtual world. visualization of robotinteractions within a virtual world permits sophisticated analyses of robot performance that are otherwiseunavailable. sonar visualization permits researchers to accurately "look over the robot's shoulder" or even "seethrough the robot's eyes" to intuitively understand sensorenvironment interactions. theoretical derivation of sixdegreesoffreedom hydrodynamics equations has provided a general physicsbased model capable of replicatinga highly nonlinear (yet experimentally verifiable) response in real time. distribution of underwater virtual worldcomponents enables scalability and rapid response. networking allows remote access, demonstrated via mboneaudio and video collaboration with researchers at distant locations. integrating the world wide web allows rapidaccess to resources distributed across the internet. ongoing work consists primarily of scaling up the types ofinteractions, datasets, and live streams that can be coordinated within the virtual world (brutzman, 1994a,b).interaction: multiple caves using atm and vrmla cave is a type of walkin synthetic environment that replaces the four walls of a room with rearprojection screens, all driven by realtime 3d computer graphics (cruzneira et al., 1993). these devices canaccommodate 10 to 15 people comfortably and render highresolution 3d stereo graphics at 15hz update rates.the principal costs of a cave are in highperformance graphics hardware. we wish to demonstrate affordablelinked caves for remote group interaction. the basic idea is to send graphics streams from a master cavethrough a highspeed, lowlatency atm link to a less expensive slave cave that contains only rearprojectionscreens. automatic generation of vrml scene graphs and simultaneous replication of state information overstandard multicast links will permit both caves and networked computers to interactively view resultsgenerated in real time by a supercomputer. our initial application domain is a gridded virtual environment modelof the oceanographic and biological characteristics of chesapeake bay. to better incorporate networked sensorsand agents into this virtual world, we are also investigating extensions to ip using underwater acoustics (reimersand brutzman, 1995). as a final component, we are helping establish an ambitious regional education andresearch network that connects scientists, students from kindergartens through universities, libraries, and thegeneral public. vertically integrated web and mbone applications and a common theme of live networkedenvironmental science are expected to provide many possible virtual world connections (brutzman, 1995a,b).conclusions virtual environments are an allinclusive superset of nii 2000. any workable solution must address scaling up to arbitrarily large sizes. lower and middlenetwork layer problems are basically solved. four key communications components are used in virtual environments: šlightweight entity interactions (e.g., dis pdus); šnetwork pointers (e.g., urls), usually passed within a lightweight entity interaction that includesidentifying context; šheavyweight objects (e.g., terrain databases or large textured scene graphs) which require reliableconnections for accurate retrieval; and šrealtime streams (e.g., audio and video), usually via mbone.internetwork infrastructure requirements for virtual environments119the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. the virtual reality modeling language (vrml) will take the web into three dimensions and makepowerful applications available across platforms that were previously restricted to expensive graphicsworkstations.recommendations it is the upper network application layers relating to entity interactions that need funded research work. a virtual environment network testbed is needed to enable research schools to cheaply connect and testexperimental services without jeopardizing production networks. behaviors are needed in vrml that allow 3d graphics objects to be driven by šinternal algorithms (e.g., open inventor sensors), šscripted actions (embedded or live via network streams), šmessage passing (by user or agent, e.g., muds/moos), šopen extension mechanism (e.g., http attributes), and šdis (or superior) entity interaction protocol. virtual reality transport protocol (vrtp) will soon be needed: šbandwidth, latency, and efficiency requirements will change relative to the design assumptions of currenthttp servers; šlightweight objects, heavyweight objects, multicast realtime streams; šsmall sets of servers will combine to serve very large client/peer base. compelling applications, not protocols, will drive progress. some examples: šsports: live 3d stadium with instrumented players; user chooses view; šmilitary: 100,000player problem; šscience: virtual worlds as experimental laboratories for robots, people; and šinteraction: affordable linked caves for remote group interaction.projectionsif one considers the evolving nature of the global information infrastructure, it is clear that there is noshortage of basic information. quite the opposite is true. merely by reading the new york times daily, anyindividual can have more information about the world than was available to any world leader throughout most ofhuman history! multiply that single information stream by the millions of other information sources becomingopenly available on the internet, and it is clear that we do not lack content. mountains of content have becomeaccessible. what is needed now is context, a way to interactively locate, retrieve, and display the related piecesof information and knowledge that a user needs in a timely manner.within two lifetimes we have seen several paradigm shifts in the ways that people record and exchangeinformation. handwriting gave way to typing, and then typing to word processing. it was only a short whileafterwards that preparing text with graphic images was easily accessible, enabling individuals to perform desktoppublishing. currently people can use 3d realtime interactive graphics simulations and dynamic "documents"with multimedia hooks to record and communicate information. furthermore such documents can be directlydistributed on demand to anyone connected to the internet. in virtual environments we see a further paradigmshift becoming possible. the longterm potential of virtual environments is to serve as an archive and interactionmedium, combining massive and dissimilar data sets and data streams of every conceivable type. virtualenvironments will then enable comprehensive and consistent interaction by humans, robots, and software agentswithin those massive data sets, data streams, and models that recreate reality. virtual environments can providemeaningful context to the mountains of content that currently exist in isolation without roads, links, or order.what about scaling up? fortunately there already exists a model for these growing mountains ofinformation content: the real world. virtual worlds can address the context issue by providing information linkssimilar to those that exist in our understanding of the real world. when our virtual constructs cumulativelyinternetwork infrastructure requirements for virtual environments120the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.approach realistic levels of depth and sophistication, our understanding of the real world will deepencorrespondingly. in support of this goal, we have shown how the structure and scope of virtual environmentrelationships can be dynamically extended using feasible network communications methods. this efficientdistribution of information will let any remote user or entity in a virtual environment participate and interact inincreasingly meaningful ways.open access to any type of live or archived information resource is becoming available for everyday use byindividuals, programs, collaborative groups, and even robots. virtual environments are a natural way to provideorder and context to these massive amounts of information. worldwide collaboration works, for both people andmachines. finally, the network is more than a computer, and even more than your computer. the internetbecomes our computer as we learn how to share resources, collaborate, and interact on a global scale.referencesbell, gavin, anthony parisi, and mark pesce, "the virtual reality modeling language (vrml) version 1.0 specification," draft, http://www.eit.com/vrml/vrmlspec.html, november 3, 1994.bernerslee, tim, luotonen cailliau, ari nielsen, henrik frystyk, and arthur secret, "the worldwide web," communications of theacm, vol. 37, no. 8, august 1994, pp. 76œ82.brutzman, donald p., "a virtual world for an autonomous underwater vehicle," visual proceedings, association for computingmachinery (acm) special interest group on computer graphics (siggraph) 94, orlando, florida, july 24œ29, 1994a, pp. 204œ205.brutzman, donald p., a virtual world for an autonomous underwater vehicle, ph.d. dissertation, naval postgraduate school, monterey,california, december 1994b.brutzman, donald p., "remote collaboration with monterey bay educators," visual proceedings, association for computing machinery(acm) special interest group on computer graphics (siggraph) 95, los angeles, california, august 7œ11, 1995.brutzman, donald p., "networked ocean science research and education, monterey bay, california," proceedings of internationalnetworking (inet) 95 conference, internet society, honolulu, hawaii, june 27œ30, 1995. available at ftp://taurus.cs.nps.navy.mil/pub/i3la/i3laisoc.html.comer, douglas e., internetworking with tcp/ip, volume i: principles, protocols and architecture, second edition, prentice hall,englewood cliffs, new jersey, 1991.cruzneira, carolina, jason leigh, michael papka, craig barnes, steven m. cohen, sumit das, roger engelmann, randy hudson, trinaroy, lewis siegel, christina vasilakis, thomas a. defanti, and daniel j. sandin, "scientists in wonderland: a report onvisualization applications in the cave virtual reality environment," ieee 1993 symposium on research frontiers in virtualreality, san jose, california, october 25œ26, 1993, pp. 59œ66 and cp3.curtis, pavel, and david a. nichols, "muds grow up: social virtual reality in the real world," xerox palo alto research center, paloalto, california, 1994. available at ftp://ftp.parc.xerox.com/pub/moo/papers/mudsgrowup.ps.deering, steve, "host extensions for ip multicasting," request for comments (rfc) 1112, ftp://ds.internic.net/rfc/rfc1112.txt, august 1989.durlach, nathaniel i., and anne s. mavor, eds., virtual reality: scientific and technological challenges, national research council,national academy press, washington, d.c., 1995.foley, jim, and james pitkow, eds., research priorities for the worldwide web, national science foundation (nsf) information, roboticsand intelligent systems division workshop, arlington, virginia, october 31, 1994. available at http://www.cc.gatech.edu/gvu/nsfws/report/report.html.gelernter, david, mirror worldšor the day software puts the universe in a shoebox – how it will happen and what it will mean,oxford university press, new york, 1992.hughes, kevin, "entering the worldwide web (www): a guide to cyberspace," enterprise integration technology inc., may 1994.available at http://www.eit.com/web/www.guide/.ieee standard for information technologyšprotocols for distributed interactive simulation (dis) applications, version 2.0, institute forsimulation and training report istcr9315, university of central florida, orlando, florida, may 28, 1993.internet network information center (nic), request for comments (rfc) archive, ftp://ds.internic.net, 1994.international widearea year (iway) project, 1995, information available at http://www.iway.org.internetwork infrastructure requirements for virtual environments121the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.macedonia, michael r., and donald p. brutzman, "mbone provides audio and video across the internet," ieee computer, april 1994,pp. 30œ36. available at ftp://taurus.cs.nps.navy.mil/pub/i3la/mbone.html.macedonia, michael r., a network software architecture for large scale virtual environments, ph.d. dissertation, naval postgraduateschool, monterey, california, june 1995.macedonia, michael r., michael j. zyda, david r. pratt, donald p. brutzman, and paul t. barham, "exploiting reality with multicastgroups: a network architecture for largescale virtual environments," ieee computer graphics and applications, 1995, toappear.ousterhout, john k., tcl and the tk toolkit, addisonwesley, reading, massachusetts, 1994.pesce, mark, and brian behlendorf, moderators, "virtual reality modeling language (vrml)," working group home page, http://www.wired.com.vrml, 1994œ1995.reimers, stephen, and donald p. brutzman, "internet protocol over seawater: towards interoperable underwater networks," unmanneduntethered submersibles technology 95, northeastern university, nahant, massachusetts, september 25œ27, 1995, to appear.schulzrinne, henning, and stephen casner, "rtp: a transport protocol for realtime applications," audiovideo transport workinggroup, internet engineering task force, working draft, oct. 20, 1993, available as ftp://nic.ddn.mil/internetdrafts/draftietfavtrtp04.ps.stallings, william, data and computer communications, fourth edition, macmillan, new york, 1994.sun microsystems corporation, java language home page, 1995, http://java.sun.com/.wernicke, josie, the inventor mentor: programming objectoriented 3d graphics with open inventorž, release 2, addisonwesleypublishing, reading, massachusetts, 1994.zyda, michael j., david r. pratt, john s. falby, paul t. barham, chuck lombardo, and kirsten m. kelleher, "the software required for thecomputer generation of virtual environments," presence: teleoperators and virtual environments, vol. 2, no. 2, mit press,cambridge, massachusetts, spring 1993, pp. 130œ140.author informationdon brutzman is a computer scientist working in the interdisciplinary academic group at the navalpostgraduate school. his research interests include underwater robotics, realtime 3d computer graphics,artificial intelligence, and highperformance networking. he is a member of the institute of electrical andelectronic engineers (ieee), the association for computing machinery (acm) special interest group oncomputer graphics (siggraph), the american association for artificial intelligence (aaai), the marinetechnology society (mts), and the internet society (isoc).mike macedonia is an active duty army officer and ph.d. candidate at the naval postgraduate school. hereceived a m.s. degree in telecommunications from the university of pittsburgh and a b.s. degree from the u.s.military academy. his research interests include multicast data networks, realtime computer graphics, andlargescale virtual environments. his leadership on the joint electronic warfare center and centcom staffswas instrumental in successfully deploying and integrating advanced computers, networks, andtelecommunications systems during operations desert shield and desert storm.mike zyda is professor of computer science at the naval postgraduate school. his research interests includecomputer graphics, virtual world systems, and visual simulation systems. he is executive editor of the journalpresence: teleoperators and virtual environments, published by mit press. his recent accomplishmentsinclude organizing and serving as general chair for the 1995 acm siggraph symposium on interactive 3dgraphics.internetwork infrastructure requirements for virtual environments122the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.15electric utilities and the nii: issues and opportunitiesjohn s. cavallini and mary anne scottu.s. department of energyrobert j. aikenu.s. department of energy/lawrence livermore national laboratorythe electric utility industry is immersed in a changing environment, and deregulation is a key pressuredriving this change. careful strategic planning is needed to determine what the future business of the electricutilities will be, and there are concerns about the utilities' competitive status. embedded in this restructuringevolution and coupled to the growing need to provide more in supplyside and demandside management is theopportunity for the electric utility industry to become a significant player in the deployment of the nationalinformation infrastructure (nii). as an electric power research institute (epri) study concluded,energy production and delivery will be tightly coupled with telecommunications and information services for theforeseeable future. in order to control access to the customer and prevent erosion of their customer bases, utilitieswill be driven to become more aggressive in deploying both supplyside information technologies for improvedoperation of their generation, transmission, and distribution facilities; and demandside energy informationservices (eis). those information services will enable utilities to provide higher quality services at lower cost withlower environmental impact, and to give their rate payers better control over their power usage1.the entry of the electric utility industry into the telecommunications arena, which is driven by the need toprovide energy information services for the generation, delivery, and utilization of electric power, can affectcompetition in this market and contribute to the goal of universal service. however, significant policyimplications must be addressed.an industry in transitionafter two decades in which competition in the energy market increased somewhat, the energy act of 1992provided legislative authority to the federal energy regulatory commission (ferc) to mandate open wholesaletransmission access. as a result, there is an even greater imperative for the electric utility industry to reexamineits business strategy. utilities must determine how to remain competitive as this historically vertically integratedmonopoly becomes restructured into focused organizations that will compete in the marketplace for energysupply and delivery services. the overall issue is how to provide highquality energy cost effectively, whilereducing the need for the capital investment of new generating capacity, cutting back on the depletion ofnonrenewable fuels, and reducing emissions. coupled with these elements is the need to focus on customerservice and satisfaction. on an individual utility basis, the issue is deciding what part of the market to focus on,how to maintain and build the appropriate customer base, and how to maintain and increase market share. theutility choice of business lines includes generation, transmission, distribution, and valueadded services, andthese businesses may or may not be segregated into separate subsidiaries.in a strongly regulated environment, industry has little incentive to be innovativešthere is a vested interestin proven technology. the prevailing philosophy is that it's okay to be number two and that there is no need tolead the pack. however, the competitive, nonregulated marketplace encourages innovation, and industries adoptstrategies to manage rather than avoid risks in their use of new technologies. the electric utility industry iselectric utilities and the nii: issues and opportunities123the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.moving toward the latter and as such will be forced to consider the opportunities offered throughtelecommunications and information services to a much greater extent. as is discussed below, this is fortuitousfor the deployment of the nii. it supports the vision that it is possible to create an information infrastructure andthe tools to support smart energy decisions by all consumers and lessen the u.s. balance of payments anddependence on foreign energy sources.many electric utilities have extensive telecommunications facilities that they use for conducting theirbusinessšmaintenance and operation of their generation and distribution systems to ensure reliability andquality of service to their customer base. but some utilities rely on services provided by telecommunicationsproviders. important assets that utilities bring to the table that could be used in providing telecommunicationsservices for themselves or others include extensive rightsofways extending to businesses and residences, aubiquitous presence in residential and business locations, and extensive system facilities (such as conduits, poles,transmission towers, and substation sites) that could be used in telecommunications networks.the ferc recently announced a notice of proposed rulemaking (nopr) that outlines the mandate for theelectric utilities to open their transmission facilities to all wholesale buyers and sellers of electric energy2.included in the nopr is a notice (rm95900) of a technical conference on "realtime information networks"(rins) that would give all transmission users simultaneous access to the same information under industrywidestandards. thus it is clear that within a relatively short time frame, the need for this information system andtelecommunications service will directly affect the 137 utilities required to open up their transmission facilities.the organization of the electric utility industrythere are three main categories of electric utilities todayšthe investorowned utility (iou), the municipalutility, and the rural cooperative. a fourth, oftendiscussed utility is the registered holding company (rhc), asubcategory of investorowned utilities. these are the multistate, investorowned holding companies that must beregistered with the u.s. securities & exchange commission (sec) under the public utility holding companyact (puhca). reform of this act is being considered.when the electric utility industry was born late in the nineteenth century, it appeared first in the form ofious with the corresponding corporate charters shaping its existence. as electric service spread and came to beviewed as a necessity rather than a luxury, public discontent grew with the selective nature of the coverageprovided by the ious. by 1900, many cities and some counties had created a municipal utility as a unit of thelocal government, with the charter to provide service to all its constituency. financing was primarily through taxproceeds and future system revenues. later, during the new deal, the rural cooperative was born as a customerowned, notforprofit membership corporation. this progression of organizations was driven by requirements ofcustomers and (to some extent) strategic objectives of the industry. this progression continues today, albeit in asomewhat different form.about 95 percent of the u.s. population is served by the electric utilities3. of these customers, 76.4percent are served by ious, 13.7 percent by municipal utilities, and the remaining 9.9 percent by ruralcooperatives4. few can choose their supplier of electricityšin most cases it has to be the local provider.however, the possibility for options in choosing a provider offered through what is referred to as "retailwheeling" could make more competitive choices available5.utility authority with respect to telecommunicationseach utility organizational type functions differently within regulatory environments that vary according tojurisdictional boundaries. as discussed extensively in a report examining these issues, all have legal authoritywith respect to telecommunications to build infrastructure and to deliver at least some telecommunicationsbasedservices related to the delivery and consumption of energy6. it is legally sound, though possibly contentious,for a utility to build the facilities needed to communicate with its customers and possible suppliers and todevelop the information services needed for effective energy management. however, the reaction of regulators andelectric utilities and the nii: issues and opportunities124the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.competitors could be negative should the utility attempt to exploit the excess capacity created by using thisinfrastructure. the utility has at least three strong arguments to counter any challenges and support its position:first, since utilities have clear rights to bring such facilities and enterprises into being, they have considerableleverage with regulators and competitors alike to achieve new arrangements that acknowledge the utilities' rightsand interests and build upon them.second, since utilities have undeniable rights to read their own electric meters through telecommunicationspursuant to their existing authorities, they may also have first amendment protections–to send any informationthey wish over their wires.–finally, the nation's need to finance construction of the nii and make it universal gives electric utilities acompelling reason to gain regulatory favor for the use of and profit from their excess telecommunicationscapabilities6.of these arguments, the final is the most compelling, because it provides leverage that can further the goalof universal access for the nii, especially in rural areas. the cable industry and telecommunications providersare less likely to provide the necessary infrastructure in rural areas because of the lowdensity customer base.benefits of energy information servicesthere are several characteristics of energy supply, distribution, and consumption that can be and have beenexploited to realize efficiencies and energy savings. the use of telecommunicationsbased information servicescould potentially enhance efficiency and savings substantially. from the industry's perspective there are manybenefits to be accrued from the application of both supply and demandside information services. from thesupply side, realtime information permits a shift to less conservative operations. smoothing of peak power loadsis possible, and spinning reserves can be reduced. increased information can lead to reduced energy theft. loadbalancing between utilities is feasible. meters can be read remotely, and remote control of service is possible.most importantly, power can be used more efficiently, reducing the need for new generating facilities.for example, linking electronic sensors and automated control systems along transmission and distributionlines offers the possibility for applications such as the optimization and brokerage of dispatch and emissionscredits, automatic meter reading, and remote service control. similar deployment could be made in a commercialbuilding with transmission to the utility of detailed information on enduse energy consumption, along withtemperature and other relevant measurements. the analyses of such data could produce models that could beapplied to improve utility load forecasting, improve building energy models, improve analysis of building energyefficiency, and diagnose electrical system problems in the building.energy consumption varies with the consumer as well as the time of day. residential, commercial, andindustrial sectors have differing requirements. the use of realtime pricing (rtp), which provides a directrelationship between the cost of electricity and its demand, can provide an incentive to encourage customers toconserve power when demand (and hence cost) is highest. this choice gives the customer a reasonable amountof discretionary control. the use of direct load management involves direct utility control of customer appliancesto reduce consumption during periods of peak demand. target appliances include those used for electricalheating, air conditioning, and hotwater heating. the demandside management approach with the most stringenttime response requirements is rapidly curtailable loads (rcl). load must be shed within 20 seconds or less.only large commercial and industrial customers currently have interruptible power tariffs. customer applicationsare generally referred to as demandside management (dsm). a more comprehensive term, energy informationservices (eis), is used when dsm is coupled with remotely provided information services.although each of these approaches (as well as others) is currently being used, the benefits derived can beenhanced with the increased application of the evolving information and telecommunications technologies.electric utilities and the nii: issues and opportunities125the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.issues and approachesthe pressures of increased competition and rapidly evolving technology converge forcefully enough tomandate that utilities develop robust business strategies. there are also compelling arguments to justify theconsideration of public policy issues related to electric utility involvement of the electric utility in an effectivedeployment of nii. those arguments are tied to economics, technology, and regulatory issues. utilities,legislators, and the public are struggling with the notion that utilities could provide the ''lastmile" energy servicecapability as well as more general nii access to the consumer, either competitively or together with the cable andtelecommunications services sectors. should the electric utilities provide the lastmile access to support theirenergy services requirements, only about 5 percent of the fiberoptic network capacity would be needed tosatisfy that requirement. the excess capacity could be leased or sold to other information service providers,subject, of course, to appropriate regulatory provisions. all these elements are important to the vision for the niiwherein deployment is accomplished economically and equitably in an open, competitive marketplace and withuniversal service assured.economic issuessince electricity is fundamental in some way to every product and every service in the united states, it isnot unexpected that the cost of electricity should have an economic impact. consistent cuts in real electric powercosts during the 1950s and 1960s were a factor in economic growth. the following decade saw a reversal in thattrend, with real prices for electric power increasing. energy information services offer the means to help reducetotal energy costs. summarizing the benefits discussed above, total energy costs can be reduced through (1)improved operating efficiency, safety, and productivity of electric utilities; (2) optimized power consumption,reduced energy costs, and improved energy efficiency for customers; and (3) deferral of capital investments innew generation, transmission, and distribution facilities, and minimization of environmental impacts. in addition,there are at least two other significant effects with positive economic implications: (1) the creation of newenergyrelated businesses and jobs, and (2) new growth opportunities for utilities as well as for other sectors ofthe economy. finally, telecommunications facilities and services provided by the electric utility that did not existbefore would be available to support the emerging information infrastructure. these benefits can be illustratedquantitatively by economic research analyses, by proposed utility projects and their projections, and by utilityprojects in operation.in a plan filed by energy, leastcost integrated resource planša threeyear plan, with several publicutility commissions in its service area, that utility proposed a customercontrolled load management (cclm)project that would eventually serve over 400,000 customers. its projections showed a cost/benefit ratio of 1 to1.57. that is, the projected 20year electricity benefits (or avoided energy supply costs) would be $1,845 perhousehold, while the cost of deploying and maintaining broadband telecommunications infrastructure would be$1,172 for each household7. the local regulators in the city of new orleans, included in the plan, challengedenergy on the basis of technical and regulatory uncertainties. the utility's intentions for the use of excesscapacity and accounting for it were never clearly stated. critics were concerned that the utility stockholderswould exploit the windfall excess bandwidth for profitable, unregulated telecommunications. this concern mustbe dealt with for any future such proposals.there are also indirect economic benefits of electric utility contributions to information infrastructure andservices deployment. several recent studies have considered and quantified the ties between telecommunicationsand competitiveness. a selection is highlighted here.1. by instituting policy reforms to stimulate infrastructure investments in broadband networks, an additional$321 billion in gnp growth would be possible over a 16year period beginning in 1992.82. jobs can be created through investments in telecommunications infrastructure. should california increaseits telecommunications infrastructure investment by $500 million to $2.3 billion per year over an 18yearelectric utilities and the nii: issues and opportunities126the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.period, such investments would produce 161,000 jobs, $9.9 billion in additional personal income, and$1.2 billion in additional state and local tax revenues.93. productivity is enhanced through access to telecommunications infrastructure. it has been calculated thatbetween 1963 and 1991, telecommunications have saved the u.s. economy $102.9 billion in labor andcapital expenditures10.4. export activities are also supported by telecommunications infrastructure investments. one studydemonstrated that between 1977 and 1982, increased competitiveness induced by improvements intelecommunications infrastructure led to an increase in u.s. exports of over $50 billion11.a recent study conducted by dri/mcgraw hill under conservative assumptions and determined that thedeployment of energy information services and the full use of the telecommunications infrastructure supportingthose services could make a significant contribution to the u.s. economy. specifically, it would do the following12: improve u.s. productivity by reducing total real energy costs by $78 billion between 1995 and 2010; increase national employment by an average of 63,000 jobs per year; produce a cumulative $276 billion increase in gnp between 1995 and 2010; achieve a cumulative increase of $25 billion in u.s. exports because of improved business productivity; reduce the federal deficit by 2010 by a cumulative $127 billion; and increase real personal income by a total of $173 billion, or $280 per household, through both energy costsavings and improved economic activity.as noted above, most utilities have telecommunications infrastructure that they use for their internalbusiness needs. some have begun to develop and test energy information services. few, however, have enteredinto providing telephone or cable tv services. an exception is the municipally owned electric utility inglasgow, kentucky, a community with a population of 13,000. in 1988, the glasgow electric power boardinstalled a broadband network designed to support the standard communications needs of the utility, to provideinnovative new energy information services, and to offer cable tv services. the program has been quitesuccessful. an interesting observation made by this utility is that a customer might not be interested in having hisor her water heater controlled to gain a credit of $3 to $4 a month on the electric bill but may well be interestedin having the water heater controlled in exchange for reception of a premium channel such as hbo13. thisobservation offers support for the concept of having a single gateway into the home for all information services.the customer's perception of value is just as important as the real value.this competition in cable tv in glasgow has reduced prices. before the broadband network installation, thelocal cable provider, telescripps, offered standard service in the area for $18.50. after the utility offered itsservice at $13.50, telescripps immediately dropped its rate to $5.95 and increased the basic service from 21 to48 channels. despite this aggressive competition, the utility has reached a market share of about 50 percent. amore significant point, however, is that the local citizens in glasgow have reduced their cable tv bills, and thosesavings have stayed in the local economy and are supporting local development13.statistics are not available on the energy savings results, since to this point those have been limited.glasgow purchases all its electricity from tva, and at the time the network was installed tva did not offer atimedifferentiated wholesale rate structure. it was not possible to pass the cost savings from load shift tocustomers. however, when wholesale wheeling becomes available, glasgow is clearly positioned to use that toits advantage. it is also well positioned to provide its population access to the nii.recent strategic alliancesin recent months there have been a number of alliances announced that are aimed at teaming players in theareas of energy services with those who provide telecommunications services and develop sensors. retailelectric utilities and the nii: issues and opportunities127the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.competition for large customers is likely in the near future, analogous to the situation wherein many largefacilities have more than one major telecommunications service provider selected on the basis of the bestpossible price and for ensuring reliability. the time line for extending retail choice to residences and otherindustrial and commercial sectors is less certain except for isolated pockets that demonstrate certain advancedtechnology applications. in any case, it is clear that the alliances mentioned below, as well as others, are movingin anticipation of this eventuality by virtue of changes in their business and technical models.in january 1995, at&t announced the formation of a multicompany team "to develop and deliver a costeffective, fully integrated, twoway customer communications system for the utility industry"14. itsdevelopment partner in this activity is public service electric & gas (psg&e). members of the team areamerican meter company, anderson consulting, general electric, honeywell, and intellon. the shorttermobjective is to provide remote meter reading, power outage detection, realtime load management, and warningof meter tampering. the longterm objective will focus more on increasing customer control of their energy use.a notable observation about this alliance is that the utility has managed to share the risks15. although at&t isheading the project, the niche expertise of several other companies is key to the success of the endeavor.in midapril 1995, teco energy and ibm announced a pilot agreement "to demonstrate and evaluate anadvanced 'smart' home energy management and communications system that will enable residential electricutility customers to better control and track their energy consumption, right down to their appliances"16. inaddition, the system can serve as a gateway to the home for a variety of communications by serving as theinterface for providers of voice, video, and data servicesšit can be the access point for the emerging informationinfrastructure. the system is configured to connect energy measuring and monitoring devices to a personalcomputer in the home and to a control processor attached outside the home. this processor acts as a centralcontroller for a local area network via the existing inhouse electrical wiring. with this configuration, residentialcustomers can measure their energy use and costs 24 hours a day, and utilities can administer new, moreeffective energy management programs.also in january 1995, cableucs was announced as a consortium of four of the nation's top cable operatorsšcomcast corporation, continental cablevision, cox communications, and telecommunications inc.17.cableucs was formed to foster, build, and manage strategic relationships between cable operators and utilitiesšgas and water, as well as electricšand to promote the development of equipment and systems that will use thetwoway telecommunications capabilities being deployed by the cable companies.other notable strategic alliances are (1) energy with first pacific networks, (2) socal edison with coxcable, and (3) pg&e with microsoft and tci. in addition to the energy services mentioned specifically above,others addressed by these alliances include energy theft detection, customer surveys, realtime pricing, powerquality monitoring, and distribution system automation.factors influencing strategic alliancesas discussed above, the electric utilities have a choicešdeploy and operate their own telecommunicationsinfrastructure, or use infrastructure provided by other telecommunications service providers. a study byanderson consulting stated, "in most cases, the benefits and risks of using a thirdparty provider's networkoutweigh the benefits and risks of a utility owning and operating its own network"18. this is especially truewith respect to the utilities' supervisory control and data acquisition (scada), which requires real time andhigh reliability.although most of the management of the electric grid is through use of the utilities' owntelecommunications infrastructure, there are some experimental projects that use cable, wireless, and otherservices for providing lastmile access for energy services management. possible synergism between the utilitiesand the cable and telecommunications providers could be exploited such that the utilities could take advantage ofthe lastmile infrastructure already in place to address the energy application currently deployed. however, thefuture will require enhanced capabilities, as noted in the anderson consulting study: "for the present,narrowband alternatives such as radiobased infrastructures are adequate to deliver many of the communicationsenabledelectric utilities and the nii: issues and opportunities 128the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.services being considered. nevertheless, trends in other industries are moving toward customer interfacesthrough televisions and personal computer; these interfaces will require broadband"18.to use existing cable infrastructure for twoway communication, the cable providers will be required toretrofit their infrastructure to handle highbandwidth, twoway interactive traffic. in addition, since most of theircurrent infrastructure is residential, they will have to partner with other providers or extend their reach to thebusiness and industrial sectors. the cable companies reach about 62 percent of residences19. the localtelecommunications providers, localexchange carriers or regional bell operating companies (rbocs), have amuch larger footprint with respect to residential customer base, 94 percentšone percentage point less than theelectric utilities13. however, even though the telecommunications industry has made strides in the amount ofinformation provided over conventional twisted pair lines, a more economical approach for the utilities isprobably providing fiber directly to their customers. this could be funded by the sale of excess capacity asmentioned above or through energy savings earned through energy management services by the utility or thecustomer.technologiesthe current technologies for energy information services are quite diverse. they range from proprietaryscada and energy services protocols to systems based on open protocols, such as open systemsinterconnection (osi) and the transmission control protocol/internet protocol (tcp/ip).most residential solutions rely on the x10 protocol, implemented with simple command controls usingfrequency modulated power line carrier communications. cebus, a protocol proposed by the electrical industryassociation, and the lonworks protocols developed by a joint effort of the echelon corporation, motorola,and toshiba, are two new entries in the residential energy management arena20. these two solutions can beimplemented over a variety of media that include twisted pair lines, infrared, fiber, coax, and radio frequency.yet most of these solutions have significant drawbacks for addressing the requirements of a general local areanetwork configured to support advanced information and energy appliances.advanced energy distribution and management systems are being tested with infrastructure deployed byutilities, through internet services, and with infrastructure formed by combining the resources of existing utility,telecommunications, and cable providers13,21. there are also some instances of more advanced systems.pg&e is using the tcp/ip protocol suite for managing its energy distribution system. in addition, epri providesan energy information services network that is multiprotocol, using both osi and tcp/ip based services such aswais and mosaic.in the future, technical requirements will require compatibility with existing as well as emergingtechnologies such as asynchronous transfer mode (atm), multimedia, field data transfer, security, wirelessquality of services/resource management, tcp/ip, and more. trials are under way for enhanced monitoring andpredictive maintenance of energy distribution and generation systems. this application requires a large numberof addressable nodes with the capability for gathering and transmitting observation data to a centralized locationfor analysis. access to such energyrelated information, including accounting and billing information, will berequired by users. such access will be over both the internet and any alternate route provided as part of the utility/user infrastructure for realtime energy demand management and control. to meet this requirement, utilities mustoperate a multiprotocol system, and the end user will need an inexpensive multiprotocol gateway/interface/desktop box. realtime protocol support will be needed for such applications as timeofday pricing or realtimepricing and both multicast protocols and the infrastructure to handle the reverse aggregation of responses in realtime. scalable network management tools are also required to handle the plethora of devices employed tosupport both the energy related and information related infrastructure. these same tools must also allow for theendtoend maintenance and operation of the energy system over the various media that may be used. finally,secure mechanisms and protocols are needed before any real energy related business will be conducted over anopen packet switched network such as the internet. remote control of enduser appliances can be done only in asecure environment, and users will likely demand reasonable control over the other data and information thatpass through their information access gateway. technologies are beginning to emerge that can handle theserequirements, and so considerable growth in this area can be forecast.electric utilities and the nii: issues and opportunities129the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.a real need exists that is not being adequately addressed. a general purpose gateway and a control modelimplementation should be configured for residences so that consumers can control both their energy and theirinformation appliances from a single sourceša pc or a unit attached to their television. in addition this conceptshould permit users to control what data and information pass beyond the walls of their premises. to effect sucha solution, the utilities must define the data and information exchange paradigm, that is, the electronic datainterchange (edi) between the utility and the customer. the beginnings of this activity are the focus of part ofthe recent nopr issued by ferc and discussed above.the decision of utilities to adopt a narrow set of technical standards for the purposes of achievinginteroperability or economies of scale may at first seem to be a sound choice. but over the long term thisdecision may put them at a disadvantage22. the nii and the internet, as well as any energy systeminfrastructure, will continue to be a heterogeneous mix of media and protocols. hence, interoperability will bepossible through the use of open access and standards for gateways and interfaces, not through an endtoendhomogeneity based on a single protocol.recommendations and issues to be resolvedthe question could be asked, is there a compelling application for the nii that is serving as a driving forcefor a rapid deployment? and are the applications that could be suggested as an answer to this questioncontributing to the competitiveness of the united states? why not put more emphasis on providing energyinformation services, since this is a ready nii application? why not partner electric utilities with other serviceproviders? there may not be sufficient revenues to deploy the nii without participation by the electric utilities12.technology needsin early 1994, the national research council was directed to examine the status of the high performancecomputing and communications initiative (hpcci), the main vehicle for public research in informationtechnologies. two of the recommendations from the report of the committee that conducted that review arerelevant here23. recommendation 7. "develop a research program to address the research challenges underlying our abilityto build very large, reliable, highperformance, distributed information systems based on the existing hpccifoundation" (p. 10).the three areas identified by the committee where new emphasis is critical to supporting the research needsassociated with the information infrastructure are "scalability, physical distribution and the problems it raises,and interoperative applications." recommendation 8. "ensure that research programs focusing on the national challenges contribute to thedevelopment of information infrastructure technologies as well as to the development of new applicationsand paradigms" (p. 10).it is clear from the discussion of technologies that these recommendations are relevant. for example, thedevelopment of an open nonproprietary premises gateway may prove to be the single most importantadvancement for both the energy services and the nii information services. it will enable new and innovativesupporting hardware, software, systems, and services that will not only drive the energy supply and consumptionapplications arena but also feed the further development of the nii.the electric utilities will need to deploy telecommunications infrastructure to support the energymanagement services, but they should also provide generic nii and internet access. the technologies involvedmust be capable of interoperating with and using the services of the other service providers.electric utilities and the nii: issues and opportunities130the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.appropriate security mechanisms are required for the delivery of adaptive energy services and forinterutility as well as utilitytoconsumer business activities. an issue that needs to be addressed is the need forconsumer privacy. industry, commercial concerns, and homeowners will all want to control who collects andwho has access to the information collected regarding their use of electricity.regulatory and legislative issuesderegulation in progress or being considered in both the electric utility and the telecommunications arenaswill affect how the utilities will or will not be involved in the nii. the 103rd congress consideredtelecommunications reform legislation but failed to pass the proposed bills. the 104th congress is likewiseconsidering such reform. the question of how that should be structured should be framed with the ultimateinterests of the public as the strongest driver. is the "twowire" model the appropriate scenario, and is it trulycompetitive? although it is agreed that everyone should be able to compete, the problem lies with the details ofhow and when that can be accomplished.another factor not often discussed is the cost of deploying the nii. if the cost for deploying to one residenceis about $1,000, deployment to 100 million homes would cost $100 billionšclearly not pocket change. theconsequences of the twowire model effectively double this cost. what markets will be there to recover suchcosts? can the united states afford to spend twice as much as is needed?there is general agreement between both political parties that the electric utilities should be allowed to enterthe marketplace the same as any other "entity." as such the utilities could go headtohead with the other serviceproviders. but is this the best approach for all parties? the anderson study18 has a theme of dissuading theutilities from building the networks themselvesšthe twoway broadband communications that are projected tocreate benefits of $700 million a year. instead, anderson suggests that the electric utilities should partner withthe cable tv companies and invest in upgrading the conventional video networks. this may make sense if theutilities are to own equity in the resulting network. this would allow the utilities to finance the investmentthrough ratable contributions since the justification is energy management applications.the electric utilities, however, also have the opportunity to partner with the local telephone companies.these companies are struggling under the burden of regulation that prevents them from entering two competitiveinformation servicesšvideo and long distance. should another entity such as the electric utility build andmanage the infrastructure, the telephone companies would be in a more advantageous position for competition.regardless of how the legislation proceeds, the electric utilities should be positioning themselves for thefuture. someone who has considerable experience in dealing with telecommunications issues has this to sayabout what utilities should be doing today24: start building "common infrastructure"šswitched broadband telecommunications networks that the utilitywill use itself to deliver energy information services to consumers. work with regulators, where applicable, to ensure that they accept such facilities in the utility's electric ratebases. this should be premised on the utility's undertaking to deliver energy information services via itsinfrastructure to all its electric customers in a negotiated time frame. lease "excess capacity" to cable operators, telephone companies, and others on a nondiscriminatory basisšsubject to regulationsanctioned protections of incumbents to guarantee their continuity of service andrecovery of investment. voluntarily avoid competing with service providers to encourage amicable differentiation of marketsegments and to safeguard an open market in services.references[1] "business opportunities and risks for electric utilities in the national information infrastructure," epri tr104539, electric powerresearch institute, october 1994.[2] "the energy daily," vol. 23, no. 61, p. 1, thursday, march 30, 1995.[3] statistical abstract of the united statesš1993, pp. 561, 563, 589, 728.electric utilities and the nii: issues and opportunities131the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.[4] pocketbook of electric utility industry statistics, edison electric institute, p. 119.[5] "orders instituting rulemaking and investigation of the commission's proposed policy governing restructuring of california's electricservices industry and reforming regulation," california public utilities commission, r.9404031 and i.9404032, april 20, 1994.[6] "positioning the electric utility to build information infrastructure," doe/er0638, november 1994.[7] the powerview system: twoway communications for electric utility applications, edison electric institute, 1994.[8] cohen, robert. 1992. the impact of broadband communications on the u.s. economy and on competitiveness. economic strategyinstitute, p. 1.[9] cronin, francis, et al., telecommunications network modernization and the california economy. dri/mcgraw hill, may 1993.[10] cronin, francis, et al., pennsylvania telecommunications infrastructure study, vol. v, pp. xiiiœ28, 29, dri/mcgraw hill, may 1993.[11] cronin, francis, et al., "telecommunications technology, sectoral prices, and international competitiveness," telecommunicationspolicy, september/october 1992.[12] the role of electric companies in the information economy, mesa research, redondo beach, calif., august 1994.[13] glasgow's fully interactive communications & control system, glasgow electric plant board, glasgow, ky.[14] "news from at&t," press release, january 23, 1995.[15] "surfing the energy channel," frontlines, bruce w. radford, ed., may 15, 1995.[16] press release from teco energy and ibm, april 1995; contact mike mahoney, teco; and john boudreaux, ibm.[17] press release, january 23, 1995; contact steve becker, cox communications; doug durran, continental cablevision; joe waz, comcastcorporation; and lela cocoros, telecommunications inc.[18] the role of broadband communications in the utility of the future, anderson consulting, san francisco, calif., 1994.[19] kagan, paul, marketing new media, december 14, 1992, p. 4.[20] "supply and demand of electric power and the nii," the information infrastructure: reaching society's goals, report of theinformation infrastructure task force committee on applications and technology, nist special publication 868, u.s.government printing office, september 1994.[21] san diego: city of the future, the role of telecommunications, international center for communications, san diego state university,march 1994.[22] aiken, r.j., and j.s. cavallini, "standards: too much of a good thing?," connexions: the interoperability report 8(8), and acmstandardview 2(2), 1994.[23] computer science and telecommunications board, national research council, evolving the high performance computing andcommunications initiative to support the nation's information infrastructure. national academy press, washington, d.c., 1995.[24] private communication, steven r. rivkin, washington, d.c.electric utilities and the nii: issues and opportunities132the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.16interoperation, open interfaces, and protocol architecturedavid d. clarkmassachusetts institute of technologyabstractthis paper provides a framework for defining and understanding the terms interoperation and open; thisframework is then used to develop operational criteria for assessing different approaches to achievinginteroperation. this framework is related to the report released by the nrc entitled realizing the informationfuture (rtif1). to illustrate the utility of the framework, two other interoperation proposals, one developed bythe crossindustry working team (xiwt2) and one by the computer systems policy project (cspp3), arecompared. finally, as an illustration of the framework, it is mapped onto a particular set of network protocols,the internet protocol suite.introductionsuperficially, interoperation is simply the ability of a set of computing elements to interact successfullywhen connected in a specified way. from the consumer's perspective, this operational test is sufficient. however,at a deeper level, it is useful to ask why interoperation was achieved in a particular situation, because the whyquestion will shed some light on when and where this interoperation can be expected to prevail. interoperationcan be achieved in a number of ways, with different implications.interoperation is based on the use of common standards and agreements. one could imagine a computerapplication being specified in such a way that all the needed standards are described as part of the applicationitself. however, in most cases a more modular approach is taken, in which the application is defined to dependon certain externally defined services. these services, called protocols in network parlance, are often organizedin layers, which help structure the dependencies that exist in achieving interoperation. thus, in the protocols thatare part of the internet protocol suite, an application such as mail (which has its own specifications andstandards) depends on the transport control protocol (tcp), which in turn depends on the network internetprotocol (ip). these protocols, organized into layers of building blocks, provide an infrastructure that makes iteasier to achieve interoperation in practice. this sort of dependency in protocols is often illustrated by drawingthe protocols in a stack, as in figure 1.however, if one examines the various layers of protocols, one usually finds that there is a layer belowwhich common standards need not be used to achieve interoperation. again, using the internet suite as anexample, mail can be exchanged between two machines even though one is attached to an ethernet and one to atoken ring lan.why are common standards needed at one layer but not at another? certain protocols are designed with thespecific purpose of bridging differences at the lower layers, so that common agreements are not required there.instead, the layer provides the definitions that permit translation to occur between a range of services ortechnologies used below. thus, in somewhat abstract terms, at and above such a layer common standardscontribute to interoperation, while below the layer translation is used. such a layer is called a "spanning layer" inthis paper. as a practical matter, real interoperation is achieved by the definition and use of effective spanninglayers. but there are many different ways that a spanning layer can be crafted.interoperation, open interfaces, and protocol architecture133the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 a simple protocol stack, organized as layers.the internet protocols as an exampleexpanding the internet example above may make these points clearer. the internet protocol suite contains aprotocol that plays the role of a spanning layer, ip. consider how ip supports the forwarding of simple text mail.the format of mail is defined by the standard rfc822,1 the required common agreement at the applicationlayer. this protocol in turn depends for its delivery on the internet's tcp. and finally, tcp uses the services ofip, which provides a uniform interface to whatever network technology is involved.how does the ip spanning layer achieve its purpose? it defines a basic set of services, which were carefullydesigned so that they could be constructed from a wide range of underlying network technologies. software, as apart of the internet layer, translates what each of these lowerlayer technologies offers into the common serviceof the internet layer. the claim is that as long as two computers running rfc822 mail are connected tonetworks over which internet is defined to run, rfc822 mail can interoperate.this example illustrates the role of the spanning layer as the foundation on which interoperation sits. todetermine whether two implementations of rfc822 mail can interoperate, it is not necessary to look at theimplementation of internet over any specific network technologies. the details of how ip is implemented overethernet or over atm are not relevant to the interoperation of mail. instead, one looks at the extent, in practice,to which ip has succeeded in spanning a number of network technologies. and the practical conclusion, asreflected in the marketplace, is that ip defines a successful spanning layer. the functions and semantics of the iplayer are well defined, as shown by the fact that many companies have become successful by selling routers, thedevices that implement the translation required by ip.a proposal for a spanning layeras a part of its proposed open data network (odn) architecture for the national information infrastructure(nii), the report realizing the information future (rtif1) proposes a specific spanning layer, a module it callsthe odn bearer service. this is illustrated on p. 53 of the report, in the "hourglass picture," which is reproducedhere in figure 2. it illustrates a collection of applications at the top (presumably desirous of interoperation), andat the bottom a collection of network technologies, which support the interoperation.in the middle of this picture, at the narrow point in the hourglass, is the odn bearer service. this layer isthe key to the approach that rtif takes to interoperation. the bearer service provides a set of capabilitiessufficient to support the range of applications illustrated above it. it implements these capabilities by building onthe more basic capabilities of the various network technologies below. the bearer service would thus span thebroad range of network technologies illustrated below it, hide the detailed differences among these varioustechnologies, and present a uniform service interface to the applications above. the bearer service is thus anexample of a spanning layer, with specific features and capabilities.interoperation, open interfaces, and protocol architecture134the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2 a fourlayer model for the odn (reprinted from1, p. 53).the odn bearer service is shown as the narrow point in the hourglass to illustrate that there must be anarrowing of the range of alternatives at that layer. there can be a wide range of applications supported over thebearer service and of technologies utilized under it, but the bearer service itself must represent the point at whichthere is a single definition of the provided capabilities. the power of the scheme is that programmers for all theapplications write code that depends only on this single set of capabilities and thus are indifferent to the actualtechnology used below. if instead there were competing alternatives in the definition of the bearer service, theapplication programmers would have to cope with this variation, which in the limit is no more useful than havingapplications deal directly with the individual network technologies. in computer science terms, if there are napplications and m technologies, the bearer service reduces the complexity of the resulting situation from n × mto n + m. but for this to succeed, the bearer service must be a point of agreement on a single set of capabilities.interoperation, open interfaces, and protocol architecture135the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.other examples of a spanning layerthe bearer service is not the only way that one could achieve interoperation. there are other approaches inuse today, with different objectives as to the range of spanned technologies and the range of supportedapplications. some specific examples may make the alternatives clear. national television system committee (ntsc) video delivery: all the systems that are part of deliveringntscšover the air broadcast, cable, vcrs, satellite, and so onšinterconnect in an effective manner todeliver a video signal from various sources to the consumer. but ntsc video delivery is the onlyapplication supported. this circumstance has a different set of objectives for the range of technology that itspans below and the range of applications that it supports above. compared to the rtif hourglass, thiswould look more like a funnel sitting on its wide end. atm (asynchronous transfer mode): some of the developers of atm, which is one of the networktechnologies in the bottom of the rtif hourglass, have expressed the ambition that an eventually ubiquitousatm would come to serve for all networking needs. were this to happen, it would not then be necessary todefine a separate bearer service above it, because the service provided by atm would become the universalbasis for application development. while this vision does not seem to be working out in practice, itillustrates an approach to interoperation at a lower point than the rtif bearer service. this, if drawn, mightlook like a funnel sitting on its narrow end. the virtue of this approach is that a single technology solutionmight be able to offer a sophisticated range of services, and thus support an even wider range of applicationsthan, for example, the internet approach. so the funnel, while narrow at the bottom, might be wide at the top.all of these schemes have in common that there is some ''narrow point" in their structure, with a singledefinition of service capability. whether it is a definition tied to one application, or to one network technology,this narrow point in the picture corresponds to a spanning layer and is what forms the foundation of the approachto interoperation.comparing approaches to interoperationthe various examples given above suggest a fundamental way to evaluate different approaches tointeroperation, which is to assess what range of technology they can span "below" and what range of applicationsthey can support "above." the essential claim of the rtif bearer service is that the nii needs an hourglass ratherthan either of the two possible funnel pictures as its key spanning layer. an approach that spans a broad range ofnetworks is needed because there has never been, and in practice never will be, a single network technologysufficiently powerful and general to meet everyone's needs. the atm solution, if fully embraced, would requirereplacing all the legacy networks, such as ethernet, token ring, the telephone digital hierarchy, and so on. andeven if this victory over heterogeneity occurred, we must expect and plan for new innovations, which will in turnreplace atm. for example, there is an ongoing research effort attempting to define a nextgeneration networkbased on wavelength division multiplexing (wdm) over fiber. this evolution will always happen; it is anecessary consequence of innovation, cost reduction, and so on.at the same time, the nii must support a broad range of applications, not just one, because there is widedisagreement and uncertainty as to what the successful applications of the nii will finally be. one schoolbelieves that the nii will be dominated by video on demand. others think it will be more internetlike, with arange of interactive and transactionoriented applications, and a wide range of service providers on a globalinfrastructure. not knowing for sure what the range of applications is, it seems foolish to propose an applicationspecific approach to interoperation. so rtif proposed a middle ground of interoperation, which permits both arange of applications above and a range of technologies below.the internet provides a concrete example of this tradeoff in span below and range of common functionabove. while the internet protocol is the primary spanning layer of the suite, it is not the only one. for example,the internet mail transfer protocol smtp (rfc821) is carefully written to be independent of tcp or any otherinteroperation, open interfaces, and protocol architecture136the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.transport protocol. thus internet mail has "built in" an alternative spanning layer. any reliable byte stream canbe used to deliver internet mail. and indeed, there are viable products, called mail gateway, that support mailforwarding between networks with different transport protocols.using the mail protocol as a spanning component provides a very wide range of spanning options. thiswider range of technology and protocol options translates into a wider range of real world deployment; iplevelinternet access reaches about 86 countries today, while internet mail reaches about 168.2 the price for this is thatthe only application supported is mail. there is no access to remote login, file transfer, the world wide web,and so on. so interoperation at the mail level is, again, a funnel sitting on its wide end.spanning layersšone or many?an obvious question at this point is whether there must be a single point of narrowing in the hourglass or,instead, there can be multiple points of common agreement. in fact, as these examples suggest, the situation thatprevails in the real world is more complex than the single hourglass might imply, and one finds multiple pointsof narrowing at the same time. but the idea of a narrow point is always present in any general approach tointeroperation. the hourglass of the rtif should thus be thought of as both an abstract and a concrete approachto interoperation. it illustrates the key idea of a spanning layer and then makes a specific assertion as to wherethat point ought to be.what is openness?since successful interoperation is based on the common use of protocols and standards, a key issue iswhether the necessary standards are available openly, so that different implementors can use them as a basis forimplementation. real interoperation is thus closely related to the issue of openness. there are two aspects todefining open. one is the question of just how public, free, and available a specification needs to be in order tobe called open. the issues here are control of intellectual property rights and the control of how interfaces maybe used. for a good discussion of competing views on this point, see the paper by band4 in this volume.the other aspect of openness is its relationship to the layering that was described above. if our goal is openinteroperation, what specifications need to be open? all, or perhaps only some? the key to sorting this out is torealize that, from the consumer perspective, what matters is the interoperation of applicationsšthe softwarepackages that actually perform functions directly useful to users. since applications are what customers careabout, the constructive definition of openness starts with applications. an application supports openinteroperation if the definition of the application and each of the layers below it are open (by whatever definitionof open prevails), down to an acceptable spanning layer. it is a simple test.is internet text mail an open application? mail is defined by rfc822, which is an open standard.3 itdepends on smtp, the internet mail transfer protocol, which is open, and this depends on tcp, which is open;this in turn runs on ip, which is open. since ip is a spanning layer, internet text mail provides open interoperation.it is usually not necessary to look below the spanning layer to see if all those network technology standardsare open, exactly because that is the benefit of a spanning layer. instead, one looks to the market to see if aspanning layer is successful. even if some of the technologies below the spanning layer were proprietary,applications still would provide open interoperation, so long as the translation boxes that implement the spanninglayer have dealt with the issues of attaching to that technology.even if all the definitions at the application layer are open, the application does not support openinteroperation if any of the layers it depends on between it and a spanning layer are not open. an application thatis openly defined, but that runs on top of a proprietary service layer that runs on tcp and ip, does not supportopen interoperation. this situation is uncommon but illustrates how the test for open interoperation might fail.interoperation, open interfaces, and protocol architecture137the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.symmetryša final aspect of interoperationa final aspect of interoperation is the issue of whether, when two (or more) entities interact, they do so asequals or peers, or instead in some asymmetric way, for example as client and server, or as provider andconsumer. protocols are sometimes designed in such a way that not all the participants in the protocol can playall the roles. for example, some of the early remote file access protocols were designed so that the functions thatimplemented the file server and the file user were separately specified. this made it easy to produceimplementations that omitted the server code, so that the resulting software could only play the role of the user.it was then possible to sell the server code for additional money.in general, symmetry, like openness, tends to reflect business issues. however, some asymmetries arejustified on the basis that they usefully simplify one end of an interaction. for example, some of the tasks havingto do with network operation (such as resolution of faults or traffic routing) are often designed so that thenetwork carries most of the burden and the endnode has a simplified interface. the other framework documentsdiscussed below offer additional insights on the implications of asymmetric interfaces.working with heterogeneity or papering over differences?there are two different views or philosophies about a spanning layer and its manifestation, which is atranslation of some sort. one view is that heterogeneity is inevitable, and thus translation is inevitable. aproperly designed spanning layer is thus a key to successful technology integration. the other view is that aproper technology design would include the goal of interoperation as a core function, and so the need for ahigher level translation capability is a papering over of engineering failures at a lower level.in different situations, both views may be true. the phone system today represents an example of anintegrated approach to spanning, in which there is a wide range of underlying technologies, from copper wire tothe digital hierarchy, and (in the future) atm. however, interoperation among all of these has been a goal fromthe beginning and is integrated into the technology as a basic capability.in contrast, internet interoperation has not been engineered into most of the technologies over which it runs.the internet has sometimes been called a "hostile overlay," because it runs above (and invisible to) theunderlying technology. this sometimes leads to operational issues, since the management tools below theinternet level cannot be used to manage the internet. however, it is the key to the growth and evolution of theinternet, since the internet is not restricted to operating over only those technologies that were designed for thatpurpose.in fact, this distinction is probably key to understanding how new services will first emerge and then maturein the nii of the future. given that spanning layers can be defined to operate on top of other spanning layers (forexample, the internet mail spanning layer on top of the ip spanning layer), it is easy to bring a new spanninglayer as well as a new application into existence by building on one of the existing interfaces. if the serviceproves mature, there will be a migration of the service "into" the infrastructure, so that it becomes more visible tothe infrastructure providers and can be better managed and supported.this is what is happening with internet today. internet, which started as an overlay on top of pointtopointtelephone circuits, is now becoming of commercial interest to the providers as a supported service. a keyquestion for the future is how the internet can better integrate itself into the underlying technology, so that it canbecome better managed and operated, without losing the fundamental flexibility that has made it succeed. thecentral change will be in the area of network management: issues such as accounting, fault detection andrecovery, usage measurement, and so on. these issues have not been emphasized in many of the discussions tothis point and deserve separate consideration in their own right.interoperation, open interfaces, and protocol architecture138the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.comparison to other modelsrtif took a stand as to where the spanning point ought to be: a layer just above the network technologyoptions, which the report called the odn bearer service. other reports have attempted to define interoperation ina somewhat different way, by cataloging critical interfaces.the computer systems policy project (cspp3) has proposed four critical interfaces; the crossindustryworking team (xiwt2), seven. they are summarized in table 1, where they are grouped to best show theircommonalities. these interfaces are proposed as the key to open interoperation. how do these relate to the rtifmodel and the framework developed here? although the approach to interoperation expressed in these twomodels may seem rather different from the definition proposed above, based on the identification of a spanninglayer, these models can be mapped easily onto this framework and express a similar set of objectives.table 1 interfaces of the cspp and xiwt modelscsppxiwtnetworknetworknetworknetworkappliancenetworkappliancenetworkresourcenetworkapplicationapplicationapplianceapplianceresourceresourceresourceapplicationapplianceapplicationsystem control pointnetworkthe networknetwork interfacea good place to start with the cspp/xiwt models is the networknetwork interface. this interfaceconnects different network technologies and thus implies a spanning layer. however, it could be implemented inseveral ways as discussed above. one approach would be technology specific, such as the networknetwork interface (nni) of the atmforum. the internet or the rtif odn model would employ a lowlevel but technologyindependent spanning layer. another approach would be a higherlevel but applicationindependent layer such as a clientserverinvocation paradigm. finally, a networknetwork interface could be constructed at the application layeršfor example, a mailgateway.just calling for an open networknetwork interface does not define which of the above is intended. thexiwt report suggests that there may be several forms of the networknetwork interface, while the cspp reportsuggests that their concept is a fairly lowlevel interface.the applicationapplication interfacenext, the cspp report identifies an interface called the applicationapplication interface, which it describesas the protocols that one nii application uses to communicate with another application. the xiwt has a moredetailed structure, with three versions of this interface as listed in table 1. to unravel what this interfaceinteroperation, open interfaces, and protocol architecture139the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.is, one must start with some known networknetwork interface, since that characterizes the spanning layer andthus the foundation of the common standards. we then apply the test for open interoperation offered above. twoapplications have an open interface for interoperation if their interface specifications, and all the layers belowthem, are open, down to a suitable spanning layer. that spanning layer is the networknetwork interface.these options are clearly illustrated in the cspp report. in one option,4 there is a messaging interfacebetween the networks, which is a highlevel (applicationspecific) example of a networknetwork interface.another option,5 instead of having a messaging interface between the networks, has a lowerlevel networknetwork interface, together with the other standards necessary to interwork the applications over that interface.the coexistence of these two forms is the exact analogy of the internet example above, where mail can interworkeither by a messaging interface (rfc822 mail gateway) or by a full protocol stack of 822 over smtp over tcpover ip, which would be the lowerlevel networknetwork interface.the appliancenetwork interfacethe appliancenetwork interface defines the way an appliance (host, settop box, or whatever) attaches tothe network. this interface, although it may not be so obvious, is another manifestation of the spanning layer. aninternet example may help illustrate this. there are lowlevel technology interfaces that connect appliances tonetworks, such as the ethernet or the token ring standards, or isdn. but the key to internet interoperation is thatthe applications in the appliance are insulated from which technology is being used by the internet layer. thus,the internet layer not only spans divergent network technologies but also "spans" the host interface to thosetechnologies.the above discussion suggests that the appliancenetwork interface could be similar to the networknetworkinterface discussed above. in particular, the capabilities of the spanning layer must be supported across both theappliancenetwork and the networknetwork interface. to first order, in the internet protocol these two are thesame. in practice, there are differences in detail, because it is desirable to simplify the appliancenetworkinterface as much as possible. for example, the internet does not define appliances (hosts) as participating in thefull routing protocol, which is an essential part of the internet networknetwork interface, but instead provides amore simple appliance interface to routing. the atm forum is defining both the usernetwork interface (forusers, or appliances) and the networknetwork interface, for hooking networks together. they have manysimilarities, but they differ in the details of what commands can be issued across the interface to set upconnections, request services, and so on. the odn model, which does not develop the networknetworkinterface in much detail, defines them only to the extent that both must support the same endtoend capabilities,both based on the bearer service.the applianceapplication interfacethe cspp identifies the applianceapplication interface, which is the api for the infrastructure (e.g., theoperating system on the computer). it is concerned with software portability as much as interoperability. thus,rtif does not emphasize this interface. but the interface makes a good illustration of the utility of a spanninglayer to insulate higher levels from issues and options at the lower levels.applications today can be ported between a unix, a windows system, and a macintosh, perhaps with somedifficulty, but with some success. even though the operating systems have different interfaces, there is somealignment at the abstract level among the services of the three. this is not totally true, and some applications donot port, but the point is that if an application builder accepts an abstract service definition (an interface to aspanning layer) to the operating system, then that application is much more likely to avoid getting trapped onto aconcrete interface that is proprietary or has license problems. that freedom is the equivalent of what the bearerservice gives us. the lower layer, whether the operating system or the network technology, can evolve. forexample, if the wellknown token ring patent had been too much of a problem, the community could haveabandoned the specific technology, without any changes at the higher levels. in exchange, the applicationinteroperation, open interfaces, and protocol architecture140the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.builder must accept that the details of the appliance technology may be hidden to some extent beyond thisabstract interface, and someone (the application builder or a third party) must write "glue code" at the bottom ofthe application to port the application to each operating system. this code is the equivalent of what the routervendors write in their devices to map the ip layer onto specific network technologies, and the software above the"network device driver" code, which performs the same mapping in the host.so this interface, even though it is somewhat outside the framework proposed for interoperation, canillustrate why an abstract service definition (a spanning layer) is powerful as a point of convergence.other interfacesin addition to the interfaces discussed above, the xiwt report proposes an interface to a service controlpoint, which is outside the scope of this discussion. it also breaks apart the previously discussed interfaces intovariants that reflect the different roles of an information resource provider and a user of the network. there arethree variants of the applicationapplication interface: the applianceappliance interface, the resourceresourceinterface, and the resourceappliance interface, and there are two variants of the appliancenetwork interface: theappliancenetwork and the resourcenetwork interfaces. these variations in the interfaces capture the idea,presented above, that the various participants in an interaction may not play as equals, but rather in someasymmetric way.the internet philosophy is that any user of the network is also to some extent a provider, and the distinctionbetween user and provider is not sharp but a continuous gradation. thus, the internet, in its ip layer, makes nodiscrimination between the two. at the technology level, a provider might like to attach to the internet in waysthat are different from those of a user, perhaps with higher bandwidth, or with more redundancy, and so on.these differences are important, but the internet protocols treat this as happening below the spanning layer.one might take a different tack and provide a (resource) provider attachment paradigm that is different froman appliance (user) attachment in its relation to the spanning layer itself. one would have to see the technicalarguments in favor of this discrimination to judge the benefit. in some cases, the purpose of this distinction hasbeen not technical but financialšthe provider, if he must attach using a different interface, may be thusidentified and separately billed, presumably at a higher rate.as noted above, the appliancenetwork interface and the networknetwork interface have a lot in common,in that they must both support the same endtoend capabilities. in the internet case, the networknetworkinterface is symmetric, which means that the networks meet as equals. the routing protocols do not force onenetwork to be controlled by another, nor do the management protocols. the appliancenetwork interface isintentionally asymmetric; for example, as discussed above, the appliance cannot participate in the routingprotocols but must use a surrogate protocol.the purpose of this asymmetry was simplicity, but one should also note that asymmetry, especially incontrol functions, mandates the shifting of functions to one side of the interface, which affects both the technicaland the business arrangements of the network. for example, in the telephone system, there is a networknetworkinterface of sorts between a pbx and the public telephone system. however, this interface is not symmetric anddoes not let the pbx attach to the public system as a full peer. this is, no doubt, not a mere oversight.there are many other examples of interfaces that might or might not be symmetric. the networknetworkinterface sits at a point where symmetry or lack thereof in the interface will dictate to a large extent the relativepower and business position of the various providers. this is also true of applicationapplication interfaces. acritical point of assessment for approaches to interoperation is the symmetry, or lack thereof, in the interface.summarythis paper proposes a framework for understanding and assessing interoperation and openness.we offer a careful definition of interoperation, beyond the operational test of "does it work?" twoimplementations of an application can interoperate if (1) they are based on common definitions and standards atinteroperation, open interfaces, and protocol architecture141the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the application layer, (2) they use supporting services in a consistent manner, (3) they are based on a commonspanning layer as a foundation for these services, and (4) the range of underlying network technologies to whichthe applications are actually attached is within the range that the spanning layer can reach.there are two important aspects to this definition. first, it is stated in terms of interoperating applications. ifthe question of interoperation has been framed in terms of a different entity, such as a computer, there is no wellformed answer. computers may interoperate, or not, depending on what application is running. second, thespanning layer is defined as the "foundation" on which the definition rests. this word was chosen because onedoes not have to look below it when testing for interoperation. if the spanning layer is well defined and is knownto work over the network technology in question, then the fact that the application is defined in terms of thisspanning layer is sufficient.a spanning layer is characterized by three key parameters, which define the sort of interoperation it supports: the span of infrastructure options over which the interoperation occurs; the range of higherlevel applications the spanning layer supports; and the degree of symmetry in the services of the spanning layer and its interfaces, and in the higher layers up tothe application definitions.different approaches to interoperation can be evaluated by assessing these characteristics; this paperattempts to map the rtif model and the xiwt and cspp interfaces in this way. in general, there is no conflictbetween these models, but the rtif framework1 is more specific on certain points, in particular as to the exactmodularity of its bearer service, which provides the basis in the rtif framework of the networknetwork, theappliancenetwork, and (if it separately exists) the resourcenetwork interface. the xiwt2 and cspp3 reportsare more specific, on the other hand, in calling for a distinct and full definition of the networknetwork interface,while rtif discusses only those aspects that relate to the endtoend capabilities of the bearer service.using this framework, rtif observes that openness is a characteristic of interoperation. that is, if all theprotocols that accomplish a particular sort of interoperation are open, from the application down to the relevantspanning layer, then one is justified in calling that form of interoperation open. again, the definition is in termsof an application, which, from the user's perspective, is the relevant objective of an interoperation test.rtif also uses the internet architecture as an example of a concrete set of protocols that embody an rtifstyle bearer service approach to interoperation: internet defines a largely symmetric bearer service, the internetprotocol itself, which has been shown in the marketplace to span a wide range of network technologies andsupport a wide range of applications. the networknetwork interface in internet is defined by ip itself, togetherwith the routing, management, and other control protocols. it is completely symmetric. the appliancenetworkinterface is defined by ip, together with icmp and the other hostrelated control protocols. there is no separateresourcenetwork interface.acknowledgmenta number of people have read and commented on this paper. neil ransom, from the xiwt, and tomgannon, from the cspp, provided valuable feedback on my comparisons. marjory blumenthal and louisearnheim, both of the computer science and telecommunications board of the nrc, offered extensivecomments that greatly improved the paper.references[1] computer science and telecommunications board (cstb), national research council, realizing the information future: the internet and beyond, national academy press, washington, d.c., 1994.[2] crossindustry working team (xiwt), corporation for national research initiatives, an architectural framework for the national information infrastructure, cnri, reston, va., 1994.interoperation, open interfaces, and protocol architecture142the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.[3] computer systems policy project (cspp), perspectives on the national information infrastructure: ensuring interoperability, computersystems policy project, washington, d.c., 1994.[4] band, jonathan, "competing definitions of 'openness' on the nii," in this volume.[5] object management group (omg), object management architecture guide, object management group, framingham, mass., 1990.appendix i more detail on the bearer service: what is it exactly?where is it?the general argument in favor of an hourglass is not enough to support the specific details of the rtifbearer service approach. other spanning layers could be defined that, while still "in the middle," are at a higherlevel in the protocol stack.one approach would be to standardize a protocol by which a user program can call a server across thenetwork, such as the clientserver invocation protocol proposed by the object management group (omg5.interoperation through a standard clientserver invocation paradigm would in fact permit a wide span below,because it could run not only over a range of network technologies but also over a range of protocol suites. itcould, for example, operate over tcp/ip or vendor specific protocols such as novell's ipx or ibm's systemnetwork architecture (sna).another approach would be to standardize an interface directly above the transport layer in the protocolstack, corresponding to tcp in the case of the internet suite. ibm has recently proposed such a thing, a spanninglayer that spans tcp and sna. such a layer would permit an application from either the internet suite or sna torun over any combination of the two protocols.6 spanning both tcp and sna covers a wide range of underlyingtechnologies and might prove powerful.what is the limitation of these schemes? first, these potential spanning layers are separated from the actualnetwork technology by more protocol modules (more layers of software) than the odn bearer service is. theseintervening modules make it less likely that a higherlevel spanning layer can interact with the networktechnology to change the quality of service delivered. various applications need different performance from theunderlying network. some services, like realtime video, require delivery within a predictable delay. others, likeemail, may require only the most minimal of besteffort delivery. (this variation in offered performance iscalled quality of service, or qos, in the network community.) most existing protocol suites do not provide auseful way, from high in the protocol "stack," to reach down across the layers and control the qos. this arguesfor an interoperation layer at a low point in the protocol stack, close to the network technology itself.for example, in the case of the sna/tcp spanning layer, the capabilities of the service are constrained bythe particular protocols out of which the service was built, tcp or sna, which offer a reliable delivery protocolwith no ability to bound delivery delays. so the sna/tcp spanning service, although it has a broad span overnetwork technology, cannot make use of other qos in the lower layer and thus cannot support applications suchas realtime audio and video.second, one of the features of a spanning layer must (almost always) be an address space or numberingplan.7 one cannot interoperate with something one cannot name or address. so any protocol layer designed tospan a range of lowerlevel technologies must implement an address space at this layer. experience suggests thatproviding a single global address space at a low level is most effective.there are a number of examples of different address or name spaces in use today. the internet mailprotocol was designed as a spanning layer and thus has its own address space, the mailbox names. the syntax formailbox names can get quite complex and can express a range of functions such as source routing. however,since the address space is not as well defined or engineered as the ip address space and is not as well supportedby management tools, problems with mailbox names and mail forwarding when using mail as a spanning layerare well known.another name space of interest is the naming of information objects in the world wide web, the socalleduniversal resource locators, or urls. the www transfer protocol, http, is currently defined to run on top oftcp. however, it provides its own name space and thus might be able to function as an applicationspecificspanning layer. this extension would require some reengineering of the url name space, an area of currentresearch.in contrast to ip or the internet mail protocol, if one were to ask if tcp (the transport protocol that sitsabove ip) could be a spanning layer, the answer is no. it is defined to depend on ip and cannot be run on top ofanother internetwork layer protocol. tcp is part of the internet interoperation story (it defines a popular commonservice), but it is not a spanning layer.interoperation, open interfaces, and protocol architecture143the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.notes1. the internet standards are specified in a series of documents called rfcs, or requests for comments, which are available on the internet.contact rfcinfo@isi.edu.2. these numbers are from the international connectivity data collected by larry landweber and provided by the internet society as a part ofits web information collection. see http://info.isoc.org/home.html.3. all internet standards are published for use in any way without any constraints or license. this perhaps represents the most open form of"open."4. cssp [3], p. 16, example 3a.5. cssp [3], p. 17, example 3e.6. note that this proposal does not support the interworking of an sna application with an internet application. by our definition ofinterworking, that would require a spanning layer specific to each pair of applications to be interconnected. the goal here is more modest: totake existing applications from either protocol suite and support them over a new spanning layer that allows them to sit above both tcp andsna.7. there are examples where a spanning layer tries to avoid a common address space by supporting multiple address spaces or by translatingamong several. these approaches are often complex.interoperation, open interfaces, and protocol architecture144the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.17service provider interoperability and the nationalinformation infrastructuretim clifforddyncorp advanced technology servicesabstractthe vision of a national information infrastructure (nii) introduces many scenarios that imply the use andassumption of flexible, interoperable telecommunications services. the existing telecommunicationsinfrastructure offers interoperability between local exchange carriers (lecs) and interexchange carriers (ixcs)for traditional circuit switched and dedicated services and among peers at agreed exchange points in the case ofthe internet. the anticipated evolution of commercial telecommunications services does not guarantee thatcompeting service providers will interoperate.current trends compromise the basis for interoperability among the providers of the telecommunicationsinfrastructure. the merging of lecs and ixcs toward joint competition suggests a model similar to today's ixcenvironment, with its extremely limited interoperability. the commercialization of the internet offers a secondexample where the internet policy of free exchange of data tends to become compromised by the growing callfor settlements between carriers. early experience with advanced services such as atm and frame relay, likelykey enablers of the nii, also suggests a reluctance on the part of competing carriers to interoperate. for example,the longterm availability of x.25 public data networks has not resulted in interoperability among competingservice providers.the recommendations suggest several possible approaches to ensure the continuation and expansion ofinteroperability among telecommunications service providers. a first approach examines past models ofinteroperability, (e.g., for voice service or the internet) and examines the suitability of these models to an nii. astechnologyrelated issues will come into play, prototyping activities could be established to define interfaces,address performance, and consider management between service providers. the network access points (naps)established by the national science foundation may also be considered as a governmentsponsored vehicle forinteroperability. finally, it is advisable to continue the analysis of policy and regulatory reform currently underway to ensure continued interoperability among service providers in the nii.statement of the problemthe vision of a national information infrastructure (nii) introduces many scenarios that imply the use of aflexible, interoperable telecommunications infrastructure. the existing telecommunications infrastructure offersinteroperability between lecs and ixcs for traditional circuit switched and dedicated services and among peersat agreedupon exchange points in the case of the internet. the evolution of commercial telecommunicationsservices does not guarantee that competing service providers will interoperate. in fact current trends compromiseinteroperability among future providers of telecommunications services, thereby creating the potential for manyislands where exchange of information beyond island boundaries becomes difficult, less responsive, anduneconomical.service provider interoperability and the national information infrastructure145the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.backgroundthe following sections offer a brief background of the broad and diversified telecommunications market.the first section summarizes the current service providers (with a focus on local and interexchange carriers) anduses the internet as an example of telecommunications services offered outside of the traditional carrierenvelope. the second section describes the generic services currently available and focuses on theinteroperability relationships in today's environment necessary for usertouser communication. interoperabilitytoday typically occurs at the lowest common denominators of service; the third section therefore provides anexample of future services available to the nii. interoperability among service providers in the realm ofadvanced services appears to be a significant challenge.telecommunications service providersthe telecommunications infrastructure is composed of an extensive set of physical communicationsfacilities owned by a relatively small set of telecommunications companies. the lecs and the ixcs providemost of the bulk of the actual physical facilities that deliver telecommunications services. however, a broaderdefinition of infrastructure is offered here that includes a large set of service providers that leverage the physicalinfrastructure to deliver service. this latter group includes the carriers themselves but also involves manyorganizations that build services based on specific markets or communities of interest. the internet is the mostprominent example of this latter group, where several thousand networks make up the internet and only a smallsubset of the lecs and ixcs actually provide internet services. in addition, new elements of infrastructure haveemerged in the areas of cable infrastructure and wireless media. although many of the cable and wirelessproviders have become tied to traditional carriers, their presence implies a strong potential fortelecommunications evolution. with the exception of the internet, the existing infrastructure offers limitedinteroperability, principally between a federally mandated tier of service providers and at the lowest commonlevels of information transport.local service implies that a service provider is principally restricted to offering connectivity within localaccess transport areas (latas). the local service area continues to be dominated by the regional bell operatingcompanies (rbocs). however, many local service providers include traditional service providers and anemerging class of competitive access providers (caps). the traditional service providers offer rboclikeservices in independent areas of the country and include a broad spectrum, from large providers such as gte andsprint local (both over 100 years old) to small providers such as splitrock telephone co. of south dakota.traditional services offered by the rbocs and other local providers have increasingly featured voice services,private lines, and a gradual influx of local data services. caps, such as metropolitan fiber systems and teleportcommunication group, have focused on the provision of dedicated access between highdensity customerlocations (e.g., government and business offices) and the ixcs. the cap focus can be attributed to the strongbusiness opportunity for local access and the large investment required to reach private residences. caps havealso gradually increased their portfolio of services offered to include data services. the most prominentupcoming change in the local service area is the likely removal of restrictions on the rbocs for the delivery ofservices beyond the local area.the ixcs, or longdistance carriers, provide telecommunications services between the latas. thedivestiture of the rbocs by at&t in 1984 created the opportunity for new longdistance service providers andhas resulted in the creation of several prominent competitors of at&t, principally mci and sprint, and a strongcadre of growing service providers, such as willtel and lci. the resulting competition has brought rapidtechnology evolution, new services, and lower costs to longdistance services. the longdistance infrastructurehas also become highly robust and diverse, currently including multiple highcapacity networks withindependent control structures. for example, a typical span in a major carrier network may consist of severalactive fiber pairs, each carrying in excess of 1 gigabit per second (gbps) of capacity. the ixcs make up multiplecrosscountry (east to west and north to south) routes that, taken together, represent hundreds of gbps ofcapacity. the ixcs support many telecommunications services, dominated by voice (especially for the top threeproviders) and private line. in addition, the ixcs offer various public services for data or videobased services.service provider interoperability and the national information infrastructure146the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.during this period of competition in the longdistance market, the local carriers have played a key role asthe common exchange point among the services provided by the ixcs. however, in the local area, where thetraditional telecommunications monopoly has remained in effect, technology, services, and prices have tended tolag behind the longdistance market, often significantly. although the local carriers enabled the emergence ofmultiple longdistance carriers through a common and expensive local infrastructure, the capability gaps andhigher prices in the local area have become considerable. the ixcs typically claim that approximately 40percent of their total costs result from local carrier charges. the local carriers have moved to close these gaps asthe promise of unrestricted competition has become feasible. the ixcs have also begun to posture for the entryof local providers through arrangements to build local infrastructures that will lessen the local monopoly ofservice.the internet is one example of telecommunications services that offer a valueadded capability on top of thecarrier infrastructure. it has become prominent over the last several years, principally due to an unabated growthrate. as a simple definition, the internet is a collection of networks bound together using a common technology.the internet originated with department of defense research on networking and sharing of computing resources.primarily through government subsidies for research and education support, the internet grew significantlythrough the 1980s and began to attract commercial attention due to its growth and strategic government andprivate investment. currently the internet has become a major source of commercial enterprise and continues togrow rapidly. an explosion of commercial internet service providers has paralleled the growth of the internetand offers wide area connectivity among the expanding population of internet users and component networks.the internet has also become a prime model for many visions of the nii, because of its ability to interconnectvarious technologies, people, and sources of information.overall, the telecommunications infrastructure, taken as an entity without regard for regulatory or businessconcerns, offers a multifaceted and ubiquitous platform for the delivery of services consistent with the nii.current market and regulatory factors have placed restrictions on the delivery of services by local providers. as aresult, local services tend to be less capable and more expensive than those provided by ixcs. this status quohas begun to evolve rapidly as local providers prepare for a relaxation of their restrictions. the ixcs continue toenhance their abilities to deliver many services at everlower prices. numerous service providers, bestrepresented by the internet community, have capitalized on this infrastructure to provide effective informationservices.telecommunications servicesit is also worthwhile to briefly review the services delivered by the service providers. thetelecommunications infrastructure supports three types of connection that enable a wide variety of services: circuit switched services that allow dynamic establishment of a connection between users and support voice,video, and modembased traffic; private line services where a carrier establishes a full period connection between users at a predefinedcapacity; and statistically multiplexed services that offer the ability to carry traffic dynamically as required by the user.public data networks, the internet, and emerging frame relay and atm services employ statisticalmultiplexing.especially important to this paper are the relationships between the providers of these services that enableinteroperability from user to user. although transparent to a user, virtually all telecommunications services infact require an exchange of information between multiple service providers. today a subtle but complex web ofrelationships addresses the information transfer, operational support, and business mechanisms necessary tomake telecommunications a reality. interoperability among carriers has typically settled to the least commondenominators of service such as switched voice or private line services. the demand of the nii, for more robust,featured services, poses a more difficult and a new challenge of service interoperability to the growing cadre ofservice providers.service provider interoperability and the national information infrastructure147the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.telephonebased voice services are by far the dominant mode of telecommunications. this dominance canbe expressed in many ways, including percentage of infrastructure consumed, ubiquity of service, and generationof revenue. voice services have evolved over the past 100 years to where significant complexities in the deliveryof service have become transparent to the typical user. the voice service infrastructure also supports a largeproportion of all dataoriented services through its support of modembased communications. in general, voiceservices involve two handoffs between carriers: a local carrier passes a call to the ixc, and the ixc passes thecall to the destination local carrier.telecommunications carriers also provide private line services, which allow users to connect separate siteswith basic transport services. a private line represents an extension of the carrier infrastructure to an individualuser; the carriers devote a dedicated time slot of their network to that customer. a user may attach a large varietyof customer premises equipment (cpe) to the private line to support virtually any type of service. large users oftelecommunications use private line services as the cost of dedicated services tends to be high relative to that forswitched services such as voice (note that while customers only pay for voice services when they are in use,private lines incur flat rate charges independent of usage). paradoxically to the higher relative costs, private linesrepresent the least possible value added by the service provider. however, private line services provide theunderpinning of virtually all services. for example, the internet has long been based on the connection of devicesthat implement the internet protocols (i.e., ip routers) with private lines.in fact, service providers distinguish their services from customer use of private lines by the movement ofvalueadded equipment from the customer premises to the service provider premise. for example, in the case ofinternet services, many large enterprises have deployed internet technology within the enterprise as ip routersinterconnected by dedicated services. in effect, these enterprise networks are analogous to a private internet (i.e.,many enterprise networks interconnected). conversely, an internet service provider deploys internet technologywithin its infrastructure and offers service through dedicated connections from individual user locations to thebackbone network. similarly, enterprise networks based on local intelligent equipment such as smartmultiplexers, x.25 equipment, or framerelay technology continue to proliferate.public data services originated with the same government development activities as the internet but moveddirectly to the support of commercial services. the emergence of public data networks based on the x.25protocol suite through the 1980s created a global infrastructure for data transport. however, unlike the internet,x.25 networks tended to focus on services for individual users over a virtual private network, rather thanbetween independent networks. these virtual private networks (a concept later extended to voice services)allowed many users to share the backbone of a single service provider. these x.25 networks have recently foundnew life as the access vehicle of choice for consumer information services, such as america online, owing totheir deployed dialup facilities. x.25 networks also offer a working model for carrier network interconnection inthe form of the x.75 gateway, the technical designation for interconnection between x.25 networks provided bydifferent publicservice providers. however, while x.75 offers the technical mechanism for interoperability,competing service providers have been reluctant to deploy x.75 connections. public data services have alsoreemerged in the form of framerelay and atm services.similar to the emergence of public services for data communications, carriers have initiated the delivery ofpublic videoconferencing services over the past several years. a videobased service takes the form of providingtelecommunications service between public or private video rooms. the video rooms, essentially conferencerooms modified to support video teleconferences, are interconnected via carrier facilities. the carriers haveoffered reservation and gateway services to allow users to schedule facilities ahead of time and ensure that thevideo systems at both the source and destination ends will interoperate. the ability of a service to offerinteroperability between disparate types of video equipment has become an important feature of the overallservice. in a limited fashion, the video teleconference capabilities of the carriers support interoperability,although primarily in the context of specific user implementations rather than generic service features.the nii can be expected to increasingly take advantage of a wide range of service capabilities. today'scommon model for the nii, the internet, tends to focus on data exchange modalities. conventionaltelecommunications services offer limited models for interoperability among competitive carriers or foradvanced services. we can expect that the nii will increasingly embrace multimedia services that will require arobustservice provider interoperability and the national information infrastructure148the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.service infrastructure and place demands on service provider interoperability that exceed the scope of existinginteroperability models.emerging infrastructure and servicesthe strong growth and competition in the telecommunications industry have accelerated the introduction ofadvanced technologies to the existing infrastructure. the result is a dynamic set of services that offer greatlyenhanced services and new capabilities to the nii. these capabilities will begin to change the manner of deliveryof information services, likely with significant improvement in price and functionality. however, the earlydefinition of these services has tended to follow existing models for carrier interoperability (i.e., via private lineor eventually local carrier interconnection to the ixc). rather than attempt a comprehensive review of emergingtechnologies, the following section discusses the potential of asynchronous transfer mode (atm), a good modelfor the introduction of new telecommunications services enabled through advanced technology.advanced technologiessince its definition in the mid1980s, atm has represented something of a holy grail to thetelecommunications community. atm, as defined, supports all communications services and thereby representsa merging of the telecommunications and computing portions of the information technology industry. theformation of the atm forum in 1991, now including over 700 members from all segments of industry andgovernment, sparked the development of atm specifications to support early atm implementation. theemergence of early atm capabilities in late 1992, along with atm's steady growth over the past 30 months, hascreated considerable excitement as the harbinger of a new era of networking consistent with the nii. over thistime atm has taken a preeminent position on the leading edge of the telecommunications industry. atmsimultaneously offers service providers more effective operation and users greater flexibility. service providers,from large public carriers to the telecommunications groups of private organizations, have embraced the atmconcept as a means to streamline internal operations and improve the delivery of user services. for example,atm will enable carriers to significantly reduce overall provisioned bandwidth and allow instantaneousprovisioning of service through the statistical multiplexing capability of an atm backbone. although fiberopticnetworks have greatly reduced the overall cost of network capacity, carrier investments in network facilities stillrepresent billions of dollars in investment and operating expense. the ability of an atm approach to reducecosts and improve operations constitutes a significant market advantage. in addition to enhanced internal factors,atm will enable service providers to deliver advanced services at a fraction of current prices and create newmarkets for services.the vision of an atmbased network also offers significant advantages to the user community. existingtelecommunications services mirror the carrier networks, where multiple independent networks share a commontime division substrate to deliver separate services. for example, the voice, internet, and video servicesmentioned here exist as separate networks within the carrier facilities, requiring separate capacity, operationalsupport, and management. users tend to have a relatively small set of options for service, which are rigidlydefined. atm offers a dramatically different view. a customer may send any type of informationšvoice, video,data, image, or multiple types at oncešat transfer rates from 1 to 1 billion bits per second. atm also promisesindependent quality of service guarantees for each application, thereby allowing users to select services withintheir own context of budget, urgency, and quality, all on a dynamic basis. despite the great promise of atm,true interoperability of atm services offered by competing carriersšthat is, addressing technological,operational, and business concernsšappears several years away.service provider interoperability and the national information infrastructure149the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.emerging servicesthe evolution of information technology has significantly enhanced the capabilities and range ofinformation services. numerous reports and articles have already described emerging information services, butthree trends become important to future service provider interoperability: information services as an entity are growing. providers such as prodigy, compuserve, and america onlineare showing phenomenal growth. the internet also continues to grow rapidly, especially the phenomenon ofthe world wide web. the access and availability of critical information can become a differentiatingfeature of a service provider. many telecommunications carriers have described intentions to provide information content. though theconcept of carriers providing information has been slow to develop, and somewhat dominated byapplications such as video movies on demand, it seems likely that more useful and broader cases will arise.several carriers have developed prototype capabilities for the support of applications such as collaborativeresearch, distance learning, and telemedicine. telecommunications service providers will try to move away from competition founded in the lowest pricefor a commodity service and differentiate their transport through information content.the continued evolution of information services and their combination with telecommunications willstrongly influence the nii. the influence will be seen in the changing nature of competition betweentelecommunications carriers, information providers, and new hybrids of the two. we can expect thatcommercialization of information services will affect interoperability as a market driven by information contentmay evolve.analysis and forecastinteroperability among telecommunications carriers is a key attribute of the infrastructure. for example,voice or switched services and private line and internet services all support interconnection between usersconnected to different carrier services. meanwhile, private networks and emerging services tend to struggle forinteroperability beyond specific provider boundaries. the current models for interoperability among serviceproviders offer a reasonable baseline today. however, key trends such as regulatory changes, market forces, andtechnology development are emerging that affect the continued assumption of interoperability.current models for interconnectionthere are three significant models for service provider interoperability that may come into play within thefuture nii. since divestiture, local carriers have set up agreements with ixcs for the exchange of services. theseagreements have focused on two types of service: switched service supporting voice; and private lines thattransport many types of information, with little value added from the carriers aside from provision of theconnection. although these arrangements are technically sophisticated, the most important feature involves thebusiness mechanisms that track the exchange of information and convert the volume of information to financialsettlements. meanwhile, the data community has set up interoperability models consistent with data exchange.the x.75 protocol was developed to address the exchange of information between national public data networksthat employed the x.25 protocol. the internet is thriving and is synonymous with the concept of interconnectednetworks. it presents a powerful model for the nii, often used interchangeably with future nii concepts.however, today's internet does not support explicit business mechanisms for financial settlement but has begunto address settlements between service providers and is likely to evolve to some appropriate model. the existingmodels for interconnection offer limited guidelines for future service provider interoperability.service provider interoperability and the national information infrastructure150the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.lecixc interconnectthe most important instance of service provider interconnect, in terms of the volume of traffic handled,supports the connection of switched traffic, such as voice, and private lines between lecs and the ixcs. thisrelationship emerged with the divestiture of at&t in 1984, the establishment of many local carriers, and thegradual introduction of competition in the ixc area. the interconnects are mandated by the agreementssurrounding the divestiture providing for equal access to local facilities to any ixcs prepared to provide service.the lectoixc interconnect employs sophisticated technology oriented to the establishment ofconnections on a dynamic basis between users that often span three service providers. both lecs and ixcsestablish multiple connections between their networks and other carriersšfor example, bell atlantic supportsinterconnects to sprint, at&t, and mcišand the interconnect discipline has thus been replicated many times.each of the carriers has invested heavily in improving the signaling that controls these interconnects. improvedsignaling both increases performance (e.g., in the sense of call setup time) and enhances feature availability. inaddition to the technology inherent in these interconnects, the establishment of business mechanisms thatmeasure traffic exchange and enable financial settlements represents a major contribution to the interconnectmodel. the volume of both the traffic (tens of billions of call minutes per month) and the financial exchange(billions of dollars per year) makes the current lecixc a powerful model for interconnect.the lecixc model does not necessarily represent the best possible approach. the interconnect ismandated through regulatory means rather than through the dictates of an open market. the absence of anysubstantial interconnect scheme between competing service providers suggests that the current scheme may notextend well to competitiveservice providers. for example, interconnection between sprint, mci, and at&t ismostly nonexistent; it is unclear whether typical lec to ixc agreements would translate to two ixcs.the technologies associated with this interconnect have not been shown to adapt well to other types ofservices. the ixclec emphasis on the lowest common denominators of telecommunications, switched voiceservices and private lines, does not extend well to an nii vision that takes advantage of various advancedtelecommunications services. recent efforts have begun to address the interconnection of advanced services suchas atm and frame relay between lec and ixc. however, operational interconnects have only reached planningstages, with issues like service guarantees and appropriate financial settlements still poorly understood.public data network interconnection: x.75a second form of carrier interconnect in place since the early 1980s and widely deployed is the gatewaybetween public data (x.25based) networks, referred to as x.75. this technology applies to gateways that mayexist between public x.25 data networks, often those operated by national telecommunications carriers (i.e.,france telecomm; in the united states, the primary x.25 carriers are mci and sprint). the x.75 interconnectoffers an historical model for large data service provider connection in a commercial environment. thoughlimited in functionality and becoming obsolete, the x.75/x.25 model is important because it supports the dataenvironment, involves financial settlements between carriers (including international entities), and bears somesimilarity to the emerging atm environment (i.e., it supports connectionoriented service).x.75 operates by setting up virtual connections from user to user as a spliced connection of multiple virtualconnections. in this sense x.75 represents the opposite of the internet ip protocol, which uses a connectionlessmodel for service delivery. x.75 also thereby tracks current efforts toward the development of atm services,which is also connection oriented and a potential model for future services. the x.75 gateways also includemechanisms for the tracking of data exchange and settlement procedures between networks. as an interconnectbetween public networks, the x.75 model has the added advantage of defining interconnects betweeninternational entities. many countries still operate a single, monopolized telecommunications carrier andtherefore a single public data network. notably, the evolution of public data networks has tended to downplaythe interoperability offered through x.75 connections. the expansion of global services by primary carriers hascreated direct competition between service providers and resulted in reduced desire for interoperability.service provider interoperability and the national information infrastructure151the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.internetthe internet offers an interesting model for interoperability. from its inception, virtually anyone couldconnect to the internet, given compliance with the set of procedures and formats dictated by an evolving set ofinternet standards. by the late 1980s an architecture emerged that included a backbone transit network, thensfnet (which supported connectivity between regional service providers) and an acceptable use policy(which restricted traffic to research and education). the nsfnet exchanged traffic with other majorgovernment backbone networks, through federal internet exchanges (fixs) on the east (fixeast) and west(fixwest) coasts. the fixs provide a technical and policybased interchange point, but they do not includemechanisms typically found in business environments for collection of traffic data and billing.the growth of the internet has precipitated commercialization. at first, small service providers and thengradually larger service providers (including sprint and then mci) developed services that employ internettechnologies and offer connectivity to the internet (a combination usually referred to as internet service). thecommercial service providers created a commercial internet exchange, the cix, to allow the exchange ofcommercial, policyfree traffic. the shift to commercialization, with the government as a major catalyst, isprompting the gradual withdrawal of government influence. through the nsf's solicitation 9352, the nsf hasdecommissioned the nsfnet as the transit backbone for the domestic internet and promulgated a commercialinfrastructure that includes three primary network access points (similar to fixs) and commercial networkservice providers (nsps). both the naps and the cix are provided by thirdparty vendors (the facilities for eachare actually provided by carriersšsprint, ameritech, pacbell, and mfs) and support the exchange of traffic.at this time there are limited barriers or policy for connection at a nap or a cix. the nsf has set a policythat any entity funded by the nsf must employ an nsp that is connected to all three primary naps. in this waythe nsf has ensured that the research and education of the internet will be fully connected, at least for the 2 to 3years planned for the nsf funding. within each access point members must establish separate bilateralagreements to facilitate interconnection. in some cases, service providers have included place holders forsettlement agreements in these bilaterals; however, no agreements have been set at this time.attributes of service provider interconnectionthe preceding profiles of major interconnection methods suggest several key attributes for service providerinterconnection. technology. the interconnection method must employ technologies capable of supporting the types andvolume of information exchanged. the technologies must provide scalability to ensure that future growthcan be supported. flexibility. the interconnection should support flexibilityšfor example, through support of multiple formsof information, including voice, video, and data. replicable. the interconnection should enable replication and preferably employ agreedupon standards thatrequire consensus among a broad part of the service provider community. stability. the introduction of new members to an interconnection point should not affect the existing serviceproviders. operational support. the interconnection point should be supported by solid operational agreementsbetween connecting members to address failures in the connection. management. the interconnect point should enable oversight of activities at the interconnect point to includemeasurement of traffic exchange, service quality, and security. financial. the interconnect point should define methods for settlement of traffic between service providers.service provider interoperability and the national information infrastructure152the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.key trends affecting carrier interoperabilitytoday's telecommunications environment exists through a web of interoperability agreements amongnumerous service providers. in general, these agreements exist to facilitate the fundamental exchange ofinformation. in virtually all cases the agreements exist between noncompetitive entities where cooperationbenefits both parties. however, many commercial and regulatory trends have begun that threaten this basis forservice provider interconnection. the expected relaxation of constraints on lecs for the provision of longdistance services will create a new competitive relationship among lecs and ixcs. both groups have rushed toform strategic relationships and extend capabilities to prepare for this new era. in addition, astelecommunications services have become more of a commodity, service providers have looked for ways todifferentiate their services through technological improvements as well as in combination with informationservices. the internet is undergoing a parallel change where the trend toward commercialization will createcompetitive relationships among previously complementary entities.current trends in the telecommunications regulatory structure suggest strongly that local carriers will beginto extend their role beyond local transport services to include wide area connectivity. the lecs have alreadywon the right to provide longdistance services for cellularbased customers. local carriers will also continuetheir expansion into other nontraditional service avenues, especially in the area of information service. althoughthe pace of this evolution of the lec role is unclear, it is likely that the local carriers will tend to dominate thelocal infrastructure for many years to come. despite major ixc investments, the extensive, inplace lecconnectivity to every residence will be extremely difficult to duplicate. early in this evolution, it thereforeappears likely that the lecs will dominate locally and have multiple options for longdistance exchange.the ixcs have also begun extensive efforts to move into alternate forms of service, including internetservices, local service, and cellular/wirelessbased services. the ixcs have recognized the large portion of costsapportioned to local service, along with the growing threat of lec competition in the longdistance arena, andhave begun to focus on the extension of facilities to local areas. for example, a major ixc announced apartnership with three major cable companies and a cap in 1994. these activities recognize the likelihood thatthe ixcs will soon directly compete with the lecs for customertocustomer service. the evolving roles oflecs and ixcs suggest the possibility of a paradigm shift in agreements for traffic exchange between carriers.for example, today the flow of financial settlements travels in one direction, from the ixcs to the lecs; futureenvironments where today's lecs and ixcs become competitors suggest a twoway flow.both lecs and ixcs have also recognized that simple transport services will become more like commodityservices. as a result, many carriers have begun to expand their services to include advanced services andinformation capabilities. advanced services, such as atm, enable the service providers to create value with theirofferings and differentiate themselves from other providers based on the features and capabilities of the servicesoffered. the addition of information services on top of the transport presents a further opportunity for carriers todifferentiate. a likely evolution of information services and telecommunications will bind the two together. thepurchase of information services will therefore imply the selection of a specific carrier.the commercialization of the internet presents a parallel scenario for interoperability. as in the traditionalcarrier networks, interoperability is a defining feature of the internet. however, the open exchange ofinformation that created the internet has occurred without the influence of business considerations. as theinternet has grown, its potential value to commercial ventures has become apparent. these trends have resultedin an internet dominated by commercial service providers (note that sprint's sprintlink service recently becamethe largest global supporter of the internet, surpassing the nsfnet). as commercial ventures, internet servicesstill represent a small fraction of carrier revenues. therefore, today internet service providers exchangeinformation more or less freely through agreedupon exchange points without traffic measurement orsettlements. however, as revenues grow, one can reasonably expect that competitive influences will drive theinternet toward more concise, businessoriented operation.interoperability tends to contradict service provider efforts to move away from commoditization. as thecarrier networks tend to interoperate, they share features and functionality and offer access to the same sourcesof information, thereby reducing the differentiation associated with informationbased services. interoperabilitybetween emerging services, such as atm, has also evolved slowly. although this is partially due to theservice provider interoperability and the national information infrastructure153the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.immaturity of the technologies, the desire for stable customers also contributes to inertia over serviceinteroperability. this is desirable from an nii perspective, but the creation of interoperable networks tends tocounter marketplace tendencies among competitors.summaryexpansion of the roles and capabilities of lecs and ixcs will eliminate the current stratification betweenlocal and longdistance services. it will also erase the natural requirement for interoperability between lecs andixcs. financial settlements for the exchange of services will follow marketbased formulas dictated byindividual service providers. the larger carriers may be able to leverage their greater reach of services to forceunbalanced settlements. although the evolution of these trends is uncertain, the coexistence of parallel,competing service providers suggests an unstable environment that may affect endtoend connectivity and thesupport of emerging nii applications.the commercialized internet will begin to mirror the evolving carrier environment over time. the majorixcs have already captured prominent positions in the commercialization of the internet. as internet servicesreach traffic/revenue levels comparable to those of mainstream carrier services, service providers will movetoward settlement scenarios that will once again favor the larger providers. the traditional carriers own thefacilities and a growing share of the expertise that make a growing, commercial, unsubsidized internet feasible.however, numerous nontraditional service providers now make up a substantial portion of the internet. thoughthese providers depend on the traditional carriers for physical connectivity, they have established strongfoundations based on internetlevel services and a growing ability to delivery information services. segmentationof the internet appears highly unlikely, but the effect on connectivity is at least uncertain.the possibility of limited interoperability among service providers, including those offering internetservices, should have a profound effect on the emerging nii. in principle, many of the applications contemplatedin nii literature would still be feasible; but numerous obstacles would almost certainly emerge. for example, theconcept of telemedicinešthe delivery of medical services over the niišimplicitly assumes a seamlesstelecommunications fabric. otherwise, medical resources that reside on one service provider's network could notreach a patient on another network. although this scenario is consistent with the desire for service providers todifferentiate, it clearly threatens the realization of telemedicine services.recommendationsthe recommendations presented here suggest several possible approaches to ensure the continuation andexpansion of interoperability among telecommunications service providers. overall, further analysis needs to beconducted on the nature of interoperability for the nii and the suitability of the existing models for thisrequirement. today it seems that interoperability among competing service providers has been assumed as anatural attribute of a telecommunications infrastructure. on the contraryšthere are numerous examples wheretoday's competing service providers do not interoperate. furthermore, the evolution of the telecommunicationsinfrastructure in terms of technology, economics, and service relationships will tend to eliminate existing areasof interoperability and provide disincentives for further development of interoperable services.the regulatory changes under way, one of which will relax restrictions on lecs, represent a majorinfluence on service provider interoperability. explicit consideration of the effect of interoperability of theseevents should be undertaken. in addition, serious consideration should be applied to additional regulatory actionsnecessary to ensure continued interoperability among service providers in the nii.the internet represents a national and global treasure of information. as the internet continues towardcommercialization, the effects on interoperability among service providers become less clear. the growth of theinternet has brought traditional service providers into the role of internet service delivery. although thetraditional carriers play a key role in the scaling of the internet because of their ownership of the physicalinfrastructure, they also bring the traditional service models and notions of interoperability. the future of theservice provider interoperability and the national information infrastructure154the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.internet is widely debated, but specific attention should be given to interoperability as the internet migrates to afully commercial enterprise.numerous technological factors will affect the evolution of the nii and the interoperability among serviceproviders. prototype models for carrier interconnection for various services, technologies, and special usercommunities should be established to determine feasibility and preferred approaches for deployment in the futurenii. the naps offer an excellent platform for these prototypes, as they represent by definition a means forinteroperability among service providers.service provider interoperability and the national information infrastructure155the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.18funding the national information infrastructure:advertising, subscription, and usage chargesrobert w. crandallbrookings institutionthe stunning changes in information and communications technologies are focusing attention on the gapbetween the potential services available from new telecommunications networks and the much more limitedservices now available from our existing communications infrastructure. while this focus is useful in galvanizingpolitical support for action, it may divert attention from the magnitude of the economic task involved in buildingthe new information superhighway. investors will be committing enormous resources to risky new technologieswith uncertain information on future service revenues, the cost of these services, and the prospect of furthertechnical breakthroughs that could render new facilities obsolete in a short time.in this environment, policymakers must be careful not to foreclose the opportunity to develop new facilities,new services, and new sources of economic support for both. the federal and state governments should beencouraged to allow new services to develop over a variety of distribution systems and to permit vendors ofthese services to select the funding mechanisms for such systems or services. indeed, it is precisely because wehave already begun to open our telecommunications markets to competition and to reduce government regulationof them that we are now likely to develop the new national information infrastructure more rapidly than mostcountries and that other countries are moving to emulate us.1in this paper, i review the recent historical data on the evolution of u.s. communications and mediaindustries with a particular focus on the sources of revenue that have propelled this growth and the effects ofrestrictive government regulation. in addition, i provide a brief review of the potential costs of the newtechnologies and the uncertainties involved in implementing them. my conclusions are as follows: advertisersupported media have shown the most rapid growth since 1980. even subscribersupportedmedia, such as cable television or the print media, have relied heavily on advertising for their growth. most of the competing technologies for the national information infrastructure will require large capitalexpenditures, perhaps as much as $125 billion, in a risky environment. regulatory restraints on new media, designed to protect incumbents, have inevitably suppressed the growthof the communications sector and have often proved unnecessary or even counterproductive. in the current risky environment, it would be imprudent to exclude any new services or sources of funds thatcould be used to defray the costs of building the national information infrastructure (nii).note: robert crandall is a senior fellow at the brookings institution. the views expressed herein are the author's and notnecessarily those of the brookings institution, its trustees, or other staff members.the coalition for advertisingsupported information and entertainment, which sponsored this paper, is a joint effort of theassociation of national advertisers, the american association of advertising agencies, and numerous leading advertisersand advertising agencies across the nation.funding the national information infrastructure: advertising, subscription, and usage charges156the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the growth of communications mediai define ''communications media" as all of those delivery systems through which information andentertainment are transmitted to and among businesses and consumers. these include traditional print media(newspapers, magazines, journals, etc.), broadcasting, cable television, satellites, and traditional and newerwireless forms of telephone service. historically, these various media occupied somewhat independent marketsin the united states. the print media provided information and advertising aimed at specialized national marketsor at decidedly local markets. radio broadcasting provided entertainment services for which it had little directcompetition other than records, but it competed with print media in disseminating local and national news. whentelevision broadcasting emerged, it began to offer entertainment that competed with motion picture theaters, butit too became an important distribution medium for national and local news. cable television developed as asimple form of televisionbroadcasting retransmission, butšwhen allowed to develop without regulatory controlsšrapidly became a medium for distributing a wide range of entertainment and information services. morerecently, direct satellite transmissions have begun to compete with cable television and broadcasting in thedistribution of national entertainment and information services.virtually all of the above media have developed with at least some form of advertiser support. in the case ofcable television, it was the fcc's decision to allow cable operators to import distant advertisersupported signalsthat provided the impetus for cable systems to expand into large urban markets and to increase their channelcapacity. these expansion decisions, in turn, facilitated the development of new basic cable networks that werealso supported by advertising.the telephone industry, on the other hand, has traditionally been supported by direct customer access andusage payments. the only source of advertiser support for traditional telephone carriers has been from thepublication of yellow pages directories. this reliance upon direct customer support was dictated by the nature ofthe service: twoway switched voice communications. it would have been difficult and disruptive to haveadvertisements interspersed with these conversations. however, with the development of computer connectionsthrough modems or localarea networks, the delivery of information and entertainment services over telephonecircuits became possible. as with other communications services (other than voice telephony), customers maynow be offered a choice: pay for these services directly, accept advertisements in them, or accept somecombination of the two. for example, electronic yellow pages may be advertiser supported, require a subscriberusage charge, or both. similar options may be available for electronic want ads, homeshopping services, or evenelectronically delivered financial services.the role of advertising revenues in the growth of the various media is evident in table 1. since 1970, therehas been substantial growth in all media that offer advertisersupported content. video markets, in particular,have grown substantially, but even the print media have enjoyed an expansion of total revenues from advertisingand direct circulation fees. in recent years, multichannel video media, such as cable television and directbroadcast satellite (dbs), have grown more rapidly than traditional radiotelevision broadcasting. in the next fewyears, as additional dbs services and even terrestrial multichannel wireless systems are built, video servicerevenues are likely to continue to grow, supported by both advertising and direct consumer outlays.for more than a decade, regulation restricted the growth of pay or subscription television services in orderto protect offtheair broadcasters. this, in turn, limited cable television to the retransmission of advertisersupported offtheair television broadcasting. this repressive regulation was slowly eliminated by the courts, thefcc, and the congress in the period from 1977 to 1984, spurring cable to grow very rapidly and to expand itschannel capacity.even though cable subscription revenues grew very rapidly, however, broadcasting revenues continued toexpand, in part because cable retransmissions made marginal uhf stations more viable, a result predicted byacademic studies of the medium.2 thus, cable subscription growth induced further growth of advertisersupported broadcasting. but even though cable operators would be able to survive and even expand by relyingsolely on direct subscriber revenues, they also found that they could now tap advertising markets to grow evenmore rapidly. direct cable television advertising expenditures began to grow very rapidly in the 1980s,increasing from virtually nothing in 1980 to nearly 20 percent of total cable revenues in 1993.funding the national information infrastructure: advertising, subscription, and usage charges157the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 1 the growth of communications media revenues since 1970yeartv homes(million)tvadvertisingrevenue(billion $)cable tvhomes(million)cable tvsubscriberrevenues(billion $)cable tvadvertisingrevenues(billion $)tv/cablerevenues pertvhousehold ($)newspaper &periodicalcirculationrevenues(billion $)newspaper &periodicalsadvertisingrevenues(billion $)telephoneindustrytotalrevenues(billion $)telephoneindustrydirectoryadvertising(billion $)197058.53.65.10.3š623.0a7.019.40.7197568.55.39.80.8š775.3a9.733.41.1198076.311.417.52.4š1499.2a18.159.82.3198584.920.336.78.80.824913.230.992.32.3199090.826.851.717.22.532317.940.1114.62.6199393.928.757.222.24.034820.341.2128.72.9(a) estimated by the author from census data.sources: paul kagan associates; statistical abstract of the united states; fcc, statistics of communications common carriers.funding the national information infrastructure: advertising, subscription, and usage charges158the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 2 real (inflationadjusted) growth in revenues for video, print, and telephone sectors, 1970œ93 (1982œ84dollars)yeartelevision broadcasting andcable televisionnewspapers and periodicalstelephone companies197010.025.850.0198016.733.172.6199338.042.689.1average annual growth,1970œ805.1%2.5%3.7%average annual growth,1980œ936.3%1.9%1.6%note: all data deflated by the bureau of labor statistics consumer price index.the telephone industry, supported almost exclusively by subscriber payments for conveying voice and datamessages, has grown much more slowly. telephone companies have not moved aggressively into the provisionof content, in part because of regulatory policies that have excluded them from some of the most important ofthese markets. as a result, their growth has been much slower than that of the print and video media. indeed, inreal (inflationadjusted) terms, telephone revenues increased at a rate of only 1.6 percent per year from 1980through 1993. (see table 2.) during the same period, broadcasting and cable revenues rose at an annual real rateof 6.3 percent per year, and newspaper and magazine revenues rose at a real rate of 1.9 percent per year.obviously, telephone companies recognize that their growth has slowed dramatically and have thereforemoved aggressively to seek relief from regulation that has kept them from offering new services. in this pursuit,the regional bell operating companies (rbocs) have won court reversals of the restriction on their provision ofinformation services that was part of the decree breaking up at&t.3 in addition, invoking the first amendment,several rbocs have recently mounted successful first amendment challenges to the legislative ban ontelephone company provision of cable television service in its own region.4new communications technologiesin the next few years, cable television operators, telephone companies, and satellite operators will be facingoff in the competition to build new broadband distribution networks. these new networks, utilizing a mixture ofwireless and fiber technologies, may offer a variety of switched and distributive broadband services. at thisjuncture no one knows which technologies will ultimately succeed, no which services will propel these newtechnologies.5 this is a risky environment for private investors and policymakers alike given the magnitude ofthe necessary investments.cable television operators now have oneway "treeandbranch" broadband networks composed of fiberoptics and coaxial cables. these networks are typically unable to offer switched services to most subscribers,although such services are possible with major investments in switching equipment and changes in networkarchitecture. to upgrade cable networks simply to provide switched voice (telephone) services would probablycost $300 to $500 per subscriber.6 to provide full interactive switched video might cost as much as $800 persubscriber depending upon the amount of fiber optics already in the cable system's feeder plant.telephone companies are now exploring technologies for expanding the bandwidth of their networks so thatthey can offer video dialtone or full switched video service. many companies are pursuing a fiber/coaxial cablearchitecture that is likely to cost $800 to $1,000 per home passed.7 a full fibertothecurb architecture might bemore expensive but provide greater flexibility in terms of bandwidth available for oneway and interactiveswitched video applications. simply upgrading the existing copper wire distribution plant with asynchronousdigital subscriber loop (adsl) technology8 is much more expensive per subscriber, but allows the telephonecompany to target its investment at only those households that wish to subscribe to its video services.a recent entrant into the video distribution market is highpowered satellite delivery to the home. thisdirect broadcast satellite (dbs) service has been launched by hughes aircraft and hubbard broadcasting andfunding the national information infrastructure: advertising, subscription, and usage charges159the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.will soon be emulated by two other consortia, including one that now operates with a lowerpowered cbandservice. each of these enterprises requires about $750 million to $1 billion in initial investment plus possiblestartup losses.9other wireless services are attempting to compete with cable systems for oneway, distributive videoservices, but none has yet been shown to be a viable launching pad for the interactive broadband network of thenii variety. it is possible, however, that terrestrial wireless services can be adapted for this purpose, particularlyif the fcc shifts to using market mechanisms for all spectrum allocations. multichannel multipoint distributionservices (mmds) or cellular cable systems could have sufficient channel capacity, particularly with digitalsignal compression, to offer competition to wired systems. finally, it is possible that a system of orbitingsatellites, such as that being developed by motorola for narrowband communications, could also provideswitched broadband services.this brief review of the technical possibilities for building the nii is not intended to be comprehensive.rather, it is included to show that the number of competing technologies is growing and that the ultimate winneror winners is (are) not known or even likely knowable at this time. some telephone companies are reexaminingtheir earlier commitments to a fiber/coaxial cable technology.10 at least one large cable television/telephonecompany merger has failed, and other consortia may be foundering. dbs systems may have arrested some of themomentum for building interactive terrestrial broadband networks because the latter would probably have to relyheavily on traditional oneway video programming at first. however one interprets the recent technological andeconomic developments in this sector, it is quite clear that building versions of the nii is not becoming less risky.even at costs as low as $800 per home, the cost of extending the nii to all residential households is at least$80 billion. to extend it to all business and residential customers would require at least $120 billion.11 this latterestimate is almost exactly equal to the book value of the entire net plant of all regional bell operating companiesand about 80 percent of the book value of the net plant for all telephone companies in the united states.12 thus,if the nii is to be built by established telephone companies with technology now under development, it wouldprobably require a near doubling of these companies' assets. since no one is seriously advocating governmentsupport that would cover even a small fraction of this added investment, private firms must have the ability toobtain large cash flows from a variety of sources to fund such a massive expansion of our communicationsinfrastructure.the sobering fact about any such large investment in the nii is that it will likely be overcome by differentor improved technology soon after it is built. perhaps such networks can be continuously adapted to thesetechnical improvements, but it is also possible that a completely different technologyšperhaps based on orbitingsatellites, for examplešcould render an $80 billion to $120 billion terrestrial investment obsolete within adecade. under these circumstances, investors should be permitted to explore all possible sources of revenuesfrom the marketplace if we expect them to commit such large amounts of capital to so risky an enterprise.recent trends in communicationssector capital formationthe explosion in new technology might be expected to generate a boom in investment in the variouscommunications sectors. the advances in fiber optics, electronic switching, asynchronous switching, digitalcompression, and consumer electronics are providing all market participants with the opportunity to deliver amultitude of new services if they invest in new equipment.the evidence on capital formation in the communications sector only partially reflects this rosy assessment.telephone companies responded to the new opportunities by investing in new switching systems and fiberopticsdistribution systems through the early 1980s. much of this investment was spurred by competitive entry intolongdistance markets and even some urban localexchange markets and by the necessity for local carriers toprovide equal access to these new competitive carriers in their switching systems. thereafter, however,telephone company investment slowed markedly. (see figure 1.) in the past 5 years, investment in this sector ofthe economy has been insufficient to maintain the net value of its capital stock.13by contrast, the radiotelevision sector continued to expand its net capital stock through 1993. fueled bycable deregulation in 1977œ79, the net capital stock in this sector nearly doubled between 1979 and 1989šafunding the national information infrastructure: advertising, subscription, and usage charges160the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.period in which telephone plant increased by less than 20 percent. (see figure 2.) this growth reflects thesubstantial expansion in the number of homes wired by cable in the 1980s as well as the investments inincreasing channel capacity during these years.figure 1 real net capital stockšu.s. telephone industry, 1970 to 1993.figure 2 real net capital stockštelevision and radio industry, 1970 to 1993.a lesson to be learned from these divergent trends in capital formation is that regulatory constraints thatrestrain competition and new entry, whether in the name of promoting "universal service" or "fairness," create asubstantial risk of slowing investment and the spread of new technology. cable television investment acceleratedafter the severe regulatory restrictions on cable service, motivated by the desire to protect offtheairbroadcasting, were lifted in the late 1970s. the removal of these restrictions caused cable revenues to riserapidly, fed by both advertising and subscriber fees, while traditional broadcast advertising revenues continued torise. telephone investment rose rapidly in the 1970s after entry restrictions on longdistance competitors werelifted. once restrictions were placed on telephone company offerings of information and cable televisionservices, however, investment in traditional telephony slowed dramatically. were new investments in the nii tobe constrained by the desire to crosssubsidize new universalservice offerings or by a concern that advertisersupported servicesfunding the national information infrastructure: advertising, subscription, and usage charges161the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.might supplant other services, the prospects for rapid deployment of risky new technology could be severelyimpaired.advertising versus subscriber feesšthe implications for economicwelfareeconomists have studied the economic welfare implications of using advertising or direct subscriberpayments to support radio and television programming. the results of this research suggest that a mix of the twosupport mechanisms is likely to be the optimal source of revenues.14 the reasoning is fairly straightforward.advertisers are likely to be motivated by the desire to maximize their reach among a target population at thelowest possible cost consistent with persuading consumers of the attractiveness of their products. for this reason,they are likely to eschew the support of programming (or other software) that appeals to a narrow set of potentialconsumers. thus, advertisersupported services are likely to be less finely differentiated than are subscribersupported services under most circumstances. if some consumers are not willing to pay much for highlydifferentiated services, their economic welfare will be greater with advertisersupported services. it is for thisreason, presumably, that more than onethird of all households now passed by cable television do not currentlysubscribe despite the fact that cable offers much more choice than the available array of broadcast stations ineven the largest markets.on the other hand, some consumers may desire more choices than those that can or would be supportedtotally by advertising. subscribersupported services can reflect the intensity of consumer preferences, notsimply their willingness to use a servicešthe criterion that is important to advertisers. it is for this reason thatpaycable and payperview services have proliferated in the current multichannel video environment. only afew hundred thousand subscribers, each paying a substantial onetime user fee, may be required to support twoor three hours of a nationally distributed service or event. foreign soccer games, obscure older feature films, orlive concerts may be offered for a onetime charge, while more popular events may be advertiser supported.a third possibility for supporting services on the current or future information infrastructure is a mixture ofuser charges and advertiser support for the same service. payperview services could still carry advertisingbillboards or even interspersed advertising messages. for example, videocassettes and motion picture theatersare now supported by both subscriber charges and advertising. in addition, the sam programming could beoffered simultaneously without advertising but with a subscriber fee and with advertising and no subscriber fee.this combination of support mechanisms could lead to greater diversity and a larger array of available services.it is important, therefore, that policies for developing the nii should not directly or inadvertently excludenew service options that might contribute importantly to the cash flows necessary to amortize the largeinvestments required to build the infrastructure. as long as there are no prohibitions on advertiser support ordirect subscriber payments in a competitive marketplace, there is little reason for policymakers to attempt toprejudge the choice of support mechanisms for the current information infrastructure or for the nii. it can alwaysbe argued that virtually any information or entertainment service has attributes of a public good. one consumer'suse of such a service may not reduce the amount of it that is available for other consumers; therefore, noconsumer should be excluded from consuming it through a price that is above zero. advertising could helpprovide a solution to this problem since it can be employed to generate free broadcasting or print services withlow subscriber fees, for example. however, without a combination of advertising and direct subscriber fees,there is no mechanism for consumers to indicate the intensity (or lack of intensity) of their preferences.15regulating the niiit is impossible for anyone to understand fully the potential of the changes that are now occurring intelecommunications technology. even if the technical choices for network design could be forecast for the next10 or 20 years, no one could possibly predict how these technologies would be used. the potential changes in thedelivery of financial, shopping, entertainment, medical, and government servicesšto name a fewšthat are likelyfunding the national information infrastructure: advertising, subscription, and usage charges162the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.to occur are so vast that they cannot possibly be forecast with any precision. but the ideal design of newnetworks will be highly influenced by both technology and the evolution of and demand for these new services.therefore, investments in new networks will necessarily take place on a trialanderror basis.in this environment, it would be counterproductive for government policymakers to attempt to guideinvestments or to require carriers to offer services to segments of the population at nonremunerative rates in thename of "universal service." such demands can only be made on carriers that are protected from competition inother markets because intrafirm subsidies require that the firm earn supracompetitive returns somewhere. ifcertain (early) nii services are selected as the domain for universalservice requirements, the protection affordedthe carrier(s) in other markets may inhibit entry and the introduction of new technology elsewhere. this wasprecisely the outcome of the fcc's protection of television broadcasters from the development of cable in the1960s and 1970s and of the federal/state protection of telephone companies from competition in customerpremises equipment, longdistance services, and wireless services.the calls for preferential access to the nii from a variety of worthy public institutions are understandable,but they should be resisted if they require carriers or other service providers to crosssubsidize them from otherservice revenues. if there is a legitimate case for subsidizing some of these institutions' access to the nii, itwould be far better to provide these subsidies directly from public tax revenues. otherwise, the builtin crosssubsidies will serve as the rationale for barring new entrants with even newer services in the years to come. ifthere is one lesson to be learned from past exercises in regulatory protection in this sector, it is that suchprotection delays new services, such as cable television, cellular telephony, personal communication service,videoondemand, or direct broadcast satellite services. free entry to new facilities, new services, and all sourcesof revenues will provide the fastest route to the nii.notes1. for example, japan is now urgently considering new approaches for allowing competitive entry into its telecommunications markets,including the possible emulation of our approach to breaking up at&t. the european community, which featured entrenched postal,telegraph, and telephone authority (ptt) control of its national telephone monopolies, is now moving to liberalize its telecommunicationssector by january 1998. australia has already admitted a second telephone carrier, optus, and new zealand has not only privatized itsnational carrier and opened its market to competition but has also completely deregulated telecommunications. finally, canada has recentlyadmitted entry into its longdistance sector and is considering the liberalization of local telephone service and video delivery services.2. r.e. park was the first to show that cable television would increase the viability of uhf broadcast stations. see r.e. park, "cabletelevision, uhf broadcasting, and fcc regulatory policy," journal of law and economics, (1972), vol. 15, pp. 207œ31.3. this restriction was one of the lineofbusiness restraints on the rbocs that were built into the antitrust decree that was entered in 1982 tobreak up at&t. the informationservices restriction was removed in 1991. (see michael k. kellogg, john thorne, and peter w. huber,federal telecommunications law. boston: little, brown and company, 1992, section 6.4, for a detailed history of this restriction.)4. the successful challengers have been u s west, bell atlantic, southwestern bell (now sbc), nynex, southern new englandtelephone, gte, bellsouth, and ameritech. these cases have been brought in various u.s. district courts and, in two cases, u.s. courts ofappeals. the supreme court has yet to rule on the issue.5. an excellent example of this uncertainty was provided by bell atlantic in the past month. it has withdrawn its section 214 applications forthe hybrid fiber/coaxial cable and adsl technologies, announcing that it now wishes to pursue the fibertothecurb technology and variouswireless technologies, such as mmds.6. see david p. reed, putting it all together: the cost structure of personal communications services. fcc, opp working paper #28,november 1992, p. 35, and "will the broadband network ring your phone," telephony, december 6, 1993, p. 34.7. this estimate is based on recent filings by pacific bell before the federal communications commission (see, for example, robert g.harris, "testimony in support of pacific bell's 214 application to the federal communications commission," december 14, 1994). slightlyhigher estimates are to be found in david p. reed, residential fiber optic networks: an engineering and economic analysis. boston:artech house, 1992.funding the national information infrastructure: advertising, subscription, and usage charges163the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.8. this technology allows telephone companies to deliver video services over the paired copper wires that are now used to connectsubscribers to the companies' plant.9. see "digital tv: advantage, hughes," business week, march 13, 1995, pp. 67œ68. similar estimates may be found in leland l. johnsonand deborah r. castleman, direct broadcast satellites: a competitive alternative to cable television? rand, 1991.10. bell atlantic has recently withdrawn its section 214 applications for hybrid fiber/coaxial cable technology but remains committed tobuilding a broadband network using other technologies, such as fiber to the curb.11. this estimate is based on a national total of about 150 million access lines. (united states telephone association, statistics of the localexchange carriers, 1993.)12. fcc, statistics of communications common carriers, 1993/94 edition, p. 38.13. it should be noted that part of this slowdown is attributable to the liberalization of customer premises equipment. since the late 1970s,customers have been able to purchase their own telephone handsets, key telephone systems, pabxs, modems, answering machines, and faxmachines. as a result, perhaps 20 to 25 percent of all telephonerelated capital equipment is owned by telephone customers. see r.w.crandall, after the breakup: u.s. telecommunications in a more competitive era. washington, d.c.: the brookings institution, 1991.14. for an excellent review of this literature, see b.m. owen and s.s. wildman, video economics. cambridge, mass.: harvard universitypress, 1992, ch. 4.15. for an excellent compendium of studies of the publicgoods problem, see t. cowen (ed.), the theory of market failure. fairfax, va.:george mason university press, 1988.funding the national information infrastructure: advertising, subscription, and usage charges164the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.19the nii in the homed. joseph donahuethomson consumer electronicsa successful national information infrastructure (nii) strategy must include a clear vision, plus an actionprogram directed at bringing nii usage in the home to the pervasive level at which we use telephones andtelevision today.fortunately, there is an approach that allows the unique strengths of two large industry groups to be appliedto the introduction of extensive nii capabilities into a full cross section of american homes. these two industriesare broadly defined as the computer and television industries. neither alone can provide the full range of servicesin response to the consumer's interests and desires. full utilization of the strengths of both industries will yield awinwin strategy that could greatly accelerate the introduction and acceptance of diverse nii services inconsumer households.the powerful capabilities of pcs, combined with online services and the internet, have already provided theinitial interest and stimulus for the concepts of a comprehensive nii. the importance of the continued growthand acceptance of these capabilities cannot be overemphasized. one need only look at the sales of computers andsoftwarešor the use of the internetšto feel the everincreasing utilization in commerce and in the home. andthere is no end in sight for the dynamic expansion of the capability of these products and services.the television industryšwith 99 percent household penetrationšcan also make profound contributions tothe acceptance and growth of nii in the home. until recently, the television industry was solely based on analogtechnology, which has many limitations when viewed from today's vantage point. the current movement of thetelevision industry to digital systems will allow television to diversify and expand its capabilities in a manneranalogous to that of the products and services of the computer industry.almost all of the new television systems are based on mpeg 2, which uses flexible transport packets. newdigital television signals are thus no longer simply television signalsšthey are mpeg 2 packetized bit streamsthat can be used to deliver any desired mix of video, audio, and data. interactive services over television systemsare now also a reality. for example, a thomson consumer electronicssun microsystems partnership recentlyannounced an interactive operating system, "opentv," designed to work over digital systems with interactivityinterfaced through the tv remote control or the pc.the strong consumer interest in television will allow interactivity to be introduced in a nonthreateningmanner to the broad segments of society not currently disposed to using a pc. as a result, all members of societywill be able to learn to use interactivity with everincreasing levels of sophistication. digital television, whichwill initially be purchased for its entertainment value, can be a key vehicle that can be used to attract consumersand help finance the installation of the digital pathways to digital hardware in the home. digital hdtv deliveryover any media will provide a 19.4mbit/sec service to homes. in traditional oneway services such as terrestrialbroadcast and direct broadcast satellite, the telephone line is used for the return path. this arrangement isstandard in thomson's digital satellite system (dss) now used to deliver directtv and ussb signalsnationwide.1the dual evolution and penetration of computer and digital television services and hardware into the homerepresents a major winwin victory for all parties concerned. both industries will use many of the advances ofthe other. pcs are adding audio and video. televisionreceiving products will become digital with microprocessorsthe nii in the home165the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.and interactivity. both will stimulate the construction and acceptance of improved interactive networks to thehome.consumer interest will determine whether interactivity is first introduced into homes through the pc or thedigital television. over time, homes will contain a powerful pc capability in a den or work location and asophisticated digital television entertainment center in the family room. many of the communication networksand software programs will serve both home setups. full use of the attractive services and products of bothindustry groups will greatly accelerate the development and use of nii in the home.maximum implementation of digital television and interoperability across all media require certain federalgovernment and industry actions. what is key is the completion of the federal communications commission (fcc) advisory committee onadvanced television service (acats) process, with the fcc adoption of the grand alliance (ga) hdtvstandard as early as possiblešlate 1995 or early 1996šas recommended by the may 1994 nist workshopon advanced digital video in the nii and the recent report by the nii technology policy working group(tpwg). fcc action must include allocation of new 6 mhz transition channels to all broadcasters. thisacatsfcc action is moving along toward completion. establishment of new infrastructure network rules for the previously separate industries of local telephone,longdistance telephone, cable, broadcast, and so on. for all nii type services is vital. maximum networkdevelopment and investment must await a clear set of regulations. the more difficult governmentindustry challenge is the establishment of an open interoperableinfrastructure within the digital video world. closed proprietary systems and the potential for many differentvideo systems and interfaces will retard consumer acceptance. confusion over systems, standards, andinterfaces, as in the past, will cause consumers to delay acquisitions. one key ingredient that would help isto provide consumers with the option of buying all home hardware at retail from competitive suppliers.consumer decisions plus competition will help to establish open and interoperable systems and products.a concern expressed in some quarters is industry's commitment to commercialize systems and products.commercial commitments will not be a significant problem if the obstacles cited here can be dealt with. as anexample, the digital television actions of thomson consumer electronics are tabulated below. every reasonableeffort in the areas of standards, product development, and promotion is being supported, to accelerate theconversion of the home entertainment center into an exciting new interoperable digital center with uses farbeyond those common today.thomson's digital video activities include the following: key participant in development of mpeg 1 and mpeg 2 standards. leading participant and early proponent of the use mpeg flexible packets for the u.s. hdtv standard.charter member of earlier advanced digital hdtv consortium and of the recent grand alliance hdtvdevelopment team. developer, manufacturer, and marketer of the rca digital satellite system (dss), the first high unitvolume digital video system ever implemented. in cooperation with sun, announced an interactive digital operating system, "opentv," that is extremelyeconomical and that can be interfaced through a tv remote control or a pc. announced mpeg 2 encoder capability for sdtv (1995) and hdtv (1997). announced capability to produce settop receivers with full microprocessor capability. in cooperation with hitachi, demonstrated and announced commercial plans (1996) for a digital dvhsvcr for the home recording of the dss signals. in cooperation with toshiba and others, announced standard, manufacturing, and commercial plan (1996)for a digital videodisc (dvd) player. chaired worldwide working group that reached consensus on a digital recording standard (dvc) for the gahdtv system. announced plans for manufacture and sale of digital television receivers with interactivity (1997).the nii in the home166the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. announced plan for digital hdtv receivers with interactivity (1997). announced program for a dvdrom product (1997).note1. as an aside, thomson announced the shipment of the onemillionth dss home unit in less than ten months from a standing start. no otherconsumer electronics or other major productšcolor tv, vcr, cd, and so onšhas ever been accepted at a rate even approaching this level.consumers are prepared to accept the new digital television systems and hardware.the nii in the home167the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.20the evolution of the analog settop terminal to a digitalinteractive home communications terminalh. allen ecker and j. graham mobleyscientificatlanta inc.abstractthis paper addresses the evolution of the cable home terminal from past to present and presents a mostlikely scenario for its future evolution. initially, a simple converter allowed subscribers to tune more channelsthan the initial vhf channel 2 through channel 13. next, conditional access and addressability allowedprogrammers and network operators to offer subscriptionpaid premium programming. today, advanced analoghome communications terminals (hcts) allow downloading of software to provide electronic program guides,virtual channels, and menudriven navigators. evolution will continue from advanced analog hcts to a fullyinteractive digital hct.the enabling factors that allowed the cable industry to grow to become the primary entertainment deliverysystem to the home are the following:1. the availability of satellite delivery of content to headends;2. the availability of a broadband plant allowing many channels; and3. the availability of settop terminals that have grown in functionality to truly become hcts with reversepath capability.currently, cable systems provide broadband delivery to the home that allows broadcasting of manychannels. in the broadcast mode, all programs pass each hct so that each and all hcts can receive and displaythe programs. in most instances, premium programming is scrambled before transmission so that only authorizedcustomers can descramble premium channels. the authorization can be handled through individual addressing ofeach hct.to evolve to an interactive digital terminal that allows client/server type applications, four key ingredientsare required.the first is the requirement for a migration strategy to digital that uses broadband hybrid fiber coaxialsystems. this strategy allows the digital signals to coexist with analog broadcast channels from which mostrevenue is derived today.the second ingredient is the development of highdensity integrated circuits to reduce the cost of thecomplex digital and analog signal processing inherent in any interactive terminal. circuit density will increasefrom 0.8 micron to 0.5 micron to 0.35 micron geometries over time.third is the development of a multimedia operating system (os) and user interface (ui) that allows the userto navigate and interact with the wide variety of content applications made available. this os/ui must also beprogrammer friendly so that many different applications providers can develop applications that will run onhome terminals.finally, the system must be designed for full interoperability at critical interfaces and must operateefficiently within the regulatory environment. specifically, the interactive home terminal must contain a networkinterface module (nim), which would be provided by the network operator but could have a consumer digitalentertainment terminal (det) that could be moved from network to network and that has an open architectureoperating system and an open application program interface (api).the evolution of the analog settop terminal to a digital interactive home communicationsterminal168the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.historical backgroundcommunity antenna television, or catv as it is now called, has its roots extending as far back as the late1940s. it is widely accepted that the first catv system was constructed in mahanoy city, pennsylvania, by anappliance store owner who wanted to enhance the sale of television sets by making programming available tocustomers that could not get reception of offair signals since the community was located in a valley in theappalachian mountains. a large antenna was erected on top of a utility pole atop new boston mountain and wasused to feed wires and repeater amplifiers. the system was ''able to provide sufficient reception of televisionprograms so as to not only sell television sets but to obtain subscribers to his cable connection"1. from thisearly attempt to provide cable service, additional systems were constructed with the intent to provide clearreception of locally generated signals to customers unable to receive offair signals. by the mid1960s there wereover 1,300 cable systems feeding approximately 1.5 million customers.architecturally, these early cable systems were merely distribution systems that received and amplified offair signals without processing and reassigning redistribution frequency. however, the 1970s brought newtechnological developments. the launching of geostationary satellites, coupled with the allowance by the fcc tomake these satellites available for domestic communications, brought about the growth of satellite delivery ofprogramming directly to cable headends. for the first time, television programmers and programmingdistributors could use satellites to broadcast their programming with complete u.s. coverage. catv systemscould receive and distribute these signals via cable to customers with little incremental investment. televisionbroadcast stations such as wtbs in atlanta and wgn in chicago made use of this technology to createsuperstations with full continental u.s. coverage of their signals. in addition, fm satellite receivers, largesatellite receiving antennas, and video processing equipment provided cable headends with the necessaryelectronic equipment to receive and distribute these programs over selected channels in the cable system.how the analog cable set top evolvedfor the first time, there were more channels available over the catv system than the normal channel 2œ13vhf television could tune. this availability of greater channel capacity led to the development of the cable tuneror "settop converter." these early units, developed in the late 1970s, were capable of tuning up to 36 channels.they did little more than convert the tuned cable channel to a standard channel 3 or channel 4 output frequencyso that a standard vhf television set could receive the cable signals. in effect, these units were the cable analogof the uhf converter prevalent in the late 1950s. recognizing the vast market made available by the satellite andcable distribution system in place, content providers such as home box office, cinemax, showtime, the moviechannel, and the disney channel as well as espn, usa network, and others sought to use the existinginfrastructure to provide "cable only" delivery of premium and "niche" programming content to a growingaudience hungry for content variety.this programming was meant to service only those who purchased it and therefore required means forselectively enabling only those viewers who paid for these special services. to fulfill the requirement, frequencytraps were installed in cable drops to nonsubscribing customers. this technique had several drawbacks:1. traps were used to prevent reception, proving to be a costly proposition since the expense was distributedover nonsubscribing customers.2. customers who changed their viewing tastes required a service call by the cable operator to either removeor add traps.3. if the cable operator added a new premium service, new traps were needed for each subscriber notdesiring the new service.scrambling and addressability removed these road blocks and made premium programming a viablebusiness. in the 1980s, scramblers began replacing frequency trap technology. premium programming contentwas scrambled before being redistributed over the cable system. newer settop converters were provided thatthe evolution of the analog settop terminal to a digital interactive home communicationsterminal169the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.contained addressable descramblers. these units had unique preloaded serial numbers so that individualconverters could be addressed remotely from the cable headend and either enabled or disabled for descramblingof particular premium programming content or tiers of programs.more channels and better signal quality using analog fiberin an attempt to increase channel capacity, improve penetration, and provide better quality of service, cablesystems in the 1990s substituted analog fiber technology for the coaxial rf trunk and distribution systems.analog fiber provided a number of advantages:1. it reduced the number of amplifiers needed to span large distances, thereby increasing reliability andsignal quality.2. it allowed cable systems to penetrate deeper into outlying areas without a sacrifice in signal quality.3. it reduced the powering requirements.4. analog fiber was selected instead of digital fiber because it made conversion from fiber back to analogcoaxial distribution inexpensive and simple.the addition of analog fiber also changed the architecture of a typical cable distribution plant. in the processof providing better signal quality deeper into the distribution system, the topology slowly converter from a "treeand branch" layout, in which feeder lines split from a main trunk, to a "star" topology where clusters ofsubscribers were connected to a fiber node that was fed directly from the cable headend. this architecture,known as fiber to the serving area (fsa) or hybrid fiber coax (hfc), reduced the number of subscribers beingfed from a single node and increased the number of nodes fed directly from the cable headend. this evolutionhas enhanced a cable system's ability to convert from a broadcast topology, where bandwidth is assigned to aprogram, to an interactive topology, where bandwidth is assigned specifically to connect a content provider and asingle customer.hybrid fiber coax also provides more bandwidth for reverse path signaling. in the time warner fullservice network in orlando, reverse path amplifiers are used to connect hcts to the fiber node in the spectrumbetween 750 mhz and 1 ghz. reverse path time division multiple access (tdma) signals are converted fromrf to analog, fiber at the nodes and piped back to the headend over fiber. other systems operate in the same wayexcept that the band from 5 to 30 mhz is used between the fiber nodes and the home hct. the "star" topologyof hfc provides more reverse path bandwidth per home because of the lower number of homes connected to anode.cable becomes a systemcable in the 1990s has evolved into a complete communications system that provides broadband delivery tothe home of many channels, both basic unscrambled as well as premium scrambled channels. through the use ofcomputercontrolled system managers coupled with a conditional access system, individual settop terminals canbe quickly and efficiently electronically addressed to allow for the descrambling of particular premium channels.to aid in program selection, advanced analog settop terminals today feature downloaded fully integratedprogram guides and a system navigator that informs subscribers of the vast programming available. also, usingthe 150 kbps average bandwidth available in the vertical blanking interval of a standard video channel, theseunits incorporate "virtual channels" that can be used to send sports updates, financial services, news, and othertypes of digital services. these units also can be equipped with a standard readonlymemory (rom)basedoperating system or with an addressably renewable operating system (aros) that allows the operator todownload new software to the hct via the system manager to provide new user interfaces, unique electronicprogram guide onscreen formats, bitmapped graphics, and multilingual onscreen displays.the evolution of the analog settop terminal to a digital interactive home communicationsterminal170the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.also, reverse path signaling has made impulse payperview (ippv) and nearvideoondemand (nvod) apracticality. with an integrated ippv module, subscribers can view events at their leisure. also, nvodcapability offers subscribers advantages similar to those of vcr tape rental by defining events with staggeredstart times on multiple channels. advanced hcts have the capability to emulate a vcr's pause, fastforward,and rewind features by making use of the staggered simulcast nature of nvod channels and the builtinintelligent software resident in the hct to keep track of the appropriate channel needed to view a desiredprogram segment.digital communications with analog tv signals: an added dimensionwhile current cable systems use analog signals for video and audio, advancements in digital technologynow allow cable systems to add a digital video layer to increase channel capacity with little or no increaseddistribution bandwidth.two technological breakthroughs in digital processing are clearing the way for digital video and audioprogram content. the first is the adoption of standards for digitizing, compressing, and decompressing videoprogramming. in 1992, the moving picture experts group (mpeg) of the iso set out to develop a set ofstandards for digitizing and compressing an analog video signal. this international standards group laid thegroundwork for standardizing the algorithms, syntax, and transport format necessary to allow interoperabilityamong different suppliers of video compression and decompression systems. the attitude at the outset was that ifdigital television was to flourish, equipment built by different vendors must be compatible and interchangeable.the international standards adopted by the mpeg committee allow freedom to be creative in the encodingprocess within the parameters of the defined "encoding language" while maintaining compatibility with standardmpeg decoders. the analogy in the computer programming world is to say that software programmers canapproach a programming problem in many different ways; however, if their program is is written in c, forexample, any c compiler can compile and execute the program.the second development was a means for delivering the digital signals to the customer. schemes formodulating and demodulating an rf carrier using either quadrature amplitude modulation (qam) or vestigialsideband modulation (vsb) have been developed. these approaches are compatible with standard analog cablesystems and can deliver data information rates up to 36 mbps in a single 6mhz channel. the combination ofdigital compression of video and audio and digital transmission over cable can increase the number of videoservices in a single 6mhz channel by a factor of approximately 5 to 10 depending on programming content andpicture resolution.not only does digital processing increase capacity, but it also allows greater flexibility in the types ofservices that can be provided. no longer will there be the restriction that the information provided be a videosignal. digital computer games, digital music, and other multimedia applications will all be likely candidates forthe new broadband digital cable.the cable settop terminal needed to fully utilize these new services will evolve to a fully interactive homecommunications terminal (hct) that allows client/server functionality.migration strategy for broadband cable systems of the futureso how do we evolve from the cable system of today that broadcasts analog video services to the home tothe fully interactive digital system of the future? this evolution will most likely occur in four phases.the first phase in the migration would employ advanced analog home communications terminals (hcts)that are compatible with existing hybrid fiber coax distribution plants. these terminals, available today, not onlycan tune up to 750 mhz but also have both forward and reversepath digital communications capability fordownloading digital applications, providing onscreen display of program guides and handheld remotenavigators for ordering payperview and nvod.the evolution of the analog settop terminal to a digital interactive home communicationsterminal171the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the second phase most probably will use digital broadcast hcts that can not only tune and display thestandard analog video but also receive and decode digital video services. these digital services would beoverlaid on the hfc cable plant using unused spectrum above the standard analog video channels. this strategywould allow the digital services to coexist with analog broadcast channels from which most of today's revenue isderived. system upgrades and even "new builds" will need to provide today's analog channels to existing settopterminals and customers who are content in viewing basic and premium scrambled broadcast services. a hybridfiber coax system transmits the digital information on analog rf carriers designed to be frequency multiplexedinto a normal broadband channel. the bandwidth required is compatible with a normal analog video channel sothat the digital signals can be assigned a vacant space in the spectrum or, because of enhanced ruggedness, canbe placed at the upper end of the spectrum outside the guaranteed performance range of a distribution plant.the third phase in the migration would provide the analog/digital hcts with full interactive capability.this phase would require an incremental upgrade in the hct to provide more memory and enhanced reversepath signaling and full interactivity. however, the major upgrades would evolve over time as the plantarchitecture migrates from a "tree and branch" to a "star" configuration and more cable headends becomeinterconnected. this change would be coupled with the growth of contentprovider digital applications stored onfile servers, market demand for specialized applications, and the success of true ''video dialtone" as comparedwith nvod.the final phase, illustrated in figure 1, is a fully integrated broadband network that provides fullconnectivity to the home for pots services, broadband digital modems for personal computers (pcs), analogand digital video services, digital applications, and utility monitoring. either a fully integrated hct or a splitbetween the functionality of an hct and a customer interface unit (ciu) could be used. this last phase couldhappen as slowly or as rapidly as needed depending on market demand for interactive services.the phases outlined make sense because they allow for the coexistence of analog only, digital broadcast,and full interactive services and hcts on the same system and do not require flash cuts to implement, nor dothey sacrifice analog video revenues. thus, the revenues track with the cost of upgrades.cost drivers for the fully interactive home communications terminalthis migration will be paced by key cost drivers in the hct, the distribution plant, and the headend. hctcost appears to be the controlling factor because each digital user must have a digital hct. also, developing aneconomic model that allows enough incremental revenues to support needed upgrades will be key.the cost of the fully interactive digital hct depends on the availability of a costeffective set of applicationspecific integrated circuits (asics) for digital and analog signal processing and digital memory inherent in theinteractive digital terminal. currently most if not all the necessary asics have been designed to perform thecritical processing functions. these functions include digital demodulation of 64/256 qam digital signals,adaptive delay equalization of the digital signals, error correction, program demultiplexing and conditionalaccess, and finally mpeg video and audio decompression. also, microprocessors and graphics accelerators areavailable for a platform for the terminal operating system (os) and user interface (ui). several trial systems suchas the time warner full service network in orlando, florida, and the us west fully interactive trial in omaha,nebraska, are using hcts that include these asics. in 1995 other systems using these chips will be deployed bybellsouth, ameritech, southern new england telephone, and pactel.however, these chips are currently designed using today's 0.8micron technology, and the number needed tobuild a fully interactive hct is about seven to nine chips containing about 975,000 gates and 1,500 pinouts. toreduce cost, the goal is to design the required functionality into one or two chips. projections are that bymid1996 the number of asics could be reduced to four using 0.5micron technology, and by 1998 the goal oftwo chips could be realized using 0.35micron technology. reducing the number of chips would dramaticallyreduce the number of pinouts, thereby decreasing cost and increasing liability.the second key cost driver is the cost per subscriber for the headend and distribution plant. in an analogbroadcast or digital broadcast scenario, including nvod, the cost per subscriber for the headend and distributionplant can be normalized over the number of subscribers. therefore, increasing penetration reduces operating costthe evolution of the analog settop terminal to a digital interactive home communicationsterminal172the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 a fully integrated broadband network providing full connectivity to the home.the evolution of the analog settop terminal to a digital interactive home communicationsterminal173the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.per subscriber. however, in a fully interactive network, interactive server/client bandwidth must growproportionally with the number of subscribers, so that plant and headend cost increases with penetration to meetthe anticipated heavier demand for simultaneous interactive sessions between client and server. an acceptablevalue for subscriber costs for the fully interactive system is heavily dependent on the economic model forincremental revenues that might be expected from the additional services available to a customer. to put itanother way: just how much is a customer willing to pay and for what types of services? to answer thisquestion, a number of fully interactive trials are being conducted for consumer acceptance as well as technicalevaluation.current projections for 1995œ96 revenues that might be expected with various services using advancedanalog hcts, digital broadcast mode hcts, and digital interactive hcts are shown in table 1.table 1 projected revenues from services over various hcts, 1995œ96 (billions of dollars)advanced analog hctdigital broadcast hctdigital interactive hctbroadcast video282828pay per view/nvod81416video on demandšš4other services4828total405076source: paul kagan, 1994, projections for 1995/96 cable revenues.these projections indicate that a fully interactive hct could provide almost a 100 percent increase inmonthly revenues over an advanced analog unit. however, a large portion of the projected increase is in thevague category defined as "other services" that include home shopping and other services whose revenuepotential is not proven.currently, a reasonable compromise is to provide advanced analog hcts with nvod capability for thenear term, but add additional capacity using digital broadcast, and offer digital interactive services on a customerdemand basis with digital interactive hcts.requirement for interoperability and industry standardsthe promise of emerging digital interactive services cannot reach critical mass without the elements ofinteroperability, portability, and open standards at critical interface points in the system. because of thecomplexity of the systems involved, these elements are best resolved through industry groups representing majorproviders of equipment as opposed to government standards that could overlook critical issues.the following are one set of definitions for the above terms: interoperability is the ability to easily substitute or mix similar products from different vendors. examplesare television sets, vcrs, fax machines, and almost any electronic product sold at retail. equipment portability is defined as consumers' ability to move their owned equipment across a town, state,or country and still be able to use the equipment as before in a new location. examples are tvs, vcrs,radios, cellular telephones, directtv, and almost any public telecommunications equipment. currently,catv settop converters do not fit this category. open standards are standards that provide complete descriptions of interfaces at critical points in the systemwith reasonable, nondiscriminatory license fees if any intellectual property is involved. examples are ntsc,pal, dos, mpeg, telecommunications interface protocols, and many others.interoperability, consumer equipment portability, and open standards are critical to the success of a fullyinteractive system for at least three reasons.the evolution of the analog settop terminal to a digital interactive home communicationsterminal174the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.first, the risk is lowered for all participants. equipment manufacturers are able to invest in the developmentof products that are guaranteed to be compatible with all networks and the suppliers of content on those networks.second, it assures the lowest cost hardware and content. competition between equipment manufacturers toproduce in large volume and therefore achieve economies of scale is enhanced because these products can servethe entire market base as opposed to many smaller proprietary networks. also, content providers will be inclinedto participate in content and applications development because these applications will be compatible with allconsumer terminals.third, it will jump start the creation of many new interactive applications by removing the chickenandeggphenomenon. once standards are agreed upon, content providers can begin working on applications softwareknowing that network architectures and consumer terminals will be available in the near future. likewise, onceattractive applications are available, consumers can acquire home terminals and subscribe to the network.a major step toward the goals of achieving interoperability, portability, and industry standards has beenaccomplished by the mpeg committee in the adoption of standards for compressing, transporting, anddecompressing video signals. as a result, chip manufacturers have developed a set of decompression chips thatare functionally interchangeable, and equipment manufacturers now have a video standard to work from.because of mpeg's success, a digital audiovisual council called davic was formed in april 1994 to helpresolve remaining compatibility issues. this committee currently has over 120 member companies worldwide.the charter for davic is to assist in the success of the emerging digital video/audio applications and services byproviding timely agreedupon specifications for interfaces and protocols. davic is working on a timetable thatcalled for a strawman baseline document in december 1994, experiments and tests to occur during the spring andsummer of 1995, and publication of final recommendations by december 1995.while the goals of davic are ambitious to say the least, much of the intent could be realized byrecognizing that two critical interface points exist in the fully interactive network.the first interface where standards are important is the connection between broadband integrated gatewaysat a cable headend and the data networks used to achieve full connectivity between cable systems and betweencable systems and telephone companies. an interface standard must be adopted that allows sonet, fddi,ethernet, atm, and other protocols to provide inputs to the access part of the network easily. this approachwould provide the connectivity necessary to access remote content providers either over satellite, or from remotevideo file servers on the metropolitan network or connection through a trunk from other areas. interactiveapplications providers with the capability to communicate over the internet or get applications resident outsidethe local cable network can also be sources of content.the second critical interface connects the broadband network to the home. if interoperability and portabilityare to be realized along with network integrity and flexibility, the home terminal must split into a part that mustbe provided by the network operator and a part that is not necessarily a part of the network and could evolve tobecoming consumer electronics sold at retail.a first part of the terminal that could be called the network interface module (nim) must be provided by thenetwork operator for several reasons: the technical characteristics of networks will evolve over time. this evolution will include networkuniquemodulation of digital streams that will start out using 64 qam but migrate to 256 qam or even vsbmodulation, upgrades in networkspecific signaling techniques for both forward and reverse path, and areallocation of bandwidth used to transmit the digital channels. the hct will need to interface with other types of delivery media including 28 ghz wireless, 2 ghzwireless, or even a satellite network. networks will have different security and conditional access systems. software needed to run the hct and communicate with network control will be network specific.the nim would be provided by the network provider as a standard sized plugin module with standardizedi/o on the customer side and specialized interface to the network. it would contain the tuner, demodulators,conditional access system, and reverse path signaling common to a specific network and wouldthe evolution of the analog settop terminal to a digital interactive home communicationsterminal175the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.allow the many different network architectures, signaling structures, and connection management systems, manyof which are already in place today, to communicate with a part of the hct that one day might be purchased atretail. to add further flexibility and network integrity, the addressability and conditional access system (ac&e)could be installed on a "smart card" that plugs into the nim.this hct concept can be explained by figure 2, which shows the major functional blocks contained in aninteractive digital hct. these include the following: a network interface module (nim) as described above to provide connectivity, conditional access, andcontrol between the network and the hct; a platform for executing multimedia applications, including an operating system (os) and a publishedapplication program interface (api) and a user interface (ui) to process downloaded applications; a digital entertainment terminal (det) to receive and process both mpeg compressed digital video andaudio, as well as analog video for presentation to a video monitor or tv set (it would also contain all thenecessary home entertainment interfaces for tvs, stereos, pcs, and other consumer electronics); and software that is networkspecific to interface with the det and the nim for constructing reverse pathmessages and interpreting addressable instructions sent downstream to the terminal.figure 2 proposed twopart broadband digital home communications terminal architecture to resolveinteroperability and retail sales issues.concluding remarksthe following statements are supported by the previous sections: today's advanced analog hcts include digital communications that provide controlled access,downloadable digital applications (e.g., electronic program guides and virtual channels), and a significantrevenueproducing service with analog video and nvod. digital interactive hcts have processing power equivalent to pcs and the additional ability to communicateover broadband networks and provide multimedia applications. digital hcts are already available for trials in digital networks. their widescale deployment will be madepossible by using highly integrated silicon to reduce cost of the hct. for an economic transition from analog to digital that does not preclude revenueproducing analog video,digital hcts will also incorporate analog as well as digital capability.the evolution of the analog settop terminal to a digital interactive home communicationsterminal176the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. standards will allow the market to develop more rapidly and will help drive the cost of the fully interactivehct down faster. nims and dets allow networks to be deployed that provide interoperability and portability for the networkprovider and the customer without inhibiting technology and service growth. hfc provides the plant configuration that allows a smooth transition to the future and can be the platformfor a fully interactive network that includes video, audio, telephony, and data without sacrificing analogservices and associated revenues.reference[1] goodale, james c., 1989, "all about cable," law journal seminarspress, chapter 1, p. 6.the evolution of the analog settop terminal to a digital interactive home communicationsterminal177the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.21spread aloha wireless multiple access: the lowcostway for ubiquitous, tetherless access to the informationinfrastructuredennis w. elliott and norman abramsonaloha networks inc.statement of the problemthe ability to access the national information infrastructure (nii) on a tetherless, broadband basis is oftendiscussed. but today, the capability to interchange information with and through the network on a tetherless basisis limited and expensive. the promises of new tetherless access approaches for data, such as the current personalcommunication system (pcs) implementation approaches, have been only thatšpromises. but a new wirelessmultipleaccess approach will change this situation in a revolutionary way. this new wireless multipleaccessapproach is called spread aloha and is being developed under u.s. small business innovative researchgrants from the national science foundation and from the advanced research projects agency of thedepartment of defense.envision a robust nationwide packet radio data network in place with millions of users having tetherless,broadband user communications devices allowing easy, lowcost automatic interface into the public and privatenetworks of the national information infrastructurešanywhere, anytime. a user with a portable pc, a personaldigital assistant (pda), or another device containing a spread aloha pcmcia card or embedded chip wouldhave instant access to a network of choice.this paper describes the market need for a wireless multipleaccess approach that offers robust, wirelessmultiple access to the nii at an affordable price. a strategic plan for implementation of spread alohaarchitecture having an increase of two orders of magnitude in capability over existing wireless data multipleaccess approaches is discussed.backgroundthe market: what people want and needas the nii evolves, users will increasingly want and demand ready access to acquire information, sendinformation, or communicate with each other. some of this information may be manually or automaticallygenerated or requested by a user or by an application or stored in a database for manual or automatic request.note: spread aloha is a trademark of aloha networks inc.spread aloha wireless multiple access: the lowcost way for ubiquitous, tetherless access tothe information infrastructure178the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.access to the nii will either be by wire (telephone, cable tv, etc.) or wireless. the wireless case formillions of users is the focus of this paper.wired data access to the nii and predecessors with capabilities adequate for certain applications and withdistinct evolutionary paths or alternatives exists today. but wired data access has a fundamental limitation. it isnot always where the users are, and it does not travel easily with the users. one has to "plug into another jack"whenever one moves. the wire is a tether. thus, wireless data access can prove a boon to those users who wantand need information access wherever they are. the ability to operate on a tetherless basis generates new powerto use information for almost everyone. (those who have experienced a good, tetherless computing or dataaccess situation can testify to this.)but wireless data access capability to interchange information with and through the network on a tetherlessbasis is limited and expensive today. to date, new wideband tetherless access approaches for data appear to be"vaporware"šmany have tried and none have succeeded. there are fundamental technology limitations inwireless multiple access that have led to economic limitations of these dreams.the market: quantizationcharacterizing a market in which one introduces a product of at least two orders of magnitude morecapability than currently exists is difficult at best. the following discussion forms a probable baseline that wouldbe the lower bound for the market addressed by spread aloha architecture.the addressed market for nomadic, tetherless computer networking employing spread aloha architectureconsists of a large fraction of the users of portable computing devices, such as portable pcs and pdas. thereality is, however, that with the introduction of a breakthrough technology such as spread aloha, a newemphasis on new devices and applications comes into play, which tends to stimulate and transform the market.aloha networks expects that the pc/pda market will be only the base for the nomadic, tetherless computingnetwork market. new applications using a pcmcia card (beyond pc and pda applications) and embeddedspread aloha wireless technology will develop as well.even recent forecasts for growth of the mobile data market can now be revised upward:the mobile data market in the united states will increase from 500,000 users in 1994 to 9.9 million users in 2000šwith a compound annual growth of 64 percent, according toa new report from bis strategic decisions.– the"mobile professional" sectoršprofessional employees who spend 20 percent or more of their time away from theirdesksšrepresent a potential user population of 31.9 million by 2000.–1the rebirth of the pda market will hinge on repositioning the devices as communicationsoriented pccompanions.– bis predicts portable pc users will increasingly communicate using mobile data services. weforecast that 2.8 percent of portable pcs that are wirelesslyenabled (300,000 units in 1994) will grow to 16 percentof the installed base, or 2.6 million units, in 1998. the increased usage of mobile data will be a direct result ofimprovements in availability, functionality, and pricing for services. we also expect that the mobile data marketwill grow at a compound rate of 80 percent through 1998. although at most 12 percent of pdas are currentlywirelessly enabled, that percentage will grow to 75 percent by 1998 [1.7 million units from authors' graph]. thecurrent low percentage rate reflects the death of lowcost mobile data alternatives. once users have more costeffective options to choose from, the number of wirelesslyenabled pdas will climb.2from the above forecast made without knowledge of the breakthrough spread aloha technology, it canbe assumed that the market for user units (as opposed to infrastructure) will likely be about 4.3 million units in1998 and will approach 10 million units by 2000.spread aloha wireless multiple access: the lowcost way for ubiquitous, tetherless access tothe information infrastructure179the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.existing and nearterm marketplace alternativesthere are several alternative wireless data service approaches available or nearly available today. broadlyspeaking, the services can be broken into three general categories: oneway, packetbased services (primarilypaging); twoway, packetbased services; and circuitbased services. packetbased services chop the informationthat is to be sent into data packets, attach addresses identifying the transmitter and the recipient if needed, andsend the packet over a channel that is shared by multiple users. circuitbased services allocate a specifictransmission channel to the end user when a transmission is requested, and that user holds that circuit until thetransmission is completed.the most common form of circuittransmitted data utilizes cellular modems to provide mobile datacommunication over the existing analog cellular infrastructure. by incorporating sophisticated error correctionprotocols, these modems attempt to compensate for the relatively low line quality and allow portable computeror fax machines to communicate on a wireless basis. this approach benefits from the existing broad coverage ofanalog cellular and, except for the continuing problem of broken connections, is effective for large file transfers.but the quality of service has been generally low, and the price to the user has been high because of long setuptimes, low data rates, and high cellular airtime pricing.wide area applications such as the ardis and ram mobile data services are struggling. (according to anew york times article,3 ardis reportedly has 38,000 users in 10,700 towns and cities and ram mobile datahas 17,000 users in 7,000 towns and cities). these services, which are employed where customer data must becaptured regionally or nationally, do not live up to their initial promise because they are narrowband andtherefore fundamentally expensive and limited in data rate.cellular digital packet data (cdpd) services by at&t mccaw, gte, and others, which use the analogcellular infrastructure in the united states, are moving toward implementation in individual cellular carriers'regions. cdpd transmits packet data in the limited voice analog cellular infrastructure. though it employs apacket structure good for many data applications, there are two limitations: (1) access to the infrastructurerequires much of the cumbersome multiple access required for voice uses and (2) data rates are limited to whatfits into the analog voice channels of that infrastructure, which are less than ideal.metricom, geotek, and nextel frequency hop/time division and time division, multipleaccessšbasedservices and new twoway paging services, such as mtel's destineer, are also being deployed. metricom offersa narrowband approach of uncertain throughput today, and its peertopeer architecture limits the data volumesits network can handlešnot a desirable trait for ubiquity. none of these systems can provide tetherless dataaccess that is wideband, robust, user unlimited, and inexpensive.the technologythough broadcast transmission (the transmission of significant amounts of data from one to many) is a wellunderstood problem, the transmission of significant amounts of data from many to one (i.e., the network node)can be quite difficult. the new wideband spread aloha wireless multipleaccess technology, contained inforthcoming chips and boards for manufacturers of nomadic, tetherless computing products, allows robust,"bursty," tetherless access to the nii at an extremely low cost. the implementation of spread aloha is simplerand far less expensive than any of the various pcs or wide area data network approaches defined to date.spread aloha is an advanced wireless multiple access technology that can provide the capabilitiesrequired for digital networks with large numbers of remote terminals. spread aloha combines the provensimplicity and operational flexibility of conventional narrowband aloha wireless multiple access with the highbandwidth and high throughput of spread spectrum. (conventional aloha multiple access was developed atthe university of hawaii and is employed in ram mobile data's network, in the inmarsat maritime satellitesystem, and in many vsat networks, and it is the underlying basis for ethernet.)spread aloha compares favorably to time division multipleaccess (tdma) and frequency divisionmultipleaccess (fdma) approaches because its capacity is data limited, not channel limited as are tdma andspread aloha wireless multiple access: the lowcost way for ubiquitous, tetherless access tothe information infrastructure180the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 1 comparison of spread aloha to two other access technologiesconventional alohaspread spectrum cdmaspread alohabandwidthlowhighhighnumber of usersunlimitedlowunlimitedcomplexitylowvery highlowflexibilityhighlowhighfdma. that is, capacity in the spread aloha architecture is limited only by the amount of datatransmitted in the network rather than by the number of users who access the network. table 1 compares spreadaloha to the other data limited approaches, conventional aloha and spread spectrum code divisionmultiple access(cdma). it illustrates why spread aloha is the most efficient multipleaccess technique forlarge numbers of users.the number of users in either conventional aloha or spread aloha is limited only by the total data rateof the channel. conventional aloha is low bandwidth and thus has a low data rate because of the practicalrequirements of maintaining a constant pulse energy as the data rate increases. in cdma the practical limit onthe number of users is cell constrained by the requirement to implement a separate receiver at the hub station foreach active user. in the is95 cdma standard for cellular voice, the maximum number of users per cell is lessthan 40.spread aloha can be viewed as a version of cdma that uses a single, common code for all remotetransmitters in the multipleaccess channel. in a spread aloha channel, different users are separated by arandom timing mechanism as in a conventional aloha channel rather than by different codes. since only asingle code is used in a spread aloha channel, only a single receiver is required in the spread aloha hubstation, rather than a separate receiver for each remote terminal as is required in cdma. in addition, because ofthe elimination of multiple codes, many of the most complicated features required in a cdma receiver can beremoved. the elimination of unnecessary system complexity makes possible a degree of system flexibility,which can be important in today's rapidly evolving wireless markets.for example, a spread aloha hub station need only be capable of synchronizing to received signals, all ofwhich use the same code, a much simpler problem than that faced by a cdma hub, where the codes received areall different. in a spread aloha hub station, packet interference can be eliminated by a cancellation processmade practical by the fact that the interference terms generated by all packets are identical. and in a spreadaloha channel it is possible to select a spreading code that has only half as much interference as codes used ina cdma channel.spread aloha can provide a throughput 100 times greater than any conventional aloha network now inoperation and is much easier to implement than current wideband multipleaccess techniques, such as cdma.no other technology can provide robust data networking with large numbers of users. spread aloha combinesthe proven simplicity and operational flexibility of a conventional aloha multipleaccess channel with thehigh bandwidth and high throughput of a spread spectrum channel.analysis and forecastgeneralspread aloha will make it possible to build a nationwide broadband packetradio data network allowingeasy, lowcost, automatic interface into the public and private networks of the national informationinfrastructure. a user with a portable pc, a pda, or another device containing a pcmcia card or embeddedchip would have instant access to the network of choice. smaller campus, metropolitan, or regional networkscould be addressed initially as a way of beginning what could ultimately be a national network.spread aloha technology also holds great promise for data/voice pcs applications. because of its lowercost, spread aloha offers a potential for ubiquity that does not exist with other approaches for supporting dataapplications within pcs. however, the pcs market cannot be easily approached without anspread aloha wireless multiple access: the lowcost way for ubiquitous, tetherless access tothe information infrastructure181the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.established standard for spread aloha pcs applications for voice and data. the establishment of a standard isexpected to take some time, but aloha networks, in cooperation with others, plans to begin these efforts in1995, looking toward the second phase of pcs standards and equipment in 1998œ99. at the bottom line,however, if a nationwide data network can be implemented before the acceptance of a standard in this area,spread aloha will be positioned as a de facto standard. aloha networks believes that its spread alohatechnology will ultimately form the basis for the most viable pcs data standard.economic advantagesthe economics of using the spread aloha technology are compelling. spread aloha technology allowsfor a simple implementation and has no user population limits. this simple implementation can lead to lowcostuser units. in addition, the cost of implementing the hub or microcell facilities is lower for a large number ofusers than any alternative.ardis, ram mobile data, and gte's cdpd services appear to price their services at about $0.50œ$1.00per kilobyte.4 this is essentially a user cost of $0.50 per second of use! at&t mccaw cellular has announcedcdpd service prices ranging from $0.08 to $0.16 per kilobyte.5 the typical average monthly bill for theseservices has been estimated to range from $25 to $200.aloha networks estimates that spread aloha multiple access can substantially increase networkcapacity as well as individual ''burst" transmission rates without significant added cost over other alternatives.the existing and planned data networks tend to be constrained to an operating rate of about 20 to 50 kilobits persecond. with large network volumes, price per kilobyte could be reduced by one to two orders of magnitude. asa corollary, spread aloha could allow a pricing of $0.50 to $1.00 per 100 kilobytes if one assumes demand isstimulated by such a substantial price decrease.aloha networks anticipates that by 2000 users of a high proportion of notebook pcs, pdas, and otherembedded microprocessors in portable platforms will expect to be able to communicate with remote points,public networks such as the internet, private networks, or with the user's office. in fact, with a lowcost tetherlessapproach, the aloha networks' spread aloha technology could significantly stimulate the market fornotebook pcs, pdas, and other devices not yet conceived! with the anticipated growth of nomadic computing,this would translate into 10 million to 100 million user units (either pcmcia cards or embedded) in 2000.system issuesthe nomadic, tetherless computing network is envisioned as a nationwide system, accessible from almostanywhere in the united states. this network could either be integrated with other networks or interfaced to othernetworks at various nodes. the hub or microcell stations would be spaced according to propagationcharacteristics in every area, similar to pcs or perhaps ardis or ram mobile data. (in fact, the existinginfrastructure of these networks could be employed for this.)frequency spectrumspread aloha can operate in almost any frequency band. however, the system envisioned here wouldoperate in a given, yet to be determined frequency band. the possibilities are (1) allocation of a new frequencyband for this service, (2) use of the existing ism bands, (3) use of the existing esmr bands, or (4) use of theexisting and future pcs bands. since the radio frequency transmission is spread spectrum, the selected bandmust, of course, be suitable for such uses. the frequency band approach, which will allow the fastestimplementation but yet allow the expected ubiquitous growth, should be explored.spread aloha wireless multiple access: the lowcost way for ubiquitous, tetherless access tothe information infrastructure182the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.network infrastructurethe network infrastructure for a nationwide nomadic, tetherless computing network would entail microcellsites with the spread aloha hubs. these hub sites would be trunked together using existingtelecommunications company and interexchange carrier facilities. at selected hub sites, the network would beinterfaced with the internet and other internetlike networks.to establish ubiquitous coverage, from 2,500 to 12,000 hub sites will have to be established, dependingupon the selected frequency band and coverage patterns. a spread aloha installed infrastructure of this size isestimated to cost between $100 million and $500 million. the exact cost depends on what the coverage patternis, whether the data network overlays another network, and whether a voice network is implementedsimultaneously. such a network can be implemented on a phased basis, covering the highest user populationareas first.remote user terminal communications devicesaloha networks envisions remote user terminal communications devices that are small and inexpensive.assuming large quantities of devices, aloha networks estimates the cost of the spread aloha chip set orchip for remote user communications cards in a microcellular system to be substantially below $100 in the 1998time frame, with the normal "moore's law" cost reductions beyond 1998. user software in the terminal devicewould employ "standard" user software such as the general magic or other user operating software products.ultimately, aloha networks envisions these devices as being embedded in many different computingappliances, with the communications and microprocessor elements not particularly discernible to the user.strategic relationshipsto implement such a concept, the existing and potential wireless infrastructure owner/operators must beinvolved in the evolution of the system together with wireless networking equipment manufacturers. theseparties must have a reasonably common objective.recommendationsforum for development of wireless infrastructureestablish a forum for those private and publicsector entities involved in infrastructure for wireless data.encourage analysis of the spread aloha architecture and the establishment of strategic relationships amongthe parties, assuming that the effectiveness of that architecture is demonstrated. in conjunction with existingprivatesector infrastructure providers, develop an implementation approach to overlay a spread alohaarchitecture on existing wireless networks and determine the most appropriate frequency allocation. coordinateand make appropriate filings with the federal communications commission for the selected frequency use. thisinfrastructure implementation will be the critical factor in realizing such a nomadic, tetherless computing network.pcs data standardsencourage standardization proceedings to be initiated for a spread aloha wireless air interface for pcsdata and voice/data applications. the lowestcost user pcmcia card approach would require a hubinfrastructure similar in coverage and spacing to voice pcs. though it is recognized that the initial pcsimplementations will be oriented toward telephony, the second implementation should be more attentive to data,thus offering a good opportunity for a broadband approachšspread aloha.spread aloha wireless multiple access: the lowcost way for ubiquitous, tetherless access tothe information infrastructure183the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.integrate tetherless approach with nii planningthe concept of tetherless data access to the nii should be integrated into other nii studies and planning.most such studies and planning envision a person sitting at a desk. the tetherless concept is an important aspectof making full use of the nii in life beyond relatively static libraries, schools, and offices.additional resourcesabramson, norman (editor). 1993. multiple access communications: foundations for emerging technologies, ieee press, new york.abramson, norman. 1994. "multiple access in wireless digital networks," invited paper, proceedings of the ieee, september.notes1. mobile data report. 1995. "bis estimates u.s. market to reach 9.9 million users in 2000," vol. 7, no. 8, april 24.2. nelson, paul, and dan merriman, bis strategic decisions. 1994. "wirelessly enabling portable computers: a major growthopportunity," the red herring, september/october, pp. 64œ65.3. flynn, laurie. 1994. "the executive computer: 3 ways to be unplugged right now," new york times, december 4.4. leibowitz, dennis, eric buck, timothy weller, and john whittier. 1995. the wireless communications industry. donaldson, lufkin &jenrette, new york, winter 1994œ95, p. 34.5. mobile data report. 1995. "mccaw prices cdpd as low as 8 cents/k to cover 75 percent of its markets in 1995," vol. 7, no. 8, april24.spread aloha wireless multiple access: the lowcost way for ubiquitous, tetherless access tothe information infrastructure184the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.22plans for ubiquitous broadband access to the nationalinformation infrastructure in the ameritech regionjoel s. engelameritechstatement of the problemthe national information infrastructure (nii) can be visualized as consisting of four components: (1) accesstransport from the premises of the end user to an access node; (2) access nodes on an intracity, intercity, andinternational network of networks; (3) the internodal network of networks, including the internetwork gateways;and (4) information repositories. the access nodes, network, and information repositoriesšthe last threecomponentsšare common facilities; their costs are predominantly proportional to usage and can be sharedamong many users. in contrast, the access transportšthe first componentšis a dedicated facility connecting thepremises of a single user; its cost is predominantly fixed, independent of usage, and borne by that single user.as a result, the access transportšpopularly referred to as the "last mile," although, in fact, it is typically afew miles in lengthšpresents the greatest challenge to the provision of a ubiquitous, truly "national," widebandnii. because the access nodes and information repositories can be provided modularly and grow with usage,they are already being put in place, by small entrepreneurs as well as by large service providers. since theintracity, intercity, and international traffic can be multiplexed into the traffic stream on the existingtelecommunications network, that portion of the ''information superhighway" already exists. the majorinvestment hurdle, then, is the deployment of ubiquitous broadband access to individual user premises.at first glance, it might appear that cable television systems constitute such broadband access, but closeranalysis reveals that this is not so. cable systems are predominantly oneway downstream, some with minorupstream capability, usually relying on polling to avoid interference among upstream signals. more constraining,they are broadcast systems, with 50 or so video channels being simultaneously viewed by several thousand users.the same cost hurdle described above exists to upgrade these systems to provide dedicated broadband access toindividual users.currently available technologies for access transport span a wide range of capabilities and costs. since thecosts cannot be shared among users, at the low end of the range they are shared among usages, particularly withvoice telephony. modems provide dialup capability over standard telephone lines at speeds typically up to 9,600bits per second, with less common capability at 14,400 and 28,800 bits per second. integrated services digitalnetwork (isdn) is rapidly becoming widely available, providing dialup capability at 56, 64, and 128 kilobitsper second. various local carriers, including the local exchange carriers, are offering frame relay service at 56kilobits per second and 1.5 megabits per second, and switched multimegabit digital service at 1.5 and 45megabits per second, with plans for migration to asynchronous transfer mode (atm) at oc1 (45 megabits persecond) and higher sonet rates. however, all of these require a dedicated leased digital line between the user'spremises and the carrier's switch of a capacity equal to the peak bit rate used. as a result, these higherspeedservices are cost effective only for large user locations that generate sufficient traffic to fill them. the challenge,again, is to provide costeffective, highspeed access to individual homes and small business locations.plans for ubiquitous broadband access to the national information infrastructure in theameritech region185the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.deployment plans for the ameritech regionameritech has committed to providing broadband access in its region, using a hybrid optical fiber andcoaxial cable architecture that can support a wide range of applications, including video services similar tocurrent cable television, expanded digital multicast services, interactive multimedia services, and highspeed dataservices. by providing a platform for a wide range of applications, a costeffective solution is achieved(economic data are provided in a later section).construction is expected to begin in 1995 in the chicago, cleveland, columbus, detroit, indianapolis, andmilwaukee metropolitan areas, ramping up to an installation rate of 1 million lines per year by the end of 1995and continuing at that rate to deploy 6 million lines throughout the region by the end of the decade. in december1994, the federal communications commission granted approval for ameritech to construct the first 1.256million lines, distributed among the metropolitan areas as follows: chicagoš501,000, clevelandš137,000, columbusš125,000, detroitš232,000, indianapolisš115,000, and milwaukeeš146,000.technical architecturethe system architecture employs a hybrid transport network of optical fiber and coaxial cable. signals aredelivered over ameritech's atm network to video serving offices, each serving 100,000 to 150,000 customerlocations. the signals are then distributed on optical fiber to individual nodes, each serving a total of 500customer locations, not all of whom may actually subscribe to the service. from each node, the signals aredistributed on four parallel coaxial cable systems, each serving 125 customer locations. with this architecture,the coaxial cable network is less than 2,000 feet in length and contains, at most, three amplifiers to any customerlocation.the signal on both the optical fiber and the coaxial cable is a broadband analog video signal. the initialdeployment will have a bandwidth of 750 megahertz, with capability for upgrade to 1 gigahertz when thereliability of such electronics becomes proven, yielding 110 channels of standard 6 megahertz video bandwidth.the allocation of these 110 channels to various applications is flexible and will be adjusted to satisfy user needs.based on current estimates, approximately 70 of the channels will carry analog video signals for applicationssimilar to current cable television, including basic and premium channels and payperview. the remaining,approximately 40, of the channels will be digitized using 256 quadrature amplitude modulation, yielding a usablebit rate of over 36 megabits per second on each channel. approximately 30 of these digitized channels will beused for multicast services, with multiple users viewing each transmitted program. approximately 10 of thedigitized channels will be used for switched interactive services, for which each user requires a dedicated digitalcircuit for the duration of the session.on the digitized channels, the video signals will be compressed using the mpeg2 compression standard.depending on the particular application, each such signal will require a fraction of the 36megabitpersecond orgreater capacity. the signals will be multiplexed at the video serving offices and demultiplexed by the customerpremises equipment, using the mpeg2 transport layer protocol.in addition to the downstream capacity, the system will have an upstream capability provided by up to 20channels, each of 1.5megabitpersecond capacity. depending on local conditions of noise and interference, it isexpected that at least 15 of these will be usable on each coaxial cable system serving 125 customer locations.the system is intended to be a platform for a wide range of applications. accordingly, the customerpremises equipment may be a settop box for use with a television set or an adjunct to a personal computer.plans for ubiquitous broadband access to the national information infrastructure in theameritech region186the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.system capacityas described above, each branch of the distribution network will consist of an optical fiber to a nodeserving 500 customer locations, with four coaxial cable buses from the node, each serving 125 customerlocations. each branch will have a downstream capacity of approximately 360 megabits per second for switchedinteractive services.the system is designed for economical growth in capacity as the market expands and traffic increases.initially, each fiber branch will share a linear laser in the video serving office with two other fibers, through apassive optical splitter, so that the 360megabitspersecond downstream capacity will serve 1,500 customerlocations, not all of whom may subscribe to the service. when the traffic grows to exceed this capacity, the fiberscan be driven by independent lasers so that the 360 megabits per second downstream capacity will serve 500customer locations. when the traffic requires it, up to four fibers can feed each node, one for each coaxial cablebus, so that the 360 megabit per second downstream capacity can serve 125 customer locations.the downstream capacity will be assigned to the users on a persession basis, depending on the particularapplication. there is some uncertainty about the bitrate required for each application. human factorsexperiments and customer trials appear to indicate that live programs other than athletic events, compressed inreal time and displayed on fullsize television screens, can be delivered at 3 megabits per second. material that isprerecorded and stored in compressed digital form can be delivered at a lower bitrate, since the compression isnot performed in real time. during the compression process, the results can be observed, and the parameters ofthe compression algorithm can be optimized to the particular material. similarly, video material thataccompanies text and occupies only a portion of a computer screen requires less resolution and can be deliveredat a lower bitrate. not all interactive applications will involve video, and those that do will generally involvematerial that is stored in compressed digital form and displayed in a window occupying a portion of the screen,so that an average of 3 megabits per second per session is probably a conservative estimate. at that average bitrate, the 360megabitspersecond downstream capacity could serve 120 simultaneous sessions.further, although the initial system will assign a fixed bitrate for the entire duration of the session, equal tothe bitrate required for the most demanding segment, the capability will exist for statistically multiplexing thesignals and using a bitrate based on instantaneous requirement. this capability will be employed if necessary. inthat event, an average bitrate per session of 3 megabits per second would be quite conservative.it is estimated that, during the peak usage period of the day, 15 percent of the subscribers will be using theservice at the same time. that would consume the entire 120session capacity when 800 of the 1,500 customerlocations became subscribers, equal to 53 percent market penetration. at that point, the next step in capacitygrowth, sharing the capacity among 500 customer locations, would be implemented. if the peak usage persubscriber turned out to be higher than the 15 percent estimated, then the capacity growth would be implementedat a lower number of users. this would not affect the economic viability of the system, since it is the totalamount of usage that will generate both the need and the revenue to support the increased capacity.at the limit of the current architecture, the 120session capacity will be shared by the 125 customerlocations on each coaxial cable bus. that would support 96 percent simultaneous usage at 100 percent marketpenetration. if that turned out to be insufficient, because of multiple simultaneous users per customer location, orhigher bitrate applications, statistical multiplexing could be employed.in addition to the downstream capability, each coaxial cable will support at least 15 usable upstreamchannels at 1.5 megabits per second each, and these will be multiplexed onto the fiber from the node to the videoserving office. therefore, unlike the downstream capacity, this upstream capacity will be shared by 125 customerlocations from the start. these upstream channels must utilize a time division multiple access protocol, whichdoes not allow for 100 percent "fill"; nevertheless, traffic studies indicate that a single 1.5megabitpersecondupstream channel could easily support all of the digital video requirements, including video on demand, of the125 customer locations, leaving the remaining 14 or more for the more interactive multimedia and dataapplications.there are two parameters of the interactive services that determine the required upstream capacity: (1) thefrequency (and size) of the upstream messages generated by the user and (2) the required latency, or speed ofresponse to the message. analyses of the types of applications that are anticipated indicate that the secondplans for ubiquitous broadband access to the national information infrastructure in theameritech region187the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.parameter is controlling. if enough upstream capacity is provided to assure that the transport delay contributes nomore than 100 milliseconds, it does not matter how frequently the user generates upstream commands.of course, if the users were to generate large files for transmission in the upstream direction, rather than theinquiry and command messages typical of interactive services, this analysis would not apply. but, by definition,such users would be generating sufficient traffic to use one of ameritech's other, more symmetric, data offeringscost effectively, and would not be using the hybrid fiber/coaxial cable system.at a 100millisecond latency, each upstream channel could support approximately 20 simultaneoussessions, for a total of at least 280 simultaneous sessions on the 14 or more upstream channels. this is greaterthan the 125 simultaneous sessions supportable by the downstream capacity that will only be generated by 96percent simultaneous usage at 100 percent market penetration.economic analysisat this time, the fcc has approved construction of the first 1.256 million lines, and the economics for thatinitial phase, which is extracted from ameritech's application for construction approval, is presented below.table 1 presents the economic data for the first 10 years. it is important to note that these data are strictlyfor the transport network; they do not include any costs or revenues for the provision of "content."construction of the first 1.256 million lines is planned for completion early in the third year. both marketpenetration and usage per customer are expected to grow throughout the period, as shown by the revenueforecast. these revenues are for the transport of all types of content, including broadcast television, video ondemand, interactive multimedia services, and highspeed data access. the total costs each year consist of threecomponents: (1) the costs of constructing additional lines, which ends in the third year; (2) the costs of addingequipment for additional customers and additional usage per customer; and (3) the costs of providing customerservice.the cash flow each year, revenues minus costs, is discounted to year 1 equivalent values and cumulated,and is presented in table 1 as cumulative discounted cash flow (cdcf). as shown, the cdcf turns positive inthe eighth year. in actuality, as additional lines are constructed in each of the six major metropolitan areas,building on the initial base, economies of scale are expected to make the economics even more favorable.table 1 economic data for initial phase of ameritech's deployment of broadband access transport infrastructureyearcustomer locations passed(millions)revenues ($millions)costs ($millions)discounted cumulative cashflow ($millions)10.50.963131.2(130.2)21.05847.9162.5(233.8)31.25684.285.8(235.2)41.256115.565.8(198.4)51.256137.475.4(157.0)61.256154.071.1(106.9)71.256162.965.1(53.5)81.256176.366.40.891.256191.168.355.6101.256200.248.1117.0conclusionsthe greatest challenge to the realization of a ubiquitous wideband national information infrastructure, annii that is truly "national," is the provision of economical broadband access transport to individual residencesand small businessesšpopularly referred to as the "last mile." access transport to large business locations canbe based on usage, because such locations, as well as the nodes and internodal network and the informationrepositories, are all shared facilities and are used (and paid for) by multiple users. by contrast, access transportplans for ubiquitous broadband access to the national information infrastructure in theameritech region188the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.to individual customer locations must be sized for the peak requirements of the most demanding application,even though that capacity is unused most of the time, and its cost must be borne by that individual customer.although these facilities cannot be shared by multiple customers, they can be shared among multipleservices to the individual customer. ameritech has committed to providing broadband access in its region,utilizing a hybrid optical fiber and coaxial cable architecture, supporting a wide range of applications. theseinclude video services similar to current cable television, expanded digital multicast services, and interactivemultimedia services, as well as high speed data services. by providing a platform for a wide range of services, acosteffective solution is achieved.ameritech plans to begin construction in 1995, ramping up to a rate of 1 million lines per year by year endand continuing at that rate to deploy 6 million lines throughout the region by the end of the decade. the fcc hasgranted approval for construction of the first 1.256 million lines, in the chicago, cleveland, columbus, detroit,indianapolis, and milwaukee metropolitan areas.plans for ubiquitous broadband access to the national information infrastructure in theameritech region189the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.23how do traditional legal, commercial, social, and politicalstructures, when confronted with a new service, react andinteract?maria farnonfletcher school of law and diplomacytufts universitystatement of the problemit is somewhat ironic that the internet, which was originally pioneered for national security reasons in theunited states, has evolved into an international network. in fact, it is the open, international character of theinternet that is largely responsible for its growth. its explosive expansion has been exported from north americato europe and asia, in particular. at the same time, the internet poses serious challenges to many traditionalstructures at many levels: regulatory, commercial, legal, and cultural.many governments have enthusiastically embraced the introduction of the internet without understandingits subversive character. as new services like the internet travel to countries where government intervention inthe market is strong, and where the telecommunications industry remains a stateowned monopoly, thesecountries will seek to define the internet in traditional terms. the determination to construct a nationalinformation infrastructure (nii) by imposing standards, reserving service transmission for a monopoly provider,and controlling content reflects a complete misunderstanding of the internet model.this paper evaluates the development of the international internet as a case study of how the evolution anddiffusion of a new service are affected by preexisting factors. in other words, how do traditional legal,commercial, social, and political structures, when confronted with a new service, react and interact? even if theinternet represents an entirely new paradigm of services technology, it seems clear that domestic governmentsand industries will seek to apply a familiar framework, and thus exert the same control and derive the samebenefits that the current telecommunications system allows. after reviewing the historical structure of thetelecommunications industry, this paper evaluates the introduction of networking in several geographic regionsto determine how a new service affects the traditional telecommunications model. the question then becomes,what can be done to preserve the originality of the internet as a decentralized, scalable network on a global basis?backgroundthe old paradigm: government ownership and monopoly servicetelecommunications operators (tos) have traditionally been structured as stateowned monopolies forseveral reasons:1. natural monopoly. telephone service has long been cited as an example of a "natural monopoly,"characterized by significant ex ante investment (initial fixed costs) and increasing returns to scale. thesefactors imply that the size of the firm must be large to capture economies of scale and to recoup the initialinvestment.how do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?190the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.2. public good. government ownership is explained by the existence of "public goods," where the marginalcost of providing this good to an additional user is zero, and where it is difficult or impossible to limitconsumption of the good. thus, a commercial firm has no incentive to produce this type of product. anexample of a "pure" public good is national defense. telephone service, once the network is in place,might also be considered a public good.3. incomplete markets. incomplete markets occur whenever markets fail to produce a good or service, evenwhere the cost is less than what people are willing to pay. telephone service to rural areas or even to lowvolume residential users could be considered an incomplete market, because firms may not generateenough business to recoup their investment. thus, governments have justified their monopoly oftelephone services as a welfare obligation, with the ultimate goal of providing access to every citizen, or"universal service."4. national security. keeping the communications infrastructure out of foreign ownership and in the controlof the government was long considered vital for protecting the national security, especially in times ofcrisis or war.though it is overly simplistic to suggest that state ownership is inherently less efficient than privateownership, because of the large size of stateowned enterprises (soes), they tend to attract stakeholders whodepend on the actions of the firm for benefits.1 the to, for example, is often the largest employer in a country.stakeholders include employees, suppliers, and the relevant government bureaucracies. because the benefits theyderive are significant, stakeholders have an incentive to organize in order to pressure the firm, and through thisorganization gain their power. even though shareholdersšnamely the publicšare the legal owners of the firm,they remain too dispersed to organize effectively. the stakeholders thus assume de facto control of residualrights. governments are willing to maintain inefficient soes for fear of antagonizing powerful stakeholders,who are also political constituents.tos in many countries across europe, asia, africa, and latin america have clearly attracted strong groupsof stakeholders, including labor unions, equipment suppliers, and the government agencies that direct the to.the employees are concerned foremost with retaining their employment, while the domestic equipment suppliersoften depend on the national to for the bulk of their orders. the third stakeholder, the government, also has aninterest in preserving the to under state ownership. by setting high tariffs for longdistance use, and subsidizingshorter domestic calls, politicians garner public support. furthermore, a profitable to can be used to crosssubsidize other operations, such as the postal service. in essence, the policy of high tariffs for phone usageconstitutes an indirect tax.2a changed consensusthe question is why the consensus that supported the "natural monopoly" and state ownership of telephoneservices began to dissolve in the early 1990s. there are essentially three motivating factors:1. introduction of digital technology;2. privatization of soes around the world; and3. globalization of business.the most fundamental driver for this trend is the revolution in digital communications technology. agovernment monopoly was easy to justify for a service that was fairly undifferentiated (i.e., basic phone calls)and for which demand was relatively inelastic. furthermore, the provision of universal service was a legitimategoal of governments, given that private industry was unwilling to assume the cost of laying lines to everyhousehold. however, since the 1970s, telephone penetration no longer constitutes a legitimate measure of theeffectiveness of the telephone company. with the introduction of enhanced services, including voice mail, tollfree calling, and the establishment of data networks, the diversity and scope of services have become much moreimportant indicators of progress.how do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?191the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the extraordinary growth of private valueadded networks (vans), especially within multinationalorganizations, demonstrates the failure of the to system to adjust to technological innovation. the van marketis characterized by high product differentiation and elastic demand, meaning narrow groups of users whodemand highly customized services. it seems clear that in markets where the product is highly differentiated anddemand is more elastic, government monopolies tend to be less flexible in meeting the demands of the marketbecause of the constraints imposed by stakeholders.3 telecommunications has assumed a vital role in thecompetitive advantage of firms, to the point where pricing and the availability of advanced services affectlocation decisions. this presents a fundamental conflict with the traditional goals of the to, determined by thestakeholders, of maximizing revenues through high tariffs, and of controlling entry to the network and to theequipment procurement market. telephone rates set by tos are often not cost based, but created according topolitical pressures for crosssubsidization. although some constituencies benefit from these subsidies, includingresidential and rural callers, they raise the costs of doing business and thus harm overall economic efficiency.in addition to the onset of digital technology, fiscal and competitive pressures have convinced manygovernments of the need to privatize the telecommunications industry. telecommunications is now seen as anadvanced factor that will determine a nation's competitive position in the global marketplace. however,governments have encountered difficulty attempting to deprive to employees of their civil servant status, ormore drastically, of their jobs. the political pressures inherent in the privatization process have often resulted inunhappy compromises that are at odds with the new services. for example, the ability of the european tos toretain their monopoly of voice services even after 1998 will provoke tensions, especially when the internet isused to carry voice.finally, changes in the telecommunications industry have mandated the globalization of companies withinthe industry. these changes have taken place at the regulatory level, as well as at the levels of the customer, thecompetition, and the industry as a whole. the consumer who purchases voice, data, and video services isgrowing more and more homogeneous across national borders. at one time, consumers in the developed worlddemanded more sophisticated products, whereas consumers in developing nations strove simply to acquire basictelephone service. however, reliable communication is now recognized as providing the means to successfulcompetition across the world, and new technology has given consumers the opportunity to leapfrog outdatedcommunications systems to achieve a certain measure of equality. thus, the cellular phone is no longer simply astatus symbol for the wealthy but is instead a vital business tool from london to moscow. the globalization ofmany industries provides for synergistic development: as financial services, for example, become standardizedworldwide, the technology that supports these services must likewise go global.the u.s., british, and japanese companies that underwent liberalization in the 1980s, and are thus free frompolitical constraints that bind other tos, have been much more aggressive in their international strategies. thesefirms are seeking to provide onestop shopping for all telecommunications services by creating worldwide,seamless networks. at the same time, these companies have all realized that no single firm possesses theresources to offer global services alone. the result has been a host of alliances, including at&tunisource (thetos of holland, switzerland, and sweden), eunetcom (france telecom and deutsche telecom), and the britishtelecomšmci joint venture.together, the revolution of digital technology, the privatization of soes across industrialized as well asdeveloping countries, and the internationalization of business have totally undermined the traditional tostructure. the internationalization of the internet can only hasten this trend. the internet, as a decentralized,uncontrolled service, stands in direct contradiction to the centralized, governmentcontrolled model of the oldto system. the old system includes not only the state enterprises themselves, but also the entire system ofstandards setting (including supranational institutions like the international telecommunications union), apricing structure distorted by political considerations, a national body of law that regulates areas includingintellectual property and pornography, and even the national security of certain countries. yet in responding tothe challenge of the internet model, countries often rely on traditional methods of control that are renderedineffective by the new services. in singapore, for example, internet users are encouraged to report seditious orpornographic material to the singapore broadcasting authority. in the united states, senator jim exonsponsored the communications decency act in response to pornographic images available on the internet. ineurope, the european union's (eu) council of ministers has agreed on a directive that limits the transmission ofinformation about individuals,how do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?192the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.especially to countries that do not offer adequate protection of privacy. all of these efforts will be impossible toenforce without literally pulling the plug on users.the problem is that many societies remain wedded to traditional structures that no longer match the newcommunications services. the eu has set 1998 as the deadline for the liberalization of the telecommunicationsmarket. the stateowned tos will face mandated competition in every are but voice communications. however,new services like the internet can integrate voice, data, and images, rendering old distinctions between basictelephony and valueadded services meaningless. perpetuating the voice monopoly of the tos will significantlyhinder the advance of countries that fail to recognize the advancing convergence of technologies and services.though governments should continue to ensure that the majority of the population receives telephone access, theinternet demonstrates the foolishness of reserving a small slice of communications services for a monopolyprovider.analysisthe internet has grown far beyond the original vision of a network connecting research institutions anduniversities in the united states. more than 70 countries now have full tcp/ip connectivity, linking over 30million users, and about 150 countries have at least email service. about 1 million users are joining the internetevery month worldwide.4 governments across the world have identified international networking as critical totheir national competitiveness, although few seem to actually have considered the subversive role of the internetin undermining traditional political, commercial, and social structures.political issuesas subcomandante marcos, the leader of the zapatista rebellion in the chiapas region of mexico,explained, "what governments should really fear is a communications expert."5 though traditionalcommunications outlets such as radio and television are centralized and often controlled by the government, theinternet remains decentralized, diffuse, and nearly impossible to police. thus, communiqués from the zapatistasare transmitted throughout the world via the internet. likewise, during the 1991 coup in moscow, reformerstrapped inside the russian parliament sent out bulletins on the internet, which the voice of america picked upand then transmitted back to russia over the radio. obviously, the most repressive societies that restrict access tothe necessary hardwarešincluding north korea and iraqšwill not be toppled by the information revolution. itis regimes like china and singapore that recognize advanced telecommunications as a vital component ofeconomic development, yet seek to maintain control over new services, that will face the most serious dilemmas.legal issuesthe totally unregulated nature of the internet has allowed the diffusion of content that violates not onlypolitical controls but also legal structures. some of the most public controversies have surrounded the diffusionof pornographic images on the internet. in the united states, senator jim exon sponsored the communicationsdecency act, which would set fines and jail terms for "lewd, lascivious, filthy, or indecent" material on theinternet or other networks. in new zealand, the primary link to the internet is threatened by a newantipornography law.6 u.s. service providers have responded by increasingly policing their own users. forexample, america online recently cut off subscribers using the network to transmit or receive child pornographyimages and eliminated the online fan club of the music group hole.other pressing legal issues include the protection of intellectual property rights and of privacy on theinternet, as well as censorship more generally. essentially, the internet's tradition of open access and its userdriven content challenge the traditional methods employed by societiesšincluding copyright laws and limits ondistributionšto restrict access to and use of controversial or protected information.how do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?193the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.standards settingstandards setting has long been of critical importance in the telecommunications industry, even more so asinternationalization has progressed. the interface of various tos, equipment suppliers, and users requires somedegree of cooperation and uniformity. standards for basic telephony services have traditionally been set bynational governments through their ownership of the monopoly to, and through cooperative agreementsbrokered by international organizations such as the international telecommunications union (itu). however,the inability of these organizations to keep pace with the advent of new services has allowed users and othernontraditional groups to subvert the topdown, "perfectionoriented" model of standards setting.the success of the internet centers on its structure as an eminently flexible, open architecture, which hasallowed its evolution to be determined largely by the users themselves. bodies like the itu have grownincreasingly irrelevant with the introduction of new services such as the internet, and have seen their turf erodedby new organizations that do not necessarily have official government sanction. for example, the massachusettsinstitute of technology (mit) and the european laboratory for particle physics (cern) announced in july1994 that the two institutions will work together to further develop and standardize the world wide web(www).7 likewise, countries such as denmark and singapore, which are aiming to become the networkinghubs for their respective regions, are seizing the initiative to establish the regional standard. as peter lorentznielsen of the danish ministry of research commented, "we can't wait for brussels to act."8the problem of compatibility obviously arises when systems such as unix and edi meet, but users whobecome dependent on networkingšfrom students to home shoppers to small and large businessesšwillincreasingly demand interoperability. the internet offers the possibility of entirely userdriven services andtechnologies. thus, despite the recent pronouncements of the group of seven (g7), governments may ultimatelyhave little to say about the internet or other new services. the debate over integrating the tangle of supranationalstandards institutions (itu, ccit, itsc, and so on) may therefore be moot as well. the internationalization ofdata networks from the fidonet to the internet demonstrates above all the importance of open, flexiblearchitectures that can be adapted to regional demands and constraints. centralized standards setting makes aslittle sense as centralized network control.regional studiesasiaalthough the internet is growing at a phenomenal rate in asia, three critical components are lacking: legaland social structures, and software interfaces. furthermore, dictatorships (china) or merely repressive regimes(singapore) will undertake significant political risks by introducing networks that have international reach andthat are impossible to perfectly monitor. in addition, the dominance of english on the international internetprovokes charges of "linguistic imperialism" that might "weaken indigenous languages and colonize the mindsof the asians."9 nevertheless, within east asia only laos and burma currently remain without any form ofinternet access.hong kongalthough hong kong's supernet is the oldest and main provider of access lines on the territory, eight otheraccess providers have appeared since late 1994. in a classic example of the threat the internet poses fortraditional tos, the hong kong telecommunications authority raided and closed down seven commercialaccess providers to the internet in march 1995 for operating without a license. the companies claim thatproviding access to the internet falls outside of current regulations.10how do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?194the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.singaporesingapore leads asia in building digital networks to support an information infrastructure, and is clearlyaiming to serve as the hub for the rest of the region. despite singapore's rapid advance, the government isattempting to police the internet by encouraging users to report any criminal or antisocial behavior to other usersas well as to the singapore broadcasting authority, a government agency. as george yeo, the minister forinformation and the arts in singapore, admitted, ''sba cannot police on its own without the support andcooperation of the members of the singapore cyberspace community."11 nevertheless, the telecommunicationsauthority of singapore plans to issue licenses for two more access providers for the internet in addition tosingapore telecom, which is currently the sole provider of internet access through singnet.chinachina's leaders have quickly realized that the explosive economic development of the country can only besustained by a modern telecommunications infrastructure. as a result, the chinese government has been forcedto introduce limited competition and to welcome foreign investment. this is especially significant for a countryruled by an authoritarian regime, where communications have long been defined as a national security issue. injune 1994, china broke up the telephone monopoly, which resulted in the formation of several new companies.liantong communications was formed in july 1994 by the cities of shanghai, beijing, and tianjin as thecountry's second operator. the people's liberation army established ji tong communications as another newcarrier. despite the urgent need for capital, however, foreigners have so far been banned from actually owningand operating networks, although they have been allowed to invest in the production of telecommunicationsequipment. foreign companies are thus forbidden to take any equity stake in telephone networks.china has one of the lowest telephone penetration rates in the world, with only three lines available forevery 100 people.12 the government plans to lay about 15 million lines per year until it reaches 100 million linesby the end of the century. however, china's leaders have not restricted their vision to basic voicecommunications, but also plan to leapfrog to the latest technologies through the socalled "golden projects." thegolden projects include three separate initiatives:1. golden bridge: the backbone for data, voice, and image transmission;2. golden card: a national credit card transaction system; and3. golden gate: a paperless trading system to support offices, financial institutions, and foreign trade.to accomplish the golden projects, the chinese government has called for overall planning, jointconstruction, unified standards, and a combination of public and special information networks.13 china willconcentrate its investment in three main areas: digital switching equipment, fiber optic cable networks, andexpanding the mobile phone networks.14 the chinese ministry of electronics industry assigned the goldenbridge project to ji tong communications company, which will construct a nationwide information networkusing satellites and fiber optic cables. although many major cities have already built local data networks, thegolden bridge will ultimately link 500 cities and more than 12,000 information sources, including largeenterprises, institutions, and government offices. ji tong has entered an alliance with the china national spaceindustry administration, as well as with ibm, to support construction. ji tong has established the shanghaijinquiao network engineering companyštogether with shanghai pushi electronics company and twouniversity and scientific institutionsšto begin building the golden bridge in shanghai. the jinquiao networkengineering company will be responsible for designing, building, and maintaining a system that is to link thestate council, provincial capitals, more than 400 cities, and over 1,000 large enterprises throughout the country.15however, the reality of the communications culture in china remains in conflict with the stated goals of thegolden bridge. at present, information centers for the central government's 55 ministries store 75 percent of allofficial information in china but are not interconnected and keep almost all of their information secret.according to a report in economic information daily in november 1994, only about 6 percent of all officialhow do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?195the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.information is allowed to publicly circulate. partly as a result of the continuing perception of information asproprietary, only 300 of the country's 600 data banks are in fact operational, and fewer than 60 of these offerpublic access.16 thus, most of the software and equipment that china has introduced from abroad to support theinformation infrastructure remains useless. economic information daily pointed out that 90 percent of the100,000 electronic mailboxes imported from the united states in 1992 remain idle. essentially, the premise ofthe golden bridge remains questionable within a regime whose attitude toward information is ambivalent at best,and where institutional jealousies regarding data prevent its diffusion even among government offices. there aresigns, however, that this is slowly changing. for example, china's central bank released current money supplyfigures in 1994 for the first time since the communist takeover.17europeat the moment, about 1 million computers in western europe are connected to the internet. however, thedevelopment of international networks in europe has been hampered by the perpetuation of the stateowned,monopoly to structure. as a result, movement toward a european information infrastructure has been mostlycontrolled by the state telecommunications companies (telcos), and thus fragmented along national lines. forexample, while britain offers the most liberal telecommunications market, where even cable television operatorsare allowed to offer voice services, not only does france telecom continue to maintain a total monopoly overvoice communications, but the french government has also failed to propose any framework for privatization. initaly and spain the basic telephone system itself requires significant investment.in addition, concerns about u.s. cultural dominance, which have pervaded eu legislation on media andfilm, have entered the discussion on the internet. france is especially resistant to opening the country tointernational networks that might threaten local culture and language.18 (ironically, internet traffic is growing ata monthly rate of 15 percent in france.19) eu commissioner edith cresson of france has proposed a 0.5 percenttelecommunications tax to subsidize european content for online services. jacques santer named the erosion ofcultural diversity as the number one risk of the technological revolution in the opening speech of the g7conference on the information society.20finally, european governments disagree among themselves as to what information belongs onlinešscandinavia's tradition of open government contrasts with the more secretive british structure.21 partial elementsof an information infrastructure already exist in europe in the form of national academic and commercialnetworks, but europe still lacks guidelines for coordinated regional development. denmark is determined toestablish itself as the network leader in europe, with the goal of bringing all government offices, hospitals,schools, businesses, and research institutions online by the year 2000 in the "infosociety 2000" project. as thefirst mover in europe, denmark also hopes to create the standards for the whole continent. according to thedanish proposal, all government agencies would go online first, creating a large body of users to drive costsdown before introducing the network to the general population. yet denmark has not addressed the monopolyposition of the stateowned to, teledenmark, which was only partially privatized in 1994. furthermore,infosociety 2000 reveals a statist approach to telecommunications development that is seriously misplaced in aworld where the private sector continually pushes innovation and creates its own standards.the concern that europe will be left behind as the united states roars ahead on the informationsuperhighway prompted the creation of an eu task force led by martin bangemann, the german minister ofeconomic affairs. the task force consists of 20 politicians and the heads of major european communicationsand computer firms. the task force reported their findings to the european council of ministers in may 1994,beginning their statement with an affirmation of "market mechanisms as the motive power to carry us into theinformation age. it means developing a common regulatory approach to bring forth a competitive europewidemarket for information services. it does not mean more public money, financial assistance, subsidies, orprotectionism."22 nevertheless, there remains a wide gap between these words and the reality oftelecommunications development in europe. in contrast with the haphazard and anarchic evolution of theinternet in the united states, the process in europe aims to be much more deliberative and topdown. as horstnasko, vice chairman at siemens nixdorf, stated "in the u.s. they seem to shoot first and aim later. – what wearehow do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?196the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.trying to do is aim first and shoot when we are sure about what to do."23 this sentiment obviously contradicts thedecentralized character of the internet that has been so critical to its widespread acceptance.africaan underdeveloped telecommunications structure continues to hamper africa's access to all levels ofservice. overall, charges remain much higher and line quality much worse than elsewhere. besides the lack ofinfrastructure, the legal model of most national tos forbids thirdparty traffic, making the commercial provisionof online services impossible. nevertheless, led by nongovernmental organizations (ngos) such as the southafrican development community (sadc), as well as by commercial entities, some progress has occurred. thesadc's early warning unit gathers advance data on the food supply situation from each of the 10 membercountries as well as from international agencies to provide advance notice of impending food shortages.24 thesadc employs the magonet node through harare, and converts to fidonet nodes as they become available tocommunicate with various institutions via email. rascal datacom is currently working with barclays bank toestablish an overseas banking retail network (obrn), which will provide email service to 11 africancountries, the middle east, the caribbean, and the united kingdom. the initial phase of the projects is centeredin nairobi and is based on a combined lan and x.25 solution. ethernet lans provide connectivity betweenusers in major sites, whereas users across broader regions will apply more flexible solutions appropriate to localuser requirements and available services and technologies.25 by employing solutions that match local needs andinfrastructure, both the obrn project and the sadc early warning system demonstrate that online connectivityis possible and highly useful even in regions that have missed the traditional route to telecommunicationsdevelopment.another route to establishing connectivity in less developed areas is the galvanizing force of a "localleader." for example, an individual in south africa has connected the country's research and academic internetbackbone to similar institutions in botswana, lesotho, mauritius, mozambique, namibia, swaziland, zambia,and zimbabwe.latin americaas in africa, local telecommunications infrastructure remains an obstacle to highspeed international links.although international operators like mci and at&t offer service, coverage is not total, forcing users toemploy the local to the "lastmile problem" becomes especially acute for communications outside ofmetropolitan areas. many multinationals in latin america have thus opted to create private vans. eastmankodak, for example, relies on a private x.25 data network that links to its rochester, new yorkbasedcomputing center and to other host mainframes in mexico and brazil. kodak opted for the x.25 network becauseof its ability to operate on bad wiring.26 as companies in africa and latin america create their own networkingsolutions, this sometimes allows the local population to piggyback on private carriers. for example, investmentby banco pacifico has encouraged the diffusion of internet connectivity in ecuador.27 however, tight control bythe government on international traffic can limit this option. thus, the internet remains fairly underdeveloped inlatin america.forecast and recommendationsthe international internet versus local networksmany multinational corporations have already established proprietary valueadded networks (vans) tocarry vital data among farflung subsidiaries, suppliers, and customers. likewise, industry groups in variouscountries have created their own networks to promote information sharing. an example of the latter is electronicdata interchange (edi) in europe. edi was invented by the retailing and automobile industries as a "businesshow do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?197the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.process and not a technology."28 the essential characteristics of the internet as a global, scalable, decentralizednetwork that delivers information at cost stands in contrast to the highly centralized, planned, and focused natureof these local business networks.29 nevertheless, both services are similar in that they are user rather thanvendordriven.in terms of standards, the internet assigns arbitrary addresses, while edi employs an x.400 messagingsystem, in which addresses are logical and hierarchically structured. searches on the internet require a good dealof time and luck, but searches on local networks are usually constructed to be both dependable and repeatable.furthermore, the services offered on businessoriented vans are carefully planned to meet specifically definedinformation needs. mike holderness, writing in the guardian, called these local vans the "outernet" andviews the emerging information infrastructure as a battle between the internet and the outernet models.the internet has succeeded precisely because it is not tightly controlled to deliver specific bits, but insteadoffers whatever the users themselves see fit to make available. by definition, this structure cannot adopt astandard to offer 100 percent reliability. in contrast, the outernet perpetuates the hierarchical, "onetomany"communication model of traditional media.30 thus, the question is whether the internet can succeed as a modelfor an international network, given the concerns of national governments over issues of control, or whether theouternet model will succeed instead.leaders versus followersbesides the less developed countries (ldcs), even the member nations of the eu are worried that theunited states is beating them in the race to construct an information superhighway. south african deputypresident thabo mbeki reminded the group of seven (g7) meeting in march 1995 that manhattan offers moretelephone lines than all of subsaharan africa. politicians warn that the digital revolution will divide societiesand nations between the informationrich and informationpoor. this pervasive sense of some countries gaininga lead in the information revolution is somewhat misplaced. though the dearth of actual hardware obviouslylimits the ability of a country to connect to international networks, once connectivity is established, even theeconomically poor immediately become informationrich.the question remains, if the global information infrastructure will be constructed by blending nationalinformation infrastructures, what of the countries that lack the resources to develop this infrastructure? theextent of network services has been directly related to overall economic development.in many developing countries, connectivity is available only through a fidonet link to a few pcs. incontrast, for most of the industrialized world, building a national backbone is now a top priority. for many ofthese countriesšincluding australia, chile, japan, south africa, and switzerlandšthis has resulted from thecreation of research and academic networks.31 nevertheless, the fidonet, which is based on a simple protocol,has provided even the poorest countries with a networking tool to store messages and check for transmissionerrors. furthermore, the fidonet enables users to evade the local to by allowing messages to be polled from asite outside of the country.as mentioned in the "regional studies" section above, often a country will act as a local leader within aregion, such as singapore in southeast asia, costa rica in central america, or south africa in southern africa.as demand and experience increase, the connections may evolve from fidonet to uucp to ip.32obstacles to the international internetdespite the clear intention of the industrialized world to foster the building of national backbones, and thegradual diffusion of connectivity in many developing countries, the traditional to structure, as well as theresulting legal and commercial models this structure fosters, remains a serious obstacle to a truly internationalinternet. although technical difficulties can be overcome with resources from institutions such as the worldbank, ngos, and governments themselves, the traditional mindset of control over the communicationsinfrastructure and services is more difficult to displace. as discussed in the "background" section of this paper,how do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?198the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.governments have long justified their ownership of the telecommunications operator on the basis of nationalsecurity reasons and also derive significant political and revenue benefits from this ownership. although thisstructure has been seriously undermined in the united states, the european union, and parts of asia, it remainsstrong elsewhere.conclusionideally, an international network like the internet should provide a protocol that is easily adapted to a widevariety of infrastructure development stages, and offer services that can be tailored to respect the cultural, legal,and regulatory norms of every country. however, the model that the internet has provided demonstrates that aninternational network will, by definition, still act to undermine many traditional structures that have evolvedaround the old to system. rather than seeking to impose old standards of behavior and control on the internet,governments can best encourage the development of national information infrastructures by eliminating theinherent conflicts that exist between the new services and the domestic organization of telecommunications. thismeans introducing competition into all levels of service and allowing the market to drive pricing and standards.notes1. this discussion in drawn specifically from a lecture by professor aaron tornell of harvard university, 1 march 1994, and more generallyfrom his course, "privatization" (economics 1470, spring 1994). i am grateful to professor tornell for providing the methodologicalframework for analyzing the structure of an soe.2. dutch, raymond m. 1991. privatizing the economy, telecommunications policy in comparative perspective. university of michiganpress, ann arbor, mich., p. 141.3. dutch, raymond m. 1991. privatizing the economy, telecommunications policy in comparative perspective. university of michiganpress, ann arbor, mich., pp. 144œ145.4. oecd observer, february 1995; and goodman, s.e., l.i. press, s.r. ruth, and a.m. rutkowski. 1994. "the global diffusion of theinternet," communications of the acm, vol. 8, august, p. 27.5. watson, russell, john barry, christopher dickey, and tim padgett. 1995. "when words are the best weapon," newsweek, february 27,p. 36.6. lever, rob. 1995. "battle lines drawn in debate on smut in cyberspace," agence france presse, march 29.7. isa, margaret. 1994. "mit, european lab to work on world computer network," boston globe, july 8, p. 62.8. neuffer, elizabeth. 1995. "europe's tiny cyberpower," boston globe, march 27, p. 1.9. dixit, kunda. 1944. "communication: traffic jams on superhighway," inter press service, august 8.10. liden, jon. 1995. "internet: asia's latest boom industry," international herald tribune, march 8.11. richardson, michael. 1995. "singapore to police information superhighway," international herald tribune, march 9.12. ramsay, laura. 1995. "china sees great leap forward in telecommunications ability," financial post, april 1, p. 30.13. yu'an, zhang. 1994. "china: country embracing information revolution," china daily, october 7.14. murray, geoffrey. 1995. "china sets priorities for massive telecom investment," japan economic newswire, march 31.15. wei, li. 1994. "china: company's creation marks 1st step towards network," shanghai star, october 18.16. chandra, rajiv. 1994. "chinacommunications: dial 's' for secrecy," inter press service, november 28.17. murray, geoffrey. 1995. "china sets priorities for massive telecom investment," japan economic newswire, march 31.18. neuffer, elizabeth. 1995. "europe's tiny cyberpower," boston globe, march 27, p. 1.19. monthly report on europe. 1995. "european union: online data service to shed some light on the eu," vol. 127, march 23.how do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?199the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.20. rapid. 1995. "opening address by jacques santer at the g7 conference on the information society," february 24.21. neuffer, elizabeth. 1995. "europe's tiny cyberpower," boston globe, march 27, p. 1.22. brasier, mary. 1994. "european business: race for knowledge on superhighway," the daily telegraph, july 25.23. tate, paul. 1995. "putting their heads togetheršeurope's top execs take on info highway challenge," information week, april 11, p.26.24. african economic digest. 1995. "africa and the information superhighway," january 30.25. african economic digest. 1995. "africa and the information superhighway," january 30.26. lamonica, martin. 1995. "latin american telecom remains in state of flux," network world, march 13, p. 37.27. goodman, s.e., l.i. press, s.r. ruth, and a.m. rutkowski. 1994. "the global diffusion of the internet," communications of the acm,vol. 8 (august), p. 27.28. computing. 1995. "edimarket growth," january 26.29. holderness, mike. 1995. "networks: when two cultures collide," the guardian, february 2, p. 4.30. holderness, mike. 1995. "networks: when two cultures collide," the guardian, february 2, p. 4.31. goodman, s.e., l.i. press, s.r. ruth, and a.m. rutkowski. 1994. "the global diffusion of the internet," communications of the acm,vol. 8 (august), p. 27.32. goodman, s.e., l.i. press, s.r. ruth, and a.m. rutkowski. 1994. "the global diffusion of the internet," communications of the acm,vol. 8 (august), p. 27.how do traditional legal, commercial, social, and political structures, when confrontedwith a new service, react and interact?200the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.24the internet, the world wide web, and open informationservices: how to build the global information infrastructurecharles h. fergusonvermeer technologies inc.summarythe rise of the internet represents the most fundamental transformation in information technology since thedevelopment of personal computers. we are witnessing the emergence of an open, distributed, globalinformation infrastructure based on the internet, world wide web servers, and mosaic. at the same time,inexpensive servers, fast networks, clientserver technology, and visual software tools usable bynonprogrammers are transforming the strategic use of information by organizations. taken together, thesedevelopments offer an opportunity to revolutionize information services and electronic commerce, generatingboth an enormous business opportunity and a chance to vastly improve access to information for everyone.over the next 5 years, the internet software industry will construct the architectures and products that willbe the core of information infrastructure for the next several decades. the promise of these technologies toenable a global information infrastructure can hardly be exaggerated. decisions made during this critical periodwill have a profound effect on the information economy of the next century. but there is a serious risk thatavoidable mistakes and/or entrenched economic interests will cause the opportunity to be lost or much reduced.this paper therefore discusses the principles that should drive technology development and adoption in theinternet market, especially for the world wide web. our goal is to promote the development of an openarchitecture internet/web software industry, and to support the deployment of the internet/web softwareindustry, and to support the deployment of the most open, easy to use, and productive information infrastructurepossible. we believe that a simple set of principles, if adhered to by vendors and buyers, can maximize theopenness, interoperability, and growth of both the internetbased infrastructure and the industry providing it.vermeer technologies intends to succeed as a firm by contributing to the development of this openarchitecture, webbased software industry. vermeer is developing open, standardsbased, clientserver visualtools for collaborative world wide web service development. these visual tools will enable endusers to use theinternet to inexpensively develop and operate powerful world wide web information services, which currentlyrequire complex programming.the opportunity to build a global information infrastructurethe global internet has rapidly evolved into the basis of an open, nonproprietary, global informationinfrastructure. the internet now contains nearly 25,000 networks and 30 million users and is growing at a ratenote: in january 1996 vermeer technologies was acquired by microsoft corporation.the internet, the world wide web, and open information services: how to build the globalinformation infrastructure201the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.of 10 percent per month. within the next year, internet connectivity will be bundled as a standard function withnearly all newly sold personal computers. the internet architecture already provides excellent nonproprietarystandards for basic connectivity, electronic mail, bulletin boards, and, perhaps most critically of all, the worldwide web architecture for distributed information services. the internet engineering task force (ietf) and thenewly formed mitcern world wide web consortium (w3c) will continue to evolve these basic standards(e.g., by adding security) and we support these efforts completely.at the same time, there is an explosion of commercial investment in development of internet software suchas web servers, web browsers, and internet access products. in general, we support the development of theinternet software industry because there is a real need to bring stateoftheart, commercial software technologyto the internet. in this way the internet and the web, originally developed by and for researchers, can and shouldbe made fully accessible to the entire world of endusers. we feel that this is an extremely important goal, nearlyas critical as developing the internet itself: information infrastructure should be easily usable by everyone, onany computer, and should not be available only to programmers and researchers. we do not require thateveryone learn typesetting and printing to write a book; we should not restrict electronic publishing to those whocan write computer programs.the independent software industry has already developed a large set of technologies that address theseproblems in other markets and that could be of huge benefit to an internetbased information infrastructure. stateoftheart commercial technologies applicable to the internet include visual tools and wysiwyg techniquesthat enable endusers to develop applications that previously required programming; clientserver architectures;online help systems; platformindependent software engineering techniques; and systematic quality assuranceand testing methodologies. adobe, quark, powersoft, the macintosh gui, and even microsoft have used thesetechniques to make software easier to use. if these techniques were applied to internet software, the result couldbe a huge improvement in everyone's ability to use, communicate, publish, and find information. however,commercial efforts must respect the openness, interoperability, and architectural decentralization that have madethe internet successful in the first place.the world wide web and the revolution in information serviceswith the possible exception of basic electronic mail, the world wide web (www) is the most vital andrevolutionary component of internetbased information infrastructure. the www architecture provides aremarkable opportunity to construct an open, distributed, interoperable, and universally accessible informationservices industry. the web, started about 5 years ago, now contains tens of thousands of servers and is growingat a rate of 20 percent per month. it is now being used not only to publish information over the internet but alsoto provide internal information services within organizations.in combination with tcp/ip, the internet, and smtpbased email integration services, the web will enablethe development of a new information services sector combining the universal access and directory services ofthe telephone system with the benefits of desktop publishing. if we develop this industry properly, and continueto honor the openness of the web architecture, the result will be an explosion of information access and a hugenew global industry.the importance of the web, of its open architecture, and of enabling everyone to use it can hardly beoverstated. the world wide web offers, for the first time, the opportunity to liberate computer users, publishers,and information providers from the grip of the conventional online services industry. this $14 billion industry,which includes such firms as america online and bloomberg, is strikingly similar to the mainframe computerindustry. it once represented progress but has long since become technologically obsolete. it maintains itsprofitability only by charging extremely high royalties and by holding proprietary control over closed systems.some current online services vendors continue to retard progress to maintain their financial viability.there is consequently a real risk that entrenched incumbents in the online services industry will try tosuppress the web or to turn it into simply another collection of proprietary, closed, noninteroperablearchitectures. there is a similar risk that other companies, such as vendors of commercial web servers,the internet, the world wide web, and open information services: how to build the globalinformation infrastructure202the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.browsers, tools, or system software, might attempt to do likewise to establish a new generation of closed,proprietary systems.such a return to the world of centralized, proprietary systems would be a disaster, but it need not take place.if developed properly by the emerging internet software industry, the web offers huge advantages relative toconventional, centralized online services and would generate gigantic revenues because the web enables manyapplications unreachable by the current industry. webbased services are enabling the free publication of hugequantities of information, realtime access to individual and workgroup data, the rise of largescale internalcorporate information services, the use of online services for educational and library applications, and the growthof information services and electronic commerce as a strategic component of all business processes.conventional services cannot do any of these things. they cannot make use of local, distributed, and/or realtime information stored on personal systems or workgroup servers; their capacity is severely limited; they are notinteroperable with each other; they cannot be used for internal information services; they cannot be managed bythose who create their content; they cannot integrate with database systems and legacy applications used inoperating businesses; they are expensive and uneconomical for many services, including most free services; theycannot be linked with each other; and they cannot be viewed using a single, common graphical program such asmosiac. in contrast, the world wide web offers the potential for millions of electronic publications, informationservices, authors, and publishers to evolve in a layered, open, interoperable industry with support fromnavigation and directory services.the current situation and some principles for future developmentthe internet, the web, and mosaic have already laid an excellent foundation for the development ofstandardized, open, distributed information services. however, two major problems remain. the first is that thisfoundation will come under attack from vendors interested in slowing progress or exerting control via closedsystems. the second problem is that the internet, and especially the web, remain much too hard to use.the first problemšattacks on the openness of the webšmust be dealt with simply by industry vigilance.internet software vendors should adhere to, and customers should insist on, several basic principles in thisindustry. these include complete support for current and future nonproprietary ietf and w3c standards; openarchitectures and vendorindependent apis; crossplatform and multiplatform products; and completeindependence of each product from the other products and services provided by the same vendor. thus buyersshould resist attempts by vendors to link viewers, servers, tools, operating systems, specific information services,and/or internet access provision with each other. every product and architecture should be open and forced tocompete independently.the second problemšthe fact that web services are still overly difficult to create and usešrequires furtherwork. at present, tools are extremely hard to use, do not provide wysiwyg capability, do not manage thecomplexity of hypertextbased services well, and do nothing to eliminate the need for custom programming.most interesting user interactions with web serversšsending or requesting electronic mail, performing textsearches, accessing databases, creating or filling out formsšrequire custom programming on the web server.the importance of this problem is frequently underestimated because of the computer science origins of theinternet community. however, a few simple facts can illustrate this point. first, information services and/or webservers can remain hard to develop only when there are few of them. there are still at most 100,000 web serversin use, most of them deployed in the last 6 months. but this year, 2 million to 4 million intelbased servers willbe shipped, and the server market is growing at a rate of 100 percent per year. if in the long run 10 percent of allservers and 5 percent of all personal computers run web servers, then the web server installed base will soon bein the millions. if the average web server holds content from 5 people, then it will be necessary or at leastdesirable to enable 5 million to 10 million people to develop web services. there are not enough programmersto do that.even more importantly, the people who understand what the services should look like are the professionalsclose to the application, not the programmers currently required to code it. furthermore, the development andmaintenance of information services should be a seamlessly collaborative clientserver activity.the internet, the world wide web, and open information services: how to build the globalinformation infrastructure203the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.it should be possible to develop, debug, and edit services over the internet, using pcbased graphical tools,in collaboration with other remotely located or mobile developers.we have seen situations like this before. before development of spreadsheets, accountants performedspreadsheet computations by asking mis to write a cobol program. with the advent of pcbased spreadsheets,professionals could perform such computations themselves far more effectively and could share them withcoworkers. desktop publishing, presentation graphics, and visual application development tools such aspowerbuilder had similar effects. we believe that modern graphical tools will do the same for the constructionof webbased online information services.vermeer technologies and its missionvermeer technologies inc. is a venturecapital funded independent software firm founded in 1994 bycharles ferguson and randy forgaard. vermeer technologies intends to become an industry leader in internetsoftware by contributing to the construction of an open, standardsbased information infrastructure available toeveryone. in particular, we plan to make it easy for anyone to develop a webbased information service, eitherfor internal use within their organization or for publication on the internet.accordingly, vermeer is developing open, standardsbased, clientserver visual tools for collaborativeworld wide web service development. these visual tools will enable endusers and professionals (collaboratingacross the internet) to inexpensively develop and operate powerful world wide web information services,without the need for programming. (these services currently require complex custom programming.)nonprogrammers will be able to develop services for the first time, and professional developers will gain highlyleveraged productivity tools. our architecture also supports many usage models ranging from individual selfpublishing to collaborative remote authoring for large commercial web hosting services. our architecture isplatform independent and will be available on all major client and server computer platforms, on all operatingsystems, and for all standardconforming commercial web servers. our vendorindependent, open apis willenable us to construct partnerships with other industry leaders in complementary areas such as text indexing,electronic payment systems, highperformance web servers, and other functionalities to be developed in thefuture.vermeer intends to rigorously support ietf and w3c standards and is a member of the world wide webconsortium. vermeer's architecture relies on and supports all current standards and is designed to accommodatefuture standards as they are finalized. vermeer is an entirely independent firm and has no entrenched interestsderived from existing or proprietary products or businesses. vermeer is therefore completely free to focus on theconstruction of the most open, easy to use, interoperable products possible.the internet, the world wide web, and open information services: how to build the globalinformation infrastructure204the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.25organizing the issuesfrancis dummer fisheruniversity of texas at austinsummarythe following should be kept in mind as the rich submissions to the nii 2000 project are synthesized:1. division of markets. the local telecommunications markets should be differentiated; the residentialbroadband market should be distinguished from the downtown business market and from the ruralmarket. the downtown market is likely to take care of itself, and the rural market has unique problems.in view of the importance of universal coverage, the residential market should be emphasized.2. ''competition" and the closed character of cable tv. it should be clearly recognized that cable tv, sinceit controls both network and content, has a strong economic interest in excluding other content suppliersfrom unrestricted access to the cable tv broadband network. even were there to be more than onevertically integrated broadband network reaching homes, openness would not be assured without eitherregulation or antitrust litigation. confusion between "competition" and "openness" should be avoided.3. state and local regulation. federal regulation was emphasized in the nii 2000 project white papers anddiscussion. yet federal power to force openness may be limited, as suggested by the claim of thosesupplying video that the first amendment confers rights to select whatever video messages the networkowner wishes. it may ultimately be the power of local governments to place conditions on use of theirrightsofway that must be invoked to achieve universal and open use of broadband networks.4. a broadband connection to the internet. a connection to the internet for anyone who is served by a localbroadband network would achieve many of the goals of the national information infrastructure (nii)initiative.the residential home marketthe phrases "local exchange" and "local telecommunications market" can gain from greater precision. it isparticularly important to distinguish between the market in the downtown business area and the residentialmarket. the residential market can be defined as those homes and businesses that are passed by cable tv butthat are not located in the downtown market. in the downtown market, traffic is great and distances small, andthe relative cost of entry to serve the market is low. many different networks and services will be provided.hence, there is no problem of "openness." in the residential market, traffic is much lower in relation to the areacovered. the problems in the residential market will be to assure universal open service by at least one providerand to arrange that the network will be open.one reason for distinguishing the residential market from the rural market (customers not served by cable)is that the technology for providing broadband service in the former seems much clearer at this time. indeed, forthe residential market, there was in the nii 2000 project activities substantial agreement on the probabletechnology: a digital service over largely fiber networks with the ability to increase twoway bandwidth asdemanded. special problems in the rural market, besides providing the technology of choice, include difficultiesin providing bandwidth "symmetry" and the likely need for subsidies to assure universal service in sparselypopulated areas.organizing the issues205the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.since many of the most important policy issues that must be faced in constructing the nii are presented inassuring universal broadband communications to the home, it will help to address these issues in the specificcontext of the residential market."competition" and the closed character of cable tvthe wire broadband network that exists today in the residential market is that of cable tv. it is especiallyimportant, therefore, in considering how it might evolve as part of the nii to recognize the strong economicinterest of cable tv in continuing to operate a closed network. the problem in designing the nii is only partiallytechnical; it is equally economic. and unless we attend to it, economics may dictate technology."openness" is easy to define. a network is open if it permits users, at their choice, to be connected tocompeting sources of information and permits providers easy access to users. in a truly open network, users andproviders cannot be distinguished, although those connected to the network can be distinguished by the amountof bandwidth they require. the phone network is an example of an open network.for cable tv, an open network would permit a customer to connect to packaged tv programs offered byfirms that compete with the network owner and the packages it offers.cable tv is a closed, vertically integrated system. the existing cable tv system is well described in thewhite papers in this volume. what is important here is that the same company owns the network and sells, orarranges for the sale of, the content moving over the network. as contrasted with common carriage or an "opensystem," suppliers of content over cable tv do not compete directly for the business of customers, striking withcustomers whatever deal the market demands. the business arrangements are, rather, first between contentproducer and the cable company and then, for those products that cable tv decides to offer, between the cabletv company and those connected to its network.it could be argued that the profits of cable tv flowing from its monopoly character are necessary to raisethe money with which to upgrade the home network, but that harsh argument has not been advanced in the papers.perhaps understandably, the papers do not make clear the closed character of vertically integrated networksin the absence of regulation. bailey and chiddix state that "while companies such as time warner will be one ofmany content providers, these pc networks that the industry is building will be networks that success in acompetitive world will demand be kept open." rodgers contends that "where the network provider facescompetition" it has "an incentive to make interconnection as easy as possible." these comments ignore the profitmaximization behavior of competing virtually integrated companies. powers et al. postulate that where there isunbundling and resale on a fair basis, providers of services can compete, but the additional statement that"effective competition can bring about those results'' is not supported.in brugliera et al., one paragraph stands out:it is further arguable that regulation should work to inhibit or even prohibit any single entity from owning orcontrolling both program production and program delivery operations, if such control results in the customer beingdenied access to any available program source. consumer access should be a [sic] prime consideration.yet the general scenario presented in that paper is that of the "500 channel" technology that satisfies what isdescribed as "the consumer's primary desire for passive entertainment, and not interactivity." one wonders howthe writers would decide, if forced to choose for their homes today, between the telephone or the tv as a singlepermitted technology. would not interactivity and choice be reasons for their likely preference for the phone?why should those considerations diminish as bandwidth grows?of course, if one of the network competitors offers an open system, as does the phone company fortransporting voice and data, cable tv as a competitor will probably offer equally open transport as to thoseservices. according to bailey and chiddix, "regardless of whether pc interconnection ultimately flows througha number of competing national online services or through the laissezfaire anarchy of the internet, cableintends to offer a highly competitive avenue for local residential and business access to any viable serviceprovider." personick states, "all rbocs have expressed a commitment to deploy broadband access services asquickly asorganizing the issues206the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the market demand, technology cost trends, and regulatory/legal environment permit." these comments seem toapply to voice and data, and not to video. phone companies entering the video market claim that they, like cabletv companies, have the right to use their video network to capture customers for those particular contentservices offered by them in their videosupplying role.the interest of cable tv companies in closed systems is reflected not only in the fact that cable tvcompanies trade for much more than the capital costs of the network investments. it is shown also by thereluctance to be clear about "openness" as even an eventual goal for video networks. in austin, texas, where thecity is offering access to its rightofway (poles and conduits owned by the municipal electric utility) to any firmwilling to operate an open network, the local cable company, far from applying for the resources that it could usein its investment program, has sought to block the citysponsored effort to achieve openness.competition does not imply openness. even if competing networks did develop in the home market, it doesnot follow that customers would have the option of being connected to any information source of choice, or thatsuppliers of content would be able to access the network, if the competing companies were vertically integrated.for it would probably be in the interest of both such "competing" video providers not to open their networks toother content providers. for example, each network might offer an alarm service, but a third alarm service mightwell be denied access over either network.huber et al. consider the possibility of overbuilding local networks and estimate that the savings would"swamp" the unnecessary costs of overbuilding. their paper provides little support for this estimate, and itscomparison between monopoly and oligopoly probably applies to the monthly charges for those content servicesoffered. it does not follow that a competing deliverer of content could even gain access to either network.if one cable network could handle all the traffic in the residential market and were open to all, thecompetition between services envisaged by huber et al. could still take place, just as it can take place by firmsusing common poles or conduits and without the cost of overbuilding. this illustrates that it is important to beprecise in specifying exactly what it is that is in competition: the services of local carriers (alarm services, voicemail, etc.), the content provided, or merely two largely identical fiber cables.in sum, the economic interests of cable tv are at present in opposition to achieving the goal of the nii in anopen network; to achieve the desired openness on these important broadband networks will require public action.federal, state, and local regulationinterconnection and regulationsome of the papers in this volume intimate that the regulation required to achieve openness is fairlyminimal. arlow recommends that all providers of telecommunication services should be required to interconnectwith each other, "but there should be no other daytoday regulatory oversight or standards to which providersare obliged to adhere." mohan urges the public sector to "remove impediments to the deployment andinterconnection of competitive local exchange and access capabilities." these authors, and huber et al.,exemplify the tendency to treat deregulation and interconnection as feasibly consistent. in fact, however,mandated interconnection, without regulation of interconnection charges, would be meaningless, and with suchprice regulation closely resembles common carriage regulation, the heart of traditional government interventionin wired telecommunications.role for states and localitiesin both the papers and the discussion of them at the forum, there was a strong tendency to identifygovernment action as the action of the federal government. but it may well be that by applying the firstamendment to video content, courts have substantially reduced the federal power to prescribe the kind ofopenness for video that pervades the voice and data system of the telephone network and, to date, of the internet.organizing the issues207the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.as ithiel desola pool presciently forecast, it may ultimately be the power of local government to control itsrightofway that provides the governmental lever to open up the cable video networks.a mandate for broadband connection to the internetin this political climate, it could be that openness and switched access to competing content suppliers mightbe more readily achieved through connection to the internet than by what might sound like a backwardinvocation of common carriage regulation of video.in view of the unsettled constitutional basis for a federal mandate of openness in broadband networks, itcould be that a simple statement by the national research council urging localities to make broadbandconnection to the internet a condition of renewing municipal cable franchises could be an important step inachieving the goals of the nii.organizing the issues208the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.26the argument for universal access to the health careinformation infrastructure: the particular needs of ruralareas, the poor, and the underservedrichard friedman and sean thomasuniversity of wisconsinstatement of the problemthe national information infrastructure (nii) offers an opportunity for the poor, persons living in ruralareas, and the otherwise underserved to obtain a wide range of educational and medical services to which theycurrently have only limited access. these benefits can occur only if these groups gain access to the nii.universal access to the telecommunications infrastructure has been a stated goal for many decades. as a result ofa combination of funding transfers, funded and unfunded mandates, and regulations, the telecommunicationsindustry has achieved a 94 percent overall telephone access rate. unfortunately, the very groups that may benefitmost from access to the nii are the very ones that have had the lowest penetration rate in the existingtelecommunications infrastructure.upgrading the current system to the higher bandwidths that will be required for the interactive capabilitiesrequired in telemedicine will be expensive. unfortunately, current technologies are making it possible forcompetitive access providers to bypass the local access providers who are maintaining the existingtelecommunications infrastructure. they can preferentially provide services at a discount to the lowestcostclients. the local access providers are left with the highercost users such as the poor and rural population. theyare increasingly finding this an unreasonable burden, and many existing carriers are finding it uneconomical toupgrade services to these populations.these are the very populations that may benefit most from the nii. the impact on their education andhealth care will be dramatic and the benefits to society as a whole will be extensive. it will therefore benecessary for society (i.e., the federal government) to make sure that these populations have access to the nii.this will require a new funding mechanism that no longer relies on internal industry fund transfers. it willrequire additional government subsidies, new taxes on competitive access providers, or legislative mandates.we argue that the best course would be a system of valueadded taxes on all services that use the nii. thefunds from such taxes could be used to underwrite the development and maintenance of the nii.backgroundrural education and medical services are in the midst of dramatic changes. shrinking rural populations aremaking community schools and hospitals hard to justify. while many states have legislated new curricula thatattempt to improve the competency of students, rural schools generally lack the funds to attract the specializedteachers necessary for courses in these topics. in many rural areas there is a shortage of physicians. localhospitals are being forced to close because they are noncompetitive. medicare and medicaid provide decreasedphysician and hospital reimbursement in rural areas. many poorer patients in rural areas rely on medicare andmedicaid funding for medical services, yet these funds no longer cover the costs of providing even these basicservices. many residents of rural areas must travel long distances to regional health care centers. ruralthe argument for universal access to the health care information infrastructure: theparticular needs of rural areas, the poor, and the underserved209the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.preventive medical care is compromised because of the greater distances and limited access to services.the economic competitiveness of rural areas is being compromised because of their inability to access highspeed transmission lines for electronic mail, telecommunications, video conferencing, and access to largecentralized data bases used by libraries, in inventory control, and in updating of government records. personsliving in rural areas must pay toll charges when contacting government or regional health care offices, whereasresidents in larger cities have no such expenses.the case has been made that the nii will make it possible for rural, poor, and underserved populations toaccess highquality educational and health care resources. these resources have previously been denied to thesepopulations due to a combination of factors that include their inability to access these resources, the costs ofthese resources, and the limited number of highquality providers of these services who were willing to relocateat sites convenient to these populations. if these services can be provided via the nii at a level of qualitycomparable to that provided to other populations, then the main impediment to these groups' obtaining thisbenefit will be their ability to access the nii.distance education holds the promise of bringing specialized teaching programs to poor and rural schools.programs in foreign languages, science, and the arts, which are not available due to inadequate funding or classsize, could be made available by video linkages. participation in government programs and the provision of an"informed" electorate could result from improved access to computerbased educational resources. ruralconstituents could sign up for government programs, renew licenses, submit applications for benefits, and obtainmany other services, thus avoiding the time and expense now involved in travel to government offices. healthcare consultation by experts at distant regional health care centers might be provided at local clinics. preventivemedicine programs, health education, access to health resources, and rapid clinical consultations are all possibleconsequences of access to the nii.the provision of highquality educational and health care services over the nii is currently being evaluated.preliminary studies have shown that highquality services are possible. the mayo clinic maintains videoconferencing links between three sites in florida, arizona, and minnesota. it recently conducted a telemedicineprogram linking its clinic in rochester, minnesota, with the pine ridge indian reservation in south dakota. themedical college of georgia has several dozen sites on its telemedicine network. this year it has begun trialswith direct links to patients' homes so that individuals recuperating from heart surgery can be linked to theirdoctors for followup. the navy runs a teleradiology program estimated to save $14 million a year. studies ingeorgia have shown that using telemedicine can increase the number of patients admitted to rural hospitals.rural hospitals typically charge up to $500 less per day than hospitals in large cities. telemedicine offers a wayto retain and retrain medical professionals and help keep rural hospitals open.1norway funds the largest telemedicine program in the world. in that country telemedicine is part of thegovernmentrun health care system. practitioners regularly conduct consultations in a variety of specialtiesbetween remote hospitals and main academic centers. one ent specialist suggested that the number of cases hereceives from general practitioners has decreased by 50 percent since the introduction of telemedicineconsultations.2the nii is already being used to provide a wide range of medical information via bulletin boards,newsgroups, and electronic mailing lists via the internet and commercial services such as compuserve, prodigy,and america online. these services provide a great deal of patientoriented medical information. the worldwide web (www), a multimedia hypertext service, is the fastest growing component of the internet. it containsa plethora of medical services, including newsgroups and bulletin boards as well as diagnostic, educational, andpreventive services. most major state and federal health care groups provide online access to their resources viathe www. many medical schools and university hospitals already provide patient information, course material,uptodate medical literature, and even consultative services via the nii.many clinical services are available via the nii. the problem remains one of access. the cost of thecomputer equipment required to interface with the nii is falling dramatically and will eventually be only a smallfraction of the expense of providing quality educational and health care services by conventional modalities. themain expense will be the heavy investment necessary to bring the telecommunications infrastructure to thehomes and communities of all individuals, particularly those who are poor or live in rural or inner city locations.giventhe argument for universal access to the health care information infrastructure: theparticular needs of rural areas, the poor, and the underserved210the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.their locations in either decaying, densely populated inner city areas or widely distributed and sparsely populatedrural locations, it is extremely expensive to provide access.the relatively low income of these individuals makes it unlikely that commercial providers of the nii canprofit from providing this access. the commercial incentives for providing nii access to these groups may notexist. however, the benefit to the nation of providing quality education and health care to these individuals willbe great. because economic benefit will accrue to the nation as a whole rather than to the companies buildingand maintaining the nii, the usual capitalistic incentives that would make providing such services worthwhileare not appropriate. the government must either subsidize the provision of such services or somehow mandatethat the providers of such services include these populations as a condition of any license to provide services tothe remainder of the population.in the past, mandates for rural electrification or universal telephone access took care of this problem. at thetime these services were originally provided, it was decided that such universal access was in the public good.the idea of universal telephone service has been a foundation of the information policy of the united statessince the communication act of 1934 was passed, creating the federal communications commissionfor the purpose of regulating interstate and foreign commerce in communications by wire and radio so as to makeavailable, so far as possible, to all the people of the united states a rapid, efficient, nationwide, and worldwide wireand radio communications service with adequate facilities at reasonable charges, for the purpose of nationaldefense, for the purpose of promoting safety of life and property through the use of wire and radio communications,and for the purpose of securing a more effective execution of this policy by centralizing authority.the current telephone penetration rate for u.s. households is approximately 94 percent.3 in addition,approximately 1.3 percent have a phone available or nearby. roughly 4.5 percent of americans (4.4 millionhouseholds and 11.6 million individuals) have no telephone available.4with respect to atrisk populations, the elderly actually do better than young parents with children withrespect to telephone access. access to telephone service for retired persons at all income levels is at the nationalaverage or better. among the elderly, only those persons receiving supplemental security income have a lowerthan average penetration of telephones (79.7 percent to 84.9 percent).5 the disparity in access to telephoneservice is most pronounced for people of all ages with low incomes. it is noted that 31 percent of all familiesreceiving food stamps have no telephone.6 for families completely dependent on public assistance thepercentage rises to 43.5.only 2.2 percent of homeowners have no telephone, but 21.7 percent of persons in public housing have notelephone and 40.2 percent of those living in hotel rooms or boarding houses have no telephone. women andchildren are particularly vulnerable. households headed by women with children who are living at or below thepoverty line have a telephone penetration rate of only 50 percent. race and ethnic background appear toconfound the impact of income on telephone access. the percentage of white households without telephones is 8to 10 percentage points lower than black and hispanic households.persons living in rural communities outside metropolitan areas lack telephones in 9.9 percent of cases. inrural new mexico only 88 percent of all households have phone service. in the four corners areas of colorado,new mexico, arizona, and utah, where 36,000 navajo households are scattered about on 27,000 square miles,21,300 households (63 percent) have no telephones. in that region installation fees can range as high as $5,000for a single telephone.7it therefore appears that factors that affect telephone penetration are low income, household headed by awoman with children, race and ethnicity, and rural location. yet these are the very groups that would appear tobenefit from the recent advances in telecommunications.analysistoday the public mandate for universal access is being removed or bypassed, and in some rural areas,universal access to telephone or electrical utilities does not in effect exist. to obtain these services the user mustthe argument for universal access to the health care information infrastructure: theparticular needs of rural areas, the poor, and the underserved211the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.pay for linkages to trunk lines that may be at a considerable distance from the home. the cost of such a link canbe expensive. consequently, many rural areas do not have cable or cellular telephone services. the equipmentfor new highbandwidth connections necessary to provide stateoftheart nii services may be prohibitivelyexpensive for many rural and/or innercity users.however, these are the groups most likely to benefit from such services. the improvement in theireducation and health care will in turn be of great benefit to society in general. once nii access is provided, theprovision of such educational and health care services to these groups will represent only a modest additionalmarginal expense. it therefore is in the interest of the government and the general population that the nii is madeavailable to these populations and further that it is provided as a governmental service or as a governmentmandate to the providers of such services.the current method for providing near universal access to telephone services relies on funding transfersamong carriers, transfers from other service providers, transfers between customers, and governmentcontributions. the primary transfers have been from business and toll service users to basic residentialsubscribers and from urban to rural exchanges. before the at&t divestiture, these transfers were accomplishedlargely within a single corporate umbrella. since then the systems has become more complex, with a system ofrules governing transfers among carriers based on political compromise rather than economic logic. currentlythere are three major fund transfer mechanisms: (1) funds transferred from toll services to local access providers,(2) transfer from lowcost local exchanges to highcost local exchanges (i.e., from urban to rural exchanges), and(3) differential allocation of local service costs through rates charged to different customer classes (i.e., higherrates for business than for residential users). there is also a system of subsidization for social support servicessuch as lifeline programs for low income customers, 911 emergency services, and services for the hearingimpaired, all of which are covered through local telephone charges.8with the arrival of increased competition and the exponential growth of providers of equipment, tollservices, business services, etc., the rules are being altered. the difference between core services and access linesis blurring. as fiber and other broadband media continue to move into local neighborhoods and business districtsand as the already existing cable networks begin to offer switched services, the points of interconnection amongthese paradigms are changing. the growth of alternate access providers has made it possible for large tollcustomers to directly connect to toll service providers without paying the usagebased withinsystem transfersthat were traditionally collected through local access carriers. as local exchange competition develops, it will bemore difficult for local exchange carriers to support highcost areas or provide the default capacity relied on forsystem backup by competitive network providers. as cable television networks and wireless personalcommunication networks develop, they will make it more difficult for local access providers to continue toprovide social services below cost. they will also threaten the ability of local exchange carriers to provideservice to highcost customers (inner city, rural, poor, etc.).9the recent advances in communication technologies have raised the concern as to what is a reasonable basictelecommunications service. is it plain old telephone service (pots), fiber optics, microwave, satellite uplinks,etc.? is there a single basic service modality? some newer communications modalities such as cellular telephoneaccess may be less expensive to provide to rural areas than are telephone lines, but the equipment to receivethese services may be more expensive (i.e., cellular telephones). will the telephone companies be required toprovide only access, or access and equipment? what will be the minimum bandwidth that these companies willbe required to supply?if one agrees that there should be universal telecommunications access to the nii, then one must next decidethe minimum bandwidth that should be allocated. will it be for audio communication only, as previouslydefined, or must it now include video and computer communication capabilities? if it includes video, will it besingle frame or action capabilities? will it be five frames/s or the full 30 frames/s necessary for true actionvideo? will it be only black and white, or should it include color? for computer communication capabilitieswhat will be the necessary speed? this is a particularly difficult challenge because we must select a bandwidththat is broad enough to supply a range of services (audio, freezeframe video, action video, data interchange,etc.) yet economically realistic.in deciding the minimum level of services to be provided we must also realize that the method ofconnecting to the nii will not be uniform. some areas already have an extensive communications infrastructurethe argument for universal access to the health care information infrastructure: theparticular needs of rural areas, the poor, and the underserved212the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in place, and therefore it may be more economical to upgrade that existing infrastructure than to replace itwholesale. in other areas where no such infrastructure exists, consideration might be given to providing a newertechnology. in some areas wireless communication may be more reasonable, while in others a cable system willbe appropriate. after dealing with a relatively stable technology (twisted pair telephone line) for many decades,we are now faced with a rapidly changing panoply of technologies (digital telephone lines, fiber optic cables,microwave towers, satellite linkages, etc.) with variable communication bandwidth. in addition, the bandwidth ofeach modality is changing rapidly. existing twisted pair analog telephone linkages can be upgraded to digitallines (isdn), thus increasing their bandwidth. using compressed video techniques, we can send video imagesover traditional telephone lines today in a manner not possible only six months ago.it would appear that the minimal service configuration should be determined not by the physical type oflinkage but rather by its bandwidth capability (i.e., its ability to attain a level of service rather than the physicalconfiguration of that service).another major consideration must be the connecting focal point of that communication. traditionally thecommunication linkage was to a physical location (home or office). universal communication meant that allamericans had access to a telephone in their living space. this too is changing. we are now able to providecellular communication linkages to individuals rather than to physical structures. shortly we will be able toprovide a seamless linkage across the entire nation so that one can move from cellular cell to cell withoutneeding to register in the new area. the minimum standard for telecommunication access may involve servicenot to a physical household or building but rather to an individual.in the health care field one could argue that fully interactive video capabilities are necessary if we are to beable to provide remote delivery access. certainly, to provide remote consultation such video capabilities arenecessary. however, if one is to provide remote teleradiology services, a higher bandwidth is necessary; tosupply only patient information access and preventive services information, then perhaps a lower bandwidthwould be appropriate.finally, if we are to provide universal service then we must define not only the bandwidth of that servicebut also its capabilities. in the past, universal access included access to the physical telephone. does universalaccess now involve access to video equipment, computers, highresolution screens, etc.? we now can usestethoscopes, otoscopes, and ophthalmoscopes at remote sites via telemedicine facilities. will we include aremote stethoscope as the minimum telecommunications configuration in every household where a person withhypertension resides? in the end it would save lives and might be cost effective.the major problem remains, who is to maintain the nii? in the past these costs were either mandated to thecarriers, who added them to the general cost of the service, or were explicitly included as an item on the bills ofusers of the system. they were essentially mandated "unfunded" government benefits, which are now unpopularand insupportable. we must therefore determine another way to support these services. this will becomeparticularly important as access to the nii becomes a "portable" access that is not tied to a physical location.given the increasingly "portable" nature of this service, it will become more difficult to add the cost to abasic local access service charge. the concept of the local access company may gradually disappear as wirelesssystems spread. the only reasonable way to finance such a system is either via a tax on all telecommunicationsservices or by government funding of the basic access service.there are many ways the government can sustain the idea of universal access. one would be to mandatethat the local access provider in a particular area provide service to all households and businesses or individualsliving in that area at the same basic rate. this would continue the practice of cross subsidization within a localarea and might make some local provider areas unprofitable for any carriers. one could set up a bidding systemfor local access provider areas, with the highest bidder getting the franchise for the area. these funds would thenbe available to subsidize services in local access areas that were not cost effective and did not attract any bidders.this would also result in crosssubsidization since companies successfully bidding for the more soughtafterfranchises would pay a higher fee and therefore find it necessary to charge higher tariffs to their customers. themajor problem with this proposal is that the whole idea of a local access area is changing. the physical accesslocation may be an individual. would the area then become all the persons living in that area, working in thearea, and visiting the area? wireless communication negates the importance of the local access area. in addition,the argument for universal access to the health care information infrastructure: theparticular needs of rural areas, the poor, and the underserved213the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.microwave and satellite linkages make it possible for commercial access providers to bypass the local accessprovider. such a mandate might be unenforceable.a second method would be to follow that utilized in japan, where the government supports localtelecommunications services to all households. with this method the government could decide a basic level ofservices to all households or individuals and determine the most cost efficient method for providing it, be ittelephone line, microwave tower, cellular telephone, etc. the government would then contract with commercialproviders to provide these services. such contracts could be "bid" to the lowestcost provider who would agree tosupply all households or individuals in an area for a set fee.10 such an undertaking would be very expensive andwould have to be funded from tax revenue. this would, once again, result in a major cross subsidization fromhigher taxed groups to other groups. the chances of legislating a tax to pay for the free provision of services thatmost of us already pay for would be remote.a third approach would be to use valueadded service surcharges. these would be calculated as a fixedpercentage of gross revenues that potentially could be collected from all businesses selling valueaddedtelecommunication services. valueadded services include the services of all service providers interconnectingwith the public switched network, except local loop services provided to homes, businesses, or eventuallyindividuals by statecertified common carriers with provideroflastresort obligations.11 the valueadded taxwould be the easiest to monitor. funds from this tax could be used to subsidize basic services to all household orindividuals.the government has the most to gain in promoting universal telecommunications services. if, as expected, itimproves our educational system, provides continuing medical education services, enhances health care, andbrings these services to the poor, underprivileged, and rural populations, then the government (which currentlysupplies many expensive remedial series to these groups) would benefit the most. a better educated, healthier,and better informed electorate would greatly benefit the federal government. these benefits would be to thecollective good and might not necessarily accrue to the companies supplying these telecommunication services.the government must therefore be the supplier of last resort.recommendationsbasic levels of servicethe level of service must include a bandwidth specification based on type of service. we would argue thattwoway, realtime video communication is the minimum level of service that should be accepted. currenttwisted pair telephone communication will not realize this level of service, but combined digital circuits, fiberoptic cables, and microwave and satellite modalities certainly can achieve this standard. there need be no singlemodality used for the nii. depending on the area and/or population, a combination of these modalities might beappropriate.physical access unitwe believe that the access unit must gradually change from the household to the individual. we believemobile communication will continue to grow to the point that we will have individual telephone numbers or ipnumbers of telecommunication access numbers. an actual geographic access location will cease to be important.access modalitywe believe that there will be a gradual movement from the telephone as the standard of access to the niiinto a broader audio and video interface. as a part of universal access, each home, business, and individual willhave both audio and video capabilities as well as some minimal computer component. it is still too early toclearly define the actual instrument that will provide this interface, but it will surely be more complex than thethe argument for universal access to the health care information infrastructure: theparticular needs of rural areas, the poor, and the underserved214the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.telephone. we must, however, begin to define the basic capabilities of this unit because it will dictate the level ofservice available.method of funding universal accesswe favor a valueadded tax on the gross revenues of all providers of telecommunications services. thiswould include not only competitive access providers but also groups that use the nii to provide data,entertainment, news, financial information, etc. such a tax is the easiest to calculate and enforce. it should raiseenough money to fund the basic telecommunications infrastructure. the government would fund the entire niiwith funds from this tax. once the bandwidth and physical access unit were agreed upon, the government wouldsolicit bids for the supplying of services to a particular area. the suppliers would agree to supply services to allindividuals or households in the designated area. as the physical access unit gradually changed from thehousehold or business to the individual, the actual geographic area might be replaced by population groupings.we believe that the government must provide an nii, that it cannot be achieved through a nonregulatedenvironment. the benefits are potentially too great to allow any segment of the population to be "displaced"because of limited commercial costbenefit analysis. we must consider the total benefit of universal access, andthis can be done only with government intervention. continued government involvement can result inefficiencies of scale, uniform standards, and universal access.referencesbeamon, clarice. 1990. "telecommunications: a vital link for rural business," opastco roundtable.belinfante, a. 1991. "monitoring report: telephone penetration and household family characteristics," no. cc, docket no. 80286.federal communications commission, washington, d.c.bumble, w.a., and g.j. sidak. 1993. toward competition in local telephone markets. mit press, cambridge, mass.dordick, h.s. 1990. "the origins of universal service," telecommunication policy 14(3):223œ38.dordick, h.s., and m.d. fife. 1991. "universal service in postdivertiture usa," telecommunications policy 15(2):119œ28.gallottini, giovanna t. 1991. "infrastructure: the rural difference," telecommunications engineering and management 95(1):48œ50.hudson, heather e. 1984. when telephones reach the village: the role of telecommunications in rural development. ablex, norwood,new jersey.mueller, m.l. 1993. "universal telephone service in telephone history: a reconstruction," telecommunications policy 17(july):352œ69.notes1. see linder, a. 1994 "global telemedicine and the future of medical science," healthcare informatics, november, pp. 63œ66; andmcgee, r., and e.g. tangalos. 1994. "delivery of health care to the underserved: potential contributions of telecommunicationstechnology," mayo clinic proceedings, vol. 69, pp. 1131œ1136.2. linder, a. 1994. "global telemedicine and the future of medical science," healthcare informatics november, pp. 62œ66.3. schement, j.r. 1994. "beyond universal service: characteristics of americans without telephones, 1980œ1993," communications policyworking paper #1, benton foundation, washington, d.c.4. belinfante, a. 1989. "telephone penetration and household family characteristics," no. cc, docket no. 87339. federalcommunications commission, washington, d.c.5. schement, j.r. 1994. "beyond universal service: characteristics of americans without telephones, 1980œ1993," communications policyworking paper #1, benton foundation.6. belinfante, a. 1989. "telephone penetration and household family characteristics," no. cc, docket no. 87339. federalcommunications commission, washington, d.c.the argument for universal access to the health care information infrastructure: theparticular needs of rural areas, the poor, and the underserved215the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.7. ''how to flourish in an all digital world," wilt letter, vol. 1, no. 4, december 21, 1993.8. hudson, heather e. 1994. "universal service: the rural challenge changing requirements and policy options," working paper #2,benton foundation, washington, d.c.9. egan, b.l., and s. wildman. 1994. "funding the public telecommunications infrastructure," working paper #5, benton foundation,washington, d.c.10. egan, b.l., and s. wildman. 1994. "funding the public telecommunications infrastructure," working paper #5, benton foundation,washington, d.c.11. egan, b.l., and s. wildman. 1994. "funding the public telecommunications infrastructure," working paper #5, benton foundation,washington, d.c.the argument for universal access to the health care information infrastructure: theparticular needs of rural areas, the poor, and the underserved216the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.27toward a national data network: architectural issues andthe role of governmentdavid a. garbinmitre corporationthe last two decades have seen several revolutions occur in the telecommunications field, encompassingboth the underlying technologies used to construct networks and the services offered to customers over thesenetworks. in this paper, we follow two threads on their converging paths: the emergence and evolution of packetswitching as the dominant technology for data communications and the central management of computercontrolled switches as a mechanism to create virtual private networks (vpns) out of a national infrastructure. by1990, the second thread culminated in the dominance of vpns for the voice communications requirements oflarge customers. now, both threads can combine to create a similar phenomenon for data communicationsrequirements.these events are playing out against a background of explosive growth in requirements for datacommunications. growing public interest in the internet and the availability of userfriendly access tools iscausing a doubling of internet traffic every 12 months. federal government programs focusing on service to thecitizen and more efficient operations within government are driving federal agency requirements higher andhigher. finally, there is a national initiative to bring the benefits of reliable, inexpensive data communications topublic institutions as a whole through the creation of a national information infrastructure (nii). the federalgovernment role in bringing the nii into being is unclear at the present time; current proposals call for theprivate sector to play the major role in actually building infrastructure. however, it has been postulated that thefederal government could use its vast purchasing power to facilitate the development of an open data network, akey building block of the nii.1it is well known that telecommunications networks exhibit economyofscale effects; unit costs decrease asabsolute volume increases. this paper explores the economics of singleagency networks, governmentwidenetworks, and networks with the span of the proposed nii. specifically, it explores the benefits of a governmentwide network based on shared public switches and transmission facilities. such a network would yield the bestunit prices for the government and create a solid infrastructure base for the nii at no additional cost.basic conceptsbefore we begin, a basic review of the key concepts underlying the issues explored in this paper iswarranted to avoid any confusion over terms and definitions. these concepts are best discussed in terms ofcontrasts: voice versus data communications, circuit versus packet switching, and shared versus dedicated networks.in a simple sense, data communications are between computers, and voice communications are betweenpeople. however, that basic fact results in different characteristics for data traffic as opposed to voice traffic.toward a national data network: architectural issues and the role of government217the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.data traffic is "bursty" in nature; it occurs in periods of intense activity followed by (sometimes long)periods of silence. if delivery of the data cannot be immediate, they may be stored in the network and deliveredat a later time (how much later depends on the application). data traffic is not tolerant of errors; any error passedto an application will cause problematic behavior of some kind. the above characteristics lead to the requirementfor protocols to package the data, transmit the package through intermediate points, check for and correct errors,and deliver the package to the distant end. by contrast, voice communications are continuous in nature and havea realtime delivery requirement. a varying delay for a voice signal results in totally incomprehensible speech atthe receiving end. however, voice signals are robust and tolerant of errors. speech itself is so redundant thatwords remain comprehensible even at error rates of 1 in 100 in the digital bit stream. it is clearly more importantfor voice applications to deliver the signal on time than to deliver it with 100 percent accuracy.these different requirements have led to different switching techniques for voice and data communications.circuit switching sets up a path with a guaranteed bandwidth from end to end for each voice call. while thiswould also work for data communications, it would be inefficient since the reserved bandwidth would be unusedmost of the time. a technique known generically as packet switching was developed to effectively sharetransmission resources among many data communications users. when we refer to data networks in this paper,we are talking about networks employing some form of packet switching.the final concept we need to explore is the concept of shared versus dedicated networks. in the early daysof telephony, all customers used (shared) the public switched network for voice communications. the backboneof the public switched network was composed of large circuit switches located on the telephone companies'premises. by the 1960s, manufacturers began producing economical switches designed for use on customers'premises to lower costs for large users. while these were primarily for local service, it was soon discovered thatlarge customers could connect these premises switches with private lines and create private networks for longdistance voice communications. public switched service rates at the time were high enough that these privatenetworks were economical for large corporations (and the federal government).backgroundas packet switched data networks came into being in the 1970s, the private network alternative was theonly one available to customers. there was no public packet switched data network, nor was there a largedemand for one. private data networks grew alongside the private voice networks, with packet switches oncustomer premises and private lines connecting the switches. computer processing technology limited thecapacity of packet switches to that required by a single large customer; little penalty was paid in not locatingswitches on carrier premises where they could be shared by many customers.in the 1980s, two forces converged to spell the end of the private voice network. divestiture created a verycompetitive interexchange market, and computercontrolled switch technology evolved to the point where thepartitioning of a large network in software became feasible. in this case, the large network was the publicswitched network of each interexchange carrier that was serving the general population. over time, competitiondrove the unit prices being offered to a wide range of customers down to levels consistent with the large totalvolume. volume ceased to be a discriminator for price beyond a level of onetenth of the federal governmentwide traffic. this service, which now dominates the voice communications market, is called virtual privatenetwork (vpn) service.the current approach to data communications for large customers is still the private network. there are twoshortcomings to this approach: economies of scale beyond a single user are never obtained, and the proliferationof switches on user premises does not further the development of a national infrastructure. technology such asasynchronous transfer mode (atm) switches and highcapacity routers is emerging that makes carrierpremisesswitching feasible. at the same time, initiatives in government and in the research and education communitiesare generating a large future demand for data communications. the consolidated demand of the federalgovernment could create an infrastructure that pushes unit costs well up on the economyofscale curve.however, this will only be the case if a shared network is used to satisfy these requirements. we call thisnetwork, based on standard interfaces and protocols, the national data network (ndn). this paper presents thetoward a national data network: architectural issues and the role of government218the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.architecture and economics of such a network as it is used to satisfy the requirements of a single federal agency,the federal government as a whole, and the general population as the transport element of the nii. theimplications of this architecture for universal access to the information highway will be apparent.assumptionsany study of telecommunications alternatives must make assumptions about the technological andregulatory environment that will be present during the projected time period. in this case, the time horizon isfrom the present over the next 5 years. we assume that router and atm switch technology that is now emergingwill be ready for widespread deployment in operational networks during this period. although the regulatoryrequirements that mandate the current restrictions relating to local exchange carriers (lecs), interexchangecarriers (ixcs), and local access and transport areas (latas) are likely to be modified by the end of the period,the current structure is assumed for this study. even if latas as a formal entity were removed, they wouldcontinue to be useful to represent statistical areas for traffic purposes.all switching will be performed at lec and ixc locations, beginning at the lec central office, or wirecenter, serving the customer. access to the wire center could be through a dedicated local loop or through ashared channel on an integrated services digital network (isdn) local loop. local loop costs between thecustomer's location and the wire center are not part of this study. routers at wire centers or at tandem officeswithin a lata will be provided by the lec. lata and regional switches that are used for interlatatransport will be provided by the ixc. note that, in this scenario, both lecs and ixcs must implement sharedsubnetworks and cooperate in achieving the level of sharing needed for endtoend economies to be realized. inmost cases, this is not the case in today's networks. each carrier generally uses the other only for dedicatedcircuits between its switches. the realization of an endtoend shared network architecture is critical for theformation of a national data network.a final assumption that does not affect the economics of this study, but that is essential to the viability of ashared network, is the successful addressing of security issues. most present networks use encryption only oninterswitch links; the traffic in the switches is in the clear and is protected by physical security. since this wouldbe an issue if the switch were on carrier premises, more complex security systems that encrypt traffic at thesource before the network protocols are added are required to maintain security. fortunately, such systems arebeing designed and deployed since they also provide a much higher level of security than the present system.study methodologythree physical network models were formulated and evaluated for costeffectiveness over a range of trafficlevels. these models were crafted to represent the generic data communications requirements of a single federalagency, the federal government as a whole, and the general public as a user of the nii. the models differedprimarily in the number and distribution of wire centers served by the network. the agency model served 1,350wire centers; the distribution of wire centers served was derived from projected treasury locations during thetime frame of the study. the government model expanded the coverage to 4,400 wire centers. thesecorresponded to the wire centers currently served by fts2000. for the nii model, all the wire centers in thecontinental united states (21,300) were served.a network was designed for each model and traffic level. transmission and switching capacity were sizedto meet the throughput requirements for the traffic. each network was then priced using monthly prices fortransmission and amortized prices for switching equipment. a percentage factor was applied for networkmanagement based on experience with agency networks. the total costs and unit costs for each model and trafficlevel were then computed and analyzed. the following sections provide additional details on the networkarchitecture and the traffic and cost models used in the analysis.toward a national data network: architectural issues and the role of government219the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.network architecturea fourlevel hierarchy was used for the network architecture; the locations of the switches were based onthe current lec and ixc structure that exists today. the equipment used for switching was based on the currenttechnology of highend routers at the lower levels of the hierarchy and atm switches at the backbone level. asatm technology evolves, the current routers will likely be replaced by atm edge switches. for the purposes ofthis study, the capacity and cost factors of these two technologies would be similar.at the top of the hierarchy is a backbone consisting of fourteen switches and connecting circuits. thecountry was divided into seven regions corresponding to the current regional bell operating company (rboc)boundaries. two switches were placed in each region for redundancy. the topology of the interconnectingcircuits was based on the interswitch traffic, with the requirement that each switch be connected through at leasttwo paths. the backbone subnetwork is shown in figure 1.within each region, one switch is placed in each lata in the region. this switch serves as the "point ofpresence" in the lata for the backbone network. each lata switch is connected to one of the regionalbackbone switches. intralata traffic is switched internally at this level and does not reach the backbone.figure 2 illustrates this connectivity for the northeast region, showing the two backbone switches and fourteenlata switches.within each lata, traffic from the wire centers is concentrated at tandem routers before being sent to thelata switch. these tandem routers are located at lec central offices, usually those serving a large number ofexchanges. the number of tandem locations is somewhat dependent on the model in use, but a typicalconfiguration is shown in figure 3 for the maine lata (seven tandem switches are shown).figure 1 national data network backbone.the final level in the hierarchy consists of routers in the lec wire centers serving actual users. access tothese routers occurs over the customer's local loop and can take various forms: dedicated local loop connected directly to data terminating equipment, isdn local loop using d or b channel packet mode access, and analog local loop used in dialup mode.toward a national data network: architectural issues and the role of government220the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2 ndn lata subnetwork (northeast region).figure 3 ndn tandem subnetwork (maine lata).as stated above, the number of wire centers served is dependent on the model being evaluated.traffic modela single, scalable traffic model was used to evaluate all three physical network models (agency model,government model, nii model). in the future, data applications will encompass all facets of governmentoperation, not only data center operations. consequently, the current latatolata voice traffic distributionof the government as a whole was used as the basis for the traffic model. this reflects the level of governmentpresence and communities of interest within the country. the resulting traffic matrix represents a genericapproach to characterizing a national traffic distribution.toward a national data network: architectural issues and the role of government221the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.within a lata, the traffic was assigned to wire centers based on the number of locations served (or, in thenii model, the number of exchanges served). the base unit used to characterize traffic was terabytes per day (1terabyte = 1 million megabytes). as a calibration point and a way to put the traffic units into perspective, table 1gives the approximate traffic volumes for existing and proposed networks.table 1 traffic volumes for typical networksnetworkvolume (terabytes/day)current fts2000 packet switched service0.0075treasury tcs (projected)0.1œ0.4march 1994 nsf backbone0.6all fts2000 agencies1.4œ5.6dod operational networks&223c;6.0nii>20.0the current fts2000 packet switched service (pss) carries only a small percentage of the civilian agencydata communications traffic; most of the traffic is carried over private networks using circuits procured underfts2000 dedicated transmission service (dts). the department of the treasury is procuring such a privatenetwork at this time and has estimated its traffic requirements over the life of the new contract. analysis ofcurrent dts bandwidth utilization by agency indicates that treasury represents about 7 percent of the totalfts2000 agency requirement for bandwidth. this includes department of defense (dod) circuits on fts2000but does not include the large number of dod mission critical packet switched and ip router networks. thevolume of traffic on these networks may be as large as the fts2000 agency estimate.as a point of comparison, the march 1994 internet traffic traversing the nsfnet backbone is given.2 thistraffic doubled in the past year and shows every indication of increasing growth rate. of particular interest wouldbe the amount of regional internet traffic that could use the infrastructure generated by the ndn for moreeconomical access and transport service.the traffic volume generated by a mature nii cannot be estimated; truly, the sky is the limit if any of thefuture applications being contemplated grabs the imagination of the general public.this study used volume ranges appropriate to the physical model under consideration. the agency modelwas evaluated at volumes ranging from 0.006 to 0.5 terabytes per day. the government was modeled asrepresenting 14 typical agencies and was evaluated at volumes ranging from 2 to 8 terabytes per day. note thatalthough the government model had 14 times the traffic of the typical agency, it utilized only 3.3 times as manywire centers. the nii model extended the reach of the network to 5 times as many wire centers; it was modeledas carrying the traffic of 8 governmentsize networks (16 to 64 terabytes per day).circuit cost modelthe cost of circuits used in this study was based on the current, maximally discounted tariffs for interofficechannels (i.e., channels between carrier premises). local channels were not used since all switches in the studywere located on carrier premises. rates for oc3 and oc12 were projected as mature rates following the currenteconomyofscale trends. carrier projections for these rates support this view. the five line speeds used forcircuits were as follows: 64 kbps, 1.5 mbps (t1), 45 mbps (t3), 155 mbps (oc3), and 620 mbps (oc12).toward a national data network: architectural issues and the role of government222the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 4 circuit cost model example.figure 4 shows the monthly cost of a 500mile circuit at different line speeds, illustrating the economies ofthe higherspeed circuits.equipment cost modelthe wire center, tandem switch, and lata switch cost models were based on highend router technology(e.g., cisco 7000). serial interface processors were used for link terminations. atm interface processors wereassumed for t3 links and above. atm concentrator switches in the future should exhibit cost behavior similarto that of the router configurations. the backbone switch cost model was based on highend, widearea atmswitch technology (e.g., at&t gcn2000).the onetime cost of equipment was amortized over a 5year period to obtain an equivalent monthly costthat could be added to the monthly transmission costs. before amortization, the capital cost of the equipment wasincreased by 20 percent to account for installation costs. finally, a monthly cost of maintenance was added at arate of 9 percent of the capital cost per year. these factors correspond to standard industry experience for thesefunctions.management cost modelnetwork management costs were estimated at 25 percent of the equipment and transmission costs, based oncurrent experience with agency networks. implementing management cost estimates as a percentage assumesthat network management will show the same economyofscale effects as the other cost elements. in fact, largenetworks will probably realize greater economies in network management than in any other area. these costs aredriven mostly by personnel costs, which are relatively insensitive to traffic volume and only marginally relatedto number of locations.resultsthe results of the analysis are presented here in two formats. the first graph for each physical model showsthe variation of monthly cost with volume. the second shows the variation of unit cost with volume. unit costsare presented in cents/kilosegment (1 kilosegment = 64,000 characters).toward a national data network: architectural issues and the role of government223the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.agency modelfigure 5a shows the variation of monthly cost versus volume for the agency traffic model. the curvedemonstrates the classic economyofscale shape, although the effect is more discernible in the unit cost curvepresented in figure 5b. the unit costs predicted by the model at the lowest traffic levels are consistent with thecurrent unit costs for fts2000 packet switched service, which is operating at these traffic levels. it can readilybe seen that large single agencies can achieve economies with private networks at their current volumes of 0.1terabytes (tb) per day.government modelthe monthly and unit cost curves for the government model are presented in figures 6a and 6b. thecombined costs of multiple singleagency networks comprising the same volumes are also shown. significantcost savings are achievable with a governmentwide network versus the multipleagency networks. unit costs atgovernmentwide volumes (2 to 8 tb/day) are approximately onethird the unit costs realized using the volumesof even the largest single agencies (0.1 to 0.4 tb/day). the economies achievable for the smaller agencies wouldbe much greater. a portion of the reason for the substantial economies realized is the more efficient use offacilities from the local wire centers up to the lata switches. with multipleagency networks, a large numberof inefficient, lowspeed circuits exist in parallel at the same local wire center. with a shared network, the trafficon these circuits can be bundled over more efficient, higherspeed circuits. the situation is made worse if themultiple agency networks use switches on customer premises rather than in wire centers.nii modelthe relative economies in moving from governmentsize networks to networks on the scale of the nii showa similar pattern (figures 7a and 7b). the savings are not as great as in the previous example since extra costs areinvolved in extending the reach of the nii into all wire centers. nevertheless, if the traffic increase is assumed tobe on a par with the increased coverage (8 times the traffic with 5 times the wire centers covered), then theeconomies are still there and the enormous benefits of full coverage are realized.the single nii network produces a 37 percent unit cost savings over the 8 multiple networks comprising thesame volumes. note that large increases in traffic from the wire centers already serving federal governmenttraffic could be handled at little additional cost. this would be the case in most urban and suburban areas.investment coststhe cost figures presented above represent the costs as equivalent monthly costs, including the amortizedcost of equipment and installation. it is instructive to break the equipment and installation costs out separatelysince these costs represent capital investment. in particular, table 2 presents the additional investment requiredto carry increased traffic, given that a governmentwide network carrying 4 tb per day of traffic has alreadybeen constructed. the investment in equipment required to build a network of that size is approximately $160million. while substantial, this investment is commensurate with the estimated investment made to providefts2000 services to the federal government in 1988. these investment costs would be recovered through servicecharges to the government.toward a national data network: architectural issues and the role of government224the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 5a monthly costšagency model.figure 5b unit costšagency model.toward a national data network: architectural issues and the role of government225figure 6a monthly costšgovernment model.the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 6b unit costšgovernment model.figure 7a monthly costšnii model.figure 7b unit costšnii model.toward a national data network: architectural issues and the role of government226the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 2 additional investment needed to carry additional traffic (millions of dollars)+4 tb/day+12 tb/day+28 tb/day+60 tb/daygovernment model3467123199nii model250378518669as table 2 shows, additional traffic can be carried through the same wire centers that serve the governmentwith little additional investment. additional capital is needed to expand toward a fuller nii infrastructure thatwould require 5 times as many wire centers to be covered as are covered in the government network. however,the government network would still provide a significant jumping off point for the complete network. forexample, an nii network serving all wire centers at 8 tb/day would require an investment of $410 million inequipment ($160m for the first 4 tb/day through the government wire centers plus $250m for the additional 4tb/day through the remaining wire centers). the government network would have already caused 40 percent ofthat investment to be made.the largest portion of the investment and monthly costs is in the access areas of the network, the portionthat is normally provided by lecs. this reinforces the point made above in this paper that the shared networkconcept must be extended all the way to the user. it also points out the need for uniform standards for interfacesand switching in all regions (a minimum requirement for any open data network).conclusionsthree major conclusions can be drawn from the analysis presented above: the infrastructure costs of a national data network show a marked economyofscale effect at the volumesrepresented by the federal government data communications traffic. significant economyofscale benefits can be achieved by aggregating agency requirements onto a commonnetwork. the infrastructure created to support federal government requirements can significantly reduce the cost ofextending service to larger communities in the public interest.the savings resulting from the ndn approach are substantial enough to justify the complexities of anaggregated procurement (coordination of requirements, security, standards). such a procurement would have tobe carefully structured to harness the competitive forces necessary to motivate both local and interexchangecarriers to pass on the cost savings shown above through lower prices. the end result would be a quantum stepforward for the government and the country on the road to the information technology future.notes1. computer science and telecommunications board, national research council. 1994. realizing the information future: the internet andbeyond. national academy press, washington, d.c.2. computer science and telecommunications board, national research council. 1994. realizing the information future: the internet andbeyond. national academy press, washington, d.c.toward a national data network: architectural issues and the role of government227the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.28statement on national information infrastructure issuesoscar garcia for the ieee computer societystatement of the problema key challenge facing the information technology community is to integrate humancomputer interaction(hci) research with that of broadband communication in an economically effective manner serving the broadestpossible community. over the next 5 to 7 years, we expect this issue to become one of the central economic andscientific drivers of the entertainment industry, as well as a key influence on how science and education areconducted in technologically advanced nations. our organization is particularly concerned with the latter arena.we want to ensure that scientists, students, and other serious information seekers are able to exploit the newtechnology and, in the case of a large segment of our membership, also contribute to it. one of the manynontechnical barriers is the lack of a forum for dissemination of information about the national informationinfrastructure (nii), and the ieee computer society, with its technical groups and wideranging publications,can be a significant facilitator of such a forum. the deployment should include a strong leveraging of theeducational potential, given the more likely entertainment and business aspects. few universities, high schools,and elementary schools are prepared to profit from the nii deployment, and even fewer understand what changesin their educational modus operandi are likely to take place if they participate. these issues are not frequentlyincluded in public policy studies of technology because the responsible curricula designers and implementors areoften absent. one certain aspect is that usability via good interfaces must be taken seriously, given the broadspectrum of users.areas and examplesfive key areas of hci research activity are summarized below, together with related key developments,enabling technologies, and capabilities:1. humancomputer dialogue. natural language, voice, and other modalities (pen, gestures, haptic, etc.) canbe combined to produce dialogue techniques for hci. these techniques can be robust enough to tolerateacoustic noise, disinfluencies, and so on. relevant topics here include speech recognition and synthesis;natural language processing and understanding;1 lipreading technologies in noisy environments;2 andsearch, navigation,3 and retrieval from cyberspace using multimedia.4 speaker identification can becombined with secure passwords for use in control of computer access, and language identification maysupport improved multilingual interfaces.5 understanding of natural language and speech also concernssemantics and prosody (for speech, obviously) and probably the use of semantic nets and semanticencyclopedias now under construction. technologies of understanding beyond symbolic matching orstatistical techniques allow searches invoked by description of the searched object in a less restrictivespoken or written form. roe and wilpon6 present a thorough analysis of the potential uses of speechrecognition and synthesis, including its use in aiding physically disabled people. speech could play avery significant role if used in the new asynchronous transfer mode (atm) technologies, which aredesigned, precisely, as a compromise between voice communications and videostatement on national information infrastructure issues228the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.transmission. the use of multiuser dialogue is likely to extremely useful if there is usable softwareavailable, such as in the case of scientific research and engineering design.72. machine sensors for hci and general i/o to facilitate telepresence and teleoperation. current computersare ''sensory deprived," representing a barrier to both humancomputer interaction and machine learning.such sensors as microphones in single and array configurations, infrared and other means of scanningand computing distances, optical sensors at lightwave range including chargecoupled device (ccd)cameras of small size, haptic interfaces, and alternatives to clickandpoint devices should be studied.fusing sensor inputs to the computer with intelligent or learned actionresponse behavior would create amore realistic approach to machine learning and complex inferencing techniques, involving symbolic,fuzzy, and probabilistic approaches. this area has been researched with different objectives, but seldomwith that of trying to improve the humancomputer interface. standardizing environments (e.g., via ahumancomputer interaction workbench; hciwb) can improve measurements. such an experimentalenvironment is also useful in the study of human behavior in real and virtual modalities related to the nii,and provides comparisons in human subject variabilities between real and virtual environment behavior,navigation, and orientation. the potential for research in the fusion of the modalities is enormous.8 thechallenge of this research area is to fuse multiple sensor inputs to the computer in a cohesive and wellcoordinated manner. one such example would be the integration of a ccd camera input with a hapticexperiment using force feedback and synthesized video output. another helpful experiment could involvemechanisms for the localization of sound in virtual environments9 using the hciwb.3. large storage (archival and nonarchival), database, and indexing technologies, includingmultiresolution and compression for different modalities. video and audio technologies will require largecompression factors and mechanisms for rapid encoding and decoding and are difficult to index andaccess for retrieval, and even then, mass storage database techniques will be required. this area is alsoindirectly related to the speech and video synthesis technologies, since highresolution synthesisapproaches imply efficient encoding, possibly at different resolution levels. similarly, virtualenvironment research requires efficient storage and compression technologies for input and output. thereare good reasons to believe, for example, that highquality audio can be encoded at rates of 2,000 bpsusing dynamic adaptation of perceptual criteria in coding and articulatory modeling of the speech signal.therefore, encoding research should include both generation and perceptual factors.10 additionally,multimedia databases require techniques for providing temporal modeling and delivery capabilities. anovel interface, called "query scripts," between the client and the database system adds temporalpresentation modeling capabilities to queries. query scripts capture multimedia objects and theirtemporal relationships for presentation. thus, query scripts extend the database system's ability to selectand define a of set objects for retrieval and delivery by providing a priori knowledge about clientrequests. this information allows the database system to schedule optimal access plans for deliveringmultimedia content objects to clients. one more example of an area of concern related to the overallthroughput capability of the nii is the earth observing system (eos) of nasa. this system is coupledwith a data information system (dis) in a composite eosdis, which is expected, when operational in1998, to require transport of one terabyte per day of unprocessed data and possibly an order of magnitudemore when processed, roughly equivalent to the total daily transport capacity of the current internet. thequestion is, will the nii provide the capacity for even a fraction of such volumes of data?4. virtual environments and their use in networking and wireless communication (tethered and untethered)networked environments11 will have an impact on the nii. virtual environments relate to telepresence andtelecommuting, as well as to personal communication services for digital voice. the technologies fortelepresence and telecommuting involve a mixture of multimedia and networking. wirelesscommunication technology also includes techniques such as geopositioning measures, local indoorinfrared sensors for location, communications technologies at low, medium, and high bandwidth, and soon. the technical challenges of wireless messaging are well known.12 in particular, the proposed use ofatm lans will integrate virtual environment research at different sites with communication research.13the concept of virtual environments is taken here in a broad sense, including both headmounted andenclosed cavelike environments,14 telepresence, and their human factor considerations for the realtime and residual longterm psychological effects of immersion. strong encouragement for a researchemphasis on the humancomputer interface is provided by the national research council's committee onvirtual reality research and development, whose final report15 makes specificstatement on national information infrastructure issues229the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.recommendations for federal national research and development investments. virtual environments arealso likely to be used for education and experimentation over distances when there are sufficienteducational and psychological developmental technologies and related network communication. forexample, realistic virtual environments could be used for technology education in areas that involve highcost and risk, such as in welding training.165. applications of software engineering and case to the r&d of complex software systems and browsersto be used in hci.17 many modules of the software and interfaces for the different modalities might bedeveloped in a compact and reusable manner, taking advantage of existing and newly developed softwaretechniques. it has been found that 50 to 90 percent of all lines of code in industrial and military softwareare dedicated to humancomputer interfaces.18 in this sense, we include usability studies in the scope ofsoftware engineering measurements for interfaces.19 a special interest is anticipated in experimentationwith the facilitation of interactive multimedia educational software development, particularly related toscience and engineering topics. software financial investments in the nii applications would be affectedby their ability to be easily accessible to the broad community of nii users. software engineering for thenii is likely to have a flavor quite different from what has been done in the past at research institutessuch as the software engineering institute, strongly based on ada environments.interaction among technical and nontechnical (legal/regulatory,economic, and social) factorsthere are legal concerns with regard to the balance between security and freedom of communications. inparticular, a thorny issue to be discussed is the degree of responsibility, if any, that carriers have for transmittingillegal material or for the theft or penetration that may take place when security is breached. there are newsocially explosive issue (pornography, copyright issues, etc.) that need to be addressed in the context of networksand information systems. they are related to the financial viability of the humancomputer interaction on a largescale by big populations and have a tremendous impact on the publishing industry. a new type of "nii electronicforensics" needs to be established, and it must have a strong technical basis to stand legal scrutiny. this is anarea that only highly secret intelligence agencies have dealt with and that universities have incorporated onlysporadically in their research areas. it is a delicate area of concern for the public, since it is often related tosecurity and privacy.contingencies and uncertaintiesthe entertainment industry is most likely to dominate the field. it is most likely (but uncertain) that only afew educational institutions will be able to afford the expenditures associated with supplying educationalservices to their constituencies. it is not clear how the telephone and publishing industries will react and whattheir investments will be, but much of it will depend on intellectual property rights protection and the availabilityof sources of materials. how users will react to this can only be gleaned from some experiments such as the"electronic village" at virginia polytechnic institute's department of computer science. the digital librariesinitiative of nsf, arpa, and nasa needs to continue and be more widely coordinated in a national forumaccessible to all.usersclasses of users to be served include the following:1. the public. the public will have access to the media. in general the spectrum of "public users" has abroad range of sophistication. a distribution of knowhow would have a large number of naive users(mostly browsers and email users) and then a small number of highly sophisticated users. age is not afactor in thestatement on national information infrastructure issues230the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.experiential knowhow. a study of this population should provide a sociotechnical profile, which wouldbe useful in this study. these are people not associated with an educational, government, or industrialinstitution but rather "home users." they may be seeking information from educational, government, orindustrial sources but on an irregular basis.2. those associated with educational institutions, such as students, teachers, administrators, and librarians.3. those public servants who interface with the public and are in charge of dissemination of governmentinformation; also, individuals able to provide services to those citizens who must be licensed, tallied by acensus, taxed, certified for licenses and renewals, and so on. this population could be categorized intofederal, state, and local government public servants.4. industrial users. this category has a large subcategory of entertainment, and possibly "edutainment."these are the salespeople on the network or electronic commerce providers of sources, technology, andproducts and include, of course, videoondemand providers. there are many subcategories here.disadvantaged persons or those in geographical areas remote to broadband access will be the most difficultto serve, partly because of their technical access problems and partly because, in general, they will most likely beat the low end of user sophistication. they will also be those who are likely to benefit the most from havingaccess to resources that would otherwise be unreachable.market rampupthe market will have to provide "substance" or content. the cost of providing is high. how to providesubstantive content, create a cottage industry of providers, allow those potential providers the opportunity toaccess and sell in a free market, and draw lines of responsibility and legality are but some of the issues that willdetermine the speed of the rampup. interactivity is expensive, as is any twoway communication, but thebandwidth does not have to be symmetric in both channels. this is an area where technology could have animpact if we understand the humancomputer aspects of interactive "dialogue" in a broad sense. openness shouldmean possible accessibility to all the users who fall within the service potential of a provider on an equal basis,but should be restrictive, of course, on the basis of registration for cases where financial transactions are to takeplace. the determination of viable means to charge for services is a technoeconomic factor that is offundamental importance for early resolution and fast rampup. the scalability may also be viewed from the pointof view of the user's sophistication and needs. our "help" menus are insufficient and too slow to solve theproblems of specialized use for nonspecialized but proper users of the facilities. new approaches to diagnosis ofthe user's difficulty are a part of the "hci problem" and are required for fast progress by the public user and evenby the moderately sophisticated industrial or government user.references1. cole, r., o.n. garcia, et al. 1995. "the challenge of spoken language systems: research directions for the nineties," ieeetransactions on speech and audio processing, january.2. garcia, o.n., with a.j. goldschen and e. petajan. 1994. "continuous optical automatic speech recognition by lipreading," proceedings of the twentyeight annual asilomar conference on signals, systems, and computers, october 31œnovember 2, pacific grove,calif.3. shank, gary. 1993. "abductive multiloguing: the semiotic dynamics of navigating the net," electronic journal on virtual culture 1(1).4. vin, harrick m., et al. 1991. "hierarchical conferencing architectures for intergroup multimedia collaboration," proceedings of theacm conference on organizational computing systems, atlanta, ga., november.5. wilpon, j., l. rabiner, c.h. lee, and e. goldman, 1990. "automatic recognition of keywords in unconstrained speech using hiddenmarkov models," ieee transactions on acoustics, speech, and signal processing, assp38, november, pp. 1870œ1878.statement on national information infrastructure issues231the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.6. roe, d.v., and j. wilpon (eds.). 1994. voice communication between humans and machines. national academy press, washington, d.c.7. anupan, v., and c.l. bajaj. 1994. "shastra: multimedia collaborative design environment," ieee multimedia, summer, pp. 39œ49.8. koons, david b., c.j. saparrel, and k.r. thorisson. 1993. "integrating simultaneous inputs from speech, gaze, and hand gestures," inintelligent multimedia interfaces, m. mayberry (ed.). aaai press/mit press, cambridge, mass., chapter 11, pp. 257œ276.9. gilkey, r.h., and t.r. anderson. 1995. "the accuracy of absolute speech localization judgements for speech stimuli," submitted to thejournal for vestibular research.10. flanagan, j.l. 1994. "speech communication: an overview," in voice communication between humans and machines, d.v. roe and j.wilpon (eds.). national academy press, washington, d.c.11. kobb, b.z. 1993. "personal wireless," ieee spectrum, june, p. 25.12. rattay, k. 1994. "wireless messaging," at&t technical journal, may/june.13. vetter, r.j., and d.h.c. du. 1995. "issues and challenges in atm networks," communications of the acm, special issue dedicated toatm networks, february.14. defanti, t.a., c. cruzneira, and d. sandin, 1993. "surroundscreen projectionbased virtual reality: the design and implementationof cave," computer graphics proceedings, annual conference series, pp. 135œ142.15. durlach, n.i., and a.s. mavor. 1995. virtual reality: scientific and technological challenges. national academy press, washington,d.c.16. wu, chuansong. 1992. "microcomputerbased welder training simulator," computers in industry. elsevier science publishers, pp. 321œ325.17. andreesen, m. 1993. "ncsa mosaic technical summary," national center for supercomputing applications, software developmentgroup, university of illinois, urbana, ill.18. myers, brad a., and mary beth rosson. 1992. "survey on user interface programming," proceedings sigchi'92: human factors incomputing systems. monterey, calif., may 3œ7, p. 195.19. hix, d., and h.r. hartson. 1993. developing user interfaces: ensuring usability through product and process, wiley.statement on national information infrastructure issues232the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.29proposal for an evaluation of health care applications on theniijoseph gitlinjohns hopkins universitystatement of the problem and issuestelemedicine is being advocated as a process of delivering health care to all segments of the populationwith the potential of reducing the cost of care while maintaining its quality. however, little is known about theefficacy and costeffectiveness of the technology in routine diagnostic and therapeutic practice. a welldesignedevaluation based on stringent criteria is needed to determine the merits of telemedicine in utilizing the nationalinformation infrastructure (nii) in the environment of health care reform. within this context, some of the moreimportant concerns related to the realization of telemedicine are the following: the lack of "bandwidth on demand" to provide data rates when they are needed at affordable costs to thehealth care community; the lack of availability of highdensity, lowcost, digital storage and related software for efficient access byauthorized users; and the lack of standards and interfaces for both health care data acquisition and for the effective use of suchinformation. this is particularly applicable to integration with heterogeneous legacy systems used by a widevariety of health care providers.projections regarding the problem and related issuesassuming the telecommunication "tariff" issue under the jurisdiction of federal, state, and local authoritiescan be resolved, it is anticipated that the technical barriers to bandwidth on demand will be overcome in the next5 to 7 years. the efforts under way to develop a reliable storage and retrieval system that is suitable for medicalimages and other health care data should be realized before the year 2000. though improved interfaces andsystem integration techniques are expected to be available shortly, the accommodation of heterogeneous legacysystems may be delayed by economic and cultural factors for several years.status of key developmentsrecent developments in information technology and the recognition of the need for reform provide a uniqueopportunity for health care decisionmakers to capitalize on the availability of the nii. if, for example, medicalimaging advances are to be available to all patients regardless of situation or geography, the storage,transmission, and retrieval of large volumes of data must be accommodated in all areas of the country. also,access by secondary users to clinical information for teaching, research, and management (within appropriatesecurity and privacy restrictions) requires that the information be readily available to medical students, researchinvestigators, and health care policymakers.proposal for an evaluation of health care applications on the nii233the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.key developments in telecommunications essential to meeting the demand generated by health careactivities include the availability of highspeed communication networks in sparsely populated areas. theeconomic and cultural aspects of the telemedicine applications can then be identified, demonstrated, andevaluated. technology related to highdensity storage and retrieval of medical images and related patient data isimmature. much work remains to be done to achieve reliable and costeffective systems that will support patientcare, medical research and education, and health care administration. the proliferation of personal computersand the increase in computer literacy are major factors in user acceptance of telemedicine and related technology.interaction between technical and nontechnical factorsthe regulatory authority of federal, state, and local agencies to set tariffs has resulted in barriers to theeconomic reality of telemedicine. if the quality of health care is to be maintained in a costeffective mannerusing the nii, clinical data must be transmitted promptly within cost constraints.other legal and regulatory issues that must be addressed include the privacy of patients and health careproviders, and the security of data against unauthorized access. many questions need to be answered regardingthe "ownership" of medical information and the responsibility for retention of medical records. the differingstate medical licensure requirements must be rationalized to permit access, when needed, to specialists acrossstate boundaries, and malpractice regulations need to be modified to eliminate unnecessary medical proceduresthat are performed solely to reduce the threat of litigation. in the area of administration, the adoption of auniform claims data set would substantially reduce current processing activities related to reimbursement.though health care costs in the united states amount to approximately 15 percent of the gross domesticproduct, health care information requirements alone cannot support the development and deployment of the nii.however, health care is an important contributor to the information community and is one of many largeeconomic segments that must be included in the utilization of the nii. if the cost of the nii is shared among alarge number of major segments of the economy, the application of telecommunications will be facilitated.further advances in storage and retrieval technology are largely dependent on government agencies and sectorsof the economy other than health care. the special requirements of health care can then be met by modifying thebasic developments designed to meet other needs.the current trend of health care reform emphasizes the restructuring of the delivery system toward managedcare corporations. the driving force behind this restructuring is the recognized need for cost containment. today,the decisions to adopt new technology for use in health care are predominantly made by corporate managersrather than by individual practitioners. since health care is a labor intensive activity, in this new climatetechnology that increases efficiency is more favorably received.increased access to quality care by patients regardless of situation or geography is the primary justificationfor telemedicine and for health care reform. to some extent, this implies "patient acceptance" of the nii andrelated technology; however, health care provider acceptance is pivotal to adoption of telemedicine in practice.the acceptance of new technology requires many cultural and procedural changes by physicians, nurses, andallied health care workers. these changes have already occurred in health care financial activities such as billingand reimbursement and in medical research but are lagging in patient care delivery functions.contingencies and uncertaintiesthe investment in and deployment of new technologies applicable to health care are partly dependent on thesuccess of health care reform. "many of the political imperatives driving telemedicine derive from theanticipated use of managed care incentives to provide accessible lowcost health care to all americans."1 it isexpected that the information infrastructure will be deployed because of impetus by government agencies andindustries other than health care. the implementation of the nii for the delivery of health care is dependent onthe costeffectiveness of the technology as perceived by the decision makers within health care reform.proposal for an evaluation of health care applications on the nii234the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.other contingencies and uncertainties related to investment and deployment of the nii and its use in healthcare include the following: availability of appropriate network connections throughout the country; willingness by health care providers to share new technologies; resolution of security and privacy issues; costeffective solutions to integration of legacy systems; resolution of telecommunications tariff issues, particularly bandwidth on demand; adoption of a uniform claims data set; acceptance by physicians of "compressed" data, especially in medical imaging; and completion of the comprehensive computerbased patient record.key applications, enabling technologies, and capabilitiesseveral recent technological developments make it possible for health care to take advantage of many of thecapabilities offered by the nii. the following are among the more important developments available to healthcare providers: a range of workstations accommodates the spectrum of needs of health care providers, particularly the highresolution and luminance requirements of radiologists, as well as the needs of other specialists and primarycare physicians. several standards have been developed for health care data interchange. these include the digital imagingcommunication in medicine (dicom), hl7, and medix p1157. however, it is necessary to identify anddevelop other standards that will facilitate further use of the nii by health care providers. preliminary results of the largescale effort to develop a comprehensive computerbased patient record areavailable, and there is momentum to complete the task. various technical approaches have made "electronic signature" available. however, some legal questionsremain to be answered before broad acceptance can be achieved. recent advances in "speech recognition" technology are most important to health care provider input tomedical records. this is especially applicable to medical imaging, where interpretation of examinations isbasic to the specialty. the use of compression algorithms to decrease data volume has proved costeffective and reducestransmission time. however, there is concern about the loss of pertinent information when "destructive"compression is applied. this is especially true of medical images, where radiologists require all of theoriginal data for detecting subtle abnormalities.classes of usersin health care, there are several types of users to be considered when access to the nii is planned. amongthose users who are relatively easy to accommodate and who may adapt quickly to the new technology are thefollowing: computerliterate health care providers, researchers, educators, and students in academic settings; computerliterate health care providers and other personnel in managed care settings; and government and insurance agencies concerned with reimbursement.there are also a large number of potential health care users of the nii that will require substantial trainingand education, as well as appropriate hardware and software to be capable of using the infrastructure:proposal for an evaluation of health care applications on the nii235the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. older health care providers in small groups and solo practices, and health care providers in remote locations.if the technology proves to be cost effective, the increase in managed care organizations should expedite theuse of the nii for patient care, medical research and education, and health care administration. though theprimary factors relative to rampup expectations and determinants are outside the health care environment,government actions regarding medicare and medicaid will affect investments in technologies intended for healthcare markets.public and private sector responsessince health care is uniquely the purview of both the public and private sectors, such an application of niitechnology is affected by government and industry. acceptance by health care providers and relatedorganizations in terms of costeffectiveness and utility is affected by both political and economic considerations.recommendationsa comprehensive evaluation based on a realistic demonstration should be conducted to identify the factorsrelated to the utilization of the nii by health care providers and related organizations. the evaluation should bebased on stringent criteria that focus primarily on patient care issues such as quality and access, and that measureselected key parameters related to technology, economics, and legal/regulatory and social/cultural factors. theproject will require the participation of industry, academia, and government in cooperation with health careproviders to develop the evaluation criteria, design the study, and conduct the demonstration of costeffectivesystems that will support telemedicine, medical research and education, and health care administration. it isintended that this white paper will provoke serious consideration of health care applications on the nii.reference1. perednia, d.a., and a. allen. 1995. "telemedicine technology and clinical applications," journal of the american medical association273:483œ488.proposal for an evaluation of health care applications on the nii236the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.30the internetša model: thoughts on the fiveyear outlookross glatzerprodigy services (retired)we've heard a lot of statistics about the mushrooming growth of the "net." i believe it's much more than afad. what i see on my radar is a stunning social change as important as anything in this century. you can seesigns of it already. one example is in the area of religion. devout people are increasingly sharing their faith online. they holdreligious services, complete with sermons and music. there are online support groups for every need. by theturn of the century, we may have tithing online by way of credit cards. in politics, candidates now routinely debate issues online. citizens have been sending email to theirrepresentatives for a long time. will it be that many years before we have online voting? i think it could beavailable in a decade. the only barriers are social, not technological. consumer access to online medical information is growing rapidly. how long before you can buyprescriptions online? make medical appointments? even get consultations?clearly, the online medium is taking to a much higher level what started with radio in the 1920sšthat is,the dispersal of knowledge, culture, and democracy more directly into citizens' homes. but, unlike radiošor,later, televisionšnow it's twoway. and therein lies the profound significance of this trend.i'm going to go out on a limb and say that i think the online medium will become a mass phenomenon in 5years, or very soon thereafter. no, i don't predict that 100 million americans will be sitting at pc keyboardsevery night, squinting at their screens, typing in urls, and watching the hourglass while waiting for a grainypicture of jerry seinfeld to paintšever so slowly. there's just no way, at present course and speed, that theonline experience will become the ubiquitous social glue that television and movies are today. online resourcesare too hard to use and too expensive.so why did i say this will be a mass medium? because i don't think it's a linear world, and we're not goingto continue at present course and speed. there's too much at stake and too many smart people seekingbreakthroughs. in other words, i'm factoring in major advancesšyes, advances in the underlying technology andinfrastructure, but also some major breakthroughs in the human interface: something that will make going online as simple as hitting the power switch on a tv set. something that will replace keyboards. something that will improve the resolution of displays by a couple of orders of magnitude. something that will make navigation as intuitive as dialing grandma's phone number.note: this paper is a transcript of ross glatzer's remarks at the nii forum, may 24, 1995.the internetša model: thoughts on the fiveyear outlook237the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.it's going to happen. just look at who's working on itšcompanies like at&t, microsoft, mci, ibm, thebroadcast and cable networks, the baby bells, and many others, plus a fair number of smart people in theirgarages or home offices.but another type of breakthrough could be the emergence of our industry's holy grail: the "killerapplication." it's happened before without any change in technology. sony came out with the betamax in 1975.everyone thought then that the "killer app" for vcrs would be time shifting. and vcr usage rose steadilyšbutnot spectacularlyštoward 10 percent by 1982. that growth curve closely parallels the experience to date ofonline services. but something changed in 1982. suddenly, momandpop video stores sprang up in every town.the "killer app" turned out to be movie rentals. within the next 2 years, home penetration of vcrs approached50 percent. so we could be nearing a flash point for the online medium. all we need to do is make it easier to useand figure out the "killer app." in my opinion, the ''killer app" is communications.sure, i know, online services have had communications from day one. and these services are still a nicheproduct. but consider this: most online services to date have been built around information, not aroundcommunications. that's turned out to be backwards. it's backwards because all people communicate; but only asmall percentage of the population really cares about any one topic of informationšo.j. simpson excluded!consider this. compuserve has 2,000 categories of information. but how many of those does the averagemember use? i know from my experience that it's seldom more than four or five applications. at prodigy, there'sa terrific feature called strategic investor, yet only a small fraction of members subscribe.given these facts, anyone starting an online service today would probably be advised to build outward froma rocksolid communications core, optimizing everything for subscriber exchanges. and you wouldn't just havebulletin boards, chat, and email. you'd have instant messages, 3d chat, and easily attached files for soundphotos, video, and graphics. you'd also let subscribers create their own home pages on the web, where theycould talk about themselvesševen show their cars. prodigy will do this shortly, and the other services willquickly follow.after you'd established a firm core of communications, you'd want to start adding information andtransactions to it. but the information and transactions would be tightly integrated with the communications.that's important, because getting around online services today is like being the guy who explains where he'scalling from by saying he's in a phone booth at the corner of walk and don't walk.the leaders in online services will be those who can best integrate communications, information, andtransactions. they will build on a core of communications to create communities of interest.let me give you an example. a company like ford spends a fair amount of money each year puttinginformation online. so as a user, i can navigate to ford's advertising section and read about its new cars. evensee photos of them. ok, fine. but how often am i going to come back to see the same information? now, if i'minterested in cars, maybe i'll log on to a car enthusiasts' bulletin board. but after a while, i'll get bored talking tothe same old regulars. on another day, i might order a subscription to car and driver magazine right from mycomputer. the problem is, all these are discrete activities that i carry out as an individual. they don't createmuch excitement. they don't involve me very much.but what if an online service creates communities of interest built around worldclass communicationfunctionality? from a single screen, or with hyperlinks, i can see ford's cars, chat with other car enthusiastsabout them, debate with car and driver's editors, download model specs, check the archives of the detroit freepress for an article about carroll shelby, ask shelby a question, place a classified ad to sell my '67 mustang,look up the price of ford stocks, buy 100 shares, and send an instant message to a friend's beeper urging him toget online and join a discussion group.you can do all or most of this today on the online services. but no one has done a very good job ofintegrating it to create true communities of interest. that's what i think will make online a ubiquitous medium.and if it is, can advertisers be far behind?advertising on the web is a tricky business, however. very few advertisers really understand it. theenvironment differs from other media and will become more different over the next few years. it's anenvironment where the revenue model will become payperviewšor payperminute.today, for most users, the internet is essentially untimed. and, as a practical matter, so are the commercialonline services, since the majority of their users stay within the flatrate time limits that go with theirthe internetša model: thoughts on the fiveyear outlook238the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.monthly subscriptions. but on the internet, people will not want to subscribe to multiple features. they'll want todip in and out. and the information providers will oblige by making everything timed, including ads. very fewcompanies will offer anything free on the internet.socalled "free" applications on the web today, things like online newspapers and time warner'spathfinder, are really just experimentsšbeta tests, if you will. all these will be timed when masses of consumersarrive. the implications for advertising and transactions are profound. as commercial applications migrate to thenet, they must change dramatically. even the best of the online service advertisersšand some are very goodšmust be better on the net, because most of their customers will be paying for the time to view their applications.an advertiser or merchant on a commercial online service today pays "rent" to be in the service's ''mall." on thenet, advertisers will pay for the number of "footsteps" across their thresholdsšand the amount of time those feetstay in their stores. this puts a premium on closing a high percentage of sales. you must be a consummateretailer to prosper.and it's not easy. you can't just show up and be successful, because your online customers are far morediscerning than the typical catalog or retail customer. they simply will not tolerate anything but the highest levelof service. incredible service explains why a few companies have prospered in an online arena where many havenot.take pc flowers, for example. in the florist business, a 5 percent error rate is considered normal. at pcflowers, it is onetenth of that. and they have a more complex business than any florist. needless to say, pcflowers has a fabulous backend system, one that can easily handle more than $1 million in business on mother'sday. and on the rare occasion where they do slip up, they move heaven and earth to make it up to the customer– and the customer's customer. that's expensive. it's also profitable.the pc financial network is another online success storyš$2.5 billion in stock trades last year with anaverage trade of $6,000. but to do that accurately, they've invested in 80 fulltime employees on the back end.so while it's easy to set up a business on the world wide web, it's not going to be so easy to make itprosper in the long term. you need to make major investments. successful advertisers and merchants understandthis. they understand that information alone doesn't cut it. it takes true relationship marketing. and therelationship has to be based on communications.if the commercial online services hope to retain these advertisersšand with them, a key part of theirrevenue basešthey will have to change. one way they can change is to become the preferred access providers tothe world wide web. in doing so, i think they will eventually have to give up the idea of charging a monthlysubscription for a defined collection of content.users will pay a flat rate for access, billing, customer services, etc. but they will select content seamlesslyfrom anywhere on the net without caring or needing to know who is integrating it. the online services will needto establish a revenue model by which they receive a fraction of a penny every time one of their users clicks onan advertiser's net site. thus, prodigy's home page might feature an ad pointing users to the home page of pcflowers. netcom's home page might include a paid icon pointing users to america online. all kinds ofsynergistic relationships are possible.to summarize, here's what i see happening by 2000: the internet, especially the world wide web, will become the communications/data transfer standard. itwill be the backbone for advanced interactive services, including tv shopping. there will be breakthroughs in ease of use and input/output technology. commercial activity will be an important part of the web, but not as important as some people predict. advertising will be substantially more sophisticated and will be communications centered. a clickstream adrevenue model will replace display revenue. there will be some consolidation of the online services as we know them, although many companies willoffer net access. the online services will be forced to become one with the internet. their ability to survive as distinct entitiesand to prosper will depend in part on their success in creating communities of interest.the internetša model: thoughts on the fiveyear outlook239the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. for at least a few years, the online services will retain uniqueness in terms of structure, context, backendtransactions, billing, customer service, marketing segmentation, technology innovation, and distribution. buteven these advantages will fade over time. pricing pressures will be brutal. consider that only a few years ago, compuserve could charge $22.50 anhour for access at 9,600 baud. today, it charges $4.80 while others charge as little as $1 for access at 14,400baud. this trend will continue, which is great for the consumer, but ominous for the online services. more than 50 percent of american households will be online. but many of them will use simpler accessdevices than today's computers. like most successful trends, all these changes will be driven by the competitive market.and finally, if all this doesn't happen in the next 5 years, just remember what mahatma gandhi said: "thereis more to life than increasing its speed."the internetša model: thoughts on the fiveyear outlook240the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.31the economics of layered networksjiong gong and padmanabhan srinageshbell communications research inc.statement of the problemthe creation of a national information infrastructure (nii) will require large investments in networkfacilities and services, computing hardware and software, information appliances, training, and other areas. theinvestment required to provide all businesses and households with broadband access to the nii is estimated to bein the hundreds of billions of dollars; large investments in the other components of the nii will also be required.in the national information infrastructure: agenda for action, the clinton administration states, "the privatesector will lead the deployment of the nii."what architectural and economic framework should guide the private sector's investment and deploymentdecisions? is the open data network (odn) described by the nrenaissance committee1 an appropriateeconomic framework for the communications infrastructure that will support nii applications? a key componentof the odn is the unbundled bearer service, defined to be "an abstract bitlevel transport service" available atdifferent qualities of service appropriate for the range of nii applications. the committee states that "bearerservices are not part of the odn unless they can be priced separately from the higherlevel services" (p. 52). therationale for this requirement is that it is "in the interest of a free market for entry at various levels" (p. 52).what effect will the unbundled bearer service proposed by the nrenaissance committee have on theinvestment incentives of network providers in the private sector? will carriers that invest in longlived assets begiven a fair opportunity to recover their costs? this paper provides a preliminary discussion of the economics ofan unbundled bearer service.background: convergence and emerging competitiontechnological advances are rapidly blurring traditional industry boundaries and enabling competitionbetween firms that did not previously compete with one another. for example, cable tv providers in the unitedkingdom (some of which are now partly owned by u.s. telecommunications firms) have been allowed to offertelephone service to their subscribers since 1981 and currently serve more than 500,000 homes.2 numeroustelephone companies in the united states are currently testing the delivery of video services to households overtheir networks. in addition, new firms have recently entered markets that were not subject to major inroads in thepast. competitive access providers (caps) have begun to serve business customers in the central businessdistricts of many large cities in competition with local exchange carriers (lecs), and direct broadcast satelliteservices have begun to compete with cable providers. in sum, new entrants are using new technologies tocompete with incumbents, and incumbents in previously separate industries are beginning to compete with oneanother.the economics of layered networks241the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.analysistheoretical approaches to the economics of pricing under differing market structuresin a pure monopoly, a variety of price structures may be consistent with cost recovery, and the firm (orregulators) may be able to select price structures that promote political or social goals such as universal serviceor unbundling of raw transport. in a competitive market, this flexibility may not exist. under perfect competition(which assumes no barriers to entry and many small firms), the price per unit will be equal to the cost per unit(where costs are defined to include the opportunity costs of all resources, including capital, that are used inproduction). there is no pricing flexibility. when neither of these pure market forms exist, economic theory doesnot provide any general conclusions regarding equilibrium price structures or industry boundaries. whilesubstantial progress has been made in developing game theory and its application to oligopoly,3 no completelygeneral results on pricing are available. this is particularly true in the dynamic context where interdependenciesbetween current and future decisions are explicitly considered. some theoretical work in this area is summarizedin shapiro.4 an important result in game theory asserts that no general rules can be developed: "the best knownresult about repeated games is the wellknown 'folk theorem.' this theorem asserts that if the game is repeatedinfinitely often and players are sufficiently patient, then 'virtually anything' is an equilibrium outcome."5modeling based on the specific features of the telecommunications industry may therefore be a more promisingresearch strategy.the economic analysis of competition among network service providers (nsps) is further complicated bythe presence of externalities and excess capacity. call externalities arise because every communication involvesat least two parties: the originator(s) and the receiver(s). benefits (possibly negative) are obtained by allparticipants in a call, but usually only one of the participants is billed for the call. a decision by one person tocall another can generate an uncompensated benefit for the called party, creating a call externality. networkexternalities arise because the private benefit to any one individual of joining a network, as measured by thevalue he places on communicating with others, is not equal to the social benefits of his joining the network,which would include the benefits to all other subscribers of communicating with him. again, the subscriptiondecision creates benefits that are not compensated through the market mechanism. it has been argued that theprices chosen by competitive markets are not economically efficient (in the sense of maximizing aggregateconsumer and producer benefits) when externalities are present.6it has also been argued that "[i]ndustries with network externalities exhibit positive critical massši.e.,networks of small sizes are not observed at any price."7 the consequent need to build large networks, togetherwith the high cost of network construction (estimated by some to be $13,000 to $18,000 per mile for cablesystems8), implies the need for large investments in longlived facilities. the major cost of constructing fiberoptic links is in the trenching and labor cost of installation. the cost of the fiber is a relatively small proportionof the total cost of construction and installation. it is therefore common practice to install "excess" fiber.according to the federal communications commission, between 40 percent and 50 percent of the fiber installedby the typical interexchange carriers is "dark"; the lasers and electronics required for transmission are not inplace. the comparable number for the major local operating companies is between 50 percent and 80 percent.the presence of excess capacity in one important input is a further complicating factor affecting equilibriumprices and industry structure.to summarize: a full economic model of the networking infrastructure that supports the nii would need toaccount for at least the following features: oligopolistic competition among a few large companies that invest in the underlying physicalcommunications infrastructure; network and call externalities at the virtual network level; and large sunk costs and excess capacity in underlying transmission links.the economics of layered networks242the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.an analysis of the optimal industry structure is well beyond the scope of this paper; it is a promising areafor future research. this paper focuses on implications of two of these issues (oligopolistic competition and sunkcost with excess capacity) for industry structure and the unbundled bearer service. the internet is used as a casestudy to illustrate trends and provides a factual background for future analyses. this paper provides a descriptionof the current internet industry structure, current internet approaches to bundling/unbundling and reselling, somerecent examples of the difficulties raised by resale in other communications markets, and the increasing use oflongterm contracts between customers and their service providers. the implications for the unbundled bearerservice are drawn.layered networksthe internet is a virtual network that is built on top of facilities and services provided bytelecommunications carriers. until recently, internet service providers (isps) located routers at their networknodes and interconnected these nodes (redundantly) with pointtopoint private lines leased fromtelecommunications companies. more recently, some isps have been moving from a private line infrastructure tofast packet services such as frame relay, switched multimegabit data service (smds), and asynchronous transfermode (atm) service. specifically, among the providers with national backbones, psi runs its ip services over its frame relay network, which is run over its atm network, which in turn isrun over pointtopoint circuits leased from five carriers; alternet runs part of its ip network over an atm backbone leased from mfs and wiltel; ans's backbone consists of ds3 links leased from mci; and sprintlink's backbone consists of its own ds3 facilities.cerf net, a regional network based in san diego, uses smds service obtained from pacific bell toconnect its backbone nodes together.9these examples reveal a variety of technical approaches to the provision of ip transport. they also showdifferent degrees of vertical integration, with sprint the most integrated and alternet the least integrated isp inthe group listed above. the variety of organizational forms in use raises the following question: can isps withvarying degrees of integration coexist in an industry equilibrium, or are there definite cost advantages that willlead to only one kind of firm surviving in equilibrium? the answer to this question hinges on the relative coststructures of integrated and unintegrated firms. the costs of integrated firms depend on the costs of producingthe underlying transport fabric on which ip transport rides. the cost structures of unintegrated firms aredetermined in large part by the prices they pay for transport services (such as atm and ds3 services) obtainedfrom telecommunications carriers. these prices, in turn, are determined by market forces. more generally, thelayered structure of data communications services leads to a recursive relationship in which the cost structure ofservices provided in any layer is determined by prices charged by providers one layer below. in this layeredstructure, a logical starting point for analysis is the lowest layer: the pointtopoint links on which a variety offast packet services ride.competition at the bottom layerfor illustrative purposes, consider a common set of services underlying the internet today. at the verybottom of the hierarchy, physical resources are used to construct the links and switches or multiplexers thatcreate pointtopoint channels. in the emerging digital environment, time division multiplexing (tdm) in thedigital telephone system creates the channels out of longlived inputs (including fiber optic cables). the sunkcosts are substantial.there are at least four network service providers with national fiber optic networks that serve all major citypairs.10 each of these providers has invested in the fiber and electronics required to deliver pointtopointthe economics of layered networks243the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.channel services, and, as was stated above, each has substantial excess capacity. the cost structure of providingchannels includes high sunk costs of construction, the costs of lasers and electronics needed to light the fiber,costs of switching, the high costs of customer acquisition (marketing and sales), costs associated with turningservice on and off (provisioning, credit checks, setting up a billing account, and so on), costs of maintaining andmonitoring the network to assure that customer expectations for service are met, costs of terminating customers,and general administrative costs. the incremental cost of carrying traffic is zero, as long as there is excesscapacity.if all owners of national fiber optic facilities produced an undifferentiated product (pointtopoint channels)and competed solely on price, economic theory predicts that they would soon go out of business: "with equallyefficient firms, constant marginal costs, and homogeneous products, the only nash equilibrium in prices, i.e.,bertrand equilibrium, is for each firm to price at marginal cost."11 if this theory is correct,12 firms could recoverthe onetime costs of service activation and deactivation through a nonrecurring service charge, and recoverongoing customer support costs by billing for assistance, but they would not be able to recover their sunk cost.industry leaders seem to be aware of this possibility. as john malone, ceo of tci, stated, "we'll end up with amuch lower marginal cost structure and that will allow us to underprice our competitors."13the history of leased line prices in recent years does reveal a strong downward trend in prices. according tobusiness week,14 private line prices have fallen by 80 percent between 1989 and 1994, and this is consistent withbertrand competition. during the same period there has been a dramatic increase in the use of term and volumediscounts. at&t offers customers a standard monthtomonth tariff for t1 service and charges a nonrecurringfee, a fixed monthly fee, and a monthly rate per mile. customers who are willing to sign a 5year contract andcommit to spending $1 million per month are offered a discount of 57 percent off the standard monthtomonthrates. smaller discounts apply to customers who choose shorter terms and lower commitment volumes: a 1yearterm commitment to spend $2,000 per month obtains a discount of 18 percent. the overall trend toward lowerprices masks a more complex reality. "there are two types of tariffs: 'front of the book' rates, which are paid bysmaller and uninformed large customers, and 'back of the book' rates, which are offered to the customers who areready to defect to another carrier and to customers who know enough to ask for them. the 'front of the book'rates continue their relentless 5 to 7 percent annual increases."15 in 1994 at&t filed over 1,200 specialcontracts, and mci filed over 400.16there are some theoretical approaches that address the issues discussed above. williamson's discussion ofnonstandard commercial contracting17 as a means for sharing risk between the producer and consumers isrelevant to the term commitments described above. in addition to risk reduction, longterm contracts reducecustomer churn, which often ranges from 20 to 50 percent per year in competitive telecommunicationsmarkets.18 as service activation and termination costs can be high, reduction of churn can be an effective costsaving measure.there appears to be an empirical trend toward term/volume commitments that encourage consumers ofprivate line services to establish an exclusive, longterm relationship with a single carrier. there is littlepublished information on long distance fast packet prices. according to one source, none of the long distancecarriers or enhanced service providers (e.g., compuserve) tariff their frame relay offerings. some intralatatariffs filed by local exchange carriers do offer term and volume (per pvc) discounts, and the economic forcesthat give rise to term/volume commitments for private lines have probably resulted in term/volume commitmentsfor longdistance, fastpacket services as well.competition among internet service providersthe effect of term/volume commitments in private lines and fast packet services affects the cost structuresof isps that do not own their own transport infrastructure. it may be expected that large isps that lease theirtransport infrastructures will sign multiyear contracts, possibly on an exclusive basis, with a single carrier. theseproviders will then have sunk costs, as they will have minimum payments due for a fixed period to their carriers.competition at this level will then be similar to competition at the lower level, and we may expect to see term/volume contracts emerge in the internet. a quick survey of internet sites shows this to be the case. forthe economics of layered networks244the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.example, in january 1995, alternet offered customers with a t1 connection a 10 percent discount if theycommitted to a 2year term. global connect, an isp in virginia, offers customers an annual rate that is 10 timesthe monthly rate, amounting to a 17 percent discount for a 1year commitment. there are many other examplesof this sort.the internet is beginning to resemble the private line market in one other aspect: prices are increasinglybeing viewed as proprietary. isps that used to post prices on their ftp or web servers now ask potentialcustomers to call for quotes. presumably, prices are determined after negotiation. this development mirrors thepractice of longdistance carriers to use special contracts that are not offered on an open basis at the "front of thebook" but are hidden at the back.economics of resalekellogg, thorne, and huber19 describe the history of the federal communication commission's decision onresale and shared use. noam20 analyzed the impact of competition between common carriers and contractcarriers (including systems integrators and resellers) and concluded that common carriage cannot survive thecompetitive struggle. recent events lend some credence to this view. according to one recent study, "resold longdistance services will constitute an increasing portion of the total switched services revenue in coming years,growing at a compound annual growth rate of 31 percent from 1993 to 1995.– the number is expected to rise to$11.6 billion, or 19.2 percent of the estimated total switched services market in 1995."21 the growth of resale inthe cellular market suggests that there are equally attractive resale opportunities in this market.22 in the internet,some isps charge resellers a higher price than they charge their own customers.23 other isps, such assprintlink, make no distinction between resellers and end users. facilitiesbased carriers have had a rockyrelationship with resellers, and courts have often been resorted to by both carriers and resellers.24the pricing model that is emerging appears to resemble rental arrangements in the real estate market. in thenew york city area, lowquality hotel rooms are available for about $20 per hour. far better hotel rooms areavailable for $200 per day (which is a large discount of 24 times $20). roomy apartments are available formonthly rentals at much less than 30 days times $200/day. and $6,000 per month can be used to buy luxuryapartments with a 30year mortgage. term commitments are rewarded in the real estate market, where sunk costsand excess capacity are (currently) quite common. the telecommunications industry appears to be moving in thesame direction. contracts are not limited to fiveyear terms; mfs and snet recently signed a 20year contractunder which mfs will lease fiber from snet,25 and bell atlantic has a 25year contract with the pentagon.26the term structure of contracts is an important area for empirical and theoretical research.implications for unbundled bearer servicesunbundled bearer services have much in common with common carriage: both approaches facilitate greatercompetition at higher layers (in content, with common carriage, and in enhanced services of all types with thebearer service). the dilemma facing policymakers is that, if noam is right, competition in an undifferentiatedcommodity at the lower level may not be feasible. in his words: "the longterm result might be a gradualdisinvestment in networks, the reestablishment of monopoly, or price cartels, and oligopolistic pricing."27 thuspolicies promoting competition in the provision of unbundled bearer services among owners of physicalnetworks may ultimately fail. the market may be moving toward contract carriage based on term/volumecommitments and increasing efforts at differentiation, and away from the ideal of an unbundled bearer service.should unbundled bearer services be aligned with this trend by being defined as a spectrum of term/volumecontracts? the competitive mode that is emerging is quite complex, and the effects of unbundling in thisenvironment are hard to predict.the economics of layered networks245the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.conclusionsthis paper does not suggest specific architectures or policies for the emerging nii. it identifies somedifficult economic problems that may need to be addressed. these are the familiar ones related to resale andinterconnection, with the added complication of competition among multiple owners of geographicallycoextensive physical networks. this paper has provided references to recent developments intelecommunications markets and identified strands in the economics literature that are relevant to the centralissues raised by the bearer service.there is an urgent need for a clearer economic analysis of these issues, and it is critical that the analysis payclose attention to the realities of competition and evolving competitive strategy. three specific areas appearparticularly promising: empirical analysis of evolving price structures that quantifies the movement from pricing by the minute (theoriginal message toll service) to pricing by the decade (contract tariffs); game theoretic models of competition in longterm contracts with sunk costs; and experimental approaches to network economics.28notes1. computer science and telecommunications board, national research council. 1994. realizing the information future: the internet andbeyond. national academy press, washington, d.c.2. "cable tv moves into telecom markets," business communications review, november, 1994, pp. 43œ48.3. a recent text is fudenberg, drew, and jean tirole, 1992, game theory, mit press, cambridge, mass.4. shapiro, carl. 1988. "theories of oligopoly behavior," chapter 6 in handbook of industrial organization, richard schmalensee androbert willig (eds.). north holland, amsterdam, pp. 400œ407.5. fudenberg, drew, and jean tirole. 1988. "noncooperative game theory," chapter 5 in handbook of industrial organization, richardschmalensee and robert willig (eds.). north holland, amsterdam, p. 279.6. these externalities are discussed in some detail in mitchell, bridger, and ingo vogelsang, 1991, telecommunications pricing: theory andpractice, cambridge university press, cambridge, england, pp. 55œ61.7. economides, nicholas, and charles himmelberg. 1994. "critical mass and network size," paper presented at the twentysecond annualtelecommunications policy research conference, october.8. yokell, larry j. 1994. "cable tv moves into telecom markets," business communication review, november, pp. 43œ48.9. a fuller description of these networks can be obtained from their web pages. the urls are http://www.psi.com, http://www.alter.net,http://www.sprint.com, and http://www.cerf.net.10. these networks are shown on a pullout map in forbes asap, february 27, 1995.11. shapiro, carl. 1988. "theories of oligopoly behavior," chapter 6 in handbook of industrial organization, richard schmalensee androbert willig (eds.). north holland, amsterdam.12. an example of an alternative theoretical approach is found in sharkey, william, and david sibley, 1993, "a bertrand model of pricingand entry," economic letters, pp. 199œ206.13. business week. 1995. "brave talk from the foxhole," april 10, p. 60.14. business week. 1994. "dangerous living in telecom's top tier," september 12, p. 90.15. hills, michael t. 1995. "carrier pricing increases continue," business communications review, february, p. 32.16. ibid.17. williamson, o.e. 1988. "transaction cost economics," chapter 3 in handbook of industrial organization, richard schmalensee androbert willig (eds.). north holland, amsterdam, pp. 159œ161.18. see fcc docket 93197, report and order, 1.12.95, page 8 for statistics on at&t's churn in the business market. see also ko, david,and michael gell. n.d. "cable franchise growth in the u.k.," memo, university of oxford, for churn in the u.k. cable market.19. schmalensee and willig (eds.), op. cit., pp. 610œ614.20. noam, eli. 1994. "beyond liberalization ii: the impending doom of common carriage," telecommunications policy, pp. 435œ452.21. telecommunications alert 13(2):6.the economics of layered networks246the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.22. forbes. 1995. "restless in seattle," march 27, p. 72.23. for example, alternet's policy in january 1995 was as follows: "because wholesale customers use more of our backbone facilities andbecause they also place greater demands on our staff, we charge more for our wholesale services."24. for example, see telecommunications reports. 1994. "oregon jury decides against at&t in reseller case," july 4, p. 34, and "at&tsues reseller for unauthorized trademark use," november 7, p. 26.25. telco business report, february 14, 1994, p. 4.26. ray smith, ceo of bell atlantic, in an interview in wired, february 1995, p. 113.27. noam, op. cit., p. 447.28. plott, charles, alexandre sugiyama, and gilad elbaz. 1994. "economics of scale, natural monopoly, and imperfect competition in anexperimental market," southern economic journal, october, pp. 261œ287.the economics of layered networks247the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.32the fiberoptic challenge of information infrastructuresp.e. green, jr.ibm t.j. watson research centerthis paper discusses the role of optical fiber as the one physical medium upon which it will be possible tobase national and global infrastructures that will handle the growing demands of bandwidth to the desktop in apost2000 developed society.a number of individual applications today demand large bit rate per user, such as supercomputerinterconnection, remote site backup for large computer centers, and digital video production and distribution, butthese are isolated niches today. the best gauge of the need for the infrastructure to supply large amounts ofbandwidth to individual users is probably to be found in the phenomena associated with the use of the internetfor graphicsbased or multimedia applications.just as the use of noncommunicating computers was made much easier by the emergence of the icon andmousebased graphical user interface of the macintosh, windows and os2, the same thing can be observed forcommunicating computers with the world wide web. the modality in which most users want to interact withdistributed processing capability (measured in millions of instructions per second [mips]) is the same as it hasalways been with local mips: they want to point, click, and have an instant response. they will in fact want tohave a response time from some remote source on any future information infrastructure that is a negligibleexcess over the basic propagation time between them and the remote resource. they will want the infrastructureto be not only widebanded for quick access to complex objects (which are evolving already from just stillgraphics to include voice and video) but also to be symmetric, so that any user can become the center of his orher own communicating community. this need for an "anytoany" infrastructure, as contrasted to the onewayor highly asymmetrical character of most of our wideband infrastructure today (cable and broadcast), is thoughtby many political leaders to be the key to optimizing the use of communication technology for the public good.thus, a dim outline of many of the needs that the information infrastructure of the future must satisfy can bediscerned in the emerging set of highbandwidth usage modes of the internet today1, particularly the web. thepicture that emerges from examining what is happening in the web is most instructive. figure 1 shows the recentand projected growth of web traffic per unit time per user assuming the present usage patterns, which includealmost no voice, video clips, or high response speed applications such as pointandshoot games or interactivecad simulations. as these evolve, they could exacerbate the already considerable bit rate demand per user,which figure 1 shows as a factor of 8 per year. if the extrapolations in figure 1 are correct, this means that in thedecade to 2005, the portion of the available communication infrastructure devoted to descendants of the webmust undergo a capacity growth of about 109 in order to keep up with demand.there is only one physical transmission technology capable of supporting such growth: optical fiber.fortunately for the prospects of an infrastructure that will provide society what it needs, fiber has been going intothe ground, on utility poles, within buildings, and under the oceans at a rapid rate. the installation rate has beenover 4,000 miles per day for some years, just in the continental united states alone, so that by now over 10million miles of installed fiber exist here. even more fortunately, each fiber has a usable bandwidth of some25,000 ghz, roughly 1,000 times the usable radio spectrum on planet earth, and quite enough to handle all thephone calls in the u.s. telephone system at its busiest. while this gigantic capacity is underused by at least athe fiberoptic challenge of information infrastructures248the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 predicted world wide web bandwidth demand.source: data courtesy of the internet society.figure 2 the ''last mile" bandwidth bottleneck.factor of 10,000 in today's practice, which is based on time division multiplexing, the technical means arerapidly evolving to open up the full fiber bandwidth. this is the alloptical networking technology, based ondense wavelength division, in which different channels travel at different "colors of light."so, why isn't it true that we already have the physical basis in place over which to send the traffic of thefuture? most of the answer is summarized in figure 2 2. all the communication resources we have beeninstalling seem to be improving in capacity by roughly 1.5 per year, totally out of scale with the 8timesperyeargrowth of demand shown in figure 1. the top curve of figure 2 shows the capability of desktop computers toabsorb and emit data into and out of the buses that connect them to the external world3. the next line showslocal area network capacity as it has evolved. the third one shows the evolution of highend access to the telcobackbone that allows users at one location connectivity to users elsewhere outside the local lan environment.the capacity of this third curve has been available only to the most affluent corporations and universities, thosethat can afford t1, t3, or sonet connections to their premises.the fiberoptic challenge of information infrastructures249the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.while all three of these capacities are evolving at the rate of only a factor of 1.5 per year, they representreally substantial bitrate numbers. current web users who can afford 10 mb/s lans and tcarrier connectionsinto the backbone experience little response time frustration. however, the situation of most of us is representedmore accurately by the bottom curve, which shows the data rate available between our desktop and the backboneover the infamous "last mile" using telco copper connections with either modems or isdn. there is a 10 4performance deficiency between the connectivity available between local and longhaul resources and theinternal performance of both these resources.if one compares the rate of growth of web traffic in figure 1 with the data of figure 2, it is clear that thereis an acute need to bridge the 104 gap of the last mile with fiber and inevitably to increase the bandwidths of thebackbone also, probably at a greater rate than the traditional factor of 1.5 per year.as for bridging the gap between the desktop and the telco backbone, the proposed solution for years nowhas been "fiber to the home"4, expressing the notion that it must pay for itself at the consumer level. thealternative of coaxial cable to the premises, while having up to a gigahertz of capacity, is proving an expensiveand nonexpandable way to futureproof the last mile against the kind of bandwidth demands suggested byfigure 1, and the architectures used have assumed either unidirectional service or highly asymmetrical service.what is clearly needed is fiber, probably introduced first in timedivision mode, and then, as demand builds up,supporting a migration to wavelength division (alloptical).figure 3 shows the rate at which fiber to the premises ("home") has been happening5 in the united states.the limited but rapidly growing amount of fiber that is reaching all the way to user premises today is mostly toserve businesses. the overall process is seen to be quite slow; essentially nothing very widespread will happenduring the next 5 to 7 years to serve the average citizen. however, the bandwidth demand grows daily.meanwhile, alloptical networks are beginning to migrate off the laboratory bench and into real service in smallniches.what figure 3 shows is the steady reduction of the number of homes that, on the average, lie within the areasurrounding the nearest fiber end. in 1984, when fiber was used only between central offices (cos), this figurewas the average number of homes or offices served by such a co. as the carriers6, cable companies7, andcompetitive local access providers8 found great economies in replacing copper with fiber outward from theircos and headends, the number decreased. a linear extrapolation down to one residence per fiber end predictsthat 10 percent of u.s. homes will be reached by fiber by about 2005, at best. in japan, it is quite possible that astrong national effort will be launched that will leapfrog this lengthy process using large government subsidies9.during the coming decade, several things will happen, in addition to ever increasing enduser pressure formore bandwidth to the desktop. competition between telcos, cable companies, and competitive access providersmay or may not accelerate the extrapolated trend shown in figure 3. advances in lowcost optoelectronictechnology, some of them based on mass production by lithography, could also accelerate the trend, becauseanalyses of costs of fiber to the home consistently show a large fraction of the cost to lie in the settop box,splitters, powering10, and, in the case of wavelength division multiplexing (wdm) approaches,multiwavelength or wavelengthtunable sources and receivers. it is widely felt that the price of the settop boxitself will have to be below $500 for success in the marketplace. this is probably true, whether the "settop box"is really a box sitting atop a tv set or a feature card within a desktop computer. by 2005 it should become quiteclear whether the tv set will be growing keyboards, hard disks, and cpus to take over the pc, whether the pcwill be growing video windows to take over the tv set, or whether both will coexist indefinitely and separately.in any case, the bottleneck to these evolutions will increasingly be the availability by means of fiber of high bitrates between the premises and the backbone, plus a backbone bandwidth growth rate that is itself probablyinadequate today.meantime, looking ahead to the increasing availability of fiber paths and the customers who need them toserve their highbandwidth needs, the alloptical networking community is hard at work trying to open up the25,000 ghz of fiber bandwidth to convenient and economical access to end users. already, the telcos are usingfourwavelength wdm in field tests of undersea links11. ibm has recently made a number of commercialinstallations12 of 20wavelength wdm links for achieving fiber rental cost savings for some of its largecustomers who have remote computer site backup requirements. the rationale behind both thesecommercialization efforts involves not only getting more bandwidth out of existing fiber, but also making thethe fiberoptic challenge of information infrastructures250the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 3 predicted rate at which fiber reaches user premises.figure 4 three wavelength division architecture.installation "multiprotocol" or "futureproof" by taking advantage of the fact that each wavelength can carryan arbitrary bit rate and framing convention format, or even analog formats, up to some maximum speed set bythe losses on the link.these successful realizations of simple multiwavelength links represent the simplest case of the threedifferent kinds of alloptical systems, shown in figure 4. in addition to the twostation wdm link (with multipleports per station), the figure shows the two forms taken by full networks, structures in which there are manystations (nodes), with perhaps only one or a few ports per node.the second type, the broadcast and select network, usually works by assigning to the transmit side of eachnode in the network a fixed optical frequency, merging all the transmitted signals at the center of the network inan optical star coupler and then broadcasting the merge to the receive sides of all nodes. the entire innerstructure, consisting of fiber strands and the star coupler, is completely passive and unpowered. by means of asuitable protocol, when a node wants to talk to another (either by setting up a fixed light path "circuit" or byexchanging packets), the latter's receiver tunes to the former's transmit wavelength and vice versa. broadcast andthe fiberoptic challenge of information infrastructures251the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.select networks have been prototyped and, while still considered not quite in the commercialization cost range,have been used in live application situations, for digital video distribution13 and for gigabit supercomputerinterconnection at rates of 1 gigabit per second14.aside from high cost, which is currently a problem with all wdm systems, there are two other thingswrong with broadcast and select networks. the power from each transmitter, being broadcast to all receivers, ismostly wasted on receivers that do not use it. secondly, the number of nodes the network can have can be nolarger than the size of the wavelength pool, the number of resolvable wavelengths. today, even though there are25,000 ghz of fiber capacity waiting to be tapped, the wavelength resolving technology is rather crude, allowingsystems with only up to about 100 wavelengths to be built, so far15. the problems of both cost and number ofwavelengths are gradually being solved, often by the imaginative use of the same tool that brought costreductions to electronics two decades ago: lithography.clearly, a network architecture that allows only 100 nodes does not constitute a networking revolution;some means must be provided for achieving scalability by using each wavelength many places in the network atthe same time. wavelength routing accomplishes this, and also avoids wastage of transmitted power, bychanneling the energy transmitted by each node at each wavelength along a restricted path to the receiver insteadof letting it spread over the entire network, as with the broadcast and select architecture. as the name"wavelength routing" implies, at each intermediate node between the end nodes, light coming in on one port at agiven wavelength gets routed out of one and only one port.the components to build broadcast and select networks have been available on the street for 4 years, butoptical wavelength routers are still a reality only in the laboratory. a large step toward practical wavelengthrouting networks was recently demonstrated by bellcore16.the ultimate capacity of optical networking is enormous, as shown by figure 5, and is especially great withwavelength routing (figure 6). figure 5 shows how one might divide the 25,000 ghz into many lowbitrateconnections or a smaller number of higherbitrate connections. for example, in principle one could carry 10,000uncompressed 1 gb/s hdtv channels on each fiber. the figure also shows that erbium amplifiers, needed forlong distances, narrow down the 25,000 ghz figure to about 5,000 ghz, and also that the commerciallyavailable tunable optical receiver technology is capable of resolving no more than about 80 channels.with broadcast and select networks the number of supportable connections is equal to the number ofavailable wavelengths in the pool of wavelengths. however, with wavelength routing, the number of supportableconnections is the available number of wavelengths multiplied by a wavelength reuse factor17 that grows withthe topological connectedness of the net work, as shown in figure 6. for example, for a 1,000node network ofnodes with a number of ports (the degree) equal to four, the reuse factor is around 50, meaning that with 100wavelengths, there could, in principle, be five connections supportable for each of the 1,000 nodes.as far as the end user is concerned, there is sometimes a preference for circuit switching and sometimes forpacket switching. the former provides protocol transparency during the data transfer interval, and the latterprovides concurrency (many apparently simultaneous data flows over the same physical port, by the use of timeslicing). in both cases, very large bit rates are possible without the electronics needing to handle traffic bits fromextraneous nodes other than the communicating partners.the very real progress that has been made to date in alloptical networking owes a great deal to theforesight of government sponsors of research and development the world over. the three big players have beenthe ministry of posts and telecommunications (mpt) in japan, the european economic community (eec), andthe u.s. advanced research projects agency (arpa). the eec's programs, under race1 and race2(rationalization of advanced communication in europe), have now been superseded by acts (advancedcommunication technology systems).in 1992, arpa initiated three consortia aimed at systemlevel solutions, and all three have been successful.the optical networking technology consortium, a group of some 10 organizations led by bellcore, hasdemonstrated an operating wavelength routing network using acoustooptic filters as wavelength routers. the alloptical networking consortium, consisting of the massachusetts institute of technology, at&t belllaboratories, and digital equipment corporation, has installed a network that combines wavelength routing,wavelength shifting, broadcastandselect, and electronic packet switching between littleton, lexington, andcambridge, massachusetts. with arpa and doe support, ibm (working with los alamos nationalthe fiberoptic challenge of information infrastructures252the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 5 capacity of broadcast and select networks.figure 6 wavelength reuse.laboratory) has developed an extensive set of algorithms for distributed control of very large wavelengthrouting networks, and has studied offloading of tcp/ip for supercomputer interconnection in its rainbow2network.it is fair to say that the united states now holds the lead in making alloptical networking a commercialreality, and that arpa support was one of the important factors in this progress. at the end of 1995, arpakicked off a second round of 3year consortia in the alloptical networking area, with funding roughly five timesthat of the earlier programs18.whether alloptical networking will be a commercially practical part of the nii depends on three factors: (1)whether the investment will be made to continue or accelerate the installation of fiber to the premises anddesktop (figure 3), (2) whether it proves feasible to reduce component costs by two to three orders of magnitudebelow today's values, and (3) the extent to which providers offer the fiber paths in the form of "dark fiber"šthatis, without any electronic conversions between path ends.the fiberoptic challenge of information infrastructures253the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.this last problem seems to be solving itself in metropolitan and suburban areas of many countries, simplyby competition between providers, but the problems of long dark fiber paths that cross jurisdictions and requireamplification have yet to be faced. in the united states, the federal communications commission has vieweddark fiber as being equivalent to copper, within the meaning of the communication act of 19341920; that is, ifthe public interest requires making dark fiber ends available, one of the monopoly obligations implied bymonopoly privileges is that the public should be offered it at a fair price.the optoelectronic component cost issue is under active attack. considering that there are significant effortsunder way to use lithography for cost reduction of tunable and multichannel wdm transmitters and receivers, itseems possible to predict a oneorderofmagnitude decrease in price by 2000 and two orders of magnitude by2005. this implies that the optoelectronics for each end of wdm links of 32 wavelengths should cost $15k and$1.5k, respectively, and that the optoelectronics in each node of a broadcast and select network of 32 to 128nodes should cost $1,000 and $100, respectively. if these last numbers are correct, this means that broadcast andselect mans and lans should be usable by desktop machines some time between 2000 and 2005, since thecosts would be competitive with the several hundred dollars people typically spend year after year on eachmodem or lan card for pcs.the sources of investment in the "last fiber mile" are problematical. in the united states the telcos and thecable companies are encountering economic problems in completing the job. in several other countries withstrong traditions of centralized telecommunication authority, for example japan and germany, a shortcut may betaken using public money in the name of the public interest. so far in the united states it is "pay as you go." thishas meant that only businesses can afford to rent dark fiber, and even then this has often been economical onlywhen wdm has been available to reduce the number of strands required12.whether a completely laissezfaire approach to the last mile is appropriate is one of the problemsgovernments are facing in connection with their information infrastructures. fiber has ten orders of magnitudemore bandwidth (25,000 ghz vs. 3.5 khz) and can operate with ten orders of magnitude better raw bit error ratesthat can voice grade copper (1015 vs. 105), and yet on the modest base of copper we have built the internet, theworld wide web, tendollar telephones at the supermarket, communicating pcs and laptops, prevalent fax andanswering machine resources, and other innovations. it is the vision of those working on alloptical networkingthat a medium with ten orders of magnitude better bandwidth and error rate than one that gave us today'scommunication miracles is unlikely to give us a future any less miraculous, once the fiber paths, the networktechnology, and the user understanding are all in place.references[1] a.r. rutkowski, "collected internet growth history of number of hosts and packets per month," private communication, march 26, 1995.[2] a.g. fraser, banquet speech, "second ieee workshop on high performance communication subsystems," september 2, 1993.[3] r. dodson, "bus architectures," ibm pc company bulletin board at 9195170001 (download files ps2refi.exe, where i = 1, 2, 3 and4), 1994.[4] p.w. shumate, "network alternatives for competition in the loop," supercom short course, march 22, 1995.[5] d. charlton, "through a glass dimly," corning, inc., 1994.[6] j. kraushaar, "fiber deployment updatešend of year 1993," fcc common carrier bureau, may 13, 1994.[7] "tenyear cable television industry projections," paul kagan associates, inc., 1994.[8] "sixth annual report on local telephone competition," connecticut research, 1994.[9] j. west, "building japan's information superhighway," center for research on information technology and organization, university ofcalifornia at irvine, february, 1995.[10] p.r. shumate, bell communications research, private communication, march 1995.[11] j.j. antonino (ed.), "undersea fiber optic special issue," at&t technical journal, vol. 74, no. 1, januaryœfebruary 1994.[12] f.j. janniello, r.a. neuner, r. ramaswami, and p.e. green, "muxmaster: a protocol transparent link for remote site backup,"submitted to ibm systems journal, 1995.the fiberoptic challenge of information infrastructures254the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.[13] f.j. janniello, r. ramaswami, and d.g. steinberg, "a prototype circuitswitched multiwavelength optical metropolitanareanetwork," ieee journal of lightwave technology, vol. 11, may/june 1993, pp. 777œ782.[14] w.e. hall, j. kravitz, and r. ramaswami, "a highperformance optical network adaptor with protocol offload features," submittedto ieee journal on selected areas in communication, vol. 13, 1995.[15] h. toba et al., "100channel optical fdm transmission/distribution at 622 mb/s over 50 km using a waveguide frequency selectionswitch," electronics letters, vol. 26, no. 6, 1990, pp. 376œ377.[16] g.k. chang et al., "subcarrier multiplexing and atm/sonet clearchannel transmission in a reconfigurable multiwavelength alloptical network testbed," ieee/osa ofc conference record, february 1995, pp. 269œ270.[17] r. ramaswami and k. sivarajan, "optimal routing and wavelength assignment in alloptical networks," ieee infocom94conference record , 1995.[18] r. leheny, "advanced network initiatives in north america," conference record, ofc95, march 2, 1995.[19] "four bocs denied authorization to cease providing dark fiber service," document cc505, fcc common carrier bureau, march29, 1993.[20] "dark fiber case remanded to fcc," u.s. court of appeals for the district of columbia, april 5, 1994.the fiberoptic challenge of information infrastructures255the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.33cable television technology deploymentrichard r. greencable television laboratories inc.the national information infrastructure (nii) is envisioned as having several components: an interconnectedweb of networks; information content, consisting of technical and business databases, video and soundrecordings, library archives, and images; the software applications necessary for users to access and manipulatethese stores of information; and the network standards that promote interoperability between networks andguarantee the security of information transmission. this infrastructure will potentially connect the nation'sbusinesses, schools, health care facilities, residences, and government and social service agencies through abroadband, interactive telecommunications network capable of transmitting vast stores of data at high speed.because information is a crucial commodity in an increasingly global service economy, the nii is of criticalimportance to the competitiveness and growth of the united states.the cable television industry is providing a significant part of the technological infrastructure needed tomake this telecommunications network a reality. this white paper discusses trends, predictions, and barrierssurrounding the deployment of an advanced cable television network architecture over the next 5 to 7 years. thisdiscussion lays the foundation for the subsequent consideration of the trends, projections, and barriers to thedeployment of new services over that advanced cable architecture.trends in cable television network deploymentnew technological developments within the cable industry are transforming the existing cable architectureinto a stateoftheart, interactive conduit for the nii. these developments are outlined in detail below.the evolution of cablecable television reached its current form during the mid1970s when the technology was developed thatallowed cable customers to receive satellite transmissions via the cable architecture that had evolved from itsbeginnings as community antenna television. this new delivery system enabled cable companies to offercustomers more channels than standard terrestrial broadcasting companies. cable then surpassed its originalmandate to bring television reception to rural or obstructed areas and became a means for delivering new typesof programming through specialty channels for sports, news, movies, home shopping, weather, and so on, andthrough payperview channels. cable television is a major video service provider, with 63 percent of all u.s.tv households subscribing to cable. as significant, 97 percent of u.s. households have access to cable facilities,making cable a nearly universally available telecommunications infrastructure.cable television historically operated through the technology of coaxial cable, implemented in a "tree andbranch" architecture. video signals, in analog format, from satellites, broadcast transmissions, and localtelevision studios are received or generated at the cable facility's head end, which serves as the point oforigination for the signals to be distributed to subscribers via coaxial cable. a trunk cable carries the signal fromthe headend to the feeder cable that branches from the trunk into local neighborhoods. signal amplifiers areplaced into thiscable television technology deployment256the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.coaxial cable network to prevent the signal from degrading over distance and producing noise or distortion.finally, a drop cable is run from the feeder cable into a subscriber's home and is attached to the television set.channel capacity for cable systems has grown from an average of 12 channels, mostly retransmission ofbroadcast signals, to an average of over 40 channels today. the number of cable subscribers served by systemswith 30 channels or more has doubled from 48.7 percent in 1983 to 95.4 percent in 1993. channels providednow include satellite delivered cable programming, and a variety of new educational, shopping, andentertainment networks. using this same architecture as a platform, cable companies are currently exploring theirrole in the nii by initiating new applications and offering access to other networks and resources, such as theinternet. the expansion of cable's role in the nii requires building on the foundation that was laid over the last20 years.the role of fiber opticsthe cable industry has been upgrading its coaxial cable infrastructure into a hybrid fiber optic/coaxial cable(hfc) network. cable companies have installed fiberoptic trunk lines to replace these major arteries of thecable architecture with wider bandwidth (higher capacity) links. optical fiber is constructed from thin strands ofglass that carry light signals faster than either coaxial cable or twisted pair copper wire used by telephonecompanies. it allows signals to be carried much greater distances without the use of amplifiers, which decrease acable system's channel capacity, degrade the signal quality, and are susceptible to high maintenance costs. withfurther upgrades, hybrid coaxial/fiber technology will also be able to support twoway telecommunications.therefore, a broadband cable network that is capable of delivering more channels as well as highquality voice,video, and data can be created without replacing the feeder and drop lines with fiber optic technology. this is thereason that the cable industry is perhaps the best positioned industry to deliver on the promise of the nii with areasonable and prudent amount of investment.cable companies began widespread installation of fiber technology into the trunk of the cable architectureduring the late 1980s. this use of fiber improved signal quality and lowered maintenance costs. in effect, fiberupgrades paid for themselves in terms of immediate cost reductions and service quality improvements. at thesame time, the installed base of fiber served as a platform for further deployment of fiber to serve new businessobjectives.in the early 1990s, cable further pioneered the installation of "fiber trunk and feeder" architecture in someof its markets. this approach runs fiber deeper into the network, segmenting an existing system into individualserving areas comprising roughly 500 customers. time warner provided a "proofofconcept" of this approach inqueens, n.y., with its 1gigahertz, 150channel system completed in 1991.this evolutionary step offered a number of benefits. backbone or trunk fibers may carry a multitude ofavailable cable channels out to fiber "nodes," and remaining coaxial cable to the home can carry a particulartargeted subset of the available channels. thus, customers may be presented with more neighborhoodspecificprogramming. penetration of fiber deeper into the network also reduces the number of amplifiers, or activeelectronics, remaining between the subscriber and the headend. in some designs, amplifiers may be entirelyeliminated, resulting in a socalled "passive" network design. removal of amplifiers considerably simplifies theuse of the coaxial cable for return signals from the home or office back to the headend and beyond. the portionof bandwidth reserved for return signals, usually in the 5 to 40mhz portion of the spectrum, is often subject tointerference and other impairments. any remaining amplifiers must be properly spaced and balanced, a laborintensive process that must by performed on an ongoing basis. other technical impairments are unique to thereturn path, and technical solutions must be optimized. these obstacles are the focus of current industry researchand product development. a cable television laboratories (cablelabs) request for proposals issued in the fallof 1994 has spurred a number of technology companies to accelerate the refinement of technology to addressreturn path issues. it appears that the full twoway capability of the coaxial cable, already installed to 90 percentof homes, will be fully utilized beginning in the next 12 to 18 months.full activation of the return path will depend on individual cable company circumstances ranging frommarket analysis to capital availability. there may be intermediate strategies employed by some of thesecompanies to speed the deployment of twoway or interactive services. such strategies might include alternativecable television technology deployment257the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.return paths offering support for asymmetric services, services that may require a narrowband information signalthat triggers delivery of broadband downstream information. narrowband returnpath options include wirelesstechnologies such as personal communication service (pcs) or use of the public telephone network for touchtone signaling or use of a narrowband modem.hybrid networks, then, are capable of delivering a variety of highbandwidth, interactive services for alower cost than fiber to the home, and still provide a graceful evolutionary path to full, twoway, broadbandcommunications. cable companies estimate that most subscribers will be connected to cable via fiberoptictechnology by 1996 to 1998.the transition to digital televisiondigital compression is another technological development that will vastly increase channel capacity inaddition to fostering interactivity and advanced services via cable. in contrast to current analog technology,which can collect noise (such as shadows or snow) during transmission over the air and through cable, digitalcompression technology delivers a signal sharply and clearly while employing a fraction of the bandwidth usedby analog technology. digital technology converts a video signal into a binary form that is stored in a computer,compressing signal information into a fraction of its original size while still permitting its easy transformationinto video signals for transmission. the result is that approximately 4 to 10 digital channels can be deliveredover the same bandwidth historically required to deliver 1 analog channel.thus, compression technology will enable cable customers to have a greater diversity of programmingoptions such as delivering niche programming to narrowly targeted audiences, expanded payperview servicesthat will rival the video rental market, multiplexing channels (carrying a premium movie service on severaldifferent channels with varying starting times), and highdefinition television. digital compression upgradesmake economic sense for consumers as well, since the converters necessary to decompress digital signals will beprovided only to those cable customers subscribing to these new services.the cable industry has led the development of digital compression technology, and standards for digitalcompression have been established. the cable industry's innovative work with digital television was spearheadedby cablelabs' 1991 efforts with general instrument corporation and scientificatlanta to form a cable digitaltransmission consortium, which emphasized cable's leadership role in the creation of digital transmission.technology. cablelabs has worked with the industry to foster convergence of digital coding and transmission forcable industry application, to provide technical support for the cable industry's work with the consumer,computer, home electronics, and entertainment software industries, and to cultivate awareness of digitalcompression technology. cable companies will be ready to commence the delivery of the technology afterhardware/software protocols are implemented. the industry is working to encourage its vendors to acceleratedevelopment, and cablelabs recently invented a universal analogdigital demodulation technique to enable theuse of equipment using different forms of modulation being used by different vendors.the cable industry also has looked closely at the impact of deploying digital video compression technologyin the real world. cablelabs conducted a 2year digital transmission characterization study, to (1) determine howwell cable systems can transmit highspeed digital data, (2) gauge the maximum bit rate that can reliably transmitcompressed video and audio, both ntsc and hdtv, and (3) identify the optimum modulation techniques forachieving three goals: maximum data rates, minimum data errors, and minimum costs for terminal equipment.this information has been distributed to vendors so that they understand the world in which their equipmentmust perform.advanced televisioncable's leadership in digital compression manifested early in the development of highdefinition television.this form of advanced display technology allows cable companies to bring subscribers a television picture withgreater clarity and definition than current transmission standards permit. in the nii, new forms ofcable television technology deployment258the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.information and video display will be just as important as new and expanded transmission media, andtelecommunications companies must design networks that support such display technology.the federal communications commission convened a committee in 1987 to develop a broadcast standardfor highdefinition video transmission. although japan's nhkmuse analog solution was favored owing to itsearly deployment in japan, cable industryled efforts proved that a digital solution was a better choice for reasonsof flexibility and efficiency. since that time, cable industry representatives have worked with the fcc'sadvisory committee on advanced television services to develop an industry digital compression standard. thecable industry presently has the broadband capacity to transmit hdtv; in fact, several cable systems in theunited states and canada already have experimented successfully with the delivery of hdtv in severallocations, and cablelabs is working on further testing of hdtv standards of transmission.regional interconnectionan important architectural and economic component of cable's ongoing evolution is the construction ofregional fiber optic networks to link headends and ''regional hubs," so that cable operators in the same region canshare services with one another in order to eliminate headends. capital intensive network components, such asvideo storage, signal compression, or advertising insertion, may be shared among operators in a regional hub,thereby permitting operators to offer more services to customers. beyond the economic benefits, regional hubdesigns allow cable operators to interconnect with other telecommunications services so that cable can providevideo, audio, and textual data to homes and businesses from a variety of sources, and subscribers can request thedelivery of specified services (such as electronic newspapers, home shopping, or video teleconferencing).regional hub systems are being built in san francisco, denver, central florida, boston, long island, andpennsylvania, among many other markets.interconnection of cable headends with each other and with other types of networks raises issues ofinteroperability. cable already has the incentive to work toward standards for video transmission and otherservices in order to link cable systems together. such standards must be extensible to other types of networks andhave global compatibility as well. both the cable industry and its competitors acknowledge that interoperabilityis critical to successful deployment of, and access to, the nii; thus, there is a great incentive for industries tocooperate to arrive at standards and otherwise foster open networks.for example, cablelabs has tested the mpeg2 (moving picture experts group) standard for compressionand decoding of digital video and audio bit streams, which allows software, hardware, and network componentsfrom different manufacturers to communicate with one another. the mpeg2 standard will likely beimplemented in 1995. cable is also active in the atm forum, an international consortium chartered to acceleratethe use of atm products and services. atm refers to a cell switching technology featuring virtual connectionsthat allow networks to efficiently utilize bandwidth. the cable industry views atm as a technology with greatlongterm potential, and the atm forum is a useful venue to discuss interoperability specifications and promoteindustry cooperation. the atm forum is not a standards body but works in cooperation with standards bodiessuch as ansi and ccitt. and finally, cablelabs has taken a leadership role in the digital audiovideocouncil (davic) that was recently created to promote interoperability among emerging digital audiovisualapplications, such as video on demand, particularly in the international marketplace. this interoperability amongtechnologies will ensure that technological development will be less costly, that the free flow of information ispromoted, and that the nii will be brought to consumers more quickly.information technology convergencecable is in the thick of communications and information technology convergence activity. as part of theongoing development of cable technology applications, the cable industry is launching initiatives with thecomputer industry to combine highcapacity transmission with the latest developments in software technology.for example, software developments will enable cable carriers to provide the "road maps" customers will need tocable television technology deployment259the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.navigate the information superhighway (such as interactive program guides). other developments will allowcable to offer expanded services, such as teleconferencing, transaction processing, and home shopping. intel andgeneral instrument, in conjunction with telecommunications, inc. (tci), and rogers cable systems of canada,are working to create a cable delivery system that is capable of sending data to personal computers at speeds thatare up to 1,000 times faster than today's modems. intel is also developing services tailored to the cable market,including multimedia online services, personal travel planning, software distribution, and internet access, whichwill allow the personal computer to become a powerful communications tool in the foreseeable future.elsewhere, silicon graphics is playing an integral role in time warner's full service network in orlando, fla.and microsoft recently announced the demonstration of its server software architecture called tiger for thedelivery of continuous media, such as voice and video on demand. tiger, which is deployed in a cable system'sheadend and in software for inhome receivers, is being tested by rogers cable systems limited in canada andby tci in seattle.outlook for future deployment of cable television architecturecable companies plan to invest about $14 billion to institute equipment and plant upgrades through the endof the decade. some cable companies now predict that the construction of fiber/coaxial hybrid networks will becomplete between 1996 and 1998. upgrade costs for this hybrid network are relatively low since cable'sbroadband coaxial cable to the home is already in place, making the total investment in broadband/digitaltechnology cheaper for cable than for competitors, such as telephone companies, to develop. the cost of mediaservers (the digital storage devices that cable systems will use to handle simultaneous requests for data, voice,and video services to the home) and of settop boxes or home terminals that consumers will use to accessmultimedia and other digital services are relatively high at present, but will decline as production increases. settop boxes with digital decoders, which will bring such services as movies on demand and onscreen programguides to the home, are expected to be widely available in 1996 to 1998. costs of servers and switchingtechnology that could bring advanced services such as true video on demand or video telephony are expected tocontinue to fall and will likely be incorporated into cable architectures around the end of the decade.today, fiber nodes are being installed, upgrade planning is in progress, regional hubs are being developed,and a number of cable msos are conducting nearvideoondemand, videoondemand, or full service networktrials, in places like orlando, fla., omaha, nebr., and castro valley, calif. satellitebased video compression,nearvideoondemand services, and some first generation interactivity, such as customer navigation or programselection, will be deployed in 1995 and 1996.by 1997 and 1998, 750mhz upgrades should be complete in many areas, with that capacity likelyallocated to 78 analog channels and the equivalent of 100+ digital channels. also in this time frame, nearvideoon demand will be a mature service, second generation interactivity will begin to appear, and deployment oftelecommunications services such as telephony, personal communication service (pcs), highspeed data, andperhaps video telephony will begin. by 2000, broadband, fullservice networks will be widespread, featuringatm/sonet technology, media servers used to provide true video on demand, full motion navigation tools,and advanced television services.barriers to future deployment of cable network architecturethe ability of cable companies to make necessary investments in network upgrades and new technologymay be affected by regulatory issues currently being debated within congress. the telephone companies' stateregulated monopoly over local telephone service is a primary hindrance in the development of a competitivetelecommunications marketplace. moreover, the rate reregulation mandated by the cable act of 1992 hasamounted to a loss of $2 billion in cable industry revenue through 1994. capital formation critical fortechnological development requires a stable and competitively neutral regulatory environment. the cableindustry's development of the hybrid fiber/coaxial network has nevertheless resulted in cable's increased ability tocable television technology deployment260the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.act as a competitor with telcos since the hybrid system provides a higher capacity than phone companies' twistedpair wire. legislation is being debated in congress that explores the possibility of eliminating the barriers toentry into local cable and telco markets and preempting state regulations that deter phone companies frommaking their network available for use by the cable industry. regulatory reform efforts are under way in manystates as well. stateimposed restrictions on competition within local telecommunications markets must be liftedso that cable companies can develop competitive services on a national scale; otherwise the informationinfrastructure may not become national in character. moreover, competition must be promoted so that consumerscan choose among service providers and so that diverse services will be available.trends in telephony deploymentthis discussion has described the historical and prospective evolution of cable television architectures todeliver entertainment and educational video. this deployment has reached a watershed in that evolution, as theexisting cable platform becomes capable of delivering a range of new services, including competitive telephonyservices.telephone service has historically been a monopoly service, but this situation began to change with theopening of the old bell systems as early as the 1950s. by the 1970s, longdistance markets had becomecompetitive, and technological and regulatory forces led to the divestiture by at&t of its local telephonecompanies. in the 1980s, further advances in technology led to the birth and rapid growth of competitivetelecommunications companies, called competitive access providers (caps), that targeted network accessservices connecting local telephone customers to interexchange carriers. such competitive options were initiallylimited to large business customers, but the trend has led inexorably to the need to open up the local loop tocompetition. this trend has led a number of cable television providers to recast themselves as fullfledged cabletelecommunications companies. several cable companies operate competitive access subsidiaries, and aconsortium of cable companies now owns teleport communications group, one of the two largest competitiveaccess companies.outlook for deployment of telephony servicesmany cable companies are assessing their technological and financial capabilities for competing in thetelecommunications business. the advanced capabilities being provided by the evolving cable architecture willprovide a platform capable of providing telephony services. a request for proposals (rfp) fortelecommunications service was issued by cablelabs in 1994, and the rfp served to announce the intent of sixleading cable companies to buy up to $2 billion worth of hardware and software. this equipment would enablecable operators to provide telephone service to residential and business customers over cable television hybridfiber/coax networks. the rfp has focused vendors on devising affordable answers to issues of reliability andbandwidth management of multiple services over the same hfc network. further, cable companies are fullyaware of the particular demands of lifeline telephone service, and the rfp stipulates requirements for fullnetwork reliability.regional hub evolution has already led a number of cable companies to deploy class 5 telecommunicationsswitches. continental cablevision in new england, cablevision systems on long island, and time warner inrochester, n.y., and orlando, fla., are among the companies that have done so, aggregating a large number ofcustomers on a regional basis in order to share costs.cable companies telecommunications, inc., comcast, and cox cable formed a consortium with sprint in1994 to provide telecommunications services nationwide. the consortium's initial action was to participate inpcs spectrum auctions, but plans also commit the companies to invest in network upgrades and to providecapital to other cable companies that may wish to upgrade plant in order to participate in the telecommunicationsventure.time warner in rochester, n.y., may be the first cable company to provide headtohead competition witha local telephone company. regulatory actions have cleared the way for competition, and time warner hascable television technology deployment261the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.devoted resources to upgrading its hfc network in rochester and installing a complete service managementsystem accommodating backoffice functions such as billing.personal communication servicepersonal communication service, or pcs, is a form of wireless twoway communication using a lightweighthandset similar to cellular telephone technology. this technology is compatible with existing cable networkarchitectures. using microcells that transfer a user's call from one cell to another as the user travels, pcs uses thecable industry's fiber optic backbone and feeder plant to interconnect the cells, and thus cable has a builtinadvantage over other potential providers by virtue of an existing infrastructure. pcs will distinguish itself fromcellular telephone service primarily by its lower cost and greater convenience. more than 26 cable companiesreceived fcc approval to test pcs technology. cox cable received the fcc's "pioneer's preference" license forspectrum space to explore pcs systems, and time warner has successfully completed testing in orlando, fla., ofpcs technology and its interconnection with the cable infrastructure. the sprinttcicomcastcox consortiumsuccessfully bid for licenses covering 29 of the top 50 markets, covering 180 million pops (points of presence),making it the largest winner in the recent pcs spectrum auction.barriers to deployment of telephony servicescurrent modes of telecommunications regulation hamper cable efforts to enter the telephony business. inparticular, a number of states still prohibit or hinder any competition to the entrenched local telephone companymonopoly. the cable industry is seeking to safeguard competition by careful removal of regulatory restrictions.the cable industry is supporting telecommunications reform proposals currently before congress. theseproposals will clarify the rules governing the development of the nii. ultimately, competition will best stimulatedevelopment of new technology and services; furthermore, domestic competition will best build u.s.competitive strengths.current legislative proposals can provide a rationally managed transition to foster facilitiesbasedcompetition for telephone service. permission to enter the business must include provisions for reasonable accessto unbundled elements of the public switched telephone network, as it is in the public interest for interconnectionto be as fair and as seamless as possible.the ability to make decisions in a competitive environment in turn will stimulate appropriate investments inr&d and technology deployment. presumably, appropriate allowances can be made for joint ventures andmergers, especially for small companies serving lower density areas, to permit the capital formation necessaryfor building a truly national information infrastructure.apart from regulatory barriers, cable companies have recognized that providing lifeline and othertelecommunications services requires a high standard of customer service. the cable industry in 1994 initiated acomprehensive customer service program including "ontime" guarantees and other elements that it recognizesare crucial elements in playing its part in the nii. further, the industry is implementing a new generation ofnetwork management and business support systems that are an integral part of a multiple service endeavorproviding transactional and other twoway services. in fact, cable is deploying stateoftheart systems fortelecommunications that give it an advantage over existing telephone companies, which are hampered bydependence on "legacy" network management and business support systems.trends in deployment of personal computerbased servicescable companies have historically carried a number of data services. these have ranged from news andweather feeds, presented in alphanumeric form on single channels or as scrolling captions, to the x*pressinformation service, a oneway transmission of data over classic cable systems. x*press has transformed itself inrecent years, forming a multimedia educational service called ingenius in order to respond to the ongoing changescable television technology deployment262the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in the cable infrastructure, which now supports expanded capabilities. x*press/ingenius is now joined by a hostof information providers targeting cable as the delivery mechanism of choice for advanced data services.these changes parallel those taking place in the commercial and residential data transmission markets. thepc explosion of the 1980s was rapidly followed by leaps in computer networking technology. more and morepeople at work and at home were becoming familiar with computer networking, ranging from commercialservices such as compuserve or prodigy to the wideranging global internet. increased awareness has led toincreasing demand for service, and for enhanced levels of service. cable is in a unique position to meet thesedemands.the same highly evolved platform that enables cable to provide telephony also supports highspeed dataservices. here, cable's high bandwidth pipeline offers a strong advantage in delivering useful data servicesaccessible by residential and commercial customers over their personal computers and other data terminals.information servicescable technology can provide customers with access to a variety of information services, including catalogshopping, financial data, and household billpaying. because cable systems have a higher capacity for data,audio, and video information, cable subscribers can send and receive information services through theircomputers at a much faster rate than traditional telephone lines offer. one such service, ingenius executive, isoffered by cable systems across the country and permits subscribers to connect their personal computers to theircable, using a software interface to receive information services via their computers without tying up theirtelephone lines. ingenius executive subscribers may access news, sports, and weather information as well asstock market and consumer information. ingenius also operates a service called x*change, which providessubscribers with a news feed from journalists in more than 20 locations throughout the world, as well as othereducational data. the online service prodigy is offering similar services in several markets across the countryvia media general, cox cable, viacom, and comcast. some of the informational features available throughprodigy over cable include reference databases that offer consumer reports, restaurant reviews, and politicalprofiles; children's features such as sesame street, national geographic, and the academic americanencyclopedia; stock quotes and charts of key economic indicators; travel services; bulletin boards; shopping; andlocal community information. several continually updated videotext services are available via cable as well, suchas upi datacable, which features international and national news, financial reports, and weather, or cnbc, acommercialfree version of the cable channel's programming combined with financial presentations to corporateclients.companies may take different or evolving approaches to online service access. for some applications,customers may be accessing information stored on cdrom databases at or near the cable headend or regionalhub. some forms of information, such as encyclopedias, are particularly suited for such an approach. this mayserve as a transitional approach until wide area cable interconnections are in place to allow information accessfrom any remote sites. some forms of frequently updated material may require such networked access.internet accessin addition to these information services, upgraded cable networks are now able to provide high capacityaccess to the internet to customers with home computers. with a cable connection to the internet, businesses andconsumers can pay a flat monthly fee for access and can receive electronic mail, access to usenet discussiongroups, ability to connect to computers around the world via telnet, and access to information archives throughfile transfer protocol (ftp) and gopher. this option was recently offered to continental cablevision customers incambridge, mass., in a joint project with performance systems international, inc. subscribers receive internetaccess for a monthly fee through a cable converter box that does not interfere with cable television service andworks at speeds hundreds of times faster than telephone modem calls. the halfmegabit per second cable linkallows customers to download large data files in a fraction of the time it takes over the telephone, and it evencable television technology deployment263the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.provides enough bandwidth to watch videos over a personal computer. cable's internet gateway not only allowscustomers faster access to the internet but also a greater variety of creative applications of internet multimediafeatures.lan interconnectionsome larger businesses presently use cableprovided facilities to link their local area networks to transmithigh volumes of data. the evolving cable network architecture will soon enable smaller businesses to benefitfrom advanced telecommunications services. tci and digital equipment corporation are working jointly todevelop new business telecommunications applications using tci's hfc facilities and dec's computernetworking (ethernet) technology. cable's high bandwidth infrastructure combined with dec's computernetworking technology will allow remotely located lans to share computer resources and data. cablenetworking technology also will enhance opportunities for product design and manufacturing as well as scienceand engineering research. for example, times mirror cable, dec, and arizona state university have tested abroadband metropolitan network called economic commerce net (ec net) that supports manufacturingapplications for aerospace businesses in the phoenix area. ec net allows these businesses to collectivelyimprove manufacturing by offering desktop videoconferencing on manufacturing processes, a computer aideddesign (cad) tool that permits remote businesses to view and manipulate designs simultaneously, and amultimedia storage and retrieval facility for data, video, purchasing specifications, and other information.cablecommutingthe cable industry's hfc infrastructure has the potential to increase the already popular notion of workingfrom home via the information superhighway. approximately 8 million americans already work through someform of telecommuting, and cable's highvolume, highspeed broadband technology will allow millions more to"commute" to work through cable. this technology will lessen the burden of commuting for both businesses andtheir employees who may be geographically isolated, physically disabled, or single parents. tci is currentlytesting a cablecommuting project that will allow its denver area customer service representatives to receivecustomer calls at home. tci is also testing a service, developed by hybrid networks, inc., in the san franciscoarea that transmits highspeed data over a standard cable channel and lowspeed data through telephone lines tocreate an interactive corporate/home network. in addition, technologies such as video teleconferencing and highspeed fax transmission that will be delivered via cable in the future will enhance cable commuters' options.research supportthe cable industry's broadband architecture will allow researchers in a variety of fields to share advancedcomputing applications or scientific instruments from remote locations, resulting in comprehensive research at afraction of current costs. one such network is being offered by cablevision systems in the new york citymetropolitan area. fishnet (fiber optic, islandwide, super highspeed network) is a highcapacity fiber opticnetwork that links the state university of new yorkstony brook with the brookhaven national laboratory andgrumman data systems. the system has been used to revolutionize the efficient use of medical imaging anddiagnostic techniques as well as to develop modeling procedures for the transport of contaminants in groundwater.outlook for pcbased applicationspersonal computers continue to penetrate the home market. moreover, most home pcs today include a cdrom attachment. this indicates strong demand for contentbased services running over pcs. cdroms,however, may be a shortterm solution. networkbased content servers interconnected via cable company highspeedcable television technology deployment264the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.links are a more efficient way for customers to access updated databases, news, and other forms of information.further, data networking has been an explosive growth area within information technology in recent years.business applications require more and more bandwidth, especially in interconnecting disparate lans, and cableis the provider with the greatest capacity to meet the mushrooming demand for bandwidth.barriers to deployment of pcbased servicesthere appear to be very few barriers to cable deployment of highspeed data transmission. the cableplatform is steadily evolving to a hybrid digital and analog transmission system. data modems already exist, andimproved models are under development by such companies as intel, lancity, zenith, general instrument, andhybrid networks, inc. modem prices are dropping precipitouslyšlancity announced in may 1995 that its 10mbps cable modem was priced at $595, compared to several thousand dollars just 8 months earlier. return pathissues that were mentioned previously in connection with cable plant upgrades also will need to receive attentionin the context of pcbased services. customers may not desire totally symmetrical data communication,particularly residential user, but the return path still must be reliable. bandwidth demand for return signals maybe very dynamic, requiring cable systems to be able to allocate frequency in an agile manner. upstream,broadband transmission over cable is expected to be fully supported within 5 years.trends in the deployment of interactive televisionthe convergence of telecommunications and computer technology is transforming american society. in thecable industry, the convergence of cable's highspeed transmission capability and computer hardware andsoftware intelligence is not only enabling cable to deliver telephony and highspeed data services, but is creatingnew opportunities in entirely new forms of entertainment, education, health care, and many other areas. cablecompanies, such as time warner in orlando, have now deployed operational systems actually delivering suchservices.video on demanddigital compression technology, which enables cable companies to offer a greater number of channels aswell as interactive capability, ensures that video on demand will be part of the information superhighway'sfeatures. this service allows customers to select from a range of movies, sporting events, or concerts for viewingat their convenience. many cable operators, such as paragon cable in san antonio, are already offering the lesscostly option of nearvideo on demand, which allows customers to view programs with start times every 15 or 30minutes. true videoondemand systems are currently being tested in omaha, neb., by cox cable; in yonkers,n.y., by cablevision systems; in orlando, fla., by time warner; and in littleton, colo., by tci. a specificservice, your choice television (yctv), packages and delivers television programs on demand. for about $1per program, viewers can order a weekly program up to a week after it airs and a daily program the day after itairs. yctv has been tested in eight markets, including continental cablevision in dayton, ohio, and tci inmount prospect, ill.interactive entertainmentwith the pioneering technology that allows interactive capability, many new information and entertainmentoptions are being created within the cable industry. for example, the actv network, which was tested viacontinental cablevision in springfield, mass., and is now being offered to subscribers in montreal, allowsviewers to alter the content of the tv screen during the course of a program. viewers can call upcable television technology deployment265the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.statistical information on players during sporting events, watch a synopsis of the daily news headlines, andchoose the stories on which to receive detailed reportage, or even change the pace of an exercise program.interactive commercials will soon be available on actv so that customers may receive more information ordiscount coupons for particular products or services.education and distance learningcable technology has enabled the development of interactive educational applications and distance learningthat improve not only the way students learn but also the way teachers teach. jones international's mindextension university has been offering college courses via cable television for years, and cable companies areexpanding educational opportunities by building networks devoted to education. students can now learn fromnational experts or students in different cities, access online libraries from around the world, or take "virtual fieldtrips" to museums while remaining in the classroom. one such interactive learning pilot project, called theglobal laboratory project and currently run by continental cablevision and the national science foundation,connects students in cambridge, mass., with those from 27 other states and 17 foreign countries to exploreenvironmental problems. the students monitor climate change, pollution, and ultraviolet radiation and share theirdata among themselves and with scientists to gain a global perspective on the environment.as part of its effort to expand learning opportunities through the use of cable technology, the cable industryhas initiated a nonprofit program to provide schools with basic cable service, commercialfree educationalprogramming, and teacher support materials. the project, cable in the classroom, serves nearly 64,000 schoolsand supplies more than 525 hours of commercialfree programming for educators each month. highlighting theindustry's role in distance learning, continental cablevision now operates an interactive learning network calledthe cable classroom in enfield, conn. it permits teaching in one school district and simultaneously offering toseveral others advanced math and language classes that would otherwise not be offered because of lowenrollments. because the system uses two channels, the teacher can also view the students and home cablecustomers can change channels to watch either the students or the teacher. the cable classroom also offersteachers professional development by conducting interactive meetings with teachers in four connecticut schooldistricts. in another example, the ohio cable foundation for education and technology is promoting distancelearning through a range of applications throughout the state. for example, tci in zanesville provides dataretrieval services and other multimedia distance learning applications.interactive program guides and navigatorsbecause of the expected explosion in future cable programming and service choices, customers will seekgreater choice, control, and convenience with regard to their viewing environments. these needs will spurdemand for a personalized form of channel navigation. several interactive subscriber guides are being developedand tested. one of these onscreen guides, starsight, is available in castro valley, calif., through viacom cable.time warner has developed its own navigation system for the orlando full service network.interactive shopping and advertisingcompanies like time warner are testing interactive shopping services, such as shoppervision, that enablecustomers to see and evaluate products over interactive video catalogs before purchasing. immediate orderingand the requisite billing and payment mechanisms are integrated as well.interactivity will permit cable subscribers to request consumer information on businesses, products, andservices at the touch of a button. real estate advertisements have been among the first to make an impact in theburgeoning marketplace. a program called home economics, available throughout new england viacontinental cablevision, permits viewers to request specific information about homes. moreover, an interactivechannel devoted to classified real estate advertising is in the works; the real estate network will providecustomers withcable television technology deployment266the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the ability to view real estate in fullmotion video and to access details on contractors, mortgage rates, andlending institutions.gamesthe sega channel, a joint venture between time warner and tci that offers a library of sega video gametitles for download through cable systems, is being tested in several markets. subscribers use a "tuner/decoder"cartridge that tunes in the channel, provides a menu, and downloads a game choice. scientificatlanta andgeneral instrument corporation are presently manufacturing adapters that will permit their settop boxes to serveas vehicles for the sega games. multiplayer games are also a possibility across full service networks such astime warner's orlando system.health carethe cable industry's bridge to the information superhighway has important consequences for newdevelopments in health care. through the use of telecommunications technology for medical applications,doctors can consult with researchers or specialists in other locations, conduct video seminars throughout thecountry, consult visual information from remote libraries or medical centers, and share medical records orimages such as xrays. paragon cable in portland, ore., uses its institutional network to transport a telemedicineor "cablemedicine" network that connects 12 medical facilities and schools throughout the city to provideinformation from video conferences held across the united states. cable also has the potential to deliver astandardized means of electronic health insurance claim filing, and personalized health information systems arebeing tested that use terminals in the home to connect consumers to a health care databank that advisescustomers about self care based on their personal medical records.outlook for interactive televisionthe preceding discussion outlined a variety of potential interactive services that may be provided overcable. the range of experiments, both technical and market trials, appears to bode well for the flexibility of cableto provide the services. there have been no technology hurdles discovered in making the cable systems performat a level needed to support interactivity. consumer demand for such services over the next 5 to 7 years is thewild card for all prospective services. but early cable trials point to reasons for optimism. tci's trial in a denversuburb compared near video on demand (offering a small selection of hit movies starting every 15 minutes) tovideo on demand (offering a large library of movies, starting immediately on demand). indications were thatcustomers are receptive to increases in choice and convenience. the initial reports on the new dbs services alsoindicate increased buy rates from customers given a larger menu of programming from which to choose. thus,along the dimension of upgraded entertainment services with elements of interactivity in terms of navigation andcontrol, it appears that consumer demand will support growth of such services over the next 5 to 7 years.more advanced levels of interactivity also appear to make sense. the same desire for choice andconvenience appears to be driving early success for the sega channel, which delivers video games to the home.the availability of more video game choices in the home is attractive, and one might assume further that addingelements of interactivity, such as multiplayer games, would drive increased customer acceptance.in the area of shopping and related services, many providers are optimistic about market prospects.electronic commerce over the internet is spawning a flurry of activity by companies trying to perfect a securemeans of conducting business over electronic networks. cable's broadband capability appears to providesignificant enhancement to such commerce as a natural showcase for products through video catalogsincorporating customer interaction.as noted above, the cable platform supporting such interaction is likely to be widely available within thenext 5 years.cable television technology deployment267the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.barriers to deployment of interactive televisionbarriers to interactive services may manifest as financial concerns or as marketing uncertainties. as withdata services, interactive services will take advantage of the inplace cable platform capable of flexible, highcapacity support. deployment of interactive services will require only incremental investments, largely in thecustomer premise equipment necessary to plug into the cable platform. such an incremental approach willminimize financial risk given remaining uncertainty about the potential size of the market. as noted above,technology and market trials give reason for optimism. nevertheless, cable's evolutionary approach offerswelcome security in the event that forecasts are too optimistic.conclusioncable television companies operate a highly evolved platform capable of delivering a variety oftelecommunications and information services. additional technology components that will enable particularservices contemplated for the national information infrastructure to run over that platform appear to be comingon stream at reasonable time frames and cost levels. the evolutionary nature of cable allows cable companies toinvest incrementally and cost effectively only in those technologies that serve clearly defined needs or marketdemands. there are no significant technological or financial barriers to continued deployment of cablearchitecture. however, regulatory uncertainty still remains problematic.addendum1. for your company or industry, over the next 10 years, please project your best estimate of scheduledconstruction of new broadband facilities to the total residential and small business customer base, in 2year increments.the cable industry already provides broadband facilities to 64 percent of television households, and makessuch facilities available to 96 percent of television households. cable facilities are available to approximately 90percent of business locations as well, although businesses have not historically subscribed to cable. beyond thislevel of service, cable companies are currently upgrading existing systems by migrating fiber optics deeper intonetworks, creating individual service areas of roughly 500 homes. this expands available bandwidth to around750 mhz. the deployment of such upgrades may develop as follows:199525 percent199765 percent199980 percent200590 percent2. over the same period, please project dates when each family of service will first become available, andits subsequent penetration of the total base, again in 2year increments.telephonycapabilities for providing wireline telephony over cable will track the progression of cable system upgradesas described in the response to question 1. telecommunications switch deployment is under way as systemsmigrate to regional hub designs. several cable companies have formed partnerships with sprint and othercompanies to provide a full range of telecommunications services, including wireless personal communicationsservices. actual penetration of such cableprovided services in a competitive market is difficult to project. in theunited kingdom, cable operators offering local telephony service have reported selling telephony services toover 20 percent of homes where the service is available, indicating a significant market for competing providersof telecommunications.cable television technology deployment268the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.pcbased applicationsdigital, pcbased applications may be rolled out almost immediately, based on cable's broadband pipe intothe home and the development of affordable cable modems, which are probably less than a year away. alternatereturn paths such as telephone lines could be used in a hybrid approach. availability of a broadband return pathfor such services will follow the progression of cable system upgrades as described in the response to question 1.penetration of such services is difficult to project, particularly in the face of phenomenal growth rates in internetconnectivity.interactive television 1996šfirst generation interactivity (navigation or program selection), satellitebased digital compressionfor near video on demand. 1998šearly deployment of second generation interactivity, such as multiplayer video games. 2000šdeployment of full service networks, enabling serverbased true video on demand, full motionnavigation tools, video telephony types of services.again, it is important to note that interactive services may be deployed on an incremental basis matched tocustomer demand, once an upgraded infrastructure is in place. thus, availability of these services will closelytrack the progression of cable system upgrades as described in the response to question 1. evidence from earlyinteractive field trials indicates a definite customer preference for increased choice, convenience, and control bythe customer.3. please outline the architecture(s) that will be used to build this broadband plant.this question is answered in the foregoing white paper.4. please outline the peak switched digital bandwidth (in kbps or mbps) available to an individualresidential or small business user when you launch broadband service, and how that bandwidth canevolve to respond to increased peak traffic and to new, highcapacity services (which may not now exist).cable's existing bandwidth to the home, combined with upgrades described previously, will reach 750mhz. using 64 qam digital modulation, this implies a total digital bandwidth of close to 3 gigabits per second.allowing for preservation of existing analog services, available bandwidth would be closer to 1.3 gigabits persecond. current industry plans call for reservation of the 5 to 40mhz portion of the spectrum for the returnpath use by customers for upstream transmission. using qpsk or 64 qam digital modulation, 35 mhz couldhandle anywhere from 50 mbps to 135 mbps. thus, the amount of raw bandwidth available over cable issubstantial.cable modems under development call for effective bandwidths up to 27 megabits per second per 6 mhzchannel. the actual amount available depends upon usage within the roughly 500 home nodes describedpreviously. assuming subscriber penetration of 60 percent gives 300 potential users. for pcbased services, onemay further estimate that 40 percent of homes have pcs (numbers are roughly similar for cable or noncablehomes). further, less than 20 percent of pc homes have pcs with modems, so that the range of potential dataservice users may fall anywhere between 8 percent and 40 percent. assuming growth in pc penetration, we mayassume that 33 percent of subscribers are potential data transmission customers, or 100 homes. and probablepeak usage will be less than 33 percent, so that one might expect 33 simultaneous users. thus, close to 1 mbpswould be available given the above assumptions. of course, data are packetized so that individual customerswould use considerably less than a full megabit per second at any given time.this is a lower limit. one might initially increase capacity by allocating additional 6 mhz channels ondemand. cable companies may migrate fiber further into distribution networks, creating even smaller nodes andexpanding bandwidth from 750 mhz up to the feasible maximum of 1.2 ghz. cable companies are researchingdynamic bandwidth allocation techniques allowing the flexibility to meet demand. digital modulation techniqueswill also continue to allow greater amounts of data to be transmitted over existing bandwidth.5. please project the capital investment you or your industry plan to make on a perhomepassed basis toinstall broadband infrastructure, and on a persubscriber basis to install specific services.the cable industry plans to spend $28 billion over the next 10 years on plant and equipment upgrades(according to paul kagan associates). this works out to approximately $460 per subscriber. upgrades to evolvecable systems tocable television technology deployment269the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.multipurpose platforms cost in the range of $100 to $150. costs are rapidly dropping for cable modemsnecessary for pcbased data services, and should reach the $200 to $300 range in the next year or two. capitalinvestment for such services is incremental, and will be matched to new revenue streams, which may in turnexpand capital available for further technology deployment. costs for telephony service over cable are difficultto quantify because they are highly dependent on penetration assumptions. interactive television applicationsmay require server and other technology costing as much as $500 per subscriber when widely deployed. again,investment will be incremental and matched to new revenue streams.6. please respond to the concerns raised in vice president gore's letter regarding the ability of users of yournetwork to originate content for delivery to any or all other users, versus the control of all content by thenetwork operator.the cable platform is steadily evolving to a hybrid digital and analog transmission system. data modemsalready exist, and improved models are under development by such companies as intel, lancity, zenith,general instrument, and hybrid networks, inc. return path issues that have been raised in connection with cableplant upgrades do not appear to present obstacles to customer generation of content. bandwidth demand forreturn signals may be very dynamic, requiring cable systems to be able to allocate frequency in an agile manner.upstream, broadband transmission over cable is expected to be fully supported within 5 years.7. please specifically enumerate the actions that you or your industry believe that the federal governmentshould take to encourage and accelerate the widespread availability of a competitive digital informationinfrastructure in this country.current modes of telecommunications regulation hamper cable efforts to enter the telephony business. inparticular, a number of states still prohibit or hinder any competition to the entrenched local telephone companymonopoly. the cable industry is seeking to safeguard competition by careful removal of regulatory restrictions.the cable industry is supporting telecommunications reform proposals currently before congress. theseproposals will clarify the rules governing the development of the nii. regulatory relief from some cable pricingregulation will permit rational capital investment. ultimately, competition will best stimulate development ofnew technology and services, and further, domestic competition will best build u.s. competitive strengths.current legislative proposals can provide a rationally managed transition to foster facilitiesbasedcompetition for telephone service. permission to enter the business must include provisions for reasonable accessto unbundled elements of the public switched telephone network, as it is in the public interest for interconnectionto be as fair and as seamless as possible.the ability to make decisions in a competitive environment in turn will stimulate appropriate investments inr&d and technology deployment. presumably, appropriate allowances can be made for joint ventures andmergers, especially for small companies serving lower density areas, to permit the capital formation necessaryfor building a truly national information infrastructure.cable television technology deployment270the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.34privacy, access and equity, democracy, and networkedinteractive mediamichael d. greenbaum, bell atlanticdavid ticoll, alliance for converging technologiesnetworked interactive media have already wrought irreversible changes in the way we live, work, andeducate ourselves in america. this transformative set of technologies, like others before it, is the focus of broadhopes and boundless anxiety. for some, it is a singular force for goodšempowering the individual, unlockinghuman potential, and ushering in a new era for wealth creation in a networked economy. for others, it raises darksuspicionsšas a tool for control or an enabler of fringe elements in our society.at the very least, the new interactive media raise serious issues relating to individual privacy, access andequity for the underprivileged, and, ultimately, the impact of these media on the evolution of democracy.the creation of a new interactive media infrastructure represents yet another ''westward migration" forpioneering americans. like all frontier environments, the boundaries and structures of the interactiveinformation and communications frontier are still very fluid. as such, efforts to "civilize" it through the force ofregulation should be tempered with caution. past lessons of success and failure argue that we embrace theinevitability of change with our eyes wide open.the development of too many regulations at this nascent stage of development would dramatically slowinnovation and deployment of this new public infrastructure in the united states.the challenge ahead is threefold: (1) to deliver the promise of these emerging communications andinformation products and services to the consumer with an element of individual control over the collection,distribution, and use of personal information; (2) to achieve reasonably broad public access to new media andinformation content, particularly with regard to education and training opportunities; and (3) to develop thisinfrastructure to its fullest as a force for individual development and expression and the public welfare.we recommended a coalition of consumer, business, and government interests to engage in a constructivedialogue and work toward the development of guiding principles for privacy, access and equity, and democracy.as a disciplined, selfregulating body we will see an interactive information and communications infrastructureevolve as a major force for economic development and individual opportunity within this generation.general contextšthe electronic frontierharold innis, an early critical voice in the electronic media age and teacher of marshall mcluhan, pointedout that new media have precipitated political change throughout history, shifting power toward the citizenry."monopolies or oligopolies of knowledge have been built up – [to support] forces chiefly on the defensive butimproved technology has strengthened the position of forces on the offensive and compelled realignmentsfavoring the vernacular."note: the ideas expressed in this paper are those of the authors and not necessarily those of their employers.privacy, access and equity, democracy, and networked interactive media271the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.libraries based on clay documents enabled priests to monopolize knowledge in ancient babylon. papyrusscrolls supported the limited democracy of greek citystates, and the rule of law in ancient rome. paper and theprinting press, used to reproduce texts in the language of the common people, precipitated the reformation, theend of feudalism, and the emergence of parliamentary democracy, capitalism, and the industrial revolution.technology continues to transform our lives and forward the aspirations of people around the world. whocan forget, for example, the student dissidents in beijing who used computers and facsimile communications tomake their case before the world and launch communist china's first democratic uprising?individual technologies like the fax machine can make a difference in our lives and even influence events.the convergence of interactive multimedia and the information highway marks a truly disorienting, potentiallyliberating step ahead for humankind. this phenomenon, like that of the affordable massproduced automobileand the interstate highway system, can change the economy, the physical landscape, and the very nature ofcommunity in america.tv, video, computers, networked communications, and a host of new digital, microprocessorbasedapplications are all, it turns out, pieces of a puzzle that is only now coming into view.this dynamism of convergence is extraordinary. as people explore its possibilities into the future, theinteractive frontier will unfold in ways that its pioneers today can't possibly imagine. in fact, any presumptiontoday about how convergence will ultimately change the lives of our children and grandchildren is not likely tobe a sound basis for public policy. though vice president albert gore (as a u.s. senator) predicted an explosionof activity and innovation on the "information superhighway" 6 years ago, few others gave much credence to theinternet or the then emerging phenomenon of the world wide web.the data are finally catching up with gore's optimism. a newsweek poll published in february 1995 foundthat 13 percent of adult americans said they had gone online, 4 percent had perused the web, and 2 percent hadlogged on for an hour or more each day. as interest among consumer and business computer users climbs, onlineactivity accelerates exponentially. growth of commercial online services (e.g., prodigy, america online, andcompuserve) remains in the double digits. products promising easy world wide web navigation, such asmosaic and netscape navigator, are proliferating, and modem ownership doubled in the second half of 1994.interactive television, video teleconferencing, work group computing, and other important interactive mediaare also gaining footholds as the socalled "killer" applications that will catapult them into the mainstream beginto emerge. together, these applications will justify a national information infrastructure (nii) analogous to basictelephone or cable television service.today's context: which way to the future?today's convergence of interactive electronic media is unique in more ways than one. in addition to thenovelty of the new interactive media and their intriguing capabilities, we have witnessed unprecedented publicinput and participation in their development. in large part these new media are closely linked to the ascent of thepersonal computer. unlike other communications infrastructures, most recently cable tv, the new media owe asmuch to the individual pc developer, engineer, and enthusiast as they do to any corporate, institutional, orgovernment initiative.this, in part, explains the popularity of new interactive media among educated, middle and upperincomeamerican families. these are people who use computers and online services at work and at home for bothserious tasks and leisure. there is an enthusiasm for interactive media among this group akin to the frontier spiritthat propelled individuals and families westward in search of opportunity and adventure.the new interactive frontier, however, is a "virtual" one, defined by the fact that it is always changing andnot limited by traditional constructs of geographic proximity, time, or selfactualization. it is at once infinite inscope and capable of redefining one's sense of community. it provides a forum in which individuals,organizations, and social groups are empowered to create, collect, exchange, and distribute information inpreviously unimaginable ways.this virtual frontier is predictably confounding many existing standards and ideas about legal jurisdiction. itis also dredging up new versions of old questions about what constitutes social equality. andprivacy, access and equity, democracy, and networked interactive media272the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.democracy itself is being reexamined. speaker of the u.s. house of representatives newt gingrich is perhapsthe most vocal proponent of an interactive democracy, featuring virtual town halls with direct online, even videoteleconferenced connections to local representatives. the key issue is, will such access improve our system, orwill it deliver a new tyranny of the majority?privacy, access and equity, and the function of democracy are all issues on the table for discussion anddebate. as we approach a more pervasive virtual frontier we are well advised to raise the level of publicdiscourse on these issues. soon, this frontier population, perhaps more diverse and complex than our own, willdemand new standards for justice, law, and order that make sense in a world without boundaries.privacythe u.s. constitution provides no explicit help in determining what privacy means. on the other hand,everyone seems to have a strong notion of what it should mean. for the purpose of this discussion, privacyincludes an individual's desire to be left alone, the ability to restrict the disclosure of personal information, andthe ability to restrict the way such information is used.on the virtual frontier, privacy issues abound. individual privacy claims are running headlong into theadministrative requirements of government agencies, the competitive concerns of business, and, increasingly, thebest efforts of law enforcement. a carefully considered balance will be required to ensure that reasonablestandards of privacy survive the information age.the right to be left alonethe right to be left alone would seem straightforward enough in cyberspace, but individuals increasinglyfind themselves vulnerable to assault. some of these uninvited assaults are as innocuous as unsolicitedadvertising messages distributed throughout the net. others are more serious. in 1992, the fbi arrested asherman oaks, calif., man for terrorizing women on the internet. a man in cupertino and another in bostonwere arrested for pursuing young boys over the net.though it may be in decline at the office, sexual harassment is alive and well online. the experience mighthave an unreal quality for the harasser hidden behind an anonymous online identity, but the threat oruncomfortable approach comes across as anything but virtual for the victim.another disturbing development online is the breakdown of the noble "hacker" ethic idealized in stevenlevy's 1984 book hackers: heroes of the computer revolution. a culture coveting access to secureenvironments for the sake of it is giving way to a more destructive ethos. the new rogue hacker unleashesviruses causing millions of dollars in software and file damage, steals phone and credit card numbers, and usesnetwork access to terrorize individuals.the ability to restrict disclosure and use of personal informationperhaps the most chilling privacy issue is the degree to which we can manage or contain the digital trail weleave behind. large databases that piece together life histories and personal preferences have been a fact of lifefor more than 20 years. the sophistication with which that information is applied to marketing programs, humanresource department analysis, surveys, and investigations is more recent.as david chaum, a leading cryptography expert, writes,every time you make a telephone call, purchase goods using a credit card, subscribe to a magazine or pay yourtaxes, that information goes into a database somewhere. furthermore, all of these records can be linked so that theyconstitute, in effect, a single dossier of yourlifešnot only your medical and financial history but also what youbuy, where you travel, and whom you communicate with.privacy, access and equity, democracy, and networked interactive media273the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the new interactive media will provide many more opportunities for the collection of personal history andpreference information, from the tv shows we watch to the groceries we buy. without knowledge of theserecords, the ability to verify information contained within them, or control over their disposition and use,consumers are at a distinct disadvantage.these sophisticated systems, however, also offer many benefits. information can improve the service thatconsumers receive in retail environments. it may mean less time is spent filling out forms with redundantinformation. and it helps marketers trim databases to include only those who are predisposed to their message intheir direct mailings.the danger, of course, is that information shared among companies, nonprofit organizations, banks, creditorganizations, and government agencies is inaccurate or used for a purpose that defies a reasonable standard ofprivacy. in addition, different consumers will have varying standards as this issue becomes increasingly apparent.the nii's privacy working group has taken on the task of developing guidelines for privacy in theinformation age. these guidelines, though still in development, are an attempt to update the code of fairinformation practices established by the organization of economic cooperation and development.the essence of these principles is quite simple: limit collection of information (only collect what you need); where possible, collect information directly from the individual to whom it pertains; inform subjects of the purpose of the collection (tell them why you need it); only use the information for the purpose intended and for which the subject was informed; and give subjects the opportunity to access their personal information and the right to seek its correction.as a minimum, information about a transaction should be strictly between the end customer and theinformation user (product/service provider). the network service provider, for example, should not be privy tothe content of a transaction.an important element in the new nii guidelines is the emphasis on information user responsibility. itstresses the need for information users to "educate themselves, their employees, and the public about howpersonal information is obtained, sent, stored and protected, and how these activities affect others." however, tothe extent that the nii places the onus of responsibility on individual consumers, it may be assuming too much.this is an increasingly sensitive area in light of recent highly publicized abuses of personal databasedinformation by federal and state government employees: in 1992, operation rescue members used connections at the california department of motor vehicles to getthe addresses of abortion clinic escorts so they could harass them at home. the irs says the agency has investigated 1,300 of its own employees for browsing through the tax files offamily, friends, neighbors, and celebrities since 1989; of these, 400 have been disciplined.abuses of personal information have also surfaced in the private sector with rogue employees misusingaccess to private records including personal and financial information, occasionally leading to further criminalacts. in such instances, responsibility for the security of personal information should be clear. decentralized,interactive, digital communications vastly complicate the issues of privacy and security.privacy and the lawprivacy issues have always loomed large for local carriers in the telecommunications industry. regionalcarriers are held to the highest standards for privacy and security of any industry. as the national networkinfrastructure shifts from analog to digital with an increasing ratio of data to voice traffic, these standards mustbe reviewed and amended to consider a host of new players and technologies.privacy, access and equity, democracy, and networked interactive media274the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the internet, for example, presents a whole new ball game when it comes to privacy and security. a criticalelement of the new network infrastructure is that it be a level playing field where both transport and contentproviders are equally accountable and operate under a common set of rules.if a comprehensive, equitable framework for privacy and security is not established, a frontier vigilanteethic will take hold among consumers of the new interactive media. we are already seeing this tawdry trendemerge on the internet.a defensive reaction to invasive information collection and the perceived threat to privacy posed bygovernmentspecified "clipper chip" technology is the widespread use of privacy tools in the online community.sophisticated encryption techniques, accessible through shareware on the internet, allow users to protectconversations, electronic messages, even databases from unauthorized view. this move toward selfprotectionshould put service providers and information users on notice. the continued use and proliferation of privacytools will have a destabilizing effect.what happens, for instance, when law enforcement officials face kidnappers, child molesters, and terroristswho use encryption shareware to protect their communications?shortly after the tragic bombing of the oklahoma federal building, law enforcement officials learned ofincriminating bulletin board and email messages that had been posted on an online service by a suspect andothers with possible knowledge of his plans before the act occurred. such evidence is invaluable and should beaccessible to law enforcement.regional telecom carriers have shown that accommodation of wiretaps and other courtordered lawenforcement efforts targeting telephone customers can be achieved without compromising other constitutionallyprotected communications. bell atlantic, for example, has instituted a privacy policy that calls for fullcooperation with any government or consumer group to help resolve privacy issues. volunteer jointindustryconsumer panels should develop model standards for joint collaboration to assist with dispute resolution.networked interactive media present broad legal, ethical, and technical challenges. for example, there is nophysical wiretap capability online. service providers are required either to grab information realtime frombulletin board conversations or drill down through massive data repositories, which include primarily protectedconversations. cooperative efforts and careful compromise are needed to safeguard privacy and security in theseemerging areas in the face of serious public safety demands.the right of privacy and free speech will always be weighed against the public demand for law and orderand for community standards of decency. although many of the specific technology and jurisdictional issues aredifferent or somehow transformed in today's virtual environment, the act of balancing competing concernsremains the same.the management at prodigy services, for example, has made a number of controversial decisions regardingextreme behavior onlinešfrom a suicide threat to a racist hate group forum in which other subscribers wereverbally attacked. the decision to pull the plug on free discussion and provide authorities with personalinformation that saves lives is not always easy. many such cases will likely end up in court.in another case, threats issued by one individual subscriber to another on prodigy led to an investigation inwhich three federal agencies, three states, and seven municipalities made similar jurisdictional requests forinformation. this is another issue demanding guidelines as the networked interactive infrastructure expands. ifall legal jurisdictions are to be honored, how are transport and content providers to comply with contradictoryrequests, or even foreign legal inquiries? what if, for example, a transport provider receives a legal requests forprivate information from a rogue police state within the virtual boundaries of the network? this raises thepotential for causing not only a violation of privacy or human rights but also of sovereignty and national security.in the absence of exacting legal precedents, service providers have established tough but pragmaticstandards for privacy. such individual voluntary efforts are needed as broader privacy guidelines are tested andcooperation among consumer, business, and government representatives brings a workable consensus.privacy, access and equity, democracy, and networked interactive media275the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.access and equityas the networked interactive media revolution ramps up in this country of 250 million people, some of usare inevitably better positioned to leverage it for our own development and prosperity than are others. it is theobligation of business, educational institutions, and government to plan for the broadest possible inclusion ofpeople, bringing diversity of perspective and ideas to the virtual frontier.the widening fissure between the haves and the havenots in the united states and around the world is oneof the most serious challenges to modern society. though the issue of poverty defies direct assault, efforts toinclude, educate, and learn from the economically underprivileged among us will chip away at the core problemsand improve the quality of discourse in america.interactive product and service providers investing in or supporting the development of a networkedinteractive media infrastructure must find ways to leverage their limited efforts so that the broadest possibleinclusion can be achieved. the two major obstacles are access and the capacity to use.access to networked interactive mediathe issue of access is perhaps more varied and complex than privacy within the context of networkedinteractive media. it is certainly more demanding of the available resources and requires that tough choices bemade on an ongoing basis toward some ideal of universality. in general, women, children, old people, poorpeople, those who are legally blind and those who are illiterate, and nearly the entire continent of africa aredisproportionately absent from the virtual frontier.in the united states we have to determine who among the underrepresented are least likely to get therewithout direct action on their behalf.in computer ownershipša characteristic considered most likely to precede active interest in networkedinteractive servicesšincome, education, and race are the statistically significant factors.according to a 1995 u.s. census bureau report, households headed by people with a college degree arenearly eleven times more likely to own a computer than households headed by those who did not complete highschool. white households with incomes above $75,000 are three times more likely to own a computer than whitehouseholds with incomes of $25,000 to $30,000. among blacks, higher earners were four times more likely toown a computer, while the likelihood was five times among hispanics.these discrepancies are not likely to improve as the digital revolution progresses. the costs of going onlineat home are not at all insignificant. at least several thousand dollars must be spent on computer andcommunications equipment before a consumer can hook up to one of the popular online services. these servicescharge an average $10 per month plus up to $150 or more for 30 hours of connect time and content delivery.giving people tax credits to buy pcs, as newt gingrich has suggested, may be impractical, but somethingmust be done to provide disadvantaged people with the tools needed to succeed in a knowledge economy.in the future, direct access to networked interactive services will be an integral feature of commonplaceconsumer electronic equipment. in the interim, schools, libraries, and other public facilities will likely serve asthe point of entry to the virtual frontier for many americans. interactive product and service providers are welladvised to work with these institutions to determine what facilities and related services will produce the bestresults for those without access.the capacity to use networked interactive mediaa significant finding in recent years has been the correlation between poor and illiterate segments of ourpopulation and the lack of both interest in and access to the new online media. this correlation is dangerous inthat this group is one of the most likely to benefit from the eventual mainstreaming of interactive media.privacy, access and equity, democracy, and networked interactive media276the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.networked interactive media will serve to fill in gaps left by economic disparity. they could help, forexample, address the current lack of affordable training in literacy, work, and communications skills thatprevents people from getting and keeping employment. and as networked interactive media become less textintensive and more verbal, visual, and highly intuitive, they will be more easily accessible to those who are lesseducated or able. the opportunity is upon us to deliver everything from comfort food for couch potatoes toinformation services for learning and empowerment, particularly with the development of webbased andinteractive television (itv) services.access to these services for people with a limited capacity to pay for or utilize them will hinge on ease ofuse and basic economics. unfortunately, delivering such slick applications directly into millions of homes inunderprivileged communities during the early stages of commercialization is not realistic. the cost could notreasonably be passed on to the remainder of the customer base or the taxpayer, and the investment communitywould have no incentive to participate without expectation of returns.as early test markets prove successful, the cost of rolling out network infrastructure drops, and interest ininteractive media increases among lowincome people, a solid economic motivation to wire these communitieswill emerge.in the short term, costsensitive alternative solutions can be considered as a means to boost the onlinepopulation. again, the schools and other public facilities will be a critical point of access.itv trials and highbandwidth trials are in the early stages of planning and implementation, so it ispremature to draw any conclusions regarding deployment of these technologies.concerns have been raised about equitable deployment and, in some cases, charges of "economic redlining" have been raised. many content and transport providers appear to be seriously addressing the "redlining"issue. bell atlantic, for example, in its initial deployment plans, specifically addresses this concern. its currentlytargeted "hot sites'' slated for field trials have a population composed of 36 percent of the cited minoritycategories in comparison with a 24 percent minority population in the total region. most lecs and rbocs alsohave state educational initiatives in place to address access and equity issues.in addition, service providers, government agencies, and other organizations might consider limitedinteractive service into homes or public areas that can be delivered at lower, less costly bandwidths than itv.many valuable, albeit streamlined, interactive products could be offered at 14.4 or 28.8 kbps or via isdn.new york city's united community organization, for example, has installed 200 pcs with isdnconnections to the internet. the new facilities, paid for with $1.4 million in federal grants and private donations,are used by uco staff and neighborhood residents.realizing access and equityinformation technology is becoming part of the popular vernacular. the networked interactive informationmedia will further this evolution. they will bring good (learning, communication, selfhelp, andentrepreneurialism) and evil (crime, propaganda, and stultifying mass culture). but ultimately, they must deliversubstantial inclusion if they are to transcend the vernacular and serve the larger goals of individual and economicdevelopment in a free democratic society.larry irving, assistant secretary of the u.s. department of commerce and director of the nationaltelecommunications information administration, says, "it is going to require a concentrated effort not to be leftbehind.– we have to get the technologies deployed in minority communities, make sure our children aretechnologically literate, and seize the entrepreneurial opportunities."vice president gore said, "this is not a matter of guaranteeing the right to play video games; this is a matterof guaranteeing access to essential services. we cannot tolerate, nor in the long run can this country afford, asociety in which some children become fully educated and others do not."the networked interactive media infrastructure, described by vice president gore as the "informationsuperhighway," can evolve into an infrastructure as fundamental as the interstate telephone or electrical powersystems, enabling individuals of limited means and capacities to meet their own needs. and like thesemomentous projects, universality will take time and careful allocation of limited investment resources.privacy, access and equity, democracy, and networked interactive media277the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.democracynetworked interactive media have many potential implications for democracy. the extent to which formaland informal aspects of the democratic process in the united states will be transformed is impossible to gauge.if the internet's early impact is any indication, it will not be insignificant. the following are a few examples: a loose confederation of angry online activists is credited as a key catalyst in the downfall of exhousespeaker tom foley. online constituent service and information are provided by senator ted kennedy and more than a dozenother members of the house and senate. world wide web sites are maintained by powerhouse political action groups including the christiancoalition and the nra. online participatory government, from town meetings to national referendums, has been proposed byopinion leaders including newt gingrich and ross perot.in the third world, information and communications technologies have already transformed the process ofpolitical dissent and even revolution. in mexico, rebels have waged a public relations war via laptop computer,modem, and fax from the remote state of chiapas. their assaults via the news media have captured publicsympathy and reversed repeated government offensives. information is replacing the standard weapons of war.according to howard rheingold, online services are making less dramatic, but no less remarkable,transformations possible in american society. these services, he claims, enable the creation of badly needed"virtual communities" based on shared interests, which hearken back to the world before radio and television,both of which diminished social discourse as a pursuit distinct from work and obligation.at their best, the networked interactive media will give cohesion and community to underrepresentedrealms of opinion. at their worst, they will enable terrorists and hate groups to reach wider audiences undercover of anonymity. in all likelihood, they will add yet more dimension and subtlety to the social and politicallandscapes.information and community are powerunlike other media that have been coopted and at times successfully manipulated by political leaders, theinternet is too diffuse and decentralized. it is better suited to affinitybased constituency building than to masscommunication.the internet and other online services enable the formation of virtual communities built on shared interests.in a fastpaced information age during in it is increasingly difficult for people to assemble and share ideas face toface, the ground for virtual communities is fertile. with thousands of forums and news groups proliferating,cyberspace is host to many of these virtual communities. some of these have come to function as special interestgroups. whether they facilitate constructive dialogue, involve a greater number of people in the democraticprocess, or just further derail the deliberative aspect of representative government is a matter of personalperspective.the intimate "back fence" feel of the virtual community is perhaps its greatest attribute as a forum forpolitical communication. republican presidential candidate lamar alexander, for example, followed his firsttelevision appearance as candidate with an extended public questionandanswer session online. he became thefirst candidate to do so.both campaigns and special interests turn to the internet for information. in addition to news and countlessdatabases, the internet provides an efficient way to collect competitive intelligence. speeches, position papers,voting records, and a host of other information can be collected through myriad web sites. any forumprivacy, access and equity, democracy, and networked interactive media278the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.open to supporters is also open to the opposition. it is even possible to listen in on discussion groups hosted byopposing candidates or interest groupsša new twist on focus group research.teledemocracy and controlin the 1992 election campaign, ross perot proposed the use of technology to support electronic townmeetings at the national level, with instant plebiscites on a multitude of issues. this concept of the virtual townhall of "teledemocracy" did not die with perot's presidential hopes. similar ideas of varying degree have recentlybeen endorsed by opinion leaders, most notably representative gingrich. but the impact of a virtual polity maygo too far, according to many critics who fear that such immediacy will overwhelm the point and purpose ofrepresentative government.for some, the control of the networked interactive media infrastructure is an important issue for democracy.where there is control of distribution, there is control of content and therefore opinion that is represented. theopenness of the internet has quashed early fears about freedom of speech and access in the new media. but asmajor content and transport providers begin to position themselves as major players in networked interactivemedia, concern swells.however, the interactivity of the new media, the proliferation of competitive service providers, andconsumer demand for diversity of content will limit the influence or manipulative power of any one serviceprovider. competition for consumer attention, in fact, should intensity efforts to identify and cater to specificaudiences. in such an environment, content can more easily reflect the views of participating consumers than inthe past.recommendationson regulation and enforcement, we recommend the following: pure voluntary selfregulation byinformation collectors and users is a good start, but it can and should be supplemented by government standardsto ensure that "bad actors" are dealt with appropriately and that privacy, access, and democratic standards aredefined and protected. consumers face a confusing patchwork of selfadministered approaches that vary by state,industry, and company or service provider. many examples of abuse have already surfaced. legislation shouldset minimum standards along the lines suggested here. a joint industryconsumer panel should enforce thestandards, educate and consult, and provide dispute resolution services. disputes that cannot be resolved throughthis voluntary mechanism should be referred to the courts.privacy, access and equity, democracy, and networked interactive media279the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.35as we may work: an approach toward collaboration onthe niimarjorie greenefirst washington associatesabstractan approach is introduced for information sharing and retrieval on the national information infrastructure(nii) that focuses on the communications paths between collaborative organizations in a telecommunicationsnetwork rather than upon the content of the information that is stored in distributed databases. direct connectionsbetween domains of information are classified in the form of trails through the network where they can beretrieved and shared in natural language. an application of the approach in shared health care systems isdiscussed.the ability to connect to global networks and communicate with millions of people has made every user a publisher– but just as important, it has made every user an editor, deciding what's important and real. in this medium, youget the filter after the broadcast.špaul saffo, institute for the futureour technologies [will] become more a medium for designing how we should work together, rather than merelydesigning how we share our information.šmichael schragestatement of problemcontemporary organization theories have suggested that interorganizational networks developed tocoordinate activities across organizational boundaries will become the "new institution" of the future. not onlywill these networks be important mechanisms for providing superior economic performance and quality but theywill also survive, largely because of their "adaptive efficiencies"šthat is, because they have the ability to adjustmore rapibly to changing technology and market conditions, to produce more creative solutions, and to developnew products and services in a shorter period of time (alter and hage, 1993).public institutions such as schools, hospitals, libraries, social service organizations and state and localgovernments are also beginning to work together to provide their own services more efficiently, and they viewthe nii as an important tool for enhancing their efforts. these institutions will serve as catalysts for furtherdeveloping the nii and will ultimately create the demands for the private sector's vision of an informationsuperhighway that offers more practical services that address a growing and demographically shifting population.there are, however, formidable barriers to the deployment of the technologies used in collaborativenetworks. the recent white house miniconference on aging highlighted a major problem when it pointed outthe need for "a standard protocol linking national, local, 'online,' offline, public, nonprofit and privatedatabases" in delivering services to the elderly. "differing classification schemes, confusing terminology, andlack of 'infoglut' screening mechanisms" are limiting access to information and preventing the effective deliveryof integrated care (''accessing eldercare via the information highway," 1995).as we may work: an approach toward collaboration on the nii280the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the vision of linkages between users of patient information within communities in which each health carefacility and practitioner would connect to a network through an information system is greatly hindered by theinability to create, store, retrieve, transmit, and manipulate patients' health data in ways that best support decisionmaking about their care. this is the problem that is addressed in this white paper. it is hoped that the approachpresented here for information classification and retrieval through the nii will lead to further investigation of itspotential.related initiativescommunity networksseveral efforts are already under way to promote the widespread use of advanced telecommunications andinformation technologies in the public and nonprofit sectors, especially at the community level. (see, forexample, the u.s. department of commerce national telecommunications information administration/tiiapinitiatives.) the private sector is also beginning to explore the use of information technology in communitynetworks, including those designed to support and enhance collaboration among health and human servicesproviders (greene, 1995). eventually, a system of "global, shared care" is expected to evolve in which thecoordinated activities of different people from different institutions will apply different methods in different timeframes, all in a combined effort to aid patients medically, psychologically, and socially in the most beneficialways. because the ability to move data is considered fundamental to the process of integrated care, attempts havebeen made to find costeffective ways to share data among the participants. however, this approach has beenfraught with difficulties that are largely unrelated to the ability of the technology to provide solutions. questionsof ownership, confidentiality, responsibility for health outcomes, and semantics are paramount, and clinicians arethemselves calling for new solutions that do not require "knowledge" to be formalized, structured, and put intocoding schemes (malmberg, 1993).the european approachmany europeans have also recognized that one of the major problems in designing the shared care system ismanagement of the communications process among the different institutions and health care professionals. theyare taking a different approach and conducting field studies to evaluate the feasibility of using patientowned,complete medical record cards, which patients would carry with them and present to the institution carrying outthe treatment. although they reconize the importance of natural language processing and the potential of opticalstorage technology to reduce costs, they conclude that the technology will only be available within the respectiveinformation systems that contain medical records and that new solutions such as the chip card of the hybrid cardmust be found in order to extend communication to all health care providers (ellsasser et al., 1995).the digital libraryinformation sources accessed through the nii also represent components of emerging universallyaccessible, digital libraries. the national science foundation, in a joint initiative with the advanced researchprojects agency and the national aeronautics and space administration, is supporting research anddevelopment designed to explore the full benefits of these libraries, focusing on achieving en economicallyfeasible capability to both digitize existing and new information from heterogeneous and distributed sources ofinformation and to find ways to store, search, process, and retrieve this information in a userfriendly way(national science foundation, 1994). it has been suggested, however, that "for digital libraries to succeed, wemust abandon the traditional notion of 'library' altogether.– the digital library will be a collection ofinformation services; producers of material will make it available, and consumers will find and use it"(wilensky, 1995). new research is neededas we may work: an approach toward collaboration on the nii281the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.that is fundamental to the development of advanced software and algorithms for searching, filtering, andsummarizing large volumes of data, imagery, and all kinds of information in an environment in which users willbe linked through interconnected communications networks without the benefit of preestablished criteria forarranging content.vannevar bush and the new technologiesthe concept of a dynamic, useroriented information system was introduced as early as 1945, whenvannevar bush suggested that an individual's personal information storage and selection system could be basedon direct connections between documents instead of the usual connections between index terms and documents.these direct connections were to be stored in the form of trails through the literature. then at any future time theindividual or a friend could retrieve this trail from document to document without the necessity of describingeach document with a set of descriptors or tracing it down through a classification scheme (bush, 1945).in 1956, r.m. fano suggested that a similar approach might prove useful to a general library and proposedthat documents be grouped on the basis of use rather than content (fano, 1956). this suggestion was followed 10years later by a pioneering contribution of m.m. kessler at the mit technical information project, whodeveloped a criterion for such grouping of technical and scientific papers through "bibliographic coupling," inwhich two scientific papers cite one or more of the same papers (kessler, 1965). this concept of bibliographiccoupling has been extended to other types of coupling and refined to the present day, largely through computerbased techniques that identify sets of highly interrelated documents through "cocitation clustering" (garfield,1983).although it was recognized that the model of "trails of documents" as suggested by dr. bush 50 years agohad useful features that the subsequent partitioning models did not offer, research has not been conducted on itspotential for classification and retrieval in modern communications networks. perhaps this would be a good timeto revisit the concept, especially as traditional computerbased systems are merged with communications systemsin a network of networks such as the nii. and because citation characteristics are an indication of how scientificdoctrine is "built," we might want to combine the idea of trails of documents (represented as "communicationspaths") with sets of documents (represented as "domains of information") into a more general model that can beused for both classification and retrieval of information. such a model has been developed for military"command and control'' and is presented here for further consideration by the nii community.analysismessage traffic among higherechelon commands during the early part of a crisis situation is extremelydifficult to classify. this is because such communications do not generally fall into categories that deal withspecific predetermined military tasks, but instead are much less precisely defined, less routine, and consistprimarily of the exchanges of information along with recommendations, advice, and other messages that arenecessary before any tactical systems can be put into effect. by the same token, these communications aredifficult to retrieve in any formatted sense because the unexpected, evolving, and interdependent nature of theinformation places an even greater emphasis upon natural language communication.in an attempt to avoid the inadequacies inherent in any classification system while at the same timerecognizing that as the amount of available information grew there was a parallel need for a more precise way toretrieve specific data, a technique was developed for associating messages with each other that required nointerpretation of the subject content of the messages (greene, 1967). this technique is based upon the thesis thatif a message referenced a previous message, the previous message must have influenced that message in someway. for example, a message might say, "this is in answer to your question in reference a." often a messagereferenced a previous message that referenced a yet earlier message. still other connections of messages throughtheir references are possible.as we may work: an approach toward collaboration on the nii282the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in figure 1, if each number represents a message and if an arrow from 2 to 1 means 2 referenced 1, then wecan interpret figure 1 as follows: message 2 references message 1, message 4 references message 2 but alsoreferences message 3, and message 5 is another message that references message 3. thus, we can speak of a"referenceconnected" set of messages s = (1, 2, 3, 4, 5)šthat is, a set of messages that are connected in anyway through their references. (this concept is analogous to the one of "joining" in directed graph theory.)figure 1 a referenceconnected set of messages.it is noted that in figure 1, messages 4 and 5 are "bibliographically coupled." another type of couplingoccurs if two papers are cited by one or more of the same papers (e.g., 2 and 3). and finally, there is the simplecitation relationship between 1 and 2, 2 and 4, 3 and 4, and 3 and 5. these three basic types of referenceconnectivity have been used as separate partitioning criteria for retrieval systems in the past. however, they havenot been combined into a single dynamic system for both classification and retrieval, nor have they been used tolink databases for interorganizational collaboration, as this white paper suggests.it was found that during the early part of a crisis situation when messages throughout the commandstructure and in different locations were put into referenceconnected sets, these sets in most cases uniquelyidentified particular events during the crisis. for example, one set that was constructed from crisisrelatedmessage traffic found in files at three command headquarters contained 105 distinct messages that dealt with thepreparations for landing airborne troops. other sets of messages represented the communications related to otherevents such as the provision of medical supplies, the preparation of evacuation lists, and sending surgical teams.all of these events were represented by unique message sets in the investigated files of crisisrelated traffic.referenceconnected sets proved to be valuable tools in analyses of command information flow as well asof the operations they describe. deficiencies in flows and use of information were much more easily identifiedwhen focus was placed upon a specific event represented by communications throughout an entire commandstructure. the natural application of these sets to information retrieval was also noted because it was possible tofile messages automatically into appropriate message sets by noting only the references that were given. thesesets then represented events during a crisis and were available for answering queries regarding their status.predetermined subject categories were not required, nor were any restrictions placed upon the format of themessages. the method simply provided a way of quickly locating a message that had the information (as it wasexpressed in natural language) that was necessary to make a decision.automatic classificationa simple filing method was used in the analysis for automatically classifying messages into referenceconnected sets. if a message referenced a previous message, it was put into the file of the previous message. so,for example, in figure 2, message 2 would be filed with message 1 because it referenced message 1. message 3does not reference a previous message and would thus begin a new file 2. however, message 4 referencedmessages in both files and therefore connected the two. two subsets were identified in this way. one subset(assigned the number 1) contained messages 1, 2, and 4. the other subset (assigned the number 2) containedmessages 3, 4, and 5. message 4 is the link between them and, in the language of directed graph theory, may beconsidered to be a linking point between two maximal paths in the semipath from message 1 to message 5.as we may work: an approach toward collaboration on the nii283the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2 automatic classification of a referenceconnected set.the structure of a referenceconnected set identifies subsets (as in the preceding section) that can beinterpreted in a number of ways. for example, it is noted that a subset will occur only if there is a message withinthe set (such as 3 in figure 2) that does not reference a previous message but that is eventually linked to the set.such a message may begin a "new" event that eventually becomes related in some way to the earlier eventinitiated by message 1. however, the structure of a referenceconnected message set is also a function of anotherimportant factoršthe organizational chainofcommand and the distribution of information throughout thischain. for a message cannot reference a previous message unless its author is cognizant of the previous message.consequently, the paths in a referenceconnected set (and thus the corresponding subsets) will often reflect theinformation flow between specific commands although the event is essentially the same.it is easily seen that this model can be extended and adapted to other interorganizational networks in whichinformation is exchanged to meet a common goal, such as provision of health care. the application of the modelalso becomes more complex as additional nodes are included and multiple addressees are allowed. nevertheless,two important characteristics should be noted that illustrate this model's potential in supporting the collaborativeprocess: the subsets of a referenceconnected set often correspond operationally to the sets of messages received atvarious nodes for the same event and therefore define "domains of information" stored and processed at eachnode in the network for that event or, in the case of health care, patient episode. (this concept can bepursued further in order to address concerns about excessive centralization in integrated systems, to protectpatient privacy, and to measure the benefits of collaborationši.e., health outcomesšin terms of the costsassumed by individual participants. these are all potential advantages of this approach when applied tohealth information networks.) rules of referencing could be established that would guarantee that certain sets and subsets would appear.for example, if each node in the network has a record (through the references) of all previouscommunications dealing with an event, all of the messages would automatically form a referenceconnectedset at each node even though all of the messages were not processed at every node. (again, there areadvantages here for social service agencies that would like to "track" clients without sacrificing clientconfidentiality or losing control over their own administrative processes.)conclusion and recommendationsin a medium in which "the filter comes after the broadcast" and in which users everywhere have directaccess to the full contents of all available material, finding information will be a key problem. how can aclassification system be developed for a communicationsbased system in which the unexpected, evolving, andinterdependent nature of the information places even greater emphasis on natural language? new approaches willhave to be found that avoid both the problem of describing the content of information and the problem ofintegrating new information into a predetermined classification code. the collaborative networks of the futurewill focus on information flows. they will lead to dynamic useroriented information retrieval systems that areas we may work: an approach toward collaboration on the nii284the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.based on communications paths and direct connections between distributed information sources rather than upontechnologies that mechanically or electronically select information from a store. new paradigms of interactionappropriate for multimedia distributed systems will be the focus of new technologies, and automated, intelligentsearch agents will be found that help consumers as well as providers to find and use what is important and real.new technologies, combined with the concept of referenceconnected sets, may offer another potentialsolution to the management of the communications process among different institutions in collaborativenetworks. future research on community networks should be focused on the operational level rather than theadministrative level by linking users of information from the "bottom up" and by searching throughcommunications paths rather than through the content of the information that is stored in distributed databases.this would give communities an opportunity to assess the role of the nii without large investments intechnology and would allow participating organizations to gain the economic benefits of the network only in sofar as there is a need to collaborate.an approach is presented here that does not attempt to guide users through the vast domains of informationthat will be available through the nii. instead, it helps them to find quickly the others user within theircommunity of interest that may have the information they are seeking. this approach could provide the protocolneeded to link national, local, "online," offline, public, nonprofit, and private databases for increased access tocollaborative networks. it could also enable providers of health and human services to work together to aidpatients medically, psychologically, and socially in the most beneficial ways. it is a tempting approachreferences"accessing eldercare via the information highway: possibilities and pitfalls," a 1995 white house miniconference on aging, march.alter, c., and j. hage. 1993. organizations working together. sagebush, vannevar. 1945. "as we may think," atlantic monthly 176(1):101œ108.ellsasser, kh., nkobi, j., and kohler, c.o. 1995. "distributing databases: a model for global, shared care," healthcare informatics,january.fano, r.m. 1956. documentation in action, chapter xive, pp. 238œ244, reinhold publishing corporation, new york.garfield, e. 1983. citation indexingšits theory and application in science, technology, and humanities. isi press, philadelphia.greene, m.j. 1967. "a referenceconnecting technique for automatic information classification and retrieval," research contribution no.77, operations evaluation group, center for naval analyses, the franklin institute, march.greene, m.j. 1995. "assessing the effectiveness of community services networks in the delivery of health and human services: aneconomic analysis model," research conducted under hrsa contract no. 94544 (p), march.kessler, m.m. 1965. "bibliographic coupling between scientific papers," american documentation 14(1):10œ25.malmberg, carl. 1993. "the role of telematics in improving the links between primary health care providers," annual symposium oncomputer applications in medical care.national science foundation, digital library initiative, fy 1994.u.s. department of health and human services. 1993. toward a national health information infrastructure, report of the work group oncomputerization of patient records, april.wilensky, r. 1995. "uc berkeley's digital library project," communications of the acm.as we may work: an approach toward collaboration on the nii285the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.36the use of the social security number as the basis for anational citizen identifierw. ed hammondduke university medical centerstatement of the problemalthough there is little disagreement that some type of unique universal citizen identifier is necessary forcreating a complete, lifetime, patientcentered, computerbased health record, there is considerable disagreementover what that number should be. this paper makes the argument that a number derived from the social securitynumber (ssn) and administered by the social security administration (ssa) is the best and most economicalsolution to this problem. arguments against the ssn are, for the most part, arguments about any identifier thatmight be universally used to identify individuals for bringing together all data relating to their health care.new models for health care delivery, particularly managed care, can be fully supported only through anintegrated, electronic information system. the concept of a lifetime, patientcentered health record containing, atleast logically, all data from all sources are key to delivering high quality, costeffective care. patients receivethat care from a variety of providers in a variety of settings. the information system must be able to aggregatedata about a person into a single, logical record. to do this integration, the identity of a person must beunequivocally established in the sending and receiving systems.there are two different problems in establishing patient identity. the first problem is to establish theidentity of a person with respect to a presented identification number. this process is called authentication, andseveral options are available. in the past, authentication has usually been accomplished by a person presenting acard with an identification number. biological identifiers, such as a thumb print reader, are becoming affordableand can establish a person's identity with a high degree of certainty. the other problem occurs when data arebeing transferred between two systems, and the patient is not available.some people propose the use of demographic data such as a person's name, date of birth, mother's birthname, and/or address. inconsistency in these data parameters are a source of trouble. in the case of a name,comparison of databases shows inconsistency in the use of name order, full names versus initials, nicknames,and the occasional omission of suffixes, such as jr. or sr. many people have multiple addresses, mailingaddresses, home addresses, and incomplete entries. the listed date of birth, particularly in the medical recordssetting, may be in error. the literature and my own experience suggest that approximately 30 percent of theentries in two databases that are being merged require resolution by a human.backgroundthe social security act was signed into law on august 14, 1935, by franklin d. roosevelt. the socialsecurity board recommended the adoption of a ninedigit numbering system for identification purposes and wasgranted authority by the treasury department on november 1936 for the assignment of numbers to people whowere employed. the ssn is a ninedigit number broken into three groups. its form is 999999999. the firstthree digits, called the area number, are determined by the address shown on the application for the ssn (nowbased on zip code). initially, the united states was divided into 579 areas numbered from 001 to 579. each statewas assigned certain area numbers based on the number of people in the state expected to be assigned an ssn.at present, area numbers 001 through 647, with the exception of 588, have been assigned. in addition,the use of the social security number as the basis for a national citizen identifier286the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.area numbers 700 through 728 were assigned to railroad workers until 1963, at which time the practice wasdiscontinued. the area number has little meaning today due to the mobility of people. the next twodigit group,called the group number, has no special significance except to break the numbers into convenient blocks. thelast fourdigit group, called the serial number, is assigned sequentially within each group. note that no groupcontains only zeroes.in a study done at duke university, examining the ssns of approximately 150,000 individuals, the last sixdigits of the ssns were uniformly distributed. this uniform distribution is particularly valuable for certain hashcode indexing techniques.in the 1960s, the use of the ssn spread to the internal revenue service for tax purposes, the department ofdefense for military designation, and the civil service commission for employee identification. in 1976, stateswere authorized to use the ssn for tax purposes, public assistance, and for driver's license or motor vehicleregistration. a number of states use the ssn on the driver's license.analysis and forecastvalue of a universal citizen identifiersimply put, the most reliable method of integrating data from multiple sources is to have a uniqueidentification number known to all sources. in the absence of such a number, combining data from multiplesources or even reliably identifying a person within a single source is difficult. if we fail to identify a person inthe health care environment, that person's data are split into multiple records and valuable data are misplaced.community health care information networks (chins) and statewide alliances are becoming popular inwhich health care information about a person is available, with proper safeguards, to those people responsible fora patient's care. failure to associate known health care data about a patient can lead to serious consequences. forexample, if the patient is allergic to a certain drug and he or she is misidentified and that information is notavailable, that important point could be missed. if, in fact, we believe that information about the patient's health,medications, allergies, problems, and treatment plans is important, then we must be sure that the information isavailable to the proper health care providers. the highest probability of making that happen is through the use ofa unique universal identifier.requirements for a universal citizen identifierthe universal citizen identifier (uci) must be unique. each person must possess one and only oneidentification number. a uci number, once assigned, can never be reassigned. a uci should be assigned at birthor when a person becomes a resident of this country.the uci should be context free. the uci is a pointer to data about a person. it should not attempt to conveyany information about gender, age, or geographical area where a patient was born or now lives. its sole purposeis to link the number to one or more data banks.a system must be established for creating an identification number for foreign visitors and illegal aliens.such a number must also possess the characteristic of uniqueness and must never be reassigned. we now haveinternational telephone numbers that use a country code. these numbers are of various lengths and format. wemight use a similar scheme for personal identifiers. the popularity of international travel and the availability ofthe internet make it particularly feasible to transmit a person's health record to any country. a knownidentification number would make that process more reliable.one of the commonest errors that results in the misidentification of a patient, even with the use of a patientidentification number, is the transposition of two numbers. the use of a check digit would provide a solution.there are several check digit algorithms. generally the check digit is generated by multiplying each digit of theidentifier, in order, by a weighted multiplier. the resulting product is divided by some number and the remainderis taken as the check digit. this digit becomes part of the identification number and is entered into thethe use of the social security number as the basis for a national citizen identifier287the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.computer. the computer, in turn, calculates the check digit and compares it to the entered number. if they match,the entered number is assumed correct. if it is different, the number is rejected. astm recommends the use of alookup table to determine the check digit.the uci should use both letters of the alphabet and numerals to make up the identification number. certainletters, which might be mistaken for numerals, should be omitted. examples are the letters "o" and "q," whichmight not only be mistaken with each other but also with the numeral "0." if lower case letters are used, the letter"l'' might be mistaken for the numeral "1." in any case the number of unique combinations of some 30 elementsšand with lower case some 62 elementsšwould more than handle the population of the world for a long time.for economic reasons, i recommend that numerals be used as long as unique combinations are available, and thatletters then be added one position at a time. most legacy systems could accommodate numerals without aproblem, and there would be ample opportunity to plan for the accommodation of letters.validation of the ucithe biggest problem with any personal identifier system is establishing and maintaining an errorfree linkbetween the actual person and the associated number. the internal revenue service recently reported 6.5 millioncases of missing, invalid, or duplicate social security numbers (fix, 1995). most of these errors were the resultof recording errors. other duplications occurred in connection with an attempt to defraud the irs. in one case, anssn was used more than 400 times. there is no question that duplicate ssns exist. one story suggests that whenthe announcement of the ssn program was published in the newspapers, a sample ssn was included. manypeople apparently thought that this number was what they were supposed to use and accepted that publishednumber as their ssn. another story is that many people, in purchasing a new wallet that included a dummy ssncard, accepted that number as their ssn. in some cases, the ssa apparently reissued ssns. in other duplications,people have simply made recording errors and have been using incorrect numbers for many years. increased useof the ssn has resulted in a significant reduction in these duplications for a number of years.validation of the uci will require the creation of a database containing demographic and identifying dataabout every resident of the united states. considerable thought is required to define this database, and it willultimately be a trade between what is required to identify an individual uniquely and what should not be includedto protect the rights of the individual. this database could be used for other purposes as well. certainly, theexistence of such a database would reduce the effort of producing a census and of being able to do populationbased statistics. many citizens would not be concerned about the existence of such a database; others wouldconsider any database an invasion of privacy. nonetheless, everybody is already in many databases and theanonymity of these databases permits easy abuse. legislation would be required to protect the contents and useof such a database. this topic is explored below.keeping a uci database up to date would be a difficult challenge. some items should never change, othersmight change infrequently, and others might change with some frequency. elements in the database wouldinclude a person's name, gender, marital status, race or ethnicity, date of birth, and address. persons would beresponsible for informing the agency of change, perhaps as part of some annual event.arguments for advantages of using the ssn over other proposalsunder the assumption that a personal identifier system is selected, that system would have to beadministered by some agency. one possibility is that a private, trusted authority could be given the responsibilityof assigning the uci and maintaining the accompanying database. another possibility is that a new governmentagency could be created to administer the uci. another option is to use the existing ssa to administer the uciprogram. setting up a new agency with the accompanying bureaucracy would take longer and cost more thanusing an existing agency.the use of the social security number as the basis for a national citizen identifier288the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.there are over 1300 social security offices distributed around the united states where a person can applyfor and receive a social security number. evidence of identity, age, and u.s. citizenship or lawful status isrequired. all applicants over the age of 18 must apply in person. individuals under age 18 or those seekingreplacement cards may apply in person or by mail. nonwork ssns may be assigned to illegal aliens if theyreceive benefits payable in some part from federal funds.ssns are assigned at the ssa's central headquarters in baltimore. key data elements are a person's fullname, date and place of birth, mother's maiden name, and father's name. these elements are used to screen thessa database to prevent the assignment of more than one number to the same person. if no match occurs, a newssn is assigned. if a significant match occurs, a replacement card is issued. the current system assigns an ssnwithin 24 hours of receipt of the application. cards are sent by mail and usually require 7 to 10 days for delivery.beginning in 1989, the ssa began a program in which an ssn can be assigned to a child as part of the birthregistration process. this procedure currently requires a parent's approval. the percentage of birth registrationsincluding a request for an ssn is more than 75 percent and is increasing.as of march 3, 1993, 363,336,983 ssns had been issued. the number of currently active ssns (of livingpeople) is estimated to be approximately 250 million. it is estimated that approximately 4 million individualsmay have more than one number.the privacy act of 1974 (5 u.s.c. 552a) states that it is unlawful for any federal, state, or local governmentto deny an individual any legal rights or benefits because the individual refuses to disclose his/her ssn. there isno legislation concerning the use of the ssn by nongovernment entities.the ssa recently announced that the agency was undergoing a reorganization. my recommendation is togive the ssa the tasking authority and the required funding to administer a uci program.the case for a single identifier for all purposesthe increasing use of the ssn for identification purposes supports the argument that a universal, uniqueidentifier has value. an individual's having only one number that he/she would use for any identification purposewould represent a considerable savings for federal agencies, vendors, health care agencies, and any otherorganization that creates a database. the suggestion that a single number could be used to access patient data inany of these databases or to join data from any database regardless of purpose or owner is frightening. yet, inthis age of connectivity and computerization, it is a trivial problem to link any number system, particularly if 100percent accuracy is not sought. anyone who thinks that confidentiality is preserved by requiring differentnumbers is misinformed. i would argue the opposite. given a single number, it would be possible to providemore positive controls in making sure that the number is not misused. i therefore recommend that the uci bepermitted to be used in any legal operation subject to the individual's approval.confidentiality, privacy, and securityin a recent opinion poll conducted by the louis harris organization, 85 percent of those polled agreed thatprotecting the confidentiality of people's medical records is essential (louis harris and associates, 1993). in thatsame pool, 67 percent indicated a preference for the ssn as the preferred national health care id number.there can never be any security in a publicly known personal identifier. security and protection of anindividual's privacy must be provided through each database and the supporting applications. all individualshave certain rights relating to who sees data about them, how those data are used, and the opportunity to reviewand correct errors in the database. in the case of health care data, the patient should be able to define, in writing,by whom and under what circumstances those data may be used. on the other hand, a health care providershould be told when data are being withheld and, except in emergency situations, should be able to refusetreatment. in an emergency situation, if the provider makes an incorrect decision due to lack of completeinformation, that provider should be protected from malpractice lawsuits. individuals should be able to request alist of all persons who have accessed their data.the use of the social security number as the basis for a national citizen identifier289the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the inability to correctly identify a patient's data from some type of patient id might actually result in less,rather than more, protection of confidentiality. for example, if a patient indicated that "my primary carephysician" could see the record and the patient's id did not match the record, such a discrepancy would permitinappropriate access to data. overly strict rules and computerenforced rules are risky where patient care isinvolved. blocking health care providers from access could lead to serious consequences in the case of anemergency. proper education of users of data and emphasis on the need to preserve confidentiality are ofparticular importance.federal legislation must be passed making it illegal to acquire data of any type against a person's writtenwishes. if legally or illegally acquired data are used for purposes for which they were not intended, the individualacquiring the data should be punished by law. such action should be considered to be as serious as a bankrobbery, and punishment should be similar. individual confidentiality can only be assured through legalconstraints. it cannot be achieved through confusing identifiers that might prevent databanks from beingaccessed or linked.federal legislation should also spell out the security requirements required for each organization that woulduse the uci as the pointer to data contained within the databank. each of those organizations should be requiredto have an information security officer who would ensure that confidentiality and security requirements were met.recommendationsi recommend that legislation be passed that will task and fund the ssa to be the administrator of a universalcitizen identifier, which may be used for a variety of purposes as a patient identifier. use of this number for adatabank must be requested by an organization and approved by the ssa. access to data must be logged byindividual and organization, date and time, and purpose. the uci would be based on the ssn and would be thecurrently assigned ssn plus a check digit. the ssa, in establishing the validating databank, would eliminateduplicates. an added advantage of this approach would be eliminating errors in calculating and paying socialsecurity benefits.new ucis would be issued electronically to newborns and to individuals moving to this country, either ascitizens or as legal entrants. illegal aliens would be assigned a number from a selected and identifiable set.foreign visitors would also be assigned a permanent number. legislation protecting the use of the uci andguaranteeing protection of the rights of an individual would be simultaneously introduced.electronic access to a regional office would be by internet, a state information network, or even by modem.information would be transmitted electronically. that information would be verified before the assignment of theuci was made permanent. special efforts would be made to avoid fraud. ssn cards would be coded to makecreation of false cards very difficult.the american college of medical informatics, of the american medical informatics association (acmi,1993), the computerbased patient record institute, and the working group for electronic data interchangehave all recommended the use of the ssn as a uci. several states are now using the ssn for identificationpurposes, including in the management of health care benefits. many thirdparty payers use the ssn as the basisfor the subscriber identification.we recognize the emotional issues associated with the use of a uci (donaldson and lohr, 1994; task forceon privacy, 1993). those emotions are correct and understandable. unfortunately, the suggested solution of nothaving a universal identifier, or even of restricting such an identifier to use only in the health care setting, willprovide little protection. instead, open use of an identifier with safeguards and audits will provide greaterprotection. the advantages of being able to integrate personal health care data over a variety of settings andsystems far outweigh the risks of such a system. the important thing is to recognize that the use of a universalhealth care identifier, and specifically the ssn, does not in itself mean a lack of concern for patientconfidentiality or an inability to preserve that confidentiality.already we are paying a penalty for the lack of such an identifier. time is important. now is the time foraction.the use of the social security number as the basis for a national citizen identifier290the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.acknowledgmentmuch of the information relating to the ssa was taken from an early draft of an astm document (astm,1995), "guide for the properties of a universal healthcare identifier," written by dr. elmer r. gabrieli andprovided by andrew j. young, deputy commissioner for programs, social security administration.additional resourcesamerican college of medical informatics (acmi). 1993. "resolution for the use of the ssn as a universal patient identifier," acmi,bethesda, md., february.astm. 1995. "guide for the properties of a universal healthcare identifier," draft proposal developed by astm, philadelphia, pa., january.donaldson, molla s., and kathleen n. lohr (eds.). 1994. health data in the information age: use, disclosure and privacy. institute ofmedicine, national academy press, washington, d.c.fix, janet l. 1995. "irs counts 6.5 million errors so far," usa today, april 5.louis harris and associates (in association with alan westin). 1993. health information privacy survey 1993. a survey conducted forequifax inc. by louis harris and associates, new york.task force on privacy. 1993. health records: social needs and personal privacy . task force on privacy, office of the assistant secretaryfor planning and evaluation and the agency for health care policy and research, washington, d.c., february 11œ12.work group on computerization of patient records. 1993. toward a national health information infrastructure. u.s. department of healthand human services, washington, d.c., april.the use of the social security number as the basis for a national citizen identifier291the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.37estimating the costs of telecommunications regulationpeter w. huber, manhattan instituteboban mathew, yale universityjohn thorne, bell atlanticthe licensing schemes embodied in federal telecommunications regulation forbid people to sell goods orservices without the sayso of a federal regulator. they frequently reflect the conclusion that regulators canallocate goods and resources and set prices more efficiently than can market forces. in some instances (decliningin number as new technology and the globalization of most markets spur competition) this may be true.unregulated "natural monopolies," for example, may price too high and produce too little.but the beneficial effects of regulation (such as they are) can only be realized if regulators perform theirfunctions efficiently, on schedule, on the basis of uptodate information, and to protect the public, not industryincumbents. the record of federal telecommunications regulation is not good. routine licensing decisions takefar longer than they should. when issued, licenses are loaded up with restrictions and conditions that servemainly to promote bureaucratic control. regulation often solidifies the economic status quo, protects incumbentsagainst wouldbe competitors, and deprives the public of new services at lower prices.this paper examines the costs that restrictions on the use of both the electromagnetic spectrum and thewireline impose. the largest component of costs attributable to such zoning is the lost opportunity cost ofpreventing highervalue uses of the airwaves, and of restricting socially desirable uses of wireline media.outright restrictions and delays in approving new uses of the two media also hamper competition in existingmarkets for telecommunications services, to the detriment of consumers in those markets. finally, zoning oftenleads to the existence of large economic rents actively sought by market players through socially wastefulactivities. it is, of course, impossible to measure precisely the total social cost of zoning restrictions.1 butconservative calculations of specific restrictions suggest that the costs are very high.in this paper, we describe the zoning process and estimate the economic costs that several of the restrictionsentail.wireless zoningthe airwaves are "owned" by the federal government. the government licenses private users for fixedperiods. with few exceptions, the licenses are given away; they are not sold. for the most part, the licensesstrictly prescribe how the spectrum is to be used.note: peter huber is a senior fellow, manhattan institute for policy research; boban mathew, m.a./m.phil economics1995, yale university, is a j.d. candidate 1996, harvard university; john thorne is vice president and associate generalcounsel, bell atlantic. the authors wish to thank evan leo for significant assistance and research in the preparation of thispaper. the views expressed in this paper are those of the individual authors; they should not be attributed to any company orclient.estimating the costs of telecommunications regulation292the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the modified final judgment (mfj)šthe consent decree that broke up the old bell systemšimposesfurther zoning requirements on the networks of the bell operating companies.2zoning the airwavesthe federal government began to nationalize the airwaves in 1912, when congress gave the secretary ofcommerce authority to license broadcasters.3 but most empty airspace could still be occupied freely."homesteaders" simply had to register their claims with the department of commerce. no exclusive rights wereassigned.4 by the mid1920s, courts were beginning to affirm private property rights in spectrum.5the radio act of 1927, however, placed almost every aspect of radio broadcasting under the control of thenewly created federal radio commission (frc).6 seven years later, the provisions of the 1927 act were rolled,largely intact, into the communications act of 1934.7 the frc became the federal communicationscommission.the licensing of broadcasters is conceptually straightforward. the fcc first zones the real estate, allocatingblocks of spectrum for particular uses such as am radio, fm radio, vhf tv stations, uhf tv stations, and soon. within each block, it then assigns licenses to particular users. the commission has virtually unboundeddiscretion in both regards. the law simply requires distribution of broadcast "licenses, frequencies, hours ofoperation, and power among the several states and communities so as to provide a fair, efficient and equitabledistribution of radio service to each of the same."8however chosen, licensees do not get a formal property right in their spectrum. the 1927 radio actexpressly declared that licensees were entitled to the "use of channels, but not [to] the ownership thereof."9licenses were to run for only "limited periods of time."10 (only in 1981 were the original 3year broadcastinglicense terms extended to 5 years for television and 7 years for radio.11) licenses may not be transferred withoutcommission approval.12 the commission may revoke a station license for any reason that would have warrantedrefusing a license in the first place.13zoning of cellularthe provision of cellular service is zoned in several ways. the allocation of spectrum for cellular serviceswas originally split between telcos and other nonwireline carriers.14 in 1981, the commission decided that two(and only two) cellular carriers would be licensed in every cellular service area.15a quite different and independent set of zoning requirements has come into existence by way of the mfj.the mfj's line of business restrictions preclude bell cellular affiliates from offering "interexchange" services.bell cellular affiliates thus may not arrange with a particular interexchange carrier to provide discounted serviceto their customers.wireline zoningin contrast to the airwaves, wireline networks are privately owned. but wireline media are zoned even morestrictly than the airwaves. local telephone facilities are still "zoned" to provide mostly voice services. for years,cable television operators were strictly "zoned" to supply simple carriage of broadcast video signals; to this daythey still operate under an array of quasicommoncarrier and other zoning obligations that sharply diminish thevalue of cable networks and greatly reduce economic welfare.estimating the costs of telecommunications regulation293the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.zoning of telephonesection 214 of the 1934 communications act prevents phone companies from constructing new facilities ordiscontinuing existing service without advance permission.16 section 201 of the act enables the commission toregulate what sort of devices can be connected to the telephone network, and thereby what kind of addonservices can be offered over telephone lines.17 the fcc has used its authority under both statutory provisions tozone the telephone to provide basic voice services, and not much else.the franchising of telephones was initially a way of reducing the confusion and hubbub of competition.18the useless duplication of facilities, congress believed at the time, would lead to higher charges to the users ofthe service.19 what started as a means of suppressing excessive telephone competition in an industry marked bydeclining average costs, however, has become a tool for suppressing telco involvement in broadband servicesand, when such services are permitted, regulating them in minute detail.video services are largely zoned out of the telephone franchise, too, though phone companies have recentlybeen successful in challenging some of these regulations on first amendment grounds. in 1968, the fccdeclared that cable television was an "interstate" service and that telephone companies therefore needed fccpermission to build or operate cable networks.20 in 1970, the commission barred telephone companies fromproviding cable tv service.21 in 1984, congress codified this prohibition in the new cable act.22 the "cabletelevision service" language of the 1970 rules was replaced with "the provision of video programming."23 theprohibition extends to everything that can be characterized as ''providing" video over the telephone company'sown network, including selection and packaging for resale of programming created by others.24a second sphere of telephonewire zoning involves the fcc's twodecade crusade against allowing"enhanced services" into the telephone network. in 1966, the commission undertook to examine the convergenceof computers and communications.25 in an abundance of caution, the commission insisted that "enhancedservices" should not under any circumstances be intermingled with basic phone service. for a 20year period,from the mid1960s to the mid1980s, the commission enforced a policy of "maximum separation" betweenfamiliar voice services and networkbased data processing, electronic services, and computers.26 it has sinceattempted to back off from that somewhat, but has been thwarted (so far) by litigation.zoning of cablecable operators may not operate without a franchise.27 for many years, the fcc sharply curtailed cable'sright to bid for programming, accept advertising, and generally compete unhindered against broadcast television.to this day, cable is required to devote onethird of its channels to carry local tv stations28 and is required to setaside additional channels for lease and "public access.29 like the rules that zone telcos out of video, some of thecablezoning rules are also under first amendment attack. for now, however, they remain in place.costs of federal zoningcalculating the welfare consequences of zoning restrictions is an inherently speculative task. the mostpotent criticism of such calculations is that they generally entail a partial equilibrium analysis. the introductionof new products and services and the infusion of greater competition in existing markets will necessarily affectother sectors of the economy. to be theoretically sound, one ought to consider such effects. for example, theintroduction of electronic voice mail may adversely affect the answering machine market. competitioninducedreduction in the price of longdistance calls may reduce the demand for overland mail services. to remaintractable, empirical welfare calculations abstract from such considerations. if and when such effects are large andobservable, they ought to be included. otherwise, the welfare calculations must be interpreted cautiously.estimating the costs of telecommunications regulation294the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the social costs of zoning restrictions can be broken down into four categories:1. use of spectrum by lowvalue users as dictated by zoning artificially reduces social welfare by notallowing highervalue users to employ the scarce resource. analogous costs do not result fromrestrictions on the use of wireline, which faces no capacity constraint.2. zoning restrictions reduce the extent of competition in existing markets and thus reduce consumerwelfare. spectrum zoning, for example, makes it extremely difficult for a third cellular provider to entermost markets. the prohibition of video delivery by bell operating companies stunts competition in cabletelevision markets. even if cost penalties due to scale economies are present in these markets, greatercompetition would likely improve social welfare (the sum of consumer and producer surpluses).3. zoning that prohibits or delays the provision of innovative services reduces the welfare of consumerswilling to purchase these services.4. zoning creates economic rents pursued by the providers of telecommunications services through sociallywasteful rentseeking activities and reduces government revenues that could be generated throughauctions by effectively excluding bids by highervalue users of the spectrum.although these various costs are analytically separate, they are not mutually exclusive. for example, a thirdcellular provider that displaces a current lowvalueuse occupant of the spectrum could spur competition in thecellular market. similarly, an innovative video delivery system could generate more competitive pricing in thecable market.lowervalue use of spectrumin 1992, the office of plans and policy of the fcc completed a study analyzing the welfare implications ofreassigning spectrum currently earmarked for use by a uhf channel in the los angeles area for purposes ofproviding cellular phone services.30 the calculations are based on alternative uses of spectrum during the years1992 to 2000; numbers are expressed in 1991 dollars. under plausible assumptions, the study estimates thewelfare loss attributable to the loss of a single uhf channel in the los angeles area to be approximately $139million for the years 1992 to 2000. this figure is the sum of welfare foregone by consumers and the producer ofuhf television services. the study also estimates that the welfare gains in the absence of any competitive effects(no price or output effects) of two incumbent cellular providers would be approximately $118 million.reassignment of the spectrum under such a scenario would, of course, not be welfareenhancing.consider, however, the strategy of nextel (formerly "fleet call"). the company purchased spectrumpreviously licensed for other purposesšmostly dispatching taxis. in 1991, nextel persuaded the fcc to permit itto use that spectrum to provide digital wireless telephone service.31 nextel has aggressively developed newdigital radio services, and is now positioned to be the third major wireless operator in many urban markets. it hasalready launched its digital mobile network service in many regions of california, including los angeles andsan francisco; in 1995 it will roll out service in new york, chicago, dallas, and houston.32 with its acquisitionof several regional smr companies and motorola smr frequencies across the country, nextel will have thespectrum to serve all 50 of the largest u.s. markets.33 in the los angeles market, it purchased approximately 9mhz of spectrum for roughly $45 million.34assume that the two cellular incumbents in the los angeles market rather than nextel had purchased equalshares of the 9 mhz of spectrum. if this spectrum had been evenly divided between the two existing cellularproviders, the increase in social welfare (inclusive of the purchase price) would have been in the range of $37million and $73 million for the years 1992 to 2000. to the extent that radio dispatching services were continued,one need not consider the foregone welfare of those customers. if spectrum zoning had prevented the use of thepreviously assigned frequencies for cellular service, it would have reduced social welfare by roughly $50 millionfor 9 years alone. zoning of the spectrum may prevent socially efficient transfers of a scarce resource from lowvalue to highvalue users even in the absence of any competitive effects.estimating the costs of telecommunications regulation295the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.zoninginduced reduction in competitionthe case for allowing transfers from lowvalue users to highvalue ones becomes even more compelling ifsuch transfers have the effect of spurring competition in existing markets. assuming that nextel faced nocompetitive disadvantage visàvis incumbent cellular providers, the presence of a third competitor would likelyincrease competition and drive down prices. assuming unitary elasticity of demand for cellular services, theauthors of the fcc study estimate that a reduction in cellular prices of 25 percent would lead to an increase intotal welfare in the los angeles cellular market of $799 million for 6 mhz of additional spectrum and $893million for 12 mhz of spectrum. if such a price reduction were indeed to occur because of competition from athird supplier, net social welfare would increase by approximately $750 million to $850 million.the nextel story would be replayed hundreds of times if spectrum were dezoned across the board. the fccstudy that forms the basis of the nextel calculations also found that if a uhf television station in los angeleswere to shut down and transfer its spectrum to a third cellular provider, the overall public gain would besubstantial. the fcc study explicitly took into account the possibility of loss in scale economies in theirestimates. even with duplication of vital inputs by the new entrant, the net welfare gain is substantial: $63million for a 5 percent reduction in cellular prices and $783 million for a 25 percent decline. comparable gainsare almost certainly possible by dezoning spectrum licenses across the board. in other words, the simple deletionof a few lines of legal boilerplate from fcc spectrum licenses could create a substantial increase in socialwelfare nationwide.35 the case for transfers of rights in spectrum is extremely compelling when such transfersare likely to boost competition in existing markets.zoning of wireline media, including the cabletelephone crossownership rules and the interlatarestrictions placed on the bell companies, has also imposed welfare losses. cable and telephone companies couldmake deep inroads into each other's markets if freed to use their wires to compete. cable operators estimate thattheir telephone service could achieve a 30 percent penetration rate of their basic cable subscribers within 5 to 7years. telephone companies estimate they could acquire 45 percent of cable subscribers.36restrictions that ban bell companies from providing video program services have enabled cable televisioncompanies to maintain a monopoly position in most markets. in the absence of such zoning, the bell companiescould have provided viable competition to the cable television companies to the benefit of customers. in the fewmarkets where duopolistic competition exists in the provision of video programming, cable television rates havebeen estimated to be 20 to 30 percent lower than in monopolistic markets.37assuming demand elasticities of 1.0 to 2.5 and price reductions of 15 to 25 percent, consumer welfare inthe years 1983 to 1993 would have been higher by anywhere from $1.4 billion to $2.9 billion (1993 dollars)annually in the absence of zoning restrictions.38 using average revenue per subscriber yields even higher annualwelfare loss estimates ranging from $2.7 billion to $5.4 billion. our calculations are summarized in table 1.table 1 consumer welfare loss attributable to cable monopolies, 1983 to 1993dp=15%dp=20%dp=25%basic rate calculation (1993 $billions)e=1.015.821.627.6e=1.516.422.629.1e=2.016.923.630.7e=2.517.524.532.2average revenue calculation (1993 $billions)e=1.029.340.051.2e=1.530.441.854.0e=2.031.443.756.8e=2.532.445.559.7dp, decline in prices; e, elasticity.assumptions most consistent with observed facts (20 percent decline in basic rates and demand elasticity of2.0)39 yield foregone consumer welfare of approximately $2.1 billion annually (basic rates) for the yearsestimating the costs of telecommunications regulation296the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.1983 to 1993. these savings account for approximately onetenth of the total revenues in the cable televisionmarket in 1993. even if overbuilding were to lead to nonsignificant cost penalties, it is unlikely that totalproducer surplus would decrease by enough to swamp the much larger savings to consumers. the true measureof foregone consumer welfare should also take into account the provision of more channels (greater variety ofprogramming) in competitive markets relative to monopolistic ones. consumers lost out during every year thatthe bell companies were prohibited from providing video programming services. had phone companies beenauthorized to provide such services two decades ago, they would almost certainly be doing so by now absent thenovideo zoning of their networks.the costs imposed by the cable/telco crossownership restriction can be independently estimated using anevent study. on the day judge ellis of the u.s. district court in alexandria, virginia, declared the telco/cablecrossownership restriction unconstitutional, the stock price of the plaintiff, bell atlantic, rose 4.6 percent abovethe previous day's close.40 on the day of the court decision, reuters released a story about the decision in whichit stated that "shares of bell atlantic and other baby bells soared."41 on the day after the court decision, a dowjones news wire headline announced, "cable, telecom equip[ment] stocks soar on bell atlantic ruling."42 anevent study of the effect of the ruling on the value of bell atlantic stock suggests that the market believes bellcompanies to be viable competitors in the provision of services provided by cable companies.assuming that the judge's decision was unanticipated (a reasonable assumption considering that no otherphone company had prevailed in any remotely comparable first amendment suit before), that the marketabsorbed the implications of the ruling in a single day, and that no other significant valueaffecting informationwas revealed that day, the excess return attributable to the ruling was roughly 4 percent and accounted forroughly $856 million of the day's increase in bell atlantic's value.43 despite the imperfection of this estimate,the rise in the company's value suggests two facts. first, bell companies can be profitable entrants in the cablemarket. second, the restrictions on crossownership unduly suppressed viable and potentially welfareenhancingcompetition.cellular zoning also imposes great welfare losses as well. richard higgins and james miller havecalculated that if the mfj restrictions were removed, bell cellular customers alone would potentially realizeannual savings of $200 million in longdistance charges.44 they arrive at this estimate by comparing the pricesof toll longdistance and cluster services. they demonstrate that retail prices for toll longdistance servicesexceed bulk wholesale prices by at least 110 percent.45the wharton econometric forecasting associates (wefa) group has independently estimated that if mfjrestrictions were eliminated, over the next 10 years cellular consumers would save $107 billion.46 wefaestimates that monthly wireless bills would fall 30 percent within 10 years.47 they base the estimate on thefollowing three factors. first, bell cellular customers currently pay about 40 percent more for longdistance callsthan customers of independent cellular companies.48 second, in metropolitan service areas (msas) whereneither cellular provider is a bell company subject to interlata restrictions, local cellular service is 7 percentcheaper (because of unrestricted competition) than in msas where there is a bell cellular provider.49 third, bellcellular providers are unable to cluster service areas and offer large local calling areas as can independents.50 forexample, an 84mile, 10minute cellular call from indianapolis to terre haute is $2.10 cheaper using gte thanusing bellsouth.51one further example of the costs of cellular zoning involves airtouch. in 1994, pacific telesis spun off itswireless affiliate into a wholly separate entity, renamed airtouch. a primary reason for the spinoff was to freeairtouch from the mfj's restrictions.when pacific telesis announced it was considering the move, investment analysts predicted that spin wouldincrease shareholder value. before the spin, one analyst valued pactel's cellular business at $140 per point ofpresence (pop),52 while comparable independent cellular companies traded at an asset valuation of $160 perpop.53 using the difference ($20) between morgan stanley's $140 valuation of pactel's cellular interests and itsvaluation of independent cellular companies at $160 per pop, the domestic cellular operation as an independentcompany is worth $700 million ($20 × 34,893,721 pops54) more than the cellular operation as a part of pactel.assuming that half of the valuation increase is the result of airtouch's superior financial standing relative to theother independent cellular providers, the company is worth $350 million (6.25 percent) more when freed fromthe mfj.estimating the costs of telecommunications regulation297the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the mfj's ban on bell provision of interexchange services has further facilitated tacit collusion among thethree major suppliers of such servicesšat&t, mci, and sprint. evidence of high pricecost margins in variousinterexchange services supports the hypothesis of tacit collusion among these firms.55 the fcc's tariff processhas also facilitated such collusion by allowing competitors to monitor and to match each other's pricingstrategies, thereby reducing the value of deviating from a collusive outcome.removing the restrictions on bell provision of longdistance servicesševen just to permit bell entry out ofregionšwould eliminate several of the conditions facilitating such tacit collusion. first, the number of viablecompetitors would rise from three to nine. second, the current stability of market shares among the threedominant suppliers would be destroyed; bell companies would likely pursue new customers through aggressivepricing strategies. finally, the regulationimposed barrier to entry in longdistance services would simply beeliminated.the welfare costs of the interlata restrictions imposed on bell companies are substantial. such entrywould stimulate more competitive pricing of longdistance services, driving down the pricecost margins. theftc estimates that if bell entry caused prices to fall to marginal cost, it could lead to the elimination ofdeadweight loss.56 the welfare gain would then be between $17 million and $119 million per year (0.03 percentand 0.36 percent of industry revenue). consumer welfare gains would be many times more this reduction indeadweight loss. furthermore, bell companies might enjoy cost advantages for some routes because ofeconomies of scope with their existing local networks.restrictions and delays in the provision of new servicespotentially the greatest cost imposed by zoning restrictions is the opportunity cost of the inability tointroduce unhampered new services for which consumers are willing to pay. that cost is also the most difficultto quantify with precision. nevertheless, it is useful to highlight those services and provide rough measures offoregone consumer surplus. one has to estimate not only demand for a service that has never existed but also the"virtual" or "reservation" price at which demand is choked before proceeding with consumer welfare calculations.relying on survey data on demand for advanced services that could be offered by bell companies, wefaestimated the loss in consumer welfare attributable to zoning restrictions for a number of services.57 wesummarize below their estimates of foregone yearly consumer welfare for services that bell companies can butmay not provide under mfj restrictions: residential customers1. advanced portable phoneš$1.6 billion2. return voice mailš$1.7 billion small and medium size businesses1. fax overflow serviceš$1.4 billion2. return voice mail serviceš$720 million3. call manager serviceš$320 millionif one adds producer surplus from the provision of these services, the total welfare gain will be substantiallylarger. furthermore, the above list is only illustrative, not exhaustive.as described above, one large obstacle to the provision of new services has been the fcc's policy againstallowing "enhanced services" into the telephone network. today, some 1.3 million customers buy voice mailservice from bell atlantic. the service was first offered in 1988, when both judge greene and the fcc finallyagreed to dezone telephone company wires to permit them to provide such services. at roughly $5 a month permailbox, and 10 million mailboxes nationwide, it is reasonable to estimate that in this single market, excludingbell atlantic from the market needlessly suppressed over $460 million dollars of service a year to willingconsumers in bell atlantic's area alone. unable to buy the online service from u.s. enterprises and u.s.workers, most consumers probably turned to standalone answering machines manufactured in singapore orestimating the costs of telecommunications regulation298the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.taiwan. extrapolating this figure nationwide, the novoicemail zoning (part of the much broader nocomputerservices zoning) of phone companies cost the u.s. economy some $600 million a year for at least a decade.even if the restrictions are eventually lifted, the delay in the provision of socially desirable services imposesenormous costs to the economy.58 in 1985, an fcc opp working paper estimated the costs of a comparativehearing for a cellular license application, breaking it down into cost of the delay in awarding the license, cost tothe government, and cost to applicants.59 the paper found that the typical 18month delay in awarding cellularlicenses eroded the value of the license by $90,000 and cost the government $20,000 per hearing and eachapplicant $130,000 per hearing.60finally, the possibly anticompetitive effects of lifting such restrictions must be weighed against theforegone welfare of consumers willing to purchase and of producers willing to provide such services. the netwelfare effect may indeed be positive in many or most instances where zoning is imposed because of allegedanticompetitive effects. professor paul rubin, for example, has analyzed the costs and benefits of the mfjwaiver process.61 rubin estimated the cost of the waiver processšthe delay of waivers that would have hadprocompetitive consequences, administrative burdens and rentseeking, and the deterrence of procompetitiveactivities due to the waiver processšat over $1.5 billion since 1984.62rentseeking activitiesin addition to the opportunity costs described above, a number of other social costs can be identified.zoning has led to the existence of substantial economic rent in a number of telecommunications markets.evidence of such rent includes estimates of tobin's qratio for a number of markets as well as the observeddecline in stock value of incumbent firms in response to announcements of regulatory measures that enhancecompetition in their markets. to the extent that much of the economic rent results from government protectionfrom competition, one could expect substantial resources devoted to gaining such protection. although theexpenses directed at socially wasteful rentseeking activities are difficult to quantify precisely, they are certainlyin the hundreds of million of dollars.63models of dezoningthe costs of telecommunications regulation are not immutable. limited dezoning has already begun. recentdirect broadcast satellite (dbs) and broadcast regulation are small examples of what can be applied. moreover, apropertybased system of spectrum allocation could replace the current system of government ownership.dbs dezonedregulation of the relatively new dbs services broke new groundšit severely blurred the formerly cleanlines between private carriers, common carriers, and broadcasters.64 the owner of a dbs satellite can leaseunder contract, or sell outright, transmission space to private users. it may operate others on a common carrierbasis for independent programmers. and it may send its own programming over others, either scrambled andpaid for by subscribers, or "in the clear" and paid for by advertisers.65 the owner is thus free to use its satellite,and the spectrum the satellite uses, for any mix of carriage, broadcast, or noneoftheabove activities likesubscription services or purely private transport. it may change the mix as it pleases.this is perfectly sensible; it is also a radical departure from 50 years of fcc regulation under the 1934 act.the satellite broadcaster is the first spectrum licensee that has been told, in effect, to use spectrum for any usefulpurpose it can find. spectrum licenses have been issued without cumbersome "zoning" restrictions for the firsttime. no other spectrum licensee has been this free since the enactment of the radio act of 1927.estimating the costs of telecommunications regulation299the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.broadcast dezonedconventional broadcasters have crept forward in the deregulatory wake of other wireless operators, onesmall step at a time. they have begun, for example, overlaying some modest carrierlike services on top of theirbasic broadcast operations. radio stations, for example, transmit paging services using the "subcarrier"66portions of their assigned frequencies.67 television broadcasters broadcast video programming, but alsobroadcast electronic newspapers, data, computer software, and paging services within the otherwise unused"vertical blanking interval" of their spectrum licenses.68marketbased allocation of spectrumthe most significant alternative to the current zoning approach to spectrum allocation is a marketbasedone. the idea is not new. as far back as 1959, nobel economist ronald coase proposed that property rights beaccorded to radio licenses, that license owners pay for the right to use spectrum, and that bidding for that rightwas the most efficient way to allocate spectrum.69 in contrast to spectrum, the lack of any substantial capacityconstraints in the wireline medium suggests that the costs of zoning can be alleviated by simply lifting most ofthe restrictions on the medium's use.in 1991, the national telecommunications and information administration (ntia) studied the idea,reviewed the considerable literature and past proposals on the subject, and concluded that a "marketbasedsystem for spectrum management would improve considerably the efficiency and fairness of the current u.s.system, and, if properly designed and implemented, can fully address concerns about such issues as competitiveequity, spectrum 'warehousing,' and the preservation of socially desirable services."70 as part of itsrecommendation, ntia supported private transfers and subleasing of spectrum rights directly from one user toanother.71australia and new zealand are ahead of the united states in selling spectrum. the australian federalspectrum management agency recently auctioned 196 wireless cable licenses.72 the government will alsoreceive yearly license fees on the auctioned spectrum.73 the australian broadcasting authority plans to auctionnew television and radio licenses.74 new zealand has auctioned off 20year rights to new radio spectrum andreceives additional revenues from user fees.75 it has also auctioned off spectrum for cellular service76 andbroadcast television.77conclusionthe social cost of zoning the electromagnetic spectrum and wireline media is extremely high. while it isimpossible to quantify exactly the total social cost of such zoning, the welfare effects of specific restrictionssuggest that zoning imposes large opportunity costs on society. zoning misallocates resources; it reducescompetition; and it delays or prevents the provision of desired services. these costs are rarely consideredcarefully when restrictions are imposed.notes1. a study by the wharton econometric forecasting associates (wefa) group attempts to quantify the total costs of all legal and regulatorybarriers in the telecommunications, information services, equipment manufacturing, and video programming markets. the study compares abaseline economic forecast with a forecast assuming that all legal and regulatory barriers to competition are removed and that rateofreturnregulation is replaced by pure price cap regulation in all jurisdictions. the study concludes that "competition and the expected lower pricesthat competition will bring result in nearly $550 billion in consumer savings cumulatively over the next ten years." in its analysis, wefafirst develops pricing models for longdistance, local, cellular, and cable service, and then estimates the impact that competitive entry willhave on prices for longdistance, local, cellular, and cable television service. the study predicts that prices for these services will falldramatically over the study period (1995 to 2005), with both sharp onetime price adjustments (to reduce prices to competitive levels) andsteadily decreasing prices over time due to technological efficiencies. the main impetusestimating the costs of telecommunications regulation300the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.for these price changes, wefa asserts, will be entry of the bell operating companies into the various markets, after the lifting of the videoprogramming ban and the mfj restrictions on interlata service. the wefa group, economic impact of deregulating u.s.communications industries, february 1995 (hereinafter 1995 wefa study).2. for further discussion on the federal regulation of telecommunications, see generally kellogg, michael, john thorne, and peter huber.1992. federal telecommunications law, little, brown & company; thorne, john, peter huber, and michael kellogg. 1995. federalbroadband law, little, brown & company, new york.3. stat. 302 (comp. st. §10100œ10109) (1912).4. see hazlett, t. 1990. "the rationality of u.s. regulation of the broadcast spectrum," 33 j.l. & econ. 133.5. see, for example, tribune co. v. oak leaves (cir. ct., cook county, ill. 1926), reprinted in 68 cong. rec. 216 (1926).6. radio act of 1927, 44 stat. 1162 (1927).7. see emord, jonathan w. 1992. "the first amendment invalidity of fcc content regulations," notre dame journal of law, ethics, andpublic policy 93, 185.8. 47 u.s.c. §307(b).9. radio act of 1927 §1. cf. 47 u.s.c. §301; 47 u.s.c. §304; 47 u.s.c. §309(h)(1).10. radio act of 1927 §9.11. see 47 u.s.c. §307(c).12. 47 u.s.c. §312.13. 47 u.s.c. §312 (a).14. an inquiry relative to the future use of the frequency band 806œ960 mhz, 19 rad. reg. 2d (p & f) 1663, 1676œ1677 (1970).15. an inquiry into the use of bands 825œ845 mhz & 870œ890 mhz for cellular communications systems, 86 f.c.c.2d 469 (1981).16. 47 u.s.c. §214(a).17. 47 u.s.c. §201.18. see 78 cong. rec. 10314 (1934).19. see robinson, glen o. 1989. "the federal communications act: an essay on origins and regulatory purpose," in a legislative historyof the communications act of 1934, max paglin (ed.), p. 40.20. general telephone co. fcc 13 f.c.c.2d 448 (1968), aff'd sub nom. general telephone co. v. fcc, 413 f.2d 390 (d.c. cir.), cert.denied, 396 u.s. 888 (1969).21. applications of telephone companies for section 214 certificates for channel facilities furnished to affiliated community antennatelevision systems, 21 fcc.2d 307, 325 (1970).22. 47 u.s.c. §533(b).23. id.; see also, 47 c.f.r. §63.54(a).24. see telephone companycable television crossownership rules, 7 fcc rcd 5781, 5817œ18 (1991); see also h.r. rep. no. 934, 98thcong., 2d sess. 57 (1984).25. regulatory pricing problems presented by the interdependence of computer and communication facilities, final decision and order, 28f.c.c.2d 267, 269 (1970).26. amendment of §64,702 of the commission's rules & regulations, second computer inquiry, 77 f.c.c.2d 512 (1981).27. 47 u.s.c. §541(b).28. 47 u.s.c. §534(b)(1)(b).29. 47 u.s.c. §§531, 532.30. kwerel, evan, and john r. williams. 1992. "changing channels: voluntary reallocation of uhf television spectrum," office of plansand policy working paper no. 27, federal communications commission, washington, d.c., november.31. memorandum opinion and order, request of fleet call inc. for a waiver and other relief to permit creation of enhanced specializedmobile radio system in six markets, 6 f.c.c. rec. 1533 (1991).32. edge. 1993. "fleet call becomes nextel; new company name reflects new business designed to serve broader wirelesscommunications market," march 29.33. business wire. 1994. "nextel reaches agreement with u.s. department of justice," october 27.34. land mobile radio news. 1994. "motorola gains longawaited foothold in los angeles smr market," november 13, at section no. 46.35. as janice obuchowski, former department of commerce assistant secretary for communications and information and former ntiaadministrator, has said, "efficient use of the spectrum will be maximized only if licensees are given the widest possible latitude indetermining which services to offer within their assigned frequencies. inestimating the costs of telecommunications regulation301the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.principle, the flexibility granted to licensees should be limited only to the extent necessary to prevent radio frequency signal interference withother users." obuchowski, janice. 1994. "the unfinished task of spectrum policy reform," 39 fed. com. l. j. 325, pp. 328œ329.36. yokell, larry. 1994. "cable tv moves into telecom markets," business communications review, november, p. 2.37. affidavit of thomas w. hazlett, attached to motion of bell atlantic corporation, bellsouth corporation, nynex corporation, andsouthwestern bell corporation to vacate the decree, no. 820192 (d.d.c. july 6, 1994).38. following is a brief summary of the data and methodology we employed to make the consumer welfare calculations. (1) we performedtwo sets of calculationsšone based on subscribers and prices of basic service and another based on subscribers and average revenue persubscriber. (2) basic rate calculations were based on data found in the appendix to hazlett (july 3, 1994). calculations were for the years1983 (the year after the mfj) through 1993. we assumed that 98 percent of all basic service subscribers were in monopoly markets. thisassumption was maintained for all the years under consideration, while in reality competition did not exist every year in markets that werecompetitive in 1992. this assumption therefore is likely to provide a downward bias in the estimates. (3) averagerevenuepersubscribercalculations were also based on data found in hazlett. we assumed that the total number of subscribers was the same as the number of basicservice subscribers (since premium service subscribers are also basic service subscribers). we further assumed that 98 percent of totalsubscribers were in monopoly markets. again, downward bias should be expected. (4) we used average basic service prices and averagerevenue per subscriber as the monopoly price. the correct numbers to be used are averages in monopoly markets only. (5) we assumed thatincome effects due to decline in cable prices are negligible. the assumption is generally valid if the share of cable television expenses issmall relative to total income. (6) we assumed demand was locally linear. (7) we converted all calculations to constant 1993 dollars usingthe consumer price index for all goods with 1993 equal to one. (8) we performed the welfare calculations assuming various elasticities ofdemand and expected decline in prices due to duopoly competition. we considered ownprice elasticities of 0.5, 1.0, 1.5, 2.0, and 2.5 andprice declines of 15, 20, and 25 percent. to the extent that a monopolist always operates in the region where elasticity of demand exceedsone, the first two elasticities are valid only if monopoly pricing behavior is somehow constrained.39. crandall, robert. 1990. "elasticity of demand for cable service and the effect of broadcast signals on cable prices," attachment tcicomments to the federal communications commission, mm docket no. 904; levin, stanford l., and john b. meisel. 1990. "cabletelevision and telecommunications: theory, evidence and policy," telecommunications policy, december; emmons, willis, and robinprager. 1993. "the effects of market structure and ownership on prices and service offerings in the u.s. cable television industry," paperpresented to the western economics association 68th annual conference (22 june); federal communications commission, fcc cable ratesurvey database (feb. 24, 1993).40. "tradeline database," dow jones news retrieval, april 21, 1995.41. talking point/bell atlantic court decision, aug. 24, 1993.42. dow jones news wire. 1993. "cable, telecom equip[ment] stocks soar on bell atlantic ruling," august 25.43. this study assumed bell atlantic's beta to be 0.78 and that the relevant market was defined by the s&p 500 basket of stocks. it usedoutstanding shares and prices at the end of calendar year 1992 to calculate the benchmark valuation. a oneday window using the previousday's valuation as the benchmark yielded an increase in bell atlantic value attributable to the ruling of roughly $950 million. twoday andthreeday windows yielded a cumulative excess return of 7 percent and increased valuation of roughly $1.5 billion. a longer window wouldyield an even larger estimate of increased return and valuation.44. affidavit of richard s. higgins and james c. miller iii, attached to motion of bell atlantic corporation, bellsouth corporation,nynex corporation, and southwestern bell corporation to vacate the decree, united states v. western elec. co., no. 820192 (d.d.c.july 6, 1994).45. because of the mfj's interexchange and equal access restrictions, the bocs cannot obtain bulk wholesale rates and therefore cannot offerthese savings to their customers.46. 1995 wefa study, supra n. 1, at p. 4.47. id., p. 30.48. id., pp. 30œ32.49. id., p. 32.50. id., pp. 32œ33.51. id., p. 34.52. a pop is derived by multiplying the total population of a service area by an operator's interest in the license.53. morgan stanley & co., pacific telesis groupšcompany report, report no. 1219882, apr. 21, 1992.estimating the costs of telecommunications regulation302the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.54. donaldson, lufkin & jenrette. 1994. "the wireless communications industry," summer, p. 68.55. macavoy, paul w. 1994. "tacit collusion by regulation: pricing of interstate longdistance telephone service," working paper #37,yale school of organization and management, august, pp. 37œ47.56. ward, michael r. 1995. "measurements of market power in long distance telecommunications," bureau of economics staff report,federal trade commission, washington, d.c., april.57. 1995 wefa study, supra n. 1.58. milton mueller has explained that "as a barrier to entry, getting an allocation through the administrative process can be far moreformidable than paying for access to spectrum." mueller, milton. 1988. "technical standards: the market and radio frequency allocation,"telecommunications policy, march, p. 51.59. kwerel, e., and a.d. felker. 1985. "using auctions to select fcc licensees," fcc opp working paper, may, pp. 12 and 17.60. id.61. affidavit of paul h. rubin (june 14, 1994), attached to motion of bell atlantic corporation, bellsouth corporation, nynexcorporation, and southwestern bell corporation to vacate the decree, united states v. western elec. co., no. 820192 (d.d.c. july 6, 1994).62. rubin averaged data provided in several waiver requests on the fixed and annual costs of delaying approval of the waiver, andextrapolated these costs over all waivers. because over 96 percent of waiver requests are approved, and thus presumed to be procompetitive,and because individual anticompetitive requests are less harmful than individual procompetitive waivers are beneficial, the waiver processresults in significant wasted resources, time, and money. the costs due to rentseeking and deterrence of procompetitive activities arespecified in less exact terms, but rubin's reasonable estimate places them in the hundreds of millions of dollars.63. id.64. report and order, inquiry into the development of regulatory policy in regard to direct broadcast satellites for the period followingthe 1983 regional administrative radio conference, 90 f.c.c.2d 676 (1982), recon. denied, memorandum opinion and order, regulatorypolicy regarding the direct broadcast satellite service, 94 f.c.c.2d 741 (1983), aff'd in part sub nom. national association of broadcastersv. fcc, 740 f.2d 1190 (d.c. cir. 1984).65. see world communications v. fcc, 735 f.2d 1465 (d.c.cir. 1984).66. spectrum can be subdivided into a main channel and a number of "subchannels" or "subcarriers," both of which can be transmittedsimultaneously.67. first report and order, dkt. no. 82536, 48 fed. reg. 28445 (1983); second report and order, dkt. no. 21323, 49 fed. reg. 18100(1984); amendment of parts 2 and 73 of the commission's am broadcast rules concerning the use of the am subcarrier, 100 f.c.c.2d 5(1984).68. amendment of parts 2, 73, and 76 of the commission's rules to authorize the offering of data transmission services on the verticalblanking interval by tv stations, 101 f.c.c.2d 973 (1985).69. coase, r.h. 1959. "the federal communications commission," 2 j.l. & econ. 1, pp. 14 and 25.70. national telecommunications and information administration. 1991. u.s. spectrum management policy: agenda for the future,february, p. 7. henry geller, former ntia administrator and former general counsel for the fcc, coauthored a report that explained that"charging for the spectrum is particularly appropriate now in light of the weakness of traditional broadcast public trustee regulation and thegrowing demands on the spectrum overall." geller, henry, and donna lampert. 1989. "charging for spectrum use," benton foundationproject on communications and information policy options, p. ii.71. national telecommunications and information administration. 1992. u.s. spectrum management policy: agenda for the future,february, p. 8.72. market reports. 1994. "australiašpay tv overview," september, p. 19.73. ellis, stephen. 1994. "australia: mds paytv auctions net $90.6m," australian financial review, august 19.74. davies, anne. 1995. "australia: facts lay groundwork for aba legal challenge," sydney morning herald, january 27.75. new zealand herald. 1991. "new zealand: seven radio frequencies for auckland," january 12.76. new zealand herald. 1990. "new zealand: telecom free to bid for mobile phones," may 17.77. new zealand herald. 1990. "new zealand: space on airwaves costs sky $2.2m," march 28.estimating the costs of telecommunications regulation303the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.38residential pc access: issues with bandwidth availabilitykevin c. kahn, intel corporationabstractthe preeminent interactive information access device in the business world today is clearly the personalcomputer. via the pc, individuals access the exploding array of information sources both within their businessesand throughout the internet. pcs are rapidly penetrating the consumer environment as well, and we stronglybelieve that the pc will become the center for national information infrastructure (nii) access for the residentialconsumer. the intelligence encoded in pc applications and their improving human interfaces will make pcs thetool of choice for ubiquitous consumer information access. however, to achieve their great potential in this role,residential consumer pcs must have access to adequate communication bandwidth. this bandwidth must bepriced to be attractive to the residential consumer. the consumer must be free to access all information servicesusing this bandwidth.highbandwidth access to information services must get the same level of attention with respect to publicpolicy as providing competitive television and telephone delivery systems. this attention is needed to ensure thatthe bandwidth, access, and consumer choices are made available in ways that promote the growth of consumernii use.this paper develops these themes by examining the forces that are driving corporate pc network access tothe nii and the various network topologies being discussed for the residential environment (hfc, fttc, etc.),as well as possible future service directions. we then develop what we feel are the critical requirements forbandwidth availability for residential pcs.statement of problemthe personal computer has become a ubiquitous networked communications platform within the businessenvironment. in addition to traditional communications applications such as electronic mail, it is now becomingthe base for all sorts of information access and personal communications tools. these tools enable a new level ofbusiness activity that includes everything from world wide web access for general information acquisition tovarious levels of electronic commerce and personal conferencing. key to this development of widespread, costeffective information and communications applications have been a number of important technologies. firstamong these has been the deployment of highbandwidth network connections in both the local and wide areas,utilizing highvolume components, based upon open standards. free and open access to this bandwidth haspermitted any software or hardware developer or information service provider to easily enter these businesses.for example, there are numerous suppliers of network connection hardware and software, a growing number ofcompeting conferencing products, and the beginning deployment of multiple online news services fromtraditional and nontraditional information publishers utilizing the world wide web. the resultant competitionand interactions are leading to rapid development of the business use of the developing nii.the same dynamics must be allowed to operate within the critical residential environment to support thedevelopment of individual utilization of the nii. while there has been a lot of focus on the larger infrastructuredeveloping to support the nii, particularly in the context of the internet, there has been less focus on the dataaccess issues of the ''last mile" or access network to the residence. what discussion has occurred seems torevolve largely around videoondemand entertainment services and telephony. we are concerned that whilepublic policy may operate to guarantee competitive entertainment services and telephony over cable or telephonyinfrastructures, it might not guarantee the deployment of the reasonable levels of openly available, discretionaryresidential pc access: issues with bandwidth availability304the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.bandwidth required to energize an affordable and comprehensive personal computerbased informationinfrastructure.the deployment of bandwidth that can be utilized in an open manner to access nii services from the homeneeds to be encouraged. for data, this access should be via general packet protocols (based on the internetstandards of tcp/ip) that permit users direct interaction with any service they desire. specifically, data servicesfrom the home should come to be viewed like existing telephony services that allow consumers to access anyservices anywhere in the nation. it may be that for other multimedia services, such access would be betterprovided via circuitoriented services such as switched atm virtual circuits (although promising work isongoing on utilizing packet protocols for this as well), but in any case the same principles of openness and fullconnectivity must apply. we also believe that the development of industrydriven standards or implementationagreements for attaching to and utilizing this open bandwidth will be critical to the creation of a competitiveenvironment for consumer information applications. the government role should be to encourage rapid industryconvergence on such de facto standards or implementation agreements that can later be the basis of more formalde jure ones. this paper attempts to lay out some requirements for the development of consumer pc informationservices broadly within the nii.backgroundthe business environment as an illustrative exampleas a point of reference let us first consider the typical business access being deployed to allow pcs toutilize the developing nii. the typical networked pc in the office is attached to a 10mbps ethernet or similarperforming token ring network. (international data corporation estimates that in 1994, 73 percent of thebusiness pcs in the united states were attached to local area networks.) while this bandwidth is shared with theother users of the network in most cases, switched networks are beginning to be deployed that replace the sharedaccess with dedicated access. in addition, technologies such as 100mbps ethernet are appearing that will alsogreatly improve the bandwidth available to the individual user. beyond the local environment, most largecorporations are deploying reasonable bandwidth into the internet at large. this combination makes availablerelatively highspeed access to information and services whether local to the business desktop or remote.in addition to the bandwidth that is available, another key aspect of the business desktop is the developmentof standards. industrydriven network connection standards have driven down the cost of connecting to thenetwork from the pc. for example, ethernet controller cards for pcs now typically sell for less than $100,operating systems increasingly come with protocol software built in, and a growing number of applicationscomprehend networking capabilities. platform software standards have made it possible for creative developersto build software independent of the nature of the specific network. for example, the winsock de facto standardfor accessing network services on the microsoft windows operating system is allowing application developersto focus their efforts on enhancing function in their products rather than on adapting those applications to avariety of different, incompatible, network services.network protocol standards have allowed endtoend services to operate over a wide variety of networkimplementations. in particular, general packet networks have allowed a wide variety of data services as well asnew applications such as video conferencing to begin to operate without special provisions from the providers ofthe networks. an entrepreneurial developer need not make deals with a variety of platform and networkproviders to begin to deploy an application in this environment. neither is an interested business consumerrestricted from beginning to take advantage of such new services by the choices offered him by various networksuppliers.key aspects of the business environment are that it has been almost entirely industry driven and motivatedby competition. de facto standards or implementation agreements have been rapidly developed and only laterevolved into de jure standards. throughout the evolution of the business and the associated standards, theintellectual property of the participants has been respected by the processes. it is interesting to note that the moreponderous de jure first approach to standards represented by the international telecommunications union (theofficial international body for defining standards within the telecommunications industry) has been considerablyresidential pc access: issues with bandwidth availability305the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.less successful in affecting the u.s. environment than have the more chaotic and rapid moving processes ofprivate company consortia or the internet engineering task force (the de facto group that defines internetstandards).a couple of other interesting aspects of packet networks are typical in the business data environment. endsystems are always available to be connected tošthat is, they are always "online." it is thus reasonable to buildapplications that initiate connection to a user's system from a service at any time. this is different from anenvironment where, unless the user has initiated a connection to the network, information cannot find its way tothe user's end system. related to this is the fact that packet networks inherently support multiple simultaneousconnections. that is, an end system can have an unlimited number of distinct applications running at the sametime, each of which is using some of the physical bandwidth to connect to another distinct end system. there isno notion of the line being busy. again, this flexibility opens up many possibilities for applications that operatein parallel with each other over the same physical connection. it allows serviceinitiated contact with an endsystem even while the user is doing other things over his connection.the current residential environmentwe can compare this business situation to that seen in the residential environment, first by looking narrowlyat what is really available today, and then more importantly at what is being tested and deployed for the nearfuture. today's electronic service to the home consists of a number of options.first among these is the existing plain old telephone service (pots). this is universally available andprovides a completely open but relatively low bandwidth access to data services for the consumer. using 14.4kbps and more recently 28.8kbps modems, the consumer can connect to any information service or network.traditional choices have been the private information services such as compuserve, prodigy, or americaonline. however, there are a growing number of general internet access providers available via this route aswell. these provide general packet access into the internet and thus to any information service in the developingnii.an improvement over pots in terms of available bandwidth is an integrated services digital network(isdn) line, which potentially provides up to 128kbps access over a single wire. fewer information service orinternet access providers have thus far deployed isdn access, and the availability and practicality of utilizingthis level of service varies considerably around the country. like pots, this is a circuitoriented service thatmust be initiated by the consumer. that is, unlike the typical business connection, an information service usuallycannot autonomously contact a consumer's pc to provide information or service. also, while more flexible insome respects than pots, isdn is for the most part not useable by multiple separate applicationssimultaneously, thus further limiting the range of applications that can utilize it. isdn can be used effectively toconnect an end system to a general purpose router that provides a point of presence for the internet or othergeneral purpose network. used in this manner, isdn may provide our best shortterm hope for general purposeresidential access. also, since isdn can provide fast connections to another end system when compared withpots, it can approximate the speed of being always connected for certain types of applications. while inprinciple pots can also be used in this manner, the combination of increased bandwidth and connection speedmakes isdn much more practical in such a configuration.in contrast to these pointtopoint telephony based services, the cable industry has focused on highbandwidth broadcast services for video. the cable infrastructure is typically highly asymmetric in its bandwidthwith much higher speed available into the home than out. currently, in fact, most cable systems have no returnpath available. also, cable is unlike the telephone where a consumer can call anyone or access any service onany other existing telephone system. the services provided today via the cable infrastructure are generallychosen by the cable system operator or according to the "must carry" rules. cable systems are generally closedsystems that encompass everything from the content source to the settop box, which provides a conventionaltelevision signal to the appliance.there are beginning to be trials of data services over the cable infrastructure. however, in the spirit of theexisting industry, these may tend to be for services driven and chosen by the service provider rather than theconsumer. in the case of cable tv programming, the limited number of available channels means that the msoresidential pc access: issues with bandwidth availability306the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.must select which channels to broadcast. for data services, however, open packet access means that beyond theprovision of the data bandwidth, there is no need to limit access to content in this manner. on the positive side,the cable infrastructure brings the promise of relatively nearterm residential broadband service. cable couldcertainly offer the advantages of highbandwidth, multisession, alwayson data delivery. in particular, byoffering highquality internet access, cable could provide much of the flexibility we desire. it is important thatpc access to broadband information services follow the telephone model rather than the broadcast tv model ofaccess.analysis, directions in the residential environment, and forecastsaccess network technologiestwo major groups of companies are vying to provide digital services to the residential environment: theexisting local exchange carriers and the cable entertainment companies. the former are building on theirexpertise in operating a highly reliable telephony network by offering higherbandwidth services through isdnand through cable overlay systems that are capable of supplying broadband residential service. the latter areleveraging their existing investment in a physical plant oriented to deliver many analog television channels sothat they can begin to offer digital entertainment channels and various levels of broadband data communicationsto the home. as a part of this enhancement of services they frequently wish to offer telephony services via thisnetwork as well. this competition should be a positive force for consumers in the long term, provided that itoffers real competitive choices to the consumer as well as to service providers.a number of physical architectures have emerged for deployment by these companies. the most popular ofthese include the following: hybrid fiber coax (hfc): in this scheme, optical fiber is used to bring signals from a head end toneighborhood nodes. at these nodes, the signals are placed on a coax system for distribution to theresidences. the cable industry target for residences served by a single coax segment is 125, although in earlydeployments the number is larger. since the coax is a shared medium, the homes on a single coax segmentshare the available bandwidth. for most current deployments, the amount of bandwidth available from thehome (either on the cable or via a separate pots path) is much smaller than that available to the home. hfcis particularly attractive as an upgrade path from existing analog cable systems. fiber to the curb (fttc): in this scheme, optical fiber is used to bring signals from a head end to a pedestalon the street that is within a relatively short distance of a collection of served homes. at the pedestal, signalsare placed on coax or twisted pair lines to the home. while the optical fiber bandwidth is shared (as inhfc), the drop to the individual home is not. fiber to the home (ftth): in this scheme, optical fiber is deployed all the way to the home, where it isconverted as appropriate to provide digital services. at this point, this scheme is not being pursued to ourknowledge in the united states, although it has been proposed at times in other national markets. asynchronous digital subscriber line (adsl): for areas where the length of the lines from the last point ofelectronics to the served homes is bounded, adsl provides a technique for utilizing existing telephonyinfrastructure to carry higher data rates. it uses more complex signaling electronics over existing copper toprovide data rates capable of supplying digital video.beyond these wired topologies, experiments are also beginning that use wireless technologies. for example,direct pc utilizing direct broadcast satellite technology provides an asymmetric bandwidth connection similar tousing cable combined with pots for a return path.residential pc access: issues with bandwidth availability307the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.bandwidth and parallelisma common theme among all of these approaches is that the allocation of bandwidth into the home isdifferent from the allocation of bandwidth out of the home. from a purely architectural perspective this isunfortunate, since it is an asymmetry that makes an artificial distinction between providers (who need highoutbound bandwidth) and consumers (who can get by without high outbound bandwidth). in the full nii wheretelecommuting will be common, video telephony will be widespread, and a true cottage information industry willbecome an important part of the national economy, this asymmetry will become a barrier. however, as apractical matter for the shortto midterm, the key will be reasonably high bandwidth into the home and moderatebandwidth out. this arrangement will cater to information retrieval applications where the bulk of the data flowis inbound with much smaller flows out to make queries. provided that the outbound bandwidth is high enough,it can also support limited video telephony and telecommuting applications. we and others have demonstratedthe practicality of such application at isdn speeds (128 kbps) via products like prosharež videoconferencingand remoteexpressž remote lan access. nevertheless, higher rates would greatly improve performance aswell as support more parallel activities.for the long term it is critical that residential services support a full range of simultaneous activities withinthe home. it should be possible for children to be involved in remote education activities while one parent isactively telecommuting, the other is accessing entertainment or shopping services, and power or securitymonitoring is operating in the background. solutions that force artificial limits on conducting such parallelactivities will make it impractical to depend upon network access as an integral part of the home. furthermore,each of these individual activities may require simultaneous sessions. a child may be connected both to schooland to a library system; a telecommuter may be accessing corporate databases while also checking travelarrangements with an airline. a shopper may be doing comparisons on products offered by competing providers.from a technical perspective, the solution to these multiple access issues is either the provision of multiplevirtual circuits or a general packet network interface (or more likely some combination of these technologies).openness of accessa key to allowing these applications, even in the face of the sorts of bandwidth available in the neartermdeployments, is to put the use of the bandwidth completely under the control of the consumer. consider twoapproaches to providing access to information services. in the first, a cable system contracts to make availableone of the online service providers (say, compuserve) using some of its digital bandwidth to the home. from theconsumer's point of view a bundled package is offered that allows direct connection to the service for some fee.the service being provided by the cable operator is compuserve and not generic online access. if consumerswish to subscribe to a different service, they cannot use the cable bandwidth to do it. furthermore, if a newprovider wishes to enter the business and have good access to customers, it may need to arrange business dealswith many different network service providers. the comparable situation in the telephony industry would be thatthe range of people consumers could call from home would be affected by their choice of local or long distancecarrier.the alternate approach is typified by the existing lowbandwidth telephony access system and the growinginternet service providers. here, the customer gets access to the general packet switched network and uses that toprovide common carriage of data to various information service providers. isdn can be used in thisconfiguration to provide moderate rate connection to a router from which general internet connectivity can beprovided. note that the owner of the access network is not involved in the selection of the available services.that selection is between the consumer and the ultimate provider. today, this access is the norm for telephonyclass access. as we move toward higherspeed access networks, it is important that the model exist forbroadband as well. again the comparable situation in the telephone industry is that anyone can establish abusiness phone number and then advertise directly to potential consumers for business without furtherinvolvement of the network providers. where cable companies choose to provide internet access on the cable,this fits this model as well.residential pc access: issues with bandwidth availability308the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the argument here is not that cable operators should be precluded from offering packaged services. theyshould certainly be allowed to offer them as well as local content, provided that they also provide consumercontrolled access to competitive offerings. openness should exist in both directions. that is, providers should beable to attach to access networks to offer their services to consumers (similar to requirements for allowing videoservice providers to access "video dialtones"). at the same time, consumers should be able to attach via theiraccess networks to providers who have not chosen to directly connect to that access network.openness of content and the development of new serviceswe do not need to wait for the implementation of all digital networks and the general nii to see examplesof why we are concerned with the issue of open bandwidth access. we can find today in analog distributionsystems an interesting example of how fragile the relationship between openness of network capabilities, contentprovider innovation, and application creativity can be. this example appears in the use and distribution ofmaterial in the vertical blanking interval (vbi) of broadcast video signals. note that the issues in this exampleare more complex than open access to bandwidth since they deal with bandwidth ownership and contractsbetween msos and content providers. however, the example does serve to illustrate how subtle issues in thenature of the distribution networks can affect the creation and deployment of new applications.standard u.s. television signals must provide a period of time in each frame to allow the electron beam thatpaints the screen to move from the end of the screen back to the top in preparation for painting the next set oflines. this period is the vbi and can be seen as a black bar if the vertical hold of a television is misaligned.since this part of the signal is not used for carrying the picture, it can be used to carry other sorts of data withoutdisturbing the video broadcast. the most common example of this is closed captioning, which is carried in a partof the vbi.as broadcasters begin to think creatively about their content and work with application developers, otherpossible uses of the vbi to enhance the broadcast material can emerge. examples include digital forms of thestock ticker seen on some news networks, digital references to sources of additional information to augment thecontent of a show, digital indexing information to assist in selective viewing of the material, and undoubtedlymany more. however, any such programenhancing use of the vbi must reach the end consumer to be useful.while the 1992 communications act required that programrelated material in the vbi should be carried bylocal cable operators, exactly what constitutes program related is debatable. as a result, there is no guarantee thata content provider who finds a creative use of the vbi to provide, in conjunction with an application developer,an enhanced service will actually be able to deliver that service to the consumer.this issue can be viewed as an example of the difficulty of defining what openness in access means, evenwithin the existing rather limited sorts of broadband distribution that exist today. a service may no longer be atwoparty transaction between the provider and the end customer. it may now involve at least three parties andbecome dependent upon the cooperation of the access network provider. this substantially raises the hurdle toentering the business with a new service, particularly one that may be of interest to only a small percentage ofconsumers. even though a broadcast channel is already carried by the vast majority of access network providers,it may be necessary to reengage all such network providers to even begin to offer a service. this certainlyprovides a barrier to innovation that need not be present. it is exactly this sort of barrier that we are concernednot be erected as digital services begin to be deployed. it must be possible for innovation to be between theprovider and the consumer over an open digital highway. conversely, it must also be possible for the networkservice provider to benefit from the expanded use of the network.openness of equipmentanother key difference between the current directions in the cable access networks and the existingtelephony networks is important to the rest of this discussion. this is the issue of open equipment connectivityand ownership of customer premises equipment (cpe). today, the consumer owns the telephone, the tv, the pc,residential pc access: issues with bandwidth availability309the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.and the modem for the pc. however, it is generally the case that the cable access provider owns and provides thesettop box. furthermore, the lack of standards for the physical network may make it more difficult to find analternative supplier of cpe. this is quite different from the telephony world, where the access network providerterminates his network at a demarcation point, providing a standard form of connection to which customerselected and customerowned cpe can be attached. the combination of the ownership and the standards aspectsof this arrangement have together spawned a vigorous industry in cpe suppliers. for telephony, the connectivitystandard is the rj11 connector and the set of electrical and tone signaling standards supported over it. as aresult, pc access to networks via modems can be achieved with the same equipment, anywhere in the country.the importance of the openness of cpe is beginning to be understood. for example, a press release fromcongressmen bliley and markey issued on march 16, 1995, states,restricting consumers' ability to purchase and own cable "settop boxes" and other communications interfaceequipment is like putting a straightjacket on technological development.– there's no incentive for improvementbecause there's no competition.– today's advanced phones only happened because of a healthy, competitive retailmarketšand so did the revolution in computer modems and fax machines that followed.however, there is more to energizing the broadband cpe industry than just allowing retail purchase of thesettop box. consumers must see an advantage in purchasing the equipment, and vendors must see a large marketto be served in order to invest in developing products that will attract consumers. neither of these will happenvery quickly without the development of standards for the point of attachment of the equipment.consider how likely consumers would be to purchase featureladen phones if they could not expect thosephones to work after they moved homes. likewise, consider how likely a company would be to develop aninnovative new phone product knowing that it could be sold only to that set of consumers who were served bysome specific phone companies, and that the company would also have to deal with unhappy consumers whodiscovered that the product ceased to work after they changed providers. our concern is that consumers be ableto buy pc equipment and applications that will effectively attach anywhere in the united states to the nii, andthat they be able to freely move such equipment and applications with them. industrydriven discussions areunder way in various industry groups (e.g., the atm forum) toward the establishment of such de facto standardsor implementation agreements.affordabilitya considerable concern exists about the affordability of an open consumer broadband data service. clearly,the bandwidth must be priced at a level that will allow reasonable access to a broad spectrum of users. true opencompetition should cause this to occur, as can be seen by looking at how other uses of the bandwidth in theaccess network might be priced. for example, consider a higherlevel service that has been proposed, namelyvideo on demand (vod). this service promises to deliver to the consumer an online replacement for videotapes(and eventually probably much more interactive sorts of experiences). assuming that one uses mpeg2 tocompress a 2hour movie, then this service must deliver on the order or 4 to 6 mbps for 2 hours to the consumerat a price competitive with what she can rent the movie for today (on the order of $3.00). vod is an individualservice delivered to a single consumer at a time. thus it is reasonable to expect that competition will drive thecost of similar downstream bandwidth of an unrestricted sort to be similar. (actually, the resources to deliver theunrestricted bandwidth are less since no server is involved.) clearly, this accounts only for the access networkprovider part of the unrestricted service, but it at least provides a starting point. it is harder to estimate what aconsumer should be paying for upstream bandwidth, but a similar sort of analysis for a bundled service thatmakes more upstream demands (perhaps shopping or interactive gaming) should provide an estimate there aswell. we are not trying to suggest any particular price structure for consumercontrolled bandwidth, nor are wesuggesting price regulation. rather, we are trying to suggest that competitive forces should cause it to be pricedin a somewhat similar manner to similar levels of bandwidth utilized for services bundled by the local accessnetwork provider.residential pc access: issues with bandwidth availability310the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.an architecture for a longterm residential niithe actual deployment of broadband services to the residential nii will be evolutionary from today'sphysical networks. economic and practical considerations preclude any revolutionary replacement of accessnetworks with some uniform and fullservice digital infrastructure. as a result of this evolutionary reality manyof the requirements we have for a longterm infrastructure cannot be fully met in the short term. indeed, some ofthe standards that we believe will be critical to providing the full potential of the nii are still in embryonicstages. nevertheless, it is important to develop a vision of where we would ideally like to wind up if we are togenerate public policies that encourage development to proceed toward an endpoint architecture that can realizeall the potentials of the nii.we believe that the architecture should be broken into three parts: core networks, access networks, andhome networks (figure 1). core networks are those networks that provide generalized connectivity both locallyand across wide geographical areas. they correspond in today's telephony infrastructure to the longdistancecarriers plus that part of the local exchange carriers' infrastructure that carries traffic between the central officesthat serve customers. access networks are those networks that connect the core networks to residences. forexample, the hfc networks that are being considered for deployment to connect regional hub systems to homesare access networks. finally, home networks are those that operate within the residence.figure 1 elements of an architecture for realizing the full potential of a national information infrastructure. atm,asynchronous transfer mode; hfc, hybrid fiber coaxial; fttc, fiber to the curb; ftth, fiber to the home; andadsl, asynchronous digital subscriber line.note that any of these networks can be nonexistent in a specific deployment. in an existing cable systemusing hfc to deliver only broadcasttype services to dedicated settop boxes in a home, neither a core networknor a home network may exist. likewise, one could view the current telephony infrastructure as often not havingan access or home network, since for practical purposes it generally appears that the cpe connects directly to thecore network.we expect that core networks will typically be those with symmetric bandwidth. considering today's trends,these are likely to be atmbased networks. we also believe that it is in the interest of the carriers that providecore networks to encourage use of bandwidth by consumers. there is today competition among core networkproviders in the telephony longdistance business, and we would encourage such competition to continue into thebroadband world. given this competition, the easy access by the consumer to multiple core network providers,and the desire of those providers to sell their bandwidth, we believe that the requirement of sufficient, affordable,consumerdirected bandwidth will be met in these networks. furthermore, there is already much momentum inthe standards associated with communications over these networks (ranging from tcp/ip to atm) so thateffective consumer utilization of this bandwidth seems possible.residential pc access: issues with bandwidth availability311the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.home networks are not yet at all well defined. however, as one sees more digital information appliancesdeployed within the home, the desire to interconnect them will increase, and this will give rise to various sorts ofhome networks. displaying images from security cameras on a tv or pc, driving an interactive program on atv from a pc, or operating a multiplayer game across multiple tvs or pcs in one or multiple homes are allexamples of applications that will involve a home network. home networks are likely to be fairly symmetric, iffor no other reason than that there will likely not be a single special "source" to drive any asymmetry.access networks may well remain asymmetric regarding bandwidth due to the economics of some of thepopular access network topologies. for example, hfc will likely continue to supply much more downstreambandwidth than upstream bandwidth for the foreseeable future. we are most concerned about the evolution of theaccess networks since they are the critical bridge between the consumer and the core networks. while theconsumer can choose what to deploy at home, and there is already a path to active competition in the corenetworks, the access networks may have much less natural competition.once one understands these three related networks, it becomes clear that the critical issue for them is todefine stable interfaces that can allow them to evolve independently while still delivering to the consumer a highlevel of service. the connection standards between the access networks and the core networks are already gettingsome amount of attention via the discussion of open video service providers. that is, this interface is the one thata service provider will be concerned about, whether the service is a local video store or a general longdistancecarrier service. the connection standards between the access network and the home network have received lessattention. this is the interface discussed above in the section titled "openness of equipment." we believe that itwill be critical to elevate the discussion of this interface and build policy that actively separates the homenetwork from the access network via appropriate standards for the reasons discussed above. in addition to thestandards, this is also the interface across which we believe we must guarantee reasonable consumerdirectedbandwidth.if we define effective standards at each of these points, then it should be possible for an informationindustry to develop that supplies consumers with equipment and applications that allow wide exploitation of thenii. in our vision, consumer hardware has a common socket for attaching to the nii. it uses common protocolsfor interacting with nii services. these protocols operate in the packetized internet world and in the home toallow easy access to the educational and information resources of the internet. the typical home has access tosufficient bandwidth to make access to these network resources a pleasant experience. the choices of whatnetwork services are available to the residential consumer are essentially unbounded. the local access networkprovider may choose to package and sell some services, thus making them easier to use, but there are noroadblocks to open consumer access to the nii at large.the key to all of this is the existence of the interface agreements that allow development on either side ofthe interface to progress independently and that do not overly constrain the sorts of implementations permissiblebetween the interfaces. without such industrydriven standards or implementation agreements we will be indanger of one of two extremes. on the one hand, their lack will cause the coupling of what should be distinctparts of the nii, thus slowing development of what should be independent parts. on the other hand, theiroverspecification will stifle creativity by admitting only a single family of products that can operate within thenii.with open bandwidth and protocols, intelligence can exist at the periphery of the network as well as insideit. an innovator with a good idea can create an information service as an end point on the network and sell to theconsumer an access tool that resides at the consumer's system. in doing so there will not be any impediments dueto a need to negotiate business deals with various network providers. this openness will lead to an opportunityfor much greater innovation than that possible with an architecture that gives preference to the provision ofservice intelligence only inside the networks.recommendationsšrequirements for widespread pc access1. provide reasonable levels of bandwidth to and from the home at consumer costs. data bandwidth into thehome should probably at least mirror what is available today for the corporate user accessing the internet.this argues for a minimum of t1 (1.5 mbps)level rates to each home, and more likely, for peak levels ofat least ethernet (10mbps) speeds. obviously more is better, but the key issue is that the consumer mustbe able toresidential pc access: issues with bandwidth availability312the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.access rich multimedia content at speeds that make the operation a pleasant one. initially, it is acceptablethat the bandwidth from the home be less than that into it. however, extremely low outbound bandwidthwill limit the efficiency of any interaction. furthermore, there are certainly interesting applications thatwill be enabled only by relatively high outbound bandwidth as well (for example, work at home or videosecurity). ultimately, the access network should not overly constrain who can be the providers of servicesand who can be only consumers. with the right bandwidth levels, the nii can truly energize a grassroots,distributed economy. the first step in this direction can come from the rapid, ubiquitous deployment ofisdn to the residential environment. while it does not meet the full requirements, it can quickly get usmoving toward the long term and can allow interesting applications to begin to be deployed as it isembraced by existing service providers.2. permit consumer control over the use of this bandwidth to conduct simultaneous digital interactions withmultiple services via open packet protocols. it must be possible for intelligence in the user's system toefficiently access multiple services in the nii to generate value at the end system by the combination ofservices. the openness of the protocols and of the connectivity they provide is absolutely key to enablingthe development of new services and to competition among service providers. consumers should be ableto access any service in the nii, just as today they can access any telephone from their residentialtelephones. the choice of local access provider should not preclude this accessibility.3. it must be reasonable for multiple pcs and other information appliances within a residence to besimultaneously accessing the internet or other broadband services. the fact that a child is conducting aneducational interaction with some service on a pc should not preclude a parent from conducting afinancial transaction at the same time. it is desirable, in fact, that all home information appliances beviewed as equals to that there is at least logical symmetry between originating information inside oroutside the home for consumption inside or outside the home.4. encourage sufficient standards to facilitate a commodity consumer business on at least the national levelfor pc connectivity. competition drives costs down and service levels up. however, meaningfulcompetition cannot take place unless a large marketplace becomes available to the potential suppliers.consider the differences in corporate networking costs today versus 15 years ago. standards such asethernet have energized an entire industry, with the result that the cost of connectivity for an end systemhas approached $100. in the consumer market segment, the standards in place for telephone connectivityhave done the same thing for products that attach to an rj11 socket. this benefit will not accrue to niiconnectivity if every regional or metropolitan market segment requires different consumer equipment forconnectivity. for example, the consumer is not served well by the multiplicity of isdn "standards"across the country. a resident of the united states should be able to move nii equipment anywhere in thecountry and use it effectively, just as can be done today with other appliances. it is important to note,however, that the most effective standards in the business world are those that have been initiallyindustrydriven, de facto ones that only later were codified into a de jure form. (for example, the nearlyubiquitous ethernet and tcp/ip came about in this manner, while the osi protocols represented anattempt to drive standards from the de jure side first.) the role of the government should not be to imposestandards but rather to create policy that facilitates the rapid development of appropriate industrial ones.5. permit consumer ownership of the periphery of the network in terms of what kind of equipment isconnected. (of course, this should not preclude an access network provider from leasing equipment to theconsumer as an alternative.) similar to the previous point, we can see an entire industry that hasinnovated based on the ability of companies to develop products to address consumer needs, withoutneeding the permission, or worse, the active participation of intermediaries to become successful. look atthe innovation that has occurred since the carterfone decision opened up access and ownership of endequipment for the telephone network. the evolution of capabilities offered by the personal computer isanother example of marketdriven innovation. locking access networks into an environment where onlyproviderowned or providersanctioned interfaces are permitted to be attached to them recreates the oldphone network and its constrained equipment competitive environment.residential pc access: issues with bandwidth availability313the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.conclusionsit is not clear to us at this point that the natural and policy forces at work in the emerging nii will achieveall the important results outlined above. there is, unfortunately, something of a chickenandegg problemconcerning open residential use of the nii. the emergence of innovative nii applications utilizing residentialbroadband capabilities depends on the existence of those capabilities, while justification for the deployment ofthose capabilities requires hypothesizing the existence of those applications. as a result, we have a situationwhere to some extent policy choices may need to precede market development. we do not suggest a movetoward a highly regulated environment, since ultimately that is completely counterproductive to the developmentof a new industry. however, we do suggest that policymakers need to find ways to encourage the development ofan eventual architecture that supports the full potential of the nii.to some extent this notion is already present in the types of tradeoffs being made to balance the cableindustry's and telephony industry's developing competition in each other's businesses. we are simplyencouraging policymakers to take a broader view of this set of problems that looks beyond this level ofcompetition to include the rest of the infrastructure that the consumer will need to become a full citizen on thenii. policymakers should see that more is involved than allowing content providers open access to consumers orconsidering what the competitive tradeoffs should be between allowing cable systems to offer dialtone andtelephone companies to offer video. they also need to look at the provision of general data services from theperspective of the consumer. we believe that more attention should be given to the provision of open,standardized, commoditypriced network access to the nii at large. we believe that only with this capability canwe tap the pc's full potential to become the consumer's interactive access point to nii services and bring to theresidential consumer the dynamics that have so dramatically benefited the corporate pc user.residential pc access: issues with bandwidth availability314the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.39the national information infrastructure: a highperformance computing and communications perspectiverandy h. katz, university of california at berkeleywilliam l. scherlis, carnegie mellon universitystephen l. squires, advanced research projects agencyabstractinformation infrastructure involves more than the building of faster and larger computers and the creation offaster and better communications networks. the essence of information infrastructure is a diverse array of highlevel information services, provided in an environment of pervasive computing and computer communications.these services enable users to locate, manage, and share information of all kinds, conduct commerce, andautomate a wide range of business and governmental processes. key to this is a broad array of rapidly evolvingcommonalities, such as protocols, architectural interfaces, and benchmark suites. these commonalities may becodified as standards or, more likely, manifest as generally accepted convention in the marketplace.information technology has become essential in sectors such as health care, education, design andmanufacturing, financial services, and government service, but there are barriers to further exploitation ofinformation technology. pervasive adoption of specific service capabilities, which elevates those capabilitiesfrom mere valueadded services to infrastructural elements, is possible only when value can be delivered withacceptable technological and commercial risk, and with an evolutionary path rapidly responsive to technologicalinnovation and changing needs. private and publicsector investment in national information infrastructure (nii)is enabling increased sectorwide exploitation of information technologies in these national applications areas.although the private sector must lead in the building and operation of the information infrastructure, governmentmust remain a principal catalyst of its creation, adoption, and evolution.this paper explores the barriers to achieving nii and suggests appropriate roles for government to play infostering an nii that can be pervasively adopted. the main locus of government activity in research and earlystage technology development is the federal high performance computing and communications (hpcc)program. this program is evolving under the leadership of the national science and technology council'scommittee on information and communications.introductioninformation technologies are broadly employed in nearly all sectors of the economy, and with remarkableimpact. nonetheless, there are still enormous unrealized benefits to be obtained from effective application ofinformation technology, particularly the intertwining of multiple distributed computing applications into nationalscale infrastructural systems. in many sectors, including health care, education and training, crisis management,environmental monitoring, government information delivery, and design and manufacturing, the benefits wouldhave profound significance to all citizens (as suggested in iis, 1992, and kahin, 1993). these sectors ofinformation technology application have been called national challenge (nc) applications.the pervasiveness and national role of the nc applications prevent them from developing dependency onnew technologies, even when those technologies offer important new capabilities. this is so unless the risks andcosts are manageable, and there is a clear trajectory for growth in capability and scale,the national information infrastructure: a highperformance computing and communicationsperspective315the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.and that growth is responsive to new technologies and emerging needs. for this reason, while computing andcommunications technologies have taken hold in numerous specific application areas within these sectors, inmost cases the challenge remains for advanced information technology to take on a significant sectorwideinfrastructural role.the nii, in the simplest terms, consists of available, usable, and interoperable computing andcommunications systems, built on underlying communications channels (the bitways) and providing a broadrange of advanced information technology capabilities (the services). these services provide the basis for a widerange of use (the applications), ranging in scale up to national challenge applications. a key point is that nii isfar more than communications connectivity; indeed it is generally independent of how communicationsconnectivity is supplied.generally speaking, infrastructural systems consist of ubiquitous shared resources that industry,government, and individuals can depend on to enable more productive and efficient activity, with broadlydistributed benefit (box 1). the resources can include physical assets, such as the national airtraffic controlsystem or the nationwide highway system. the resources can also include key national standards, such as theelectric power standards, trucking safety standards, railroad track structure, and water purity standards. theresources are ubiquitous and reliable to an extent that all participants can commit to longterm investmentdependent on these resources. this also implies a capacity for growth in scale and capability, to enableexploitation of new technologies and to assure continued value and dependability for users. the value can be inthe form of increased quality and efficiency, as well as new opportunities for services.it is therefore clear that a critical element of nii development is the fostering of appropriate commonalities,with the goal of achieving broad adoptability while promoting efficient competition and technological evolution.commonalities include standard or conventional interfaces, protocols, reference architectures, and commonbuilding blocks from which applications can be constructed to deliver information services to end users. afundamental issue is management and evolution, and in this regard other examples of national infrastructurereveal a wide range of approaches, ranging from full government ownership and control to privatesectormanagement, with government participation limited to assurance of standard setting.the clinton administration, under the leadership of vice president gore, has made national informationinfrastructure a priority (gore, 1991; clinton and gore, 1993), as have other nations (examples: ncbs, 1992,and motiwalla et al., 1993). the nii vision embraces computing and communications, obviously areas ofconsiderable private investment and rapid technological change. the definition of the government role in thiscontext has been ongoing, but several elements are clear. at the national policy level, the nii agenda embracesinformation and telecommunications policy, issues of privacy and rights of access, stimulation of newtechnologies and standards, and early involvement as a user (iitf, 1993). the federal high performancecomputing and communications (hpcc) program, and the advanced information technologies being developedwithin it, play a key role in addressing the research and technical challenges of the nii.in this paper we examine several aspects of the conceptual and technological challenge of creatinginformation infrastructure technologies and bringing them to fruition in the form of an nii built by industry andubiquitously adopted in the nc applications sectors. topics related to telecommunications policy, intellectualproperty policy, and other aspects of information policy are beyond our scope.in the first section below, we examine the federal government's role in fostering nii technologies andarchitecture. we then analyze the relationship between highperformance technologies and the nii and describeour threelayer nii vision of applications, services, and bitways. this vision is expanded in the next two sections,in which the nc applications and the technologies and architectural elements of the services layer are discussed.we present the research agenda of the federal high performance computing and communications program inthe area of information infrastructure technologies and applications, followed by our conclusions.the national information infrastructure: a highperformance computing and communicationsperspective316the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.role of the federal government in information infrastructurenew technologies are required to achieve the nii vision of ubiquitous and reliable highlevel informationservices. many of the envisioned nii services place huge demands on underlying computing andcommunications capabilities, and considerable energy is being applied in industry, government, and research tocreating these new capabilities. but there is more to nii than making computers faster, smarter, and more widelyconnected together.creation of national infrastructure entails delivery of services with sufficient reliability, ubiquity, andfreedom from risk that they can be adopted sectorwide in national applications. the challenge to achieve this isconsiderable in any infrastructural domain and particularly difficult in information infrastructure. these goalsusually involve rigorous standards and stability of technology, which appear all but precluded by the extremelyrapid evolution in every dimension of information technology.in the development of other kinds of national infrastructure, government has had a crucial catalytic role infostering the broad collaboration and consensusbuilding needed to achieve these goals, even when industry hasheld the primary investment role in creating the needed technologies and standards.in the case of national information infrastructure, it is manifestly clear that it should not and indeed cannotbe created and owned by the government. but the catalyzing role of government is nonetheless essential to bringthe nii to realization. the government has an enormous stake in the nii as a consequence of its stake in thenational challenge applications. information infrastructure technologies play a critical role in the federalgovernment's own plan to reengineer its work processes (gore, 1993). vice president gore draws an analogybetween the nii and the first use of telegraphy:basically, morse's telegraph was a federal demonstration project. congress funded the first telegraph link betweenwashington and baltimore. afterwards, thoughšafter the first amazing transmissionšmost nations treated thetelegraph and eventually telephone service as a government enterprise. that's actually what morse wanted, too. hesuggested that congress build a national system. congress said no. they argued that he should find privateinvestors. this morse and other companies did. and in the view of most historians, that was a source ofcompetitive advantage for the united states.government fostered the technology through initial demonstrations and encouragement of privateinvestment. but the u.s. telecommunications infrastructure has been built with private funds. and analogously,the nii implementation must be a cooperative effort among private and publicsector organizations.what are the specific roles for government? addressing this question requires understanding how the niidiffers from other major federal research and development efforts. the following characteristics summarize thedifferences: the scale of the nii is so huge that government investment, if it is to have an impact, must be designed tocatalyze and stimulate investment from other sources rather than subsidize creation of the nii itself. the niiwill emerge as an aggregation of many distinct entities that compete to provide products and services. ofcourse, rudimentary elements of the nii not only are in place but also constitute a major sector of theeconomy. the nii will provide longterm infrastructural support for applications of national importance, such as healthcare and education. decisionmakers in these application sectors cannot put new technologies in a pivotalrole, even when they offer important new capabilities, unless the risks and costs of adoption are manageable.adoption risk issues include, for example, scaleup, ability to evolve gracefully, competition and mobilityamong suppliers, and commitment to internal systems interfaces.the national information infrastructure: a highperformance computing and communicationsperspective317the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. the nii will support applications by delivering common services well above the level of simple access totelecommunications. these services can include, for example, mechanisms to protect intellectual propertyand user privacy, support for information management and search, and support for managing multimediaobjects. the high level of services will continue to evolve in capability and power, but there is nonethelessan essential requirement to achieve elements of commonality in the system interfaces through which theseservices can be delivered by nii providers to application users. (this issue is elaborated on below.) the nii depends on advanced computing and computer communications technologies whose evolution is, inlarge measure, the result of continued government research investment. government basic researchinvestment continues to be a primary source of the ideas and innovations that stimulate u.s. industry,sustain a high level of competitiveness in the market, and provide a national competitive advantage.these considerations yield a fourpronged strategy for government investment in research and developmentrelated to the nii: research and new technology creation; interoperability, commonality, and architectural design; application demonstration and validation; and aggressive early use.the first of these elements, research, is clear. government has a traditional role as farsighted investor inlongterm, highrisk research to create new concepts and technologies whose benefits may be broadlydistributed. in the case of the nii, the government needs to invest both in problemsolving research, to fulfill thepromise of today's vision, and also in exploratory research to create new visions for tomorrow. governmentinvestment in research and development can support the rapid and continual transition of new nii capabilitiesinto commercialization and adoption. basic research can yield paradigmatic improvements with marketwidebenefits. intensive discussions among leaders from academia, industry, and government have been under way todevelop a view of the technical research and development challenges of the nii (vernon et al., 1994).the second element involves stimulating commonalities within the nii that can achieve economies of scalewhile simultaneously creating a foundation for a competitive supply of services. interface and protocolcommonalities foster conditions where the risks of entry for both users and creators of technology are reduced.we use the term commonality because it is more inclusive than the conventional notion of standards. it coversroutine development of benchmarks, criteria, and measures to facilitate making choices among competingofferings. it also encompasses the definition of common systems architectures and interfaces to better defineareas for diversity and differentiation among competing offerings. common architectural elements help bothdevelopers and users decouple design decisions. of course, inappropriate standards can inhibit innovation orpredispose the market to particular technological approaches. a critical issue for the nii is the speed ofconvergence to new conventions and standards. in addition, conventions and standards must themselves enablerapid evolution and effective response to new technology opportunities. these are familiar issues in the realm ofconventionalization and standards generally; but they are also among the most fundamental considerations inachieving new highlevel nii services, and are in need of specific attention.demonstration, the third element, involves government sponsorship of testbeds to explore scalability andgive early validation to new technology concepts. testbeds can span the range from basic technologies coupledtogether using ad hoc mechanisms, to largescale integration projects that demonstrate utility of services forapplications in a pilot mode. these latter integration experiments can bootstrap fullscale deployments inapplications areas.the national information infrastructure: a highperformance computing and communicationsperspective318the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.box 1 information infrastructurešshared resourcesthe analogy between information infrastructure and the interstate highway system has helped bringthe concept of the nii into the popular consciousness. the analogy is apt, and not only because of the rolevice president gore's father played in drafting the legislation that led to the interstate highway system.the fundamental commercial utility of the highway system (and the railroads) is the substitution oftransportation for local production, enabling new economies of scale and resource sharing in manyindustries. economies of scale in manufacturing industries can be such that the cost of largescale remotemanufacture combined with transport can be significantly less than the cost of local production.the highways are infrastructural in that they make entire industries more efficient, resulting in bettervalue for the customer and expanded markets for producers. it is this value that justifies the publicinvestment in the infrastructure.more importantly, it explains why public investment and policy support are necessary stimuli forinfrastructure development: because shared infrastructure provides advantage to all who use it, there is noparticular competitive incentive for specific infrastructure users (producers or consumers) to invest directlyin its creation. on the other hand, expanded markets benefit all users, and so user investment ininfrastructure is justified if it is distributed equitably. for example, public highways may receive their initialfunding from bond issues combined with direct taxpayer support, with operating costs and loan servicecosts funded by users through tolls and licensing.the highway system infrastructure is sustained through a continually evolving set of interrelatedinfrastructure elements, such as roadbed engineering standards, speed limits, roadside amenities,interstate trucking regulations, tolls, and routing architecture. nationalscale users are able to rely on thisstructure and thus commit the success of their enterprises to its continued existence without expandingtheir risks.information infrastructure development involves an analogous set of elements. network protocols,application interfaces, interoperability standards, and the like define the class of mechanisms through whichvalue is delivered. value comes from highlevel services such as electronic mail, information services,remote access, and electronic commerce all supporting a rich variety of information objects. reliance andcommitment from nationalscale users depend on breadth and uniformity of access, common architecturalelements, interoperability, and a reasonably predictable and manageable evolutionthe importance of common architectural elements to infrastructural utility must not be understated.railgauge standardization is a canonical example. but commitment to common architectural elementsmust also include commitment to a process for evolving them. achieving the right balance is a principalchallenge to creating an adoptable national information infrastructure.finally, acting in the interest of government applications, the government can take a proactive role asconsumer of nii technologies to stimulate its suppliers to respond effectively in delivering informationinfrastructure that supports government applications. possible government applications include systems forgovernment information, crisis response, and environmental monitoring.the gigabit testbeds in the hpcc program offer a model for research partnerships among government,industry, and academe and represent a resource on which to build prototype implementations for nationalapplications. each testbed is costshared between government and the private sector and embraces the computerand telecommunications industries, university research groups, national laboratories, and application developers.the key function of the testbeds is to experiment with new networking technology and address interoperabilityand commonality concerns as early as possible.relationship between highperformance technologies and the niithe federal hpcc program supports the research, development, pilot demonstration, and early evaluationof highperformance technologies. hpcc's focus in its initial years was on the grandthe national information infrastructure: a highperformance computing and communicationsperspective319the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.challenges of science and engineering, with a strategy of developing a base of hardware and softwaretechnologies that can scale up to largescale processing systems, out to widearea distributed systems, and downto capable yet portable systems (fccset, 1994; cic, 1994). these scalable technologies will contributestrongly to the nii, as will the legacy of cooperation between government, industry, and academia. these cangreatly accelerate the establishment of an evolvable information infrastructure architecture, with testbeddevelopment, protocol and architecture design, interoperability experiments, and benchmarking and validationexperiments. this legacy has helped facilitate adoption of hpccfostered technologies by independent users bysignificantly reducing their costs and risks of adoption.this twofold hpcc stimulus, of research and cooperation, combines with a program emphasis ondemonstration, validation, and experimental application to create a framework for government technologyinvestment in nii. for this reason, hpcc was expanded in fy1994 to include a new major program component,information infrastructure technology and applications (iita), focusing directly on creation of a universallyaccessible nii, along with its application to prototype nc applications. (these activities are described in moredetail in the section below titled ''the federal hpcc program and the nii.")each of the other hpcc program activities contributes to iita. for example, emerging largescaleinformation servers designed to provide information infrastructure services are based on hpccdeveloped, highperformance systems architectures, including architectures based on use of advanced systems software to linkdistributed configurations of smaller systems into scalable server configurations. the microprocessors used inthese largescale systems are the same as those found in relatively inexpensive desktop machines. highperformance networking technologies, such as communications network switches, are increasingly influenced byprocessor interconnection technologies from hpcc. networking technologies are also being extended to a broadrange of wireless and broadcast modalities, enhancing mobility and the extent of personal access. included in thiseffort are protocols and conventions for handling multimedia and other kinds of structured information objects.nii can be viewed as built on a distributed computing system of vast scale and heterogeneity of anunprecedented degree. hpcc software for operating systems and distributed computing is enhancing theinteroperability of computers and networks as well as the range of information services. the software effort inthe hpcc program is leading to object management systems, methodologies for software development based onassembly of components, techniques for high assurance software, and improvements to programming languages.these efforts will contribute to the development and evolution of applications software built on the substrate ofnii services.threelayer national information infrastructure architecturewithin the hpcc community, a muchdiscussed conceptual architecture for the national informationinfrastructure has three major interconnected layers: national challenge applications, supported by diverse andinterdependent nii communication and computation services, built on heterogeneous and ubiquitous niibitways (see figure 1). each layer sustains a diverse set of technologies and involves a broad base of researchersand technology suppliers, yielding a continuously improving capability for users over time. by delivering utilityto clients in the layers above through common mechanisms or protocols, a rapid rate of evolution of capabilitycan be sustained in a competitive environment involving diverse suppliers. thus, developments in each of theselayers focus both on stimulating the creation of new technologies and on determining common mechanisms orprotocolsšthe commonalityšthrough which that capability can be delivered. for example: the keys to scaling up in national challenge applications are often in the choice of common applicationspecific protocols. for example, manufacturing applications require shared representations for product andprocess descriptions to support widespread interoperability among design systems and tools.the national information infrastructure: a highperformance computing and communicationsperspective320the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. services such as multimedia multicast can be provided to developers of application capabilities throughproper adherence to common protocols. with welldesigned protocols and interfaces, rapid growth inmultimedia capability and capacity can be delivered to end users and applications developers withoutrequiring major reengineering of the whole applicationslevel systems. services are also interdependent andthemselves evolve in this manner. the diverse bitways technologies deliver communications capability in a uniform manner through use ofstandard protocols, such as sonet/atm. this has the effect of insulating developers of nii services fromthe details of the rapidly evolving communications technologies used to deliver information capabilities toend users and applications.this architecture addresses directly the challenge of scaleup in capability, size, and complexity within eachof the three layers. ongoing validation of concepts can be achieved, in each layer, through largescale testbedexperimentation and demonstration conducted jointly with industry, users, and suppliers of new technologies andinformation capabilities. if the evolution of the nii architecture proceeds as envisioned, the result will be theintegration of new capabilities and increased affordability in the national challenge applications. each layersupports a wide range of uses beyond those identified for the specific national challenge applications. forexample, generalized nii service and bitway technologies can also support applications on a very small scale,extensions of existing services, ad hoc distributed computing, and so on.the national challenge applications are described in more detail in the next section, with the issuesaddressed by the services layer in the succeeding section titled "services." bitways technologies are well coveredin other sources, such as realizing the information future (cstb, 1994), and are not discussed here.figure 1 a model threelayer architecture for the nii. bitways provide the communications substrate, theapplications layer supports the implementation of the ncs, and the services layer provides the bridge betweencommunications and information. (a) despite the need for similar applicationenabling services, each nc sectormight reimplement these from scratch, yielding overly expensive stovepipe systems if there were no commonservices layer. (b) a common services layer coupled to toolkits for building applications.the national information infrastructure: a highperformance computing and communicationsperspective321the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.national challenge applicationsnumerous groups have developed lists of critical applications, characterized by the potential for a pervasiveimpact on american society and exploitation of extensive communications and information processingcapabilities. for example, in 1993 the computer systems policy project identified design and manufacturing,education and training, and health care as the national challenges (cspp, 1993). a more exhaustive list has beendeveloped by the information infrastructure task force, representing the union of much of what has beenproposed (iitf, 1994). among these nc applications are the following: crisis management: crisis management systems exploit information technology to ensure national andeconomic security through various kinds of crises. this is accomplished by providing timely data collectionand intelligence fusion, advanced planning tools, rapid communications with defense forces spread aroundthe globe, and a command and control ability to respond quickly to crises. the same basic capabilities canbe deployed on a smaller scale to respond to local emergencies, such as devastating hurricanes, earthquakes,or fires. design and manufacture: these systems integrate engineering design with product manufacturing, to reducethe time to create new products, to lower production costs, and to increase product quality. in a wider sense,a pervasive design and manufacturing system should couple suppliers to their customers throughout theproduction chain. goals are more responsive product design, manufacture, and justintime warehousing andproduct delivery. education and training: these systems provide access to online instructional and research materials,anywhere and anytime, as well as more direct communication among students and educators. once createdand made accessible, instructional materials may be reused and evolved by instructors around the country.for example, educational use of the information infrastructure can enable distance learning, where studentsin remote locations can gain access to specialized instruction. training could exploit simulation coupledwith remote access to actual apparatus. environmental monitoring: these systems integrate data from ground, airborne, and spacebased sensors tomonitor (and potentially respond to) environmental changes. they may be used to discover a nuclearaccident in progress, oncoming climatic effects such as smog conditions, or can be exploited for longertermstudies such as climate change. government information delivery: citizens have a right to ready, lowcost access to governmentinformation that they have already paid for, including economic statistics, trade information, environmentaland land use information, and uniform onestop shopping for government services such as veterans' andsocial security benefits. health care: these systems use information technologies to improve the delivery of health care, byproviding ready access to patient records, remote access to medical expertise, support for collaborativeconsultations among health care providers, and rapid, paperless claims adjustment that can help reducehealth care costs.there are two additional applications that sit at the interface of the national challenges and the underlyingservice layer: digital libraries and electronic commerce. in a sense, these are fundamental enablers forinformation access and electronic exchange of value and will be extensively used by virtually all of the other ncapplications described above. digital libraries: a digital library is a knowledge center without walls, accessible from anywhere throughnetworked communications. these systems are leading to significant advances in the generation, storage,and use of digital information of diverse kinds. underlying services and technologies range from advancedmass storage, online capture of multimedia data, intelligent information location andthe national information infrastructure: a highperformance computing and communicationsperspective322the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.filtering, knowledge navigation, effective human interfaces, system integration, and prototype andtechnology demonstration. electronic commerce: electronic commerce integrates communications, data management, and securityservices to allow business applications within different organizations to automatically interchangeinformation. communications services transfer the information from the originator to the recipient. datamanagement services define the interchange format of the information. security services authenticate thesource of information, verify the integrity of the information received by the recipient, prevent disclosure ofthe information to unauthorized users, and verify that the information was received by the intendedrecipient. electronic commerce applies and integrates these infrastructure services to support business andcommercial applications, including financial transactions such as electronic bidding, ordering and payments,and exchange of digital product specifications and design data.in each of these applications there is an unmet challenge of scale: how can the service be madeubiquitously available with steadily increasing levels of capability and performance? the applicationscommunities depend on information technology for solutions but are facing scaling barriers, and hence the niigoal of crossing the threshold of ubiquity. in the absence of common architectural elements, such as interfaces,methods, and modules, it may be possible to demonstrate prototype solutions to specific applications problemsthrough monolithic stovepipes. but these solutions may not give any means to pass this threshold ofpervasiveness and dependability.servicesoverviewas we have noted, information infrastructure is more than bandwidth, switching, and ubiquitouscommunications access. it is (1) the common service environment in which nc applications are built. allapplications share generic service needs: human interfaces (e.g., graphical user interaction, speech recognition,data visualization), application building blocks (e.g., planning subsystem, imaging subsystem), data and processmanagement (e.g., search and retrieval, hyperlink management, action sequencing), and communications (e.g.,ipc, mobile computation). also, the engineering of applications requires (2) tools in the form of developmentenvironments, toolkits, operational protocols, and data exchange and action invocation standards from whichservice solutions can be combined, integrated, and reused. finally, the engineering of applications becomes moreefficient (as is already occuring for shrinkwrap software running on personal computers) in the presence of (3) amarketplace of reusable subsystems; in this manner, applications systems can be assembled from competitivelyacquired subsystems rather than built directly from the raw material of lines of code.we elaborate slightly some of the elements of the common service environment: tools, libraries, and databases: there already exist major, complex software systems that provideimplementations for portions of the national challenge applications. for example, large collections ofcomputeraided design (cad) software are already used extensively in engineering design domains.similarly, relational and objectoriented database management systems provide extensive capabilities forstructured data storage, indexing, and management. diverse sets of software tools and subsystems can beintegrated into coherent applications development environments to form the development base with which toassemble the national challenge applications. similarly, diverse libraries of program components anddatabases of data elements can be composed and integrated into the development environment. composition and integration frameworks: toolkits already exist in certain specific domains to assist in thecomposition and integration of tools, libraries, and databases. for example, the cadthe national information infrastructure: a highperformance computing and communicationsperspective323the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.framework initiative (cfi) accomplishes this by providing interface specifications for tooltotoolcommunications and tooltodatabase communications. in addition, the cfi has developed prototypeimplementations of these capabilities. these can form the basis of valueaided and commercially supportedpackages and software toolsets. commercial vendors of applications software for desktop computers aredeveloping a variety of frameworks (such as corba, ole, opendoc, and others) for integration ofsoftware applications. users expect that commercial pressures will eventually result in some degree ofintegration of these various frameworks. this issue of multiple standards is discussed further below. building block object sets: the commonality that characterizes many of the service needs of the nationalchallenge applications naturally yields an evolving shared market of software objects (that is, actions,operations, and protocols as well as data structures) to emerge that can be reused across multiple applicationdevelopment efforts. for example, a schedule object, which provides operations for allocating limitedresources to critical tasks, could be used as a component of several different applications. application customized objects: leveraging the evolving building block object sets, we expect the objectsfrom which the applications are implemented to be customized and extended for the application at hand. forexample, though there is much in common in terms of command, control, communications, and intelligence(c3i) for an intensive care unit and an environmental spill, we would expect the details of sensor integration,strategies for alerting, and demands for realtime response to be somewhat different. the elements of theunderlying object base will need customization for their use in specific national challenge applications. thedegree of commonality across applications, which we hope is large, remains to be discovered.considerations in constructing the national information infrastructurecommon architectural elements. the national challenge applications obtain service capabilities deliveredthrough common protocols or interfaces (known commercially as apis, or applications portability interfaces).though service capabilities may evolve rapidly, to the benefit of users, they are delivered through particularinterfaces or protocols that evolve more slowly. this insulates the client architecturally from the rapid pace ofchange in implementations, on the one hand, but it enables the client to exploit new capabilities as soon as theyappear, as long as they are delivered through the accepted interface. a competitive supply of services hastens theprocesses of convergence to common protocols and evolution therefrom.industry standards, stovepipes, and risk. we have asserted that commonality among the protocols,interfaces, and data representations used in the services layer of the nii will be critical for its success. to theextent that emerging or evolving industrystandard commonalities are replaced by ad hoc or proprietarystovepipe approaches for the national challenge areas, applications developers place themselves at risk withrespect to delivery of capability and future evolution path. in particular, in return for complete ownership orcontrol of a solution, they may give up the opportunity to ride the curves of growth in rapidly growingunderlying technologies, such as multimedia, digital libraries, and data communication. the challenge of thenational challenge applications is how the applications constituencies can have both control of applicationssolutions and participation in the rapid evolution of underlying technologies. government, supported byresearch, can invest in accelerating the emergence of new common architectural elements, and in creatingtechnologies that reduce the risk and commitment associated with adoption of rapidly evolving standards.evolution of commonalities. accepted protocols naturally manifest a certain stickiness independent of theirmerit, because they become a stable element in determining systems structure andthe national information infrastructure: a highperformance computing and communicationsperspective324the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.develop associated transition costs and risks. the history of tcp/ip and osi is a good example of this wellknown phenomenon, as is the recent introduction of de facto standards relating to the world wide web (urlsand html). in particular, research and government can take a leading role in establishing new commonalitiesthat foreshadow industry standards.rapid evolution and multiple standards. there are numerous standards presently in use for imagerepresentation. most, but not all, are open standards; several are proprietary or otherwise encumbered.regardless of the degree of acceptance of any one these standards, the pace of change is such that it would befoolish for a major software application developer to lock itself into accepting or producing images according tojust one of these standards. indeed, most major software applications buildingblocks accept multiple suchstandards, thus increasing the robustness of the client applications with respect to either the technicalcharacteristics or market acceptance of any one of the particular standards for bitmaps. in addition, tools arereadily available for converting among the various representations for images. thus, from the standpoint ofapplications architecture, a robust design can be created that does not depend on the fate of any one of the manystandards, but rather on the evolution of the entire suite. the multiple commonalities emerge as customers andproducers seek frameworks for competition in service niches. however, experience suggests that over timemultiple related standards may begin to coalesce, as the commercial focus (and margins) move to higher levelsof capability and the differential commercial advantage of any specific standard diminishes or even evolves intoa liability. anticipation of the process can yield robust scalable designs for major applications even when there isvolatility in the markets for the subsystems they depend on.competition and layering. with the right approach to standards and infrastructural subsystems, diverseunderlying technologies can evolve into common, shareable, and reusable services that can be leveraged acrossmultiple nc applications. alternative implementations of a frequently used service, such as display windowmanagement, eventually will lead to the identification of best practices that can be embodied in a commonservices layeršfor example, for human interfaces. and robust designs of the applications layers above willenable this rapid evolution to be accepted and indeed exploited. (consider, for example, the rapid rate of releaseof new versions of world wide web browsers, and the huge multiplicity of platforms they run on, and the rapidrate of evolution of the many multimedia (and other) standards they rely on. the web itself, however, evolves ata slower rate and is not invalidated by these changes in particular niche services. the standards on which theweb is based evolve even more slowly.) the conclusion we draw is that simultaneous evolution at multiplelayers is not only possible but also needs to be an explicit architectural goal if ubiquity is to be attained at theapplications level.concerning layers. services depend on other services for their realization. for example, a protocol formicrotransactions will likely rely on other protocols for encryption and authentication. this enables amicrotransaction system not only to be designed independently of the particular encryption and authenticationservices, but also to sustain later upgrade of (or recompetition for) those services in a robust manner. in spite ofthis dependency, services are not organized rigidly into layers as is, for example, the sevenlayer osi model. theterm "layering" is instead meant to suggest that services naturally depend on other services. but the exactinterdependency can change and evolve over time. the commonalities through which services are delivered thusform a set of multiple bottlenecks in a complex and undulating hourglass (using the analogy of cstb, 1994).service classification. a consequence of the above argumentation is that the success of the overall nii doesnot depend on achievement of a particular master infrastructural architecture. but it must be emphasized that itdoes strongly depend on emergence of a broad variety of infrastructural service architectures designed with scaleup, and indeed ubiquity, in mind. ubiquity (as suggested in the commentsthe national information infrastructure: a highperformance computing and communicationsperspective325the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.above on multiple standards) is in the appearance of representatives of a set of related commonalities, and not inany particular protocol or component. this also suggests there is no ultimately correct layering lurking in thesoup of services, but rather multiple candidates and arrangements. without commonalities there is no nationalinformation infrastructure, but the particular need for specific allencompassing commonalities is mitigated tothe extent that technologies and tools for interoperability are available. that is, suites of related evolvingcommonalities can be supported to the extent that conversion and interoperability tools are available. the issuedevolves into finding the right balance in this equation.the government thus can employ a mixed strategy in fostering national challenge applications throughinfrastructural commonalities. it can stimulate development of new services, creation and evolution of newarchitectural commonalities, and development of readily available technologies of interoperability. directresearch and development is the most effective way to stimulate new service capabilities and associatedcommonalities. the government can also exploit its own market presence (though the leverage is less), taking anactivist role in industry forums for conventionalization (informal emergent commonalities) and standards(formalized commonalities).an illustrative layered model. one possible service taxonomy, elaborated below, classifies generic servicesinto categories: human interfaces, applications building blocks, data and process management, andcommunications. human interface services include window managers (e.g., motif, nextstep), tools for speechhandling and integration (generation as well as recognition), handwriting recognition, data visualizationpackages, toolkits for audio and video integration, and so on. applications building blocks include planningpackages, scheduling packages, data fusion, collaboration support, virtual reality support, and image processingand analysis. data and process management services consist of capabilities for configuration management,shared data spaces, process flows, data integration, data exchange and translation, and data search and retrieval.communications services include ubiquitous access through various communications mechanisms (e.g., wirelessas well as wired connections into the bitways), mobility services to support users as they move through thepoints of connection into the network, interprocess communications and remote process call mechanisms tosupport distributed processing, and trust mechanisms such as authentication, authorization, encryption,password, and usage metering.the service layers themselves evolve as new underlying technologies appear that provide new functionalityor better ways of doing things. a construction kit can support the assembly and evolution of applications basedon the service suite. elements of this kit, also elaborated below, could include software environments fordeveloping applications, evolution of standard operational and data exchange protocols, software toolkits andsoftware generators for building or generating welldefined portions of applications, and frameworks forintegrating tools and data into coherent, interoperable ensembles.the value of a common services layer is conceptually indicated by figure 2. in figure 2(a), the lack of acommon services infrastructure leads to stovepipe implementations, with little commonality among the servicecapabilities of the various national challenges. in figure 2(b), a common set of services is leveraged among thenational challenges, aided by a collection of toolkits, integration frameworks, and applications generators.information enterprise elementscommonalities usually (but not always) emerge in the presence of a diversity of evolving implementations.a commonality in the form of a protocol is an abstraction away from the details of implementation that allowsutility or value to be delivered in an implementationindependent manner to the service client. this suggests athreefold analysis for service capabilities: utility of some kind, delivered through a particular commonality suchas a protocol, abstracting away the details of the diversity of implementations. of course, the commonalitiesthemselves evolve; they just evolve more slowly.the national information infrastructure: a highperformance computing and communicationsperspective326the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2 technical challenges in building a national information infrastructure.figure 3 shows examples of elements for each of the three layers of the national information infrastructurearchitecture. in the figure, the three columns indicate the following: utility: each service provides specific value to users or clients. for example, the bitways are intended toprovide ubiquitous data communications, and in a manner such that designers of applications need not knowwhether the communications links are through fiber, wireless, or some combination of links. the clientneeds only an abstract rendering of the characteristics of the aggregate link. commonality: a common protocol or api creates a framework for delivery of utility. clients engineer tothis framework (and its expected evolution), thereby insulating themselves from the underlyingimplementation details. diversification of technology occurs behind the protocol, enabling the technologiesto be made accessible to clients with acceptable risk and costeffectiveness, and also lowering entry barriersboth for new users of the technologies and for new sources of capabilities. this is the essence of theprinciple of open architecture. for example, transport protocols for bitways provide users ofcommunications services with a means to access the service independently of the particular choices ofunderlying component technologies. diversity: these are the areas of implementation technology where innovation, rapid technological growth,and diversity of supply are essential to costeffective delivery of increasing levels ofthe national information infrastructure: a highperformance computing and communicationsperspective327the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.capability. for example, a competitive supply of fiber optic connectivity is needed to provide ubiquitousaccess to high performance bitways. also, continued improvements in optical and wireless communicationimprove affordability of highperformance mobile communication.figure 3 shows examples of these concepts for each of the layers of the nii conceptual architecture. thisorganization focuses attention on two critical issues, alluded to in the foregoing, that must be addressed in thedesign of service commonalities: scalability: testbeds and other mechanisms provide means to assess the degree of scalability of new serviceconcepts and protocols. they also address the extent of dependencies among services. scalability, forinfrastructure, necessarily includes potential for pervasive acceptance. protocols that are proprietary orencumbered in other ways have a lesser chance of being accepted, because of the degree of technologicaland programmatic risk associated with them. but, as always, there is commercial advantage to being the firstto introduce a successful open protocol, so the incentive persists for commercial introduction ofcommonalities, even when they are fully open. legacy: there are two aspects of the legacy issue, a constraint and a goal. the first is the legacy we inherit,which constrains our architectural design decisions in fundamental ways. the second is the legacy webequeath in the form of commonalities from which later architectures must evolve.opportunities for competition are naturally sought by service clients, and a diversity of implementationsindicates success in this regard. at the level of bitways, for example, the space of change is rapid, and there arewideranging approaches for achieving a given capability (e.g., physical media may consist of optical fiber, landmobile wireless radios, or laser communications). the challenge for the application developer is how to exploitthe continuing innovation while remaining insulated from continuous change; the client wants to ride the curvesof growth while avoiding continual reengineering.one conclusion to draw from this analysis is that research must focus not only on creation anddemonstration of new kinds of service capability, but also on the scientific and technological aspects ofarchitectural design: designing and evaluating candidates for protocol and api definitions, looking at both thesupplier and client perspectives.the federal hpcc program and the niioverviewin fy1994, the federal hpcc program was extended with a new responsibility, to develop informationinfrastructure technology and applications (iita) to demonstrate prototype solutions to selected nationalchallenge applications using the full potential of the rapidly evolving high performance communications andinformation processing capabilities. the details of the programs evolving goals and research plans are in itsannual reports to congress (fccset, 1994; cic, 1994).with the incorporation of iita within its research agenda, the hpcc program is advancing key niienabling technologies, such as intelligent system interfaces, real environments augmented with syntheticenvironments, image understanding, language and speech understanding, intelligent agents aiding humans in theloop, and nextgeneration data and object bases for electronic libraries and commerce. this is being coupled witha vigorous program of testbed experimentation that will ensure continued u.s. leadership in informationprocessing technologies.iita efforts are designed to strengthen the hpcc technology base, broaden the markets for thesetechnologies, and accelerate industry development of the nii. federal hpcc agencies are working closely withindustry and academia in pursuit of these objectives. these objectives are to be accomplished, in part,the national information infrastructure: a highperformance computing and communicationsperspective328the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 3 examples to illustrate the concepts of diversity, commonality, and utility.by accelerating the development of readily accessible, widely used, largescale applications with significanteconomic and social benefit. the hpcc program's original focus of enhancing computing and communicationscapabilities is thus extended to address a broader set of technologies and applications that have an immediate anddirect impact on critical information capabilities affecting every citizen.as we have described in the previous section, the development of such applications is predicated on (1)creating the underlying scalable computing technologies for advanced communication services over diversebitways, effective partitioning of applications across elements of the infrastructure, and other applicationssupport services that can adapt to the capabilities of the available infrastructure; and (2) creating and inserting arichly structured and intelligent service layer that will significantly broaden the base of computer informationproviders, developers, and consumers while reducing the existing barriers to accessing, developing, and usingadvanced computer services and applications. in parallel with these activities, a more effective softwaredevelopment paradigm and technology base must also be developed, since fullscale implementations in supportof the national challenges will be among the largest and most complex applications ever implemented. this willbe founded on the principles of composition and assembly rather than construction, solid architectures ratherthan ad hoc styles, and more direct user involvement in all stages of the software life cycle. the entiretechnology base developed in this program, including services and software, will be leveraged across thenational challenges, leading to significant economies of scale in the development costs.the intended technical developments of iita include the following: information infrastructure services: these are the collection of services provided to applications developersand end users that implement a layered architecture of increasing levels of intelligence and sophistication ontop of the communications bitways. services provide a universally available, networkaware, adaptiveinterface on which to construct the national challenge applications, spanning communicationsbased servicesat the low end, to intelligent information processing services at the high end. these services include networksupport for ubiquitous access, resource discovery in athe national information infrastructure: a highperformance computing and communicationsperspective329the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.complex distributed network environment, and intelligent support services that can negotiate and adapt tothe service quality needs of the application. information infrastructure services also include system softwareand services that implement pervasive privacy, security and trust mechanisms for the informationinfrastructure, persistent object bases with which to build largescale data repositories, reliable computingtechnologies to support the missioncritical nature of the infrastructure, and defensive software organized toprotect the infrastructure from intrusion and attack. systems development and support environments: this area consists of the enabling technologies to developand support large, complex information systems that exploit a nationalscale information infrastructure.fundamental to this activity is the use of that infrastructure in the software development and supportprocess. virtual organizations consisting of end users, contractors, and management will synergisticallywork together to develop software systems that are easy to use, that can be adapted through use to fit humanneeds and changing requirements, and that enhance enduser productivity, all despite the complexity of theunderlying infrastructure. to achieve these goals, the focus is on software architectures, componentprototyping, software composition, libraries of reusable and reliable software modules, enduser tailoring,intelligent documentation and online help, machine learning, and scalable compiler and interpretertechnology. intelligent interfaces: many of the national challenge applications require complex interfacing with humansor intelligent control systems and sensors. in addition, these applications must be able to understand theirenvironment and to react to them. technology in this area consists of highlevel, networkcapableapplications building blocks for realtime planning and control, image processing and understanding, humanlanguage technology, extensive use of intelligent computerbased agents, and support technologies for moreeffective humancomputer interaction. national challenges: the concept of national challenge applications has already been described above. it isimportant to distinguish between the implementation of operational systems and the use of challengingapplications testbeds to demonstrate the value of highperformance technologies as well as to drive theircontinued evolution. the government's research and development role is to focus on the latter; the privatesector has primary responsibility for the former.each of the three technology areas (the first three bullets above) is discussed in additional detail in thefollowing subsections, which include a sampling of technical subtopics. the national challenges have alreadybeen summarized in a prior section.information infrastructure servicesservices provide the underlying building blocks upon which the national challenge applications can beconstructed. they are intended to form the basis of a ubiquitous information web usable by all. a rich array ofinterdependent services bridge the gap between the communications bitways and the applicationspecificsoftware components that implement the national challenges. universal network services: these are extensions to the existing internet technology base to provide morewidespread use by a much larger number of users. these include techniques for improved ease of use, plugandplay network interoperation, remote maintenance, exploitation of new last mile technologies,management of hybrid/asymmetric network bandwidth, guaranteed quality of service for continuous mediastreams, and scaleup of network capabilities to dramatically larger numbers of users. integration and translation services: these services support the migration of existing data files, databases,libraries, and programs to new, better integrated models of computing, such as objectoriented systems.they also provide mechanisms to support continued access to older legacy forms of data as the modelsevolve. included are services for data format translation and interchange as well asthe national information infrastructure: a highperformance computing and communicationsperspective330the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.tools to translate the access portions of existing programs. techniques include wrappers that surroundexisting elements with new interfaces; integration frameworks that define applicationspecific commoninterfaces and data formats; and mediators that extend generic translation capabilities with domainknowledgebased computations, permitting abstraction and fusion of data. system software services: these include operating system services to support complex, distributed, andtime and bandwidthsensitive applications. the services support the distribution of processing acrossprocessing nodes within the network, the partitioning of the application logic among heterogeneous nodesbased on their specialized capabilities or considerations of asymmetric or limitedinterconnectionbandwidth; guaranteed realtime response to applications for continuous media streams; and storage,retrieval, and i/o capabilities suitable for delivering large volumes of data to great numbers of users.techniques include persistent storage, programming language support, and file systems. data and knowledge management services: these services include extensions to existing databasemanagement technology for combining knowledge and expertise with data. these include methods fortracking the ways in which information has been transformed. techniques include distributed databases;mechanisms for search, discovery, dissemination, and interchange; aggregating base data and programmedmethods into objects; and support for persistent object stores incorporating data, rules, multimedia, andcomputation. information security services: these services provide support for the protection of the security ofinformation, enhanced privacy and confidentiality for users of the infrastructure, protection of intellectualproperty rights, and authentication of information sources within the infrastructure. techniques includeprivacyenhanced mail, methods of encryption and keyescrow, and digital signatures. also included aretechniques for protecting the infrastructure (including authorization mechanisms and firewalls) againstintrusion attacks, such as worms, viruses, and trojan horses. reliable computing and communications services: these include system software services for nonstop,highly reliable computer and communications systems that can operate without interruption. the techniquesinclude mechanisms for fast system restart such as process shadowing, reliable distributed transactioncommit protocols, and event and data redo logging to keep data consistent and uptodate in the face ofsystem failures.system development and support environmentsthese provide the networkbased software development tools and environments needed to build theadvanced user interfaces and the informationintensive nc applications. rapid system prototyping: these consist of the tools and methods that enable the incremental integrationand cost effective evolution of software systems. technologies include tools and languages that facilitateenduser specification, architecture design and analysis, component reuse and prototyping; testing andonline configuration management tools; and tools to support the integration and interoperation ofheterogeneous software systems. distributed simulation and synthetic environments: these software development environments provide thespecialized underlying support mechanisms for the creation of synthetic worlds, which can integrate real aswell as virtual objects, in terms of both their visual as well as computational descriptions. methods includedistributed simulation algorithms; geometric models and data structures; tools for scene description,creation, and animation; and integration of geometric and computational models of behavior into anintegrated system description. problem solving and system design environments: these environments provide the techniques that supportthe software and system design process through the use of automated tools, with particular emphasis onmaintaining flexibility and tailorability in tool configurations to enable organizations to tailorthe national information infrastructure: a highperformance computing and communicationsperspective331the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.their support environments to their needs. examples include efficient algorithms for searching hugeplanning spaces, more powerful and expressive representations of plans, operators, goals, and constraints,and the incorporation of efficient methods to facilitate scheduling and resource allocation. the effects ofuncertainty must be taken into account as well as the effects of goal interactions. software libraries and composition support: these software tools and methods support the development ofcommon architectures and interfaces to increase the potential for reusability across multiple underlyingmodels of computation, the diversity of programming languages in use, and the varying degree of assuranceprovided by software components. important elements of this area include the development of theunderlying methodology, data structures, data distribution concepts, operating system interfaces,synchronization features, language extensions, and other technology to enable the construction of scalablelibrary frameworks. collaboration and group software: these tools provide support for group cooperative work environmentsthat span time as well as space. methods include shared writing surfaces and live boards, version andconfiguration management, support for process and task management, capture of design history andrationale, electronic multimedia design notebooks, networkbased video conferencing support, documentexchange, and agents serving as intermediaries to repositories of relevant multimedia information. thetechnology should be developed to make it possible to join conferences in progress and to be automaticallybrought up to date by assistants (agents) with memory.intelligent interfacesadvanced user interfaces will bridge the gap between human users and the emerging national informationinfrastructure. a wide range of new technologies that adapt to human senses and abilities must be developed toprovide more effective humanmachine communications. the iita program must achieve a high level userinterface to satisfy the many different needs and preferences of vast numbers of citizens who interact with the nii. humancomputer interface: this supports research in a broad range of technologies and their integration toallow humans and computers to interact effectively, efficiently, and naturally. developments in this areainclude technologies for speech recognition and generation; graphical user interfaces that allow rapidbrowsing of large quantities of data; usersensitive interfaces that customize and present information forparticular levels of understanding; language corpora for experimental research; and humanmachineinteraction via touch, facial expression, gesture, and so on. the new iita emphasis is on integration, realtime performance, and demonstration of these new communication modalities in multimedia, multisensoryenvironments. heterogeneous database interfaces: this supports development of methodologies to integrateheterogeneously structured databases composed of multiformatted data. to support nii informationdissemination, a capability is needed for a user to issue a query which is broadcast to the appropriatedatabases and a timely response is returned and translated into the context of the users query. multiformatteddata may range from ascii text to numerical time series, to multidimensional measurements, to time seriesof digital imagery, etc. also of critical importance is the integration of metadata with the data and itsaccessibility across heterogeneous databases. image processing and computer vision: this activity supports research in making images, graphics, andother visual information a more useful modality of humancomputer communication. research areas includeall aspects of theory, models, algorithms, architectures, and experimental systems from lowlevel imageprocessing to highlevel computer vision. methodologies of pattern recognition will be further developed toallow automated extraction of information from large databases, in particular,the national information infrastructure: a highperformance computing and communicationsperspective332the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.digital image data. the new iita emphasis is on integration, scalability, and demonstration of easy accessand usability of visual information in realworld problems. usercentered design tools/systems: this consists of work in models and methodologies leading tointeractive tools and software systems for design and other usercentered activities. userfriendly tools thatcombine datadriven and knowledgebased capabilities is one of the areas for new research. the new iitaemphasis is on supporting the development of ubiquitous, easytouse, and highly effective interactive tools. virtual reality and telepresence: this consists of research that will provide tools and methods for creatingsynthetic (virtual) environments to allow realtime, interactive human participation in the computing/communication loop. such interaction may be through sensors, effectors, and other computational resources.the iita focus is creating shared virtual environments which can be accessed and manipulated by manyusers at a distance in support of national challenge application areas.summary and conclusionsmuch of the discussion of the national information infrastructure has been at the applications level or thelevel of the bitways. various groups, including congress and the clinton administration, have identifiedcandidate nc applications on the one hand, while others have dealt with the issues of making interoperable thevarious existing and emerging communications infrastructures. this discussion suggests a shift in focus to theservices layer. the right collection of capabilities at this level of the infrastructure will have an extraordinaryimpact on a wide range of applications.we have cataloged many of the key technology areas needed for the service layer of the nii: informationinfrastructure services, systems development and support environments, and intelligent interfaces. the furtherdevelopment of these technologies and their integration into coherent and robust service architectures,incorporating the principles of utility, diversity, and commonality as described here, will be a major challengefor the information technology research community in coming years.costshared sponsorship of pilot demonstrations and testbeds is a key role for government in acceleratingthe development of the nii. in each nc application area, opportunities exist to demonstrate early solutions,including the potential for scaling up. we suggest that in the exploration of commonality and conversion issues,testbeds can also help address the fundamental issue of ubiquity. the scale of the enterprise, and the fundamentalopportunities being addressed, necessitate cooperation among industry, government, and academia for success.we have suggested appropriate roles and approaches to cooperation, with emphasis on the roles of governmentand research. this is predicated on the assumption that government, in addition to sponsoring key basic research,has a crucial catalytic role in working with all sectors to address the challenge of the national applications toscaling up to the point of ubiquity and reliance.acknowledgmentsthe ideas expressed in this paper have been influenced by discussions with colleagues at darpa,especially duane adams, steve cross, howard frank, paul mockapetris, michael st. johns, john toole, doyleweishar, and gio wiederhold. our ideas have also benefited from extensive discussions with participants in thehpcc program from a diverse collection of federal agencies: howard bloom (nist), roger callahan (nsa),y.t. chien (nsf), mel ciment (nsf), sherri de coronado (nih), ernest daddio (noaa), norm glick (nsa),steve griffin (nsf), dan hitchcock (doe), paul hunter (nasa), jerry linn (nist), dan masys (nih), cherienichols (nih), walter shackelford (epa), and selden stewart (nist).the national information infrastructure: a highperformance computing and communicationsperspective333the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.referencesclinton, william j., and albert gore, jr. 1993. technology for america's economic growth: a new direction to build economic strength, february 22 .committee on information and communication (cic). 1994. high performance computing and communications: technology for thenational information infrastructure, supplement to the president's fiscal year 1995 budget . national science and technologycouncil, washington, d.c.computer science and telecommunications board (cstb), national research council. 1994. realizing the information future: theinternet and beyond. national academy press, washington, d.c.computer systems policy project (cspp). 1993. perspectives on the national information infrastructure: cspp's vision andrecommendations for action. computer systems policy project, washington, d.c., january 12.federal coordinating council for science, engineering, and technology (fccset), office of science and technology policy. 1993.fccset initiatives in the fy 1994 budget. office of science and technology policy, washington, d.c., april 8.federal coordinating council for science, engineering, and technology (fccset), office of science and technology policy. 1994. highperformance computing and communications: toward a national information infrastructure . committee on physical,mathematical, and engineering sciences, office of science and technology policy, washington, d.c.gore, jr., al. 1991. ''infrastructure for the global village," scientific american 265(3):150œ153.gore, jr., albert. 1993. from red tape to results, creating a government that works better & costs less: reengineering throughinformation technology, accompanying report of the national performance review. u.s. government printing office,washington, d.c., september.information infrastructure task force (iitf). 1993. the national information infrastructure: agenda for action. information infrastructuretask force, u.s. department of commerce, washington, d.c., september 15.information infrastructure task force (iitf), committee on applications and technology. 1994. putting the information infrastructure towork. nist special document no. 857. information infrastructure task force, u.s. department of commerce, may.information technology association of america (iita). 1993. national information infrastructure: industry and government roles.information technology association of america, washington, d.c., july.institute for information studies (iis). 1992. a national information network: changing our lives in the 21st century. annual review ofthe institute for information studies (northern telecom inc. and the aspen institute), queenstown, md.kahin, brian. 1993. "information technology and information infrastructure," in empowering technology: implementing a u.s. strategy,lewis m. branscomb (ed.). mit press, cambridge, mass.motiwalla, j., m. yap, and l.h. ngoh. 1993. "building the intelligent island," ieee communications magazine 31(10):28œ34.national computer board of singapore (ncbs). 1992. "a vision of an intelligent island: the it2000 report," march.vernon, mary k., edward d. lazowska, and stewart d. personick (eds.). 1994. r&d for the nii: technical challenges. report of aworkshop held february 28 and march 1, 1994, in gaithersburg, md. educom, washington, d.c.the national information infrastructure: a highperformance computing and communicationsperspective334the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.40nomadic computing and communicationsleonard kleinrockuniversity of california at los angelesabstractnomadicity refers to the system support needed to provide a rich set of computing and communicationscapabilities and services, in a transparent and convenient form, to the nomad moving from place to place. thisnew paradigm is already manifesting itself as users travel to many different locations with laptops, personaldigital assistants, cellular telephones, pagers, and so on. in this paper we discuss some of the open issues thatmust be addressed in the system support necessary for nomadicity. in addition, we present some additionalconsiderations in the area of wireless communications, which forms one (and only one) component of nomadicity.introductioncurrently, most users think of computers as associated with their desktop appliances or with a serverlocated in a dungeon in some mysterious basement. however, many of those same users may be considered to benomads, in that they own computers and communication devices that they carry about with them in their travelsas they move between office, home, airplane, hotel, automobile, branch office, and so on. moreover, evenwithout portable computers or communications, there are many who travel to numerous locations in theirbusiness and personal lives and who require access to computers and communications when they arrive at theirdestinations. indeed, even a move from a desk to a conference table in the same office constitutes a nomadicmove since the computing platforms and communications capability may be considerably different at the twolocations. the variety of portable computers is impressive, ranging from laptop computers, notebook computers,and personal digital assistants (or personal information managers) to "smart" credit card devices and wristwatchcomputers. in addition, the communication capability of these portable computers is advancing at a dramatic pacešfrom highspeed modems to pcmcia modems, email receivers on a card, spreadspectrum handheld radios,cdpd transceivers, portable gps receivers, and gigabit satellite access, and so on.the combination of portable computing with portable communications is changing the way we think aboutinformation processing (weiser, 1991). we now recognize that access to computing and communications isnecessary not only from "home base" but also while in transit and after reaching a destination.1these ideas form the essence of a major shift to nomadicity (nomadic computing and communications),which we address in this paper. the focus is on the system support needed to provide a rich set of capabilitiesand services, in a transparent and convenient form, to the nomad moving from place to place.note: this work was supported by the advanced research projects agency, arpa/csto, under contract jfbi93112, "computer aided design of high performance network wireless networked systems," and by the advancedresearch projects agency, arpa/csto, under contract dabt63c0080, "transparent virtual mobile environment."this paper contains material similar to that published by the author in a paper where the emphasis was on the researchissues to be addressed in nomadic computing (kleinrock, 1995).nomadic computing and communications335the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.nomadic computing2we are interested in those capabilities that must be put in place to support nomadicity. the desirablecharacteristics for nomadicity include independence of location, motion, computing platform, communicationdevice, and communication bandwidth, and widespread presence of access to remote files, systems, and services.the notion of independence does not refer here to the quality of service, but rather to the perception of acomputing environment that automatically adjusts to the processing, communications, and access available at themoment. for example, the bandwidth for moving data between a user and a remote server could easily vary froma few bits per second (in a noisy wireless environment) to hundreds of megabits per second (in a hardwiredatm environment); or the computing platform available to the user could vary from a lowpowered personaldigital assistant while traveling to a powerful supercomputer in a science laboratory. indeed, today's systemstreat radically changing connectivity or bandwidth/latency values as exceptions or failures; in the nomadicenvironment, these must be treated as the usual case. moreover, the ability to accept partial or incomplete resultsis an option that must be made available because of the uncertainties of the informatics infrastructure.the ability to automatically adjust all aspects of the user's computing, communication, and storagefunctionality in a transparent and integrated fashion is the essence of a nomadic environment.some of the key system parameters of concern include bandwidth, latency, reliability, error rate, delay,storage, processing power, interference, version control, file synchronization, access to services, interoperability,and user interface. these are the usual concerns for any computercommunication environment, but what makesthem of special interest for us is that the values of these parameters change dramatically as the nomad movesfrom location to location. in addition, some totally new and primary concerns arise for the nomad such asweight, size, and battery life of the portable devices as well as unpredictability and wide variation in thecommunication devices and channels. the bottom line consideration in many nomadic applications is, of course,cost.many of the key parameters above focus on the lower levels of the architecture, and they have received themost attention from industry and product development to date. this is natural since hardware devices must focuson such issues. however, there is an enormous effort that must be focused on the middleware services ifnomadicity is to be achieved. we identify a number of such services below, but we must recognize that they arein the early stages of identification and development.there are a number of reasons why nomadicity is of interest. for example, nomadicity is clearly a newlyemerging technology that already surrounds the user. indeed, this author judges it to be a paradigm shift in theway computing will be done in the future. information technology trends are moving in this direction. nomadiccomputing and communications is a multidisciplinary and multiinstitutional effort. it has a huge potential forimproved capability and convenience for the user. at the same time, it presents at last as huge a problem ininteroperability at many levels. the contributions from any investigation of nomadicity will be mainly at themiddleware level. the products that are beginning to roll out have a shortterm focus; however, there is anenormous level of interest among vendors (from the computer manufacturers, the networking manufacturers, thecarriers, and so on) for longrange development and product planning, much of which is now under way.whatever work is accomplished now will certainly be of immediate practical use.there are fundamental new research problems that arise in the development of a nomadic architecture andsystem. let us consider a sampling of such problems, which we break out into systems issues and wirelessnetworking issues.systems issuesone key problem is to develop a full system architecture and set of protocols for nomadicity. these shouldprovide for a transparent view of the user's dynamically changing computing and communications environment.the protocols must satisfy the following kinds of requirements: interoperation among many kinds of infrastructures (e.g., wireline and wireless); ability to deal with unpredictability of user behavior, network capability, and computing platform;nomadic computing and communications336the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. ability to provide for graceful degradation; ability to scale with respect to heterogeneity, address space, quality of service (qos), bandwidth,geographical dimensions, number of users, and so on; integrated access to services; ad hoc access to services; maximum independence between the network and the applications from both the user's viewpoint and thedevelopment viewpoint; ability to match the nature of what is transmitted to the network bandwidth availability (i.e., compression,approximation, partial information, and so on); cooperation among system elements such as sensors, actuators, devices, network, operating system, filesystem, middleware, services, applications, and so on; and ability to locate users, devices, services, and the like.in addition, the following components can help in meeting these requirements: an integrated software framework that presents a common virtual network layer; appropriate replication services at various levels; file synchronization; predictive caching; consistency services; adaptive database management; location services (to find people and devices via tracking, forwarding, searching, etc.)šmobile ip (perkins,1995) is an example of an emerging standard here; discovery of resources; and discovery of profile.a second research problem is to develop a reference model for nomadicity that will allow for a consistentdiscussion of its attributes, features, and structure. this should be done in a way that characterizes the view ofthe system as seen by the user, and the view of the user as seen by the system. the dimensions of this referencemodel might include the following: system state consistency (i.e., is the system consistent at the level of email, files, database, applications,and so on?); functionality (this could include the bandwidth of communications, the nature of the communicationinfrastructure, and the quality of service provided); and locality, or awareness (i.e., how aware is the user of the local environment and its resources, and howaware is the environment of the users and their profiles?).a third research problem is to develop mathematical models of the nomadic environment. these modelswill allow one to study the performance of the system under various workloads and system configurations aswell as to develop design procedures.as mentioned above, the area of nomadic computing and communications is multidisciplinary. following isa list of the disciplines that contribute to this area (in topdown order): advanced applications, such as multimedia or visualization; database systems; file systems; operating systems; network systems; wireless communications; lowpower, lowcost radio technology;nomadic computing and communications337the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. microelectromechanical systems (mems) sensor technology; mems actuator technology; and nanotechnology.the reason that the last three items in this list are included is that we intend that the nomadic environmentinclude the concept of an intelligent room. such a room has embedded in its walls, furniture, floor, and otheraspects all manner of sensors (to detect who and what is in the room), actuators, communicators, logic, cameras,etc. indeed, one would hope to be able to speak to the room and say, for example, "i need some books on thesubject of spread spectrum radios," and perhaps three books would reply. the replies would also offer to presentthe table of contents of each book, as well, perhaps, as the full text and graphics. moreover, the books wouldidentify where they are in the room, and, if such were the case, might add that one of the books is three doorsdown the hall in a colleague's office!there are numerous other systems issues of interest that we have not addressed here. one of the primaryissues is that of security, which involves privacy as well as authentication. such matters are especially difficult ina nomadic environment, because the nomad often finds that the computing and communication devices areoutside the careful security walls of his or her home organization. this basic lack of physical securityexacerbates the problem of achieving nomadicity.we have only touched on some of the systems issues relevant to nomadicity. let us now discuss some ofthe wireless networking issues of nomadicity.wireless networking issuesit is clear that a great many issues regarding nomadicity arise whether or not there is access to wirelesscommunications. however, with such access, a number of interesting considerations arise.access to wireless communications provides two capabilities to the nomad: it allows for communicationfrom various (fixed) locations without being connected directly into the wireline network, and it allows thenomad to communicate while traveling. although the bandwidth offered by wireless communication mediavaries over as enormous a range as does the wireline network bandwidth, the nature of the error rate, fadingbehavior, interference level, and mobility issues for wireless are considerably different, so that the algorithmsand protocols require some new and different forms from those of wireline networks (katz, 1994). for example,the network algorithms to support wireless access are far more complex than for the wireline case; some of theseare identified below. whereas the location of a user or a device is a concern for wireline networks as describedabove, the details of tracking a user moving in a wireless environment add to the complexity and require rules forhandover, roaming, and so on.the cellular radio networks so prevalent today have an architecture that assumes the existence of a cell basestation for each cell of the array; the base station controls the activity of its cell. the design considerations ofsuch cellular networks are reasonably well understood and are being addressed by an entire industry (padgett etal., 1995). we discuss these no further here.3there is, however, another wireless networking architecture of interest that assumes no base stations (jain etal., 1995; short et al., 1995). such wireless networks are useful for applications that require "instant"infrastructure, among others. for example, disaster relief, emergency operations, special military operations, andclandestine operations are all cases where no base station infrastructure can be assumed. in the case of no basestations, maintaining communications is considerably more difficult. for example, it may be that the destinationfor a given reception is not within range of the transmitter, and some form of relaying is therefore required; thisis known as "multihop" communications. moreover, since there are no fixedlocation base stations, then theconnectivity of the network is subject to considerable change as devices move around and/or as the mediumchange its characteristics. a number of new considerations arise in these situations, and new kinds of networkalgorithms are needed to deal with them.to elaborate on some of the issues of concern if there are no base stations, we take three possible scenarios:nomadic computing and communications338the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.1. static topology with onehop communications. in this case, there is no motion among the systemelements, and all transmitters can reach their destinations without any relays. the issues of concern,along with the needed network algorithms (shown in bold print), are as follows: can you reach your destination?: power control what access method should you use?: network access control which channel (or code) should you use?: channel assignment control will you interfere with another transmission?: power and medium access control when do you allow a new "call" into the system?: admission control for different multiplexed streams, can you achieve the required qos (e.g., bandwidth, loss, delay, delayjitter, higherorder statistics, etc.)?: multimedia control what packet size should you use?: system design how are errors to be handled?: error control how do you handle congestion?: congestion control how do you adapt to failures?: degradation control2. static topology with multihop communications. here the topology is static again, but transmitters may notbe able to reach their destinations in one hop, and so multihop relay communications is necessary in somecases. the issues of concern, along with the needed network algorithms (shown in bold print), include allof the above plus the following: is there a path to your destination?: path control does giant stepping help (takagi and kleinrock, 1984)?: power control what routing procedure should you use?: routing control when should you reroute existing calls?: reconfiguration control how do you assign bandwidth and qos along the path?: admission control and channel assignment3. dynamic topology with multihop. in this case, the devices (radios, users, etc.) are allowed to move, whichcause the network connectivity to change dynamically. the issues of concern, along with the needednetwork algorithms (shown in bold print), include all of the above plus the following: do you track, forward, or search for your destination?: location control what network reconfiguration strategy should you use?: adaptive topology control how should you use reconfigurable and adaptive base stations?: adaptive base station controlthese lists of considerations are not complete but are only illustrative of the many interesting researchproblems that present themselves in this environment. the net result of these considerations is that the typical 7layer osi model for networking must be modified to account for these new considerations. for example, wemust ask what kind of network operating system (nos) should be developed, along with other network functions(short et al., 1995); what mobility modules must be introduced to support these new services; and so on.this section addresses mainly the network algorithm issues and does not focus on the many other issuesinvolved with radio design, hardware design, tools for cad, system drivers, and so on. what is important is thatthe network algorithms must be supported by the underlying radio (e.g., to provide signaltointerference ratios,ability to do power control, change codes in cdma environments, and the like). these obviously have an impacton the functionality, structure, and convenience of the appliance that the user must carry around, as well as on itscost.if we ask what are the great applications of wireless technology that affect the fabric of our society, theneducation applications stand out among the most significant. in this application, a wireless infrastructure couldserve to provide connectivity in a costeffective fashion to rural areas that are difficult to serve otherwise; it couldnomadic computing and communications339the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.serve within a school to provide flexible sharing of devices as they move from location to location. for longdistance wireless access, it seems that direct broadcast satellite (dbs) technology would be great benefit, but itshould also provide a decent upchannel as well. for inbuilding wireless access, the availability of unlicensedspectrum for datašsay, the 60ghz rangešwould serve a number of education applications nicely.one might ask what role government could play in helping to bring about some of the advantages justdescribed. the allocation of spectrum is one of the major ways in which government can assist. currently, mostspectrum is assigned for long periods of time to specific types of services; it seems that a more liberal view onthe kinds of uses for radio bandwidth would encourage innovative applications, services, and efficient sharing ofthis bandwidth. any action (such as spectrum allocation and use) that encourages the introduction of innovativeservices is to be encouraged by whatever means government has available.conclusionthis paper presents nomadicity as a new paradigm in the use of computer and communications technologyand outlines a number of challenging problems. it is clear that our existing physical and logical infrastructuremust be extended to support nomadicity in the many ways described here. the implication is that we mustaccount for nomadicity at this early stage in the development and deployment of the nii; failure to do so willseriously inhibit the growth of nomadic computing and communications. in addition to those issues we raisehere, there are far more we have not yet identified. those will arise only as we probe the frontiers of nomadiccomputing and communications.referencesjain, r., j. short, l. kleinrock, s. nazareth, and j. villasenor. 1995. "pcnotebook based mobile networking: algorithms, architecturesand implementations," icc '95, pp. 771œ777, june.katz, r.h. 1994. "adaptation and mobility in wireless information systems," ieee personal communications magazine 1(1):6œ17.kleinrock, l. 1995. "nomadic computingšan opportunity," computer communications review, acm sigcomm 25(1):36œ40.nomadic working team (nwt). 1995. "nomadicity in the nii," crossindustry working team, corporation for national researchinitiatives, reston, va.padgett, j.e., c.g. gunther, and t. hattori. 1995. "overview of wireless personal communications," ieee communications magazine 33(1):28œ41.perkins, charles. 1995. "ip mobility support," an internet draft produced for the internet engineering task force; see http://www.ietf.cnri.reston.va.us/ids.by.wg/mobil.short, j., r. bagrodia, and l. kleinrock. 1995. "mobile wireless network system simulation," proceedings of acm mobile computing & networking conference (mobicom '95), pp. 195œ205, november.takagi, h., and l. kleinrock. 1984. "optimal transmission ranges for randomly distributed packet radio terminals," ieee transactions on communications, vol. com32, no. 3, pp. 246œ257, march.weiser, m. 1991. "the computer for the 21st century," scientific american, september, pp. 94œ104.notes1. moreover, one may have more than a single "home base"; in fact, there may be no welldefined "home base" at all.2. some of the ideas presented in this section were developed with two groups with which the author has collaborated in work on nomadiccomputing and communications. one of these is the nomadic working team (nwt) of the cross industrial working team (xiwt); theauthor is the chairman of the nwt, and this working team recently published a white paper on nomadic computing (nwt, 1995). thesecond group is a set of his colleagues at the ucla computer science department who are working on an arpasupported effort known astravler, of which he is principal investigator.nomadic computing and communications340the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.3. wireless lans come in a variety of forms. some of them are centrally controlled and therefore have some of the same control issues ascellular systems with base stations; others have distributed control, in which case they behave more like the nobasestation systems wediscuss in this section.nomadic computing and communications341the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.41nii 2000: the wireless perspectivemary madiganpersonal communications industry associationabstractas a key component of the national information infrastructure (nii), the mobile nature of wirelesscommunications provides consumers with the opportunity to access the nii from any place at any time. today,as the federal communications commission (fcc) makes available new spectrum for wireless networks thatwill support a range of new services, both voice and data, wireless communications is poised on the brink of anew era.however, new spectrum leads to new entrants, and wireless companies of the future will face a much morecompetitive marketplace. this competition will mean great things to the american consumer, who will benefitfrom the innovation and lower prices that the increased competitiveness will spark.with the introduction of more competition into the telecommunications marketplace, public policydecisions need to be crafted to ensure that this vision of a wireless future can be realized.introductionfor the wireless communications industry, 1994 was a banner year as the fcc launched the first set ofspectrum auctions for the narrowband and broadband pcs, giving birth to a whole new erašthe era of personalcommunications.the vision of pcs is the concept of anytime, anywhere communicationsšwhether that be datacommunications, voice communications, or both. but what is the real potential for this marketplace? how manyindividuals are likely to buy into the vision of anytime, anywhere communications?in early 1995, the personal communications industry association (pcia) completed a survey of pciamembers to evaluate the growth, composition, and characteristics of the existing and future personalcommunications industry and published the results in a pcs market demand forecast. the results indicate thatby 2000, combined demand for new pcs, cellular, and paging and narrowband pcs will amount to almost 118million subscriptions.to meet this level of demand in the marketplace, the wireless industry must be assured that it will be able todeploy services in a timely fashion. issues such as site acquisition and interconnection to the local exchangecarriers are critical to timely deployment of developing wireless networks and competing effectively.government must assure that the industry has the opportunity to meet the anticipated demand outlined in thepcs market demand forecast by ensuring a level playing field for all wireless telecommunications serviceproviders and by allowing, where appropriate, competitionšnot regulationšto govern the marketplace.personal communications industry associationestablished in 1949, pcia has been instrumental in advancing regulatory policies, legislation, and technicalstandards that have helped launch the age of personal communications services. through many vehiclesšpolicyboards, market forecasts, publications, spectrum management programs, seminars, techniciannii 2000: the wireless perspective342the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.certification programs, and its industry trade show, the personal communications showcasešpcia iscommitted to maintaining its position as the association for the pcs industry.pcia's member companies include pcs licensees and those involved in the cellular, paging, esmr, smr,mobile data, cable, computer, manufacturing, and local and interexchange sectors of the industry, as well asprivate corporate systems users, wireless system integrators, communication site owners, distributors and serviceprofessionals, and technicians.personal communication servicepersonal communication service includes a broad range of telecommunications services that enable peopleand devices to communicate independent of location. pcs networks and devices operate over a wide range offrequencies assigned and authorized by the fcc. there are currently seven different air interface technologiesproposed for standardization for the new pcs licensees that will be operating in the 1.8 ghz band. serviceproviders that will be operating at these frequencies either are new entrants with no established network or areexisting telecommunications service providers, such as cable, cellular, local exchange, and longdistancecarriers. with the technology choices companies make over the next few months, there will need to be analysisof how and to what extent the various wireless and wireline networks will work together.interoperability and interworkingto facilitate roaming among pcs carriers, some degree of interoperability and interworking needs to beaccomplished between the networks. pcia defines interoperability and interworking as follows: interoperability. the ability to logically connect two or more functional network elements for the purposesof supporting shared processes such as call delivery. service interoperability is defined as the assurance thata service invoked by a subscriber in a network will be performed by the other network in the same way froma user perspective. network interoperability is defined as the direct onetoone mapping of services andprotocols between interconnected networks. for example, a subscriber may invoke call waiting featuresexactly the same way in a dcs 1900 (gsm based) network in new york city as in a dcs 1900 (gsmbased) network in san francisco. in this scenario, call waiting network protocol messages map between thetwo networks on a direct onetoone basis. interworking. the ability to translate between two or more dissimilar networks for the purpose of achievingeffective interoperability. service interworking is defined as the protocol translation that may or may notresult in the service being performed in the receiving network in the same way from a user perspective.network interworking is defined as functional mapping of services and protocols across networks (someservices may not be delivered or may be delivered in a different way). for example, a subscriber with a pcs2000 (composite cdma/tdma) wireless personal terminal may register and authenticate on a sanfrancisco is41 based network, just as he or she could on a home base, dcs 1900 (gsm based) network innew york city. although the method of registering may not be identical between systems, the end result iseffectively the samešthe subscriber can be registered and authenticated on both networks, and locationservices work across both platforms.standards should be developed and are currently being worked on in domestic and international standardsbodies to facilitate features and services delivered consistently and in similar fashions to an end user, regardlessof the air interface and/or network implementation used. all networks do not necessarily need to interoperate orinterwork with every other network; those decisions will be made on a companybycompany basis. but theindustry is working to make sure that if that choice is made, the technology will be available to support it.nii 2000: the wireless perspective343the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.market forecastsince 1992, pcia has regularly surveyed wireless communications industry leaders to evaluate the growth,composition, and characteristics of the future of the personal communications industry and has published theseresults in a pcs market demand forecast. in its yearly surveys, pcia has asked respondents to provide marketsize predictions in terms of the number of anticipated subscriptions, not subscribers, anticipating that anindividual would probably subscribe to more than one type of wireless service in the coming decade. as inprevious years, the 1995 figures show that consumer demand for personal communications services is expectedto grow at everincreasing rates.demand growth for new broadband pcs customers is expected to reach 15 million subscriptions by 2000.total revenues are expected to reach $8.8 billion by the year 2000, with 7 percent of that revenue coming fromdata services. average revenue per subscription is expected to be 20 percent less than that for cellular. figuresfor 2005 indicate strong sustained growth to almost 40 million subscriptions and total revenues reaching $17.5billion, with 12 percent from data services.established voice services such as cellular are expected to grow as well. respondents expect strong cellulargrowth during the next 5 years, with the 1994 yearend subscriber count of 23.2 million expected to double toapproximately 50 million subscriptions by 2000, with nearly 65 million cellular subscriptions expected by 2005.thirty percent of the total cellular subscriptions are expected to come from the business segment, representing apresumed growth of the cellular markets into households over the next 10 years. total cellular revenues areforecast to be approximately $26 billion by 2000 and $31 billion by 2005.in the narrowband pcs arena, market size is expected to reach more than 50 million subscriptions by 2000;by 2005, 71 million oneway and 21 million twoway messaging subscriptions are anticipated. in addition,survey results forecast strong growth from new narrowband pcs and advanced one and twoway messaging andsuggest that these will become established in the wireless world over the next decade. customer segments willgrow due to new narrowband applications and services. survey results show that by the year 2000, more than 50percent of oneway and about 65 percent of twoway subscribers are expected to be from business segments.assuming that businesses will continue to upgrade services, they are expected to remain more than 50 percent ofthe total subscriber base through the next decade. total revenues are expected to reach $4.7 billion for onewaypaging and $1.9 billion for twoway paging by 2000, and $5.6 billion and $3 billion, respectively, by 2005.deployment of the national information infrastructuresite acquisition issuesacquiring pcs antenna and base station sites and gaining the appropriate zoning approvals vary by stateand local jurisdictions and are important in wireless network deployment. furthermore, there are issues regardingsite acquisition (such as faa tower regulations and the lack of a uniform policy regarding sites on federalproperty) that need to be addressed at the federal level.issues at the local levelthere are more than 38,000 local jurisdictions throughout the nation, each with the authority to preventantenna construction; establish standards that can result in site location degrading the quality of service; orprolong site selection, thereby making it unnecessarily expensive. with an estimated 100,000 new wirelessantenna sites predicted over the next 10 years, any licensing obstacles present significant problems.congress has recognized the need to remove state and local barriers to deploying cmrs facilities byprohibiting state and local government regulation of matters relating to market entry and rates. the current draftof the senate's telecommunications competition and deregulation act of 1995 states that no state or localstatute may prohibit or have the effect of prohibiting the ability of any entity to provide interstate or intrastatetelecommunications services. it further states that if after notice and comment the fcc determines that a state ornii 2000: the wireless perspective344the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.local requirement is inconsistent with the legislation, the fcc shall immediately preempt enforcement of therequirement.ctia filed a petition for rule making requesting that the fcc initiate a rulemaking proceeding to preemptstate and local regulation of tower sites for cmrs. the petition states that the state preemption language insection 332(c) of the communications act gives the commission authority to exercise such preemption, sincelocal zoning could constitute an ''indirect" regulation of entry.comments on the petition for rule making were due in february 1995. predictably, service providers filedin support of the petition, while state and local governments and consumer groups filed in opposition. thechallenge the wireless industry faces is balancing the recognized needs of the local community to have oversightand fee administration of zoning issues against attempts to meet the everincreasing demand for new wirelessservices.additionally, the fcc has imposed buildout requirements on the new pcs licensees which mandate thatcertain percentages of the licensees' markets be covered within set time frames. potential conflicts between stateand federal regulations threaten to delay the entry of wireless services.site acquisitions on federal propertyfederal property could, in many situations, provide prime locations for pcs base stations. unfortunately,many agencies of the federal government are not willing or are unable to entertain the prospect of such facilitiesbecause of perceived administrative burdens, lack of benefit to local agency staff, or lack of clear policy orregulations for leasing of federal property for such an installation. additionally, all of the federal agencies thatallow private communications facilities on their land have different regulations, lease documents, and processesfor doing so. these are often difficult, time consuming, and expensive for both the agency and thecommunications companies.making sure federal land resources continue to be available for efficient delivery of mobile communicationsservices, and ensuring that taxpayers receive a fair price from every communications company with transmitterson public lands, are goals shared by industry, the federal agencies, and the public. however, there needs to be aconsistent, governmentwide approach for managing the site acquisition process on federal property.the executive branch needs to set a clear directive in order to overcome the obstacles wireless licenseesface when trying to acquire sites on federal property. the benefits to the federal government could includeincreased revenues from the installation of pcs networks above and beyond the auction proceeds, and thepotential for improved communications on federal property.faa tower review processpcia has initiated discussions with the federal aviation administration (faa) to remove any possiblefaa obstacles to efficient deployment of pcs systems. the fcc has established licensing rules that havestreamlined the approval necessary to bring systems and pcs cell sites on line. however, due to administrativelimitations, the faa, which must review many requests for towers to ensure air safety, has experienced longerprocessing times that have delayed carriers' ability to activate certain transmitter sites. with approximately 25 to30 percent of new wireless sites requiring faa action and review, pcia fears that faa processing delays couldsignificantly burden the industry. working groups at the national and local levels have been established as aforum to explore methods of educating the industry about faa procedures and to explore ways to streamline thefaa tower review process.the faa, fcc, pcia, and the cellular telecommunications industry association (ctia) have all agreedto participate in this dialogue as part of an antenna work group (awg) in washington, d.c. pcia has alsoparticipated in dialogues with the faa southern region and will be working on a local level in other workinggroups to identify ways to improve the faa process.nii 2000: the wireless perspective345the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.federal radio frequency emissions standardas pcs, cellular, paging, and other wireless carriers build out networks, they are increasingly facing stateand local laws and ordinances based on radio frequency (rf) exposure levels, often with conflicting scope andstandards, resulting in compliance difficulties. conflicting standards affect the range of wireless services and cangreatly diminish the quality of service consumers receive. this adds greatly to the expense borne by the industry,not only in legal and other business expenses, but also in lost revenue opportunities from long delays inproviding services.the fcc has the authority to preempt local jurisdictions on cell/antenna/tower siting but to date hasapproached this issue on a casebycase basis. with as many as 100,000 new wireless sites to be installed,including new pcs sites, and additional sites that will be needed for the expansion and enhancement of serviceareas for paging, smr, esmr, and cellular service, a casebycase approach to preemption is no longer realistic.the fcc on april 3, 1993, issued its notice of proposed rule making, which proposed updating guidelinesand methods for evaluating the environmental effects of electromagnetic exposure, and adopting the standarddeveloped by the american national standards institute (ansi) with the institute of electrical and electronicengineers (ieee). in december 1994, the spectrum engineering division of the office of engineering andtechnology of the fcc issued information indicating that levels of exposure to rf at ground level below typicalcellular towers are hundreds to thousands of times lower than the proposed standard.on december 22, 1994, the electromagnetic energy association (eea) filed a petition with the fcc for afurther notice of proposed rule making. the petition requested that the fcc preempt state and local regulationof rf exposure levels found to be inconsistent with the fcc proposed ansi standard.pcia favors the establishment of a single, national rf emissions standard that may not be exceeded bylocal regulations. pcia encourages the relevant federal agencies to work cooperatively with industry on thisissue to develop such a national standard.interconnectioninterconnection with local exchange carriersnegotiating reasonable rights, rates, and terms under which companies will interconnect with othernetworks is critical to the success of pcs. with many pcs hopefuls eyeing the local exchange market as apotentially lucrative area in which to compete, the terms of companies' interconnection agreements as a cocarrier will become even more important as they strive to compete with local exchange carriers (lecs), andtherefore they will need reasonable interconnection agreements so that they can offer customers lowcostexchange service.as an example of current interconnection costs, type 2 interconnection charges for cellular carriersgenerally are measured on a perminute basis, with costs ranging from 2 cents per minute to 6 cents per minute,and 3 cents per minute often being considered a "good" interconnection rate.interconnection charges have diminished cellular carriers' revenues since the first system came on line, andthey remain a high cost to carriers today. take, for example, a cellular monthly bill of $59, which includes 86minutes of air time, at the rate of 3 cents per minute for interconnection. interconnection charges represent $2.58of the bill, or 4.37 percent of revenue.as air time costs continue to decline in the wireless marketplace, interconnection costs will begin to reducerevenues even more. for example, cybertel cellular offers 5 cents per minute of air time in kauai to competewith the local exchange carrier. at an interconnection rate of 3 cents per minute, interconnection charges couldconsume 60 percent of the carrier's air time revenue.obviously, those wishing to compete at the local loop must achieve lower interconnection costs to competewith established carriers on price. one solution to this problem is mutual compensation, where bothtelecommunications carriers are compensated for the traffic that terminates on their network.nii 2000: the wireless perspective346the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.mutual compensationmutual compensation is the concept that a carrier should be compensated for traffic that originates onanother network but terminates on that carrier's network, and vice versa. currently, wireless carriers mustcompensate wireline carriers for traffic that originates on a wireless network and terminates on a wirelinenetwork. almost without exception, wireline carriers do not compensate wireless carriers for traffic originatingon a wireline network and terminating on a wireless network.the fcc has repeatedly stated that, for interstate traffic, wireline carriers must compensate wireless carriersfor traffic originating on a wireline network and terminating on a wireless network. however, states have beenreluctant to enforce mutual compensation on an intrastate basis, and therefore wireline carriers have refused toparticipate in mutual compensation on either an intra or interstate basis.enforcement of mutual compensation rights of wireless carriers is considered to be a key to full competitionby wireless carriers in the telecommunications market and will have a significant positive financial impact forthe wireless industry.one potential solution to the high cost of interconnection would be mandating mutual compensationthrough reciprocal elimination of interconnection charges. one example of this solution is the agreement reachedin new york between time warner and rochester telephone, whereby rochester telephone will collect 2.2cents per minute for traffic terminating on its network and pay at the same rate for its own traffic terminating onother networks. according to the agreement, mutual compensation provisions are eliminated when the trafficflow differentials fall below 10 percent.numbering issuesthe issue of who controls numbers is key to the success of pcs carriers. traditionally, most nationalnumbering resources have been assigned by the north american numbering plan administration sponsored bybellcore, which in turn is owned by the bell operating companies. generally, the dominant local exchangecarrier ends up assigning numbers to wireless carriers in its local telephone market. wireless carriers usually arecharged for activating blocks of numbers in local exchange carrier networks, and the charges vary greatly.recently, bellcore has come under scrutiny for its administration of numbering resources, and actions bywireline carriers within the past few months have brought the issue to the forefront. for instance, in chicago,ameritech proposed an "overlay" area code. this would require cellular and paging subscribers to give backtheir numbers and receive a new area code, thus freeing up numbers in the almostexhausted code for newwireline subscribers. at a january fcc open meeting, the fcc found this proposal to be "unreasonablydiscriminatory" against wireless carriers.the fcc initiated a proceeding more than a year ago to examine whether an independent entity shouldoversee the assignment of numbers, and it appears as if the senate telecommunications reform effort mightmandate the formation of an independent entity to oversee the numbering assignment process.number portabilityanother key issue for those who want to compete with the local telephone company is number portability,or the ability of an end user, such as an individual or business, to retain its 10digit geographic north americannumbering plan (nanp) numberševen if the end user changes its service provider, the telecommunicationsservice with which the number is associated, or its permanent geographic location.with few exceptions, today end users may not retain their 10digit nanp number if they: switch service providers, referred to as "service provider portability" (e.g., a user switches from anincumbent lec to a new competitive access provider);nii 2000: the wireless perspective347the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. change the service to which the number was originally assigned, referred to as "service portability" (e.g., acellular telephone number becomes the wireline home telephone number); change their permanent location, referred to as "geographic portability" (e.g., an end user moves to adifferent part of the city or state, and may be assigned either a new 7digit phone number in the old areacode or a new 10digit number in a new area code).service provider portability, that is, moving a number from one service provider to another, is vital for thosecompanies that wish to compete for customers at the local exchange level. it is much easier to gain market shareif the customer a company is trying to attract does not have to change his or her phone number when changingservice providers.currently, 800 numbers are portable between 800 number service providersšan example of serviceprovider portability. this portability allows the 800 service end user to retain his or her individual 800 number,even when switching 800 service providers. portability of 800 numbers was ordered by the fcc andimplemented in 1993.industry efforts to address number portabilitythe industry numbering committee (inc), a consensusbased industry body sponsored by the intercarriers compatibility forum (iccf), has been actively addressing number portability issues since the fall of1993. the inc number portability workshop has been addressing a range of issues associated with numberportability, including a target portability architecture, network impacts of number portability, and highlevelpolicy issues such as mandated interconnection.public service obligationsthe advent of increased mobility is having an impact on telecommunications public policy. how doeswireless technology fit into public policy initiatives such as universal service and access to enhanced 911emergency calling services? policies regarding universal service were developed to apply to a strictly wirelineenvironment where competition at the local level was nonexistent. additionally, wireless technologies present achallenge to the traditional wireline approach to providing enhanced 911 emergency calling. as wireless serviceproviders begin to compete for the local loop, how wireless fits into such public policy provisions will need to beseriously considered.universal serviceuniversal service, as a public policy concept, is the belief that access to basic telephone services by thewidest possible cross section of the american public is in the social and economic interests of the united states.over a period of many years, congress has mandated the creation of universal service programs to supportuniversal service public policy goals. the fcc is charged with fulfilling these congressional mandates.within the telecommunications industry, universal service refers to a complex system of explicit andimplicit charges and cost allocation mechanisms levied on particular carriers and customers in order to provideaccess to, and subsidize the rates of, basic wireline services for residential customers, highcost customers andcarriers, lowincome customers, rural areas, and services for hearing and speechimpaired consumers.estimates of the current total costs of supporting universal service goals and policies range as high as $20billion to $30 billion annually. congress is intent upon reform of universal service policy and fundingmechanisms as part of its effort to reform existing telecommunications law. any reforms could have apotentially huge economic impact upon the wireless industry.nii 2000: the wireless perspective348the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.universal service reform is a critical part of telecommunications reform and it appears inevitable ifcongress passes a telecommunications reform bill. although it is too early to tell what shape universal servicewill take, a number of issues need to be considered.wireless access to enhanced 911 emergency servicesthe fcc, on october 19, 1994, released a notice of proposed rule making (nprm) regarding revision ofthe commission's rules to ensure compatibility with enhanced 911 (e911) emergency services. in many areas ofthe country, wireline subscribers are provided e911 service by wireline carriers, which entails transmitting theaddress and phone number of the caller to the public safety answering point. the nprm addresses pbx issuesand wireless service provider issues. the nprm outlines proposed requirements on wireless services regarding: 911 availability; grade of service; privacy; rering/call back; grade of service; liability; cost recovery; access to text telephone devices (tty); equipment manufacture, importation, and labeling; user location information; compatibility with network services; common channel signaling; and federal preemption.the proposed requirements have considerable technical and economic implications that need to be fullyexamined. pcia, in cooperation with representatives of the public safety community, drafted the joint pcia,apco, nasna emergency access position paper, which was filed with the fcc in july 1994. this joint paperdocumented the first attempt of the pcs community to comprehensively address the needs of the public safetycommunity. the fcc used the joint paper as a basis for its nprm addressing enhanced 911 emergency callingsystems.pcia fully shares the commission's important objective of maximizing compatibility between wirelessservices and enhanced 911 emergency calling systems. specifically, it concurs that subscribers to realtime voiceservices interconnected with the public switched telephone network ultimately should enjoy the same access toadvanced emergency response services as do wireline service subscribers, with due consideration for the uniquecharacteristics of radiobased technology. at the same time, however, pcia strongly disagrees with the approachtoward achievement of the compatibility objective that is set forth in the nprm.pcia believes that fullscale regulatory intervention is not necessary at this time and that the profoundtechnical issues raised by compatibility cannot be resolved through imposition of arbitrary deadlines as proposedin the nprm. pcia proposes, as an alternative to arbitrary deadlines, that the industry work to develop technicalsolutions to the public safety community's requirements and that the fcc require periodic reports from theindustry on its progress in meeting the ultimate goals the fcc has set forth.conclusionsthe fact that the nii is a complex web of wireline and wireless service providers providing both voice anddata services to the end user at home, in the office, and walking or driving down the street needs to be consideredin any telecommunications policy initiative. the new wave of wireless service providers, while providing thenii 2000: the wireless perspective349the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.consumer with more choices in services and features than ever before, presents a challenge to the publicpolicymaker who tries to determine how to ensure that telecommunications services are made available to thebroadest range of consumers. competition will take care of that to a certain extent. however, where appropriate,government may need to step in on issues such as interconnection rights, mutual compensation, and numberingto ensure that new entrants are treated as equals by incumbent carriers. furthermore, revision of universal serviceand enhanced 911 policies needs to take into consideration both the wireless and the wireline industries.additionally, the wireless industry is often faced with federal and state regulatory processes that can slowdown the deployment of new networks. federal guidelines regarding site acquisition and radio frequencyemissions are necessary to ensure timely availability of new services. there continues to be a high demand forwireless services, and the industry is poised to meet that demand. however, public policy should be developedsuch that the promise of wireless services as an integral component of the nii is realized.notes1. the composite cdma/tdma system is an air interface technology currently being standardized for pcs in the 1.8ghz band.2. broadband pcs refers to the family of mobile or portable radio services operating in the 1.8ghz range and providing a wide variety ofinnovative digital voice and data services.3. narrowband pcs services are expected to include advanced voice paging, twoway acknowledgment paging, data messaging, and bothoneway and twoway messaging.nii 2000: the wireless perspective350the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.42small manufacturing enterprises and the nationalinformation infrastructurerobert m. mason, chester bowling, and robert j. niemicase western reserve universitystatement of the problemthe vision for the future is that an emerging national information infrastructure (nii) and its defensecounterpart (dii) will equip u.s. industry to be second to none in the global economy. the nii will enable theu.s. industrial base to become more agile and to operate as a highly competitive, flexible, justintime,manufactureondemand system that facilitates free competition and specialization among manufacturers andsuppliers. all firms, regardless of size, will have ready access to product requirements and specifications andwill be able to compete fairly with other firms. moreover, the nii with the dii will encourage commercialsuppliers to respond to defense needs, enabling dual use designs and strengthening the flexibility of the nation'sdefense infrastructure.the reality is that many existing small firms are ill equipped to participate in this vision. moreover, there isconcern that the learning cycle for small manufacturing enterprises (referred to here as smes) to implementinformation technology is too long and costly for them to effectively make the transition to the nii environment.the solution to the problem is not simply one of assuring that every sme can purchase and install a newinformation system. instead, the solution requires an understanding of how a complex combination of structural,technical, managerial, and economic factors affect the diffusion of information technology in the manufacturingsector, especially among smes. from the viewpoint of our national economy, the problem is that this complexset of factors impedes the effective implementation of information technology in smes and puts at risk asignificant component of the nation's manufacturing base, a component that is responsible for up to nearly 40percent of the nation's manufacturing employment. developing nations may "leapfrog" over the united statesand other advanced nations if our established enterprises are unable to change quickly enough. the purpose ofthis paper is to help understand this set of factors and to explore how best to manage the risk associated with aslow rate of diffusion of information technology in smes.overviewthe "background" section provides a synopsis of the different views on the role of smes in the nation'seconomy and manufacturing infrastructure. this section also summarizes the different frameworks within whichwe can understand the economic, behavioral, structural, and technical issues associated with how smes mayparticipate in the benefits of the nii.the "analysis" section of the paper provides more detail on the frameworks outlined in the backgroundsection and examines the prospects for smes to become full participants in the nii. this section synthesizesadoption and diffusion studies and research on the implementation of new information technologies. theemerging framework is that of organizational learning at the level of the firm and the concept of the supply chain(or value chain). the learning framework enables us to make sense of the range of factors associated withsmall manufacturing enterprises and the national information infrastructure351the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.technology adoption, and the value chain framework illustrates why action by an individual firm is inadequatefor that firm to realize the benefits of an nii.the final section of the paper discusses opportunities for national policy to alleviate the problem of sme'sparticipation in the nii. the paper concludes that a coordinated collaboration among private, university, andgovernment resources offers the best way to assist u.s. smes in making the transition to electronic commerceand the benefits of the nii.backgroundsmes and the economysmall manufacturing enterprises (smes) are responsible for an estimated 28 to 40 percent of theemployment in the manufacturing sector.1 moreover, there is evidence that smes are more effective at jobcreation2 and job replacement,3 more innovative in the development of products and process improvements,4 andmore flexible and thus more competitive in terms of the ability to produce small quantities. all these factors mayexplain the shift to a smaller average plant size.5 the claims to new job creation are open to questionšsmesalso exhibit high failure rates, and thus new jobs may not be longlived.6 however, others point out that smallfirms will continue to add jobs because much growth will take place in industries in which small businesses haverelative advantages.7there is no question that smes are a crucial element in the nation's manufacturing base. if one believes, asmany do, that manufacturing must continue to be a foundation for u.s. economic competitiveness,8 then smeswill continue to be a crucial part of this competitiveness.9 the role for small firms appears to be increasing; thereis evidence of a trend toward more of the total production coming from smaller manufacturers.10 however, theunited states is lagging behind europe and japan, where small firms account for 45 to 60 percent ofmanufacturing employment.11smes and the niineither the global competitiveness of u.s. industry nor the future role of smes is assured. the nii visionof preserving the tradition of free market competition both among manufacturing suppliers and amonginternational companies is consistent with what porter12 suggests are the conditions for global competitiveness:demanding customers who can choose from an intensely competitive local network of suppliers.the nii and dii are expected to enable this competition and the development of dual use processes anddesigns. large manufacturers (including department of defense purchasers and contractors) can make theirspecifications available online, eliminating distribution delays and increasing the scope of distribution. in onevision currently being articulated by the u.s. department of defense (dod), the nii and dii enable the creationof an integrated data environment (ide) in which information itself (e.g., designs, production methods)becomes a commodity and is traded. with information available on both specifications and designs, firms canwork only on those opportunities for which they are most capable, reducing the risk and costs of bidding onmarginal opportunities.for smes to participate, they must have access to the nii and they must be able to use computer technologyto integrate their business and technical functions. they must understand and use electronic commercetechnologies (ects). currently, small businesses are not utilizing computers to the degree necessary to fullyparticipate. a recent survey commissioned by ibm indicated that while 82 percent of small businesses (not justmanufacturers) had desktop computers, only 37 percent had local or wide area networks.13 in a stage model ofinformation technology maturity,14 almost twothirds of these respondents would fall into the first, mostelementary, stage of maturity.smes are becoming aware of the need to adopt some form of electronic communications. with increasingfrequency, prime contractors and large firms have demanded that their suppliers have electronicsmall manufacturing enterprises and the national information infrastructure352the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.capabilities. as one would expect, this has heightened interest in electronic commerce capabilities among smalland mediumsized businesses. the interest is likely to escalate. one software vendor executive, explaining thathis company was trying to respond to customers' needs for advice and consultation about electronic commerce,put it this way, "our executives have been around long enough to tell the difference between a ripple and a wave.this one is a wave."15engineers from the cleveland electronic commerce resource center (cleveland ecrc) report similarinterest but observe that some small firms are satisfied with a "rip and read" solution: they link into a bulletinboard or valueadded network with a personal computer but use the computer as an expensive fax machine. they"rip off the printed specifications," then read them and insert them into their manual system.16 this approachworks for written specs and to some degree for drawings, but it clearly is limiting. more advanced firms install acomputer aided design (cad) system to enable them to accept design data in digital formats. often, they toohave a manual internal system and do not attempt to use the digitally stored format.compounding the technical problem is the lack of a single standard that is widely accepted; chrysler, ford,and gm use different, incompatible cad systems. for most smes, the cost of implementing multiple standardsis too high, and they either choose a single customer's standard or opt for another market. in either case, thesituation does not lead to increased competition and to the increased competitiveness of smes. a single standardwould help. standards such as pdes/step are being developed, but agreements and adoption take time, andsuch standards address primarily the technical issues of data sharing.organizational (i.e., managerial and cultural) issues are equal to, if not greater than, technical capabilities inimportance. in their discussion of agile manufacturing, goldman and nagel17 share the vision of integration ofvirtual enterprises through the use of information technology, including standards and "broadbandcommunications channels."18 they acknowledge the need for flexible production machinery but point out theneed for organizational innovations as well. the agile system they envision requires flexible production workersand managers, not just technology. getting the integration of technology and people into a new, responsivesystem is a challenge. they conclude, "an understanding of the nature of managerial decisionmaking is moreimportant than ever before."19other researchers agree with goldman and nagel that the managerial, organizational, and cultural issues areat least equal in importance to the technical challenges of tapping into the benefits of the nii. in a field study offive large firms that were judged to be implementing integrated information systems successfully, a study teamfound six shared characteristics among the firms, and only one (the goal of capturing all data in electronic format its origin) was technical.20 the other five characteristics (vision and clear strategy, vocabulary/languageincorporating metrics shared by technical and business staff members, customer focus, and a sense of urgency)were organizational factors.factors affecting smes' adoption of technologyone approach to understanding smes' use of information technology would be to view ect as atechnology that will be diffused throughout manufacturing. this diffusion approach21 uses the familiar scurveto identify the percent of smes that have adopted ects over time. factors associated with an individual firm'spropensity to adopt technology might suggest strategies for working with innovators, early adopters, and so on.22implications of this type of model for policy are discussed further in the final section.another useful approach, the one taken for the remainder of this paper, is to seek an understanding of thedecisionmaking process within the sme. from this viewpoint, we may gain some insight into the economic,technical, structural, and other barriers to adoption as seen by the sme.the stage model suggested by venkatraman23 of firms' use of information technology (figure 1 shows anadaptation of this model) is used as a basis for identifying the gap between the "as is" state and the "desired" (or''to be") state of sme capabilities.small manufacturing enterprises and the national information infrastructure353the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 stage model of information systems use. source: adapted from venkatraman, n., 1994, "itenabled business transformation: from automation to business scoperedefinition," sloan management review, winter, pp. 73œ87.small manufacturing enterprises and the national information infrastructure354the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the data from the ibm survey of small businesses24 indicate that almost twothirds of the surveyrespondents are in the first stage of maturity in applying information technology. virtually none of them haveprogressed beyond the second stage, and there is no assurance that they will go beyond this stage. for smes tobenefit from the nii, they must be at level 3 or above, developing capabilities for network/supply chainintegration. although the ibm survey was not limited to manufacturing firms, our experience with smes leadsus to speculate that small service firms and those in the retailing and trade sectors may use computers even morethan manufacturers, lowering even further the estimate of how many smes have moved beyond the first stage ofcomputer use.this stage model is descriptive, and it only indirectly suggests how an organization moves from one stageto another. our concern is to understand how the organization, particularly an sme, progresses from theapplications of isolated systems to network and supply chain integration and, more importantly, how this processcan be accelerated. the relevant fields of research are those of technology policy, innovation adoption, thedecisionmaking process within the firm, and the emerging field of inquiry on organizational learning.the concept of organizational learning,25 particularly the use of the human experiential learning modelproposed by david kolb26 and recently applied to organizations,27 provides a useful framework to interpret thefindings from the other fields. this model, shown in figure 2, illustrates the different modes by which anindividual (organization) learns. learning takes place in two dimensions: in the concreteabstract dimension(shown vertically as a continuum) and in the activereflective dimension (shown horizontally). individuals (andorganizations) have different preferences for learning and processing information in these dimensions.28 someprefer more concrete and active learning (e.g., entrepreneurs); others prefer more abstract and reflective learning(e.g., professors).figure 2 experiential learning cycle. source: reprinted from kolb, david, 1984, experiential learning:experience as the source of learning and development, prenticehall, englewood cliffs, n.j.the learning cycle model suggests that only when the organization goes through all four modes is learningcomplete. for example, a firm may introduce a new process for a customer or product line (activeexperimentation), collect sales and quality data over time (concrete experience), interpret these data and comparewith prior experience (reflective observation), and develop a projection of sales and costs of quality if the newprocess were applied to all their product lines or to all their customers (abstract conceptualization). based on themodel, the firm may choose to switch its other products to the new process, again moving to activeexperimentation andsmall manufacturing enterprises and the national information infrastructure355the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.restarting the cycle. by passing through each of the learning modes, the firm generates new knowledge. the firmlearns, and the learning is not limited to the simple aggregation of additional data or to thinking about a new ideašthe cycle is complete.using the concept of the learning cycle, we can frame our concern as that of understanding the predominantlearning modes of smes and of understanding how smes can incorporate all learning modes in their progresstoward the higher stages of information technology maturity. for this understanding, we can draw on severalareas of research about how organizations adopt technology. in each of the relevant areas, it is evident that onemust use caution in applying concepts derived from the large organizational context to the sme.29 however,some studies have focused specifically on the decisionmaking and policy formulation in the small firm, and thesestudies are particularly helpful in our efforts to understand how to accelerate learning and ect adoption in smes.analysisthe first three subheads below outline relevant concepts from three distinctive but overlapping areas ofinquiry. each has its own literature base and each offers some insight into how firms, and smes in particular,may implement and use information technologies. the fourth subhead outlines the structural issues that mayinitially inhibit smes' effective participation in the nii. the section concludes with a synthesis of ideas abouthow smes may approach the adoption of electronic commerce technologies and realize the benefits from the nii.diffusion of technologythe diffusion literature30 characterizes the industry adoption of new products by an sshaped curve. thecurve reflects exponential growth with a rate that depends on the size of the remaining market. the diffusionmodel has been used with some success in technology forecasting. with good data on when a low level ofadoption has been achieved (e.g., 5 percent), the model is effective in identifying the dates by which a specificlevel of industry penetration (e.g., 50 percent) will occur.the scurve model is often used to identify firms according to when (early or late) they make the decisionto adopt the technology. the classifications may indicate different organizational characteristics. a modificationof this conceptual model31 classifies the "buyer profiles" as being one of five types: innovators, early adopters,early majority, late majority, and laggards.recent research32 tested the idea that psychological characteristics (e.g., attraction to technology, risktaking) rather than economic variables might be used to discern buyer profiles. the study found that the benefitcost variables were better predictors. although one could argue with how the variables were operationalized andwith the limits of the study (focus groups on a single product), the researchers' conclusion has face validity:companies that pioneer new products must focus on the benefits desired by purchasers. even the early adopters,who are less price sensitive, seek benefits that meet their needs better than current technologies. what is notdiscussed in the study is the changing nature of the benefits and costs with changes in the organizationalcharacteristics and with changes in risk as the technology matures.kelley and brooks33 also showed the predictive power of economic incentives in the diffusion of processinnovations. not surprisingly, firms with high wage rates were more likely to adopt laborsaving technologiesthan were firms with low wage rates. the key is to note that the benefits and costs are established by the firms'sperceptions; these perceptions are affected by the organizational values and the firm's particular situation.as noted by schroeder,34 the survival of an sme is linked to the adoption of technology as a regular part ofdoing business. if it is in the nation's interest for smes to thrive, then the diffusion issue is how to accelerate theadoption of information technologies among smes. the diffusion model may be a useful metric by which wecan track and predict adoption rates as early data become available. however, the diffusion model does not helpexplain how firmlevel decisions are made. concepts that examine how the individual firm makes a technologyadoption decision may be more informative in the early development of the nii.small manufacturing enterprises and the national information infrastructure356the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.sme decisionmakingthe literatures relevant to an sme's decisions on technology adoption are those on corporate strategy,technology strategy, technology policy, information system implementation and planning, strategic informationsystems, and investment decisionmaking at the level of the firm. these areas of study are rich in topics that arerelevant to technology adoption, but the focus on smes and their adoption of technical innovations reduces thescope considerably.smes differ from large companies in how they develop their corporate strategies and their technologypolicies. large companies typically have welldefined processes for developing and implementing strategiesthrough a corporate planning process. small firms often use less structured approaches; strategies and policiesmay not be formulated but may "emerge" from a set of actions and experiments.35in an sme, the chief executive officer (ceo) often is onešor perhaps thešowner of the firm. in thesefirms, the ceo's viewpoint is a critical contributor to strategy and policy. a recent study of smes36 showed thatimplemented technology policies (not just written policies) in smes are strongly influenced by how the ceoperceives the world. even though all the firms in the study were immersed in the same industrial setting in thesame canadian province, the ceos differed in their view of how hostile and how dynamic their environmentwas. the firms' propensity to invest in new technology was strongly related to these views. the basis fordecisions is not an objective reality but rather a socially constructed reality37 as reflected in the viewpoint of theceo.the social construction of the adoption decision by a firm has other participants as well. for the sme, astrong influence is the supplier, who may be a major source of information.38the innovativeness of an sme is related to the firm's outward orientation (e.g., customer focus) and theparticipation of the firm's functional groups in the decision.39 there is evidence40 that the sme learns withincreasing technological capabilities so that, over time, its decisionmaking places more weight on factors that aremore closely related to the true potential of the technology.sme learningarrow41 noted that firms learn through experience. this learning normally is considered to be related toprocess improvements and is the foundation for the concept of reduced costs over time because of "the learningcurve." more advanced technologies may have greater productive potential, but the firm has less expertise inimplementing such technologies. knowing it has less expertise, the firm expects greater costs. the firm thusfaces a tradeoff in its choices of technologies to adopt.42the capacity for learning affects the rate of adoption of new technology. firms that have existingtechnological capabilities have higher "absorptive capacity"43 for new technology; they are able to learn morequickly.44a firm's installed technology also affects the extent and magnitude of benefits the firm experiences frominstalling new systems. firms that have more existing technological capabilitiesšfor example, firms that haveimplemented information technologies in both the administrative and engineering/production operationsšenjoybenefits that are greater than the sum of the benefits from individual systems. there is synergy and, because ofthe added benefits and increased capacity for learning, the "rich get richer" and vice versa. this appears to be thecase both for large firms45 and for smes.46when a technology is new to an industryšbefore its technical and economic superiority has been widelyacceptedšthe learning capacity of a small firm is related to the firm's linkages with other firms and otherindustrial organizations. these external linkages, many of which provide informal but trusted conduits forsharing of technical knowhow, appear to lower the cost of learning for the firm. kelley and brooks put it thisway: "small firms' propensity to adopt a process innovation is particularly enhanced by the nature of linkages toexternal resources for learning about technological development.– where linkages to such external learningopportunities are particularly welldeveloped we would expect to find a more rapid rate of diffusion ofproductivityenhancing process innovations to small firms."47small manufacturing enterprises and the national information infrastructure357the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.organizational learning may be "singleloop" or "doubleloop."48 in singleloop learning, the organizationimproves its efficiency, becoming ever better at dealing with a prescribed problem or environmental situation.the lowering of costs because of the ''learning curve" is an example of singleloop learning. doublelooplearning, by contrast, is characterized by a shift in viewpoint and a modification of basic premises. doublelooplearning requires unlearning prior assumptions and standard operating procedures; it involves developing newparadigms, new frames of reference, and new interpretive schemes. singleloop learning reduces variability;doubleloop learning increases variability in search of more relevant objectives or more effective strategies.because prior procedures and paradigms have a history of success, organizations have difficulty engagingin doubleloop learning; they actively resist.49 however, dynamic and turbulent environments demand that firmsexhibit more variability in order to meet changing needs. one approach to stimulating variabilityšand possiblydoubleloop learningšis organizational restructuring. restructuring (changing the top management team and/orthe ceo) is especially effective when combined with a change in strategy (e.g., new products or markets).50smes, especially the smaller ones, are less likely to adopt a restructuring approach. a turbulentenvironment sometimes stimulates an sme owner to sell or merge with a larger firm. often, however, the smethat cannot adapt quickly enough to environmental changes simply ceases to exist. the latter outcomecontributes to the statistics used by those who argue that smes provide unstable employment, even if they docreate a significant portion of new jobs.the learning model in figure 2 provides a framework that helps synthesize these issues. since completelearning means that the organization engages in each of the modes, an enterprise may engage in formal orinformal collaboration with external organizations to learn. for example, the motivation for close alliancesbetween suppliers and manufacturers51 is partially explained by the benefits of learning, and the higher rate ofinnovation adoption because of external contacts52 may be due to the expanded learning modes made possible bythese contacts.an alternative to restructuring or going out of business is to establish and maintain external relationshipsthat enable learning. such organizations, which "bridge"53 sources of knowledge about new technologies (e.g.,universities) and the smes (as potential users), have been stimulated by federal and statelevel programs thathave set up technology transfer centers and assistance networks. ohio's thomas edison technology centers, thefederally funded manufacturing technology centers (mtcs), and, most appropriately, the federally fundedelectronic commerce resource centers (ecrcs) are examples of such bridging organizations.the value of such organizations was set forth over a decade ago by trist,54 who noted that complexsocieties and rapidly changing environments give rise to "meta problems" that a single organization is unable tosolve. the solution is the development of "referent organizations" that mediate the interorganizationalcollaboration required in the organizational domain of interest.although detailed studies of the effectiveness of mtcs and ecrcs are premature given their recentformation, the political judgment seems to be that they are effective.55 studies of ohio's thomas edisontechnology centers generally have praised their value and effectiveness.56 one of the challenges noted is that of"relationshipbuilding."57 there is the explicit acknowledgment that the relationships and the process oftechnology solving are equal to, if not greater than, the importance of developing the technology itself. theseevaluations appear to support the concept that the bridging, or referent, organizations contribute to learning, andthat at least part of the new knowledge created is not migratory knowledge but is embedded in the relationshipsthat are established and maintained.58 implicit in the mt. auburn report is the notion that the relationshipbuilding role of these organizations is underdeveloped.the structural issue: of what benefit are a few telephones?the current status of electronic commerce technology may be similar to that of the early telephone. imaginebeing given the opportunity of purchasing the third or fourth (or even the fiftieth) telephone: unless you areassured that the other people (organizations) with whom you want to talk (trade/communicate) are equipped withcompatible technology, the benefits are nil. unless the advanced technology has its own appeal, a prudentbusiness decision is to "wait and see"šwait until there is a critical mass of manufacturers and suppliers withsmall manufacturing enterprises and the national information infrastructure358the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.whom you can beneficially communicate. except for the innovators and early adopters, most of the potentialsme users of ectsšif they are aware at all of the nii and its electronic commerce benefitsšare likely to thinkof these as something that may be possible in the future.one approach to the structural barrier to the diffusion of ects is to think of the smes in clusters59 thatshare a characteristic or interest. geographic clusters exhibit their own rate of technology diffusion that can beenhanced by bridging and referent organizations in those regions.60 other clusters that share other interests (e.g.,those firms in a supply chain) may be distributed geographically.for industries in which the technology is relatively stable (e.g., automobile manufacturing) compared withthe dynamism of emerging technologies (e.g., biotechnology), the shared interests of the supply chain maymotivate groups of firms to adopt ect more quickly. although the relationships of suppliers to themanufacturers has become closer over the past several years, there still are no widely accepted technicalstandards, nor are there any established social mechanisms for engaging in collaborative efforts.summary: the key conceptsthe literatures related to sme adoption of information technologies may be summarized in six key points:1. business success and implementation of new technology appear to be related.2. smes decide to adopt new technologies based on perceived benefits and costs.3. smes perceive but may not articulate costs of learning to use and integrate new informationtechnologies. this makes evolutionary changes seem less costly and less risky than revolutionary ones.4. smes appear to follow a stage model in implementing information technologies from simple, standaloneapplications to more complex and integrated applications. at the highest level, information technologybecomes a component in the strategic definition of the business. there is no evidence (except forgreenfield operations) that a firm skips levels, but the movement from level to level may be accelerated.5. smes' benefits from applications of information technologies are cumulative and synergistic, withdisproportionally greater benefits as the number of applications (and enterprise integration) increases.6. smes' learning and adoption of new technologies are related to the number and quality of interorganizational relationships in which they are active.implications for smes and the niithe potential benefits of the nii to smes go much beyond simple manufacturing process improvements. ifsmes are to realize the full benefits of the nii, they must advance their level of information technologyapplications to levels 3 and 4 in the stage model shown in figure 1.once at these levels, manufacturing costs may become loweršfor example, firms can more readilyspecialize and develop core competencies in particular processes. other benefits, however, contribute to theoverall lower costs: shorter administrative lead times, improved risk management through better informationabout future demands, more flexible (agile) production, and so on. these benefits do not arise because a singlefirm or even a few firms adopt ect; they will be realized only if a critical mass of firms in a value chain becomeinterconnected.the nii is the key element in this interconnection; it is the communications backbone. even with thebackbone, interconnections are not assured. the problem is not merely one of enabling individual firms to adoptect; it is one of enabling groups of firms to adopt ect. this framing of the problem is more than just a changein scale (from one to many); it is a major change in scope and may add significantly to the complexity of thesolution. as a minimum, it changes how we approach accelerated learning and adoption of ect in smes.small manufacturing enterprises and the national information infrastructure359the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the sme technology adoption process, as studied by most researchers and as understood today in theunited states, presumes independence among the adopters. however, interdependence, not independence, isnecessary if the full economic benefits of the nii are to be realized.this requires cultural changes in smes, and the rate at which smes change their cultures can be expectedto dominate the rate of diffusion of the technology itself (including ects) among smes. firms that traditionallyhave viewed the world through lenses of competition as a zerosum game now must view competition as apositive sum game: competition as a means of benchmarking and improving one's own performance (e.g., as inorganized sports, such as the olympics). in such a view, technological advances by other firms provide alearning opportunity for their own firm.recommendationsa strategy for setting priorities for nii services to smessmes, perhaps more than larger firms, have fewer options for secondorder learning. for most smes,moving to the higher levels of information technology maturityšthose levels required for electronic commerceand for realizing the greatest benefits from the niišwill be possible only by evolutionary change. the servicesavailable over the nii are expected to be offered by a mix of private, notforprofit, and government providers.to enable smes to benefit from the nii, these providers, to the extent possible, should:1. give early priority to encouraging and establishing highvalue, lowcost services that smes can use asindividual firms.rationale: most smes are at the lowest level of information technology maturity. they will perceivethe highest costs (including learning) to be for services that require additional technology and integration.if individual firms can learn to use networks to obtain valued information from readonly services or forsimple firmfirm communication (e.g., email), the cultural change is evolutionary and the perceivedsubsequent costs of moving to more integrated levels will be lower.2. match the services offered to the information technology maturity level of the early adopters (e.g., themost advanced 10 percent) of smes.rationale: the perceived costs of moving more than one level make the benefits of adopting a newtechnology seem "out of reach"; setting the most advanced services just above the capabilities of the earlymajority balances the need for smes to see the possibilities of greater additional benefits with affordablecosts of organizational change.a strategy for publicprivateuniversity partnershipsmuch of the technology will be developed and made available from the private sector. moreover, thefederal government is expected to continue to help establish and encourage the widespread acceptance ofinternational standards for ect. as established by the summary of research in this paper, the rate at which smesadopt ects (and benefit from the nii and dii) is dominated by organizational issues rather than purely technicalfactors. consequently, the following paragraphs outline highleverage opportunities for the federal governmentto improve the capabilities of existing public and partnership programs to address these issues.in particular, dod and the department of commerce programs such as the manufacturing technologycenters (mtcs), electronic commerce resource centers (ecrcs), and manufacturing learning centers(mlcs) provide an appropriate infrastructure for accelerating the changes required in smes. these programscomprise geographically distributed networks of centers through which smes can receive assistance. from allappearances, these programs are performing their perceived missions successfully and satisfying theirconstituents. however, there are opportunities to expand these perceived missions and to accelerate the learningsmall manufacturing enterprises and the national information infrastructure360the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.and development of smes as participants in the nii. the following is a recommended strategy for expandingthese roles:1. the doc and dod should sponsor a series of workshops on sme technology adoption that involves thecenters (either across programs or within each program), researchers in sme technology adoption, andsme leaders (and perhaps prime manufacturers) in a particular industry or value chain cluster. theobjective of the workshops is to enable the centers to visualize opportunities for additionalcomplementary efforts that would enable the smes to develop more quickly.rationale: these centers should be the leaders in proposing to expand their missions, but they first needto understand the potential roles. although many of the centers have universities as partners, thepartnership may be limited to technological topics. research results on technology adoption and themanagement of technology, especially in smes, may not be a normal part of the learning environment forthe center staffs.possible outcomes: expanded roles would emerge, for example, if these programs viewed their centersas referent organizations, operating in a particular interorganizational domain. in addition to the emphasison technology awareness and technical training that the centers provide to smes, the centers could identifyopportunities for sme learning that goes beyond the migratory knowledge that an sme can acquirethrough normal classroom environments or even shortterm consulting arrangements. in particular, thecenter's mission might include convening, facilitating, and maintaining interorganizational consortia thatare focused on particular issues. (the ecrcs are doing this to an extent now with regional interest groups.)2. the dod and doc programs, where appropriate, should expand their centers' linkages with educationalinstitutions, including public libraries.rationale: research universities (business schools and universities with management of technologyprograms) can assist the centers in understanding sme adoption of technology and benefit from thecenters' experiences. the centers could establish and support ect facilities in community colleges andpublic libraries, enabling smes to have access to bid information and other emerging nii services untilthey are prepared to invest in their own facilities.references and notes1. acs, z. 1992. "small business economics: a global perspective," challenge 35(6):38œ44; u.s. small business administration. 1993.the annual report on small business and competition, u.s. small business administration, p. 49.2. u.s. small business administration, 1993, op. cit.3. long, andrea l. 1984. "net job generation by size of firm: a demographic analysis of employer eventhistories," a report to theu.s. small business administration office of advocacy, july 12.4. acs, 1992, op. cit.; and acs, z. 1994. "where new things come from," inc 16(5):29.5. carlsson, bo, and david b. audretschlogy. n.d. "plant size in u.s. manufacturing and metalworking industries," internationaljournal of industrial organization 12(3):359œ372.6. davis, steward, john haltiwanger, and scott schuh. 1994. "small business and job creation: dissecting the myth and reassessingthe facts," business economics, july 29(3):13œ21.7. asquith, david, and j. fred weston. 1994. "small business growth patterns and jobs," business economics 29(3):31œ34.8. wheelwright, steven c., and robert h. hayes. 1985. "competing through manufacturing," harvard business review 63:99œ109; andwheelwright, steven c. 1985. "restoring the competitive edge in u.s. manufacturing," calif. management review 27(3):26œ42.9. united states congress. 1992. small business manufacturing and work force capability, hearing before the subcommittee ontechnology and competitiveness of the committee on science, space, and technology, u.s. house of representatives, 102d congress,second session, washington, march 9.10. carlsson, b. 1994. "flexible technology and plant size in u.s. manufacturing and metalworking industries," international journalof industrial organization 76(2):359œ372.11. acs, 1992, op. cit.small manufacturing enterprises and the national information infrastructure361the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.12. porter, michael. 1990. "competitive advantage of nations," harvard business review, marchœapril, pp. 73œ79.13. mangelsdorf, martha e. 1994a. "technology management in growth companies," inc 16(14):140.14. nolan r. 1979. "managing the crises in data processing," harvard business review 57(2):115œ126; venkatraman, n. 1994. "itenabled business transformation: from automation to business scope redefinition," sloan management review, winter, pp. 73œ87.15. personal communication with henry nelson, harris data, april 1995.16. personal communication with mike kolbe, cleveland ecrc, march 1995.17. goldman, steven l., and roger n. nagel. 1993. "management, technology, and agility: the emergence of a new era inmanufacturing," interscience enterprises ltd. 8(1/2):18œ37.18. ibid., p. 29.19. ibid., p. 36.20. mason, robert m., a. thomson, and h. nelson. 1993. "implementation of integrated information systems from a businessperspective," final report on a study performed for the cals program office, air force materiel command, wpafb, ohio 45431. seealso mason, r.m., and h. nelson. 1993. "implementation of integrated information systems: comparisons of field studies and theliterature," proceedings of the 27th hawaii international conference on system sciences. computer society press, los alamitos, calif.,pp. 987œ997.21. rogers, e. 1983. diffusion of innovations. free press, new york.22. ibid.23. venkatraman, op. cit.24. mangelsdorf, 1994a, and mangelsdorf, martha e. 1994b. "smallcompany technology use," inc 16(12):141.25. senge, peter m. 1990. "the leader's new work: building learning organizations," sloan management review, fall, pp. 7œ23.26. kolb, david. 1984. experiential learning: experience as the source of learning and development. prenticehall, englewood cliffs,new jersey.27. dixon, nancy. 1994. the organizational learning cycle. mcgrawhill, london.28. kolb, op. cit.29. romano, claudio a. 1990. "identifying factors which influence product innovation: a case study approach," journal ofmanagement studies 27(1):76.30. rogers, op. cit.31. moore, g. 1991. crossing the chasm. harper business, new york.32. taylor, james r., eric g. moore, and edwin j. amonsen. 1994. "profiling technology diffusion categories: empirical tests of twomodels," journal of business research 31(1):155œ162.33. kelley, maryellen r., and harvey brooks. 1991. "external learning opportunities and the diffusion of process innovations to smallfirms," technological forecasting and social change 39:103œ125.34. schroeder, dean m. 1989. "new technology and the small manufacturer: panacea or plague?," journal of small businessmanagement 27(3):1œ10.35. mintzberg, henry, and james a. waters. 1985. "of strategies, deliberate and emergent," strategic management journal 6:257œ272.36. lefebvre, louis a., r.m. mason, and e. lefebvre. n.d. "the influence prism in smes: the power of ceos' perceptions ontechnology policy and its organizational impacts," submitted and under revision for management science.37. berger, peter l., and thomas luckman. 1966. the social construction of reality. doubleday, garden city, new york.38. preece, david a. 1991. "the whys and wherefores of new technology adoption," management decision 29(1):53œ58.39. lefebvre, l.a., j. harvey, and e. lefebvre. 1991. "technological experience and the technology adoption decisions of smallmanufacturing firms," r&d management 21:241œ249.40. ibid.41. arrow, k. 1962. "the economic implications of learning by doing," review of economic studies 29:155œ173.42. parente, stephen l. 1994. "technology adoption, learningbydoing, and economic growth," journal of economic theory 63:346œ369.43. cohen, wesley m. 1990. "absorptive capacity: a new perspective on learning and innovation," administrative science quarterly35(1):128œ152.small manufacturing enterprises and the national information infrastructure362the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.44. kelley and brooks, op. cit.45. virany, beverly, michael l. tushman, and elaine romanelli. 1992. "executive succession and organization outcomes in turbulentenvironments: an organizational learning approach," organization science 3(1):72œ91.46. lefebvre, e., louis a. lefebvre, and mariejosee roy. 1995. "technological penetration and cumulative benefits in smes,"proceedings of the 28th hawaii international conference on systems sciences, vol. iii . computer society press, los alamitos, calif.,pp. 533œ541.47. kelley and brooks, op. cit.48. argyris, c., and d. schon. 1978. organizational learning: a theory of action approach. addison wesley, reading, mass.49. argyris, chris. 1977. "double loop learning in organizations," harvard business review, september/october, pp. 115œ125.50. virany et al., op. cit.51. helper, susan. 1991. "strategy and irreversibility in supplier relations: the case of the u.s. automotive industry," businesshistory review 65 (winter):781œ824; and helper, susan, and david hochfelder. 1992. "japanese style supplier relationships in theu.s. automobile industry: 1895œ1920," chapter in forthcoming book edited by june akudo for oxford university press.52. kelley and brooks, op. cit.53. westley, frances, and harrie vredenburg. 1991. "strategic bridging: the collaboration between environmentalists and business inthe marketing of green products," journal of applied behavioral science 27(1):65œ90.54. trist, eric. 1983. "referent organizations and the development of interorganizational domains," human relations 36(1):269œ284.55. a "backoftheenvelope" calculation shows why. if we posit that smes employ approximately 6.9 million people and that twothirds of them are employed at smes that are incapable of engaging in electronic commerce today (as noted by the survey cited above),we may assume that these employees are "at risk" of being dislocated or losing their jobs if their firm becomes noncompetitive. if wefurther posit that the ecrcs and mtcs enable only 10 percent of the employees who otherwise would be dislocated to keep working,and that each one earns at least $20,000 annually, then the potential savings from unemployment payments alone (onehalf of the annualsalary) approaches $4.6 billionšmaking the dod and doc programs seem like a bargain.56. mt. auburn associates. 1992. "an evaluation of ohio's thomas edison technology centers"; and commission on engineering andtechnical systems, national research council (nrc). 1990. ohio's thomas edison centers: a 1990 review. national academy press,washington, d.c.57. nrc, op. cit.58. badaracco, jr., joseph l. 1991. the knowledge link. harvard business school press, boston, mass.59. porter, op. cit.; and fogarty, michael s., and jarchi lee. 1992. "manufacturing industry clusters as a logical way to structuretechnology deployment," center for regional economic issues report, case western reserve university, cleveland, ohio.60. fogarty, op. cit.small manufacturing enterprises and the national information infrastructure363the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.43architecture for an emergency lane on the nii: crisisinformation managementlois clark mccoy and douglas gilliesnational institute for urban search and rescueandjohn harrald, niusr and george washington universitystatement of the problemresponses to disasters, both natural and mancaused, are usually multiorganization, multijurisdictionevents. this inherent organizational complexity compounds the chances for missed communications, inadequateintelligence regarding the event, and the consequent escalation of both the size and the cost in lives and dollars ofthat event. for their actions to be consistent, coordinated, and constructive, responders must have a commonunderstanding of the problems they faceša common "mental map." they must also be able to quickly create anorganization capable of meeting the disastercaused needs.the national information infrastructure (nii) with its new information technologies, properly applied, hasthe ability to greatly improve disaster preparation and response, thereby reducing the cost of these operations.the technology has the potential of supporting emergency managers in four critical areas: (1) supporting crisisdecision making, (2) managing information overload, (3) supporting first responders, and (4) capturing vitalinformation at its source. within this paper we suggest a strategy to build a command and control, computing,communications, and intelligence (c4i) system for civilian crisis management. it will bring with it the new c4idoctrine, new emergency organizational networks, and technological standards to ensure seamless interlinkingand interoperability. the proposed architecture for the nii that will provide an emergency lane on theinformation highway is developed in the recommendations section of this paper.the implementation of the new technology of information must be combined with a sober realization thatimmense cultural changes will occur within the emergency management community. if we focus on only thetechnological aspects of this change and do not consider the users' need to feel in control of the new technology,adequate assimilation of the great benefits to be derived from it could be delayed by years.state of present playthe field of the civil emergency manager is particularly caught up in this technological and cultural change.at the same time that emergencies, disasters, and catastrophes seem to be escalating by the month, theemergency manager is caught between the old and the new. traditionally, the field of emergency managementoperated in an area of scarce information about the event, whether hurricane, riot, or chemical spill. today,emergency managers are overwhelmed with a glut of information. suddenly, they have neither the training northe tools to handle this level of data. a wellknown paradox of information management is that decisionmakerswith too much information act just like decisionmakers with too little informationšthey make decisions basedon their personal experience and expertise, not on an analysis of the current situation. the technology providesno added value if all it does is provide additional information to an already overwhelmed decisionmaker. in theface of this rapid change, emergency response and mitigation have lagged behind the general rate of acceptanceof the new information technology. now, with the support of the clinton administration and the revitalizedfederal emergency management agency, it is full steam ahead. however, the difficulties involved with anydramatic change still remain.architecture for an emergency lane on the nii: crisis information management364the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.we, living and working at the change of the century, are the "'tween" generation. here is this newtechnology on the one handšnew, untrusted, and fearsome. on the other hand, there is our intuitive judgmentdeveloped through hardwon experience. we are indeed caught on the horns of a dilemma.history during other times of great change may have a lot to show us about a saner and more harmoniousway to survive this frenzied fin de siècle we are thrashing through. it is a misconception that the average personis threatened by change. people are threatened only by changes over which they believe they have no control!emergency managers are singularly beset by this phenomenon as they are encouraged to adopt newtechnologies and methods. most of these managers have had long experience in the field of disaster relief andrely on this knowledge as the basis on which they make their decisions. the more competent the commander andthe more complete the knowledge, the better the decision.but now managers are confronted with a new 21stcentury technologyšcommand, control, computing,communications, and intelligence (c4i) systems. these systems can provide great support to the emergencymanager. they can provide information as concise knowledge that can be quickly understood. they caninterpret, filter, and correlate knowledge for making rapid projections and estimates. this is a completely newenvironment for the emergency manager. where he or she once operated (and had learned to be comfortablewith) uncertainty, now the environment is filled with technologically derived knowledge. knowledge possessedby experts not at the scene can be made available to the emergency manager either through remotecommunications or by the use of expert systems. that is a monumental change. it brings with it the oftenheardphrase, "that's not how we do it here!" an important part of this rejection of new ways and new tools is a lack oftrust. the emergency manager has no sense that he or she owns them and does not feel in control of thetechnology or of the information it provides.analysis of factors influencing the realization of an emergencylane on the niiafter a 3year program of "town meetings," the national institute for urban search and rescue (niusr)has identified the needs for newage, interlinking communications in timesensitive arenas.a most useful tool in overcoming the lack of trust in new information technology is the use of simulationsin exercises and demonstrations. experience is a great teacher, but where in the real world can one gain thatexperience in a relatively riskfree environment? in the field of emergency response, too many lives are often injeopardy to risk trying out new technology in real circumstances. under stress, emergency managers, even afterbuying and installing new technology, will tend to revert to their experience and intuitive judgment, which arecomfortably familiar and have served them well in the past. one of the statements heard often from the observersin the followup days of recovery from the northridge earthquake was, "we never saw so many computers liningthe walls with no one using the information." in times of great stress, we all return to the known ways ofmanagement, command, and control.the new technology continues to become more easily usedšand more "fun" to learn. the development ofthe world wide web is only the beginning of an evolution that promises to provide a bridge into the newtechnology for hardwon experience and intuitive judgment. the seasoned emergency manager now has a toolthat lets him or her feel in control. the manager can "see" the links into the information and ask for the piece ofinformation wanted, rather than shifting through reams of data spewing out of a machine. and, most importantly,the manager can see this information displayed in a visual "picture" that allows him or her to quickly assess thesituation. the manager has grasped the knowledge within the data.the first step in achieving an effective conversion of raw data to knowledge is to import that information ina form that is easy to use. "a picture is worth a thousand words" because of the comparable speed with which thebrain assimilates one printed word and one picture. with all the picture's background and emphasis, the brainperceives a much greater range of data in microseconds.the world wide web enables the emergency manager to (1) choose the information wanted, and (2) see itdisplayed on an interactive graphic representation as a picture of the information in the setting of the emergency.to this, add the preevent simulation training that permits the emergency manager to trust the information beingarchitecture for an emergency lane on the nii: crisis information management365the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.delivered. at last we begin to convince the experienced, knowledgeable commander that this ability to controland respond to the emergency has been improved to an exponential degree through the sophisticated use ofinformation technology. no longer are we using new technology to merely pave over the cow path (icma,1994). we have moved into another realm of control. we own the future and it is ours.the barrier for cultural changeone of the idiosyncrasies of the civilian emergency manager is that both of the words "command" and"control" are forbidden. they send up red flags on the site of any emergency. who's in charge? has always beena poorly understood question within civil authority. "battles" can start on main street over this question duringthe disaster itself. so the emergency management community has elected to hide behind the word "coordinate".this euphemism must be discarded. this doublespeak must be reversed. someone has to bite the bullet and be incharge! lives are at stake, resources must be expended in increasing amounts, dollars must be spent. commandand control must be present at the disaster scene. too often, civilian response is a matter of individual heroismšemergency responders, both paid and volunteer, performing over the top in their noble efforts immediatelyfollowing the event. uncertainty is the environment of any emergency. the longer this uncertainty remains, thehigher the cost in lives lost, property damage, and recovery costs.obviously the emergency coordinator must assume command and control. the emergency coordinationmay be a multilevel, joint command, or a unified command with one spokesman. this type of arrangement mustbe practiced, simulated, and tested to be quickly effective. one timely adjunct for the civil emergencyenvironment was the adoption, in november 1994, of a national interagency command system (ics) for themultiagency response to largescale disasters. the national interagency command system is built on theoriginal firescope and national interagency incident fire control models of the old boise interagency firecontrol center. it enables the user to limit the uncertainty of the disaster scene quickly. after all, the decidingfactor in the successful control of any emergency has always been time. among other pulses, the newinformation technology provides the huge advantage of shortened time frames for the development of realinformation the commander may use in exercising his or her knowledge and judgment. uncertainty will alwaysbe a factor in any response to emergencies. if we have complete control and adequate resources to respond, it isnot a crisis! it is merely business as usual for the emergency provider. when there is a lack of trustworthyinformation and a scarcity of resources, combined with a lack of control, then the true hazards of the emergencyenvironment appear.one of the continuing problems in the response to any large, multiagency effort has remained thecoordination and movement of resources between and among the various responders. the vertical coordinationamong like agencies (for example, among law enforcement agencies) may proceed with a degree of smoothness.likewise among the responding fire agencies, coordination tends to remain cooperative and responsive.however, when different agencies such as public works or transportation or social services are brought into themultilevel response, attempts at coordination among dissimilar modes of communication and departmentalmethods of operation lead to confusion and incompatibility. further complicating coordination, government andvolunteer and nonprofit organizations have different styles of operation. the american red cross and thesalvation army have key roles to play in disaster response. in addition to these formal organizations whosepresence is anticipated, it is a welldocumented phenomenon that ad hoc groups emerge to deal withunanticipated needs during the disaster response. these organizations do not all have to be tightly linked, buttheir actions must be consistently coordinated if the needs of the victims are to be met. the inability tocoordinate policy, logistics, and operations escalates again when multiple agencies from different jurisdictionsand levels of government are involved. the fractionalization of coordinated response is even further adverselyaffected when military assistance and its chain of command are requested by civil authority, and yet anothersystem is mixed into the command equation.architecture for an emergency lane on the nii: crisis information management366the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.forecast for the next five to seven yearsa 5 to 7year forecast is a difficult (and perhaps foolish) undertaking. we do have some current trends,however, that indicate what this future might be. the internet is growing at 15 percent a month. the world wideweb is doubling every 53 days. we will take a look at the future, but the traditional rules for predicting futuretime lines may be another victim of the rate of change in information systems.one unexpected and highly beneficial spinoff from the cross communication possible on the internet andthe world wide web has been the blurring of vertical authority and its consequent concerns for the protection of"turf." both the internet and the web operate across many platforms in a seamless fashion. no one may be surewhere or how most of the queries originated. within the parameters of authorized access, the information isavailable to all. interoperability may be the most beneficial of all the various aspects of the new informationtechnology. we have left behind a time when emergencies were synonymous with uncertainty. nowemergencies, and indeed, our daily business lives, are filled with a glut of information. the next bigbreakthrough in information management will be in software filters such as profilers. some even predict thatthere will be an entire new industry of personal media assistants who will filter your information, marking foryour attention only that in which you have indicated a major interest. for emergency managers, a system to filterthe data glut must have a real time component that may be better served by expert systems, or the reasoningsupport of knowledge robots ("knowbots"). such expert systems, simulations, artificial intelligence, intelligentdecision support systems, or even some other new tack may be the next exciting advance in the field ofinformation management.expert systems are currently pushing the edge of the technology. to date there have been some welldocumented failures of attempts at such systems. these failures fall into three classes. level one. errors of commission, in which human operators make a programmatic error of commissionštheold garbage in/garbage out problem. level two. the programmer forgets to input some data altogether and it goes unnoticed. an example wouldbe the gemini v space shot that landed 100 miles from where it was supposed to come down. someprogrammer forgot to input the motion of the earth around the sun into the reentry program. level three. this is the most difficult, and interesting, of all the sources of error. this is where a group ofparameters about the real world in which the program has to operate is input, and where, after theseparameters are input, they change. this has been called the "sheffield effect" after the british destroyer sunkin the battle with the argentineans. the ship's program had the exocet missile (developed by a nato ally,france) identified as friendly. however, in the arsenal of argentina, it was decidedly unfriendly, and it sankthe sheffield.current work on expert systems (not artificial intelligence) is wrestling with two interesting parameters.they are optimal ignorance and appropriate imprecision. not to delve too deeply into this subject here, let us justsay that these are ways to limit the amount of data to something that the machine can handle without thinkingitself into a corner and freezing up. the downside of this approach is that the expert system, in many ways, islike an idiot savant. it can do one thing superbly, but only that one thing. the use of parallel processing at aremote site using highperformance computing could provide a solution for the near term. in this way, thesmaller storage and performance pcs at the site of a disaster could download the solutions and data summariesprovided by highperformance computing onsite processing. such processing is currently infeasible in real timein the field. under the current limitations of disk drive storage and the huge amounts of memory needed forimage processing and correlation of disparate databases, it is not possible to process such needed information inthe field.and we desperately need another capability for the emergency manager. we need the ability to sort, digest,summarize, and prioritize data into information, and further, into intelligence. we need to be able to convert datainto usable decisionmaking tools in nanoseconds. we need the integrated decision tools that will enable anemergency manager to apply this information to the problem at hand.information technology has the ability to provide solutions to these old problems, or it will have in the nearfuture. information technology's true value is in its ability to provide the new while enhancing the old. the newis the rapidity with which real, trusted information can be provided in an easily understood picture for thearchitecture for an emergency lane on the nii: crisis information management367the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.commander. the old is the hardgained experience that provides the knowledge and judgment of the emergencymanager.the ability of the emergency manager to comfortably use the enormous capability of the new technology isnot a matter of purchasing the new ''goodies" to reside on a shelf as another status symbol. it means learning anentire new set of tools for responding to uncertainty. it means developing a handson familiarity to these newtools. just as the driver of a highperformance race car must learn to skillfully use a stick shift, so must theemergency manager learn to use the tools of the new information technologyšnot merely learn to use that stickshift, but to be thoroughly comfortable with it and to feel as though it were an extension of his or her own hand.some emergency managers will make this effort. some will not. in all cultural changes, some adapt, andsome are overrun by the change. some continued to lay keels for fourmasted schooners after fulton invented thesteam engine. some continued to make buggy whips after the coming of henry ford's model t. our emphasiswithin the nii must be on those millions who are jumping onto the internet and the web every day. we mustfocus on those who take the leap of faith into the new. every change is cluttered with gatekeepers whose veryexistence is intended to keep change from happening.at every crossway on the road to the future, each progressive spirit is opposed by a thousand men appointed toguard the past.šmaurice maeterlinck, nobel laureaterather than worrying about those who will be left, we must focus our efforts on making these newtechnologies comfortable for those who are joining the new generation. almost everything negative that one canhear about the computer revolution was said 50 years ago about the radio. all the dire predictions of the culturalchanges brought about by the radiošfor example, listening to baseball games instead of going down to wrigleyfieldšnever came true. baseball is still baseball.there is now an entire subset of people who are as comfortable with a computer as they are with atelephone. they are lost without it. millions are carrying smaller and smaller versions of their business andpersonal lives with themšin the car, on airplanes, and on vacations. shortly, all those with a computer are goingto have to buy another, more powerful one. the world wide web, multimedia, graphics, and all those wonderful(and let us not forget, fun) things eat disk space. they need enormous increases in the speeds of transmission.the market is huge.recommendations: an architecture for the emergency lane on the niithe national institute for urban search and rescue has developed an architecture that has great promise. itis simple, involves offtheshelf components, incorporates tested systems already in place, and uses newtechnological applications. it is the development side of the equation rather than research that now needsattention and focus.the crisis communications management architecturecentral to the concept of the crisis communications management system is the nii initiative that canprovide the opportunity to make dramatic steps forward in crisis information handling. let us envision this newinformation and communications support system as succeeding grids laid one on the other. the first layer of thegrid is in space. it consists of space imaging and sensor information in all its multiforms. the second layer is aseamless communications grid, transparent to the operator. this communications grid contains virtual networkswithin the tactical area as well as national networks. the third and final layer is the tactical grid from which weconduct the crisis operations.how then do we link all this information into one understandable "picture" of the crisis for decisionmakers?to do this our communications architecture must provide both decisionmakers and tactical users with a pictureof the crisis that artificially replicates the reality of the emergency. this suggested civilian crisisarchitecture for an emergency lane on the nii: crisis information management368the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.communications management system is patterned upon the successful copernicus architecture of the u.s. navy(usn, 1993). the brain child of vice admiral jerry o. tuttle (now retired), copernicus was developed whiletuttle was director for naval space and electronic warfare. it is considered a communications milestone for theinformation age. interestingly, the similarities in operational needs and constraints between electronic warfareand crisis management are greater than the differences. the most obvious difference, other than scale, is that theend result of crisis management is safety and recovery, rather than destruction. although the end result isdifferent, the methods of communication and information exchange are remarkably similar.the crisis communications management system must supply a doctrinal, organizational, and technologicalmanagement system applicable across all functions, agencies, and organizations of the crisis managementresponse, regardless of their position in it. the system must be readily adaptable across the levels of participationas the response either escalates or decreases. it must also incorporate the widest space imaging system, includingthe global positioning system, space imaging and interpretation, weather surveillance, and so on. the interfacesmust be synergistic and seamless across the disaster area and operationally transparent to the user regardless ofthe affiliation, level of response, or component. in addition, the system must integrate command and control,information processing, resources and transportation, levels of responsibility, tactical operations, and onscenedata. such capabilities must be coordinated across the crisis arena and vertically up and down the levels ofstakeholders.the communications gridcentral to the communications grid are the widearea computer networks that link the commands andactivities of the decisionmakers and stakeholders to the response activities at the scene of the disaster. they areconfigured on a regional or operational area basis and are constructed to transport, standardize, and concentratesensor, analytic, command, support, administrative, and other data for further passage to incident commanders.the communications grid will use current and planned commonuser communications systems, such as theevolving national communications infrastructure, and the present interlinking media communications networksfor multimedia communications. the national institute for urban search and rescue believes that theemergency environment will become far more dataintensive and require far more technological agility inobtaining, handling, and transmitting data than we have experienced.a second and equally critical development over the last 10 years has been the growth of small computers,both personal computers and workstations. although the latest growth in computers has been astronomical, wesee an even greater increase in their speed and a great reduction in size. handheld computers will soon be in thefield with nearly the same processing capability as today's desktop configurations. the incident commander doesnot necessarily want to sit at a screen and pull up windows. he or she will likely want a "telestrator"type ofsystem that enables writing on the screen and placing objectives visually to be transmitted. we must be sure thatthis rapid growth supports industry standards and open systems architecture.the establishment of "information highways" and the movement toward open systems architecture makepossible the aggregation of many disparate agencies and organizations. these entities, potentially involved incatastrophes and disasters, are defined not by physical boundaries but by data addresses and a common software"veneer."the regional hubsthe regional hubs are regionally oriented and contain the major sensor and analytic nodes, both for stateand national data. the number and nature of the regional hubs are intended to be dynamic so that the architecturecan support the particular desires and needs of the area. for example, to construct a logistics, weather, planning,and/or contingency system simply means developing a software veneer for the common hardware "engines"envisioned as the building blocks of the communications grid. we can also envision contingency hubs as well asthe major regional hubs.architecture for an emergency lane on the nii: crisis information management369the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the command and control complexesa second part of the communications grid includes the command and control complexes. for those agencieswith significant areas of responsibility for civil emergency operations, such as the u.s. air force as executiveagent for inland search and rescue, the u.s. coast guard in its responsibility for the maritime area, and thefederal emergency management agency for its role in natural and manmade disaster, command complexes willbe dedicated. significant differences exist between the regional hubs and the command complexes. the regionalhubs are an aggregation of "communities of interest"; the command complexes are an aggregation of commandstructures in the particular area of responsibility.the information centersthe third part of the communications grid consists of operational information constructs, notcommunications networks. the regional hubs and the command complexes will share a consistent tacticalpicture through this series of information constructs. like the regional hubs and the command complexes, theinformation centers are not physical but virtual "nets," established at the request and in the mix desired by eachincident commander.the information contained in a single node may be provided via several communications channels or viceversa. these information centers spring from an operational decision about where to send data between theemergency and the regional hub. these nodes will be thought of in three conceptual planes: the technological and custom protocols for the exchange of information for technical applications; the operation data layeringšthat is, the doctrinal decision to place the data on a particular distributionnetwork and route it to a particular incident commander's workstation; and the transformation of data to information, which is a function of the software interface on the tacticalcomputers.communications consultant charles r. morris writes in the los angeles times that the term "informationhighway" may not be the most appropriate description for the wireless infosphere of innumerable paths ofinformation to any destination. he believes that "ocean" more realistically describes the processšone where alldata perpetually circulate until searched out and plucked down by an intelligent agent embedded in each personaldata assistant (campen, 1994a).the information centers may support eight formations of communications services and three cases ofprecedence. radio frequency (rf) communications will be undetectable, except to the designated andcooperative receiver. a glimpse of the near future includes rf signals that will all operate on top of each other,below the noise and with featureless wave forms. parasite information will ride on carriers, and antennas will bebroadband, high gain, and electronically steerable. they will be used to access multiple satellites simultaneouslyin various orbital planes along with terrestrial highcapacity data links (busey, 1994).the number of information centers will not be fixed. instead, they will be connected for the length of timenecessary to transport the data to the users for the incident and then truncate.the information centers are classified into three broad categoriesša menu is a good analogyšby"communities of interest." information centeršcommand will service the decisionmakers. these information centers are envisioned asmultiformat, including teleconferencing. information centeršsupport will include such pathways as an environmental, logistics, databasefiletransfer, imagery, geographic, and narrative message pathway. this is the only information center that isenvisioned as carrying narrative. information centerštactical will be constructed around the tactical needs of the response forces. since thevariety of emergencies can be quite different and are dependent on the mission and the resources atarchitecture for an emergency lane on the nii: crisis information management370the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.hand, these centers are seen to be a mixture of both predetermined information services (perhapsassociated with standing operations procedures) and virtual information services created dynamicallyonly for the duration of a particular event. these centers can comprise both preexisting and usercreatedinformation groups.the tactical gridthe tactical grid is conceived as a widearea system, a network of small communications links that tie allunits operating in one tactical area together regardless of the agency or function. the differences between thecommunications grid and the tactical grid are functional. the communications grid provides connectivity, whichfacilitates the movement of information among operators and analysts. the tactical grid, alternatively, connectssystems among operational units to provide information across the scene of the crisis. a good analogy for thetactical grid is a power grid. when computers of different makes and operating systems are plugged into electricpower outlets, they get a common energy system. by connecting dissimilar systems across the tactical grid, weare connecting platform sensors, fire, medical, and law enforcement units, main computers and electronicsubsystems, and so on.thinking of the diversity of sensors, communications, and resource systems as grids overlying the tacticalcrisis arena provides a readily understandable way of viewing the myriad of assets and stakeholders in the crisiscommunications infrastructure. operationally, the impact is that dissimilar resources and/or units can beconnected in the tactical grid, imposed over the operating area as though they were joining a regional power grid.this link would occur simultaneously, allowing the operators of those resources to plug into the space andcommunications grids.the architecture of the tactical command center (tcc) is intended to serve as the "nerve center" for theincident commander and his units in the response arena. this means that the tcc is not only the intelligencecenter for tactical command, but also the tactical center for individual units and the multiagency incidentcommand areas (macs). the tcc provides the tactical displays, integrated information management, andaccessibility to tactical communications in support of the response missions. it provides the requisite tacticalconnectivity to units, to other area commanders, and to the command complex of the decisionmakers.architecturally the tcc is analogous to the command complex. both will share a consistent tactical picture andconnect responsible agencies to the stakeholders, at the tactical level and regional levels.conclusionsthe essence of crisis management is an effective informationhandling capability. command must have it;analysis must have it; tactical operators must have it. without a true picture of the emergency event, we areplaying 1990s scenarios with 1940s technology. why do we continue to do this? possibly it is because, for manyyears, a lack of robust communications has prevented obtaining any understandable "picture" of emergencyincidents. routinely, it was three and sometimes four days before the full scope of a disaster became apparent.responders did what they couldšsaved as many lives as possible, shored up as many levees as appeared to beendangered, housed and fed survivors, and hoped that the body count would not be too high when the waterreceded. then came the invention of the minicam, the personal computer, and cellular communications and theirproliferation in every aspect of our lives.now emergency managers realize that it is possible to obtain a rapid and clearer picture of a disaster. youcan watch from space in real time the rise of the mississippi or a hurricane bearing down on the florida coast.you can compare snow loads in the sierra this year with measurable images from last year. you are a visceralparticipant in each and every disaster through the convenience of cnn! and yet, we still have not applied thesetools and capabilities to the actual command and control of emergency response operations.it is definitely past wakeup time for emergency managers. if they can think of fire trucks, helicopters, andambulances as "rescue platforms," the electronic equivalent is the communications platform. it is at this levelarchitecture for an emergency lane on the nii: crisis information management371the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.that systems architecture, programs, and technology converge in a level of detail that does not require specializedexpertise.it is in this tier that critical paths in operations, programming, and technologies converge and become mostobvious to managers. this idea that electronic platforms are as essential as other rescue platforms will allow usto approach design and installation of the crisis communications management system. at its simplest, theadvent of crisis management is both the recognition of the requirement and the means to operate in theelectromagnetic spectra and in space against the increased calamities threatening our traditional economy.when we think operationally of the crisis communications management architecture as having a space gridof diverse sensors, supported by a dynamic, multinode communications grid, we are conceiving and operatingthe complex and geographically disparate electronics of today's emergency response as a system to present thecrisis scene as it really appears, containing all relevant information in a transparent, easily understood format.it is the c4i system, designed to make communications transparent to the user and all sensors available incommon formats, that allows us to conceive of the space and communications grids and of informationmovement between them. the crisis communications management system has been designed to include bothlocal and widearea networks that have tied different systems and hardware together, along with higherbandwidth communications capabilities and more efficient software. we have reengineered the work processesfor improved information handling.during the twentieth century, disaster response has moved out of the trenches of landbased operations.with the addition of aircraft to traditional rescue platforms, there was an exponential increase in the space overwhich we could travel to perform rescue operations. but with this increase in the area of operations, commanderslost the ability to view the catastrophe as a whole. now, with our latest technology, we can again provide theincident commanders with a visual picture of the scene of operations. to illustrate this, our paper ends with astory quoted from the u.s. navy's publication sonata (usn, 1993), a presentation of the plan for navalcommunications through the next century.two hundred years ago when lord nelson walked out on the deck of hms victory, the tactical battle space wasobvious. he could see it and share that perception both with his captains and his enemy. the advent of carrier airpower in world war ii changed that. because a commander can no longer see the battle space, perhaps hundreds ofmiles away from where he stands, it must be artificially reconstructed for him.today that reconstruct is accomplished by messages arriving over different networks, in diverse formats and withdifferent time delays. electronic communications, imagery and radar systems, until recently were displayed onseparate screens open to mismanagement. reality was replaced by an artificial view that was too complex, tooredundant, and too slow.now we think operationally of the battle space as having diverse sensors, supported by a dynamic,multiconstellation communications grid. we are (whether we recognizeit or not) conceiving and operating thecomplex and geographically disparate electronics of modern warfare as a system to present the battle space as itreally appearsšjust as it did to nelson.the above paragraphs relate to the use of advanced systems designed for electronic communicationssupport in time of war. these same technologies are available to civilians. although a few of the concepts arestill in development, many are "off the shelf" today. why aren't we using them to save more of our own people?it seems worth doing to us. to again quote adm. jeremiad, "i urge you to leap into another dimension ofcapability by simultaneously expanding the window of our technological might and our strategic thought." wesay, press on!referencesbusey iv, usn (ret.), adm. james b. 1994. "navy's space, electronic warfare vision mirrors new defense budget framework," signalmagazine, afcea international press, march.business week. 1995. "planet internet," a special report.architecture for an emergency lane on the nii: crisis information management372the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.campen, usaf (ret.), col. alan d. 1994a. "competition, consumers are shaping information highway," signal magazine, afceainternational press, march.campen, usaf (ret.), col. alan d. 1994b. "technology trumps policy in information," signal magazine, afcea international press,february.elliott, ronald d., and maj. scott bradley, usmcr. 1995. "effective command and control," jwid '95.kelly, maj. brian j. 1993. "from stone to silicon: a revolution in information technology," unpublished white paper available from thearmed forces communications and electronics association (afcea) educational foundation, december.harrald, john r. 1993. "contingency planning using expert judgment in a group decision support center environment," unpublished whitepaper available from george washington university, department of engineering management.harrald, john r., and t. mazzuchi. 1993. "planning for success: a scenario based approach to contingency planning using expertjudgment," journal for contingencies and crisis management.international city/county management association (icma). 1994. "computer technology in local government, second survey,"government technology, march.jeremiah, adm. david. 1994. unpublished presentation at "west '94", afcea and u.s. naval institute conference and exposition, january.linden, eugene. 1994. "burned by warming," time magazine, march 14.rockhart, j.f. 1981. "the changing role of the information systems executive: a critical success factors perspective," sloan management review, pp. 15œ25.signal magazine. 1994. "consortium sows seeds for information infrastructure," february.starbuck, w.w. 1989. "clio's conceit: looking back into the future," unpublished presentation at the second international conference onindustrial and organizational crisis management, new york, november.u.s. navy (usn), office of space and electronic warfare. 1993. sonata .architecture for an emergency lane on the nii: crisis information management373the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.44aspects of integrity in the niijohn c. mcdonaldmbx inc.the national information infrastructure (nii) has a variety of definitions and gives rise to variousperceptions. one such perception is that the nii is a rapidly expanding network of networks that, in totality,achieves the national objectives set forth by congress and the clinton administration. no matter what thedefinition, this telecommunications infrastructure is a key element of the expanding information revolution thatwe are currently experiencing. plans are being made by many sectors of the economy to further use the resourcesof the nii to improve productivity, reduce cost, and maintain a competitive edge in the world economy. includedamong the sectors with growing reliance on the nii are health care, education, manufacturing, financial services,entertainment, and government.with society's increasing reliance on the nii, there is a corresponding need to consider its integrity.integrity is defined as "the ability of a telecommunications infrastructure to deliver high quality, continuousservice while gracefully absorbing, with little or no user impact, failures of or intrusions into the hardware orsoftware of infrastructure elements1."integrity is an umbrella term that includes other important telecommunications infrastructure requirementssuch as quality, reliability, and survivability.this paper discusses various dimensions of integrity in the nii. threats to integrity are presented andlessons learned during the past decade summarized, as are efforts currently under way to improve networkrobustness. finally, this paper concludes that architects and designers of the nii must take issues of integrityseriously. integrity must be considered from the foundation up; it cannot be regarded as a bandaid.threats to nii integritynetwork elements can fail for any number of reasons, including architectural defects, design defects,inadequate maintenance procedures, or procedural error. they can fail due to acts of god (lightning, hurricane,earthquake, flood), accidents (backhoe, auto crashes, railroad derailment, power failure, fire), or sabotage(hackers, disgruntled employees, foreign powers). architects and designers of the nii should weigh each of thesethreats and perform costbenefit studies that include societal costs of failure as well as firsttime network costs.users of the nii should understand that failures will occur and should have contingency plans.over the past 10 years, public networks in the united states have experienced failures resulting from mostof the threats described above. in may 1988, a fire in the hinsdale, illinois, central office disruptedtelecommunications services for 35,000 residential telephones, 37,000 trunks, 13,500 special circuits, 118,000longdistance fiber optic circuits, and 50 percent of the cellular telephones in chicago2. full service was notrestored for 28 days. the failure affected air traffic control, hospitals, businesses, and virtually all economicsectors. two months later, technicians in framingham, massachusetts, accidentally blew two 600a fuses in theunion street central office. the local switch stopped operation, and calls from 35,000 residential and businesscustomers were denied for most of the day3.in november 1988, much of the longdistance service along the east coast was disrupted when aconstruction crew accidentally severed a major fiber optic cable in new jersey; 3,500,000 call attempts wereaspects of integrity in the nii374the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.blocked4. also in november 1988, a computer virus infiltrated the internet, shutting down hundreds ofworkstations5.several wellpublicized ss7 outages occurred in 1990 and 1991 due to software bugs6,7. the first had anationwide impact and involved the loss of 65,000,000 calls. others involved entire cities and affected10,000,000 customers.in response to a massive outage in september 1991, the mayor of new york established a task force ontelecommunications network reliability. the task force noted that "the potential for telecommunicationsdisasters is real, and losses in service can be devastating to the end user"8.lessons learned that are applicable to the niinetwork infrastructure architects and designers have used redundancy and extensive testing to buildintegrity into telecommunications networks. they have recognized the critical role that such infrastructure playsin society and are mindful of the consequences of network failure. techniques such as extensive software testing,hardware duplication, protection switching, standby power, alternate routing, and dynamic overload control havebeen used throughout the network to enhance integrity.a 1989 report published by the national research council identified trends in infrastructure design thathave made networks more vulnerable to largescale outage9. over the past 10 years, network evolution has beenpaced by changes in technology, new government regulations, and increased customer demand for rapidresponse in provisioning voice and data services. each of these trends has led to a concentration of networkassets. although additional competitive carriers have been introduced, the capacity of the new networks has notbeen adequate to absorb the traffic lost due to a failure in the established carrier's network. enduser access to allcarriers has been limited by this lack of familiarity with use of access codes.economies of scale have caused higher average traffic cross sections for various network elements. fiberoptic cables can carry thousands of circuits, whereas copper cables carried hundreds. other technologies such asmicrowave radio and domestic satellites have been retired from service in favor of fiber. when a fiber cable isrendered in operable for whatever reason, more customers are affected unless adequate alternate routing isprovided. the capacity of digital switching systems and the use of remote switching units have reduced thenumber of switches needed to serve a given area, thus providing higher traffic cross sections. more customersare affected by a single switch failure.in signaling, the highly distributed multifrequency approach has been replaced by a concentrated commonchannel signaling system. also, call processing intelligence that was once distributed in local offices is nowmigrating into centralized databases.stored program control now exists in virtually every network element. software technology has led toincreased network flexibility; however, it has also brought a significant challenge to overall network integritybecause of its "crash" potential. along with accidental network failures, there have been a number of maliciousattacks, including the theft of credit cards from network databases and the theft of cellular electronic securitynumbers.in regulation, the federal communications commission has mandated schedules for the introduction ofnetwork features such as equal access. for carriers to meet the required schedules, they chose to amalgamatetraffic at "points of presence" and modify the software at a small but manageable number of sites to meet theimposed schedules. hinsdale was one such site and, unfortunately, the fire's impact was greater than it wouldhave been without such regulatory intervention because of the resulting traffic concentration.in my opinion, the most important lesson learned in the recent past regarding telecommunicationsinfrastructure integrity is that we must not be complacent and assume that major failures or network intrusionscannot happen. in addition to past measures, new metrics must be developed to measure the societal impact ofnetwork integrity and bring the scientific method of specification and measurement to the problem10.another lesson learned is that design for "singlepoint failures" is inadequate. fires cause multiple failures,as do backhoe digups, viruses, and acts of god. there has been too much focus on individual network elementsand not enough on endtoend service.aspects of integrity in the nii375the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.software is another issue. we have learned that testing software to remove all potential bugs is difficult ifnot impossible. software does not wear out like hardware, but it is a single point of failure that can take down anentire network. three faulty lines of code in 2.1 million lines of instructions were enough to cripple phoneservice in washington, d.c., los angeles, and pittsburgh in nearly identical failures between june 26, 1991, andjuly 2, 1991.improving network robustnessin recent years, efforts to improve network robustness have been redoubled. in addition to the work ofindividual common carriers, there are many organizations that are addressing these problems, including bellcore,the national security telecommunications advisory committee, the fcc, the institute for electrical andelectronics engineers, and american national standards institute committee t1.exhaustive testing of new systems and new generic software programs has been instituted by manufacturersand by bellcore. new technologies have been applied, including "formal methods." new means have beendeveloped and implemented to try and detect "bugs" that previously would have gone undetected.new network topologies have been implemented using bidirectional sonet rings and digital crossconnectsystems. the concept of design for singlepoint failure has been supplemented to include multiple failures. incases where economical network design calls for elimination of already sparse network elements, robustness hasbecome a consideration, and the reduction has not occurred.new metrics have been established to quantify massive failures and reporting means have beenimplemented by the fcc. standards have been set to quantify the severity of network outages.means have been implemented to detect the theft of cellular electronic security numbers, and new personalidentification numbers have been used. there is increased awareness by the employees of common carriers ofthe need for protection of codes used to access proprietary databases and generic software.over the next 2 to 5 years, infrastructure robustness will be enhanced through new procedures and networkelements that will soon be in production. products deploying asynchronous transfer mode (atm) will give moreflexibility in restoring a damaged network. more parallel networks will be deployed which, if interoperable, willadd new robustness to the nii.current and planned research will enhance nii robustness in the 5 to 10year window. some of theresearch topics were recently summarized in the ieee journal of selected areas in communications11. openissues addressed in this issue included user survivability perspectives on standards, planning, and deployment;analysis and quantification of network disasters; survivable and fault tolerant network architectures andassociated economic analyses; and techniques to handle network restoration as a result of physical damage orfailures in software and control systems. these subjects were organized into four categories: user perspectivesand planning; software quality and reliability; network survivability characterization and standards; and physicallayer network restoration, atm layer network restoration, network layer restoration, and survivable networkdesign methods.conclusionsover the past decade, we have learned many important lessons in the design of telecommunicationsinfrastructure that are applicable to the nii. although past networks have been designed with high levels ofintegrity in mind, these efforts have not completely measured up to the expectations of society. recently, effortshave been redoubled to improve network robustness.as the nii is defined, it is important that integrity issues be considered from the ground up. only by thesemeans will an nii be constructed that meets the expectations of society.aspects of integrity in the nii376the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.bibliography1. private communication with w. blalock, bell communications research.2. national communications system. 1988. "may 8, 1988, hinsdale, illinois telecommunications outage," aug. 2.3. brown, b., and b. wallace. 1988. "co outage refuels users' disaster fears," network world, july 11.4. sims, c. 1988. "at&t acts to avert recurrence of longdistance line disruption," new york times, november 26.5. schlender, b. 1988. "computer virus, infiltrating network, shuts down computers around world," wall street journal, november 28.6. fitzgerald, k. 1990. "vulnerability exposed in at&t's 9hour glitch," the institute, march.7. andrews, e. 1991. "string of phone failures reveals computer systems' vulnerability," new york times, july 3.8. city of new york. 1992. "mayor's task force on telecommunications network reliability," january.9. national research council. 1989. growing vulnerability of the public switched networks: implications for national security emergencypreparedness. national academy press, washington, d.c.10. mcdonald, j. 1994. "public network integrityšavoiding a crisis in trust," ieee journal on selected areas in communications, january.11. ieee journal on selected areas in communications, january 1994.aspects of integrity in the nii377the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.45what the nii could be: a user perspectivedavid g. messerschmittuniversity of california at berkeleyabstractthe national information infrastructure (nii) is envisioned as a national public internetwork thatencompasses existing networks, such as the internet, the public telephone network and its extensions, and catvdistribution systems and their extensions, as well as new network technologies yet to be invented. today, thesenetworks appear to the user to be separate and noninteroperable, in the sense that a user cannot reasonably makea telephone call over the internet or most catv systems, cannot reasonably watch video over the internet or thetelephone network (except at unacceptably poor levels of quality by entertainment standards), and cannot senddata over the telephone network or most catv systems (except in the limited sense of using these media foraccess to data networks or for pointtopoint data transmission). it is clear that underlying the nii will be acollection of proprietary networks incorporating a variety of different technologies; indeed, there is generalagreement that this is highly desirable. the question addressed in this white paper is what the nii will look likefrom the user perspective, and how it might differ from today's limitedfunctionality and noninteroperablenetworks. we address this question by describing a vision of what the nii could be from a user perspective. inparticular, we describe those characteristics of the nii that we believe will be important to users, includingconnectivity and mobility, quality of service options, security and privacy, openness to new applications acrossheterogeneous transport and terminal environments, and pricing.introductionthis white paper is an outgrowth of the planning workshop organized by the nii 2000 steering committee.representatives of a number of industries participating in the nii and its underlying technologies were present.not surprisingly, given the great variety of industries and their respective largely independent histories andmarkets, the representatives were often ''talking past" one another, not sharing a common vision of what the niishould be, and not sharing the common vocabulary necessary for productive discussion.in the deployment of a massive infrastructure such as the nii, there is great danger that nearterm tacticaldecisions made by the diverse participants in the absence of a longterm strategic vision will result in aninfrastructure that precludes the broad deployment of unanticipated but important applications in the future. suchan infrastructure will not meet the needs of the users and the nation, and will offer its builders a lower return oninvestment that would otherwise be possible. it might even result in widespread abandonment of existinginfrastructure in favor of new technologies, in similar fashion to the recent widespread and costly abandonmentof partially depreciated analog communications facilities.in this white paper, we take the perspective of the users of the future nii and ask fundamental questionsabout how it should appear to them. it is our belief that, nearterm corporate strategies aside, an nii that bestmeets the future needs of the users will be the most successful, not only in its benefits to society and the nation,but also in terms of its return on investment. thus, the full spectrum of industrial and government participantsshould have a shared interest in defining a strategic vision for the long term, and using that vision to influencenearterm business decisions.looking at the nii from a longterm user perspective, we naturally envision a network that has manycapabilities beyond those of any of the current networks or distribution systems. provisioning such a broad rangewhat the nii could be: a user perspective378the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.of capabilities would have cost implications and is economically feasible only to the extent that it provides valueto the user well in excess of the incremental costs. this is problematic if one accepts one of our fundamentalhypotheses, namely, that we cannot possibly anticipate all the bighitting applications of the nii. however, itshould be emphasized that it is not necessary that all nearterm deployments provide all the capabilitiesincorporated into a strategic vision. indeed, one critical aspect of such a vision is that it should be easy and costeffective to add new technologies and capabilities to the nii as unanticipated applications and user needsemerge. if this is achieved, it is only necessary that nearterm investments be compatible with a longtermstrategic vision, and hence not preclude future possibilities or force later disinvestment and widespreadreplacement of infrastructure. this is admittedly not straightforward but is nevertheless a worthwhile goal.one can anticipate the nii falling somewhere on the spectrum from a collection of proprietary andnoninteroperable networks (largely the situation today) to a single, universal network that appears to the user toseamlessly and effortlessly meet all user needs. we argue that from the user perspective the nii should, althoughconsisting internally of a diversity of heterogeneous transport and terminal technologies, offer the seamlessdeployment of a wide range of applications and openness to new applications. not all participants in the nii mayjudge this to be in their best interest, and of course they all encounter serious cost and timetomarket constraints.however, if they take into account longerterm opportunities in the course of their nearterm business decisions,we believe that both theyšthe usersšand the nation will benefit greatly in the long term. it is our hope that thenii 2000 technology deployment project will move the collective deliberations in this direction.terminologyfirst we define some consistent terminology for the remainder of this white paper.the users of the nii are people. the nii will consist of a network (or more accurately a collection ofnetworks) to which are attached access nodes at its edge. we distinguish between two types of devices connectedto access nodes: information and applications servers, and user terminals (for simplicity, we will abbreviate theseto servers and terminals). a networked application is a set of functionality that makes use of the transportservices of the network and the processing power in the servers and terminals, and provides value to users.servers make databases or information sources available to the terminals, or provide processing power requiredto provision applications. users interact directly with terminals, which provide the user interface and may alsoprovision processing power or intelligence in support of applications. examples of terminals are desktopcomputers, wireless handheld pdas, and catv settop boxes.there are two generic classes of applications: usertouser or communications applications, and usertoserver or information access applications. these can be mixed, for example, a collaborative application thatcombines voice telephony with database access.the business entities involved in the operation of the nii are network service providers, who provision thetransmission and switching equipment in the network, and application service providers, who provision theservers and maintain the databases involved in the applications. these may be one and the same, as is the casefor the telephone application in the public telephone network. the users may be the application service provider,as when they load software purchased at a computer store on their terminals. other entities involved are theequipment vendors, who develop, manufacture, and market the equipment (transmission, switching, terminals,etc.), and the application vendors, who develop and market applications for deployment in the nii.connectivity issueslogical connectivity of a networkthe most basic property of a network from a user perspective is the logical connectivity it offers. thenetwork is said to provide logical connectivity between two access nodes if it is feasible to transport databetween those nodes through the network. when one access node sends data to another access node, we call theformer thewhat the nii could be: a user perspective379the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.source and the latter the sink. it may be the case that each logically connected access node is simultaneously asource and a sink (a duplex logical connection) or that one may be exclusively a source and the other exclusivelya sink (simplex logical connection).logical connectivity should be distinguished from network topology. the topology refers to the physicallayout of the transmission media used in the network (coax, wire pairs, fiber, radio). examples are the startopology of the public telephone network and the tree topology of a catv system. the logical connectivity isdetermined not only by the topology, but also by the internal switching nodes. generally, the user is not directlyconcerned with the topology of the network, although some of the important characteristics of the network (likethroughput and quality of service; see below) are affected or constrained by the topology. on the other hand, thenetwork service provider is critically concerned with the topology, as it affects costs.an important distinction is between the possible logical connections in a network (which may beastronomically large), and the actual provisioned logical connections required by a particular application(typically small in number). a similar distinction must be made between the possible applications (i.e., those thathave been developed and made available to users) and those that are actually in use at a particular time. anactual application in use is called an instance of that application, and the actual provisioned logical connectionsin use by that application are called instances of connections.application connectivitythere are several important types of connections that arise in the context of specific applications: a logical pointtopoint connection, in which access nodes are connected in either simplex or duplexfashion. one node in a pointtopoint connection may be a source or sink or both, the latter in a duplexconnection. a logical broadcast connection, in which a single source is connected to two or more sinks. within thenetwork, this type of connection can be provisioned in different ways. simulcast implies separatecomponent connections from source to each sink, and multicast refers to a tree structure (where networkresources are shared among the component connections). the distinction between these alternatives isgenerally not of immediate concern to users, who see only indirect effects (cost, quality of service, etc.). a logical multisource connection, in which two or more sources are connected to a single sink. a distinctionanalogous to multicast vs. simulcast does not apply to multisource, since there is generally no advantage tosharing resources among the components of a multisource connection.multicast or multisource connections are by their nature simplex. if there are only two access nodes, theconnection is necessarily pointtopoint. if access nodes are involved, and if for example every access node cansend information to and receive information from the remaining nodes, then the connectivity can be thought of asa combination of simplex multisource connections (one to each node) and simplex multicast connections (onefrom each source). many other combinations are possible.from a technology standpoint, multisource connectivity merely requires flexibility in the number ofsimultaneous pointtopoint connections to a given sink, which is a natural capability of packet networks.similarly, simulcast connectivity requires flexibility in the number of simultaneous pointtopoint connections toa source. multicast connectivity, on the other hand, while beneficial in its sparing use of resources and the onlyscalable approach to broadcast, requires fundamental capabilities anticipated in the design and provisioning ofthe network.what the nii could be: a user perspective380the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.user perspectiveconnectivityfrom the user perspective, it is desirable to have full logical connectivity in a network. any limitations onconnectivity restrict the functionality and availability of both information access and communicationsapplications. for example: a user who purchases a telephony application from one application service provider wants the option to callall other telephones, whether they are connected to the telephone network, a catv network, the internet,etc. any application service provider who restricts destinations, let's say to only its own subscribers, will beat a disadvantage. the telephone example extends readily to other communications applications. the user will find much lessvalue if the application supplier limits connectivity to a proper subset of those other users who couldparticipate in that application (i.e., who have appropriate terminals, etc.). a user with appropriate terminals to access a type of information access application naturally desiresconnectivity to every available instance of that type of application. for example, a user with the terminalcapability to view a video presentation would prefer to maximize the leverage of the investment in terminalequipment by having access to the maximum range of source material.similarly, the user would like to see all three types of connections (pointtopoint, broadcast, andmultisource), since eliminating any one of them will preclude valued applications. for example: a "conference telephone call" and "video teleconference" are examples of communications applications thatrequire both multisource and broadcast connections. they are multisource because one participant will wantto see and/or hear two or more other participants simultaneously. they are broadcast because any oneparticipant will want to be seen by all the other participants. a remote learning class or seminar requires broadcast connectivity because many participants may want tosee the presentation, and may also be multisource if the participants have audio or video feedback to theinstructor. the unix xwindows graphical user interface illustrates the value of running applications on two or moreservers and displaying the results on a single terminal. this requires multisource connectivity.network or applications service providers may view it as in their best interest to restrict the range ofapplications, information servers, or application service providers that they make available to their subscribers.however, the experience of the computer industry makes it clear that users will choose options with greaterflexibility, given the choice and appropriate pricing. for example, restrictedfunctionality appliances such as thestandalone word processor quickly lost market share to the personal computer, which offered access to a broadrange of applications.conversely, in an environment with greater logical connectivity, it becomes more economically viable fornew and innovative applications to reach the market. application service providers with access to a broad rangeof users (not restricted to the limited market of subscribers to a particular service provider) quickly exploit theireconomies of scale. again, the computer industry offers valuable lessons. the personal computer made availablean embedded large market for new applications running on widely deployed terminals. applications vendorstargeting the most widely deployed architectures gained the upper hand because of the larger developmentinvestments they were able to make.in conclusion, greater logical connectivity and more connectivity options offer more value to users andhence make the network service provider more economically viable; in addition, there are natural market forcesthat favor application service providers that target those highconnectivity networks.what the nii could be: a user perspective381the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.mobilitythe classification of connections is simplest to apply where users are in fixed locations. users are actuallymobile. they may be satisfied with accessing the network from a fixed location, which implies that they canaccess it only at those times they are physically in that location. increasingly, however, users expect to be able toaccess the network more flexibly. there are several cases: fixed location, where an application is provisioned to be accessed from a specific access node. wiredtelephony is an example. flexible location but static access node, where the user is allowed to choose the access node from amongdifferent geographic locations, but that access node is not allowed to change during a single applicationinstance. an example is wireless access to the network with the assumption that the user remains withinrange of a single base station. moving location and access node, in which a user with wireless access is allowed to move from the coverageof one base station to another for the duration of an application instance. this allows the user be in motion,such as on foot or in a moving vehicle.the flexible and moving location options require high logical connectivity in the network. thus, greaterlogical connectivity provides great value to users who desire to be mobile. as witnessed by the rapid growth ofcellular telephony, this is a large proportion of users, at least for telephone, data, and document applications.like multicast forms of broadcast connections, the moving location option requires fundamental capabilitiesin the network that must be anticipated in its design and provisioning, since connection instances must bedynamically reconfigured. this option makes much more sense for some applications than others. for example,it is reasonable to conduct a phone conversation while in motion, but more difficult and perhaps even dangerousto watch a video presentation or conduct a more interactive application. even the latter becomes feasible,however, for users in vehicles driven or piloted by others.openness to new applicationsaside from the logical connectivity of the network, the second most important characteristic to users is theavailable range of applications. it is a given that the application possibilities cannot be anticipated in advance,and thus the network should be able to accomodate new applications.again the evolution of the computer industry offers useful insights. because the desktop computer was aprogrammable device, a plethora of new applications was invented long after the architecture was established.equally important was the availability of the market to many application vendors, which led to rapidadvancement. a primary driving force for the desktop computer was that it freed the user from the slowmovingbureaucracy of the computer center and made directly available a wealth of willing application vendors.the internet was architected with a similar objective. the network functionality is kept to a minimum, withno capability other than the basic transport of packets from one access node to another embedded within thenetwork. beyond these minimal capabilities, the intelligence and functionality required to implement particularapplications are realized in the servers and terminals. this architecture separates the development anddeployment of applications from the design and provisioning of the network itself. new or improvedapplications can be deployed easily without modifications or added capabilities within the network, as long asthey comply with any limitations imposed by the network design (see "quality of service," below). thischaracteristic has been the key to the rapid evolution of internet applications, and in turn to the success and rapidgrowth of the internet itself.to be of maximum benefit to users, we believe the nii should be designed according to a philosophysimilar to that for the internet (although without some of its limitations). one can summarize thesecharacteristics as follows:what the nii could be: a user perspective382the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. provide full logical connectivity among all access nodes, and do not limit the number of logical connectionsavailable to any single access node. to do otherwise limits future applications. do not design the nii or portions of the nii around specific applications, thereby limiting its capabilities tosupport future unanticipated applications. rather, realize within the network the minimum capabilitiesrequired across all present and future applications (to the extent it is possible to anticipate those capabilities). realize the primary application functionality in the terminals or servers, or alternatively at access points tothe network (but within the domain of the network service provider), rather than internal to the networkitself. this way new applications can be deployed by adding functionality at only those access nodesassociated with users willing to pay for those applications, without the obstacle of making uneconomicmodifications throughout the network infrastructure. since standardization presents a potential obstacle to the rapid deployment of innovative applications,consciously limit the role of standardization to the basic network infrastructure. do not attempt tostandardize applications, but rather allow them to be provisioned and configured dynamically as needed.even when the nii is designed according to this philosophy, there is still a major obstacle to the economicdeployment of new communications (as opposed to database) applications: the community of interest problem.before one user is willing to purchase an application, it is inherent in a network environment that there must be acommunity of other users able to participate in that application. for example, an isolated user can usefullybenefit from a shrink wrapped personal computer application purchased locally, but in a networked environmentmay depend on other interested users who have purchased the same application. this can place a dauntingobstacle in the way of new applications and limit the economic return to application vendors or serviceproviders. fortunately, there is a solution. if applications are largely defined in software rather than hardwareprimitives, they can be dynamically deployed as needed to terminals participating in the application. we call thisdynamic application deployment.a crucial element of the nii required to support dynamic application deployment is the ability to transfersoftware application descriptions in the establishment phase of an application instance. deployment can alsooccur during an application instance (if it is desired to change or append the application functionality). thisrequires a reliable connection to the terminal, even where other aspects of the application (such as audio orvideo) may not require reliable protocols. since such application descriptions are likely to be large, the user isalso better served if there is a broadband connection for this purpose to limit the time duration of theestablishment phase.flexibility in deployment of applications also requires a full suite of control primitives as a part of thenetwork control and signaling interface to the user terminal. anticipating all the capabilities needed here is a keydesign element of the nii. such a design also needs to control the complexity inherent in such a heterogeneousenvironment, for example by defining an independent "universal" signaling layer together with adaptation layersto different network technologies and prexisting signaling systems.quality of servicemany applications call for control over aspects of the quality of service (qos) provided by the network.from the user and application perspective, qos parameters include the following: the setup time in establishment, including configuration of the connection instances, transport of theapplication description to the participating terminals, etc. the frequency with which an application is refused by the network (due to failures, traffic overload, etc.). the interactive delay through the network (the time from user action to appropriate application reaction).what the nii could be: a user perspective383the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. the subjective quality of application components like audio and video, which is affected not only byquantization and network loss artifacts, but also the delay introduced in the transport and synchronization ofthe audio or video. the subjective quality depends not only on network qos characteristics, but also on thecharacteristics of the application implementation in the terminals, such as the algorithms used for audio orvideo compression.the user is of course also concerned with the pricing of the application, which is likely to be related to theqos it requires. the qos parameters of the network itself affect users and applications, and include: the throughput of the network, in both directions in the case of a duplex connection; the delay, variation in delay, and temporal characteristics of delay variation in transport through the network; the frequency with which losses occur, and the temporal characteristics of those losses (such as whetherthey are bunched together or spread out); and the frequency of corruption of data, and the temporal characteristics of that corruption. (for data transport,corrupted data must be discarded, whereas in continousmedia transport such as audio and video, corrupteddata are useful but cause subjective impairments.)there are two distinct philosophies of network design: the network provides guarantees on some qos parameters. the quantitative guarantees are established bynegotiation between application and network at establishment, and appropriate resources within the networkare reserved for the connection instances to ensure that the guarantees will be satisfied. the network provides besteffort transport, in which resources are provided to a connection instance on anasavailable basis, without guarantee.rarely does a network strictly follow one of these models. for example, the internet offers as one optionguaranteed delivery (zero loss) service, but does not guarantee against delay. conversely, the public telephonenetwork offers delay guarantees, but does not guarantee against corruption. even for a single qos parameter,besteffort and guarantees can be mixed for different connections, by reserving network resources for someconnection instances and providing only leftover resources to other connection instances. qos guarantees have acost associated with them, principally in reserving resources, making them unavailable to other connectioninstances even when unused. there is also a substantial increase in the complexity of the network associated withqos guarantees. the qos of the network can sometimes be modified more simply in the access nodes, forexample by introducing forward errorcorrection coding to reduce the corruption probability (at the expense ofadded delay).there is considerable controversy over the relative merits of besteffort vs. guaranteed qos transport. itappears that both models have merit and may reasonably coexist. qos guarantees will be mandatory for someapplications: consider the possible consequences of unanticipated interactive delay in a remote telesurgeryapplication! it has not yet been established or demonstrated that besteffort transport can achieve entertainmentquality video. on the other hand, the simplicity and lower cost of besteffort transport seem desirable for otherapplications, like interactive graphics. the qos requirements (or lack thereof) vary widely across differentapplications. thus, the nii should be capable of provisioning different types of qos guarantees to differentapplications on request, and should also offer a lowercost, besteffort service to other applications.for both besteffort and guaranteed qos, an important issue to the users is any inherent limitations onavailable qos. there are many network design choices that can (inadvertently or for reasons of cost) limit thebest available qos. since the nii is expected to support many applications, it is important that fundamentaldesign choices not be made that unduly restrict the best available qos, although some portions of the nii maydeliberately be provisioned in a fashion that temporarily limits qos for cost reasons. among the most importantof these design issues are the following:what the nii could be: a user perspective384the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. the network topology can substantially increase the lowest available delay. the choice of physical layer technology in conjunction with topology can severely limit qos. for example,wireless and wirepair access technologies can limit the highest available rate, as can multiaccess topologies(wireless reverse links or treetopology catv distribution system reverse links). achieving high reliability on wireless access links can be expensive (in terms of system capacity), especiallyin the worst case and especially in the context of moving terminals.because of qos limitations that are either fundamental (like propagation delay) or expensive to circumvent(like wireless corruption), it is important that applications be scalable and configurable to available qos (seebelow).delay appears to be a particular problem area for the nii. of all the qos parameters, delay is the only onethat suffers from a fundamental limit, namely, the physical propagation delay. propagation delay will be on theorder of at least 200 to 300 milliseconds round trip for a connection halfway around the world. the desired delayfor some applications is actually less than this. for example, desirable roundtrip delays for synchronouscontinuous media applications like voice telephony and video conferencing, as well as interactive keyboardapplications, are on the order of 50 to 100 milliseconds, and delays on the order of a few hundred millisecondsare significantly annoying. thus, there is little margin for introducing delays in excess of the propagation delaywithout significant impairment at the greater geographic distances. unfortunately, there are many design choicesthat can introduce significant delay that are already observed in present networks: packet networks trade network capacity through statistical multiplexing for queuing delay at switchingnodes, and this queuing delay increases substantially during periods of congestion. a given connectioninstance may traverse many such switches in a network with a "sparse" topology, and thus there is anunfortunate tendency for propagation and queuing delay to increase in tandem. a high degree of logical connectivity can be achieved in virtually any network topology, including thosewith sparse physical connectivity, by adding switching. however, as previously noted, this switching canitself introduce queuing delay. beyond this, the physical path traversed by the data can be considerablylengthened, increasing the propagation delay as well. this is a flaw in any approach involving a collection of"overlay" subnetworks with internet gateways. in packet networks, large packet headers encourage long average packet lengths at high network utilization.for lowthroughput applications like voice and audio, the packet assembly time for large packets introducesa large delay (independent of network throughput). an example is the internet protocol, which has a largepacket header (scheduled to get even larger in the future). it is tempting to insert transcoders from on compression standard to another in the network for audio andvideo applications. these transcoders force delays to add across network links on a worstcase (as opposedto statistical) basis, and also add significant signalprocessing delays. for example, digital cellular basestation voice transcoders add a oneway signalprocessing delay of about 80 milliseconds.achieving a feasible delay qos in the nii (and especially its global extensions) acceptable to the mostcritical applications will require major attention in the design phase and coordination among the network serviceproviders. past and present trends are not encouraging in this regard, as many network technologies developednominally for a limited geographical area have unwittingly introduced substantial delays.another troublesome observation is that qos guarantees will require dynamic coordination among networkservice providers at connection establishment. a typical connection instance will span at least several networkservice providers, and possibly many more, for example, localarea network and metropolitanarea networkproviders at both ends and a longhaul provider. qos parameters like delay, loss, and corruption will be affectedby all the providers' networks; however, the user cares only about endtoend qos. achieving endtoend qoswill require an allocation of impairments among the providers. such an allocation should be dynamicallydetermined at establishment, since a static allocation will require that all networks provide a qos appropriate forthe worstcase scenario, an expensive proposition. the only practical approach appears to be dynamic allocationmechanisms that relax qos objectives for individual links to fit the circumstances, such aswhat the nii could be: a user perspective385the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.local congestion or wireless access. there are no such mechanisms in place, nor a credible process to establishsuch mechanisms.security and privacya weakness of some current networks, particularly wireless ones, is lack of security and privacy. it isevident, for example, that insufficient effort has been devoted to this in cellular telephony networks in northamerica, as evidenced by the ease of eavesdropping and the widespread theft of service. this becomes an issuefor both users and network service providers. from a user perspective, the following characteristics of the niiare important: freedom from casual eavesdropping; the capability to make eavesdropping infeasible for sensitive applications, acceptably at extra cost; freedom from theft of services (obviously of interest to service providers as well); and inability to surreptitiously track the identity or movements of users.achieving all these goals requires careful attention in the design phase of the nii. as an example,transcoders already introduced in cellular telephony preclude privacy by endtoend encryption.application scalability and configurabilityas previously mentioned, the maximum benefit will accrue to the user if new applications can be freelydeployed and made available to all users, regardless of their terminal capabilities and the transport facilitiesavailable. in this model, the application will be dynamically configured to fit the environment (terminal andconnection instances), attempting to achieve the best quality consistent with the limitations. examples include: scalability to the connection qos. for example, a video application may be configured to lower resolutionor subjective quality in the case of wireless access, as opposed to a backboneonly connection. it is notdesirable for the user that an application is precluded by, for example, a wireless access; rather, the userwould prefer that some qos parameters (and thereby subjective quality) be compromised. scalability to the terminal capabilities. for example, a video application will be configured to a compressionalgorithm requiring less processing (trading that off against lower quality, resolution, or greater transportbandwidth) should the originating or receiving terminal instances have limited processing. it is not desirablefor the user that applications be limited to terminals provided by particular manufacturers or with particularcapabilities.dynamic configuration requires scalability and configurability of all aspects of the application. it alsorequires a rich signaling and control environment that passes to the application all the information needed toscale to the environment. the mechanisms described above for negotiating and configuring qos parameters ofthe transport at establishment do not by themselves provide needed information about terminal capabilities.thus, there need to be standardized signaling capabilities among the terminal instances at establishment.pricingthe pricing model is a key to the desirability and viability of applications in the nii. it is ultimately in thebest interest of the users that both network and application service providers derive revenue related to their costs.this is a difficult issue because of the great heterogeneity of networks and applications.if the nii provides qos guarantees as described previously, there must be a coupling of pricing and thecost of resources reserved to provide the qos, since otherwise applications will always request the highest qualitywhat the nii could be: a user perspective386the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.available. since the cost of provisioning a given qos will also depend on current traffic conditions, it isdesirable that pricing be traffic dependent. many connections will involve two or more network serviceproviders, each provisioning identical rate parameters, but possibly contributing quite different impairments suchas loss and delay to the endtoend qos (based on their technology, local traffic conditions, etc.). those networkservice providers should derive revenue that is related to their contribution to endtoend qos, since otherwisethey will all have an incentive to fully consume the endtoend impairment objectives.thus, we conclude that the pricing to the user and division of revenue should be established based on therate parameters, the contributions to the impairments of the individual network service providers, and localtraffic conditions. this requires a complex negotiation between the application and a set of network serviceproviders to establish an endtoend qos that achieves an appropriate tradeoff between price and qos, and apartitioning of that qos among the network service providers. one approach is a broker that mediates among theapplication and all potential network service providers. a desirable feature of a brokerage system from the userperspective is that all available network service providers could be considered, choosing the set of providers thatis most economic based on their current traffic conditions and pricing strategies.conclusionslooking at the nii from a user perspective, we can identify some key challenges for the future: to meet a wide range of application needs and provide flexibility for the future, individual network serviceproviders and their equipment vendors need to take a general perspective, as opposed to developing anddeploying technologies defined for narrow currently defined applications. major cooperation is needed among network service providers to coordinate their design and deploymentstrategies in areas like endtoend transport protocols and signaling capabilities that allow dynamicallocation of endtoend qos impairments, support scalability and configurability of applications, andprovide desired levels of privacy and security. overall planning is needed, with specific action on the part of individual network service providers, to besure that nearterm decisions do not compromise endtoend qos objectives in the nii and especially itsglobal extensions.the greatest challenge in the nii is to allow for and encourage a variety of technologies, applications,network service providers, and applications service providers to coexist in a dynamic environment, whilesatisfying the user's desire for interoperability, openness to new applications, and acceptable levels ofperformance. this will be possible only with initial planning and coordination and ongoing cooperation amongall parties involved.what the nii could be: a user perspective387the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.46role of the pc in emerging information infrastructuresavram miller and ogden perryintel corporationwhile most of the talk about the future national information infrastructure (nii) has focused on interactivetelevision, the combination of the personal computer with online services and the internet is rapidly achievingthe promise of ubiquitous information access for education and entertainment, electronic commerce, and work athome. while telephone companies position themselves to enter the television delivery business and cablecompanies attempt to provide telephone services, the foundation for the information age is actually being builtby the computer industry. the thesis of this paper is that if we are to establish an nii of any substance, it will bebased on the interconnection of personal computers with online services and the internet. pcs, not televisions,will be used by people to communicate with each other. unfortunately, a widely held belief that personalcomputers serve only a small, technically savvy and wealthy elitešcoupled with concern that computers are toodifficult for mere mortals to usešhas mistakenly put the spotlight on creating interactivity on the television.this, compounded with the fact that tvs are present in virtually every home, has resulted in some erroneousconclusions. in fact, television was designed for one thing only: moving pictures. it is a poor device for handlingthe high resolution static images required for a high degree of interactivity. with the possible exception of videogames (which serve a relatively small segment of the market), it is primarily a passive device. the fact that thereare so many tvs is not relevant. if it were, we would have long ago considered ''interactive radio." the numberof televisions has become a false beacon for media companies, communications companies, equipment suppliers,and government policy makers.since the technological and social infrastructure for interactive television does not exist, and its creationwould be a truly awesome task, government has stepped into the breach, preoccupying itself with fostering itsexistence and getting involved in the processes by which it will evolve. issues of interoperability standards,competitive markets, and universal access have become matters of public policy. this is further complicated bythe fact that television and telecommunications services are regulated, implying the need for regulation ofinteractive television and, by implication, the information superhighway.it is our contention that this approach of imposing the television as the interactive communication devicestandard is based on a set of mistaken assumptions. the fact is that the ubiquitous information device exists now,in the form of the personal computer. in fact, while the government and the telecommunications companiesargue about how best to deregulate the telecommunications industry, consumers are voting with their pocketbookby purchasing personal computers, the only available interactive information device, at record levels. the rolemodel for the diffusion of personal computers into society may well resemble the adoption of the automobile: itwould have been difficult to imagine in the early days of the "model t" that 88 percent of american householdswould have one or more automobiles. and in the case of the personal computer, business to a large extent isunderwriting both the investment in developing the networks (the role the government had to play in developingthe highway system for the car) and, very importantly, the training of a large number of consumers in the use ofcomputers (approximately 30 percent of the u.s. labor force and 47 percent of the white collar labor force use acomputer at work, and most children learn to use computers at school).telephone and cable companies have been regulated for most of their existence. as a result, they are wellpositioned to influence government policy with respect to the information superhighway. the computer industry,role of the pc in emerging information infrastructures388the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 pc installed base in u.s. businesses and government (millions of units). source: data from intelcorporation; infocorp.on the other hand, has had almost no regulation and little to no involvement with government. those of usin the computer industry need to do our part in demonstrating to our policymakers that the informationsuperhighway is already under construction. it is being built primarily on pc technology by private industry,driven by competitive market forces and with little, if any, need for government regulation.there is a role for government, however; that is to recognize and embrace the personal computer. this canprovide america with a foundation for the next century that will improve not only the quality of our lives butalso the productivity of our society and the competitiveness of our industries. the government can do manythings to move this along. it can facilitate access to computing in schools, libraries, and community centers. itcan become one of the largest (maybe the largest) content provider of online information. it can encouragebusiness to promote telecommuting. the most important thing the government can do is to recognize what ishappening. however, we caution against interference with market forces. the market moved the personalcomputer industry forward at a phenomenal pace, and we encourage following this model rather than the heavilyregulated telecommunications model, which has proven slow to evolve and respond.the personal computer industryits historywhile today the primary use of personal computers is in business, it is interesting to note that they wereoriginally conceived of as a consumer product. the first personal computers were designed either for hobbyistsšthe altairšor for consumersšthe apple ii. even ibm broke with its tradition of providing business products(the b in ibm) when it introduced the pc in 1981. this machine, which is the ancestor of over 85 percent of thecomputers sold today, actually had a game port for joysticks and an audiocassette for storage. industry leaders atthe time, such as ken olsen, then ceo of digital equipment corporation, openly referred to personal computersas "toys." few realized that this toy would completely restructure the entire computer industry within 10 years.the personal computer overtook the mainframe with its terminals as the information tool of business sometime during the 1980s. it is interesting to note that although there are clear reasons for this success, themainframe was not without its merits. as a centralized facility, a mainframe is easy to control. each user hasaccess to the same software, and support is easy. another clear advantage of mainframes is that since they are ashared commodity, it is easy to manage capacity to match the average expected load. on the other hand, underpeak usage, all users typically experience slower performance. probably the main reason for the rapid decline ofthe mainframe, however, is the slow pace of progress in both hardware and software performance.while there are many reasons for the success of the pc in business, the most important is its evolutionarynature. the "openness" of the pc allowed for rapid innovation. the open bus inspired hardware companies toadd value to the basic pc. they experimented in the marketplace. successful additions were then integrated intothe main computer. it is hard to imagine that the first pc had a game port built in, while the printer port wasoptional. application software could be created by small companies. companies like lotus and wordperfectgrew from one product, while microsoft took an early lead in the operating system and novell provided the nextwork environment. while this environment was, and still is, chaotic, it provided for rapid evolution. no industryrole of the pc in emerging information infrastructures389the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2 percent of business pcs with lan connections.source: international data corporation.or government organization dictated the direction or set the standards, yet standard interfaces evolved out ofeconomic necessity. the customers rewarded companies for innovation. when a company won in the market,others would accept its standard as the de facto standard and would seek to integrate their products into it. in theprocess, the personal computer became the tool of business. the adoption of personal computing into business inthe united states was rapid (figure 1).this was followed by the growth in computer networks (figure 2).electronic mail has followed word processing and spreadsheets as a key business application. this hasmade the pc a fundamental communication device. while email has been historically used within a company,more and more companies are using it as a way of communicating with the outside world (customers, vendors,and partners). now many individuals are finding that they can also send notes to friends and relatives. email isonly the first example of the successful marriage of personal computers and the nii: communication betweenpeople through the computer. at intel, email is a "mission critical" application; the corporation functionsthrough the rapid transaction of issues and ideas using the worldwide electronic mail system.the term "social computing" is gaining currencyša poor term no doubt, but a powerful concept. and whilestandard bodies were formed to address the methods by which different electronic mail systems wouldcommunicate (x.400 for instance), the internet became the lingua franca of email. now, as businesses connect tothe internet to send and receive mail, they are also discovering that they can access a wide variety of informationon the internet.more and more companies are beginning to use the world wide web to communicate with the outsideworld. the growth of the internet is staggering. it is estimated that 10,000 companies are adding their presenceto the internet each week.the growth of the home marketthe growth of computing in the home has been a surprise even to those in the industry. figure 3 showshistorical home penetration in the united states. this year it is expected that 10 million personal computers willbe sold to consumers for use at home.though these numbers are impressive, they do not do justice to the number of consumers that usecomputers either at work or at school. while it may be true that the use of computers is not easy to learn (is theuse of automobiles?), computers clearly satisfy a compelling need. for the first time last year, consumers spentmore money on computers than on tvs and vcrs combined, making the pc the most successful consumerelectronic device. another indication of the success of the personal computer in the home is that of the more than6 million pcs sold to consumers in 1994, about 25 percent were additional computers to homes already owningone or more pcs.in the early 1990s a critical mass of home pcs equipped with modems enabled an explosion in a newindustry: the online marketplace (figure 4). customers are now finding that much of the value of having apersonal computer resides in the ability to connect with other users and with a large number of online services.role of the pc in emerging information infrastructures390the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 3 pc installed base in u.s. homes (millions of units).source: computer retail week.figure 4 online connections in the united states (total = 8 million). source: rocky mountain news.it should be noted that although over 30 percent of u.s. homes are already equipped with pcs (a ramp upfaster than many consumer electronic products), the growth shows no sign of slowing down. on the contrary,with other countries joining in, worldwide shipments of pcs are expected to grow from 60 million units this yearto over 100 million annually by the year 2000 (figure 5).figure 5 worldwide pc shipments forecast (millions of units).source: international data corporation.today's capabilitiesthe power of today's computer is mindboggling by standards of just a decade ago. the typical computerbeing purchased has a powerful microprocessor (typically a pentium) with computing power equal to themainframe of just 6 years ago. this, combined with a high resolution display, a cdrom, and cd quality audiosound provides for a rich multimedia experience. today's pc uses compression to display video that matches thequality of a typical vcr, with only 0.5 percent of the information required for an analog video stream. thedecompression of the data is performed in the microprocessor, without any specialized hardware. in addition, itcan act as a fax machine and provide access to online networks and the internet. it is used for a variety ofpurposes from entertainment to education, work at home, and home shopping.role of the pc in emerging information infrastructures391the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.tomorrow's capabilitiesthe capabilities of the personal computer are improving at a rate that no other device has ever experienced.historically, the power of a pc has doubled every 18 months. this rate has recently increased, and thecapabilities are now doubling every 12 months. what will we do with such computing power? here are some ofthe most obvious areas of use: improvements in ease of use, including natural language input and output, powerful 3d graphics, outstanding audio/video capabilities, video telephony, and multitasking (doing several things at the same time).the structure of the industryhistorically the computer industry was vertically integrated. computer companies did everything fromdesigning their own power supplies to developing application software. each "vertical stack" was unique andproprietary. the pc changed all that. the pc industry is basically horizontally integrated, with literallythousands of companies delivering compatible products at every level of integration and in many areas ofapplications.the result of the horizontal nature of the computer industry is fierce competition at every level in bothquality and price. this competition benefits customers, who have access to the highest performance at the lowestprices. de facto interfaces have emerged at the intersection of the various segments. periodically, ad hoc industrygroups are formed to cooperate in creating new interfaces. an example is the pci bus, which has become thestandard for the highspeed internal bus of personal computers.the cable television industry still operates in the traditional vertical organization. cable companies are theonly suppliers of both equipment and services, and most consumers don't even have the opportunity to buy theirown equipment. as a result, equipment is typically not interoperable between different companies, technicalprogress is slow, and prices do not decline. this slow progress, in turn, stands in the way of improvements inother parts of the value delivery chain (such as the pictureinapicture feature that is disabled by most settopboxes). as a result, government has recently stepped in to regulate both function and price.key differences between the television and personal computerindustriestelevisions use crts to display moving pictures. most personal computers also use crts. this fact sumsup the essential similarity between these two devices. the fact is that in their purpose, their features, theirevolution, the way they are sold, and, perhaps most importantly, the way they are used and are connected to theoutside world, televisions and personal computers have very little in common.purposethe television was designed to bring visual entertainment, from the movie theater and the stage, into thehome. this emphasis on the visual is seen, for example, in the fact that until recently the quality of televisionspeakers was far below that of other consumer electronics devices and that, even today, few programs arebroadcast in stereo. to its credit (depending on your point of view), the television also enabled some new formsof entertainment such as game shows and brought us news footage. a common element to all these experiencesis that they are typically enjoyed in the family room, from a comfortable distance, in a reclined position. the pc,on the other hand, was originally designed as a work tool and a platform for gaming. two of the first applicationsrole of the pc in emerging information infrastructures392the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.for the pc were lotus's 123 spreadsheet and microsoft's flight simulator. although these applications wouldseem to be very different, they both require the user to sit close to the pc and to interact with it on a continuousbasis (whether the device used is a keyboard or a joystick is not important).featuresthe ntsc standard for television broadcast in the united states specifies 483 lines of video. in practicemost broadcasts and most vcrs are capable of only about half that. by contrast, the minimum resolution for anypersonal computer monitor sold today is 1024 × 768, with many exceeding even that. there are many reasonsthat explain the "poor" resolution of televisionsšthe most significant being that it is adequate for the purpose athand and the cost of upgrading the system is huge. but resolution is only one of the differences. probably muchmore crucial is the fact that the pc is inherently an "intelligent" device capable of processing information locallyand of interacting directly with the user. the pc also has local storage that can be used for programs, data, video,or whatever the user thinks is important.evolutionbut the single most important difference between the tv and the pc has nothing to do with any one featurebut rather with the fact that the pc is constantly changing, in effect adding features on a regular basis. for thosewho live in the world of telecommunication and television where things move very slowly (witness integratedservices digital network and highdefinition television [hdtv], the pc hypergrowth may be difficult tocomprehend. now that hdtv is being proposed, the tv is being asked to do something it has not done in 50years: evolve. with the exception of screen size, televisions have remained fundamentally the same since theintroduction of color tv almost 40 years ago (even that change was a painful one that resulted in a compromisethat still hurts the picture quality of television). while some features have been added to television, and the vcrgave it a limited linear memory, most new features are seldom used.interactivitywe define interactivity as an activity where individuals are fully engaged in a process. interaction involvesan interplay between people or between people and devices such as a computer. we wish to contrast that withcontrol and navigation. while there may be a brief moment or two where we interact with our remote control andtv in order to select a program of interest, success usually means long stretches of passive involvement with theprogram of choice. while the telephone can be used for some form of interactive information retrieval or even toperform a transaction by using the keys and following a menu of choices, most would agree that this is not a veryenjoyable experience and is unsuitable for sustained periods of time. this should be contrasted with the pc, adevice with which individuals routinely interact for hours at a time.connectivitythe tv was built from the start to be a "communications" devicešalbeit oneway communications. afterall, a tv without a connection to the outside world is useless. in fact, it is the television network (especiallycable) rather than the tv itself, that has expanded to offer more services. so it is this paradigm that hasmotivated the communications companies to approach the problem of providing interactive service to americanconsumers in a similar way. it is therefore very ironic that the pc, which gave users the independence to work ontheir own and to break away from the mainframe, is, in fact, the superior communications device. this is becauseon the pc, which is capable of producing, processing, and storing information, communications evolved fromthe very start as a twoway interaction.role of the pc in emerging information infrastructures393the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the tv is easy to use only if it is used in its simplest form. to make the tv more intelligent in order to beinteractive, something has to be added to it. this is typically called an intelligent settop box. the idea is to addto the tv many of the functions of the personal computer and the game machinešbut at a price that is consistentwith television pricing (typically, $300 is the target). since $300 does not buy a lot of computing power, much ofthe intelligence needs to be somewhere else in the network where it can presumably be shared. if this soundsfamiliar, it is because it copies the mainframe/dumb terminal concept. the computer industry has the advantageof having seen that movie and of being familiar with its pitfalls. since the intelligent pc was so successful inreplacing the mainframe more than 10 years ago, there is no need, this time, to go through the same steps.a word about the telephone industrymuch of what was said above about the television is also true of the telephone. this device, too, has beenslow to evolve (touch tone being the lastšand probably firstšbreakthrough in the device itself). here too, it isthe network that is responsible for most innovations such as 800 service and call waiting. the awaitedbreakthrough on the telephone side (equivalent to hdtv) has been some form of video telephony. all attemptshave failed because the cost and quality have not been there. on the other hand, personal computers have offeredvideo telephony, incorporating application sharing, on isdn for some time. it is expected that the personalcomputer's current trajectory will allow it to offer a marketable quality video telephony product over plain oldtelephone service (pots) within the current year with the simple addition of a camera.as the television and telephone industries eye each other's heartland but consider the massive cost of doingbattle with each other, they each hope to create an advantage by offering a new class of service: interactive tvservices. unfortunately, they both have a poor track record in evolving their business. and this time they mustbuild their new infrastructures in a matter of years, not decades.how will the pc become ubiquitousat the current rate of purchase, between 50 and 60 percent of homes will have computers by the end of thedecade. while that is an amazing penetration, the other 40 percent need to be addressed. this group can bebroken up into those who would like to have a computer at home and those who have no interest. there is anobvious economic barrier facing a portion of the people who would like to own a computer. similarly, whilealmost everyone has access to public transportation, not everyone owns a car. we strongly believe that access tothe information highway via pcs will be made available to all people via computers at schools, communitycenters, libraries, and other government and private sites where the cost is spread over many users.this "economic barrier" should not be construed as an opportunity for the television to evolve intointeractive television. we believe that the cost of interactive tv (both the device itself and the network) wouldlimit its availability to pockets of highincome families. since the pc is a multifunction device, its cost is, in fact,less of an obstaclešas can be seen by its fast ramp so far.the following key characteristics of the personal computer industry ensure that the pc will continue to bethe primary interactive device of the information superhighway: it is a highly capable device designed from the start for interactivity. its adaptability allows for rapid market experimentation with the market setting the standard. businesses have invested trillions of dollars in a computer infrastructure. consumers, in turn, have benefited from this investment in the form of training, cheaper equipment, andaccess to infrastructure. the internet, which is the only national information infrastructure, was specifically built for and withcomputers and is a common network for business and consumers.role of the pc in emerging information infrastructures394the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.requirements for niithe first requirement is an access point with an interactive device. by this we mean interactivity as definedby the current and evolving personal computers, as opposed to improvements in navigation and controlcapabilities of tv. we assume that pcs will not only become easier to use but will be able to handle "rich"media (video, audio, images, in addition to text), with increasingly better quality.next, we need a communications infrastructure that connects these access points to services and to eachother. this connection must provide adequate bandwidth in order to enable the use of rich, multimedia datatypes. the current pots environment, even with the latest 28.8kbps data rate, is extremely limiting. isdn is ashortand intermediateterm opportunity to increase bandwidth to the consumer. over time, broadbandcapabilities will be needed. it should be noted that bandwidth symmetry is not required as long as "adequate"bandwidth is available in the return path. for example, the current pctocable connection experiments, whilehighly asymmetric in bandwidth, would be quite adequate for accessing existing internet sources. clearly, wewill need higher bandwidth in the return path if we are to expand beyond current modes of use and allow everyuser to become a contributor of rich data.in an important departure from the current circuit switched networks that connect most businesses andindividuals, the network for the nii needs to be packet switched. this requirement is borne out of the desire foran "always on" mode of operation and for the ability to have multiple sessions active at the same time. theinternet is packet switched, but much of the potential is lost in the circuit switched pots network that connecthomes to the internet over the "last mile."if we are to broadly utilize the nii for applications such as electronic commerce, security considerations aregoing to become important. however, we consider this to be an issue between the two end points of thetransactions and not of the network. current industry efforts, we believe, will solve this problem quite adequately.what can/should the government do and not dothe government should do the following: recognize that the pc is the interactive device and that the internet is the information superhighway. whileneither is perfect for the ultimate tasks for nii, they are the closest in satisfying our future needs.furthermore, they are open standards with industry momentum behind them; they are the winners of thisdarwinian process of evolution. encourage employers to make work at home a major thrust. telecommuting will have economic, social, andenvironmental benefits. it will also provide computers at home for many who might not be able to affordthem otherwise. develop programs making networked computers available to all americans via schools, libraries,community centers, and other government facilities. encourage a competitive communications environment with specific advantages to those who provide thelowcost, twoway, high bandwidth required by personal computers at home. avoid trying to set standards or dictate economic behavior. the government should also avoid anyprocedures that slow down the creation of de facto, marketdeveloped standards. encourage states and municipalities to deregulate the local loop so that advanced telecommunicationservices can be made more readily available to the consumer. isdn, for instance, should not be looked uponas something for the technorich, but as a service available to all americans. finally, capitalizing on the enormous amount of information it collects but has not been able to redistributeefficiently, the government should become a major provider of content and services on the internet.role of the pc in emerging information infrastructures395the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.summarythe ubiquitous interactive device is here, and it is the pc. while telephone companies and cable companieshave focused on television and telephone, the consumer pc market has gained momentum. pcs will be in amajority of homes by the end of the decade. the internet will provide for an open information environment.government should understand and facilitate these trends. the computer industry has to play an active role inthis process.role of the pc in emerging information infrastructures396the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.47nii evolutionštechnology deployment plans, challenges,and opportunities: at&t perspectivemahal mohanat&t corporationstatement of the problemthis paper presents an at&t view of the evolution of the national information infrastructure (nii). as aleading supplier of longdistance communications services and wireless services and a major force in theresearch and development of communications and information technologies used by customers and serviceproviders, at&t has a combination of perspectives on this topic that is unique in the industry.i briefly review the current state of information infrastructure in terms of certain key attributes, outlineat&t's key new technology deployment initiatives, and recommend actions by the public sector to facilitatefaster, smoother evolution of the nii.backgroundseveral attributes of emerging communications networks offer promise for supporting new modes ofcollaboration, information access, and exchange: bandwidth and the physical medium and technology used to provide the bandwidth; the set of intelligent features that enhance the convenience or usefulness of the networking capability orservice; messaging, involving storage, processing, and forwarding of information in different forms; mobility, enabling users to communicate from anywhere using wireless technologies and personal reachnumbers; and interoperability and openness, which is an enabler for competition and rapid, ongoing innovation in each ofthe above areas.bandwidth, physical media, and technologiestwo prominent trends in the current evolution of communications networks are digital technologydeployment and optical fiber deployment. most longhaul backbone communications networks have evolvedfrom analog to digital transmission. digital deployment in local loops to end users, however, has beenproceeding slowly. however, the recent increase in the deployment of integrated services digital network (isdn)services shows some promise for bringing the benefits of digital transmission all the way to the end user,resulting in the provision of expanded bandwidth with superior performance characteristics.during the last decade, optical fiber capable of supporting large bandwidths (ranging from tens of megabitsper second to several gigabits per second), with clear, nearly errorfree transmission over long distances, hasbeen extensively deployed in backbone communications networks that support aggregate traffic from manyusers. beginning in the early 1990s, the fiber deployment has extended closer to residential neighborhoods andbusinesses, mostly still supporting aggregate traffic from many contiguous user locations, and in some casesnii evolutionštechnology deployment plans, challenges, and opportunities: at&t perspective397the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.supporting aggregate traffic from a single large business location. the ''last mile" that delivers information to orfrom individual locations has some dominant technologies today. these are twisted pair copper typicallysupporting symmetrical, twoway, narrow bandwidth (less than 56 kilobits per second) analog transport or, lesscommonly, medium bandwidth (56 kilobits per second to 1.5 megabits per second) digital transport; and coaxialcable, typically supporting broad bandwidth (the equivalent of 1.5 megabits per second or higher) analogtransportšmostly oneway, with twoway expansion widely planned.intelligent featuresintelligent features constitute a broad category, and one that tends to get less than the attention it deserves inmany discussions of the next generation infrastructure. a classic example of such a feature in voice (and somecircuitswitched data) networks is 800 number service (and the myriad related services that have emerged in thepast decade that involve database or routingtable lookup and translation in the network). the tremendous utilityof such services is demonstrated by the degree of their widespread use today, mostly for voice services. in thecase of data and multimedia networks such a concept again applies, albeit with different implementation details.directory services, database services, and networkbased security services are examples of intelligent servicecapabilities that vastly enhance the value of the underlying connectivity to users. currently these features areoffered in rudimentary form as part of data and multimedia services, often to a limited base of users. asdescribed below, efforts by at&t and other industry players are slated to substantially increase the deploymentand use of these features beginning this year, expanding rapidly over the next several years to offer more robustand useful sets of features supporting a broader user base.messagingmessaging, involving the storage, processing, and forwarding of information, is becoming widespread andaccepted as a mode of information exchange. voice messaging using premisesbased equipment such asanswering machines or computerbased voice mail systems is common now. networkbased voice messagingservices are available in some areas but are less widely used; their features, functionality, and price structureneed to evolve further to provide fullfledged competition to premisesbased systems. data messaging, or email,is now widely used in corporations and is used by the more technically oriented consumers. substantial progressneeds to be made to provide simplified user interfaces, build user awareness, and provide user training, before email can become a commonly accepted form of information exchange for a broad cross section of society. texttospeech conversion and vice versa are being actively worked on in research laboratories, with earlyimplementations being used in today's commercial applications.mobilityone of the major trends in communications during the 1990s is the explosive growth of wireless services.driven by the needs of a mobile society, greater availability of wireless spectrum, and technologies that allowincreasingly more efficient and costeffective use of the spectrum, wireless services will continue to expandrapidly.another trend in serving mobile users is the concept of a personal number that follows users no matterwhere they are, if they wish to be reached. the first generation of such services has been available for a fewyears. the next stage in their evolution is likely to link wired and wireless access to a user via a single number.nii evolutionštechnology deployment plans, challenges, and opportunities: at&t perspective398the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.interoperability and opennessthis attribute is worth singling out because it leads to competition, increased user choice, and innovation.standard, open interfaces between local telephone networks and longdistance networks, and betweeninformation appliances and telephone company networks, have enabled substantial competition in the longdistance communications and customer premises equipment industries, even while local communications ispredominantly provided in a noncompetitive environment today. open platforms have likewise facilitatedvigorous competition and innovation in many facets of the personal computer industry. as we are poised on thethreshold of broader bandwidth services and an expanding range of information services, industry consensus on,and implementation of, an expanded suite of open, critical interfaces are of vital importance.examples of interfaces that are open today are the customer premises device interface to local telephonenetworks, and local telephone network interfaces to long distance communications networks. typically closedinterfaces today include the cable network interface to settop devices at homes, and the cable network interfaceto electronic content or programming.at&t's plans, challenges, and opportunitiesthis section reviews some of at&t's key new initiatives and plans in the areas of communications servicesand information services. it concludes with a discussion of issues that industry, users, and governments need towork together on to create a framework for rapid growth of the national information infrastructure (nii).as its corporate mission, at&t is dedicated to being the world's best at bringing people togetheršgivingthem easy access to each other and to the information and services they want and needšanytime, anywhere.as indicated above, at&t has multiple roles in the evolution of the niiša major longdistance and globalservice provider; a major wireless service provider; a product vendor for builders of the communicationsinfrastructure; and a provider, to consumers and businesses, of information appliances.communications services: initiatives and directionscommunications services lie at the core of at&t's business. our worldwide intelligent network providesthe bedrock on which we are building a wide variety of communications services, driven by ever moredemanding user needs. over the past decade, our worldwide network has been transformed into one that carriesthe vast majority of its information in digital form. interconnecting highcapacity digital switches are highcapacity fiberoptic links that offer a combination of large bandwidth and clear, errorfree transmission. toprovide the highly reliable services needed by today's users, we have developed and deployed systems such asfastar to reroute and restore facilities handling hundreds of thousands of calls automatically, within seconds,in the event of any malfunction or failure in any portion of our network.using the worldwide network as a basic platform, we are building families of communications services,aimed at businesses and consumers, that meet the specific needs of different user segments. these services aredifferentiated, each in terms of features discussed in the previous section, namely bandwidth, intelligent features,messaging capabilities, mobility, and openness and interoperability.let us begin with a description of communications services for businesses, because many leadingedgeservices are first introduced to business users and, whenever appropriate, are adapted to address consumers athomes.transport technologies and servicesšalternativesusing isdn digital transport, at&t has been offering video communications services, ranging frompersonal conferencing on a desktop to group conferencing with a variety of speeds and picture resolutions. thenii evolutionštechnology deployment plans, challenges, and opportunities: at&t perspective399the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.most recent developments in this service family, known as at&t worldworx solutions, are global reach andmultipoint capability. it is now possible to conduct video and multimedia conferences between, for example,washington, singapore, and paris simultaneously, using a mix of desktop computers and group videodisplaysystems.highspeed packet transport technologies, known as frame relay and cell relay technologies, have emergedin recent years and are spreading rapidly in use. for instance, we offer a frame relay transport service (interspanframe relay service) that connects computers at high speeds using virtual circuits that can be reconfigured asuser needs change. to address user needs for simultaneous communication of voice, text, image, and video athigh speeds, we have recently begun offering interspan atm service, based on asynchronous transfer mode(atm) technology, which is emerging as a worldwide industry standard for multimedia networking.a discussion of atm from our perspective is not complete without a mention of product developmentinitiatives. at the core of our atm service is an atm broadband switching product we have developed atat&t bell laboratories called globeview2000. globeview2000 is one of a family of atm switches underdevelopment that will enable integrated switching of multimedia signals. atm technology is experiencing rapidgrowth in the field of local area networks (lans). we believe that it has the potential to be a new unifiedinfrastructure for communications, coexisting for decades to come with the current large embedded base ofcircuitswitched technology.intelligent features and collaboration toolsisdn, frame relay, and atm services, as outlined above, offer transport alternatives that can interconnectand interwork with each other, and support highbandwidth applications. communications services are alsogrowing in terms of features such as directory, security, and user interfaces. for example, this year at&t isintroducing a new family of public data services that will build on and expand the capabilities of the advancedtransport services mentioned above. these will provide "multimedia dialtone" and offer a flexible applicationsenvironment for innovators. the public data services will enable companies to go beyond having sophisticatedinternal ("private") networks, and to connect to their suppliers and customers with data and multimediainformation. at&t netware connect services and at&t network notes are part of this new family that weare beginning to beta test with selected customers and that we expect to make widely available starting later thisyear.we are developing at&t netware connect services in collaboration with novell, inc. the service willenable highspeed lan access and interconnection, both within an enterprise and between enterprises. it willconnect to the global internet but will offer much higher levels of security, ease of use, and directory anddatabase capabilities. by being an open platform with standard interfaces, this service will in turn become theinfrastructure for new services, of which at&t network notes is an example. at&t network notes, which weare developing in collaboration with lotus development corporation (now part of ibm), incorporates lotusnotes, the popular workgroup collaboration software, within the at&t network. as a result, farflung workgroups can work together on shared documents, incorporating multimedia information into them. users can relyon the at&t network to update the software and to readily incorporate new features. directory services,multiple levels of user security, and navigation capabilities will be part of the common services platform and willbe offered with new services as we develop themšindividually or with partners in the information industry.taken in combination with a variety of internet services for businesses and consumers that we will be offeringsoon, these represent a service creation platform for a new generation of data and multimedia services.consumer service and product initiatives and challengesthe description above outlines some of the emerging communications service options for businesses toenter the multimedia networking era. what about consumers? first, businesses often tend to be early adopters ofleadingedge services, and the services and technologies are subsequently adapted for use by consumers. second,nii evolutionštechnology deployment plans, challenges, and opportunities: at&t perspective400the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.many of our business communications solutions involve businesses communicating with their customers, whooften happen to be consumers. for example, tollfree 800 number services are business solutions, but the peoplewho make the 800 number calls are mostly consumers who have come to accept the services as an integral partof their daily lives.there are significant challenges in providing multimedia services to consumers on a broad scale. the mostnotable is the limited availability of twoway broadband access for connecting to our highbandwidth services.as is well known, cable tv networks offer broadband connections to many homes today. what restricts theirutility for most interactive multimedia services is the fact that they are currently offered as part of a closed,horizontally integrated scheme; in other words, they reach many homes, but they are generally not open toconnect to everything we would want them to connect to. in addition, the access links are generally designed foroneway delivery of analog information. they also have a history of modest reliability and performance, thoughsignificant new investment is going into installing fiber optics closer to clusters of homes to aggregate coaxialcables and to provide improved performance.in our role as a major provider of telecommunications infrastructure products and capabilities, we areactively working to bridge this gap. we are working with cable companies as well as telecommunicationscompanies to provide new solutions based on a combination of fiberoptic and coaxial cable links from homes toswitching or information hubs, combining high reliability with high bandwidth for interactive applications. weare providing technology and integration capabilities that are a major part of projects by telephone companiessuch as pacific bell, bell atlantic, and southern new england telephone, as well as by cable companies such astime warner, to redefine their communications infrastructure and to offer reliable, broadband access to homes inthe immediate future.two other areas in communications services deployment are worthy of note. the first is the rapid growthand digitalization of wireless communications networks. at&t wireless, created by our acquisition of mccawcellular, is investing substantially in creating an equal access mechanism for wireless access to all long distancecarriers; in expanding the reach of wireless services to serve major markets nationwide; and in expanding thedigital capability in our wireless access network and enhancing its ability to serve more users with higher servicequality. the second area is globalization. communications companies such as ours are entering into partnershipswith their counterparts in several foreign countries to offer integrated services and onestop shopping tocustomers.at&t initiatives and directionsšinformation resourcesthe two bestknown types of electronic information content today are (1) electronic online services, whichtypically provide digitally encoded information delivered through narrowband access networks and accessedusing computers, and (2) television programming, which is typically analogcoded information delivered throughoneway broadband networks and accessed using tv sets. the promise of digital convergence reflects the vastpotential that exists to create and store digital information and deliver it through twoway broadband accessnetworks.at&t is a newly emerging player in the provision of online services. our new at&t interchange onlinenetwork offers its subscribers a rich collection of online information services on a range of specialinteresttopics from different publishers. in addition to general news and reference services, interchange is workingclosely with publishing partners, such as the washington post, to offer online news and information for theirtarget customers. interchange's graphical user environment, hypertext links, and powerful searching capabilitiesmake it a precursor for online services that will utilize the emerging broadband access media.on another front, at&t bell laboratories has developed interactive tv technologies that enableconsumers to interact with the content that is delivered via their tv sets. video server technology, developedand delivered through ids, an at&t partnership with silicon graphics, inc. and time warner, inc., is capableof storing huge quantities of multimedia information in digital form in the network for interactive uses.an important aspect of making information resources useful for people is offering users the ability tonavigate among different applications, to help find and retrieve the kind of information that users need most, whennii evolutionštechnology deployment plans, challenges, and opportunities: at&t perspective401the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.they need it. to this end, we have recently developed at&t personalink service, using intelligent agenttechnology developed in collaboration with general magic, inc. the service enables users with a small handhelddevice to determine how specific kinds of information should be dealt withšfor example, highpriorityelectronic mail messages from chosen persons or on chosen subjects can be automatically forwarded to aspecified computer, and the user's paging device will be notified automatically to alert him or her of the message.on the other hand, lowerpriority messages can be sorted and stored for future retrieval, or forwarded as needed.we are at the very beginning of intelligent agent technology, and we expect that capabilities in this area and theiruse will grow rapidly over time.at the nexus of information resources and communications networks is an idea that we call hosting, and thisinvolves matching up information from a variety of content providers (information producers) with users(information consumers) no matter where they are. key to hosting are wide reach, with wireline and wirelesstechnology; open interfaces that interconnect multiple information resources with communications networks; andthe navigation technologies referred to above, enabling users to easily sort through and obtain information theyneed when they need it. we are incorporating these concepts as we develop new products and services, and weintend to continue to support the principles of open, public interfaces so critical to customers and so necessaryfor competitive markets. we are actively participating in the information infrastructure standards panel (iisp),which is an industry group sponsored by the american national standards institute to identify standards gaps intoday's evolving nii.challenges and uncertaintiesaccess alternativesaccess technology alternatives for broad deployment deserve special attention, because they are such afundamental enabler for many new service capabilities. consumer applications of many of the above serviceswill benefit immensely from the deployment of higher bandwidth access capabilities than those that exist todayfor supporting interactivity. the current installed base of access in telecommunications networks ispredominantly twisted pair copper supporting symmetric twoway, lowspeed analog transport. the currentinstalled base for cable tv access is coaxial cable supporting highbandwidth (hundreds of megahertz) analogtransport one way (downstream only).the most straightforward extensions of telephone network access involve leaving the twisted pair copperplant in place and digitizing the transport over them. using basicrate isdn, up to 144 kilobits of aggregatebandwidth can be brought to homes and businesses. isdn technology can thus support two multiuse (voice,data, or limitedspeed video) channels to the home and one or more packet data channels. these would enableaccess to information resources with text and graphics. basicrate isdn falls short of being suitable for fullmotion, large screen video applications. local telephone companies are beginning to offer basicrate isdn forresidential consumers, though price packages and ordering processes are complicated, and user awareness andtherefore "takerates" are limited.another existing technology for extending the bandwidth of twistedpair copper loops is asymmetricaldigital subscriber line (adsl). adsl involves installing matching equipment at both ends of the customer loopthat extend the bandwidth to the user substantially (typically ranging from 1.5 mbps to 6 mbps), while lessbandwidth (typically one or two 56 kbps channels) will be available for upstream communication. adsl's keyadvantage is that it can bring video services with some interactivity to users without replacing the embeddedtwistedpair loop; since the installation and changes occur at the ends of the loop, adsl devices can bedisconnected from one user and moved to another user as, for example, when the first adsl user decides toupgrade to a higher bandwidth medium such as fiber or fibercoax access.hybrid fibercoaxial cable combinations are gaining in popularity for new deployment. the reasons fortheir popularity for new deployment are that they offer abundant bandwidth capable of supporting integratedcommunications and entertainment applications; are comparable in initial deployment cost to new twisted paircopper access deployment when done on a large scale; and, by offering multiple services in an integratednii evolutionštechnology deployment plans, challenges, and opportunities: at&t perspective402the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.operation, administration, and maintenance environment, can help minimize ongoing costs relative to potentialservice revenues. by using the combination of fiberoptic feeders and coaxial cable "drops" to user locations, thestrengths of each of these media are maximized. for example, fiber provides clean, highbandwidthtransmission; but coaxial cable (unlike fiber) can be used to supply electrical power to user terminals from thecommunications network and is also more flexible and robust for ease of deployment.hybrid fibercoaxial cable combinations can be deployed in a variety of configurations supporting oneway,asymmetric twoway, and symmetric twoway services; these involve pure "bus" or "star" configurations, andcombinations of the two.as a developer of a range of access technologies, we believe that broadband access systems (such as hybridfibercoax) can be costeffective alternatives to twisted pair copper access for new installations. they can bedeployed for about $1,000 to $1,500 per home based on a broadscale installation. once deployed, they enjoy theadvantages of being able to support traditional telephony, traditional tv, and emerging interactive multimediaapplications on a single platform with a unified operations infrastructure. the actual deployment rate will dependon the extent of competition for provision of these servicesšwhich in turn will be determined by user needs andthe evolution of the regulatory climate.other challengesamong the obstacles and challenges, we have so far focused our attention on access capabilities and theirdeployment. we must also point out that there is significant uncertainty relating to user demand and willingnessto pay for new interactive consumer services. experience with early deployment of services will teach industry alot about how the emerging services should be priced, packaged, and offered to users. it is hoped that revenuefrom new access services will be adequate to recover the investment associated with deployment of new accessnetwork capabilities. based on preliminary analysis, this appears to hold true for integrated access capabilities.other uncertainties and challenges include laws and practices relating to privacy, security, liability, andintellectual property matters. an informed dialog between the public and private sector is essential to theemergence of appropriate public policy for the information age. this process has already begun, largely withinthe framework of the nii initiatives in industry and government.recommendationsthe challenge of developing the nii industries into a number of viable market areas and addressing userneeds with new products and services remains, and should continue to remain, the province of private industry.we in at&t, along with our counterparts in industry, are actively engaged in developing new capabilities andaddressing user needs. however, the public sector (at the federal and state levels) does have a key role to play inallowing these capabilities to develop toward their full potential in a speedy fashion.a major constraint on our ability to offer advanced service capabilities to consumers and small businesses isthe lack of availability of fullfledged access alternatives. the public sector needs to remove today's overt andsubtle impediments to the deployment and interconnection of competitive local exchange and access capabilitiesfor consumers and businesses. the transition from the monopoly provision of local telecommunications servicesto an environment of multiple competing and interconnecting providers needs to be facilitated by legislatorsrequiring the removal of constraints such as franchise restrictions, lack of interconnect points and standards,exchange resale prohibitions, lack of local number portability, and numerous other impediments. the publicsector needs to work with the private sector to develop criteria and metrics to determine when a market iscompetitive. regulatory efforts should be focused on opening markets to competition and doing so in a mannerthat inhibits the abuse of monopoly power where it exists. hand in hand with enabling the emergence ofcompetitive markets, the public sector needs to support industry in the development of open interfaces in criticalnii locations where interoperability is necessary, and support industryled standards for ensuring suchinteroperability.nii evolutionštechnology deployment plans, challenges, and opportunities: at&t perspective403the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the public sector needs to facilitate the ongoing availability of enabling resources such as wirelessspectrum, numbers, and rightsofway as new developments in the market strain limited resources. flexibilityand the ability to support marketdriven solutions should in general be the guiding principles in these areas.as a major user of the nii, the public sector needs to adopt open industry standards and leverage itsconsiderable market power as major commercial users would, to advance innovation. it should avoid creatingspecial networks and requirements without compelling reasons, as such efforts drain resources from themainstream development of products and services in the commercial marketplace.the public sector needs to enact laws that recognize the need for individual privacy and security ofinformation in electronic form, and that protect intellectual property rights for information created anddisseminated electronically. although the united states can lead these efforts by example, we must recognizethat these efforts are truly global in scope.regulatory efforts should be focused on opening markets to competition and doing so in a manner thatinhibits the abuse of monopoly power where it exists.nii evolutionštechnology deployment plans, challenges, and opportunities: at&t perspective404the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.48enabling petabyte computingreagan w. mooresan diego supercomputer centerstatement of the problemone expectation for the national information infrastructure (nii) is that information be readily available andaccessible over the internet. large archives will be created that will make various types of data available: remotesensing data, economic statistics, clinical patient records, digitized images of art, government records, scientificsimulation results, and so on. to remain competitive, one must be able to analyze this information. for example,researchers will want to "mine" data to find correlations between desperate data sets such as epidemiologystudies of patient records. others will want to analyze remotesensing data to develop better predictive models ofthe impact of government regulations on the environment. still others will want to incorporate direct fieldmeasurements into their weather models to improve the models' predictive power. in each of these cases, theability to manipulate massive data sets will become as important as computational modeling is to solve problems.a new technology is needed to enable this vision of "data assimilation." it can be characterized as "petabytecomputing," the manipulation of terabytesize data sets accessed from petabytesize archives1 theinfrastructure that will sustain this level of data assimilation will constitute a significant technological advantage.because of recent advances in archival storage, it is now feasible to implement a system that can manipulate dataon a scale a thousand times larger than is being attempted today. unfortunately, the software infrastructure tocontrol such data movement does not yet exist. an initiative to develop the corresponding software technology isneeded to enable this vision by 2000.backgroundadvances in archival storage technology have made it possible to consider manipulating terabyte data setsaccessed from petabyte storage archives. at the same time, advances in parallel computer technology make itpossible to process the retrieved data at comparable rates. the combination of these technologies will enable anew mode of science in which data assimilation becomes as important as computational simulation is to thedevelopment of predictive models.data assimilation can be viewed as combining data mining (in which correlations are sought between largedata sets) and data modeling (in which observational data are combined with a simulation model to provide animproved predictive system). these approaches may require locating a single data set within a data archive("data picking") or deriving a data subset from data that may be distributed uniformly throughout the archive.the latter requires supporting the streaming of data through datasubsetting platforms to create the desired dataset. therefore, handling large data sets in this manner will become dependent on the ability to manipulateparallel i/o streams.current capabilities are exemplified by commercial applications of data mining in which companiesmaintain "justintime" inventory by aggressively analyzing daily or weekly sales. the analysis of sales trendsallows a company to tune purchase orders to meet the predicted demand, thus minimizing the cost and overheadof maintaining a large inventory of goods. the amount of information analyzed is limited by current storage anddatabase technology. data sets up to 5 terabytes in size, consisting primarily of short transaction records, can beanalyzed.enabling petabyte computing405the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.examples of similar transactionbased data sets include bank check logging archives, airline ticket archives,insurance claim archives, and clinical patient hospital records. each of these archives constitutes a valuablerepository of information that could be mined to analyze trends, search for compliance with federal laws, orpredict usage patterns.the size of individual such archives is expected to grow to petabytes by 2000. part of the growth in size isexpected from the aggregation of information over time. but an important component of the size increase isexpected to come from incorporating additional ancillary information into the databases. clinical patient recordswill be augmented with the digitized data sets produced by modern diagnostic equipment such as magneticresonance imaging, positron emission tomography, xrays, and so on. insurance claim archives will beaugmented with videotapes of each accident scene. check archives will be augmented with digitized images ofeach check to allow validation of signatures.in addition, virtually all scientific disciplines are producing data sets of a size comparable to those found inindustry. these data sets, though, are distinguished from commercial data sets in that they consist predominantlyof binary large objects or "blobs," with small amounts of associated metadata that describe the contents andformat of each blob. a premier example of such data sets is the earth observing system archive that will containsatellitebased remotesensing images of the earth2 the archive is expected to grow to 8 petabytes in size by2006. the individual data sets will consist of multifrequency digitized images of the earth's surface below thesatellite flight paths. the multifrequency images will be able to be analyzed to detect vegetation, heat sources,mineral composition, glaciers, and many other features of the surface.with such a database, it should be possible to examine the effect of governmental regulations on theenvironment. for example, it will be possible to measure the size of croplands and compare those measurementsto regulations on land use policies or water usage. by incorporating economic and census information, it will bepossible to measure the impact of restrictions of water allocations on small versus large farms. another examplewill be to correlate crop subsidies with actual crop yield and water availability. numerous other examples can begiven to show the usefulness of remotesensing data in facilitating the development of government regulations.remotesensing data can also be used to improve our knowledge of the environment. an interestingexample is calculating the global water budget by measuring the change in size of the world's glaciers and theheights of the oceans. this information is needed to understand global warming, better predict climate change,and predict water availability for farming. all these examples require the ability to manipulate massive amountsof data, both to pick out individual data sets of interest and to stream large fractions of the archive through datasubsetting platforms to find the appropriate information.further examples of large scientific data sets include the following: global change data sets. simulations of the earth's climate are generated on supercomputers based onphysical models for different components of the environment. for instance, 100year simulations are createdbased on particular models for cloud formation over the pacific ocean. to understand the difference in thepredictions of the global climate as the models are changed, timedependent comparisons need to be made,both between the models and with remotesensing data. such data manipulations need support provided bypetabyte computing. environmental data sets. environmental modeling of major bays in the united states is being attempted bycoupling remotesensing data with simulations of the tidal flow within the bays. projects have been startedfor the chesapeake bay, the san diego bay, the monterey bay, and the san francisco bay. in each case, itshould be possible to predict the impact of dredging policies on bay ecology. through fluid dynamicssimulations of the tides, it should be possible to correlate contaminant dispersal within the bay and comparethe predictions with actual measurements. each of these projects has the capability of generating terabytes topetabytes of data and will need the petabyte software infrastructure to support data comparisons. map data sets. the alexandria project at the university of california, santa barbara, is constructing adigital library of digitized maps. such a library can contain information on economic infrastructure (roads,pipes, transmission lines), land use (parcels, city boundaries), and governmental policy (agriculturalpreserve boundaries). correlating this information will be essential to interpret much of the remotesensingdata correctly.enabling petabyte computing406the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the capacity of today's hardware and software infrastructure for supporting data assimilation of large datasets needs to be increased by a factor of roughly 1,000, as shown in table 1.table 1 infrastructure needed to support data assimilationtechnologycurrent capacityneeded capacitydata archive capacityterabytespetabytescommunication ratesmegabytes/secondgigabytes/seconddata manipulationgigabytes of data stored on local diskterabytesexecution rate on computer platformgigaflopsteraflopsfurthermore, increased functionality is needed to support manipulating data sets accessed from archiveswithin databases. the development of the highercapacity, greaterfunctionality petabyte computing systemshould be a national goal.analysis and forecasttechnology advancements now make it possible to integrate analysis of massive observational databaseswith computational modeling. for example, coupling observed data with computer simulations can providegreatly enhanced predictive systems for understanding our interactions with our environment. this represents anadvance in scientific methodology that will allow solution of radically new problems with direct application notonly to the largest academic research problems, but also to governmental policymaking and commercialcompetitiveness.the proposed system represents the advent of "petabyte computing"šthe ability to solve important societalproblems that require processing petabytes of data per day. such a system requires the ability to sustain localdata rates on the order of 10 gigabytes/second, teraflops compute power, and petabytesize archives. currentsystems are a factor of 1,000 smaller in scale, sustaining i/o at 10 to 20 megabytes/second, gigaflops executionrates, and terabytesize archives. the advent of petabyte computing will be made possible by advances inarchival storage technology (hundreds of terabytes to petabytes of data available in a single tape robot) and thedevelopment of scalable systems with linearly expandable i/o, storage, and compute capabilities. it is nowfeasible to design a system that can support distributed scientific data mining and the associated computation.a petabyte computing system will be a scalable parallel system. it will provide archival storage space, dataaccess transmission bandwidth, and compute power in proportional amounts. as the size of the archive isincreased, the data access bandwidth and the compute power should grow at the same rate. this implies both atechnology that can be expanded to handle even larger data archives as well as one that can be reduced in scopefor costeffective handling of smaller data archives. the system can be envisioned as a parallel computer thatsupports directly attached peripherals that constitute the archival storage system. each peripheral will need itsown i/o channel to access a separate datasubsetting platform, which in turn will be connected through theparallel computer to other compute nodes.the hardware to support such a system will be available in 1996. data archives will be able to store apetabyte of data in a single tape robot. they will provide a separate controller for each storage device, allowingtransmission of large data sets in parallel. the nextgeneration parallel computers will be able to sustain i/o ratesof 12 gigabytes/second to attached peripheral storage devices, allowing the movement of a petabyte of data perday between the data archive and the compute platform. the parallel computers will also be able to execute atrates exceeding 100 gigaflops, thus providing the associated compute power needed to process the data. sincethe systems are scalable, commercial grade systems could be constructed from the same technology by reducingthe number of nodes. systems that support the analysis of terabytes of data per day could be built at greatlyreduced cost.enabling petabyte computing407the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.what is missing is the software infrastructure to control data movement. mechanisms are needed that allowan application to manipulate data stored in the archive through a database interface. the advantages provided bysuch a system are that the application does not have to coordinate the data movement; data sets can be accessedthrough relational queries rather than by file name; and data formats can be controlled by the database,eliminating the need to convert between file formats. the software infrastructure needed to do this consists of alibrary interface between the application and the database to convert application read and write requests intosql* queries to the database, an objectrelational database that supports userextensible functions to allowapplicationspecific manipulations of the data sets, an interface between the database and the archival storagesystem to support data movement and control data placement, and an archive system that masks hardwaredependencies from the database. sql* is a notation that has been developed in a previous project for a futureansistandard extended standard query language.each of these software systems is being built independently of the others, with a focus on a system that canfunction in a local area network. the petabyte computing capability requires the integration of these technologiesacross a wide area network to enable use of dataintensive analyses on the nii. such a system will be able tosupport access to multiple databases and archives, allowing the integration of information from multiple sources.technology advances are needed in each of the underlying software infrastructure components: databasetechnology, archival storage technology, and datacaching technology. advances are also needed in theinterfaces that allow the integration of these technologies over a wide area network. each of these components isexamined in more detail in the following sections.database technologya major impediment to constructing a petabyte computing system has been the unix file system. whenlarge amounts of data that may consist of millions of files must be manipulated, the researcher is confronted withthe need to design data format interface tools, devise schemes to keep track of the large name space, and developscripts to cache the data as needed in the local file system. database technology eliminates many of theseimpediments.scientific databases appropriate for petabyte computing will need to integrate the capabilities of bothrelational database technology and objectoriented technology. such systems are called objectrelationaldatabases. they support queries based on sql, augmented by the ability to specify operations that can beapplied to the data sets. they incorporate support for userdefined functions that can be used to manipulate thedata objects stored in the database. the result is a system that allows a user to locate data by attribute, such astime of day when the data set was created or geographic area that the data set represents, and to performoperations on the data.to be useful as a component in the scalable petabyte system, the database must run on a parallel platform,and support multiple i/o streams and coarsegrained operations on the data sets. this means the result of a queryshould be translated into the retrieval of multiple data sets, each of which is independently moved from thearchive to a datasubsetting platform where the appropriate functions are applied. by manipulating multiple datasets simultaneously, the system will be able to aggregate the required i/o and compute rates to the level neededto handle petabyte archives. early versions of objectrelational database technology are available from needed tohandle petabyte archives. early versions of objectrelational database technology are available from severalcompanies, such as illustra, ibm, and oracle, although they are designed to handle gigabyte data sets.a second requirement for the objectrelational database technology is that it be interfaced to archivalstorage systems that support the scientific data sets. current database technology relies on the use of directattached disk to store both data objects and the metadata. this limits the amount of data that can be analyzed.early prototypes have been built that store large scientific data sets in a separate data archive. the performanceof such systems is usually limited by the single communication channel typically used to link the archivalstorage system to the database.the critical missing software component for manipulating large data sets is the interface between thedatabase and the archival storage system. mechanisms are needed that will allow the database to maintain largeenabling petabyte computing408the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.data sets on tertiary storage. the envisioned system will support transparent access of tertiary storage byapplications. it will consist of a middleware product running on a supercomputer that allows an application toissue read and write calls that are transparently turned into sql* queries. the objectrelational databaseprocesses the query and responds by applying the requested function to the appropriate data set. since thedatabase and archival storage system are integrated, the application may access data that are stored on tertiarytape systems. the data sets may in fact be too large to fit on the local disk, and the query function may involvecreating the desired subset by using partial data set caching. the result is transparent access of archived data bythe application without the user having to handle the data formatting or data movement.the missing software infrastructure is the interface between objectrelational databases and archival storagesystems. two interfaces are needed: a data control interface to allow the database to optimize data placement,grouping, layout, caching, and migration within the archival storage system; and a data movement interface tooptimize retrieval of large data sets through use of parallel i/o streams.prototype data movement interfaces are being built to support data movement between objectrelationaldatabases and archival storage systems that are compliant with the ieee mass storage system reference model.these prototypes can be used to analyze i/o access patterns and help determine the requirements for the datacontrol interface.archival storage technologythirdgeneration archival storage technology, such as the high performance storage system (hpss) that isbeing developed by the doe laboratories in collaboration with ibm, provides most of the basic archival storagemechanisms needed to support petabyte computing. hpss will support partial file caching, parallel i/o streams,and service classes. service classes provide a mechanism to classify the type of data layout and data cachingneeded to optimize retrieval of a data set. although hpss is not expected to be a supported product until 1996 or1997, this time frame is consistent with that expected for the creation of parallel objectrelational databasetechnology.extensions to the hpss architecture are needed to support multiple bit file movers on the parallel datasubsetting platforms. each of the data streams will need a separate software driver to coordinate data movementwith the direct attached storage device. the integration of these movers with the database data accessmechanisms will require understanding how to integrate cache management between the two systems. thedatabase will need to be able to specify data control elements, including data set groupings, data set location, andcaching and migration policies. a generic interface that allows such control information to be passed betweencommercially provided databases and archival storage systems needs to become a standard for dataintensivecomputations to become a reality.to other national research efforts are investigating some of the component technologies: the scalable i/oinitiative at caltech and the national science foundation metacenter. the intent of the first project is to supportdata transfer across multiple parallel i/o streams from archival storage to the user application. the secondproject is developing the requisite common authentication, file, and scheduling systems needed to supportdistributed data movement.datacaching technologymaking petabyte computing available as a resource on the nii to access distributed sources of informationwill require understanding how to integrate cache management across multiple datadelivery mechanisms. in thewide area network that the nii will provide, the petabyte computing system must integrate distributed computeplatforms, distributed data sources, and network buffers. caching systems form a critical component of wide areanetwork technology that can be used to optimize data movement among these systems. in particular, the numberof times data are copied as they are moved from a data source to an application must be minimized to relievebandwidth congestion, improve access time, and improve throughput. this is an importantenabling petabyte computing409the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.research area because many different data delivery mechanisms must be integrated to achieve this goal; archivalstorage, network transmission, databases, file systems, operating systems, and applications.dataintensive analysis of information stored in distributed data sources is an aggressive goal that is not yetfeasible. movement of a petabyte of data per day corresponds to an average bandwidth of 12 gigabytes/second.the new national networks transmit data at 15 megabytes/second, with the bandwidth expected to grow by afactor of 4 over the next 2 to 3 years. this implies that additional support mechanisms must be provided if morethan a terabyte of data is going to be retrieved for analysis. the implementation of the petabyte computingcapability over a wide area network may require the installation of caches within the network to minimize datamovement and support faster local access. management of network caches may become an important componentof a distributed petabyte computing system. until this occurs, dataintensive analyses will have to be done in atightly coupled system.future systemsa teraflops computer will incorporate many of the features needed to support dataintensive problems. sucha system will provide the necessary scalable parallel architecture for computation, i/o access, and data storage.data will flow in parallel from the data storage devices to parallel nodes on the teraflops computer. most suchsystems are being designed to support computationally intensive problems.the traditional perspective is that computationally intensive problems generate information on the teraflopscomputer, which is then archived. the minimum i/o rate needed to sustain just this data archiving can beestimated from current systems by scaling from data flow analyses of current cray supercomputers3. for theworkload at the san diego supercomputer center, roughly 14 percent of the data written to disk survives to theend of the computation and 2 percent of the generated data is archived. the amount of data that is generated isroughly proportional to the average workload execution rate, with about 1 bit of data transferred for every 6floatingpoint operations3. various scaling laws for how the amount of transmitted data varies as a function ofthe compute power can be derived for specific applications. for threedimensional computational fluiddynamics, the expected data flow scales as the computational execution rate to the 3/4 power.using characterizations such as these, it is possible to project the i/o requirements for a teraflops computer.the amount of data movement from the supercomputer to the archival storage system is estimated to be between5 and 35 terabytes of data per day. if the flow to the local disk is included, the amount will be up to seven timeslarger. a teraflops computer will need to be able to support an appreciable fraction of the data movementassociated with petabyte computing.this implies that it will be quite feasible to consider building a computer capable of processing a petabyteof data daily within the next 2 years. given the rate of technology advancement, the system will be affordable forcommercial use within 5 years. this assumes that the software infrastructure described previously has beendeveloped.the major driving force to develop this capability is the vision that predictive systems based on dataintensive computations are possible. this vision is based on the technological expertise that is emerging from avariety of national research efforts. these include the nsf/arpafunded gigabit testbeds, the scalable i/oinitiative, nsl unitree/hpss archival storage prototypes, and the development of the metacenter concept.these projects can be leveraged to build a significant technological lead for the united states in data assimilation.recommendationsa collaborative effort is needed between the public sector and software vendors to develop the underlyingtechnology to enable analysis of petabyte data archives. this goal is sufficiently aggressive and incorporates awide enough range of technologies that no single vendor will be able to build a petabyte compute capability. theimplementation of such a capability can lead to dramatic new ways for understanding our environment and theimpact our technology has upon the environment. building the software infrastructure through a public sectorenabling petabyte computing410the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.initiative will allow future commercial systems to be constructed more rapidly. with the rapid advance ofhardware technology, commercial versions of the petabyte compute capability will be feasible within 5 years.references[1] moore, reagan w. 1992. ''file servers, networking, and supercomputers," advanced information storage systems, vol. 4, sdsc reportgaa20574.[2] davis, f., w. farrell, j. gray, c.r. mechoso, r.w. moore, s. sides, and m. stonebraker. 1994. "eosdis alternative architecture," finalreport submitted to hais, september 6.[3] vildibill, mike, reagan w. moore, and henry newman. 1993. "i/o analysis of the cray ymp8/864," proceedings of the 31st semiannual cray user group meeting, montreaux, switzerland, march.enabling petabyte computing411the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.49private investment and federal national informationinfrastructure policyorganization for the protection and advancementof small telephone companies (opastco)introduction and statement of the problemthe organization for the protection and advancement of small telephone companies (opastco) is anational trade association representing nearly 450 small, independently owned and operated telephone systemsserving primarily rural areas of the united states and canada. opastco membership includes both commercialand cooperative telephone companies, which range in size from fewer than 100 to 50,000 access lines andcollectively serve nearly 2 million customers.this white paper examines network deployment, specifically, the networks used to access informationproviders, the location of customers trying to access the information, and the problems associated withconnecting to the information providers from rural areas. the information providers' databases are uniqueentities that are separate from the network providers and their networks used to access them and are beyond thescope of this paper.further, this paper suggests solutions to networking technology investment problems. the following fivesuch problems, or barriers, typical to small, rural telephone companies are discussed in this paper:1. limited local calling areas requiring toll access,2. long loops with load coils and station carrier,3. interconnection availability and distance,4. limited but great needs, and5. high cost due to poor economies of scale.existing network deployment and definitionsthe information market consists of information providers, network providers, and customers. whileopastco and its members fall into the category of network providers, they may have some freedom that theregional bell operating companies (rbocs) do not because of modified final judgment (mfj) restrictions thatapply only to gte and the rbocs. information providers rarely provide their own network. instead, they leaseaccess from network providers that add a surcharge based on the number of minutes connected or packetstransferred. in metropolitan areas, customers dial a local number or lease an inexpensive highspeed local directconnection to the local node for higher speed to information providers. in most rural areas, the customer is forcedto dial a longdistance number and pay perminute toll charges directly, or dial an 800 number and pay an hourlysurcharge to the information provider. having toll and 800 number access usually doubles or triples that cost ofaccess depending on the minimum hourly or subscription cost that the information provider charges.today, data networks fall into three categories: private data, data packet, and highspeed open interface.the first type, private data, are networks that companies and government units build or lease. these networksmay be privately owned and built or leased from local exchange carriers (lecs), interexchange carriers (ixcs),and data packet network providers, or any combination of the three. private data networks can be truly separateprivate investment and federal national information infrastructure policy412the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.networks or virtual private networks sharing equipment, but they cannot be truly interconnected except throughan optional gateway node. these networks are relatively expensive, with high startup costs. however, suchnetworks exist, and if the company or government unit can justify the need, there are few technical problems orproblems obtaining return on investment by small telephone companies in providing this type of network.the second type of networks are first generation, usually x.25, special data packet networks. tymnet andtelenet are examples of special data packet network providers. these networks are provided by ixcs, largeregional lecs, and special data packet network providers. the high cost of the initial investment, limited speedand capabilities, and limited willingness to interconnect small nodes to the existing networks have limited theexisting participation in providing data packet network access by small, rural telephone companies. highspeedopen interface is the third type of networks. router and atm switched networks, as well as the internet, areexamples of highspeed open interface networks. the internet is an interconnection of numerous networkssupported by regional, nationwide, and worldwide networks that provide access to a host of information sources,as well as electronic mail to anyone connected to the "net." however, the internet in its existing configurationand capability is not the complete functional network that is needed to provide the full range of services desiredby information providers, private industry, universities, and other small business and residential information users.analysis of problems and alternative solutionsbarrier 1: limited local calling areas requiring toll accessthe key to affordable information access for residential and small business customers is local access to thenetworks that provide access to the information providers. unfortunately, few rural communities have networknodes. only those small exchanges close enough to metropolitan areas to have extended area service can accessthe network providers with a local or tollfree call. many special data packet network providers place nodes incalling areas that have a population of at least 100,000, but some require areas to have a population of at least250,000.special data packet providers are telecommunications providers that use local networks to connectcustomers with various information providers for a profit. normally, they order standard business lines from thelec and install banks of modems concentrated with packet assemblers and share the backbone transmission linkbetween metropolitan nodes. ixcs are required to pay the lec for originating and terminating access on a flatrate per trunk or on a perminuteofuse basis. the packet data network providers currently are exempted frompaying access revenues to the lec.one factor to consider when planning to provide local data access from a small telephone company is thelost toll revenue from current subscribers to information service providers. in addition, if local access is providedand the volume of local calling increases significantly because of the highly elastic nature of informationservices, there will be an increased investment in equipment and a decrease in revenue. for small telephonecompanies that have costbased toll settlements through the national exchange carrier association, the impactof the loss in direct toll revenue and the jump in local calling dramatically shifts the revenue requirement fromthe toll to local, which forces local rates to increase. average schedule companies experience the loss in tollrevenue but are not aware of the negative double shift in revenue impacts of increased local usage. any nationalpolicy to foster investment in infrastructure should assess the impacts to both cost and average schedulecompanies.integrated services digital network (isdn) has not been deployed to any extent in rural markets because theswitching systems commonly used by small telephone companies do not have isdn available yet or they havejust recently made it available, or the cost to add isdn services is too high for the limited rural market. evenoverlay networks that allow a small initial investment to provide small quantities of isdn service are high incost when considered on a perline basis. as the isdn market penetration increases throughout the country,increased pressure to provide isdn in the rural markets will occur. unfortunately, isdn services are basicallylocal services, and unless the network provider has a local isdn node, the high cost of isdn toll access remainsa barrier. fortunately, in the last year, 28,800baud modems with error correction and compression haveimproved voicegrade lines' data access speeds considerably.private investment and federal national information infrastructure policy413the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the only technical solution to the local access barrier that exists today is to provide local access. thefollowing is an examination of the barriers that should be reduced to make local access more feasible.barrier 2: long loops with load coils and station carrierboth load coils and analog carrier limit data capability to voice grade circuits. the maximum bandwidth ofall voice grade lines is 4,000 hz, which may limit data transmission to 7,200 baud or less. analog carrier inaddition to bandwidth limitations also has limited custom local area signaling service (class) capability andfor 5 years has been in an industrywide phaseout. only within the last 2 years has there been any trend towardeliminating loaded loops.according to the usdarea 1992 telephone loop study, the average loop length was 4.11 miles, and17.84 percent were greater than 6.1 miles. in 1969, the average loop length was 3.3 miles, and about 20 percentof the loops were greater than 6.1 miles; at the same time, less than 2 percent of the bell companies' loops weregreater than 6.1 miles, and they averaged only 2.16 miles. when there was an effort to reduce loaded loops, theincrease in average loop length was probably due to a conversion from multiparty lines to all oneparty service.the usdarea study also showed that in 1992 38 percent of the rea loops studied used load coils toenhance voice transmission, which was down only 1 percent from 39 percent in 1985. fortunately, in 1992, only2.1 percent of the loops studied were served via analog station carrier.long rural loops with load coils and analog station carrier limit dialup access only slightly, but they do notallow for isdn or higher data speeds. if only one or a few lines of highspeed data are required, then special datacircuits can be built, but only at high percircuit cost. if ubiquitous service is required, an entire system redesignmay be required.for the last 10 years, there has been a trend to minimize loaded loops, but only within the last 2 years haverural, independent telephone companies considered totally eliminating loaded loops with remotes or digitalserving areas and loops limited from 12,000 to 18,000 feet. until now, these designs have been implemented toreinforce for high growth or to transition fiber in the loop to replace deteriorated air core cables.if ubiquitous highspeed data service is the objective, then considerable investment in digital serving areafacilities will be required. an evolving definition of universal service and a continuation of the universal servicefund (usf) for highcost areas will be necessary to foster the added investment in the infrastructure.barrier 3: interconnection availability and distanceone factor in availability is the distance from rural exchanges to the nearest network providers. networkproviders have established access nodes in most metropolitan areas with a population of 100,000. the distancefrom these access nodes to rural exchanges can vary from 20 to 500 or more miles. the cost for transport of 56kbps and t1 or higher speed data circuits is distance dependent. if multiple incompatible or proprietary networkconnections are required, then the already high cost for transport multiplies.another factor in availability is finding a willing network access provider to connect to an independenttelephone company network. to date, only regional internet providers have been willing to provide directindependent data network connections.one solution to the lack of available network connections would be a single standardized network enablinga common transport link, as well as lower costs to build in the desired redundancy. another solution would be toestablish regional hub nodes that would decrease the distance of transport, thus allowing a number of companiesto share the cost of common transport to the metro nodes. one example of a regional hub network has beenimplemented by minnesota equal access network services (means) and iowa network systems (ins).together the companies have built statewide networks to share in the cost of nodes and transport.private investment and federal national information infrastructure policy414the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.barrier 4: limited but great needswhile there may not be a great demand in rural areas for communications in terms of numbers and volume,there is a great demand for communications, and data communications in particular, due to the travel distancesand the need to be competitive with urban areas. rural areas strive to expand employment and increase economicdevelopment, and providing uptodate communications is imperative for hightechnology jobs and is needed toattract business to rural areas.there is no solution to the problem of travel distances and the need for communications in rural areasbeyond providing for the basic communications needs of rural customers.barrier 5: high cost due to poor economies of scalethe average opastco member company serves approximately 6,000 customers, and the averageexchange or wire center has approximately 1,200 access lines; compare that to a population of 100,000 or even250,000, which is necessary for some data packet network providers to invest in a node. the high minimum costof investment coupled with comparatively few customers and a low volume of data traffic make the cost percustomer or volume of traffic much higher than in urban areas. in addition to the high cost of investment inequipment and transport, an even higher cost may be required for test equipment, training for maintenance staff,and training and staffing for customer support. in addition to the training of support staff, an additional cost orbarrier to marketing data network technology in rural areas is educating customers to the benefits and general useof computers.one partial solution would be standardized modular node equipment manufactured in volumes large enoughto reduce the cost per node. again, if single network and regional hubs were implemented, the costs percustomer could be reduced. shared networks and even a shared pool of test equipment and support personnelcould further reduce costs.continued application of dial equipment minutes (dem) weighting and further application to datanetworking equipment are existing mechanisms for recovering the cost of investment and keeping the costs torural customers at an affordable level.recommendations and summarystandards development, standard equipment, regional hubs, and a single common data network are partialsolutions to the problems of providing affordable data network access to rural areas. further, an evolvingdefinition of universal service and the continued or expanded application of the usf coupled with continued orexpanded dem weighting are necessary to provide affordable information services to rural america.private investment and federal national information infrastructure policy415the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.50thoughts on security and the niitom perrinesan diego supercomputer centerstatement of the problemthe rapid introduction of the internet into u.s. (and global) society has challenged the knowledge, ethics,and resources of the culture. educational activities, both traditional and in many new forms, are rapidly makinginformation about the internet and personal computing and communication widely available. the ethicalconsiderations are being addressed by organizations such as the electronic frontier foundation (eff) and thecomputer professionals for social responsibility (cpsr). there is also a renewed emphasis on ethics in thetechnical communities, as well as a growing understanding of technical issues in legislation and law, as theseareas struggle to adapt to and codify new issues raised by emerging technologies.the internet has many of the characteristics of a frontier, including a dearth of security and lawenforcement services. this discussion focuses on the security mechanisms that must be developed over the next5 to 10 years to make the internet (and its successors) a safe computing and communications environment forindividuals and to protect the commercial interests of the businesses beginning to establish themselves on theinternet.backgroundthe internet is becoming an increasingly popular medium for delivering products, services, and personalcommunications. unfortunately, none of these commercial or personal activities were anticipated by the originaldesign of the internet protocols or by the architecture of the new class of common carriers, the internet serviceproviders (isps).the internet has become a new frontier for many americans. like any frontier, most of the inhabitants arepeaceful, interested only in exploration and settlement. but, like any frontier, a minority of inhabitants are moreinterested in exploiting the more peaceful inhabitants. another inevitable consequence of a frontier is the (initial)inability of law enforcement to keep pace with the rapid expansion in the number of inhabitants. if all of thissounds like the american old west, it is not a coincidence.networking and computing as communications services have created new problems, and put a new spin onold problems, in the security and lawenforcement resources of the american society. these problems can beaddressed on three levels: threat and protection models, deterrents, and lawenforcement resources.threat and protection modelsall security practices depend on the development of a "threat model," which details foreseeable risks andthreats. then a "protection model" is developed to address the perceived threats and risks, tempered byadditional factors such as law, policy, and costs. traditional models of both threats and protection have had flawsthat have increased the cost of secure computer systems and networks.threat models that have been developed for computer and network security in the past have reflected a"laundry list" of potential threats, with no regard for the cost (to the attacker) of any particular attack method. inthoughts on security and the nii416the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.other words, all threats have been considered equally likely, even if the cost of producing an attack might beprohibitive. if a threat is considered "possible," it must be addressed by the protection model.protection models have not been without their problems, as well. historically, most attempts at buildingsecure computer systems and networks have followed the "castle" model: build high, thick walls with a few wellunderstood gates. this paradigm is reflected in the terminology used in information security: firewall, bastionhost, realm, password, domain, and trojan horse.this mindset limits the ideas that can be discussed and thus the tools that will be developed. furthermore,approaches focused on prevention are limited to the scope of the modeled threats and typically are strictlyreactive to demonstrated examples of these threats. but, to date, no sufficient threat models have been developed.this approach is the epitome of passive defense, which is not a viable strategy in the long term as advances inoffensive technologies will always overwhelm a static defense. to go beyond this focus on prevention toencompass investigation and prosecution, we need to consider alternate modes of thought about informationsecurity.deterrentsa deterrent is anything that deters a person from performing some undesirable action. it can be as simpleand direct as a padlock, or as indirect as strict punishments if a person is caught and convicted.traditional, technical, computer and network security has focused on building better "locks," stronger"doors," and so on. until recently, crimes committed via computer or network were almost impossible toprosecute. the laws were silent on many issues, the courts (including juries) were uneducated concerningcomputers and networks in general, and law enforcement for such whitecollar crimes was seen as less criticalthan that for violent crime.with more awareness of the internet, the spread of home computers, and increasing reliance on computingresources for daytoday business, there has been a popular push for more legal deterrents (laws) and for bettereducation for judges, attorneys, and lawenforcement personnel. as a result of increased media attention to theinternet and more computers in homes, schools, and business, it is now no longer impossible to get a jurycapable of understanding the cases.lawenforcement resourceslawenforcement resources will always be at a premium, and crimes against property will always(rightfully) be of less importance than violent crime. as a result, computer and network crimes will always becompeting for resources against violent crimes and other, more easily prosecutable ones. in other words, only thelargest, most flagrant computer crimes will ever be considered in a courtroom.analysis and forecastover the next 5 to 7 years, the internet will most likely become the de facto national informationinfrastructure (nii). talk of hundreds of channels of tv, videophones, and so on will continue; but it is access topeople and data on demand that has driven and will continue to drive the growth of the internet. the internet ishere, and it works. new technologies such as integrated services digital network (isdn) and asynchronoustransfer mode (atm), higherspeed links, and new protocols such as "ipng" (internet protocolšnextgeneration) will become part of the internet infrastructure, but it is unlikely that a separate, parallel network ofnetworks will be constructed.the problems of making the internet a safe computing environment will require significant research anddevelopment in the areas discussed above: threat and protection models, deterrents, and lawenforcementresources.thoughts on security and the nii417the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.threat and protection modelstsutomu shimomura (san diego supercomputer center), whit diffie (sun microsystems), and andrewgross (san diego supercomputer center) have recently proposed a completely new approach to computer andnetwork security. this new model actually combines the threat and protection models into a new model referredto as the confrontation model.a new research activity at the san diego supercomputer center, undertaken as a cooperative venturebetween academia, government, and industry, will soon begin exploring an approach to information securitybased on confrontation in which we engage the intruder by using winning strategies within the scope of policy.hence, we call our model the confrontation model. as alluded to above, many of our ideas come from conflicttype situations such as might be found in business, intelligence work, law enforcement, and warfare, and so wedraw on all these areas for ideas and examples. the research for this new paradigm will require developing bothstrategies and tactics.using the paradigm of an intrusion as a confrontational situation, we can draw from centuries of experiencein warfare. the network and other infrastructure are the "terrain" upon which our "battles" are fought. from atactical viewpoint, certain resources will be more valuable than others (e.g., fast cpus for analysis, routers tochange the topology of the terrain, and critical hosts near the activity for intelligence gathering). we need toknow the terrain, make it easy to monitor, and use it to our advantage against intruders. once we understand theterrain, we can plan infrastructure changes that allow us to control it or position ourselves strategically within theterrain, and thus make it easier to counter intrusions.executing strategies within the terrain is complicated by the need to adequately identify an intruder's intent.confused users may at first appear to be hostile, while real intruders may try to hide within the terrain. torepresent this, traditional threat models must be amended to incorporate the extended terrain.a proactive approach is needed that simultaneously considers the "terrain" in which the engagement isoccuring, the disposition of resources to counter intrusions most effectively, and a costbenefit analysis ofcountermeasure strategies. such an approach to information security proved successful in the apprehension ofwanted computer criminal kevin mitnick. note that all conflict occurs within the scope of policy. such policiesinclude criminal law and its rules of evidence. in business, they include contract law, civil procedure, and codesof business ethics.in addition to understanding the "warfare" context, there is also a need to communicate with and becomepart of existing lawenforcement structures. instead of trying to adjust law enforcement to fit the peculiarities ofcomputer crime, we need to adjust the way we think about computer security to more accurately match the lawenforcement model to facilitate prosecution of computer crimes.deterrentsnew deterrents will be developed over the next 5 to 7 years. many of these will be in the form of strongerdoors and locks. these technical advances will come from research in many different areas and can be expectedto proceed at a rapid pace.it is expected that such proactive technical measures, leading to identification and prosecution of intruders,will be an effective deterrent. if intruders are aware of the risk they incur when attempting to compromisecomputer systems and networks, they may modify their behavior.more important, however, are the societal deterrents: ethics and law. a more vigorous campaign ofeducating business and the public will need to be undertaken. this education will need to focus on privacy rights,intellectual property rights, and ethics in general. it is not unreasonable that every computer education course ofstudy include an ethics component. this is already starting to happen in many engineering and computer sciencecurricula.the law of the land will require updating, not wholesale change, to accommodate the digital landscape.however, instead of kneejerk reactions to highly publicized events (child pornography on computers, etc. thathave resulted in laws dealing specifically with the internet and computers, we need expansion or reinterpretationthoughts on security and the nii418the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.of existing laws in the light of computers and networks. if something is already illegal, why should there be aseparate law making such an act illegal when a computer or network is involved?increasing the ability of existing enforcement structures to initiate and carry through successful prosecutionof crimes that happen to involve computers and networks will indirectly increase the deterrence to commit suchcrimes. this will require educating existing judicial personnel, as well as changes in policies and procedures, andincreased resources as well.lawenforcement resourcesas already noted, lawenforcement resources will always be at a premium. there will never be enough lawenforcement resources to fully investigate every crime, and crimes against property, including computer crime,will always (rightfully) be of less importance than violent crime. but this limitation primarily refers togovernment lawenforcement resources.as on the american and other frontiers, one solution will almost certainly be resurrected: private securityforces. just as the american frontier had its pinkerton agents and wells fargo security, the internet will soonhave private investigative and security organizations. in fact, the internet already has the equivalent of privatesecurity agents: the consultants and companies that deal with computer and network security. these agentsperform such work as establishing "safe" connections to the internet for companies and providing securitysoftware, intrusiondetection, and auditing software and hardware, and so on.but what about the investigative side?as part of the research on a confrontation model mentioned above, there is growing commercial interest inprivate investigative services to perform intrusion analysis and evidence gathering, for use in civil or criminalproceedings. the confrontation model will lead to technical solutions (tools) that will be available to bothgovernmental and private investigative services.a recent defense advanced research projects agency (darpa) broad area announcement (baa)stressed the desire to commercialized computer security services, including the detection of intrusions and thetracing of intrusions to their source (perpetrator). at least two existing companies are investigating entering thisfield.recommendationsthe government must support open, public security standards and mechanisms. it must removeinappropriate impediments to privatesector development of security technologies, including encryption. thisapproach will require support of research activities, legislative changes, and increased awareness of how digitalcommunications change the lawenforcement landscape.research activitiesthe government must foster more research into new protection strategies, and this work must be done inconjunction with the private sector. the computer industry is well aware of the problems and is (finally) beingdriven by market forces (consumer demand) to increase the security of its products.however, the computer industry does not always have access to the proper theoretical groundwork, and soacademia and government must find ways to cooperatively develop open standards for security software andhardware. this will inevitably lead to more joint research efforts, which may require revisiting the currentinterpretations of some antitrust laws.as part of cooperative research and development, testbeds need to be built to provide a better understandingof the battleground. this understanding will enable us to predict the types of intrusion strategies that can beexpected and will allow us to develop appropriate counter strategies. a better understanding of intrusions willallow us to better predict the intruder's intent. given what we believe the intent to be, we then needthoughts on security and the nii419the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.mechanisms to identify an appropriate response, appropriate for the chosen policy. for instance, if we identify anintent to access critical resources, then the response may need to support more comprehensive data collection tofacilitate prosecution.to be a viable platform for analyzing the confrontation paradigm, any proposed testbed must be a collectionof hardware and software systems that encompass the complexity and extent of today's networkinginfrastructure. the testbed will be a heterogeneous collection of vendor computer platforms, network routers,switches, firewalls, operating systems, and network applications. these are the terrain in which a confrontationoccurs.understanding the range of intrusions is required to build credible defenses. insight must be developed forboth the feasible intrusion mechanisms and the types of countermeasures that should be pursued. this insightmust quantify the cost of an intrusion, the cost of the countermeasure, and the level of risk that is being reduced.a costbenefit analysis is needed to understand the best possible response. a testbed serves as a tool to quantifythe risk associated with providing desired services and allows the development of mechanisms to reduce thatrisk. once the risks are quantified, it should be possible to create systems of graduated resilience as a function ofthe provided services.the testbed will be used for "war games," actual intrusion attempts against both current and emergingtechnology. one person can develop an intrusion mechanism and distribute it widely on the network, resulting ina widespread problem that puts our entire infrastructure at risk. an equally wide distribution of defensiveabilities is needed to counter this. evaluation of successful intrusions from the games will show where effortshould be put to best bolster system security. the system bolstering can be in the form of cryptography, betterprogramming standards, and a better understanding of the actual system functionality. vulnerabilities can becreated when a developer's perception of the function of the system differs from its actual function.as a product of the analyses done in the security testbed, prototype mechanisms will be developed. anapplication of the confrontation paradigm was used by shimomura and gross to analyze the flaws exploited inthe intrusion of shimomura's computers on december 25, 1994. their analysis resulted in an understanding ofthe "addressspoofing" technique that was used. the tools, most of which they developed on the fly, focused ontwo areas: noninvasive examination of the preserved system state of the compromised computers and packettrace analysis. understanding the initial intrusion mechanism and the goals of the intruder required analyzing thesituation with minimal disruption to the traces left by the intrusion. these tools enabled an appropriate responseto this particular intrusion. other intrusions may require a different tool set.it is important to note that although tools exist to examine the integrity of a suspected compromised host(for example, tripwire), they all rely on computing cryptographic check sums. this computation requiresreading all the critical files, which destroys all access time stamps in the file system. in some cases, it may beappropriate to have a toolset that examines the system kernel memory and all ondisk structures noninvasively,preserving all available information for further analysis (and as evidence).the confrontation paradigm provides a framework that can be used to understand intrusions. the actualmechanisms may be built from scratch, such as reconstructing data sets that were "deleted" from a disk. or theymay be built by modifying existing security tools such as logging mechanisms. for example, logs of packetsseen on a network were constructed to reproduce all the simultaneous sessions, either keystroke by keystroke orat least packet by packet. (these tools are capable of correlating the simultaneous activities of multiple sessionsto trace their interactions on a target computer system or network.) playback of the sessions in real time washelpful in understanding what the intruder was trying to accomplish and his relative level of sophistication.analysis of other intrusion mechanisms may require the construction of a different set of tools. in this case, lossof packet logs necessitated a more subtle and thorough analysis.analysis tools have been developed that extract relevant log records from centralized log facilities.sophisticated pattern matching tools were built to monitor for suspicious or unusual events. such patternmatching tools constitute a software implementation of the knowledge that was acquired. the particularimplementation is only valid for a specific set of tactics.thoughts on security and the nii420the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.legislative activitiesthe state legislatures and congress must become more aware of the impact of digital technologies on thecitizens, residents, and businesses of the united states. this will necessarily include education, briefings, andtechnical information from researchers and users of the internet.all computer and network security methods rely on cryptographic technologies in one form or another.congress must remove impedimentsšsuch as the current classification of all cryptographic technologies asmunitionsšto domestic production of cryptographic methods. if the technologies cannot be exported, then u.s.companies are at a disadvantage in the world market.recognition of digital communications as ''protected speech" as defined in the constitution wouldsignificantly clear the currently muddied waters and greatly simplify the legislative and lawenforcement burden."jurisdiction" is also a current problem. consider the case of kevin mitnick: he was a fugitive from the losangeles area, allegedly intruded into computers in the san francisco area, but was actually in seattle and raleigh.lawenforcement landscapethe lawenforcement landscape is going to change. along with new technologies for fighting computercrime will come an increased burden for investigation. education of lawenforcement agents to include computercrimes and methods will help, but it seems inevitable that private computer security investigators will play anincreasing role in the prevention, detection, and investigation of computerrelated crimes.additional resourceshafner, katie, and john markoff. 1991. cyberpunk. simon and schuster, new york. farmer, daniel, andeugene h. spafford, "the cops security checker systems," proceedings of the summer usenix conference,pp. 165œ170, june 1990. stoll, clifford. 1989. the cuckoo's egg. doubleday, new york. tzu, sun. 1963. art ofwar. oxford university press, cambridge.thoughts on security and the nii421the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.51trends in deployments of new telecommunications servicesby local exchange carriers in support of an advancednational information infrastructurestewart d. personickbell communications research inc.statement of the challengethe telecommunications industry, the computer industry, and other industries have been performingresearch and development for more than two decades directed toward the realization of "information age"applications of computer and communications technologies. in the last few years, this vision has been articulatedin the contexts of the "information superhighway," the "national information infrastructure'' (for the unitedstates), and more broadly as the "global information infrastructure." while the definition of a nationalinformation infrastructure (nii) is subject to some differences in viewpoints, several consensus definitions havebeen published. for example, the definition published by the private sector council on competitiveness includesthe following paragraph:1the infrastructure of the 21st century will enable all americans to access information and communicate with eachother easily, reliably, securely, and cost effectively in any mediumšvoice, data, image, or videošanytime,anywhere. this capability will enhance the productivity of work and lead to dramatic improvements in socialservices, education, and entertainment.although definitions of the emerging nii may differ in detail, they appear to be quite similar in spirit andintent in their focus on the following features: applications developed for all sectors of the economy; affordabilityšenabling all members of society to derive an improved quality of life and a higher standardof living; ubiquity (anytime, anywhere); support for easytouse multimedia applications (voice, data, image, video), which provide intuitive userfriendly interfaces to people; and dependability (reliable and secure).a key component of such an nii is the underlying communications fabric, which allows users to connectwith other users via their communicating/computing appliances (telephones, computers, personal digitalassistants, fax machines, settop boxes, etc.). in the united states, this underlying communications fabric iscomposed of the diverse networks of local exchange carriers, cable tv providers, wireless (cellular) providers,alternate accessnote: submitted on behalf of ameritech corp., bell atlantic corp., bellcore, bellsouth corp., nynex corp., pacifictelesis group, sbc corp., and us west inc.trends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure422the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.providers, interexchange (longdistance) carriers, and valueadded networks that are built on top of these (e.g.,the internet).traditional telecommunications networks have satisfied many of the requirements implied by the vision ofthe nii, and, indeed, form the communications fabric of today's information infrastructure. they are affordable,ubiquitous, easy to use, and dependable, and they have supported a wide and increasing range of applicationsincluding telephony, data communications (using modems), fax, access to the internet, voice messaging, emailmessaging, voiceresponse services, and access to variety of information services. in addition to the applicationslisted above, which are supported by ubiquitous dialup telephone services subscribed to by 94 percent ofhouseholds,2 there is a variety of higherspeed and/or specialized telecommunications services provided tobusinesses and institutions for such things as highspeed data transport and video teleconferencing, and forinterconnecting internet routers (packet switching nodes).the ongoing challenges in telecommunications networking today focus on the following:1. realizing affordable, higherspeed communications networking capabilities to support multimediaapplications for residences and small businesses, starting with the widespread availability of integratedservices digital network (isdn) access. the challenge is driven by the convergence of thetelecommunications, computing, information services, and broadcasting industries.2. realizing the ability to offer customized telecommunications services to residences and businesses (e.g.,calling name delivery, personal telephone numbers, personalized call screening and routing) by using theemerging advanced intelligent network (ain) capabilities of public telecommunications networks, andsupporting customers' needs for mobility by using combinations of wireless access technologies and ainfunctionality in core public network platforms.3. meeting the challenges of "information warfare" as u.s. telecommunications networks increasinglybecome the target of hackers, criminals, and terrorists seeking to exploit the increasing dependency ofu.s. citizens and institutions on networkbased applications.4. making increasingly complex and diverse telecommunications networks appear seamless and easy to usefrom the perspective of users and their applications.meeting these challenges in providing an advanced communications fabric for nii applications requires theinvestment of billions of dollars of research and development funds, and the investment of hundreds of billionsof dollars in new network facilities on a nationwide basis over the next two decades. these investments includethe installation of combinations of optical fiber, coaxial cable, wireless technologies, and network softwarethroughout the united states. one cannot overestimate the challenges associated with making networks andnetwork services reliable, secure, and easy to use, and doing so at costs that are compatible with the expectationsand ability to pay of residential and small business consumers. the vast majority of these software investmentsare directed at meeting these challenges. since the demand of residential and institutional consumers for thenewer applications that are envisioned within the framework of the nii is highly uncertain, and by implicationthe demand and associated revenues for the telecommunications services that the advanced communicationsplatform can support are uncertain, these investments involve high risk, except in situations where a combinationof existing revenue streams and cost savings can justify the investments independent of the demand forspeculative new services. the rapid depreciation of computer and communications technologies, in terms ofrapidly improving performance/price ratios, makes these investments even more risky because investments madein advance of market demand may never be recovered in a competitive marketplace.further compounding the risk associated with the large investments required to put in place thetelecommunications fabric of the nii is the uncertainty associated with the regulatory and legal frameworkwithin which network providers must operate. the regulatory and legal framework of the past is ill suited for anenvironment of large investments targeted toward highly uncertain market needs using rapidly depreciatingtechnologies in a competitive marketplace. for example, the requirement of a network interface device erects anartificial barrier that prevents local exchange companies from providing complete services to their customers.the regulatory and legal framework of the future is still being defined in a slowmoving set of processes. thesetrends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure423the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.processes are, to a large extent, setting the pace for the deployment of the nextgeneration telecommunicationsfabric needed to support the full range of envisioned nii applications.in summary: the regional bell operating companies (rbocs) are fully committed to supporting the development of thenii and its infrastructure and services. the rbocs are fully committed to providing ubiquitous, reliable, secure, and easy to use stateofthearttelecommunications products and services, particularly mass market telecommunications, at the lowest costconsistent with providing the required quality of service and operations. the rbocs are making major investments in research and development and in deployments in theirnetworks that are consistent with customer demands for enhanced functionality and customizability ofnetwork services, greater bandwidth in communications channels, improved responsiveness to changingcustomer needs, and improved reliability, usability, and affordability of services. a major challenge is the resolution of regulatory and public policy issues consistent with local exchangeservice competition and the rapid deployment of new technology to meet market demand.background: evolution of the networkssince the creation of the former bell system and passage of the communications act of 1934, and until thepast decade, the business of telecommunications has primarily been focused on cost reduction and improving thequality of telephone calls in terms of the ease and speed of setting up a call and the quality of communicationafter the call is set up. in the late 1940s and early 1950s, capabilities to allow direct dialing of longdistance callsby calling customers were created and deployed on a nationwide basis. in the 1960s and 1970s the emergence ofmodern electronic circuitry made possible the introduction of microwave and coaxial longdistance cablesystems that provided higherquality connections and achieved lower transmission costs. in this same period, andcontinuing throughout the 1980s and 1990s, the introduction of computing technology, both in stored programcontrolled switching systems and in the automation of network operations to streamline and facilitate manualtasks, resulted in dramatic increases in efficiency and the ability to serve customer needs quickly. theintroduction of fiberoptic systems, starting in 1979, further improved the quality of local and long distanceconnections and further reduced the costs of transmissions. the introduction of local digital switching systems inthe 1980s dramatically reduced maintenance costs.figure 1 shows a trend in the total number of local exchange access lines per employee in a typical u.s.telephone company over the last two decades. in the past decade, this number has increased from 167 accesslines per employee to 250 access lines per employee. this improvement in efficiency has been enabled by theongoing investment in new network technologies such as fiber optics and softwarecontrolled remote(unattended) electronic systems, and in softwarebased systems that are used to facilitate all aspects of thebusiness, including negotiating with customers to take orders for new or changed telephone services, determiningthe availability of equipment that can be assigned to new customers, assigning installation staff to connectcustomers to equipment that has been reserved for their use, and determining the causes of service problems andarranging repairs. with stateoftheart computerized systems, which involve tens of millions of lines of code,many of these functions can be substantially or completely automated.in addition to the cost reductions that have been achieved by the continuous investment in advancedtechnologies (hardware and softwarebased), there has traditionally been an emphasis on the use of subsidies tomake basic residential services universally affordable. business services and longdistance services havetraditionally subsidized residential service. services in locations that have lower associated costs (e.g., urbanareas) subsidize services in locations that have higher associated costs (e.g., rural areas).these subsidies have resulted in low prices for basic residential services. as mentioned, this has resulted inthe ubiquitous availability of affordable telephone services. however, in many places in the united states, basictelephone service is priced substantially below cost. as the nation moves into an environment oftrends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure424the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 trend in enterprise efficiency, as indicated by total number of local exchange access lines per employee ina typical u.s. telephone company, 1970 to 2000.source: courtesy of bellcore.competitive provision of new telecommunications networking services, the historical subsidies will slowdown the widespread deployment of new services for the following reasons: traditional providers will not offer new services below cost in an environment where competition makes itimpossible to charge prices substantially above cost for other services in order to subsidize these belowcostservices; and consumers, who have become accustomed, through public policy, to belowcost, subsidized services andtheir associated low prices, will be reluctant to subscribe to advanced services that are substantially moreexpensive, even if they have the ability to pay for those advanced services.in the last 10 years, the traditional focus on reducing costs and improving the quality of telephone servicehas been supplemented with a focus on providing new telecommunications services that are associated with thevision of the emerging information age, as captured recently by the vision of the nii. three of the main thruststhat have emerged in the context of services directed toward meeting the needs of mass markets (residential andsmall business customers) are as follows: personalized and customized telecommunications capabilities enabled by the advanced intelligent network(ain); new digital twoway access capabilities, enabled by integrated services digital networks (isdn) in the nearterm, and by broadband access in the mid to longer term; and mobility services based on wireless access and the ain to support people on the move.conversion of networks from analog to digital technologiesover the last several decades, starting in 1962, digital transmission technologies have been introduced inpublic telecommunications networks. initially, this took the form of t1 carrier service on relatively shortdistance connections between telephone switching machines. since 1979, it has taken the form of fiberopticsystems that link switching systems and that reach out directly to business customers, and to unattended remoteterminals that serve residential customers. in addition, since the second half of the 1970s, analog switchingsystems have been upgraded to newer systems that employ digital switching technologies. the net result is theability to provide endtoend digital services to customers to support emerging multimedia applications.the advanced intelligent networkthe introduction of stored programcontrolled (computerized) switching systems starting in the 1960s madeit possible to go beyond direct dialing of calls to offer customized telecommunications services to meet users'needs.trends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure425the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the earliest customized services to be offered included customercontrolled call forwarding, threewaycalling, and networkbased speed dialing. recently these have been supplemented by services that depend uponthe calling party's number such as caller identification, automatic recalling of the last call received, and callblocking. these services depend upon the combination of lowcost memory (storage of information) andprocessing power that is enabled by stateoftheart electronic technologies. however, these types of serviceshave traditionally been implemented by making changes and additions to the large software programs that runthe switching systems. making changes to these large mainframelike systems is very costly and timeconsuming. furthermore, since switches are purchased from a multiplicity of suppliers and come in amultiplicity of types, implementing new services has traditionally required the development and deployment ofnew generic switching software by multiple suppliers for multiple switch types. this costly and timeconsumingprocess is not consistent with the rapid deployment of a wide range of new telecommunications services that arecustomized to meet users' needs.thus, the local exchange carriers have implemented the ain as a clientserver approach to creating newservices. in this approach, the switches act as clients that interface with softwarebased functionality in servernodes called service control points (scps), service nodes, and intelligent peripherals. the switches, servicenodes, and intelligent peripherals implement building block capabilities that can be mixed and matched by thescps to create new services for network users. since all switches implement comparable building blockcapabilities, new services can be created and deployed quickly by implementing new functionality in the servernodes. following are three examples of new services that can be implemented in this way: a large chain of pizza stores requested a new service where its customers could call a single 7digit numberanywhere in a large geographical area and have their call directed to the nearest pizzeria. the 7digit numberis easy to remember and has the "feel" of a local number, which gives customers the confidence that theirpizza will be delivered warm and fresh. using the ain, this service was implemented as follows. when apizza customer calls the 7digit number, the call is delivered to a specific telephone switch that recognizesthis number as a special number. the switch sends an inquiry to an scp, along with the calling number. thescp accesses a geographical information system that determines the nearest pizzeria based on the 9digit zipcode of the caller and a map associating zip codes to pizzerias. the telephone number of this nearest pizzeriais returned by the scp to the switch, which then forwards the call to the nearest pizzeria. this example canbe generalized within the ain capability framework to include areawide abbreviated dialing for businesseswith multiple locations and a wide variety of special callednumber translation capabilities based on varyingnumbers of digits. individuals have many telephone numbers associated with them. these include their home (residence)telephone number, their office telephone number, their fax telephone number, their car telephone number,etc. some individuals would like to have a single telephone number that would make them accessiblewherever they are. using the ain, one can implement what is sometimes generically referred to as "personalnumber calling." one way to implement personal number calling is to utilize a special "access code" such as500 to signify a call that requires special handling. for example, a personal telephone number might be 500callann (5002255266). when such a number is called, the switch would recognize it as a personalnumber, temporarily suspend call processing, and send an inquiry to an scp to determine the currentphysical telephone number (wireline or wireless) to which this customer's calls should be directed.furthermore, depending on the preferences of the called party ("ann"), the call might be routed to her or toher voice mail or to her secretary, depending on the time of day and the number of the calling party. all ofthis service logic can be implemented in the scps and their associated database and processing systems. a service that appears to have very high market demand is voiceactivated dialing. this service allows usersto record speakerdependent (and eventually speakerindependent) templates of numbers they wish to dial.subsequently, those numbers can be dialed by repeating the utterance that was recorded. for example, afamily might have each child record the message "call mom," and arrange that this utterance be convertedinto one number or a sequence of numbers to be tried. with ain, this service can be implemented for endusers who pick up any ordinary phone in their home or office. when the child lifts the handset (i.e., goes"off hook"), the switching system automatically connects the line to a centralized ain intelligent peripheral(ip) server, which stores the recorded speech templates and performs the speech recognition function. thisip returns the telephone number to be called to the switching system and can also work with other ainfunctionality to prompt the switching systemtrends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure426the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.through a sequence of numbers that may depend on the time of day, day of week, or other parameters chosenby the subscriber.the key advantages of the ain are that it can provide customized telecommunications services to end usersin a manner that is easy to use and that works with a wide variety of appliances ranging from simple telephonesto advanced personal digital assistants.advanced digital accesstoday's residential and small business telephone customers typically access the network using dialuptelephone services delivered over the copper wires that connect them to the network (except wireless customersas described below). using modern electronics, it is possible to extend the range of uses of these wirepairbasedconnections far beyond what was originally contemplated. for example, the latest modems allow for digitalcommunication at 28.8 kbps on dialup connections. in order to support multimedia applications such as accessto stored images, video clips, compactdiskquality audio clips, and twoway multimedia teleconferencing/collaborative work, customers need more than a modem and a dialup line. one of the largest technical,economic, and regulatory challenges facing the telecommunications industry is how to create a path forwardtoward a ubiquitously available, affordable (by residential users), highspeed digital capability to supportmultimedia applications.one of the steps in this direction is the widespread deployment and availability of integrated services digitalnetwork (isdn) capabilities. in most cases, isdn uses existing telephone wires to provide twoway,simultaneous digital connectivity at up to 128 kbps, plus a 16kbps digital channel for network control signalingand additional packet data. users of isdn have reported greatly facilitated access to the internet world wideweb and other sources of multimedia information, and the ability to carry out multimedia teleconferencing/collaborative applications that include facetoface video that is of reasonably high quality.because isdn uses the existing wire pairs for most users (the distance from the user to the terminatingpoint in the network is the key factor, with roughly 6 km being the limit using existing wire pairs), it can beprovided with a relatively moderate (but still large) initial capital investment. the existing switches must beupgraded with hardware and software to support isdn. analog switches must be replaced with digital switches,or the isdn customer must be reterminated on a digital switch. note that approximately half of the telephonelines today terminate on stored program (computer)controlled analog switches that still offer highqualityservice for wirepair telephone lines.the softwarebased operations support systems that are used to automate operations in the network must beupgraded to accommodate isdn. the individual customers who order isdn must employ special terminationsthat are compatible with the isdn interface, whether built into an isdn telephone, a special interface board of acomputer, or an isdn terminal adaptor. the decision of a rboc or a local exchange carrier (lec) to makeisdn available throughout its territory (make it available to its roughly 10 million to 15 million subscribers) is amultibilliondollar investment decision. the capital investment by a customer for an isdn interface is acommitment of several hundred dollars at this time, but this cost will drop rapidly as isdn usage rises over thenext several years.the ongoing charges for isdn access vary throughout the country and are based on a combination of a flatmonthly fee and a usage charge that may depend on minutes of use, packets sent, or a combination of these. asmentioned above, the challenge for the telecommunications provider is to recover investment costs in anenvironment of traditionally subsidized, belowcost pricing of basic residential telephone services.beyond isdn, in the intermediate and longer term, is the challenge of providing residence and smallbusinesses with digital access that is capable of supporting applications such as highquality, fullmotion video.such applications require more than 1 mbps and can range up to 20 mbps or more for fullquality compressedhighdefinition television signals. to provide such services requires the deployment of a new physicalinfrastructure consisting of optical fibers, coaxial cable, and broadband wireless technologies. the investmentcost is likely to be in the range of $1,000 to $2,000 per subscriber location served, amounting to several hundredbillion dollars on atrends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure427the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.national level. since the applications and the user demand are speculative at this time, creating an economic,regulatory, and legal framework that will encourage investors to take the enormous risks associated with such adeployment is a national challenge. network providers are reluctant to deploy highercost architectures andtechnologies 5 or more years in advance of speculative market needs for such things as very high bandwidth(more than a few hundred kilobits per second) upstream capabilities on a peruser basis. nonetheless, somerbocs have announced plans to deploy broadband networks throughout their service areas over the next 20years, and all rbocs are deploying broadband networks on a limited basis to test market demand andtechnology capabilities.in a competitive marketplace, other firms can learn from mistakes made by the first entrant and can thenenter the marketplace later with newer technologies and a better understanding of customer needs. the potentialadvantages gained by waiting may give rise to a "getting started" problem, as all potential investors wait forsomeone else to make the first move. longterm, largescale projects like the nii may not be allocated adequatecapital. to offset this risk partially, network providers would like to begin recovering their capital investments assoon as these new networks are deployed. existing revenue streams for traditional telephony and entertainmentvideo services are less risky than unproven and speculative new services. by offering old and new services on anew shared platform, network providers can reduce their revenue risk and also benefit from economies of scope.the need to reduce risk and share the costs of network deployment across many users and services may be animportant factor driving many telecommunications companies' interest in entertainment video and many catvcompanies' interest in traditional telephony.services to support people on the movesince its introduction in 1984, cellular telephony has grown approximately 40 percent per year to servenearly 20 million customers today. paging services and cordless telephones are also highly popular. in the nextseveral years, new kinds of personal communications services based on digital technology and supporting bothtraditional voice and computer/multimedia applications are expected to be widely available. it has beenestimated that by the year 2003, there will be 167 million u.s. subscriptions to personal communicationsservices, with many customers subscribing to multiple services.3while wireless provides the physical access mechanism for an untethered telephone and other appliances,the advanced intelligent network (ain) provides the softwarebased functionality to people on the move. homeand visitor location registers (ain service control points) keep track of where nomadic users are and provide theinformation required to direct incoming calls to those users. ain can screen or block incoming calls according tothe calling number, time of day, day of week, or other parameters specified by the called party. ain functionalityallows "multitier" telephones to access cordless base stations, highpower vehicular cellular base stations, lowpower pedestrian cellular base stations, and, in the next several years, lowearthorbiting (leo) satellitesystems, depending on which is most economical and available at any given time. as wireless telephonytransitions toward nomadic multimedia computing and communications, the advanced intelligent network willprovide access control (securityrelated) mechanisms, interworking functionality, screening, customized routing,media conversion, and other "middleware" functionality to support people on the move.forecastadvanced intelligent networkbased on demographics, it is probable that all rbocs and most other local exchange carriers in the unitedstates will deploy the advanced intelligent networking capabilities described above nearly ubiquitously over thenext 5 to 7 years. some carriers have already made these services widely available. these services will representto telecommunications subscribers what the advent of the personal computer represented to its user community.users will be able to define and obtain customized call processing capabilities to support both voice and data/multimedia applications such as customized screening and routing of calls, automated media conversion tofacilitate the deliverytrends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure428the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.of messages, personalized telephone/network numbers, and access control (securityrelated) services. these willbe provided in a way that is easy to use, reliable, affordable, and capable of interworking with a wide variety ofappliances and terminals, ranging from simple telephones to personal digital assistants. ain will enhancemultimedia communications by enabling users to control multiple channels in a single communications session,and by interfacing with a variety of user terminal devices in a userfriendly way.integrated services digital networkisdn is widely deployed and available today. a detailed deployment schedule for isdn is shown infigure 2. isdn is a major step forward in enabling twoway, interactive access to multimedia information,multimedia messaging, and multimedia teleconferencing and collaborative work. it will be the backbone of thetransition of residential access toward broadband over the next 20 years. along with today's dialup modembased access, isdn will be a principal access technology for residential and small business users accessing theinternet over the next 20 years. isdn, in both its basic rate (two 64kbps "b" channels) and primary rate (twentythree 64kbps "b" channels) forms, will be used by businesses to meet their traditional telecommunications andinternet access needs, and it will be used by cellular and emerging personal communication service (pcs)providers to connect into the core telecommunications networks. isdn will be a principal access mechanism fork12 schools, libraries, and community centers to connect to the national information infrastructure.higherspeed switched and nonswitched servicesuntil recently, the primary method by which businesses and institutions obtained nonswitched private lineconnections between their locations was to use dedicated 1.5mbps t1 lines, and dedicated 56kbps digitalprivate lines rented from telecommunications carriers, including the local exchange carriers. some largerbusinesses and institutions have used higherspeed 45mbps private lines for pointtopoint connections.recently, new types of digital services, including frame relay, switched multimegabit data service (smds), andatm cellrelay, have been introduced by telecommunications carriers, including local exchange carriers. smdsis a packetswitched, connectionless data service that allows the destination to be specified independently foreach packet. frame relay and atm are currently nonswitched services that utilize predetermined destinations fortraffic; switched versions of these services are under development. all these services offer the advantages ofimproved sharing of facilities (fibers, terminations on electronic equipment, etc.) through statistical multiplexing.these new services, particularly atm, can also support advanced multimedia applications that require high datarates and low delay variability between communicating endpoints.these higherspeed services are being deployed in concert with market demands and are expected to bewidely deployed and available over the next 5 to 7 years.wirelesscellular networks are widely deployed in urban and suburban population centers, and coverage andconnectivity are steadily improving. these networks are being expanded to meet the growing user base with thedeployment of smaller cells and newer technologies. lowpower cellular (personal communications services) tosupport people on the move is being implemented and will be widely deployed over the next 5 to 7 years.wireless networks are being upgraded to employ digital technologies that support data and multimediaapplications. in addition, these digital technologies enable the incorporation of encryption methods to improvethe resistance of wireless services to eavesdropping. the use of advanced intelligent network functionality andservices will allow for improved roaming and mobility for wireless users and will enable access to multiplewireless networkingtrends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure429the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2 percentage of lines with actual or planned isdn availability through 1996. source: courtesy ofbellcore, based on sr2102, issue 5, november 1994.services (e.g., cordless telephony, highpower cellular, lowpower cellular, and satellitebased services)from a single telephone handset. such "multitier" applications are being deployed now by some local exchangecarriers, and they are expected to be widely available over the next 5 to 7 years.internetthe internet, as it exists today, is built on services provided by local exchange carriers and interexchange(longdistance) carriers. users access internet routers (switches) through dialup telephone lines and 56kbps or1.5mbps t1 private lines leased from telecommunications network carriers, primarily local exchange carriers.routers are interconnected with 56kbps, t1, and 45mbps private lines, typically leased fromtelecommunications carriers. increasingly, fast packet services (such as frame relay and smds) are being usedto replace pointtopoint links.recently, several local exchange carriers have announced offerings of complete internet protocol (tcp/ip)offerings, including routing functionality, mail boxes, and support services. it is likely that most local exchangecarriers will offer complete internet service product lines in the next several years. however, there are regulatoryissues that can delay the rbocs' offerings of internet services. the modified final judgment (mfj) prohibitsthe rbocs from carrying traffic that crosses local access and transport area (lata) boundaries; such trafficmust be handed off to a long distance carrier selected by the consumer. it is not clear whether, and if so, how, therestriction applies to the provision of internet service. in testimony before the house subcommittee on science,george clapp, general manager of ameritech advanced data services, made the following statement:offering a ubiquitous internet access service with the burden of the long distance restriction would increase ourcapital costs by 75 percent and expenses by 100 percent. the following factors contribute to these additional costs:trends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure430the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. latas in which there is low customer demand cannot be served from other sites in other latas. customers of our switched data services frequently demand redundancy within our own network to assureservice availability. because of the longdistance restriction, we cannot use sites in other latas to provideredundancy. current internet routing technology requires us to dedicate a router to each longdistance provider in eachlata.4at this point, some rbocs are interpreting the mfj restrictions to apply to their internet service offerings.this is an area where regulations need to be changed to allow the rbocs to compete on an equal basis withother carriers that are not subject to mfj restrictions.broadband accessas described above, the provision of ubiquitous, affordable broadband access to residences is one of themost difficult challenges facing telecommunications carriers. all rbocs have expressed a commitment todeploy broadband access services as quickly as the market demand, technology cost trends, and regulatory andlegal environment permit.the rbocs have collectively invested approximately $20 billion per year in upgrades to their networkssince divestiture in 1984. they have committed to increase their investments substantially if regulatory reformsare enacted that enable them to be fullservice providers of telephony, video, and multimedia interactive servicesin an environment that is conducive to the highrisk investments required to deploy broadband access networks.several of the rbocs and other local exchange carriers have market trials of broadband access under way orplanned.the deployment of symmetrical twoway capabilities, which permit residential users to originateindividualized very high speed (greater than several hundred kilobits per second) upstream communications is amajor challenge. one must differentiate between the concept of symmetrical twoway access, which has beenraised as an issue by the government and other stakeholders, and the concept of twoway capability. the mostdemanding twoway capability that has been identified in the context of networks that serve residences is twoway multimedia collaborative work, also called multimedia teleconferencing. research has shown that twowaymultimedia collaborative work can be supported, to a large extent, by basic rate isdn, and that nearly all needscan be met with a twoway capability of 256 to 384 kbps. most broadband access network architectures beingconsidered for deployment by the rbocs can support this capability on a switched basis for all subscribers. atissue is whether there is demand for still higher speed twoway capabilities, comparable in speed to the onewaycapability needed to deliver entertainmentquality video to residential customers. the data rate associated withentertainment video ranges from 1 mbps for vhs quality to 20 mbps or more for hdtv quality video. theability to deliver entertainmentquality video both downstream to residential users as well as upstream fromresidential users is what is called symmetrical twoway access.although a large number of alternative architectures have been extensively studied from a capability andcost perspective, it appears that in many situations substantial incremental investments are required to providesymmetrical twoway capabilities. it is unlikely that these incremental investment costs will be recovered in acompetitive marketplace if they are made many years ahead of the demand for such highspeed upstreamservices. the details of the tradeoffs among alternative broadband architectures vary from rboc to rbocdepending on such things as the density of housing units.dependable, usable networksthe tradition of the telecommunications industry has been to provide network services that are highlyreliable, secure, and usable by the widest possible range of telecommunications services customers. as new,interactive, multimedia networking services and applications are deployed, using a wide range of new andtrends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure431the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.heterogeneous technologies, it will be a great challenge for all industry participants to maintain this tradition inthe context of the nii. if individuals, corporations, and institutions are to reengineer themselves to becomedependent on networked applications, then those individuals, corporations, and institutions must be providedwith networkbased services and applications that are even more dependable than today's telephony services.they will expect those services and applications to be easy to use, to work all of the time, and to be secure fromintrusions and other security threats. the rbocs are committed to maintaining their tradition of reliable, secure,and easytouse services through a combination of technological and operational methods. in particular, the useand sharing (in public forums such as the national security telecommunications advisory committee) of bestpractices among network providers are essential to help prevent and minimize such threats. cooperative testingbetween networks to detect incompatibilities, particularly of management protocols that protect faults frompropagating into large outages, is an essential ingredient of this process.as we move into the future, the role of telecommunications networks in facilitating interoperability andease of use will become increasingly important to consumers. while early adopters and those who create newtechnologies have a relatively high tolerance for complexity and unreliability and are willing and able to investsubstantial amounts of time in learning to use applications and in resolving problems, mass market users expecttheir applications and services to be extremely dependable and intuitive. in theory, softwarebased functionalitycan be placed in end users' terminals to enable interoperability, to resolve incompatibilities that would beperceived by customers as application failures, and to make complexity transparent to end users. in reality, this isachieved today by forcing end users to be systems administrators of their complex terminal software, or toengage others to administer their systems for them. traditionally, the telephone networks have hiddencomplexity from end users and have resolved incompatibilities among end user terminals by employing''middleware" in the networks. for example, an end user in new york can make a call from an isdn telephoneto an analog cellular phone in london. as applications such as multimedia teleconferencing, multimediamessaging, and remote access to multimedia information become increasingly important in mass marketapplications, telecommunications networks will play a critical role in resolving incompatibilities betweendifferent types of user terminals and between user terminals and servers, in facilitating the location of resources,in helping users manage their communications services, and in providing capabilities such as multimedia bridging.broad recommendationsmost of the technologyrelated challenges in creating the national information infrastructure can be bestaddressed by the private sector, with the cooperation of the public sector. from the point of view of local exchange network providers, the regulatory impediments discussed abovemust be addressed to enable an investment climate that is appropriate for the high risks associated with thelarge deployments of network infrastructure needed to provide broadband access. the public sector shouldwork with the private sector to address issues related to universal access and service and to promote opensystems and interoperability among networks, systems, and services. the public sector should removebarriers to full and fair competition, such as the mfj restrictions and the network interface devicerequirement discussed above, and should avoid creating new barriers in the future. the public sector should use networks provided by the private sector rather than building networks incompetition with the private sector's commercial service providers. the public sector must also address the myriad legal issues related to intellectual property, liability,applicationspecific law (e.g., practicing medicine across state lines), and other issues that are impedimentsto the emergence of new applications. many of these issues have been identified in forums such as thenational information infrastructure advisory council and the private sector council on competitiveness. the public sector should be a role model user of the emerging nii and should continue its initiatives tocreate a broad awareness of the potential of the nii to address many of society's challenges in education,health care, and government.trends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure432the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. the public sector should support and fund precompetitive research and development targeted towardenabling nii dependability, interoperability, and ease of use by the broad population. the public sectorshould also support and fund precompetitive research and development on advanced technology for nextgeneration networks and advanced applications. the public sector should collaborate with the private sector on programs that will lead to the realization ofthe goal of having k12 schools and libraries connected to the nii by the year 2000. the public sector should work with the private sector to protect u.s. interests in matters related to the globalinformation infrastructure, with particular emphasis on intellectual property protection and trade reciprocity.notes1. council on competitiveness. 1993. vision for a 21st century information infrastructure. council on competitiveness, washington,d.c., may.2. federal communications commission. 1994. statistics of communications common carriers, 1993/1994 edition. federalcommunications commission, washington, d.c., table 8.1.3. personal communications industry association. 1994. 1994 pcs market demand forecast. personal communications industryassociation, washington, d.c., january.4. clapp, george h. 1994. internet access. testimony before house subcommittee on science, october 4.trends in deployments of new telecommunications services by local exchange carriers insupport of an advanced national information infrastructure433the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.52the future nii/gii: views of interexchange carriersrobert s. powers, mci telecommunications inc.tim clifford, sprint, government systems divisionjames m. smith, competitive telecommunications associationsummarywe are not starting a new national information infrastructure/global information infrastructure (nii/gii)from a blank page; we are building on an information infrastructure, corporate structures, regulatory practices,billing practices, services, and public expectations that already exist. so the challenge is not so much "what is theideal world that we would write down on a blank page?" but rather "how do we get there, starting with today'srealities?" this paper presents views of the interexchange carrier community, as to the future nii/gii potentials,the stones we must step on to cross that tricky creek from "now'' to "then," and the stones we could trip on andend up all wet. our principal emphasis is on how to achieve fair and effective competition, utilizing theregulatory process as necessary to help us get from "now" to "then," and assuring fair access to networks andservices, for all americansšindividuals, businesses, governments, educational institutions, and other entities.introductory assumptionsthis paper begins with some postulates upon which we think we can all agree and then discusses wherethose postulates lead us, and how we can actually accomplish the things that our assumptions imply.postulate 1: there already exists a "national/global information infrastructure." it has existed, in somedegree, since telegraphy became widely implemented. but now we're facing enormous enhancements: the focusin the past has been largely on the "communications" aspects of an nii/giišgetting voice or data (information)from one place to another. the future nii/gii will broaden to include vast improvements in creation, storage,searching, and access to information, independent of its location. to achieve these improvements will requiresophisticated customerpremises equipment and skilled users. the government may find it appropriate to assistin developing user skills and providing customers' equipment; but the associated costs must not be imposed onthe telecommunications industry.postulate 2: it is economically feasible, in many instances, for there to be competition in the form ofcompeting local telecommunications facilities, as well as services. but there will surely be some locations inwhich there is only one local facilitiesprovider. furthermore, even if there are multiple local facilities, there maybe only one active facility connection to a given home or office. in that case, will the multiple competingproviders of services of all kinds have open and fairly priced access to that same endlink, so that any serviceprovider can have fair access to customers? our postulate: such bidirectional open access can be achieved byfacilities providers' unbundling their services and providing them for resale on a fair basis; and effectivecompetition can bring about those results. but it will be a gradual transition process from regulation ofnote: since the drafting of this paper, mr. clifford has moved to dyncorp, advanced technology services, as vicepresident for engineering and technology.the future nii/gii: views of interexchange carriers434the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.monopolies to effective market competition, varying in speed of accomplishment in different locations. theassociated gradual relaxation of regulation must be done carefully, in such a way that monopolies are freed fromregulation in proportion to the reality of effective competition.postulate 3: a fundamental difference between the information superhighway and certain other widely usedfacilities such as the interstate highway system is in the mechanism for paying for their use. if you pay forsomething out of the public purse, and give it away for free or for a flatrate charge, you create the potential forenormous waste of that resource if excessive use of that resource is of some benefit to the users. in the case ofasphalt highways, that turns out not to be much of a problem: a person can "waste" the resource only byspending time driving on the roads. but in the case of the nii, one could send megabyte files to thousands offolks who don't want them, just by pressing an <enter> key, if the service were free. so postulate 3 says thatalthough there will be specific instances of flatrate billing for services, usagebased billing will be widely used,to limit wasteful use of the resources and to correlate costs with benefits.postulate 4: as often stated by federal representatives, the nii/gii will be built by privatesectorinvestments, not by governments. that clearly does not exclude governments from building facilities for theirown use when specific needs cannot be met by privatesector providers, or from exercising their regulatorypower to assure fair competition and to achieve other public interest goals, or from having their purchasingpower influence the direction and speed of development and implementation. the purchasing power of the top10 to 15 federal agencies is comparable to that of the fortune 500 companies, so that power can surely influencethe marketplace. but governments won't build the basic infrastructure.postulate 5: there are, however, publicinterest goals that governments must participate in achieving. asidefrom relaxing current laws and regulations that prevent competition in networks and services, we postulate that amajor governmental role will be in assuring implementation of what we prefer to think of as the "successor touniversal service." the successor to universal service could take either of two quite different forms, or somecombination of the two. it could mean simply equitable access, meaning that the nii should be sufficientlyubiquitous that anyone can have access to the network at equitable rates and at some accessible location. or itcould be an advanced universal service, which would enable qualified persons or entities to use selected servicesavailable via the nii. in either case, any subsidy should be targeted to the needs of end users, not to specificproviders. all providers must be able to compete for the end user with equal access to the subsidy funds orvouchers.in whatever combination evolves, the mechanisms for funding and distributing the subsidy pool will bedifferent from today's mechanisms, with multiple network and service providers being involved instead oftoday's situation with monopoly providers of networks and services. the mechanisms for creating any subsidypools that may be required must be fair and equitable to all of the contributors to those pools. contributors couldbe network and service providers, but could also be governments, using income from some form of taxes.clearly, some level of regulation will be required during the transition from today's "universal service" totomorrow's version of that concept.postulate 6: the anticipated vast increase in use of the gii for personal and business purposes offerspotential for enormous compromises of security (of both personal and business confidential information),personal privacy, and protection of intellectual property, unless preventive measures are implemented. and sincepeople don't want their privacy invaded and businesses simply cannot afford to have their proprietaryinformation exposed to others, people and businesses will adopt encryption and/or other mechanisms to preventsuch intrusions and exposures. government and law enforcement agencies must recognize that reality and mustnot unduly restrict the use and worldwide trade of encryption technologies.the vision: what is the nii/gii?we begin with a highlevel description of what we are expecting will evolve and then discuss what must bedone to get there. one of the most concise definitions of an nii/gii is found in the 1994 report putting theinformation infrastructure to work, from the national institute of standards and technology, "the facilities andservices that enable efficient creation and diffusion of useful information."the future nii/gii: views of interexchange carriers435the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the concept of nii/gii obviously encompasses a "network of networks," including telephone networks,cable tv nets; the internet; satellites; and wirelessaccess networks, domestic and foreign. we also suggest thatthe broad concept of nii/gii includes not just these basic networks, but also the end equipment, the informationresources, and the human beings that interact with the equipment, networks, services, and resources.the gii should and, we believe, will provide at least the following highlevel functions: persontoperson "virtual proximity"šthat is, the ability of individuals to communicate with each otherindependent of location. access to both oneway and twoway communication channels, narrowband or broadband as required, ondemand. multiple mechanisms for such access, depending on location. in highdensity areas there will benonradiating channels (glass or metallic) as well as wireless access to allow for personal mobility. lowerdensity locations will gravitate more to wireless access, since wireless will be less costly than wires or fibersin those lowdensity areas. and the areas of very low density will have access to both low and highcapacity services by means of satellites. (that's not to imply that satellites will be used only in lowdensityareas.) new mechanisms for protecting the privacy of individuals and the security of financial and othertransactions. these mechanisms will surely include encryption, and also new mechanisms for transmittingor obtaining information anonymously. highly reliable physical and logical networks. network providers are already implementing circularpathnetworks, rapid restoration capabilities, and even arrangements to use networks of their competitors forenhanced reliability. the marketplace demands such actions, when the entire operation of more and morebusiness depends on the availability of realtime communication among themselves, with suppliers, and withcustomers. access to vast amounts of information, with the ability to search those treasure troves for specificinformation, enabling new services such as automotive navigation, location of the nearest gas station, andmany others. that information access, and associated services, will further support fundamental societalchanges such as telecommuting, telemedicine, enhanced education opportunities, and others. indeed, thesuccess of a gii might well be measured by the proliferation of new services and their effects on society.today's "carriers" have opportunities to provide easy access to such services, if not the services themselves. global interoperability. it is not yet true, and probably never will be, that precisely the same equipment,radio spectrum, and operational protocols are used in every location around the globe. but it is truešand thepressures to make it true are constantly increasingšthat gateway mechanisms to achieve transparency fromthe user's point of view are being and will be implemented, both domestically and globally. we do note,however, that technical feasibility is not the only barrier to be overcome: business arrangements amongcompetitors, to achieve compatibility, are not always easy to achieve. but such arrangements are in the longterm interests of those competitors, and we believe they are achievable. billing mechanisms that reflect the cost of providing both the network services and the information servicesto which the users have access. especially when the cost of usagebased billing is for some reasonsignificant when compared with the cost of the actual service, flatrate billing mechanisms or even freeservices may be provided. asphalt highways provide an example here: the costšand the nuisance factoršof billing for every entry and exit and every tonmile of use of roads is so great that we've found other waysto pay for the facilities, in such a way that does not invite excessive waste of those facilities. new mechanisms for protection of intellectual property rights, and new assumptions as to what those rightsactually are. this is one of the most difficult characteristics to achieve, since once information is transferredelectronically, it is virtually impossible to prevent the recipient from passing it along to others. it's cheaperto buy a second copy of a conventional book than to produce a hard copy of it, so the author gets paid twice,by the two purchasers. but if each of us can send the electronic book to dozens of friends just by pushing an<enter> key, then how can an author be assured of being compensated in proportion to the distribution ofhis or her writing? and even if we postulate such a farout suggestion as to say that the network could orshould detect the transmission of copyrighted material, how could the network do that if the transmissionwere encrypted? we suspect that (1) hardcopy books will continue to be in demand, for the foreseeablefuture, but that (2) thethe future nii/gii: views of interexchange carriers436the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.basic concept of protection of intellectual property, when that property is available electronically, willevolve in an unpredictable fashion over time, and that (3) protection mechanisms will more and more beconfined to commercial use of intellectual property, rather than simply forbidding the copying of suchinformation.the role of interexchange carrierstoday's roletoday, the basic role of interexchange carriers (ixcs) is carriage of narrowband and widebandtransmissions over "large" distances, where the difference between "large" and "small" distance is defined byregulators and lawmakers, and the term "large distances'' certainly includes global delivery.in addition to simply providing those services and dramatically lowering their costs to users as a result ofcompetition, ixcs have been instrumental in the development and implementation of new telecommunicationtechnologies and services. these include implementation of digital transmission techniques, singlemode opticalfiber, synchronous digital techniques, and "intelligent network" services such as "800" service, which not onlyprovides for the called party to pay, but also allows for such flexible services as timeofday routing and routingon the basis of the location of the caller. the internet backbone is, of course, an example of a major ixc role.this is not simply to brag about past achievements, but to point out that ixcs have competed in the past and willcontinue to compete in the future with each other and with other telecommunications entities. in that process,they will push development and implementation of new technologies and services as hard and as fast as they can.tomorrow's rolesin tomorrow's gii, the terms "longdistance carrier" (lec) and "interexchange carrier" will become lessused. yes, there will be companies that concentrate more on building and selling local services, and companiesthat concentrate more on transcontinental and global networks. and both of those types of companies willprovide at least some of their services using network capacities leased from the other types of companies. but thecustomer/user will hardly care about the details of what network is used; the user will care about the nature,availability, reliability, and price of the services provided.as the conventional boundaries between local and long distance companies eventually disappear in the eyesof consumers, it will become the strategy of many retail providers to offer endtoend services that ignore thistraditional distinction. the critical element is the introduction of "wholesale" local exchange services that longdistance companies can resell as part of their endtoend packages. wholesale long distance products are alreadyavailable for local telephone companies to resell in combination with their local services to provide endtoendservice (to the extent that such resale is not currently restricted by the modified final judgement). with a limitednumber of local networks expected to develop, it is especially important that a comparable wholesale localservice be introduced to foster competitive diversity.but that's the long view. how do we get there from here? how do today's ixcs play in that game? here areour basic views: there is room for a large number of specialized ixcs in this country obviouslyšhundreds exist now. only alimited number do now and will in the future provide nationwide facilitiesbased networks and services; avastly larger number can and will provide various services using those facilities. current ixcs will dramatically increase their emphasis on endtoend services, intelligentnetwork services,information services, transaction processing services, and global coverage. they will also take on the role as"hosts" to various information services that they do not actually create, but do deliver to end users. as ixcs increase their emphasis on global endtoend services, they will require more and more widebandcapabilities in the "lastmile" links, to match the high capacities they already have in their longdistancenetworks but that are now generally missing in those lastmile links. the demand for data and video servicesšthe future nii/gii: views of interexchange carriers437the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.often requiring higher bitrates than voicešis growing rapidly and is expected to continue to do so. it iscritical that the most efficient providers of these services be the ones that survive in the marketplace. toachieve this goal, it is important to prevent crosssubsidy of these services by lec's stillmonopoly localvoice services. as the emphasis changes from simply distance to access to information, today's ixcs will surely move toproviding more and more informationrelated services, rather than just transport. that does not imply thatpresent ixcs will put libraries and catalog marketers out of business. but ixcs are in an ideal position toassist users in gaining access to information services independent of location of those services. already, wesee competing provision of nationwide telephonenumber information services, for example. why would itbe a surprise to see today's ixcs beginning to offer informationbased services as helping people findinformation that they want in government databases? or helping medical practitioners to locate othermedical practitioners that may know something they suddenly need to know about an obscure disease? orassisting schools and universities in finding just the right other school or classroom in which the sameproblems, issues, or history lessons are being addressed? and as more and more business is conducted by means of electronic cash of various formsšcredit cards,debit cards, "smart cards" with builtin computing poweršwho would be surprised to see today's ixcsproviding secure, private, reliable, and realtime transaction information? for example, an ixc couldprovide the validation of the gas station customer's smart card; assure the customer that the gas station's cardacceptance device is telling the truth about how much money is getting withdrawn from the customer's bank;and transmit the resulting information from the gas station to the gas station's bank, the customer's bank, andany intermediate parties that have any fiduciary responsibility. the carrier might even offer somefunctionality in terms of realtime execution of monetary exchange rates, even though the banks have thefinal fiduciary responsibility.architecturea discussion of architecture is of course closely linked to the above discussion of the future role of today'sixcs. they will clearly have a key role in developing and employing tomorrow's network architecture; however,it may differ from today's.network intelligencethe requirements of such services as "find me anywhere, anytime," sophisticated and flexible billingservices, call routing dependent on time or day, store and forward, messaging, and others that we have yet toimagine will clearly require more and more sophisticated network intelligence and functionality. ixcs havealready made great progress in these areas in the last decade or so; but the demands of video/multimedia servicesand increasing demands for flexibility and valueadded features will require considerable expansion of"intelligent network" functionality. there is no doubt that such functionality will be implemented. and when it isimplemented in any bottleneck portion of the network, all providers must have access to that monopolyfunctionality.longdistance transportthe architecture for tomorrow's long distance transport will not be all that different from today's, from afunctional point of view. clearly it will have even more capacity than today's already highcapacity twowayhighquality digital networks. network protection and restoration capabilities will be enhanced. newermultiplexing technologies such as sonet, and packet technologies such as frame relay and atm, will beemployed to provide ever faster and easier ways to insert and pick off individual transmissions from highbitratebulk transmissions and to increase reliability and lower costs for certain services. and mechanisms to protect theprivacy and security of users' information will surely be widely incorporated into the networks.the future nii/gii: views of interexchange carriers438the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.some have asked whether the interexchange networks will have the transmission capacity that will beneeded, if data traffic continues to grow at an exponential rate. data traffic could, it is said, be equal to thevolume of today's voice traffic in a few years, requiring a doubling of total network capacity. it is our view thatcapacity simply is not a problem in the longdistance networks, although it clearly is a problem in the "last mile."longdistance networks are already almost entirely optical fiber. the capacity of a given pair of those fibers hasbeen doubling roughly every three years since 1983, based only on changes in electronic and photonic equipmentattached to the fiber, not changes in the buried fiber itself. the bitrates that can be shipped through a given fiber,using a given optical wavelength, have increased as follows, with an even bigger leap expected in 1996:yearrate (megabits per second)1983405198681019881,80019932,40019969,600 (expected)200040,000 (expected)further, wave division multiplexing (wdm) is becoming practical, so that a given fiber can carry two oreventually even four or more signals, each transporting the 9,600 (or more) megabits per second. so as early as1996 or 1997, using fourwindow wdm, it could be feasible for a given fiber pair to carry 38,400 megabits persecond, compared to the 405 megabits per second available as recently as 1983. in terms of the conventional 64kilobitpersecond circuits, the ratios are not quite the same because of overheads and other technical factors, butthe result is just as impressive: a fiber pair in 1996 or 1997 could be carrying 616,096 circuits, compared with6,048 circuits that that same glass could carry in 1983šan increase by a factor of almost 102! capacity is simplynot a problem, in the longdistance network, for the foreseeable future.lastmile connectionsclearly, today's ixcs are moving toward providing endtoend services. the reasons for this include thedesire of many customersšlarge and smallšto deal with a single service provider, and the need for uniformprotocols and services, end to end. the move to endtoend services is also driven by the interest of both ixcsand their customers in cutting the costs of the lastmile links, which are now such a large portion of the costs ofproviding longdistance telecommunications and are priced significantly higher than actual costs. ixcs payaccess charges to lecs, on a perminuteofuse basis, for calls delivered or originated by the lecs. thoseaccess charges amount to approximately 45 percent of the gross revenues of ixcs. various estimates made bylecs indicate that the actual cost of local access is about a penny a minute or less. but the access charge forinterexchange carriers is three (or more) cents per minute at each end of the link. competition will help eliminatethese excess noncostbased access charges now imposed on ixcs and, finally, on end users.there are two potential mechanisms for cutting those lastmile costsšand they are not mutually exclusive.one is for regulators to require that the access charges be cost based. the second is the introduction of effectivecompetition in the local networks, which cannot develop if the new entrants are forced to pay a noncostbasedaccess charge to keep the existing monopolist whole. it should also be noted that there is potentially a structuralbarrier that could dilute the effectiveness of access competition: obviously, the "access business" of theinterexchange carrier must go to whichever network the customer has selected to provide its "lastmile"connection. an ixc's leverage to bargain for lower access prices would be limited by the ixc's ability toinfluence the subscriber's choice of local network provider. therefore, until and unless either regulation oreffective local competition causes the local network providers to offer open interconnection to all interexchangeservice providers, on efficient and competitively priced local networks, there could still be limitations to theability of every service provider to reach every end user, with fair access costs. because we expect that theselocal network providers will also be offering their own line of retail local and longdistance services, it is notclear that these network owners will have a great deal of interest in reducing their rivals' costs. until such time astruethe future nii/gii: views of interexchange carriers439the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.competition removes such barriers, regulation will be needed to assure that access to the end user by unaffiliatedretail providers is fully available and priced fairly. one possible regulatory approach is to require that newentrants provide equal access to lastmile links, with rates capped at the rates of the incumbent provider.we further note that simply the introduction of competition in local networks will not be fully effective inbringing down the lastmile costs if existing lecs maintain a dominant position in service provision and areallowed to continue charging noncostbased rates for carrying services that originate or terminate oncompetitors' networks. and even if the new entrants build networks that are significantly more efficient and lesscostly than existing lecs' networks, those reduced costs cannot be fully passed through to end users if the lecnetworks remain inefficient and those inefficiency costs are passed through to those newentrant networks whenthe dominant lecs provide the lastmile carriage. existing lecs will have minimal motivation to become moreefficient and lower their actual carriage costs as long as they hold the dominant market share so that they cancharge their smaller competitors the lecs' full lastmile costs.some argue that the access charge income, over and above actual cost, is used to support "universalservice." however, studies indicate that the excess access chargesšover and above actual costšare far greaterthan what is needed to support universal service. a study by monson and rohlfs 1 concludes that ixcs and otherratepayers are charged about $20 billion per year over and above the actual costs of providing the access servicesneeded. and a study by hatfield associates 2 estimates that the actual subsidy that lecs need to supportuniversal service is $3.9 billion. the excess income goes to support lec network inefficiencies, domestic andforeign investments by lecs, and other lec activities in which neither ixcs nor the public at large haveinterests.our bottom line suggestion here is that competition in the local communications marketšwhich willinclude not only today's lecs and ixcs but also today's cable tv companies and perhaps local utilitycompanies that have access to rightsofwayšcould go even further toward providing lowcost access than doesthe current subsidy by means of access fees. but to achieve that goal will require some mechanisms formotivating existing dominant lecs to improve their network efficiencies and lower their costs and thereforetheir access charges.numbering plansnumbering plans will clearly change dramatically in the future, as individuals demand multiple numbers fortheir own individual use and as the population grows. already, the use of 10digit numbers is required for moreand more local calls, as both overlays and geographic splits are implemented because of exhaustion of numbersin a given area code. the demand for nongeographic "findmeanywhere" numbers, and for separate numbers forhome phones, wireless phones, office phones, fax machines, and other special services, will surely make ourgrand kids gawk when we tell them we used to be able to make phone calls with only 7 digits! (some of usremember using only 4 or 5 digits, but never mind.–)new servicesenhanced "phone calls"personal numbersthere's debate about whether everyone will want a "personal number for life," so that the same number willring her or his phone as long as she or he is around. for example, there's some benefit in a west coasterknowing that 703 is in virginia, and it might not be polite to ring that number at 10:00 pm pacific time. a geoindependent number would not give the caller such a hint. but clearly there's a market for such geographicallyindependent and permanent numbers. the ixcs, responding to their competitive market environment, have putin place intelligent network capabilities nationwide and therefore are likely to be in the forefront of making sucha service practical.the future nii/gii: views of interexchange carriers440the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved."findmeanywhere" service"findmeanywhere" service may or may not be tied to the "numberforlife" idea. the number for life mayor may not be set up to follow the called party wherever s/he tells it to, on a realtime basis. but clearly such a tiecould be executed and would be popular. for the same reasons as above, today's ixcs are in a fine position topioneer such service, as they are now doing.messaging serviceswe already have messaging services, in the form of remotely reachable answering machines as well asservices that can be provided by both local and longdistance carriers. as more and more people use the personalnumber and findmeanywhere services, there will be more and more need to cut off such access when people goto bed at night, say, six time zones away from where their callers expect them to be. so we expect messagingservices to grow rapidly in popularity, for this as well as other obvious reasons of convenience and reachability.there will be services offering messages of limited length (shortmessage services) as well as the ability to leavelonger messages.multimedia servicesaside from whatever multimedia services today's ixcs themselves provide, it is likely that one of theirfuture roles will be in providing transparent interfaces between other service suppliers and end users, when theusers and the service suppliers use different equipment or protocols for storage or transport. also, ixcs willprovide billing and security services associated with multimedia services for which they themselves are not theoriginating providers.digital servicesdo users care whether the connection network they are using is analog or digital, voice or "data"? no, theyjust want to talk, or have their screen filled with pictures or thoughtprovoking words or good graphic diagrams.it is the network providers and spectrum managers and equipment builders and capacitytheorists like claudeshannon that notice and care about the differences. but in practice, there are clearly lots of differences, in termsof efficient use of network and spectrum capacities, flexible services, accuracy of data to be passed from oneplace to another, and ability to protect privacy. and since the entire ixc network is going digital anyway, fortechnical and cost and quality reasons, whether the transmission is voice or data, the result has huge benefits interms of cost, speed, and reliability of the data transmissions that will be so important in tomorrow's globaleconomy and lifestyle.in this case, one could argue that there is no special role for ixcs compared to local providers, since we areall going digital in the end and will therefore eventually be able to provide the needed speed and reliability. butfrom the practical point of view there is indeed a major role for today's ixcs, and that is back to the point ofproviding competition and therefore much faster progress in building the digital highbitrate, lastmile networksthat will be so important to customers who need the features that can be provided with broadband twowaydigital networks.financial and other transaction processingthe united states led the world in implementation of credit cards and debit cards, to replace paper cash andchecks in many financial transactions. but now, europe is clearly ahead of the united states in itsimplementation of smartcard technology and services, to take advantage of the enormous capability andthe future nii/gii: views of interexchange carriers441the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.flexibility of cards with computer chips built in. we are not at all out of the runningšwe do know aboutcomputer chips, and there is growing interest here. but at the moment european entities are in the lead in theimplementation of smartcard technologies. ixcs will surely not be the only players in this game; but they dohave an enormous opportunity to incorporate smartcard technology into many of their service offerings, andthereby bring the united states back into a leadership role in this new technology and service platform.new billing mechanismsbilling, as has been hinted at above, will be an interesting issue in the future gii. we are rapidly learningšwith the help of smart cards and other technologiesšhow to reliably establish that someone who wants to use aservice is indeed the person she or he claims to be and will pay the bill when it comes. but we do have thefascinating challenge of learning how to bill proportionately to either the use or the value of "bursty" digital bitstreams. if a user is hooked up to the network for an hour and hits the <enter> key only once, does she or heget billed for 1 hour? or for sending the 100 or the 100,000 packets that resulted from the touch of the<enter> key? obviously, the provider does not bill separately for each packet, the way it writes a separate lineon today's bill for each phone call. is there a way to make some kind of average or statistical count of the numberof packets or bits that flow to or from a given user and bill on that basis? of course there is. the question is howit will be done to be understandable and fair to both the user and the provider, and how it can be confirmed byeither party. again, this will not be the role of longdistance carriers alone but will evolve on the basis of trialsby all the carriers competing in both local and global services.the successor to "universal service": access to the networkas suggested in postulate 5, above, the current concept of "universal service," in the sense of havingessentially everyone be reachable by phone, will surely evolve as technologies and new markets and servicesevolve. there is no public interest in everyone's having access to every single "service" that the nii/gii willprovide, any more than there is a public interest in everyone's driving a rolls royce. clearly it is in the publicinterest for everyone to have at least some access to the gii, just as it is in the public interest for all of us to haveaccess to streets and roads and telephones. but with the dramatic expansion of the types of services that will ariseon the gii, we must learn to be more specific about who gets subsidized for use of what services, and how thatcrosssubsidy can be managed in a competitive structure. this seems clearly to be a case where all the serviceproviders, as well as consumers and the public at large, have a deep interest in designing and executingmechanisms to provide "equitable access" and/or "advanced universal service" in a way that gives all of us thebenefits of achieving those societal goals, as they may eventually be defined. today's ixcs are as anxious asanyone to have the networks and services provided in such a way that students, workers, and all others haveaccess to the gii, although there will clearly be services for which a subsidy to users is totally inappropriate. butwe do insist that the contributions to any crosssubsidies that are required must be managed in such a way thatthe purchasers of any given service or network access are not unfairly required to subsidize some other unrelatedservice or access. a crossservice subsidy system would not only unfairly tax the users of a particular service; itcould even prevent the provision of some desirable service or prevent the survival of a potentially valuableservice or network provider.fraud preventionfraud was not too great a problem when each subscriber was permanently connected by hard wires to agiven port on a given switch, so that it could reliably be established what subscriber made what phone calls. butas the cellular industry has dramatically demonstrated, once the subscriber is no longer so readily identified,fraud sky rockets. now, however, based principally on smartcard technology but also potentially making use ofother technologies such as fingerprints, voiceprints, and other biometrics, we know very reliable ways to uniquelythe future nii/gii: views of interexchange carriers442the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.identify the person or entity utilizing the network or network services. the big question here is not whether fraudcan be effectively prevented, but rather what balance we should settle upon, between fraud prevention and thecustomer's convenience and privacy. wešthe ixcs as well as wireless and other service providersšwill haveto gain some experience in order to reach this balance. but there is no doubt that a balance appropriate to bothcustomers and the business world can be reached.regulatory issuesin a marketplace situation truly like adam smith's, there would be essentially no need for regulation oftelecommunications services. but that is not where we now sit. during the transition from monopolytelecommunications markets to effective competition, a new regulatory framework is needed. that frameworkmust simultaneously grant the incumbent lecs the flexibility to respond to competition in an appropriatefashion and at the same time protect consumers and potential competitors from anticompetitive abuse of thelecs' substantial remaining monopoly power. we must move incrementally from the monopolybasedassumptions and regulations with which we have lived for many decades to as close to the freemarket situationas we can practically get. the goals to be achieved include the following: a fair chance for all providers, equitable access, spectrum management, elimination of restrictions on who offers what services, and international regulations.why can't the marketplace settle these issues? let us look at them one at a time.a fair chance for all providersafter living in a local monopoly environment for many decades, and building local infrastructure with manybillions of dollars, and establishing rights of way that are difficult and expensive to duplicate, there is no way tosimply drop all local regulation and declare that everybody has a fair chance at the market. one of adam smith'spostulates to describe a "market" was that there should be easy (equitable) entry and exit from the market. overthe long term, that could be accomplished in local telecommunications, especially with the opportunitiesprovided by wireless transport. but it cannot be done as quickly and easily as renting a building and starting anew restaurant. so we do have a challenge here, to provide a fair chance for all providers, at least for theforeseeable future, under a regulatory framework that simulates competitive marketplace conditions.the existing local bottlenecks not only give the lecs potential leverage in providing local services; theyalso provide significant unwarranted leverage to regional bell operating companies (rbocs) in provision oflongdistance services, if rbocs are permitted to provide longdistance services before those bottlenecks areeffectively eliminated. at least in the short run, those local bottlenecks will continue to exist. indeed, there maybe some network functions for which competition is infeasible for the indefinite future.equitable accessequitable accessšone of the successors to "universal service"šhas been discussed above. but clearly itmust be in this list also, as it will require some form of industry and public agreement as to what is required andhow to achieve it in a manner that is fair and equitable to all playersšproviders and customers alike. thatagreement could either be by "regulation," in the usual sense of having it imposed by government, or it could beby mutual agreement among the parties to the process. it remains to be seen what balance of these twoapproaches will succeed in this case, which is more complex than was the case with plain black phones. a majorthe future nii/gii: views of interexchange carriers443the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.challenge will be to achieve a reasonable level of access for essentially everyone, without having to provide sucha high level of subsidy that valuable services are priced out of the market and multiple providers cannot survivein that market.an equally important aspect of equitable access is, of course, access by competitors to bottleneckfunctionalities, whether those bottlenecks are owned by other competitors or by entities that are not directcompetitors but that have the ability to favor some competitors over others by means of their bottleneckfunctionalities.spectrum managementthe management of radio spectrum is one instance in which we believe adam smith would agree that thereis no practical marketplace. mr. smith wisely postulated that in a proper marketplace both the buyer and theseller must be able to understand what they are buying and selling. the radio spectrum is just too complex to beclearly defined in a market situation. i can buy a shirt from you, and we know who owns what. you can buy anacre of land from me, and we can draw clear lines around it. those are simple two or threedimensional objects,which we can clearly define. but how many dimensions does the radio spectrum have? one dimension offrequency; three dimensions of space; one dimension of time, which could be measured in days, weeks, hours, ornanoseconds; one dimension of power; a few (not clear how many) dimensions of modulation technique. wecould argue all day about how to count the number of dimensions, or variables, that it takes to describe the radiospectrum. but it is clear that in general the spectrum is too complex an object to be readily handled in amarketplace, especially since it is not easy to establish who might be illegitimately using some piece of it at agiven moment.therefore, we must continue to have some governmental management of spectrum use. there are somespecific cases, as has been recently demonstrated in the case of personal communication service license auctions,where a marketplace can be created. but even there, it was not "spectrum" that was being bought and sold in themarketplace. the process was first to create radio licenses, using the conventional engineering and policyprocess; then the spectrum managers determined that there was no significant public interest in who (among thequalified applicants) held the licenses; and only after that determination was made could the spectrum managerin good faith put those licenses (not the "spectrum") up for bids. surely, future spectrum managers could broadenthe specifications of particular licenses, so that the licenseholders could have more flexibility in the servicesprovided. but the basic spectrummanagement process must be maintained.international regulationsas we develop into a truly global economy, with global transactions taking place by the millions everyhour, we have more and more need for global interoperability of telecommunications systemsšnot necessarilyglobally identical systems, but surely systems that can readily talk to each other. the current problems associatedwith the new wireless communications systems are a fine example. the global system for mobilecommunications standards are being implemented in most of the nations of the world. if they were implementedeverywhere, a person could carry a smart cardšcalled a subscriber identity module (sim) in this particular casešwherever she or he went, rent a phone at the airport if the wireless frequencies happened to be different fromthose at home, insert the sim into the phone, and have access to the same account and many or all of the servicesavailable at home. but right now there is no certainty that such interoperability will be achieved in the unitedstates.the question is, should there be international regulations to impose standards that would enforce fullinteroperability? we believe the answer is no, at least in the case of equipment, although international standardsare certainly appropriate in certain instances of spectrum management. we suggest that it is up to the providersto decide whether to adopt voluntary standards and be compatible, or bet on some allegedly better technology,and either win or lose in the marketplace. in any case, it seems highly unlikely that the world is ready right nowto have an international standards or regulatory body make mandatory equipment standards and enforce themacrossthe future nii/gii: views of interexchange carriers444the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.national borders. there is a clear need for some level of international spectrum management, but not such a clearneed and practicality for international regulation of the details of how internal telecommunications issues areresolved within nations.government rolesthe federal government's role as a regulator and spectrum manager has been addressed in the above section.but in addition to the issues mentioned above, we must recognize that the federal government does have asignificant potential influence as a major purchaser of telecommunication services. that purchasing power couldbe usedšintentionally or notšto influence the direction and speed of development of the nii. we urge thefederal agencies to be conscious of this potential, but we insist that the specific needs of specific agencies mustnot be distorted in order to fit into some effort to use that purchasing power to influence network development.another major step that must be taken by the federal government, to affect not only telecommunications butalso many other aspects of u.s. business and its global competitiveness, is to relax or eliminate the currentrestrictions on export of encryption hardware and software. encryption technology already exists globally, andso the current restrictions have little or no longterm effect on national security but have a major effect on u.s.manufacturers and nii/gii participants.we also recognize major concerns with the roles of state and local governments. there are major potentialbarriers in that domain just because of the structure of local regulation. there are, for example, over 30,000 localfranchising agencies in the united states, which exercise some control over cable tv systems. obviously, such astructure could give rise to major problems of compatibility and implementation, for networks and services thatwill be far more complex than today's cable television.the next 5 to 7 years?it is our position that how far we can advance toward the longterm nii/gii goals in the next 5 to 7 years isprimarily dependent on how well we do at the incremental process of implementing competition in the localtelecommunications marketplace. if we do start implementing that competition quickly, at the state and locallevels as well as at the federal level, then we can expect several fundamental changes within that time period: we will have fiber to the neighborhoods, in many areas of many cities, but surely not ubiquitously. that fiber will provide highcapacity local access, to match the already high and increasingly higher bitratesand capacities that exist in the long distance networks. when and if local competition becomes real, there will no longer be a requirement for the restrictions of themodified final judgment, in manufacturing, information services, and longdistance services by rbocs. as the services offered on those networks become clearer, and their importance to various segments of theeducation and health care and other communities as well as the general public becomes clearer, we will beable to better define what we mean by "equitable access" to the nii/gii. and with that better definition, andthe cooperation of all providers in creating and funding a pool to provide support for those who need it, wefeel confident that we will be able to achieve equitable access without imposing undue burdens onratepayers and service providers.the ultimate nii will take longer than 7 years. we will not have a fiber to every single home and office in 7years. but we are confident that, with proper leadership and cooperation at all levels of government and in thelaws and regulation, we can move so far toward those goals that we would want never to return to the situation of1995.the future nii/gii: views of interexchange carriers445the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.notes1. monson, calvin s., and jeffrey h. rohlfs. 1993. "the $20 billion impact of local competition in telecommunications," unitedstates telephone association (usta), washington, d.c., july.2. hatfield associates inc. 1994. "the cost of basic universal service," hatfield associates inc., boulder, colo., july.the future nii/gii: views of interexchange carriers446the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.53technology in the local networkj.c. redmond, c.d. decker, and w.g. griffingte laboratories inc.the local telephone network is that part of the telephone network that connects individual subscribers,homes, businesses, and so on, to an endoffice switching center and includes the end office itself. in its earlyembodiment, the local network was simply a telephone (or other instrument) at a subscriber's residence or placeof business connected to a pair of wires that led to a switching office (figure 1). at the switching office,connections were made between local users, or the signal was sent via a tandem or longdistance path forsubsequent connection through another part of the telephone network.the early design goals placed on the local network were relatively simple (at least from our perspectivetoday): to provide reliable transmission and switching of voice signals that could be easily understood at thereceiving end. there were other considerations, such as ringing the phone, that were also necessary, but, mainly,the subscribers just wanted to hear intelligible voices.in concept, the local network is not much different today than it was in the past except that the terminationat the subscriber's premises is made at a standard interface that does not include the customer's onpremiseswiring or telephone (or other equipment). of course, things are much more complex now, because the demandsfor added bandwidth, new services, and overall cost efficiency have greatly changed the design goals used toplan and implement the network.the local telephone network is evolving rapidly from its historical manifestation as a narrowbandconnection of physical addresses to a more complex network of networks that includes narrowband, broadband,and variableband transmissions to physical and logical addresses. the added capabilities and increasedefficiency of the telecommunications network have allowed the introduction of new data services such as framerelay and switched multimegabit data service; developed new, intelligent features across a broad spectrum ofusers; and positioned the network for significant growth in the future.history of the local networkat the time the telephone was introduced, the telegraph was regarded as far more important to commerce.the product of the telegraph was a written record of the communicated message. this tangible record provided alink to the familiar handwritten discourse of the commerce of the day. because it did not provide a record of themessage, the telephone was initially regarded as a novelty.however, as the need for communications increased, the telephone soon surpassed the telegraph as themedium of choice. in fact, having a telephone became so popular that the proliferation of the supporting wiresbecame objectionable. engineers were forced to find a more compact means of running the wires from point topoint. the engineers found that wrapping the copper pairs with paper insulation and encasing the resultingbundles of pairs in lead allowed a more compact transmission medium, called a cable.it was also obvious that it was impractical for users to string a pair of wires from their location to eachperson to be called. the solution was to run all of the pairs of wires from the customer's premises to a centrallocation. there, ''operators" could connect sets of wires together according to instructions from the customers.this created a center where customers were switched at the central point of the wires. this was the origination ofthe oldtimer's call, "hello, central!"technology in the local network447the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 traditional access to customers.the switching at central was originally accomplished by human operators who manually interconnected thecustomers' calls. the transmission medium was copper wires either cabled or open. control was provided by thecustomers' verbal instructions and the operators' manual actions. this division in function (i.e., transmission,switching, control, and terminal equipment) still exists in today's telecommunications networks, albeit inradically different forms.transmission media and multiplexingtransmission equipment for telephony has evolved from simple open wire carrying a single conversation tooptical fibers that can carry many thousands of conversations.signals from a home or business are carried over a twisted pair of copper wires, called the local loop, to acentrally located local office. hundreds and even thousands of wire pairs are carried together in a single largecable, either buried underground in a conduit or fastened above ground to telephone poles. at the central office,each pair of wires is connected to the local switching machine. the transmission quality of the early installationswas highly variable. today, however, the plant that is being installed has the capability to transmit at least basicrate isdn (144 kbps). this is true even for long loops (greater than 12,000 feet) that require loop extensionequipment or lowerresistance wire. (future plans will reduce the number of those long loops.)in order to reduce costs, methods were developed to combine (multiplex) a number of subscribers on asingle transmission medium from the central office, with the individual wire pairs split off nearer to thesubscribers (figure 2). as advances in technology progressed, multiplexing kept pace by increasing the numberof conversations carried over a single path. only a few years ago, multiplexing provided tens of conversationpaths over a pair of wires. initially this was accomplished by shifting each telephone signal to its own uniquefrequency band.a major advance in multiplexing was accomplished when normal voice signals were converted into a codeddigital form. in this form, the digital signals could be regenerated repeatedly without loss of the voice or otherinformation content.with today's time division multiplexing, each telephone signal is converted to a digital representation. thatrepresentation is inserted into fixed time slots in a stream of bits carrying many digitized telephone signals, withthe overall stream operating at a high bitrate. (an uncompressed voice signal requires 64,000 bits per second[bps] in digital form.)the multiplexed signals can be transmitted over a variety of transmission media. the most commonmultiplexing system, called t1, operates over two pairs of copper wires carrying 24 telephone signals, at anoverall bitrate of 1.544 million bits per second (mbps). first installed in 1962, the system is still widely usedtoday.with optical fiber, a beam of light is transmitted through a very thin, highly pure glass fiber. the lighttravels in parallel rays along the axis of the fiber. many telephone signals are multiplexed together, and the lighttechnology in the local network448the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2 using pair gain for access to customers.source is simply tuned on and off to encode the ones and zeroes of the digital signal. a single strand ofoptical fiber used in today's telecommunication systems can carry 30,000 telephone signals. also, the repeatersin an optical fiber can be separated much farther (tens of miles as opposed to 1,000 feet) than in an electricalsystem (see figure 2).the greatly increased capacity of fiber links at relatively low cost has led to the practicality of "bypass," inwhich high usage subscribers bypass the local provider and feed directly into the telecom network (figure 3).the history of transmission media and multiplexing shows an ever increasing progression of the totalnumber of conversations that can be carried over a specific generation of the technology. the moreconversations carried, the lower the cost per call, or the greater the bandwidth available per subscriber.switching equipmentthe telephone network is a switched network. the connection from one telephone to another is created andmaintained only for the duration of each individual telephone call. in the early days, switching was performedmanually, by operators who used cords to connect one telephone line to another. the automation of switchingwas first accomplished by allowing customers to directly control electromechanical relays and switches througha "dial" attached to the telephone instrument. electromechanical switching equipment reduced the need forhuman operators. however, the equipment's capacity and capability for supporting new features was limited.most of today's switching machines switch signals that are in digital format. digital switching interfaceswell with the timedivisionmultiplex technology of today's transmission systems.as technology has advanced, it has blurred some of our old categorizations in the local networks. we nowhave remote units in the feeder network of various sizes and capabilities that combine the transmission,multiplexing, and switching roles previously accomplished by discrete systems. initially, these changes weredone to reduce costs, but now this added complexity has given the networks much greater flexibility to grow andexpand in capability.signalingthe telephone system uses a myriad of control signals, some of which are obvious to the customer andothers of which are unknown to him or her. the early signals between the customer and central were a crank onthe magneto to summon the other end. instructions on whom to call were exchanged by verbal commands.today's customers are very familiar with the telephone's ring, dial tone, dialing keypad tones, and busytone. however, control signals that are sent between switching offices, over circuits separated from the voicetechnology in the local network449the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 3 local access and transport area.channel, do not directly involve the customer's attention. these separated control signals used within thisseparate network are called "common channel signaling system number seven," or ss7.although the initial motivation in the introduction of common channel signaling was an improvement incall setup, this change has supported the movement of network intelligence out of proprietary switching systemsand into a set of distributed processors, databases, and resource platforms connected through welldefinedindustry standards.thus, we have seen the implementation of the intelligent network in which intelligence is added to thenetwork to implement new features and services such as personal number usage, virtual pbxs, voice responsetechnology in the local network450the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.services, and others (see figure 4). further, this intelligence in the network has the potential to evolve to be theservice infrastructure for future broadband and multimedia applications such as interactive video games, remotelearning, and others.telephones and other station apparatusthe telephone itself is a rather simple appliance. a microphone (the transmitter) and an earphone (thereceiver) are contained in the handset. the modern keypad dialer sends unique combinations of two singlefrequency tones to the central office to indicate the particular digits dialed.newer instruments, especially personal computers, are now common to the network. their capabilitiesinclude simultaneous voice and picture communications and computers with telephone capabilities, and they willinclude features yet to be invented.figure 4 the advanced intelligent network adds significant capability to the network by using intelligence at aservice control point or intelligent peripheral to provide new features and services.future evolution of the local networkthere is currently a revolution under way in the local telephone network that is being brought about bychanges in the technical, competitive, and regulatory arenas. from the technical perspective, there has been agreat advance in the ability of networks generally to handle and process information. more precisely, there hasbeen a digital revolution brought about by the ever increasing power and ever decreasing cost of microelectronics.one of the real drivers in this revolution is electronics technology, specifically progress in semiconductorfabrication capabilities. today's 0.5µm feature size for complex production integrated circuits is expected totechnology in the local network451the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.shrink to 0.25 mm by 1999 and 0.18 mm by 2002. this trend will allow microprocessor speeds to increase fromtoday's approximately 200 mhz clock rates to 500 mhz over the same time frame (see figure 5). in addition,dram (dynamic random access memory) chip capacity will also increase from today's 16 mb chips to anestimated 1024 mb capacity in 2002. putting these advances in electronics to work in development of risc(reduced instruction set compiler) processors, the heart of desktop workstations or settop boxes, for example,will mean that these processors can be expected to perform calculations such as those needed, for example, forvideo compression ten times faster in the years after 2000 than they do today.while the text files transferred today between users are normally about 100 kb, it is not uncommon for fileswith graphics information to routinely exceed 1 mb. it is now becoming common for users to attempt to send 1mb files over the existing telephone modem lines, with the result that the commonly used techniques are seen tobe quite inadequate. thus, at least a factoroften improvement in available data rate is required.the data services that users will soon demand certainly exceed the capability of the existingtelecommunications network. as traffic begins to include fullscreen, highresolution color images, files willbecome of the order of 1 gb in size, dictating a further increase in capacity of three orders of magnitude. thissort of increase in capability will require some fundamental changes. growth will be required not only in thepipelines that provide the data but also in file server technology, network management infrastructure, and usersoftware to enable rich new services.though we are dealing here explicitly with the local telephone network, the impact of this revolution alsoaffects local networks of all kinds, including telephone networks, catv networks, private data networks,cellular radio networks, and so onša profound technological convergence.figure 5 risc (reduced instruction set compiler) processor performance trends. data for this graph were takenfrom manufacturers' specifications as well as from an article by ohr1.as an example of the data growth envisioned for the network, the increase in ds1 and ds3 access lines isenlightening. figure 6 shows this growth to the year 2003.key elements in this robust capability for digital processing are that it is independent of content (such asvoice or video) and permits the distribution of the processing to the periphery of the network, thus allowingcomputing power to migrate to the user. these developments have enabled the conception of a whole range ofinteractive services that meld and blend the areas of communications, computers, and video. in fact, it is thiscommonality of technology that has led to other implications in the areas of competition and regulation.technology in the local network452the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.it is the desire to offer broadband video and interactive services that has created the incentive for the localexchange carriers to evolve their plants to provide bidirectional broadband access. actually, the buildout of thebroadband network is a process that has been going on for nearly two decades, beginning with the firstintroduction of optical glass fiber for carrying trunk traffic in the telephone company (telco) network around1980. in the intervening years, fiber has replaced virtually all the metallic cable in the interoffice plant and hasbegun to migrate into the feeder portion of the distribution plant.the most costly, but at the same time the most restrictive, portion of the access network, the subscriberloop, at this point remains copper. this is key in considering evolution toward a broadband infrastructure. withthe digitization of the switching infrastructure, the state of the current network includes a totally fiber interofficeplant, a fully digital narrowband switching infrastructure, and a partially fiber feeder plant.the approach to a broadband network must be formulated from this vantage point. there are two maintechnological thrusts that are enabling the digital video revolution. the first is the ability to compress digitalvideo with high quality to the point where one can deliver video streams in a costeffective way to individualsubscribers (figure 7). even with the high capacity of optical fiber, uncompressed digital video would haveremained a challenge in terms of transport, transmission, and switch capacity.the other technical event contributing to the availability of digital video has been the development ofbroadband switching technology in the form of asynchronous transfer mode (atm). key features of thedevelopment of atm include the ability to multiplex and to switch together (in a packet or cell format) thecontent of mixed streams of multimedia traffic, and, what is more, to do this isochronously, so that the timeinformation of each stream retains its integrity. it is anticipated that atm switches will become dominant afterthe turn of the century. one plan, shown in figure 8, predicts 100 percent deployment by 2015.the first plant upgrade enabled by the digital video revolution has been the migration of the existing catvnetwork to a fiberfed technology where fiber is brought to within two or three radio frequency (rf) amplifiersof the subscriber. the fiberfed bus architecture, or socalled hybrid fiber coaxial (hfc) system, providescapability possible to provide approximately 80 analog and 150 digital channels over a 750mhz hfc network.if such anfigure 6 growth of ds1 and ds3 lines. source: reprinted from ryan et al.2.technology in the local network453the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 7 compression of digital video.hfc network serves areas of 500 homes, there are clearly enough channels available with some statisticalconcentration to provide selected channels for individual subscribers.it is gte's plan to offer 80 analog video channels in the initial rollout of the hfc system to about 400,000customers in 1995, followed by 500,000 more in 1996. the cost should be in the range of $700 per customer.subsequent upgrades will include adding 150 digital channels of broadcast mpeg in early 1996 at an added costof about $200 per customer (settop box). this will allow delivery of nearvideoondemand. in late 1996 or1997, switched mpeg will be added for videoondemand and other interactive services for a furtherincremental cost of $100 to $200 per customer.further hfc additions beyond 1997 will depend on results obtained with the initial system.with a large number of channels available, even though the bus architecture is a shared medium, it providesmost of the functionality of a switched starstar architecture that is typical of most telephone networks. theenhanced upstream connectivity allows the addition of voice and data as integrated services along with video.the approach of the local telephone carrier to bringing broadband services to the loop involves theevolution to a broadband distribution network, which includes fiber that will go closer and closer to thesubscriber and ultimately to the premises itself. the particular approach to bringing fiber to the loop is a functionof cost. at the present time, fiber to the home is too expensive a solution. bringing fiber to some intermediatepoint is the preferred option. the hybrid fiber coaxial system, while being implemented initially by catvoperators, is clearly one such approach.while several local exchange carriers have embarked on network rollout programs with hybrid fiber coaxialtechnologies for their initial thrust into video distribution, it is clear that there is no straightforward way tointegrate hfc with the existing twisted pair copper loop plant (e.g., power, ringing the telephone, etc.). theadditional costs of managing and maintaining two networks appear to be a distinct disadvantage for thisapproach. in some cases, where aging loop plants are in need of full replacement, hfc can be put forward forintegrated services and replacement of the existing copper plant.the challenge is to find a mode of migration that will provide a sufficiently robust set of services to meetcustomers' needs in the coming broadband environment while maintaining an effective and elegant migrationpath from the existing copper plant. one such approach is asymmetric digital subscriber line (adsl) technology(figure 9). a pair of modem transceivers on each end of the copper loop provides digital line coding forenhancing the bandwidth of the existing twisted pair.figure 8 atm switch deployment.technology in the local network454the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 9 very high speed adsl (vhadsl) to provide video to the home.the approach is asymmetric because the capacity or bandwidth in the downstream direction, toward thesubscriber, is greater than that in the upstream direction. one adsl (very high speed adsl, or vhadsl)approach for particularly high bandwidth in the 25 to 50 mbps range involves serving areas consistent with fiberbeing brought into the loop within several thousand feet of the subscriber, and is thus consistent with themigration path that brings fiber close to the subscriber premises.one of the transceivers is therefore installed at a fiber node. eventually this may go to a fibertothecurbsystem and ultimately, when economically justified, to fiber to the home. there is a continuum of servingareasizes. but for the present, this is an integrated network that provides all services over a single plant and iscompetitive for the range of bandwidths required.one of the major advantages of this adsl approach is that it can be applied only to those customers whowant it and are willing to pay for the added services. thus, this method allows for an incremental buildup of abroadband capability depending on market penetration (figure 10).this, then, is the infrastructure we will be looking at, but what of the services and programming? it is clearthat broadcast services need to be provided in an ongoing way. additionally, various forms of ondemand orcustomized video programming formats are anticipated to be important. true videoondemand commits a portand content to an individual subscriber, and the viewer has vcrlike control of the content. nearvideoondemand shows a program frequently enough to simulate the convenience of videoondemand, but without therobustness of true ondemand services. clearly, other services will be more akin to the interactive features thathave grown up on the personal computer (pc) platform. these include various information and transactionalservices, games, shopping, and, ultimately, video telephony.the subscriber platform is also worthy of note. there are clearly two converging sources of services here.one is the cable television (catv) environment, with the settop box and television set as the platform, and thesecond is the pc. while the former has been almost exclusively associated with the domain of entertainmentservices, information and transactional services have clearly been the domain of the pc.it is clear that the carrier must be prepared to provide services that are consistent with both platforms,because one or the other will likely continue to be favored for specific applications or classes of applications. anexample of such a service is embodied in an offering called "main street." main street uses the tv with a settop box to provide a visual presentation (currently stills and sound). a telephone line is used to send signals to atechnology in the local network455the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 10 cost comparisons for veryhighrate adsl, fiber to the curb, and hybrid fiber coaxial cable versusmarket penetration.figure 11 main street.technology in the local network456the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.headend server (figure 11). services offered include scholastic aptitude test study, home shopping, accessto encyclopedias, games, etc. the service is currently offered at a few locations around the united states.these new networks of the future are much more complex and require a corresponding increase inintelligence. intelligence (defined here as the ability to selfinventory, collect performance data, selfdiagnose,correct faults, respond to queries, etc.) no longer resides exclusively in the central office but is spreading into thelocal access network, thanks to the plummeting cost and increasing reliability of processing power (figure 12).the new distributed intelligent network elements will enable a revolution in the efficiency with which telcos canperform core business functions needed to administer customer services. for example, telcos have historicallybilled for service based on time of connection, bandwidth, and distance. this approach has little meaning in thecase of connectionless data transmission, and new approaches need to be formulated.similarly, monitoring and testing network performance will attain new levels of efficiency as digitalperformance monitoring and automatic alarm collection/correlation move down to the individual line card level.probably the most exciting aspect of these new intelligent access elements is the new services they will makecost effective. reducing the cost of digital services such as isdn and frame relay, and of higherbandwidthservices such as interactive multimedia, will require intelligent elements in the access networks. dynamic serviceprovisioning, whereby services are delivered in real time on an asneeded basis, will similarly rely on intelligentnetwork elements. as these examples show, the addition of intelligence to the access network represents a newparadigm for the rapid, efficient, and costeffective addition of new services.figure 12 future network.technology in the local network457the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.telecommunications management networks (tmn) is another emerging technology that will reduceoperational costs by enabling more efficient network management 3. concept first appeared in the early 1980s,but activity remains primarily in the standards arena, where organizations such as the internationaltelecommunications union, the american national standards institute's t1, and the europeantelecommunications standards institute continue to evolve a complex set of tmn standards.in the existing situation, each network (e.g., those of long distance carriers, local telephone networks,wireless networks, etc.) has its own specialized, proprietary management network, with little or nointeroperation. indeed, at present each network element has a unique interface to the management systems. thetmn approach, which has support across the communications industry, will result within the next decade in anopen, multivendor environment with the benefits of reduced costs for operations systems and improvedmanagement efficiency.the local network and the national information infrastructurein order for the greatest number of individuals, residential or business, to access the nii, there must besupport to allow a variety of types of customer premises equipment (cpe) to gain access to the network as wellas to enable interoperability 4 among the pieces of the network. users are going to want to be attached to the niifrom any of various types of cpe (e.g., a plain old telephone system telephone, screenphone, personal digitalassistant, pc, or tv). they will connect to the network either by dialup (wired or wireless), leased line, telcoprovided video network or cable service, or by interfaces provided by the utility companies.the interoperability of these network access types is going to be dictated by the need for users (and theirapplications) to get access to other users without needing to have multiple types of cpe and/or settop boxes andwithout having to know what's on the ''other end of the line" 5. in addition, the nii will need to support a vastlyincreased degree of interoperability among both the attached information resources (and their providers) and theactive components (e.g., personal "agents") that wish to make use of these resources.just as the nii is often discussed in the context of an extrapolation of today's internet, so also the problemsof interoperability in the nii can be thought of as an extrapolation of the simpler problems of interoperability inthe context of information processing currently being extended to distributed computing. the current informationprocessing infrastructure involves a vast legacy of networks of heterogeneous, autonomous, and distributedcomputing resources, including computers, applications, data (files and databases), and the underlyingcomputing and communications technology. we endorse the computer systems policy project report,perspectives on the national information infrastructure: ensuring interoperability (cspp, 1994), which focuseson the importance of interoperability.we want to add to that report's statements that developers of nii technology cannot assume a clean sheet ofpaper for new applications. most current data are, unfortunately, tightly coupled to legacy applications. in orderfor the current vast repositories of data to be available to future nii applications, interoperability mechanismsmust be developed to provide access to these data and, in some cases, to allow current legacy applications toparticipate in the future. these legacy applications and associated data represent huge investments and areunlikely to be rewritten to upgrade their technology base.a major consideration for users on the local network is the nii interface. this interface requires anunderlying communications model that includes how a user or application can connect to another user, service,or application by using customer equipment in a networkindependent manner, and with a set of relatively simpleformats for accessing the various kinds of data that will be available in the nii. to accommodate functionallylimited cpe, transformation gateways may have to be provided at the central office close to the end user.planning for the future of the local networkthe capabilities of the communications networks have expanded significantly so that, from a technicalstandpoint, there are major overlaps. telcos are now attracted toward video delivery, cable companies towardtechnology in the local network458the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.providing telephony and data services, and broadcasters toward participating in cable networks, with all involvedin programming to varying degress.this state of affairs begs the obvious question: why do other businesses seem so attractive? this appears tobe based on two assumptions. the first is that, for the current participants, their networks can be modified tohandle the total communications needs of customers (voice, data, video) by relatively modest incrementalinvestments, and the resulting, greatly expanded network can be managed with essentially the same managementteam.the second assumption is that the communications environment brought about by the digital interactive agepromises a growth in and demand for new services that have not been experienced in recent times, if ever before.thus the stage is set, at least on the telco side, for an entry into traditional entertainment and contentbasedservices as well as an extension from voice and data into multimedia communications.the evolution of the local network is also being affected by the changes that are taking place in theregulatory arena. there has been a general social trend toward a more competitive marketdriven environment aswell as a technical basis supporting deregulation, given that technology was establishing the basis for multipleproviders for the same service.this has begun with the federal communications commission's video dialtone rulemaking, which providedthe basis for telcos to offer transport services for video within their franchise areas. the most problematicbusiness areas with respect to regulation have been those associated with content and programming. these havebeen driven by the fcc, based on the traditional concern with control of communications being in the hands of asingle entity.this has been a particular issue for the local telcos for several reasons. first, the video dialtone enablingregulation initially has proscribed the involvement of a telco in providing content over the video dialtone (vdt)network. this has been challenged by several of the local exchange carriers in federal court, with universallysuccessful results to date. while there are still appeals processes to go through, it is clear that the firstamendment right stands significantly behind the carrier's positions.the concern here has been the availability of extensive competitive programming for initial rollout of thenetwork, in order that there be a basis for competing with the current incumbent. for this to happen, a localcarrier must be allowed to prearrange to some degree the initial availability of programming, or the exercise willbe one of building the video dialtone infrastructure and hoping that sufficient programmers and/or subscriberswill arrive to pay for the cost of the network.a second incentive for involvement in content and programming is the apparent structure of the currentbusiness, whereby significant leverage and profitability remain with the content, and delivery may be a lessprofitable part of the business. this may or may not be true in the future. it is clear, however, that access tocompetitive content and programming is essential in an era of video competition.as the executives responsible for managing the companies that provide local service ponder the actions theyshould take to remain competitive (and to grow), they are faced with a number of uncertainties that are externalto their companies, such as technology advancements, competitive actions, and governmental regulation.this is a normal state of affairs for many businesses, and any business opportunity is normally undergoing anumber of dynamic changes on a more or less continuous basis. in technology, the changes include such thingsas costperprocessing capability, data storage capacities, and the like. these elemental changes may allowhigherlevel changes that dramatically affect the cost and performance of the business opportunity. in regulation,such items as allowing competition into new markets, pricing freedoms, and others likewise have an effect. and,of course, competitive changes occur as well.with highly dynamic business opportunities, the executives running the competing businesses have todecide what approach to take to address the opportunity and when to make a commitment. if the commitment ismade too early and something significant occurs, a competitor may come in slightly later and address the sameopportunity with a product or service that costs less and does more. in the event of delay, on the other hand, thereis the danger that the market will be taken and will be difficult to penetrate.there can be little doubt that there will be many approaches to providing communications services in whatis now the local loop. indeed, it is the vitality of this massively parallel approach that is one of the great strengthsof our economic system.technology in the local network459the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.if the playing field is truly competitive and without undue restrictive regulations, the best approach, with themost efficient use of technology, should win out. whether this winning approach will be a natural evolution ofthe present telephone network, an expansion and upgrade of the cable network, a wireless approach, or somehybrid of all of the above, in a competitive environment, the ultimate winners will be those who provide what themarket wants at the most economical price.many of the decisions to be made in this arena entail large capital and work force expenditures that, oncecommitted, are expensive to change. indeed, much of the investment in the local networks has not beenrecovered, and large writeoffs could occur if decisions are not made well.the executives who are faced with the decisions on how best to address these markets can protectthemselves from being blindsided by new technology development by becoming aware of the possibilities raisedby new developments. they can likewise measure the competition and feel that they all are at least playing bythe same economic rules. it is on the regulatory front that a more aggressive effort is needed to establish a levelplaying field. the network managers have to be confident that they are in a fair competitive match, and that thereare no underlying rules that favor one competitor over another. if this occurs, then the decisions can be made.the timing of the changes we can foresee, as discussed in this paper, is uncertain. the nature and timing ofregulatory reform play a large role in advancing our telecommunications network, and regulatory reform must becompleted.currently, there is a lot of asymmetry in the application of regulatory rules. the telcos, for example, have autility company obligation to provide basic telephone service at tariffed rates to anyone who requests it in theirserving territory, and to have sufficient reserve capacity to act as a carrier of last resort if some small facilitiesbased reseller experiences a network failure. for years this mandate has been financed by subsidizing individuallocal telephone service from business service and longdistance service so that billed local revenues are less thanthe cost of providing service. this situation is slowly changing as access charges are being lowered and localservice rates are being increased in some jurisdictions. however, the situation that persists in many states is thatresidential local service rates are deliberately set below the actual cost of providing service. moreover, in severalstates, the only way the local telco has been able to get any pricing flexibility for competitive services is to agreeto price freezes for residential service.while regulators have decreed that alternative access providers be granted at least virtual colocation in telcocentral offices to facilitate local telephone competition, they have not yet decided what obligation these newcompetitors have to share in the local universal service mandate. in that connection, gte has proposed to thefcc, to the national telecommunications and information administration, and to the committees in the houseand senate that are writing communications reform legislation that the entire array of explicit and implicituniversal service funding mechanisms be reviewed as a whole, rather than piecemeal, and that a process beestablished whereby multiple providers could become carriers of last resort (colr) and eligible to receiveuniversal service funding. whether or not this proposal is accepted and implemented, it is clear that until theground rules for local telephone competition are acted upon and settled, the regulatory environment couldcontinue to discourage some competitors from rapidly building out or evolving to universal broadband networksin the local loop.the ultimate solution is the establishment of a highly competitive communications environment that wouldbe marketdriven to provide the information any customer may want or need, whether it be "just plain old"lifeline telephone service or a broadband, interactive multimedia connection. the challenge for legislators,government regulators, and business leaders is to come up with a process that moves from today's situation tothis desired goal. the evolution of the local plant is really the key and, as discussed above, occupies much of thethought and planning of local telephone company operators and others who want to participate in that market.notes1. ohr, s. 1995. "fast cache designs keep the risc monster fed," computer design, january, pp. 67œ74.2. ryan, hankin, and june kent. 1994.3. glitho, r.h., and s. hayes. 1995. "telecommunications management network: vision vs. reality," ieee communications, vol. 33,march, pp. 47œ52.technology in the local network460the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.4. generally, we say that there is interoperability between two components x and y when they can interact based on a (degree of) mutualunderstanding.5. for example, you may want to listen to a video on a pots phone, interact with a database service using your tv, or have a videoconference with two of your colleagues at the same time when one of you is using a computer, one is using a screen phone, and the thirdis using a tv. the situation today is that each user would use a distinct cpe depending on the purpose.technology in the local network461the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.54recognizing what the nii is, what it needs, and how to get itrobert f. rochecellular telecommunications industry associationstatement of the problemthe national information infrastructure (nii) offers to strengthen the u.s. economy and promote social andeducational development, but those contributions depend on its deployment. in turn, the ability of wirelessproviders to deploy the facilities necessary to provide the wireless component of the nii depends on thegovernment's recognizingšat all levelsšthat restrictions on deployment also restrict these benefits. thecellular telecommunications industry association (ctia) has urged policymakers to adopt policies that willpromote deployment of the nii. these policies include (1) an executive order from the president directingfederal agencies to make available federal lands and sites for telecommunications facilities; (2) an affirmation bythe federal communications commission (fcc) of the primacy of the national technical standards applying toradio frequency (rf) emissions over local standards; and (3) an affirmation by the fcc of the primacy ofnational telecommunications policy over local policies that are hostile to competition.summarywireless telecommunications is making great contributions to the deployment of the nii. it has already metthe mobile needs of over 25 million consumers in the united states. wireless services are meeting the need forwireless inbuilding services as well, and their potential is phenomenal. for example, the ability of our schools tooffer students a rich experience and access to a broader information base often runs up against the fact that mostschools are not currently wired for telecommunications and computing, and that wiring these schools may posethe risk of exposure to asbestos or the expense of extensive renovation and removal operations. wirelesstelecommunications and computing offer, in these cases, more costeffective and efficient alternatives to wiredsystems1.wireless telecommunications is successful because it flourishes in an environment of competition in lieu ofgovernment regulation. this wireless paradigm has resulted in more than 200,000 new jobs over the past 10years, and almost $19 billion in privatesector investment 2. in spite of these gains, and the promise of as manyas 1 million new jobs and another $50 billion in investment over the next 10 years, there are impediments to totalsuccess 3. wireless service is dependent on the deployment of antenna facilitiesšcell sitesšand the ability ofwireless companies to deploy the facilities for new systems, greater capacity, and broader coverage is at risk.some local jurisdictions are preventing the deployment of antennas, either through outright bans, extensivedelays, or application of unscientific "local technical standards" to radio frequency emissions. ctia has calledfor action to redress these problems and to permit wireless to assume its full effective and efficient role in the nii.backgroundmuch of the discussion of the nii has focused on wired technologiesšpredominantly fiber opticsšas thecore of the nii. that focus fails to recognize that wireless technologies already make up a significant part of therecognizing what the nii is, what it needs, and how to get it462the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.national telecommunications network. wireless already reaches across the united states, with hundreds ofwireless companies competing to offer voice and data services, innovative applications, and value to millions ofconsumers. these hundreds of wireless service providers have been developing, funding, and deploying awireless nii for over 10 years, since the first cellular system began operating in october 1983.for all of that, wireless has almost been the secret success storyšperhaps because it is the success ofprivate enterprise. over the past 12 months, there have been 19,043 references to the nii or the informationsuperhighway in the media 4. of those stories, only 2,139 mentioned wireless or cellular. of course, the reality issinking in that the niišor the information superhighwayšis and must be more than a highfiber diet (of fiberoptic cable and other hardwired systems). the reality is that people are mobile, and mobility implies beingwireless. but being fixed does not necessarily mean being wired. indeed in many environmentsšurban and ruralšfixed services are better delivered by wireless technology than by wired technology.ctia, as the industry association for wireless providersšranging from cellular to enhanced specializedmobile radio (esmr), satellite, and personal communication services (pcs)šhas been relentless in pressing thismessage. ctia and its members also have been relentless in making it a reality. indeed, the ctia foundationfor wireless telecommunications has cosponsored and cofunded wireless education and wireless medicalprojects across the country (box 1 gives two examples)5.increasingly, wireless is being recognized as a vital part of the nii. this forum is one example of thatrecognition. last year the national institute of standards and technology's committee on applications andtechnology requestedšand receivedšcomment on the demand for an nii, and on the role of wireless in the nii6. the office of technology assessment issued a report on "wireless technologies and the nii" in august 1995.this recognition, however, is only the beginning of the battle.box 1 examples of ctiasponsored wireless projectswireless at work in educationon may 2, 1995, the ctia foundation, bell atlantic mobile, and cellular one donated stateoftheartwireless telecommunications systems to two elementary schools in the district of columbia. the classlinksm initiative intends to improve education by bringing wireless telecommunications and information to nowisolated classrooms, allowing schools to link with the internet via wireless modems.wireless at work in medicinethe ctia foundation is funding a project at new york's columbiapresbyterian medical center wherewireless is providing a system of coordinated care to tuberculosis patients. the project, done in conjunctionwith the new york city department of health and the visiting nurse services of new york city, enablesvisiting nurses equipped with laptop computers and wireless modems to treat patients in their homes.most of the wireless components of the niišcellular, esmr, and pcsšrequire the deployment of cellsites as their basic building blocks. these sites comprise antennas and towers, as well as base station equipment.the cellular industry alone, composed of two carriers per market, constructed almost 15,000 cell sites between1983 and 1994. by the end of 1994, almost 18,000 cell sites had been constructed (figure 1).as figure 2 indicates, cell sites have traditionally supported service to between 1,000 and 1,200 users persite. as the number of subscribers increases, the number of cell sites must likewise increase in order to meetdemand and preserve service quality.another 15,000 cell sites may be required for cellular systems alone in the next 10 years, based on theprojections of barry goodstadt of eds management consulting that cellular might achieve subscriber levelsbetween 38.2 million and 55.1 million by 2006 7. (although the deployment of digital technology might reducethe absolute number of additional cell sites required to meet demand because of capacity restrictions, the numberof cell sites required must still increase in order to improve geographic coverage. thus, the precise number ofsuch cell sites is a matter of speculation and is not definitively predetermined by subscribership.)recognizing what the nii is, what it needs, and how to get it463the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 cell site construction (reported as of december 1994).figure 2 reported subscribers (as of december 1994).the construction of nextel communications' esmr network demonstrates that the need for such sites is notlimited to cellular systems. by late 1994, nextel was constructing 2,000 cell sites for its esmr network, and ithad over 130 cell sites in los angeles alone. as of yearend 1994, nextel planned to construct 4,000 cell sitesover the next 5 years to complete its network8.with the deployment of pcs, the number of cell sites will increase dramatically. during the fcc's pcsproceeding, wouldbe licensees estimated that they would have to construct between four and seven cell sites inorder to provide coverage identical to that of one cellular cell site 9. the adoption of higher base station powerlimits will facilitate the use of widearea cells in some areas, thereby permitting a onetoone relationship forpcs and cellular cell sites in those limited areas. however, the need to deploy micro and picocells in order toprovide capacity and coverage in other environments (e.g., urban settings) means that the number of cell siteswill still increase many times.indeed, the rules contemplate six licensees per service area, with 5year deadlines obliging three pcslicensees (each holding 30 mhz) to construct systems covering onethird of their service area population, andthree pcs licensees (each holding 10 mhz) to construct systems covering onequarter of their service areapopulation over that period. thus, the number of cell sites required will necessarily be some multiple of a figurebased upon a single cellular licensee's systemšas pcs licensees must either build out their systems or forfeittheir licenses.analysis and forecast: what buildout will pcs require?as previously noted, the fcc has imposed buildout requirements for both major trading area (mta) andbasic trading area (bta) pcs licenses, such that three licensees will be required to cover onethird of theirpopulation in 5 years, and three licensees will be required to cover onequarter of their population in 5 years.this means that, even starting with a static model, and assuming that the size of the population to be covered isestablished by 1994 population figures, the equivalent of three nationwide carriers will be obligated to build outrecognizing what the nii is, what it needs, and how to get it464the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.systems so that each covers 86.5 million people, and the equivalent of three more nationwide carriers will beobligated to build out systems so that each covers 65 million people.the number of cell sites required to provide coverage for current cellular systems may be used to form thebasis for extrapolating pcs system buildout. using the midyear 1994 number of cellular sitesšover 15,000šwe may calculate that as many as 22,500 cell sites may be required to provide coverage for the equivalent of onehypothetical nationwide pcs system 10. since there are, in fact, six pcs systems to be licensed, including boththe mta and btabased systems, the numbers of such sites may be as much as six times that base figure. (onthe other hand, the number of sites may be lower if operators obtain multiple licenses in a single market, suchthat an mta licensee also holds a bta license in each of its component markets and thereby simply increasesits available spectrum resource to 40 mhz across its service area.)assuming that the mta and bta licensees' coverage requirement can be used to derive a ratio for cell siteconstruction, we can make the following projections: the two (theoretically nationwide) 30mhz mta licensees and the similar 30mhz bta licensees togethercould be obligated to build a minimum of 22,500 cell sites (towers or antenna sites) within 5 years. (thisassumes that each builds 7,500 cell sites or onethird of the theoretical 22,500 maximum required for onenationwide pcs system.) the three 10mhz licensees could be obligated to build a total of 16,875 cell sites over the same period.(this assumes that each builds 5,625 cell sites, or onequarter of the 22,500 maximum required for onenationwide system. this assumption may not be accurate, depending on the applications that thesecompanies seek to deliver to the marketplace. in fact, the 10mhz licensees may have to deploy three timesas many cells as 30mhz licensees to achieve equivalent maximum capacity through frequency reuse.)thus, the broadband pcs licensees could build about 39,275 cell sites over the next 5 years. includingcellular, esmr, and pcs, wireless licensees could require as many as 58,275 new cell sites within the next 5years. (this assumes buildout of multiple pcs systems, buildout of nextel's system in the projected timeframe, and cellular buildout in line with recent growth rates.) in fact, this may be a gross underestimate of thenumber of antenna sites required, in light of projections by paul kagan associates (figure 3) and other analyststhat over 124 million peoplešalmost 45 percent of the u.s. populationšwill subscribe to wireless services inthe next 10 years11.indeed, the number of new cell sites may range as high as 100,000 or more for the complete buildout of allthese licensees, if the equivalent of four nationwide pcs systems are deployed12.what's the problem?the fact of the matter is that in order to create the wireless component of the nii, these towers and antennasmust be deployed. wireless service is dependent on their existence. but the ability of service providers to bringthese services to the public is handicapped by (1) the lack of a process, in some jurisdictions, for granting thenecessary permits to build these facilities; (2) a process, in some jurisdictions, that actually hampers deploymentby imposing unnecessary delays and transaction costs; and (3) some shortsighted actors who, in somejurisdictions, actually seek to prohibit competition or restrict the buildout of wireless services by imposingunscientific "local technical standards" on rf emissions.it is important to note that these jurisdictions have failed to recognize that national policy puts competitionto work for their own citizens' interests 13. other jurisdictions have erred by applying local "technical" standardsthat conflict with the national rf standards adopted by ansi and accepted by the fcc 14. indeed, some of thesedecisions go beyond what even their own technical experts recognize as valid15.recognizing what the nii is, what it needs, and how to get it465the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.we are not singling out a specific level of government as the culprit. federal, state, and local governmentscan help or hamper the creation of the nii. in fact, state governments like those of maryland and connecticuthave demonstrated a clear understanding of the benefits of promoting deploymentšand have moved to helpextend the wireless superhighway. maryland, for example, has solicited requests for proposals and has issued alist of state governmentowned antenna sites that the state will make available to applicants for suitable fees 16.the state of connecticut has essentially established a statewide standard and process applicable to antenna siting,under the auspices of the connecticut siting council.figure 3 paul kagan associates' wireless subscriber projections (october 1994).what should be done?ctia is not advocating a national zoning policy. ctia is advocating that consumers be helped and nothampered by government at all levels.zoning rules that act as a direct bar to entryšsuch as moratoria on tower or antenna sitingšdo not helpconsumers because they obstruct competition and actually reduce the quality of service available to consumers.fortunately, such rules are properly preempted under section 332(c) of the communications act. likewise,zoning rules that indirectly bar entryšrunning counter to the buildout requirements established by the fccšalso are properly preempted under the communications act. since such unreasonable zoning and land useregulations that directly or indirectly bar entry are so clearly inconsistent with the consumer interest, the soleproblem is ensuring that they are recognized as inconsistent with national telecommunications policy.fundamentally, whether those issues are resolved at the federal or state level is immaterial.of course, the reality is that the national telecommunications policyšwhich puts competition to work forthe consumeršmust be recognized as paramount and must be implemented. otherwise, the nii will simply be agrand illusion and never a real producer of jobs, education, or security.ctia has acted to make the nii a reality. indeed, cellular companies have been building the wireless lanesand on and offramps of the nii for over a decade. now, when the means of making the informationsuperhighway are debated, the role of wireless must be recognizedšand the needs of consumers given fullmeasure. ctia has pressed for action to ensure that consumers have access to what they want, when they want it.on october 27, 1994, thomas e. wheeler, president and chief executive officer of ctia, wrote to fccchairman reed hundt, warning thatubiquitous wireless servicešincluding wireless service to schools, libraries, and public safety agenciesšrequiresubiquitous cell sites. too often the process of building the wireless network isrecognizing what the nii is, what it needs, and how to get it466the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.thwarted or delayed by "not in my backyard" activists seeking to block the construction of the national wirelesspathway for parochial or illinformed reasons.17on december 22, 1994, ctia filed a petition for rulemaking, requesting preemption of state and localregulation of tower siting for commercial mobile service providers 18. ctia also supports, through theelectromagnetic energy association, rules preempting state and local oversight of exposure to rf emissionsfrom fccauthorized transmitters19.on march 22, 1995, thomas e. wheeler wrote to president clinton, urging him to issue an executive order"directing federal agencies to expeditiously facilitate the location of wireless antennas on property over whichthey have control" 20. such an executive order would take a "giant step in reinventing government as a force toencourage innovation in competitive telecommunications services"šfostering hundreds of thousands of newjobs and billions of dollars in private capital investment, facilitating the deployment of the nii, and generatingrevenues for the treasury by leasing access to federal properties21.the states have been called the laboratories of democracyšbut the interest of consumers, and the interest ofthe national economy, are not limited by state boundaries. as house speaker newt gingrich has said,we have to look seriously at those areas where the national economy requires preemption. the reason we wentfrom the articles of confederation to the constitution was to allow preemption where necessary. as a general rule,i want to decentralize decisions as much as ican, but clearly, for example, when you are in a cellular system youought to be able to be in any cellular system in america and have it work. you cannot suddenly arrive in a deadspace that has been created by a local politician for cronies who happen to own an obsolete investment.22notes1. see "gingrich praises entrepreneurship in bringing technology to schools," bna daily report for executives, may 2, 1995, p. a84.see also "ctia foundation, bell atlantic mobile, cellular one donate new wireless systems to bring stateofart communications tod.c. public schools," ctia foundation for wireless telecommunications press release, may 1, 1995. see also "southwestern bell'sclasslink sm wireless phone concept improves life at a texas school,'' southwestern bell mobile systems press release, february 1,1995.2. see "reinventing competition: the wireless paradigm and the information age," ctia monograph series, 1995, p. 2.3. ibid.4. lexis/nexis search of the "current news" database for references to the nii or information superhighway, may 9, 1995.5. see, for example, note [1] above. see also, "nynex teams up with thirteen. wnet to provide online anytime, anywhere matheducation," business wire, january 10, 1995. wireless carriers around the country are also contributing resources to support medicalapplications of wireless telecommunications. for example, commnet cellular inc. has supported rural medical services in the form ofairtime contributions to the sioux valley hospital outreach program of sioux falls, south dakota.6. see letter dated december 23, 1994, from randall s. coleman, vice president for regulatory policy and law, ctia, to aratiprabhakar, chair, committee on applications and technology, nist.7. see "evaluating pcs markets," pcs news, january 20, 1994, p. 4. see also roche, robert f. 1994. "pcs predictions andprescriptions: highlights from 32 studies and reports on the prospects for pcs," filed in gen docket no. 90314, april 13, p. 12.8. see "nextel installs alldigital integrated wireless communications network in los angeles," rboc update, september, 1994. seealso harmon, amy. 1994. "nextel launches new wireless service in state," los angeles times, september 23, p. d3.9. see, for example, "us west petition for expedited partial reconsideration and for clarification," filed december 8, 1993, in gendocket no. 90314, pp. 7œ12 (arguing for higher power limits to facilitate competition between cellular and pcs).recognizing what the nii is, what it needs, and how to get it467the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.10. this assumes that pcs companies must deploy approximately three cell sites for every cellular cell site in order to achievecomparable coverage and that a hypothetical single nationwide cellular company would have deployed 7,500 cell sites to providecoverage.11. paul kagan associates. 1994. wireless telecom investor, october 20, p. 9.12. this assumes that cellular companies will roughly double the number of cell sites they had deployed as of midyear 1994 in order toimprove coverage and capacity, and that nextel will build out its 4,000 sites. the equivalent of four 30mhz pcs systems would requirea theoretical total of 90,000 cell sites (4 × 22,500 = 90,000). in fact, the number of antennas may be higher regardless of the number ofservice providers, simply to ensure adequate coverage and service quality.13. see, for example, "blairstown township zoning board of adjustment resolution memorializing the denial of a certain use or'special reasons' variance sought pursuant to n.j.s.a. 40:55dœ70(d)(1) to the application of pennsylvania cellular telephonecorporation seeking approval for the erection of a cellular telephone tower on block 2003, lot 14.01, on the blairstown townshiptax map application zb294," pp. 23œ24 (dated october 25, 1994; revised november 3, 1994).14. see, for example, "village of wilmette resolution 93r34." see also zoning ordinances of jefferson county, colorado, and the cityof stamford, connecticut, which provide that more stringent state or county standards may supplant the 1992 ansi standard. seejefferson county reg. section 2, p(1)(a), and city of stamford ordinance no. 527 supplemental.15. see ryser, rob. 1994. "tarrytown extends ban on installation of new cellular antennas," gannett suburban newspapers,december 6, p. 3a: "we have been surprised by the board's action from the beginning. the expert that tarrytown hired to study (antennatransmissions) came back and found our cellular installation safe."16. see cody, michael. 1995. "bay bridge is potential antenna site," the capital, march 30, p. a1.17. letter dated october 27, 1994, from thomas e. wheeler, president and chief executive officer, ctia, to reed hundt, chairman,fcc, p. 2.18. see "ctia petition for rulemaking," rm8577, filed december 22, 1994.19. see "reply comments of ctia," rm8577, filed march 6, 1995, p. 7 (referring to "petition for further notice of proposedrulemaking in et docket no. 9362, filed by eea on december 22, 1994).20. letter dated march 22, 1995, from thomas e. wheeler, president and chief executive officer, ctia, to president william j.clinton, p. 1.21. ibid.22. speech of house speaker newt gingrich at wireless '95, new orleans, february 1, 1995.recognizing what the nii is, what it needs, and how to get it468the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.55electronic integrated product development as enabled by aglobal information environment: a requirement for successin the twentyfirst centurythomas c. rochow, george e. scarborough, and frank david utterbackmcdonnell douglas corporationthis paper discusses the needs manufacturing organizations will face in the near future in workingelectronically and collaboratively to develop, market, and support products for the global marketplace. one ofthe necessary conditions for success is a rich and robust information environment. it is essential that thisinformation environment be implemented to keep pace with the practical needs of the organizations that use it.users and enablers must mutually support each other if the journey is to be made in time for the travelers tosurvive and prosper.statement of the problemmanufacturing organizations today face a confusing and constantly changing global marketplace.customers have fewer resources to procure products and services. they are less tolerant of performance belowexpectations, and their expectations are higher. budgets are tight and getting tighter as economic constraintsforce industrial companies and customer organizations to become lean. competition is fierce. outsourcingrequires an integration of services that is far more difficult than when the services resided within the sameorganization. product delivery times must be reduced, with no loss in quality. cost has joined performance as aprimary discriminator in competition, and the cost time frame encompasses the entire life cycle of a product.it is the rare company that can address the competitive needs of the marketplace entirely with its ownresources. many organizations are reverting to a simple and proven approach, with a modern twist that offersgreat promise but also brings great difficulties. the simple approach is generally referred to as concurrentengineeringšgetting the right people together, in a timely manner, giving them the right information and tools,and supporting them as they work. the modern twist is that the people, information, tools, and support elementsare not usually cohesive or localized, and tend to change in composition over time.the people are from different organizations, and seldom can more than a few be assembled in the sameplace. the time frames in which they must work have been greatly collapsed. the information they need to dothe job is in various electronic and paper forms scattered in all directions. the tools are often softwareapplications that should, but usually do not, play well together. the automation infrastructure necessary to tie alltogether is often reminiscent of a railway system spanning countries with different gauges of track.the need is for integrated development of a product, executed globally and electronically, by what isreferred to as a virtual enterprise. industrial organizations are beginning to work this way, albeit in an embryonicor at best adolescent sense. but enough has been done to prove it is possible. the challenge is to make itpractical. the compound question to be answered is this: what is required to emplace an environment thatfacilitates globally distributed teamsšof people and organizations, customers and providersšto collaborativelydevelop products and support them worldwide throughout their useful life, at costs that are affordable, withperformance that meets or exceeds expectations? it is the classic engineering problem of producing optimalresults within practical constraints. in this case, these constraints are resources, technology, culture, and time.electronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century469the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.this paper is written from the perspective of an aerospace manufacturer. although some differences withother industries exist, the essence of the needs, problems, and solutions for electronic integrated productdevelopment should be similar across industry sectors.backgroundaerospace is a hightechnology industry that necessarily pushes the envelope of the possible. it has beensaid that aerospace therefore is not a good example of the manufacturing industry as a whole. if that were evertrue, it is certainly not so today. although aerospace manufacturing technologies are often state of the art, theymust increasingly be applied within constraints of practicality; there must be a business pull. we must beinnovative in how we work, do more with less, and concentrate on making an affordable product, not amasterpiece. these are common needs among manufacturing organizations today. fortunately, aerospace canreturn to its roots for salvation, and other industries should be able to do the same.the early daysin the early decades of this century, airplane manufacturers assembled their resources in a hangar and loft.design, manufacture, and assembly were carried out by the same group of people, who often had close ties withthose who would operate and support the product. experienced staff and funds were scarce. innovation andteamwork were necessary conditions for success. the processes by which work was accomplished wereidentified implicitly by the workers, who were usually considerable stakeholders in the enterprise. processeswere reengineered on the fly, and all concerned knew why and with what results. this scenario can differ littlefrom those in other industries in their early days of growth.growth and declinewhat happened, of course, is that as airplanes grew more complex, and specifications from differentcustomers began to diverge, specialties grew and companies reorganized to gain efficiencies. as marketsexpanded commercially and governments began to focus on aviation, performance often became more importantthan product cost; and product cost was considered that of acquisition only rather than including support througha number of years. upstream innovation to help downstream functions was mostly lost. information of a "totalproduct" nature was not available, as functions concentrated on what was needed from their narrow perspectives.a world war followed by the beginning of the cold war put increasing emphasis on survival, and thus onproduct performance in increasingly complex missions. cost retreated further as a major consideration.the boundary conditions began to change in the last decade or so. as the capability of the threat increased,the likelihood of drawnout conflict decreased. the ability to keep weapons systems available for action a largerpercent of the time became increasingly important. as product complexity grew to meet the growing threat, costregained its place as a prime factor in product selection, and a product's lifecycle aspect emerged. with thedisappearance of the threat to survival in the last few years, coupled with the lethargy of the global economy andits effect on commercial aircraft sales, aerospace was suddenly presented with a completely new marketplace,with new rules and conditions, not yet stabilized. the question of survival began to apply not to the nation butrather to the company.the resulting gyrations of downsizing and becoming lean to meet market realities are well known. thoughperhaps in some cases the scope and degree are less, other industries are experiencing similar trauma. there arefatalities, mergers, and a return to a focus on base expertise. increasingly organizations are analyzing where theyneed to go, from what basis; when they have to arrive to survive and prosper; and how they might proceed withinpractical constraints.electronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century470the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.return to basicsaerospace has begun, and has actually made considerable progress. the experience of mcdonnell douglascorporation (mdc) over the past several years is probably typical of many hightechnology companies. wehave recognized that affordability of products from a lifecycle perspective is a paramount consideration. this ofcourse assumes that the products will meet or exceed customer expectations. we have realized that improvedbusiness processes are a most important element and have concentrated on the documentation, analysis, andimprovement of those processes key to product development. increasingly, management of our business is basedon management of our processes.we have also realized that we must return to the base values and practices of our industry. we havereorganized along the lines of concurrent engineering, mdc's term for this being integrated product development(ipd). this has been done in our functional support units as well as in our aircraft and missile programs. wehave also returned to our base expertise, divesting ourselves of interests not closely aligned with the coreelements of our products. we have outsourced much of our automation support and have aggressively struckteaming relationships with companies around the world, some of which continue to be our competitors in otheracquisitions. our ipd teams include customers, teaming partners, subcontractors, and suppliers. as we executeipd, we have come to understand the prime importance of our information assets, and we are taking steps toanalyze and evolve these assetsšold and newšto make their provision to ipd teams adequate for the purposesof decisionmaking and task execution in an electronic ipd environment.status and challengemdc is pleased with its progress, but far from satisfied. the f/a18e/f hornet approaches first flight indecember 1995 with the program on schedule and within budget, and with the aircraft meeting technicalperformance parameters and being below its weight specification. the program critical design review (cdr)found fewer than ten action items to be addressedšnone critical. this compares with several hundred requiredactions found historically across the industry at cdr. the major credit for this performance is given to thecollaborative ipd teams that are developing the aircraft with the highly visible, aggressive, and positiveparticipation of an involved and enthusiastic navy customer. there are no surprises on the f/a18e/f.as part of this initial success story, mdc has applied practical automation enablers in key areas. theaircraft is all digital. we execute an electronic development process that substitutes digital geometry models forthe traditional physical mockup. we define structural and systems interfaces with our major subcontractor,northrop grumman, and perform design reviews and resolve fit and function issues in real time, with datamodels shared electronically between st. louis and hawthorne, california. the product is defined in terms ofassembly layout, buildto, buyto, and supportto packages, containing all the information necessary for aqualified source to provide the part, assembly, system, or service. discipline in the execution of the processes bywhich these packages are created, reviewed, approved, and released for manufacture or acquisition is applied bya control and release system that also ensures the integrity, completeness, and consistency of the data.customers, cocontractors, subcontractors, and key suppliers are linked electronically.although pleased with this progress, we have also learned how far we must still go to be truly competitivein tomorrow's world. we must be able to execute electronic ipd as effectively and as efficiently as charleslindbergh and the folks at ryan airlines applied traditional concurrent engineering to the design andconstruction of the spirit of st. louis, in a hangar in san diego during a critical 2 months in the spring of 1927.we must do this with teammates from organizations distributed around the world, including sophisticated giantsand simple machine shops that nevertheless provide individual links that will set the strength of the chain. wemust be able to collaborate, in real time, on unforeseen problems in short time frames. we must have immediateaccess to the information critical to the task at hand, be it a management corrective action or making aninformation package deliverable against a tight schedule. the information access must be controlled, the dataexchanges must be secure, and the information provided must be correct, complete, consistent, and pertaining tothe right configuration of the part or system.electronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century471the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.this future state is necessary for our continued participation as a major player in our marketplace, just as itmust be necessary to the success of every modern organization for which collaboration and information areessential to survival. and the time frame in which we must emplace the enabling environment is short. certainlyby the turn of the century, successful organizations will work in this way.we should be optimistic rather than overwhelmed, since enough capability has been demonstrated to provethe concept. yet the issues are substantial. progress must also be substantial, immediate, and continuing if we areto arrive at our destination in time to remain competitive.not the least of the elements that enable electronic ipd is the provision of information in a costeffectivemanner to diverse teammates around the worldšinformation that resides in a vast heterogeneous "informationspace" itself distributed globally. however, all the elements are interrelated, and a certain level of understandingof them all is necessary to provide integrated solutions to specific needs.forecastcommon among successful manufacturing organizations will be their ability to assemble themselves inwhatever virtual configurations are necessary to identify, scope, pursue, and capture business, and then performto specifications within budget and schedule constraints. business will be won and executed by virtualenterprises. these will not be static over the life of a product but will change with conditions and the ebb andflow of requirements and capabilities. participants will include customer organizations, hightechnology partnersand subcontractors, and suppliers with various levels of capability and sophistication.the mechanisms by which teams interface and operate must in large measure be common, as solutionsspecific to only one opportunity will be too costly for most players. work processes will be executed in acollaborative manner, in real time where appropriate, by teammates distributed physically and in time.collocation will be electronic, and information needed for the task at hand will be immediately accessible andusable with little or no preparation. information will be provided through a variety of electronic mediaštext,graphics, voice, and video. huge volumes of data will be stored, accessed, viewed, and moved. this will have tooccur efficiently, accurately, securely, and affordably.there is little question that this scenario must be realized in the immediate future for successful enterprises.the prosperity of a nation will be proportional to the success of its commercial and government entities as theyparticipate in the interaction of the global scene. the obstacles to the emplacement and effective operation of thisenvironment are significant, and the cost of solutions in time, labor, and money will be as well. it is essential thatgovernment and industry, while retaining the essence of fair competition that is a core american value, becomepartners in providing costeffective common solutions to shared obstacles that prevent establishing the proverbiallevel playing field.analysissome of the elements necessary for success in the collaborative global marketplace that is upon us arediscussed in the following sections. we identify some obstacles to their successful emplacement, suggest anapproach for implementation, and discuss options for joint pursuit of common objectives.elements necessary for successthere is a close relationship between the collaborative processes by which we must do business and thepolicies, procedures, standards, and automation by which the necessary information is provided in a timely wayto the team members executing (and evolving) the work processes. processes and information are both necessaryelements for success. neither processes nor tools are sufficient by themselves. they must not only coexist butalso actively support each other. innovation in identifying better ways to work is often materially dependent onthe availability of key information and the timely participation of dispersed teammates. the many opportunitieselectronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century472the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.for innovation in providing essential information quickly, or electronically collocating key participants, arebeyond all practical means of simultaneous pursuit. the opportunities need to be prioritized, scoped, andscheduled based on practical requirements for business success.this suggests that the current emphasis on business process reengineering is appropriate and that it shouldproceed with a clear appreciation of the importance of providing information to process steps and players. it ishard to identify points of process integration, or key collaborations or decisions, in which information flow andcontent do not play a deciding role. processes must change, but in a manner that is consistent with the practicalability of information technology to provide the right data to the right participants at the right time.information in electronic form is growing at a tremendous rate. this growth has now reached the point thatit can easily drown a team as it attempts to find and use information related to its objectives. being able toeffectively locate and access pertinent information is important, but it is equally important to do so with the userin control, and protected from the onslaught of unwanted data and the uncertainty and inefficiencies ofunstructured searches.automation enablersthere are any number of ways to categorize the automation enablers of information provision. we havechosen for our purposes to identify four aspects: software that an individual user or team applies to create, analyze, or change data pertinent to a work step.this is often specific to a discipline, and we refer to such software as enduser applications. software and hardware that provide controlled access with appropriate security to information and relatedsoftware applications, by individuals according to assigned privileges. software that manages information (i.e., that ensures its integrity, correctness, completeness, currency, andrelevance to the specific product configuration or version being addressed by the user). the underlying elements, or infrastructure, that enable physical access to, retrieval or view of, or operationson the data permitted to the user. these elements include the hardware and software of networks; varioushardware devices for storage and display of, and operation on, data; software such as operating systems, andsoftware that supports desktop capabilities, file transfers, and electronic mail; and the various data andcommunication standards that facilitate data transfer.all these enablers must function well and in an integrated sense if the information provision environment isto be effectively available to individuals and teams executing business processes. in the desired nearfuture state,they must also perform in much the same way for the variety of virtual enterprises in which individualorganizations may find themselves.lessons from experienceto help identify specific requirements and obstacles, we now summarize mdc's practical experiences todate in our initial exercise of electronic ipd.as mentioned previously, mdc has concentrated heavily on the electronic development process for the f/a18e/f, collaborating with northrop grumman. this has given us valuable experience in the sharing andexchange of large amounts of data in various electronic forms. policies, processes, procedures, automated tools,and infrastructure have evolved. security and communications requirements have been identified and met. for aclose and intensive interaction with a large sophisticated partner, we are comfortable with our ability to perform.to extend interaction to our customers, and to subcontractors and suppliers with whom we interact moresimply or less often, we have established a simple mechanism that we call the citis node. it had its genesisseveral years ago in our new business area, as a proof of concept of the spirit of controlled access to data andapplications suggested by the draft milstd974 for a contractor integrated technical information serviceelectronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century473the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.(citis). based on a database of user profiles, within the constraints of a security firewall and the data accessgranularity of specific applications, we allowed execution of applications, and through them access to data. insome cases, this was access "in place"šthat is, access at the source of the data store. in other cases, data wereuploaded to the citis node server and either accessed there or transferred to the requester's system.no attempt was made initially to include the basic or optional functions identified in milstd974; thisfunctionality was to be added instead as required for a citis under specific contracts. what we found as wedemonstrated this proof of concept to various customers and suppliers was that there were immediateopportunities to use the citis node, in all its simplicity, for exchange of production data. suppliers wereespecially eager to do so. over the space of several years, initially with individual suppliers to our new productsarea but increasingly with customers and suppliers in all our current programs, we have added users to the citisnode and have augmented its operation.today the citis node supports over 750 users, with growth to 2,000 users expected by the end of 1995. astrategic alliance has been formed with a small disadvantaged business, aerotech services group, for the node'sadministration and support. aerotech also provides hookup and help services to suppliers if they so desire. overthe next few years the citis node will be expanded to accommodate the needs of all our programs andproduction centers, with additional functionality added to address the kinds of operations specified by bothmilitary and commercial perspectives of citis.the significance of this story is not mdc's current capability. other organizations today can present asimilar status. the significance is our growing understanding of the obstacles encountered and the lessons we arelearning, through the dual experiences of f/a18e/f electronic collaboration and the extension of electronic ipdto a broad range of suppliers and customers through the citis node.here are some our findings: a clear understanding of the work process is essential if it is to be reengineered to benefit from electronicipd. nothing is harder to facilitate than the change of a work process. this is a cultural change, often a changefrom what has been a successful process in the past. human beings, perhaps as a result of a builtinpredisposition to protect the species from the unknown, are strongly resistant to change. if a change is to be made, it can seldom be dictated. it must occur through producing clear evidence that it isbeneficial to objectives in which the organization has a vested interest. this is best done through executionof the new process by real users doing production work, with proof of the benefits captured throughaccepted metrics. new procedures, techniques, and the application of automated tools must be straightforward and simple, andaccompanied by sufficient training and onsite support. production implementation to any reasonable extent must clearly pay for itself in the near term, usuallywithin a year. collaboration cannot succeed unless all participants have a compatible automation environment. this isdifficult enough to obtain within a single organization. it is much more so when extended to customers andsuppliers in various stages of automation implementation. concepts of operationšwho will do what, when,involving what data and with what basic software and hardwarešfor all participants are essential. organizations can begin simply by exchanging digital data. however, to realize the maximum benefits fromelectronic ipd, realtime collaboration is necessary. this often entails viewing large amounts of data, forexample in an electronic development fixture. current costs to support this are prohibitive except in specificinstances. electronic collaboration requiring large volumes of data, such as electronic development of productgeometry, requires broadbandwidth digital services. shared screen sessions are usually asymmetric, and anarrow back channel will suffice. when videoconferencing is used, the back channel must be symmetric. current interoperability among networks leaves much to be desired, especially internationally. connectionsmust be initiated well in advance. shortfused collaboration is next to impossible. the internet is not viable for conducting the more demanding aspects of product development. it isunreliable, not secure, and cannot support largevolume data transfers with acceptable performance.electronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century474the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. standards and supporting software are far from what they must become if data sharing and exchange are tobe effective as enablers of the virtual enterprise. the processes used to define and validate standards do notprimarily serve the needs of the user community. standards too often do not support real needs and take fartoo long to mature; vendorspecific implementations are common. suppliers, especially small and mediumsize ones, can be overwhelmed by the need to interact with multiplecontractors, as most will have to do to survive. they require help to identify the basic environment to put inplace, as well as training, advice, and support at affordable cost. few manufacturing organizations can afford to take uncertain risks as they move toward electronic ipd.progress must be made with proven technology that can be extended as business conditions allow. finally, cost structures of automation elements and services must accommodate the need of organizations topay as they go. unless business cases can be developed through production pilots to validate real benefitsand the manner in which to capture them, organizations will be inhibited from moving forward at areasonable pace.recommendationsthe need for u.s. organizations of all kinds and sizes to be able to participate in the collaborative globalmarketplace is obvious. many of the obstacles are too large for specific solutions worked by individual players,or even by industry sectors or government agencies. for reasons of cost sharing and timely progress, u.s.industry and government must work together and must jointly cooperate with the rest of the world. the problemsare global in nature. all concerned will profit if the solutions are also global.the information environment must be defined and implemented with a clear understanding of its use, in thesense both of providing support for current production needs and providing for future growth by timelyidentification and validation of extended capabilities.there has been much discussion of the information superhighway and of the associated visions of thedefense information infrastructure, the national information infrastructure (nii), and the global informationinfrastructure (gii). these are well worth pursuing, but it is also useful to consider the example of thetransportation system in defining a practical information environment, setting priorities, and supporting theimplementation and integration of the various elements.besides superhighways, a national transportation system must have access and exit ramps, outer roads, localroads, and streets leading to neighborhoods of homes, schools, stores, and industrial parks. it must have laws,regulations, and tests to accommodate experienced drivers, beginners, and those with special needs. it must havestop lights, stop signs, and informational signs. it must provide maps of various levels of detail, and triptychswhere appropriate. it must provide a measure of free advice and assistance, as well as affordable services forfuel, maintenance, repair, and other needs. and its users must be prepared for travel in similar but differentenvironments in other countries around the world, with common regulations negotiated at the national level, aswell as reciprocal acceptance of national standards.it is beyond the scope of this paper to complete a detailed comparison of the information provisionenvironment with the transportation environment we all know so well. one difference is worth noting: thebuildup of the transportation system was mostly serial, expanding from a local base, and it occurred during amore leisurely time. the elements of the information environment must be addressed in parallel because of thepressure of fastmoving events, and no elements can be ignored, no matter how insignificant they may seem. allare necessary for its successful operation.a collaborative information environmentour primary recommendation is that industry and government together must define and describe aninformation environment to support electronic integrated product developmentšand most other kinds ofbusiness interactionsšas carried out in virtual enterprises. they must then jointly implement and enhance thiselectronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century475the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.environment over time, concentrating on addressing key aspects that can best be worked in common for thebenefit of all, and taking special care to enable small and mediumsize businesses to become active players.this will require traditional planning and project management efforts. it will also require an innovativeapproach to gathering requirements, validating elements of the environment, and aggressively addressingcommon problems and issues in much shorter time frames than those experienced today. a proactivecollaborative approach is necessary to quickly identify and emplace a basic information environment that canthen be developed as aggressively as available resources, technology, and culture will allow.there follow recommendations on specific obstacles, based on mcdonnell douglas's experience to date.the concluding section discusses mechanisms for a joint effort to define, emplace, and enhance the overallinformation environment.small and mediumsize businesseswe need to offer small and mediumsize businesses special help. this should include help withunderstanding the environment and how to get started and grow in capability at a practical rate. it should includeadvice and training opportunities. one aspect should be a free initial consultation; a second should beidentification of a set of service providers who offer continuing assistance at a reasonable cost.several governmentfunded providers of such services exist. two of these are the nist/manufacturingtechnology centers and the electronic commerce resource centers, funded by nist. there are undoubtedlyothers. one essential task is to identify such existing entities and recommend how they might best cooperate tomeet the need and minimize redundancy.standardsstandards bodies need to be driven by users and to acquire a sense of the urgency of the real world. thereshould be effective mechanisms to collect and channel real requirements and to facilitate rapid prototyping andaccelerated validation of draft standards. users need to take back the initiative from the standards bodies and thevendor community. standards are also information products, and they will benefit from collaborativedevelopment.affordable and timely implementationthe investment in new automation to execute electronic ipd is daunting for large organizations. there is alegacy of software and hardware that cannot easily be replaced without hard evidence of reasonable return oninvestment in a short time frame. in addition, however inefficiently they may be supported with currentprocesses and tools, production work and new business efforts are ongoing and cannot be put at risk during thetransition to a better world. often there is a catch22. for example, department of defense production contractsinclude no funds for government upgrades in processes and tools to complement contractor internal upgrades.business cases to obtain funding usually require verified benefits in a production environment. automationvendors are often reluctant to support contractors and their customers in such pilots, merely in the hope ofeventual broad production use.this does not mean progress is not made, but it is slow and painful to line up the participating parties andtheir resources. the completion of desired connections through existing physical paths can also be exceedinglyslow, taking a minimum of 60 to 90 days, often much longer. this inhibits aggressive expansion and prevents theflexibility needed to take advantage of shortfused opportunities.there is a need for an innovative sense of joint partnership to define and validate new processes and toolsquickly, with shared risk, among the players. ways must be found to emplace the overall environment in anaffordable manner and to pay for implementation, enhancement, and ongoing support in a shared way fromcaptured business benefits.electronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century476the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.access and securityelectronic collaboration requires ready access to information. but of equal importance is limiting access toand operations on information in accordance with assigned privilegesšnot only for reasons of commercial ornational security, but also to ensure the integrity of the data against malicious or inadvertent alteration. accessand security make opposing demands on an information environment, and we must find ways to strike a practicalbalance. understanding the requirements of electronic collaboration in support of production work, andcollaboratively addressing them, is the preferred approach.the logical course is to move to the open software foundation distributed computing environment,supplemented with security products based on publickey encryption such as rsa (named after its authors,rivest, shamir, and adelman). robust firewall systems must provide security for controlled entry into anorganization's data environment, with systems such as those provided by product data management vendors tofurther limit access to, and operations on, specific data.selfdiagnosis and prescriptionwe are entering a complex and fastchanging world. all the involved organizations, perhaps especially thesmaller players, should continually evaluate where they are, where they need to be, and how to get there in areasonable manner. there is a need for a practical way to make this evaluation and to plan corrective action andfuture growth. a capability model is required that can be selfapplied. it should be as simple as possible and stillgive the needed answers. it should support the growth of capability in a series of finite steps from initial simpleparticipation to attainment of the full sophistication of a virtual enterprise, with easily understood entrance andexit criteria for each step.two related and complementary efforts along these lines come to mind. the first is a cals enterpriseintegration capability model under development by the edi world institute in montreal, under the leadership ofsusan gillies. the second is a cals electronic commerce continuum model proposed by mike mcgrath at thedefense advanced research projects agency. the cals industry steering group is currently serving as aforum to support and relate both efforts.an information environment atlasindustry and government need an atlas of the nearterm global information environment that identifies andexplains the elements and their relationships, and how they can be used to support electronic integrated productdevelopment. this should be a natural byproduct of a plan to emplace the information environment. it shouldidentify and describe the elements and their interrelations. it should give guidance for connecting into theenvironment initially and for growing in ability to use its sophisticated capabilities. it should identify where togo for advice and help, from both government and commercial sources. it should include models and techniquesfor selfassessment of status and for determining readiness to move to a higher level of capability. it shouldinclude references for more detailed information. finally, it should describe the forums and mechanisms bywhich the information environment is supported and advanced, and how individuals and organizations canbecome active in the forums and participate through the mechanisms.forums and mechanismswe need forums and mechanisms to define and execute the plan to emplace and support the informationenvironment we have been discussing. this includes defining practical scenarios of how we must work and fromwhich we can draw requirements for policies, processes, and automation enablers valid across industry sectors.we then need to validate these elements in support of real production work and capture the metrics and lessonslearned with which to justify broad implementation.electronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century477the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.it is difficult for individual industries or government agencies to address this need. we should instead havea means to assemble the appropriate representatives and do the job for all. this is not to say that individualelements should not be addressed by the most appropriate organizations. but the description of the environment,the generation of an overall work plan to achieve and sustain it, and the monitoring of its implementation andenhancement should be done by an entity that is representative of all the players.two ways come to mind to accomplish this. one is to convene an ad hoc team, and the other is to use anexisting organization of a multiplayer nature. in either case, a description of the environment and a work planshould be generated quickly. the work plan should be of sufficient detail to allow selection of the best means toachieve each major deliverable.an existing organization that would be a viable candidate is the cals industry steering group (isg). it ischaired by usaf lt gen (retired) jim abrahamson and has members from many industries as well as closeliaison with government agencies. it has recently restructured to emphasize the requirements of both thecommercial sector and the government, and seeks to enable the reengineering of business processes through theeffective and efficient sharing and exchange of data electronically. it has begun to negotiate agreements withorganizations having similar interests to support a focus on common problems and to avoid redundant andperhaps competing uses of resources.one advantage an existing organization has is that it would be positioned by its charter and interests to actin a coordinating, monitoring, and steering role, as well as offering a mechanism for actually executing parts ofthe plan. it could collect common requirements and issues and act as an ''honest broker" in facilitating commonsolutions. it could also coordinate production validation of solutions through pilots in its member organizations,as well as support prototyping of new technologies. it is user driven, which is essential if the new environment sofacilitated is to be practical and responsive.if an organization such as the cals isg were chosen as a prime mechanism, it would be important thatfulltime staff be provided to complement the mostly volunteer resources that currently support its efforts. thereis no substitute for the enthusiasm and ability of the dedicated and talented volunteer, but the realities of thebusiness world necessarily limit the time and energy these individuals can provide.electronic integrated product development as enabled by a global informationenvironment: a requirement for success in the twentyfirst century478the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.56interoperability, standards, and security: will the nii bebased on market principles?quincy rodgers1general instrument corporation2backgroundthe united states stands on the verge of a quantum leap in the capabilities of its national informationinfrastructure (nii) 3. widespread use of fiber optics is one element of that leap, as is the incredible advance inmicroprocessing, which has supported breakthroughs in digital compression. these, when applied to video, havemade it possible to dramatically increase available communications bandwidth.just 5 short years ago, alldigital video was seen as impossible before 2005, if then 4. the amount of data orinformation required for fullmotion video is orders of magnitude greater than that required for voice ornonvideo data transmissions. the data for video demands bandwidth beyond that required for voice andnonvideo data and requires effective compression techniques, to use the bandwidth efficiently or to meet someexisting bandwidth limitation. in june 1990, general instrument corporation announced that it had solved thisproblem and would present an alldigital, highdefinition television (hdtv) broadcast transmission system forconsideration by the federal communications commission and its advisory committee for a u.s. advancedtelevision standard. one writer called this the technology equivalent of the "fall of the berlin wall" 5.the conversion of video into a usable digital data stream is the third leg of the multimedia stool. the othertwo legs are data and voice telephony, the first being inherently digital, the latter having undergone a transitionfrom analog to digital over many years. carriage of video, voice, and data constitutes the foundation of theadvanced broadband networks now being developed for both telephone and cable television networks.behind this simple concept, however, is a roiling pot of players and special interests, each trying to invadethe other's turf or at least protect its own. convergence is real because digitization breaks down the old barriersbetween service providers 6. it is increasingly hard to tell computer companies from videoequipment providers.it becomes increasingly more difficult to tell cable television operators from local exchange telephonecompanies. the growing community of internet users adds to this mix, some of whom have an almost religiousfervor about its uses and potential.all of these players approach the information superhighway differently. their perspectives and biases, atleast initially, have been parochial and have caused them to overlook matters and issues that are important toother groups. the wellworn parable of the blind men and the elephant is apt here. the nii presents differentcharacteristics and different issues, depending on how it is approached. when these converging players sit downwith each other, they often find that they not only think or see things differently, they even tend to use a differentvocabulary. what one set of players views as communication is mere babble to another.put another way, the convergence of technology is proceeding well in advance of the "cultural"convergence of the various segments of the industries. the resulting different perspectives influence the publicpolicy debates that surround the nii. this paper addresses some of the issues of those debates, based on thefollowing assertions: the nii is not synonymous with the internet; open systems do not have to be, and perhaps should not be, nonproprietaryšinteroperability is not the onlyor even the major public good that must be served;interoperability, standards, and security: will the nii be based on market principles?479the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. the private sector, not the government, should set standards; de jure standards, whether set by governmentor the private sector, are not preferable to de facto standards; and unless security and privacy are protected, the nii will not reach its full potential as a platform for electroniccommerce.these assertions may be controversial; many run counter to conventional wisdom, particularly that of thewashington, d.c., community. however, all are marketoriented. and they are fundamental to u.s. economicgoals.the nii: not synonymous with the internetentertainment is the engine that has already pulled broadband networks into over 60 percent of americanhomes. entertainment will drive the investment necessary to upgrade those networks as well. this is not a facteasily accepted by some internet aficionados and some in the computer industry. many in those groups joinmillions of other americans who have a low opinion of much of current television programming. many view theinternet as a liberator from tv's vast wasteland. but there are also other reasons why the role of entertainment isnot more widely acknowledged by the computer world 7.computer businessmen are aware that today's dominant entertainment terminal, the television set, is anextremely costsensitive (and lowcost), relatively simple piece of electronics, geared to nonbusiness consumers,with a life of over 10 years. this is a long way from their preferred business model. the consumer electronicsindustry is dominated by foreignowned companies; the computer industry is u.s. based. the television industryhas relied on interlace scanning and sees it as important to keeping down the cost of its investment 8; thecomputer industry wants progressive scanning formats.likewise, those in the computer business look at other players and find them to be quite different fromthemselves. the cable television industry has only recently emerged from its "pioneer" phase and still revels inits "cowboy" image 9. cable operators have an obsession with cost control, based on experience with massconsumer marketing. where the computer industry has traditionally sold into the business community, cableoperators have focused on residential customers. regional bell operating companies (rbocs), in contrast toboth, have previously been large, sleepy, and cumbersome bureaucracies, typical of utilities, and the antithesis ofthe computer industry 10.the computer and internet communities are not alone in their suspicion of the alien cultures that havesuddenly entered their world. about 15 months ago, a highlevel business manager 11 for a leading supplier tothe cable television industry told his staff, "somebody is going to have to explain this internet to me!" he wasreflecting not only his lack of knowledge of the phenomenon but also his exasperation at the whole range of newfactors he had to consider as he developed his current core businesses.so it is not surprising that computer business managers tend to gravitate toward an internet model of thefuture nii. they will naturally have less enthusiasm for the role of entertainment than those who have worked inthat field 12. but to recognize entertainment as the primary engine of the deployment of advanced broadbandnetworks does not denigrate the role or the importance of the internet. the nii is and should be about a lot morethan just selling payperview movies or making it possible for people to watch retuns of roseanne orbaywatch . the advanced broadband networks that will share the task of serving as the backbone of the nii areabout making video an integral part of all communications. the addition of video capability has major, positiveimplications for education, health, and business efficiency.what entertainment can do is bring this broadband capability to every home and business. it can and willcarry the major load of the investment needed to do that. when cable television operators begin to deploy digitaldecompression terminals, they will be putting into each user's home a level of computing power that is theequivalent of yesterday's mainframes. far from detracting from or conflicting with the internet, the broadbandpipes of these networks will make internet access via highspeed connections available to an increasingly widerrange of americans 13.interoperability, standards, and security: will the nii be based on market principles?480the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.achieving open systemsopen systems do not have to be, and perhaps should not be, nonproprietary; interoperability is not the onlyor even the major public good that must be served. for 2 years, the rallying cry has been, "we must take steps toensure that the new technologies result in 'open and interoperable' networks." the general accounting officehas proclaimed that "interoperability will be difficult to achieve" 14. members of congress have echoed thisbelief 15.but is the situation really dire? is there really a problem here? is government intervention required?thankfully, the conclusion is "no," because the cure could easily be worse than the disease.of course, interoperability is actually quite simple to achieve. all you have to do is standardize all elementsof advanced broadband networks and the equipment that will use those networks. this can resolve all issues ofcompatibility, portability, and interoperability. requiring compulsory licensing of any intellectual property thatunderlies those standards then provides unlimited competition in the supply of equipment for the networks. it isthat simple, at least in concept. but even laying aside the political and legal barriers to such a program, achievinginteroperability through central planning is a daunting, even overwhelming, technical task.the view that interoperability is a major problem and that it might require such stultifying and intrusivesteps rests on an outmoded view of communications capacity. in an environment of limited bandwidth,legitimate questions have been posed about the allocation of that scarce resource. when a cable television systemcarries 36 channels, issues about who gets "shelf space" in that system can take on major importance.broadcasters demand "mustcarry rules," because they are worried about retaining access to viewers.programmers express concern that their products will receive less favorable treatment than those developed bythe operator on whose system they seek to be carried. new services are concerned that they must go through anestablished multiple system operator to obtain the critical mass of viewers necessary for viability.in an environment of bandwidth abundance 16, the incentives of network providers look quite different thanthey do in an environment of bandwidth scarcity. this is particularly true where the network provider facescompetition. to pay for the network, providers have an incentive to include as many products and services aspossible. they also have an incentive to make interconnection as easy as possible, and to develop consumerfriendly approaches to equipment design. for some purposes, network operators will continue to function in abroadcast mode. as channel capacity dramatically increases and fewer homes are served per node, each home orbusiness served has the equivalent of one or more dedicated 6mhz channels 17; this service, which resemblescarriage more than broadcast, can be expected to develop as a result of consumer demand.the incentives created by competition and broadband abundance will include incentives to achieveinteroperability. moreover, interfaces on these networks will remain open and do not have to be nonproprietaryfor this to occur. network providers will avail themselves of the benefits of proprietary technologies but will, intheir own interest, require that they be open to ensure competitive markets for equipment 18.just as the technology, by altering incentives, makes interoperability more likely or more easily achieved, soalso is it inherently more conducive to interoperable solutions. problems that existed in an analog environmentare more easily solved in a digital environment.during the early days of nii debate, vice president al gore used a metaphor that was intended to justify agovernment role in setting ground rules for the nii. the vice president referred to the need for setting standardsfor interfaces on the nii and drew an analogy to the standardization of the gauges of connecting railroads 19.analysis of this metaphor suggests a major distinction that tells us something important about the digitalenvironment.railroad gauges, in fact, provide the perfect illustration of the digital environment's inherent ability toprovide new answers to old problems. the chief difficulty created by the use of different gauges by connectingrailroads was that, to move from one to the other, freight and passengers had to be transferred from one car toanother car suitable for the different gauge. people and goods moving in interstate commerce could not beefficiently transported under such circumstances 20.however, because of the essential nature of digital communications, data can be offloaded, reloaded, andmoved from one type of network or environment to another with comparative ease. indeed, this flexibility isfundamental to the digital revolution, the foundation of convergence, and the basis of multimedia. in thisinteroperability, standards, and security: will the nii be based on market principles?481the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.environment, the question of standards can often be replaced by another set of questions that relate not so muchto the difficulty but rather to the cost of conversion. that places the matter in the hands of the market, not ofregulators.moreover, in the digital world, breakthroughs continue. when addressing the issue of selecting the formatfor its highdefinition television system, the grand alliance 21 discussed whether it should employ an interlacescanning format or a progressive scanning format. upon investigation, it concluded that receivers could easilyand economically convert the signals from one to the other. the result was that the system will support sixformats and provide both interlace and progressive scanning.a similar experience is occurring in the case of the modulation schemes used by different networks.satellite and cable systems use different modulation. the grand alliance system currently in testing would useyet a third method for broadcast television. the development of multiple systems is not surprising, given the factthat network operators want to maximize their networks for their specific needs and use. there are actually goodand sound reasons to tolerate the lack of interoperability that this could create. but there, it appears, potentialissues may evaporate because cablelabs has indicated that it thinks dual or multiple modulation chips arefeasible and economical 22. thus, because of the inherent characteristics of digital technology, there is reasonablepromise of avoiding inconsistency among networks.where interoperability is being addressed in the marketplace as a result of competition 23, incentives, andthe technology's ability to support conversion, it avoids negative costs of standardization. that is indeed a goodthing because standards are a mixed blessing.standards can and do inhibit innovation. the problem was outlined in a recent speech by gi's ceo, danakerson:the lesson of the personal computer success story is that the government should not prescribe technologicalstandards in dynamic industries. such standards freeze the current level of technology in place and they stifle thedevelopment of new technologies. when the government lets the marketplace operate, innovators innovate,competition flourishes, and consumers' choices increase. and, finally, equipment prices plummet. when the time isright, the technology will mature and the market will set standards and insist on interoperability, the need forcompetitive pricing, and the availability of compatible equipment. 24where standards cannot be avoided, they should be narrowly focused. for that reason, interface standardsare preferable to standardization of functionality, such as compression, transmission, and transport. in effect,interface standards at least restrict the "zone" where innovation can be stifled and provide some opportunity forinnovation to develop around them 25.there is yet another element to standardization that should give pause to those public officials who wouldapply it to these emerging industries. standardization tends to commodize a product or technology. in their bookcomputer wars, an insightful study of ibm and business strategies in the computer industry, charles h.ferguson and charles r. morris describe the effect of standardization:japanese companies, and asian companies generally, have succeeded, by and large, by being superb commodityimplementers within welldefined, stable, open, nonproprietary standardsšstandards, that is, that are defined byregulatory agencies, other government bodies, industry standardssetting organizations, or very slowmovingincumbents, such as ibm has been in mainframes in recent years. nonproprietary standard products, such asmemory chips, printers, vcrs, cd players, or facsimile machines, are brutally competitive businesses, with highinvestment requirements and razorthin margins.but industries that are fastmoving, where standards are constantly evolving, and where the standards themselvesare within the proprietary control of an individual company, are hostile environments for commodity implementers.and the computer industry in the 1990s, under the technological impetus and creative impulse of the americanstartups, has been transmuting into just such an industry, shifting the ground out from under both the slowmovingwestern giants and the commodity manufacturingoriented japanese giants. 26–interoperability, standards, and security: will the nii be based on market principles?482the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the race over the next decade will be between the japanese and american styles of industry development. thejapanese will drive to componentize and commoditize every sector of industry so their great monolithic and leanmanufacturing skills can define the industry's future. american companies must keep accelerating the pace oftechnological change to avoid giving the japanese a stable target to shoot at, and at the same time develop theirown manufacturing skills to the point where lowend market share is not conceded too easily. 27so the problem is not merely the stifling of innovation. rather, it is that the very industries of criticalimportance to the u.s. economy are most susceptible to the negative effects of standardization. the digitalrevolution is uniquely the product of u.s. research and development. it represents a national asset, no lessbecause it arrived on the heels of a period of "technology pessimism" in the late 1980s. american leadership waswidely viewed as losing technology leadership for u.s. industries. frequently, the problem was seen not as afailure of american invention but as a failure to capitalize on that invention. many saw governmentledindustrial policy as the answer to regaining such preeminence.commoditizing the new technologies of the digital revolution will demonstrate that we have learnednothing from that earlier debate.who should set the standards?to the extent that standards are used, there is an issue about how they should be established. here again,among the converging industries, differences in experience and practice create different perspectives. thebroadcasting industry, for instance, lives in a world of government standardization, arising out of the practicalproblems created by potential interference among users of the radio spectrum 28. in addition, standards andregulations have pushed beyond this limited rationale to govern some features and functions of receivers. to anextent still being debated at the fcc, congress has applied this form of standardization to the cable televisionindustry as part of the cable television rate regulation legislation of 1992 29.similarly, the telephone industry is no stranger to standardization. in a unified bell system, standardizationwas achieved within the company. but it has been the history of the telephone industry not only to live withstandards but also to live with and even thrive under government regulation. tell a telephone company executivethat the government is contemplating standardizing some new product and the first question he asks is whetherthat isn't already regulated in his business; if it is, he dismisses it as a problem and moves on to the next issue.the computer industry traditionally takes a different view of both standards and regulation than do thebroadcast or telephone industries. appropriately, it devotes significant energy and resources to remaining free ofregulatory schemes and governmentimposed requirements. there are industry standards, but they rise and fallvery quickly and are constantly subject to attack and replacement. in the words of one broadcast official and parttime humorist: "the computer industry loves standards; it has a million of them."the result of these differing attitudes, springing from different cultures and experiences, is a general policyapproach that says the government should stay its hand and take a minimalist attitude toward standards andregulation, intervening only where there are overriding public interest concerns. however, this merely removesthe debate to whether a given standard, regulation, or other proposal falls on one or the other side of that line.the vagueness of this trigger for intervention and the fact that different biases are brought to its applicationby different groups of government officials invites politicizing of technology decisions. thus, these issuesbecome part of the political bazaar of washington, a place illsuited to making difficult technical choices. theprivate sector, not the government, should set standards; de jure standards, whether set by government or theprivate sector, are not preferable to de facto standards.the computer industry is wise to oppose governmentmandated standards 30. even where congressional andregulatory officials can clearly formulate and enunciate their policy goals, their ability to penetrate to the nextlevel of analysis and understand the technical ramifications of a given set of choices is severely limited.government intervention is rich in the possibility for mistakes and for the operation of the "law of unintendedconsequences." and government almost always seeks compromise and consensus 31.interoperability, standards, and security: will the nii be based on market principles?483the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in recognition of these weaknesses in government intervention, many in the private sector, and some ingovernment itself, have been advocating the position that these issues should not be addressed by governmentbut should be left to private, standardssetting organizations. decisions made in this fashion are preferable todecisions made by government officials. but there are good and valid reasons to ask whether substitutingindustry bureaucrats for government bureaucrats is, in the long run, a major improvement. there are validreasons to ask whether this is a procedure that will yield the best results for the u.s. economy.one problem is in the multiplicity of groups vying for the privilege of writing these standards. in the wakeof the hype that accompanied the focus on an information superhighway, there are no fewer than 49organizations or associations 32 claiming to have a role in the development of standards for digital compressionor for the "settop box" 33. some of these groups are established standards organizations. others areentrepreneurial in the washington, d.c., sense: find an issue, organize around it, raise some money, and exerciseinfluence. some groups are national. others are international. and in some you cannot be sure whetherparticipants represent a u.s. or a foreign interest.these different groups represent different perspectives, depending upon whether their members comeexclusively or in part from the broadcast industry, the cable television industry, the consumer electronicsindustry, the telephone industry, the programming and production community, or the computer industry. thislevel of activity is one indicator of the reality of convergence; they are all converging on the "settop box."another problem arises because private standardssetting efforts are as susceptible as is the government toattempts to use the process for some narrow end or to benefit some special interest. after all, losers lovestandards. the techniques vary but the result can be the same. bureaucratic standards setting is slow moving,ferguson and morris complained 34. pending standards can freeze the market and delay investment. parties thatare behind in the development process can use this delay to catch up to the industry leader. frequently, thosebehind are numerous; the leader is, by definition, alone.like government, private standardssetting organizations generally rely on consensus 35. this kind ofcompromise yields a lowestcommondenominator approach 36. the fact that favoring consensus and notinnovation can harm the economy is difficult for such groups and for the political process to comprehend; theeffects are felt over time and diffused across the economy in general.but the major problem arises because standards setting by bureaucratic bodies, whether public or private, isthe antithesis of entrepreneurial development 37. the entrepreneur invents something, innovates. the productmay or may not be technical. then the entrepreneur "undertakes" 38 to bring together all of the things needed totake a product or service to market: financing, distribution, and so on. the entrepreneur's idea is then accepted orrejected in the marketplace.by contrast, bureaucratic standards setting invents nothing. its conclusions are never really tested in themarketplace (although, if they are too far afield, they might be rejected outright).these standardssetting activities are a drain on resources for any company that participates 39. the smallerthe company, the bigger the drain. such activities require the diversion of valuable and scarce engineeringresources. thus, there is an inverse relationship between the size of the company and its ability to stay involvedand protect its interests. such private standards setting can work to the detriment of the entrepreneur to the extentthat it prevents or limits his or her ability to obtain the fruits of the innovation. it can delay or even prevent thedeployment of technologies 40. and, in addition to these factors, the entrepreneur can frequently end upoutgunned and outmaneuvered. if he or she is the inventor, the other players seek to benefit from the invention.in this environment, the entrepreneur gets little or no help from his government, which tends in any disputeto favor the side with more and larger players 41. what is more, government worries about de facto standardsbecause they are messy: people might complain.the most common argument against de facto standards is that "– we do not want to have another beta/vhs situation!" how one feels about the beta/vhs competition seems to be a matter of perspective, akin to theway that some people look at a glass of water. the "halfempty" crowd looks at beta/vhs and calls it badbecause it standed investment 42. but the ''halffull" crowd looks at the competition between beta and vhs andconcludes that it was a good thing because it resulted in improved vhs technology and ultimately providedconsumers with more choices.interoperability, standards, and security: will the nii be based on market principles?484the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.rather than engaging in government standardssetting activities or in bureaucratic standards settingactivities 43, what is needed is a new willingness (or a renewed willingness) to permit the market to work, topermit people to strike out and establish de facto standards, and to endure the uncertainty and even the temporaryincompatibility that this sometimes creates. in the words of joseph shumpeter, "let the gale of creativedestruction blow through this industry."unless security and privacy are protected, the nii will not reach itsfull potential as a platform for electronic commercethe deployment of advanced broadband systems will depend upon private investment.44 the success ofthese systems will depend upon finding applications that will support that investment.deployment requires two conditions: a robustly competitive market for digitally based products and services, with strong economic benefits tothose that develop the intellectual property that manifests itself in such products; and constant innovations in technology that facilitate this competitionšinnovations fueled by similar marketforces.these are the forces that drive a system of dynamic competition in this rapidly evolving digital world.these forces need to be encouraged and protected. this can occur only if there is (1) a recognition that securityfor protecting intellectual property is a critical element, and (2) a recognition that security systems must berenewed, or evolve, through continued technological innovation if they are to keep pace with those who wouldseek to violate property rights.based on experience over many years, there are some fundamental facts and principles that should drive allpolicy: no matter how good the security system is, it will eventually be penetrated if the value of the material beingprotected is great enough. for this reason, security must be renewable. the fact that security is renewable isitself a disincentive to attempts at signal theft; for security to be renewable, government policy must not hamper innovation in the development of newresponses to security breaches and in the development of new forms and methods of security; a single, national, uniform security standard, which is frequently advocated under one guise or another, is adangerous idea. not only does it provide attackers with a single target with enormous return, but it alsowould stifle the innovation necessary for security to stay ahead of attackers. a single, national, uniformsecurity standard should not be advocated, advanced, or supported by the government; published ("open") standards for security systems tend to weaken rather than strengthen security. thus,unbundling and openinterface requirements, where they are employed, should be limited to functions thatpose no threat to the intellectual property of programmers; security functions should be placed in the hands of those who have an incentive to protect intellectualproperty. proposals either to permit or to mandate their placement elsewhere should be resisted; and although softwarebased security may be adequate for some applications, hardwarebased security may beneeded for others.security for and protection of intellectual property are fundamental needs for the success of advancedbroadband networks, but no less important is the ability of those networks to protect personal privacyšthat is,the privacy of personal information. if advanced networks are to become "platforms for electronic commerce,"they will need to support a wide variety of applications, including delivery of electronic products, homeshopping home banking, and medical assistance. the willingness of citizens and consumers to employ thesenetworks for these purposes will depend upon their sense of confidence that their personal information is securefrom prying eyes.interoperability, standards, and security: will the nii be based on market principles?485the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.proposals to require that equipment on these advanced broadband networks (and current cable andtelephone networks, as well) be made available at the retail level 45 are an example of the way that regulating oneaspect of a network can distort the operation of all others. in this case, the negative implications are for security.consumer electronics retailers would, not surprisingly, make retail availability of equipment the central tenet ofall policy. naturally, proponents of this point of view ignore other problems that occur when this happens, suchas freezing technologies or interfaces or creating problems for maintaining security. they would turn technologydecisions into political decisions.these proposals to mandate retail sale of consumer equipment used with the network are supported bydubious analogies and assertions. one such is the attempt to apply to video distribution and developingbroadband networks the model currently used for voice telephone service, whereby an interface specificationwas developed and used to separate network functions from the functions performed by "consumer premisesequipment" 46. the telephone model is viewed as a way to create competitive markets for equipment used tosupply broadband services47.a major problem is that these proposals ignore the fact that voice telephone service requires only accesscontrol; video providers have traditionally (and for good reason, given the problems of theft of service) insistedon scrambling as well. nevertheless, there are indications that the markets will develop such separation, andprobably more quickly, without government interference 48. another claim made by proponents of this type ofgovernment intervention is that new "smartcard" technologies are available. they cite draft (not even finalized)electronics industry association standards for such security systems 49 and seem oblivious to thecontentiousness regarding the adequacy of such standards and what features are needed to bring them up toacceptable levels of security50.the principle that underlies these proposals is that the government can and should determine how muchsecurity is adequate and that that determination will bind owners of intellectual property and network providers.that principle is unacceptable51.owners of intellectual property and those acting for them should have the right to determine the level andmethod of security appropriate for their needs. these decisions should be made on the basis of business needsand realities, not by government.as network providers experiment with different types of broadband networks 52, some of which havedifferent implications for security requirements, the government should let these technologies work themselvesout in the marketplace, free from the distortions that could be created when the government picks out one factor53 among the many that must be considered, thereby affecting all other choices54.conclusionthe assertions set forth here have in common a faith in and a reliance on market forces. the role ofentertainment in driving deployment of advanced broadband networks and the need for security for thosenetworks are related. given adequate government recognition of the rights of property, and given governmentrestraint from imposing other regulations that undermine those rights, these networks can develop. this will nothappen instantaneously but will proceed incrementally over the next decade. the end result will be the extentionof new broadband capabilities to all parts of the country.interoperability issues would also benefit from an understanding of the way markets are working to addressproblems far more rapidly than government or even privatesector standards organizations can accomplish.market resolution recognizes the importance of incentives that motivate entrepreneurs. failure to understandthese markets and these incentives can have negative consequences for the economy and for internationalcompetitiveness. conversely, if the government refrains from micromanagement and can successfully providesound macroeconomic policy, the revolution that is the information age could bring a period of progress on thescale of the industrial revolution.interoperability, standards, and security: will the nii be based on market principles?486the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.notes1. quincy rodgers manages government affairs for general instrument corporation (gi). a graduate of yale college and the yale lawschool, he served as executive director, domestic council committee on the right of privacy during the ford administration. he hasalso practiced law in new york city and washington, d.c., and has served on the staff of the senate judiciary committee.2. gi is a world leader in broadband transmission, distribution, and access control technologies for cable, satellite, and terrestrialbroadcasting applications. in 1990, gi was the first to propose and, in 1991, the first to deliver an alldigital hdtv system for testing inaccordance with fcc procedures. gi has delivered alldigital compression systems for broadcast, satellite, and cable systems.3. the national information infrastructure has been recognized as a separate subject for policy attention for at least 20 years. seenational information policy, report to the president of the united states, submitted by the staff of the domestic council committee onthe right of privacy, honorable nelson a. rockefeller, chairman, national commission on libraries and information science,washington, d.c., 1976.4. this was the conclusion of at least one respected and generally knowledgeable business leader in the entertainment/communicationsindustry. japanese and european hdtv systems were apparently based on this false premise. the japanese analog muse hdtvsystem was outmoded even as it was launched. the implications of this experience for governmentled "industrial policy" are beyond thescope of this paper. see cynthia a. beltz, high tech maneuvers, industrial policy lessons of hdtv, aei press, washington, d.c., 1991.5. mike mills, "a digital breakthrough," congressional quarterly, january 15, 1994, p. 66.6. computer science and telecommunications board, national research council, keeping the u.s. computer and communicationsindustry competitive: convergence of computing, communications, and entertainment, national academy press, washington, d.c.,1995.7. of course, entertainment includes games, and the computer community is very much a part of that product line.8. some television executives and engineers believe that interlace provides a better picture than does progressive, although admittedlynot for text.9. "cabletv industry loudly likens itself to wild, wild west," wall street journal, may 10, 1995, p. b2.10. former fcc commissioner ervin duggan, now president of pbs, captured the cultural diversity of at least two players in theconverging arena when, on a panel at a cable television show, he said that, despite their size and cash, telephone companies posed nodanger to the current hollywood and programming communities because telephone executives "still sleep in their pajamas." bellexecutives may yet prove him wrong as they make major investments in creative material. are the bells going hollywood? see "staidphone giants try marriage to hollywood," wall street journal, may 24, 1995, p. b1.11. who will remain anonymous.12. following are some important differences between the computer and the entertainment worlds: computer is pointtopointcommunications, carriage, passwordoriented access control where most messages have little intrinsic value; entertainment is pointtomultipoint, broadcast, and scrambled on broadband, and the material transmitted has great intrinsic value.13. "datacom emerges as next wave," multichannel news, may 15, 1995, p. 1.14. information superhighway, an overview of technology challenges, report to the congress, u.s. general accounting office,washington, d.c., january 1995, p. 31.15. h.r. 3626 (103rd congress, 2nd session), section 405; s. 710 (104th congress, 1st session).16. george gilder, "when bandwidth is free," wired, september/october 1993, p. 38; george gilder, "telecom: the bandwidth tidalwave, forbes asap, supplement, december 5, 1984, p. 163.17. speech by daniel f. akerson, chairman & ceo, general instrument corporation, at lehman bros. conference, san francisco,california, april 28, 1994, slide 5:fiber optics plus compression = personalized channels per homebandwidth (mhz)homes/nodecompression ratiochannelschannels per hp55020,0001:1800.0047505001:11100.021,00020010:11,5007.50interoperability, standards, and security: will the nii be based on market principles?487the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.– by compressing 6 digital channels into the space of 1 analog channel, you can get 660 channels per today's node of 500 homesšorone personalized channel for each home. by further compressing at 10to1 ratios, using compression technologies such as gi'sdigicipher, a broadband system can provide 7 personalized channels per home. at this level of service, the consumer has the ability to beonline from multiple tvs and pcs in the home.18. general instrument corporation's digital television technologies are licensed under terms of agreements that it has with cabletelevision operators. gi has licensed three u.s.owned manufacturers (scientific atlanta, zenith, hewlettpackard) to build its completesystems, including access control (security). gi has also indicated that it is prepared to more widely license its digital decompressiontechnology. parenthetically, the case seems strong for more narrow licensing of security technology than for other technologies.although these licensing arrangements were established, in the first instance, as a result of vendor agreements, there are also strongeconomic incentives for vendors to license their technologies in order to drive their acceptance in the market. charles h. ferguson andcharles r. morris, computer wars, time books, random house, new york, 1993, p. 143 (hereinafter computer wars). in addition, thecomplexity of digital communications systems requires that someone perform the functions of disseminating information, answeringquestions relating to the system, and conducting compliance and interoperability testing. gi's licensing program addresses these needs.19. remarks prepared for delivery by vice president al gore, royce hall, ucla, los angeles, california, january 11, 1994.20. imperial russia turned this shortcoming into an asset by maintaining gauges different from those of its western neighbors as anadditional deterrent to invasion. lessons for the nii to be drawn from the experience of the u.s. railroads do exist: "between 1865 and1873, 35,000 miles of track were laid, a figure that exceeded the entire rail network of 1865, and inspired a boom in coal and pig ironproduction – and the rapid spread of the new bessemer process for making steel. railroads opened vast new areas to commercialfarming.– their voracious appetite for funds absorbed much of the nation's investment capital – contributed to the development ofbanking – and facilitated the further concentration of the nation's capital market.–" eric foner, reconstruction: america's unfinishedrevolution, 1863œ1877, harper & row, new york, 1988, p. 461.21. the grand alliance is made up of the companies and organizations that were proponents of hdtv systems as part of the fcc'sadvisory committee on advanced television service (acats) program for developing an advanced television standard. participantswere at&t, general instrument, mit, philips, sarnoff, thomson, and zenith. at&t and zenith jointly submitted a progressive scansystem for testing. philips, thomson, and sarnoff jointly submitted an interlace system. gi and mit jointly submitted two systems: oneprogressive and one interlace.22. "universal demodulator handles qam, vsb and ntsc," communications technology, december, 1994, p. 100.23. an interesting paper prepared for the american enterprise institute calls into question much of the conventional wisdom aboutinteroperability. milton mueller has taken a historian's look at the development of the telephone companies in the united states andexamined the effect that interconnection requirements had on the rollout of telephone service in the late nineteenth and early twentiethcenturies. mueller maintains that the absence of interconnection requirements was an important element of the rapid penetration oftelephone service in the united states. the inability of competing telephone systems to interconnect with each other, according tomueller, forced competition between the bells and independent telephone companies to focus on providing access. in the absence ofinterconnection, the desire of potential customers to have access to telephone service and to communicate, not with everyone or anyone,but with specific groups of other users, became the central focus of competition. this resulted in more rapid penetration of service,driven by suppliers of that service. it became a race between the bells and other providers to see who could sign up the most customersand most rapidly expand the network. this thesis may have implications for public policy and the rollout of new, digital services. miltonmueller, "universal service: competition, interconnection and monopoly in the making of the american telephone system," preparedfor the american enterprise institute for public policy research, working paper presented march 31, 1995.24. remarks by daniel f. akerson, chairman and chief executive officer, general instrument corporation, at the washingtonmetropolitan cable club luncheon, washington, d.c., april 11, 1995. see also peter k. pitsch and david c. murray, "a new visionfor digital telecommunications," a briefing paper, no. 171, the competitiveness center of the hudson institute, indianapolis, indiana,december 1994.in addition, see stanley m. besen and leland l. johnson, "compatibility standards, competition and innovation in the broadcastingindustry," rand corporation, november 1986, p. 135. according to besen and johnson, "the government should refrain fromattempting to mandate or evaluate standards when the technologies themselves are subject to rapid change. a major reason for thecommission's difficulty in establishing the first color television standard was the fact that competing technologies were undergoing rapidchange even during the commission's deliberations. it isinteroperability, standards, and security: will the nii be based on market principles?488the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.only after the technologies have 'settled down' that government action is most likely to be fruitful, as illustrated in the tv stereo case."25. even interface standards have the potential to inhibit development. the consumer electronics group (ceg) of the electronicsindustry association and the national cable television association (ncta) have been struggling over the development of a decoderinterface specification to be adopted by the fcc. this standard is seen by the commission as a solution to issues of compatibilitybetween televisions/vcrs and cable television systems. the proceeding is a response to the cable act of 1992 (cable televisionconsumer protection and competition act of 1992, p.l. no. 102385, 16 stat. 1460 (1992), section 17). ncta and some cable industrysuppliers are seeking an interface that can utilize a more extensive command set and more connector pins, maintaining that this will beneeded to support future services, including qwerty keyboards, an air mouse, and other yettobedeveloped but envisionedcapabilities. alternatively, the cable side has argued that the interface should permit the "pass through" of consumer remote controlcommands so that the consumer can utilize all features of the network. ceg is resisting these requests, arguing, inter alia, potentialinterference with the features its members provide in consumer electronic equipment. both sides have made extensive filings with thecommission in an ever growing record (in the matter of implementation of section 17 of the cable television consumer protection andcompetition act of 1992; compatibility between cable systems and consumer electronics equipment, fcc et docket no. 937).recently, elements of the computer industry have entered the proceeding, voicing concerns over the adequacy of the proposed interfaceand at least one feature previously agreed to by ceg and ncta. a silicon valley company has mounted a campaign against theproposed solution, charging it can become a bottleneck.26. computer wars, pp. 113œ14; see also fn. 17. during the early days of the clinton administration, this book was reportedly widelyread in the white house. current public policy debates provide a rich set of opportunities for the administration to heed its lessons.27. computer wars, p. 221; see also fn. 17.28. even this longstanding and "hallowed" justification for government regulation is being called into question. see adam d. thierer,"a policy maker's guide to deregulating telecommunications, part 1: the open access solution," heritage talking points, theheritage foundation, december 13, 1994; paul baren, ''visions of the 21st century communications: is the shortage of radio spectrumfor broadband networks of the future a selfmade problem?," keynote talk transcript, 8th annual conference on next generationnetworks, washington, d.c., november 9, 1994.29. cable television consumer protection and competition act of 1992, p.l. no. 102385, 16 stat. 1460 (1992), section 17.30. computer systems policy project, "perspectives on the national information infrastructure: ensuring interoperability," white paper,february 1994; alliance to promote software innovation and the business software alliance, "the information marketplace: theperspective of the software and computer industry," special focus paper, spring 1995.31. government standards, like all standards, need to take into account the international circumstances in which the standard willoperate. the u.s. market remains the world's most attractive, thus giving a u.s. standard additional impetus. in the digital video arena,however, there are signs that europe or japan or both may gang up and try to protect their domestic industries by promulgating astandard different from that used in the united states. at a recent meeting in melbourne of davic, a group working on an internationalcable standard for digital video, the japanese and european representatives joined forces in opposing the position of most u.s.representatives. it is too soon to tell whether this kind of activity will ultimately undermine the leadership position that the united stateshas held as the home of digital development.32. this number was used by fcc chairman reed hundt in a speech to the national cable television association annual cable show,may 9, 1995.33. the origin of this term is unclear. some early commentator made the statement that the square foot on the top of the television setwas the most valuable piece of real estate on the information superhighway. this immediately caught the attention of at least onemember of congress who worried that it could become a bottleneck. it also brought the attention of many companies that announced thatthey planned to make "settop boxes," long before they had thought through what functions the settop box would perform or whatapplications consumers wanted. the term "settop box" is almost certainly derived from the model of current cable television converters.the convenience that the use of the term provides is more than offset by the way it misdirects focus from the real issue, which is wheredifferent functionality is to be performed in a network.34. computer wars; see also fn. 17.35. sometimes these efforts operate on a principle of unanimity. at other times, minority views are outvoted and rejected. some groupsare incredibly undemocratic; prudence requires not identifying them here, now. the unitedinteroperability, standards, and security: will the nii be based on market principles?489the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.states government should acquaint itself with the characteristics of such organizations prior to embracing or affirming their policies.36. "– the bureaucratic solution is most frequently 'one size fits all.'" george gilder, speech at the virtual democracy conference,progress and freedom foundation, january 10, 1995. in remarks also applicable to this subject, gilder said that the answer to niidevelopment lies not so much in competition as in diversity.37. "– most entrepreneurs who make breakthroughs are almost by definition psychologically uninterested in working in a dull, slowprocess in which most of your time is spent listening to people who don't know as much as you do." speaker of the house ofrepresentatives newt gingrich, speech at the virtual democracy conference, progress and freedom foundation, january 10, 1995.38. translation provided by speaker gingrich, in remarks to the computer science & telecommunications board, washington, d.c.,may 15, 1995.39. from a slide offered by cablelabs at a recent cable/it convergence forum (and, one hopes, intended only to wake up theparticipants):membership requirements: subordinate your company's interests to those of a large consortium. waste hundreds of man/hours in unproductive meetings in distant locations. lock in your development process with a rigid set of technical standards. spend money on large, unaccountable dues. engage in complex, stressful political maneuvers with people in competing companies.40. general instrument corporation's digicable product, which will allow cable systems to offer the equivalent of 350 to 500 channels,is approximately 1 year late to market. six months of this is attributable to technical problems that the company and its vendorsencountered; these digital consumer decoders are the most sophisticated pieces of consumer electronics hardware ever deployed. but 6months of the delay was caused by a redesign to meet the specifications under development by the moving picture experts group formpeg2, even though mpeg2 does not constitute a complete transmission system while digicable does. "digital compression: theholy grail," cablevision magazine, march 6, 1995, p. 22.41. the tendency of politicians to count noses can work against important national resources. as foreign companies locate plants in theunited states, they bring valuable employment to americans. however, jobs are not the only contribution that technology makes tonational wealth. in particular, u.s. ownership of and benefits from research and development activity are an important component ofeconomic investment. among senior advisers to the clinton administration, the relative merits of these factors have been the source ofdebate. see robert reich, "who is us?" harvard business review, januaryœfebruary 1990, p. 53; and laura d'andrea tyson, "strongu.s. firms key to healthy world economy," los angeles times, december 9, 1990, p. d2. to oversimplify, reich thinks that foreignfirms that provide jobs should be considered american; tyson thinks that they must do r&d in the united states to qualify. neitherexpresses concern about foreign ownership in the context of national and international standards setting; european governments andeuropean multinationals, in particular, have demonstrated an all too frequent tendency to use standards to protect domestic industries.42. providing a government entitlement or backward compatibility protection for early adapters who purchased beta machines for$1,000 and more would not seem to be a major national priority; presumably they got something for their early purchase, namely to bethe "first ones on their block.–" in any case, the economy provides winners and losers. this author maintains an extensive collection ofbeta tapes and offers to buy used beta machines in good working condition for $25 to $35 and $10 to $15 for those in need of reasonablerepair. thus, a secondary market, of sorts, has already developed.43. or, worst of all, joint government/privatesector standardssetting activities!44. much of the material in this section is drawn from testimony of richard s. friedland, president and chief operating officer of generalinstrument corporation, before the nii security issues forum, july 15, 1994.45. h.r. 1555, sec. 203 (104th congress, 1st session); h.r. 1275 & s. 664 (104th congress, 1st session).46. the telephone specification was well established before regulatory rules and separation were imposed. moreover, the installed baseof telephones was huge, covering over 90 percent of the population. the current installed base of digital video equipment is minusculeby comparison and is exclusively oneway satellite. the technical complexity of voice telephone is orders of magnitude less than that ofvideo networks. there is simply no comparison, a fact that did not inhibit members of congress who showed up at a press conferenceannouncing this legislation with antique, black, rotarydial telephones.47. the market for such equipment is currently highly competitive and has numerous providers, including general instrument, scientificatlanta, panasonic, zenith, and philips, among others. in anticipation of its potentialinteroperability, standards, and security: will the nii be based on market principles?490the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.growth from the introduction of digital transmission and the advent of telephone company video services, others such as hewlettpackard, thomson, sun, and sony have said they would enter. today, telephones, like most consumer electronics, are produced byoffshore companies, although many telephones carry the bell label.retail sale proposals are advocated as consumer friendly and, as such, are supported by the organized public interest consumer groups.retailers do not propose to cut off the right of network providers to also provide equipment, by sale or lease, but would prevent it frombeing bundled. techniques such as those used so successfully by cellular telephone companies, whereby deep discounts for equipmentallow more people to take advantage of service, would be barred. there are legitimate questions as to how "consumer friendly" it is todeprive consumers of the option of lowcost entry to these new services. likewise, lease of equipment is likely to be an importantcomponent of the introduction of new services. it is one thing to buy a telephone for $15 to $100 from radio shack, knowing that thechances of its becoming obsolete are minimal. indeed, it will almost certainly break before then. it is another thing to expect consumersto rush down to circuit city to buy a $300 to $500 digital entertainment terminal and bear the risk of obsolescence and perhaps the riskthat it will not work properly with the network.48. bell atlantic, nynex, and pactel have issued an rfp for equipment, including separation of a network integration module (nim)and a digital entertainment terminal (det). the nim would include network functions, including access control and decryption. thedet would be available for retail sale. see chris nolan, "the telcos' settops," cablevision, april 3, 1995, p. 56. however, earlier thisyear, at&t and vlsi technologies announced a program that runs counter to the thrust of separation. see "at&t, vlsi to embedsecurity into set top chips," broadcasting & cable magazine, february 6, 1995, p. 36. at&t customers, like gi customers, who aredeveloping products, tend to focus, laserlike, on maximizing security. it is doubtful how many are aware that some in government havedifferent plans for them.49. congressional record, 104th congress, 1st session, april 4, 1995, p. s5143.50. although it is generally acknowledged that smart card technologies will be easier to develop and more secure in a digitalenvironment, the experience in the analog environment is that security has been badly compromised. satellite systems are badlycompromised in europe. see "sky war over 'smart' pirates," sunday mail, financial section, united kingdom, october 9, 1994.51. interesting questions would be posed by such a government determination: what happens if nontechnologists in the congress and orin the bureaucracies (or even qualified technologists, for that matter) turn out to be wrong and the technology is defeated by theftofservice pirates? since the government has chosen this technology, would or should the government be liable for the losses incurred as aresult of the defeat of this technology? if the technology were defeated, could retailers be liable for losses incurred thereby? would thatliability require a showing of negligence, such as the failure to maintain secure warehouses for the equipment? given that maintainingthe security of these systems is a matter of concern to the government (the success of our communications networks and the investmentin those networks depend upon their ability to maintain security and privacy), would it be appropriate that punitive damages be availablein the case of a break against a party, including a retailer, whose negligence caused a break? should tort reform legislation contain anexception for punitive damages in such cases? could a manufacturer refuse to make products available to a retailer who failed to sign anagreement in which the retailer promised to maintain adequate security procedures or an agreement in which the retailer promised toindemnify against losses from a breach of security?52. hybrid fiber coaxial cable systems (hfc) seem the preferred model for cable television operators and some telcos. other telcos areindicating a preference for switched digital networks (sdn). ultimate choices will be worked out in the marketplace over time, asnetwork providers experiment with different delivery models. cost will be a fundamental issue.53. for example, retail sale, or even including security considerations advanced without regard to cost or convenience.54. retail sale has implications for other values as well, such as innovation. to the extent retail sale is used to justify standardization, itcan negatively affect continued development.interoperability, standards, and security: will the nii be based on market principles?491the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.57technology and cost models for connecting k12 schools tothe national information infrastructurerussell i. rothstein and lee mcknightmassachusetts institute of technologyabstractit is presumed that universal connection to the national information infrastructure (nii) will facilitateeducational reform within schools and communities. several developments suggest that this might be the case: the federal government is committed to have every classroom in the united states connected to the nii bythe year 2000; a number of telephone and cable companies have announced plans to connect schools in their service areasat low or no cost; modern, highspeed networks have been installed at a number of progressive, pioneering k12 schools; and the internet, a global network of networks, has grown rapidly, providing connections to an abundance ofeducational resources.however, to date, there is relatively little known about the costs for connecting schools to the informationinfrastructure. even if the exact costs are unknown, they are expected to be significant. to reduce these costs, avariety of approaches are being tried.some states have implemented the costsaving program of purchasing telecommunications equipment andservices for all schools in the state. for example, north carolina has saved its schools 20 to 50 percent of thecosts for certain items. some states have passed legislation permitting the state public utility commission to setpreferential or fixed intrastate rates for educational institutions.using a baseline of service required for connecting to the nii, there will be $9.4 billion to $22.0 billion inonetime costs with annual maintenance costs of $1.8 billion to $4.6 billion. at the perpupil level, this isequivalent to $212 to $501 in onetime installation costs and an ongoing annual cost of $40 to $105.hardware is the most significant cost item for schools. however, most of this cost item is allocated for thepurchase of pcs in the schools. the value of the pcs goes well beyond their use as networking devices.therefore, the real costs for pc purchases should be allocated across other parts of the technology budget, andnot only to the networking component. if this is done, then the hardware costs for connecting to the nii dropconsiderably.excluding pc expenditures, costs for support of the network represent about onethird of all networking.support is a vital part of the successful implementation of a school network and its costs must be factored in tothe budget. support and training together constitute 46 percent of the total costs of networking schools. costs fortelecommunications lines and services represent only 11 percent of the total costs. this amount is lower than thecosts assumed by much of the technology community, including the telecommunications service and equipmentproviders.note: russell i. rothstein is a research assistant and lee mcknight is a principal research associate with the mitresearch program on communications policy. this paper is based on a report prepared when russell rothstein was avisiting researcher at the office of educational technology in the u.s. department of education in 1994. further work wassupported by arpa contract n0017493c0036 and nsf grant ncr9307548 (networked multimedia informationservices). the invaluable help of linda roberts, u.s. department of education, and joseph bailey, thomas lee, and sharongillett, mit research program on communications policy, is acknowledged.technology and cost models for connecting k12 schools to the national informationinfrastructure492the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.our research suggests that a number of programs would have a significant impact on the total costs ofconnecting to the nii. if all schools coordinate purchasing at the state level, cost savings will exceed $2 billion.colleges and universities often have the resources to provide technical support to k12 schools. if a nationwideprogram were instituted, potential savings would be $800 million to $1.8 billion. if schools were given freeinternet connectivity, the reduction in total annual costs for school internet connections would be between $150million and $630 million.finally, as the costs of networking schools are better understood, a new question arises: how will these costsbe financed? many states have programs to fund networking in schools. the federal government has a role,although it must become more flexible and coordinated. however, as vice president al gore has continued tostate, the nii will be built by the private sector. a number of states have initiated cooperative ventures betweenbusinesses and schools. an expansion of these programs may well be the key for successfully connecting k12schools to the nii.introductionon january 11, 1994, vice president al gore challenged the nation to "connect every classroom by the year2000" to the national information infrastructure (nii). in testimony before congress in may 1994, secretary ofeducation richard riley said, "we may have to go a step further and provide our schools with free usage of thetelecommunications lines that will connect school children and young people" to the nii. in an address at theharvard graduate school of education, fcc chairman reed hundt said that "if the administration's challengeis met by everyone, education in this country will be reinvented, forever and for better." universal connection tothe nii, it is presumed, will facilitate educational reform within schools. however, to date, relatively littleinformation has been available about the costs for connecting schools to the information infrastructure. thispaper presents models for evaluating the total cost of full nii connectivity for schools through an engineeringcost study of equipment, services, software, and training needs.cost models of k12 networkingfive models for connecting schools to the nii are presented in the next section in order of increasing costand power to describe the path that many schools may follow. a school will likely begin its connection throughthe lowcost dialup option described in model one. as the school builds expertise and develops a need forgreater capability, it will upgrade to a higher level of connectivity. it is not until the school acquirestelecommunications infrastructure similar to model four that it is able to take advantage of many of theeducational services and applications provided on the emerging nii. model five presents the costs for putting apc on the desktop of every student, with a highspeed connection to the internet. although this setup is notnecessary for access to many of the coming nii services, it presents a model of systemic educational reform withinformation and networking technology.these models are representations of the network technology used in schools. a level of complexity anddetail is omitted from these models, but the simplicity is helpful because the models encompass broad crosssections of network and school configurations. the models provide a clearer view of the costs and choices fornetworking k12 schools.there are numerous ways to define a school network. the models presented below follow the internetnetworking model, in which schools have digital data connections that transmit and receive bits of information.the models exclude both analog video pointtopoint networks and voice networks including pbx, centrex, andvoicemail systems. audio and video functions are possible in digital format over the internet data network.however, many schools will require video and voice networks in addition to the data networks. the costs ofthese systems are important to consider but are not modeled in this paper.it should be noted that although voice and video networks have been separated out from data networks inthis paper, schools should not consider these three types of networks to be wholly distinct. some schools haveintegrated their voice and video networks with the school data network. the sharing of resources among themultiple networks can be effective in providing significant cost savings. at a basic level, it must be understoodtechnology and cost models for connecting k12 schools to the national informationinfrastructure493the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.that as a school installs a lan and puts computer data connections in every classroom, there are minimal addedcosts to concurrently install other types of connections, including telephone lines.assumptionsthe assumptions described below are the basis for determining the costs of networking schools. theseassumptions are conservative but realistic, to ensure that the total costs are not underestimated.technology standard for connecting to the niias described by the information infrastructure task force (1994), the nii "promises every – school – inthe nation access anywhere to voice, data, fullmotion video, and multimedia applications.– through the nii,students of all ages will use multimedia electronic libraries and museums containing text, images, video, music,simulations, and instructional software." in models four and five, the school has access to these nii services. thefollowing requirements outline the needs for those models in order to have full connection to the nii: a lan within each school with connections to multiple machines in every classroom. the power of thenetwork is greatly enhanced as the number of access points increases throughout the school. a classroomwith one connection is not conducive to use of network applications in a class of 20 or 30 students.telecommunications technology will not be a tool for systemic educational reform until networkconnections are available throughout the school. a connection from each school to a community hub. from 2 to 10 schools should connect to a single hub,depending on the size of the schools. in most cases, the hub will reside at the school district office.however, in cases where there are many schools in a single district, the schools should be clustered into setsof four to six. each of these school clusters will have a group hub, probably at the district office, which willcontain the center of the network for those schools. the rationale for the use of this architecture is describedbelow. a connection between the school lan and the district office hub. with this configuration, every classroomhas a connection not only to every other classroom in the school but also to the central school district office. a connection from the school district office to a community, state, or nationwide wide area network(wan). this connection will allow all schools to connect to the wan. the internet is a good example of awan and is used throughout this report as a model and a precursor for the coming nii. sufficient bandwidth for these connections. with a highbandwidth connection, users in schools can makeuse of graphical applications (e.g., mosaic) and limited video service (e.g., cuseeme and mbone). formost school districts, the minimum bandwidth, or bitspeed, that will support these services is 56,000 bitsper second (56 kbps). therefore, the connection between the school and the hub must be at or above thislevel. for the connection from the district office to the internet, a higher bandwidth connection is necessarybecause all of the schools in the group connect to the internet through this line. the suggested minimumbandwidth for this connection is 1,500,000 bits per second (1.5 mbps), otherwise known as a t1 line. symmetric, bidirectional access to the wan/internet. it is important that the connection to a school allowinformation to flow both in and out of the school at the same rates. in this way, students can become bothconsumers and provides of information over the network. adequate remote dialup facilities. with a sufficient number of modems and phone lines, faculty, students,and parents can gain access to the school system remotely on weekends and after school hours. use of established and tested technologies. schools have benefited most from mature technologies that havebeen well tested in the marketplace. use of cuttingedge technologies has not been as successful in schoolsdue to the instability of the technologies and the large amount of resources required to support them. themodels assume the use of mature technology and transmission media. therefore, modern technologies such astechnology and cost models for connecting k12 schools to the national informationinfrastructure494the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.wireless and hybrid fiber coaxial systems are not considered in this study. however, given the rapidity oftechnological change and marketplace evolution for networking products and services, wireless and cablealternatives should be evaluated in future research.each of the successive models progresses closer to this configuration. the first three models represent theschools and districts that have not reached this level of networking. it is not until the fourth model that theserequirements have been incorporated. the fifth model continues on this path and exceeds the baselinerequirements. it is assumed that some schools will traverse the path through each of the models to connect to thenational information infrastructure.architecture of the district networkthe basic network architecture for these models follows the "star" network configuration as illustrated infigure 1. this architecture is also used in the nationwide telephone network. in the telephone network,residential telephone lines in an area are directly connected to a single district office. in the school network, eachschool building is connected to the school central hub. in most cases, the district office will serve as the centralhub. however, in cases where there are either few or large numbers of schools in one district, alternative sitesmust be chosen.figure 1 "star" network. source: newman et al. (1992).the rationale for adopting this architecture is that, when many schools are connected through a single hub,costs can be aggregated among the schools. this gives schools stronger purchasing power as equipmentpurchases are aggregated by the school district for volume discounts. it also allows schools to share resourcesšsuch as the data line to the internet, training programs, and fulltime support staffšthat each school might not beable to afford individually. therefore, there are costs both at the school and at the district level for networkingschools across the country. the star network configuration for schools, utilizing a community or district hub, isrecommended in gargano and wasley (1994) and in california department of education (1994).cost areasthe cost models presented in this paper include four types of costsšhardware, training, support, andretrofitting. the items included in these categories are summarized below. hardware. wiring, router, server, pcs, including installation, maintenance, and servicing of the hardwareand telecommunications lines.technology and cost models for connecting k12 schools to the national informationinfrastructure495the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. training. training of teachers and other school staff to use the network. support. technical support of the network. retrofitting. modifications to the school facility to accommodate the telecommunications infrastructure.this may include costs for asbestos removal (where applicable); installation of electrical systems, climatecontrol systems, and added security (locks, alarms, etc.); and renovation of buildings to accommodatenetwork installation and operation.the following cost area is not included in the models: educational software. there are "freeware" versions of many popular internet applications. however, othereducational software may be desired by particular schools. the costs for this software may be high, but theyare not included in the models. further economic analysis of software costs and their evolution in thenetwork scenarios analyzed below is required.instructional use of the networkit is likely that the type of technology model deployed in the school will greatly affect the use of thenetwork and the educational benefits obtained. a school with multiple networked pcs in every classroom (modelfour) will reap greater educational benefits from the network than a school with a single pc and modem (modelone). similarly, video and graphical applications (e.g., mosaic and cuseeme), available in models four andfive, add educational value to the textbased applications available in the lowerend models. however, there hasnot yet been a quantitative analysis of the educational benefits associated with each particular model. this paperis concerned exclusively with the costs for the various network models. study to determine the educationalbenefits of various k12 network models is also needed. when the educational benefits are quantified, theyshould be compared to the costs outlined in this paper. the synthesis of these studies will generate a costbenefitcurve for connecting schools to the network. that information is vital for determining national policy onconnecting schools to the nii.school characteristicsthe models described are based on a "typical" school and school district, as defined by the u.s. departmentof education (1993), and represent the average costs of all u.s. schools and school districts. many schools willdiffer in significant ways from the "typical" school and will therefore face somewhat different costs from thosepresented in the models.size of school. the average school has about 518 students and twenty classrooms. it employs 27 teachersand 25 other school staff. the average number of schools in a school district is about six. (these numbers arebased on a national enrollment of approximately 44 million students in 85,000 public schools in 15,000 schooldistricts.)existing equipment. according to anderson (1993), as of 1992 there was an average of 23 computers perschool at the elementary and middle school levels, and 47 computers per school at the secondary school level.about 15 percent of these machines, or three to seven machines per school, are capable of running the networkprotocol (tcp/ip) to access the internet. during the 3 years from 1989 until 1992, the number of computers inschools grew by 50 percent. given the increasing growth rate of computers in the market, it is safe to assume thatthe growth rate of computers in these schools has exceeded 50 percent over the past 2 years. given theseassumptions, in an average school there are at least seven pcs capable of running graphical internet applications(e.g., mosaic). this number of pcs is sufficient for the first two models, but it is not sufficient for establishingmultiple connections in every classroom throughout the school. therefore, for models three, four, and five, thereis a lineitem cost for purchasing additional pcs.technology and cost models for connecting k12 schools to the national informationinfrastructure496the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.cost modelssinglepc dialup (model one)the singlepc dialup model (figure 2) represents the most basic connectivity option for a school. theschool has no internal lan. there is a single connection to the district office over a modem and standard phoneline. only one user may use the connection at any time. since the system is limited, only a few teachers in theschool require training. users of the system will be able to use textbased applications over the internet (e.g., email, telnet, gopher), but will have no realtime access to video or graphics.model one is the lowestcost option for schools. many of the services and benefits envisioned for the niiwill not be widely accessible in schools using this model of connectivity. table 1 lists the cost items associatedwith the singlepc dialup model.lan with shared modem (model two)the primary difference between model two (figure 3) and model one is the existence of a lan within theschool. by connecting the modem to the lan, every computer on the network has access to the internet.however, this model supports only a few users at a time, since it is limited by the number of phone lines goingout of the school. as in model one, users of the system can use textbased applications over the internet (e.g., email, telnet, gopher) but have no realtime access to video or graphics.in model two, there is the added cost for a lan. this model assumes the use of copper wire (category 5) asthe medium for the network since it is the most affordable and scaleable option for schools in 1994. the costs forthe wiring and network cards run $100 to $150 per pc connected. including the costs for the accompanyinghardware and labor, the costs per pc are $400 to $500. therefore, for a school with 60 to 100 connected pcs (3to 5 pcs per classroom @ 20 classrooms), the total lan costs are $20,000 to $55,000.model two is another lowcost option for schools. however, many of the services and benefits envisionedfor the nii are still not widely accessible in this model. table 2 lists the cost items associated with this model.figure 2 singlepc dialup model. source: rothstein (1994).technology and cost models for connecting k12 schools to the national informationinfrastructure497the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 3 lan with shared modem model. source: rothstein (1994).lan with router (model three)the primary difference between model three (figure 4) and model two is a router in place of the modem.with the router, multiple users of the lan may access the internet concurrently. just as in the first two models,users of the system are able to use textbased applications over the internet (e.g., email, telnet, gopher) buthave no realtime access to video or graphics.since the router allows multiple users of the system, there is an opportunity to expand the entire networkinfrastructure. with this infrastructure, it is reasonable to support one pc in every classroom. therefore, there isa requirement to purchase 15 additional pcs for the average school to use in addition to its small initial stock oftcp/ipcompatible machines. it is assumed that the purchasing of these pcs is done at the district level tonegotiate better rates ($1,000 to $2,000 per pc). support and training costs are higher since there are additionalusers of the system. there are additional dialup lines required to accommodate remote access, as well assignificant retrofitting costs for the electrical system, climate control system, and enhanced security. table 3gives a line item summary of the costs associated with model three.figure 4 lan with router model. source: rothstein (1994).technology and cost models for connecting k12 schools to the national informationinfrastructure498the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.lan with local server and dedicated line (model four)the primary difference between model four (figure 5) and model three is the existence of a file server at theschool. the onsite server allows much of the information to reside locally at the school instead of at the districtoffice. this feature provides better performance since much of the information does not need to be fetched overthe network. additionally, the local server allows school administrators to exercise greater control over theinformation that flows in and out of the school. higherspeed links from the school enable the use of limitedvideo, graphical, and textbased network applications.in model four, virtually the entire school is supported on the network. as a result, the training program isextensive and the support team is well staffed. the costs of the connection to the internet are also higher due tothe larger bandwidth connection. there are significant retrofitting costs for the electrical system, climate controlsystem, and better security. table 4 lists the cost items associated with this model.the costs associated with using model four are indicative of the costs of connecting k12 schools across thecountry to the nii. these numbers indicate that there will be $9.4 billion to $22.0 billion in onetime costs, withannual maintenance costs of $1.8 billion to $4.6 billion. at the perpupil level, this is equivalent to $212 to $501in onetime installation costs and an ongoing annual cost of $40 to $105.in this model, hardware is the most significant cost item for schools. however, most of this cost item isallocated for the purchase of pcs in the schools. the value of the pcs goes well beyond their use as networkingdevices. therefore, the real costs for pc purchases should be allocated across other parts of the technologybudget, and not only to the networking component. if this is done, then the hardware costs for connecting to thenii drop considerably.if the high startup costs are amortized equally over a 5year period, then the breakdown of costs during thefirst 5 years, excluding pc purchases, is as shown in figure 6.costs for support of the network represent about onethird of all networking costs in model four. support isa vital part of the successful implementation of a school network and its costs must be factored into the budget.support and training together account for 46 percent of the total costs of networking schools.finally, it is important to note that the costs for telecommunications lines and services represent only 11percent of the total costs. this amount is lower than the costs assumed by much of the technology community,including the telecommunications service and equipment providers.ubiquitous lan with local server and highspeed linemodel five (figure 7) represents a full, ubiquitous connection to the nii. in this model, there is a pc on thedesktop of every student and teacher. there is a highbandwidth connection to the school to support largenumbers of concurrent users of the system. this model supports the full suite of text, audio, graphical, and videoapplications available over the internet.in model five, the entire school is supported on the network. a large portion of the costs for this model isthe expenditure for pcs on every desktop. assuming 500 students, there is a need to purchase 450 new pcs.since the network is ubiquitous, the training program is extensive and the support team is well staffed. the costsof the connection to the internet are also higher because of the highspeed line going into the school. the fileserver is larger than model four's server to accommodate the large number of networked pcs. the dialup systemis larger in order to allow many students, teachers, and parents to access the system remotely. the retrofittingcosts are substantial because extensive electrical work must be performed in the average school to accommodatethe hundreds of new pcs. in addition, the school in model five must make expenditures on air conditioners andsecurity locks to protect the new equipment. table 5 lists the cost items associated with this model.cost comparison of modelsu.s. expenditures on k12 education in 1992œ93 totaled $280 billion. total onetime costs for model fourrepresent 3 to 7 percent of total national educational expenditures. the ongoing annual costs represent betweentechnology and cost models for connecting k12 schools to the national informationinfrastructure499the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.0.6 and 1.6 percent of total national educational expenditures. for model five, the costs are more significant, withonetime costs representing 18 to 41 percent of total national educational expenditures.figure 5 lan with local server and dedicated line model. source: rothstein (1994).figure 6 breakdown of costs for model four. source: massachusetts institute of technology, research programon communications policy (1994).the models with advanced connectivity include significant equipment and training costs, which may bebeneficial for educational purposes other than network uses. if these costs are accounted for separately, thedifference in costs between models four and five will not be as significant as those presented here.table 6 summarizes the associated range of costs for the various technology models.potential impact of costreduction initiativesmuch more can be done by the government and the private sector to significantly mitigate costs schoolsface to connect to the nii. this section examines some possible programs and their impact on costs to schools.technology and cost models for connecting k12 schools to the national informationinfrastructure500the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 7 ubiquitous lan with local server and highspeed line model. source: rothstein (1994).table 1 singlepc dialup model costs (dollars)lowhighschool costs>onetime installation coststelephone line100250modem150250total250500annual operating costsreplacement of equipment50150telephone line (10 hr/month)3001,500total3501,650district office costsonetime installation costsfile server2,00010,000data line to wan/internet (56 kbps)5002,000training (2 to 4 teachers per school)1,00010,000total3,50022,000annual operating costsinternet service (56 kbps)5,00010,000support2,00010,000training1,0005,000total8,00025,000u.s. onetime costonetime cost per student1.68$8.47total70,000,000370,000,000u.s. annual costsannual cost per student3.4011.71total150,000,000520,000,000source: rothstein (1994).technology and cost models for connecting k12 schools to the national informationinfrastructure501the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 2 lan with shared modem model costs (dollars)lowhighschool costsonetime installation costslocal area network20,00055,000lan modem5001,200retrofitting (minor)2,00010,000total22,50066,200annual operating costsreplacement of equipment3,0008,250shared telephone line (40 hr/month)1,2006,000total4,20014,250district office costsonetime installation costsfile server2,00010,000district local area network2,0005,000data line to wan/internet (56 kbps)5002,000dialup capabilities (2 lines)2,0004,000training (5 to 20 staff per school)1,00010,000total7,50031,000annual operating costsinternet service (56 kbps)5,00010,000dialup lines300500support (1 to 2 staff per district)45,00090,000training10,00020,000total60,300120,500u.s. onetime costonetime cost per student46.02138.45total2,030,000,0006,090,000,000u.s. annual costannual cost per student28.6768.61total1,260,000,0003,020,000,000source: rothstein (1994).technology and cost models for connecting k12 schools to the national informationinfrastructure502the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 3 lan with router model costs (dollars)lowhighschool costsonetime installation costslocal area network20,00055,000personal computers (15 machines)15,00030,000router2,0003,000connection to hub (14.4 kbps or 56 kbps)501,000retrofitting (major)10,00025,000total47,050114,000annual operating costsreplacement of equipment3,0008,250connection to hub (14.4 kbps or 56 kbps)50010,000total3,50018,250district office costsonetime installation costsfile server2,00015,000router2,0005,000district local area network2,0005,000data line to wan/internet (56 kbps)5002,000dialup capabilities (8 lines)8,00016,000training (10 to 20 staff per school)1,00010,000total15,50053,000annual operating costsinternet service (56 kbps)5,00010,000dialup lines1,2002,000support (1 to 2 staff per district)45,00090,000training10,00020,000total61,200122,000u.s. onetime costonetime cost per student96.18238.30total4,230,000,00010,490,000,000u.s. annual costannual cost per student27.6376.85total1,220,000,0003,380,000,000source: rothstein (1994).technology and cost models for connecting k12 schools to the national informationinfrastructure503the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 4 lan with local server and dedicated line model costs (dollars)lowhighschool costsonetime installation costslocal area network20,00055,000personal computers (60 machines)60,000120,000file server4,00015,000connection to hub/district office (56 kbps)5002,000router and csu/dsu2,6005,000retrofitting (major)10,00025,000total97,100222,000annual operating costsreplacement of equipment3,0008,250connection to hub/district office (56 kbps)1,0005,000total4,00013,250district office costsonetime installation costsfile server2,00015,000router2,0005,000district local area network2,0005,000data line to wan/internet (1.5 mbps)1,0005,000dialup capabilities (20 lines)16,00032,000training (40 to 50 staff per school)50,000150,000total73,000212,000annual operating costsinternet service (1.5 mbps)10,00042,000dialup lines3,0005,000support (2 to 3 staff per district)66,000150,000training15,00035,000total94,000232,000u.s. onetime costonetime cost per student212.47501.14total9,350,000,00022,050,000,000u.s. annual costannual cost per student39.77104.69total1,750,000,0004,610,000,000source: rothstein (1994).technology and cost models for connecting k12 schools to the national informationinfrastructure504the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 5 ubiquitous lan with local server and highspeed line model costs (dollars)lowhighschool costsonetime installation costslocal area network50,000100,000file server10,00025,000connection to hub/district office (1.5 mbps)1,2005,000router and csu/dsu4,0007,000pc on every desk (450 new machines)450,000900,000retrofitting (major, including electrical)70,000250,000total585,2001,287,000annual operating costsreplacement of equipment7,50015,000connection to hub/district office (1.5 mbps)10,00035,000total17,50050,000district office costsonetime installation costsfile server5,00015,000router2,0005,000district local area network2,0005,000data line to wan/internet (1.5 mbps)1,0005,000dialup capabilities (50 lines)32,00080,000training (all teachers in school)55,000165,000total97,000275,000annual operating costsinternet service (1.5 mbps)10,00042,000dialup lines30,00050,000support (4 to 5 staff per district)112,200255,000training16,50038,500total168,700385,500u.s. onetime costonetime cost per student1,163.572,580.00total51,200,000,000113,520,000,000total u.s. annual costannual cost per student91.32228.01total4,020,000,00010,030,000,000source: rothstein (1994).technology and cost models for connecting k12 schools to the national informationinfrastructure505the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 6 total onetime and ongoing costs for associated models (billions of dollars)onetime costsongoing costsmodellowhighlowhighsinglepc dialup0.070.370.150.52lan w/shared modem2.036.091.263.02lan w/router4.2310.491.223.38lan w/local server and dedicated line9.3522.051.754.61ubiquitous lan w/highspeed connection51.20113.524.0210.03source: rothstein (1994).in determining the costs savings from programs supporting connection to the nii, it is imperative to definethe model of the nii. in this paper, the baseline model for nii access is a school with a lan, a local server, anda dedicated line to the district hub. this is the fourth model as described above. the costs for this model aresummarized in table 7.based on the costs listed in table 7, we have estimated the potential total cost savings for u.s. schools fromvarious programs.1. preferential telecommunications tariff rates are instituted for schools. some state utility commissionshave instituted preferential telecommunications rates for educational institutions. these rates areapplicable only for intrastate traffic. for interstate traffic, the tariffs set by the federal communicationscommission are in effect. currently, these tariffs have no preferential rates for educational institutions.the amount of money that schools will save will depend on the amount of discount if preferential ratesare adopted. the following numbers represent the estimated savings with educational discounts of 30percent and 60 percent. 30 percent reductionš$39 million to $150 million (annual)estimated savings: $89 million to $218 million (onetime) 60 percent reductionš$78 million to $300 million (annual)estimated savings: $179 million to $435 million (onetime)table 7 total u.s. costs for baseline connection to the nii (as in model four) (millions of dollars)onetime costsongoing costscomponentlowhighlowhighlocal area network1,7304,75000personal computers5,10010,20000file server3701,50000telecommunications lines298725130500router and csu/dsu22142500retrofitting8502,12500training7502,250225525internet service00150630support009902,250replacement of equipment00255701total9,31921,9751,7504,606source: rothstein (1994).2. all information technology purchasing is done at the state level . when states are involved in purchasinginformation technology, schools may secure better prices due to volume discounts. schools in northtechnology and cost models for connecting k12 schools to the national informationinfrastructure506the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.carolina, for example, have enjoyed discounts of 20 to 50 percent for hardware and labor costs. thefollowing figures indicate the possible cost savings across all 50 states, based on an average 30 percentdiscount. estimated savings: $1.9 billion to $4.1 billion (onetime); $45 million to $189 million (annual)3. universities or other institutions provide technical support to schools. universities can also play a role inproviding technical support to k12 schools. many universities have already undertaken such a projectand have provided network support to a number of k12 schools in their area. with such a program,schools will still require some dedicated support staff. however, it is assumed that schools will be able tofunction with 80 percent less technical support staff than would be required without university support. estimated savings: $790 million to $1.8 billion (onetime)4. teachers are trained on their own time. in the models, a large portion of the training costs are dedicatedeither to paying substitute teachers to cover for teachers in training, or to paying teachers to be trainedafter school hours. if teachers agree to attend classes on their own time, there will be costs only for thetrainer. estimated savings: $0 to $1.5 billion (onetime); $0 t0 $300 million (annual)5. the lan is installed by volunteers. in the models, 65 percent of the costs for installing the lan arededicated to labor. if schools can do this work with volunteers, then the cost savings are significant. asan example, val verde unified school district in california laid its wires with volunteers from parentsand community members. if such groups provide labor at no cost to schools, schools will reap significantsavings. estimated savings: $1.1 billion to $3.1 billion (onetime)6. personal computers are donated to schools. in the models, there is a need to purchase a significantnumber of pcs to provide four to five connections to the network in every classroom. the costs for thesepcs can be offset by donations of new machines from pc manufacturers. it is also possible for largecorporations to donate these computers to schools. however, the schools will need fairly modernmachines to run networking software. the success of a donation program is dependent on the quality ofthe equipment donated. donations of obsolete or incompatible equipment may be costly to schools. estimated savings: $5.1 billion to $10.2 billion (onetime)7. network routing equipment is donated to schools. this program is similar to the pc donation program.the savings are lower since the routing equipment is less expensive. estimated savings: $221 million to $425 million (onetime)8. network servers are donated to schools. this program is similar to the pc donation and router donationprograms. estimated savings: $370 million to $1.5 billion (onetime)9. internet connectivity is made free to schools. there are great potential cost savings if schools are giveninternet access at no cost. this plan could be arranged either by provision from an internet serviceprovider or from a local university or community college that has its own internet connection. estimated savings: $150 million to $630 million (annual)table 8 summarizes the potential savings for u.s. schools nationwide from each of the possible programs.technology and cost models for connecting k12 schools to the national informationinfrastructure507the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 8 total estimated savings for u.s. schools benefiting from various possible costsavings programs fornetwork connectivity (millions of dollars)onetime savingsongoing savingstype of programlowhighlowhighreduced telecom rates (30 percent reduction)8921839150reduced telecom rates (60 percent reduction)17943578300purchasing by states (30 percent reduction)1,8894,13645189support from universities007901,800teachers trained on own time01,5000300free labor for installing network1,1253,08800donation of pcs5,10010,20000donation of routers and csu/dsus22142500donation of servers3701,50000free internet connectivity00150630source: rothstein (1994).conclusionswith a clearer picture of the costs for connecting schools to the nii, a number of conclusions may be drawn: the costs to network a school are complex. it is not simple to estimate the costs for connecting a particularschool to the network. the costs for most schools will fall into a bounded range, but each particular schoolwill vary greatly, depending on its individual needs and characteristics. although this analysis puts boundson the cost figures, the numbers are rough estimates at best. the cost of the network hardware is only a small fraction of the overall costs for connecting to the nii.initial training and retrofitting are the largest onetime costs for starting connectivity to the network. thecosts for the wiring and equipment are typically not as high. support of the network is the largest ongoingannual cost that schools must face. there are two major jumps in the costs to network a school. the jumps occur in the transitions from model 1to model 2 and from model 4 to model 5, as illustrated in figure 8. the first jump in cost occurs when theschool installs a lan. at that point the school and district must pay to have the network installed ($20,000to $55,000 per school) and employ fulltime network support staff ($60,000 to $150,000 per school district).the second jump occurs if and when the school decides to purchase computers for all students to use. thenumber of networkable pcs in 1994 is inadequate for most schools; hundreds of thousands of dollars wouldbe needed to provide multiple pcs in every classroom. also, many schools will need major electrical work(possibly exceeding $100,000 each) to support the increased number of pcs in the school. in theintermediate stages between these jumps, the costs are incremental and relatively small. the startup costs for connection to the network increase at a faster rate than the annual ongoing costs asthe complexity of network connection increases. in the less complex models, the onetime startup costs are 2to 3 times the annual ongoing costs of the network. however, for the more complex models (models fourand five,) the onetime costs are 5 to 15 times the costs to start connecting to the network. the differences areillustrated in figure 9. the divergence indicates that the most significant cost hurdle that a school will faceis the initial investment in the network and computers. dispensers of educational funding should be aware ofthis circumstance, so that they can help schools overcome the initial barrier. schools should be givenflexibility to amortize initial costs, to spread out the burden over a number of years.technology and cost models for connecting k12 schools to the national informationinfrastructure508the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 8 ongoing costs per student with increasing complexity of network connectivity. source: massachusettsinstitute of technology, research program on communications policy (1994).figure 9 startup and ongoing costs per student with increasing complexity of network connectivity. source:massachusetts institute of technology, research program on communications policy (1994). costs are significantly reduced when aggregated at the district and state levels. schools stand to save a lotof money by pooling resources and purchasing power with other schools at the district and state levels.when schools share a highspeed data link, or support staff, the perschool costs drop considerably. schoolsin north carolina and kentucky have saved 20 to 50 percent by purchasing services and equipment at thestate level.further research on the costs of wireless and cable internet access methods for schools is recommended toelucidate the costs and benefits of these approaches. in addition, the issue of software and equipment costaccounting requires further analysis. this preliminary assessment of the costs of connecting schools to the nii isintended as a point of departure for analysis of these and other more detailed models of nii connectivity.bibliographyanderson, ronald e. (ed.) 1993. computers in american schools, 1992: an overview. iea computers in education study, university ofminnesota, minneapolis.technology and cost models for connecting k12 schools to the national informationinfrastructure509burns, p. 1994. models and associated costs for connecting k12 to the internet. draft white paper available from the university ofcolorado.california department of education. 1994. building the future: k12 network technology planning guide.carlitz, r, and e. hastings. 1994. stages of internet connectivity for school networking. common knowledge: pittsburgh white paper.computer science and telecommunications board, national research council. 1994. realizing the information future: the internet and beyond. national academy press, washington, d.c.gargano, j., and d. wasley. 1994. k12 internetworking guidelines. internet engineering task force (ietf) draft paper.information infrastructure task force. 1994. putting the information infrastructure to work. u.s. government printing office, washington,d.c.the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.massachusetts telecomputing coalition. 1994. models for connecting k12 schools to the internet. draft white paper.newman, d., s. bernstein, and p. reese. 1992. local infrastructures for school networking: current models and prospects. bolt, beranekand newman inc. report no. 7726.public school forum of north carolina. 1994. building the foundation: harnessing technology for north carolina schools and communities.rothstein, r. 1994. connecting k12 schools to the nii: technology models and their associated costs. u.s. department of educationworking paper.rothstein, r., and l. mcknight. 1995. architecture and costs of connecting schools to the nii. submission to t.h.e. journal.u.s. department of education. 1993. digest of education statistics . u.s. government printing office, washington, d.c.technology and cost models for connecting k12 schools to the national informationinfrastructure510the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.58geodata interoperability: a key nii requirementdavid schell, lance mckee, and kurt buehleropen gis consortiumstatement of the problema wide range of activities depend on digital geographic data and geoprocessing, and these informationresources are rapidly becoming more important as commerce expands, information technology advances, andenvironmental problems demand resolution. unfortunately, noninteroperability severely limits the use of digitalgeographic information. spatial data exist in a wide range of incompatible and often vendorproprietary forms,and geographic information systems (giss) usually exist in organizations as isolated collections of data,software, and user expertise.the potential uses for geodata in the context of the national information infrastructure (nii) reach farbeyond current uses, making current limitations even more disheartening. if legacy geodata, data generated bynew technologies such as geopositioning systems, and highresolution, satelliteborne sensors were easilyaccessible via networks, and if spatial data of various kinds were compatible with a wide range of desktop andembedded applications, the effects would be revolutionary in nii application areas such as emergency response,health and public safety, military command and control, fleet management, traffic management, precisionfarming, business geographics, and environmental management.the problem of geodata noninteroperability will be solved in part by institutional cooperation: organizationsneed to create data locally in standard formats that can be used globally. but we need to plan a technologicalsolution if we want systems in which diverse applications can transparently exchange diverse geodata types andcan access remote spatial databases and spatial processing resources in real time. the technological solution is todevelop a common approach to using spatial data with distributed processing technologies such as remoteprocedure calls and distributed objects.in this paper we look at the components of the geodata noninteroperability problem and describe thetechnical solution and the unique consortium that is providing that solution. this consortium approach, whichinvolves users in the specification process and which organizationally isolates technical and political agendas,holds promise as a model means of simultaneously developing important technologies and developing andimplementing technology policy.backgroundthe importance of geospatial information and geoprocessing servicesthe nii broadly defines an information technology environment for the development and use of manyinformationbased products of vital significance to the nation. fundamental to the design and function of manyof these products is the use of geospatial data, more commonly known as geographical information. sowidespread and critical is the use of geographical information and the geographical information systems (giss)that have been marketed to work with this information in its many forms that president clinton, in executiveorder 12906 (aprilgeodata interoperability: a key nii requirement511the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.11, 1994), established the national spatial data infrastructure (nsdi). he commissioned the federal geographicdata committee (fgdc; chaired by secretary of the interior bruce babbitt) to document the nsdi, to give itoperational form, and to provide a management structure designed to help it grow and become a focus for thecombined efforts of both public and privatesector organizations concerned in any way with the developmentand use of geospatial information.in terms of the clear objectives that give it form and sustain it, as well as its highly focused datadevelopment and standardization activities, the nsdi is positioned to be a welldefined and vital component ofthe nii and a valuable resource for those working in a variety of the application areas targeted for intensivedevelopment by nii policy planners. however, although the nsdi was conceived as a distributed data resource,it lacks the operational framework to support realtime access to distributed geoprocessing resources and theirassociated database archives. resources such as the wide area information server (wais) and governmentinformation locator service (gils) are available for catalog access and superficial browsing of spatial data sets.but the problem of remote query against the wildly heterogeneous assortment of spatial data sets that constitutethe nsdi's ''clearinghouse" environment will not be solved until an operational model for the interoperability ofdistributed geoprocessing environments is developed and accepted by the geoprocessing community.the nation's wealth of geospatial data is vast and is used more and more by developers and planners at alllevels of operation in both the public and private sectors. much of the information technology world is rapidlytransforming its basis of computation from the tabular domain (the world of spreadsheets and accountingledgers) to the spatial domain (the world of maps, satellite imagery, and demographic distributions).applications that merge digital mapping, position determination, and object icons are already being introduced inactivities such as overnight delivery service and urban transit. these three technologies and new databasemanagement systems capable of handling multidimensional data will play an important role in operationsdecision support systems, maintenance management, and asset management wherever assets and processes aregeographically dispersed.as the nii concept challenges organizations to adopt more comprehensive, enterprise processing modelsbased on widearea, multimedia communications technologies, there is a growing need to invest the nsdi withdistributed processing capabilities that can ensure the full integration of geoprocessing resources into thesemodels. commercial need for better spatial data integration is already clear in areas such as electric and gasutilities, rail transport, retailing, property insurance, real estate, precision farming, and airlines. given the criticalnature of applications positioned to combine "realtime" and geospatial attributesšemergency response, healthand public safety, military command and control, fleet management, environmental monitoringšthe need toaccomplish the full integration of nsdi resources into the nii context has become increasingly urgent.the importance of interoperability standards to niibased applicationsfundamental to the development of enterprise information systems is the concept of interoperability, whichis used at all levels of information technology development to define a user's or a device's ability to access avariety of heterogeneous resources by means of a single, unchanging operational interface. at the level of chiptechnology, the interface is a backplane or a bus specification; at the network level the interface may be ahardwarebased transmission protocol or a packet specification; at the operating system level, the interface is aset of system calls or subroutines; and at the object programming level, the interface is a specification for thebehavior of object classes. interoperability thus denotes the user's ability to function uniformly and withoutproduct modification in complex environments by applying a standard interface to heterogeneous resources.in the geodata domain, interoperability is defined as the ability to access multiple, heterogeneousgeoprocessing environments (either local or remote) by means of a single, unchanging software interface.interoperability in this context also refers to accessing both multiple heterogeneous data sets and multipleheterogeneous gis programs. by definition, this view of interoperability assumes the sort of interoperablenetwork environment envisioned by nii policy.the interoperability profile of the nii is characterized by multiple hardware and software standards, somecomplete and some under development. such standards are the product of years of concentrated effort on the partof both public and privatesector organizations, as well as such recognized standards bodies as the americangeodata interoperability: a key nii requirement512the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.national standards institute (ansi) and the international organization for standardization (iso). within theoperational framework established by these standards, it should be possible to exchange diverse informationencoded in a wide variety of data formats among all known makes and types of computer, communications, andvideo systems. already significant strides have been taken toward integrating such forms of information asdatabase tables, with the result that both database applications and nondatabase applications such as spreadsheetand word processing documents can use database information without switching applications.to integrate geospatial data into the application environment of the nii and to use it in a coherent way, thefgdc has taken valuable first steps by organizing an approach to standardizing on an exchange format thatincludes entity attribute schemes for each of the significant geoprocessing application areas in the federalgovernment and a standardized description of the content of geospatial data collections (i.e., their metadata).associated with this effort is the establishment of the national spatial data clearinghouse, a directory andmetadata catalog accessible by the internet describing a rich assortment of registered databases meant to definethe resources of the nsdi. taken together, the geodata standardization and clearinghouse initiatives make upthe data resources needed to introduce geoprocessing into the mainstream of niibased distributed applicationsystems. however, they do not address the interoperability issue. they remain available as a "static resource,"requiring either that data be accessed by software that is familiar with its native format or that entire data sets beconverted to standardized transfer formats and reconverted to the user's processing format before any usefulwork can be done with them. many databases are frequently updated, which adds to the cost of acquisition andconversion.to make the nsdi data both freely available and useful in the nii environment, it will be necessary tointroduce a standardized interoperability mechanism specifically designed for remote access of geographicaldatabases. such a mechanism must embody the interoperability principles on which the nii is based. that is, itmust enable transparent, barrierfree access to heterogeneous geospatial data sets in a distributed environment,relying on standardized interfaces for the generation of queries in real time and the consequent retrieval of queryderived data sets to the user's native environment.a significant issue faced in recognizing the necessity of such an interoperability mechanism forgeoprocessing is that the geoprocessing community has been slow to adapt many recent advances in mainstreaminformation technology. traditionally, gis packages (whether commercial or public sector) have created highlystructured and architecturally closed operational environments, tightly coupling display graphics with spatialanalysis mechanisms, and relying on a tight coupling of spatial analysis with proprietary spatial databasedesigns. such packages tend to be operationally monolithic, lacking the modularity required for an efficient useof the distributed architectures that are more and more coming to characterize enterprise computing. in the past,the gis user worked primarily in a closedshop environment on limited data sets that could be archived andmaintained in the local environment and only intermittently updated by means of tape import or batch transferover the network. with the advent of the internet and the associated surging demand for "processed data" in realtime, such closed environments fail to provide geoprocessing professionals with the same ready access to thenation's data resources as people in general are coming to expect of such institutions as libraries, legal archives,news and entertainment programming, and general information and interaction services on the internet.analysis and forecastcontingencies and uncertaintiesthe growth of the nsdi subset of the nii, like the growth of the nii itself, is a complex, manyfacetedphenomenon. as in a developing economy or ecosystem, there are many complex dependencies. organizedcooperative effort will characterize the growth in some domains; chance, raw market initiatives, and surprisebreakthroughs will dominate in others. the rise of geodata interoperability depends on the expertise,commitment, and user focus of those who are directly addressing the problem, but it also depends on theinfrastructures that support their efforts and on the vagaries of the market.geodata interoperability: a key nii requirement513the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in this section we describe a geodata interoperability standards effort in progress and note the otherstandards and new technologies on which it depends. we also describe the process by which the standard isbeing developed, and the participation essential to that process.the ogisthe open geodata interoperability specification (ogis) is a specification for objectoriented definitions ofgeodata that will enable development of true distributed geoprocessing across large networks as well asdevelopment of geodata interoperability solutions. ogis is like other emerging distributed objectorientedsoftware systems in its basic structure and benefits, but it is the first largescale application of object technologyto gis and to the management of spatial data in the global information infrastructure (gii) and nii contexts. it isbeing made sufficiently general so that it can be implemented using software methodologies other than objecttechnology, including remote procedure call (rpc) architectures such as the open software foundation'sdistributed computing environment (dce) or application linking schemes such as microsoft's object linkingand embedding (ole).the ogis will specify a wellordered environment designed to simplify the work of both developers andusers. developers adhering to ogisdefined interface standards will easily create applications able to handle afull range of geodata types and geoprocessing functions. users of geodata will be able to share a huge networkeddata space in which all spatial data conforms to a generic model, even though the data may have been producedat different times by unrelated groups using different production systems for various purposes. in many casesautomated methods will bring older data into conformance. the ogis, and the object technology that underliesit, will also provide the means to create an extremely capable resource browser that users will employ acrossnetworks to find and acquire both data and processing resources. searches will be executed using powerful,objectoriented distributed database technology developed to handle large, complex, abstract entities that cannotbe managed in conventional relational database management systems. queries will potentially return only thedata requested, not whole data sets that will require further reduction and preparation before the user can begin.with the ogis in place, the gis industry will have the tools it needs to address the expensive geodataincompatibility problems faced by federal agencies, large private enterprises, and state and local governments.the ogis will provide a technological boost to organizations committed to spatial data transfer standards. byproviding a "smart" generic wrapper around data, ogis will achieve the objectives of a universal data formatwhile respecting the widely varying missions of data producers. making good use of distributed processing onhighbandwidth networks, ogis will multiply the utility of geospatial databases and reduce the size and durationof data transfers.object technologyto understand how the ogis will work, one must understand the basics of object technology. objecttechnology will, over the next 5 years, change the way we conceptualize, build, use, and evolve computersystems. it provides a way to integrate incompatible computer resources and a way to build software systems thatare much easier to maintain, change, and expand. its robust components can be quickly assembled to create newor modified applications. it is synergistic with the information superhighway, offering a model in whichadaptable agents spawned by a user's local computer can act across the network. in harmony with today'semphasis on sharing of data and resources within enterprises, it breaks out of the earlier model that sequestersproprietary software and data within isolated systems. object technology will make it much easier fornontechnical people to access and customize information and information systems, because many instances ofdata conversion and manipulation will become transparent.the old way of programming, procedural programming, segregates data from the programs that operate onthe data. procedural programming makes sense when processing resources are in short supply and programmersare available in abundance to maintain and "debug" aging software systems that have grown intogeodata interoperability: a key nii requirement514the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved."spaghetti code." the world is moving away from this model. processing power is abundant and cheap, and largeenterprises now look to object technology to reduce escalating software maintenance costs (and data conversioncosts) that are straining their budgets.moving beyond standalone programs that operate on specific kinds of data to perform specific tasks,developers of large enterprise systems are now beginning to write software in special modules called objects.objects typically include both data and behaviors. data are spoken of as being "encapsulated" in an object. thecapsule that contains the data is a set of behaviors (or procedures, or methods) that can act on the data and returna result when requested to do so by another object. if an object is requested to do something it cannot do, itreturns an error message. older, preexisting data and applications can be encapsulated so that they will work inthe object environment.client objects in distributed object systems can learn other objects' contents and capabilities and invokeoperations associated with those capabilities. in other words, objects interact as clients and servers. the objecttoobject messaging that is central to object technology gains efficiency through a system by which objects belongto classes with common properties. classes can be nested in hierarchies. in these hierarchies, classes inheritattributes (data and procedures) from classes that are higher in the hierarchy. inheritance allows objects toefficiently represent the large amount of redundant information that is common to all the objects of a class.large object systems, especially distributed systems, typically have an interface layer called an objectrequest broker (orb) that keeps track of the objects and activities in the system. the orb screens for improperrequests, mediates between competing requests, makes request handling more efficient, and provides an interfacethrough which dissimilar applications can communicate and interoperate.just as a microsoft word user can now run analyses on a microsoft excel spreadsheet embedded in a worddocument without changing applications, a gis user will access (through the development of the ogis and theobject technology environment) a full set of gis capabilities while working in an application that may not be agis application. just as the word user no longer needs to convert the spreadsheet to tabdelimited text beforeimporting the static data into the word document, the gis user will not need to convert data formats andprojections.the ogis architecturethe ogis will provide the following: a single, "universal" spatiotemporal data and process model that will cover all existing and potentialspatiotemporal applications; a specification for each of the major database languages to implement the ogis data model; and a specification for each of the major distributed computing environments to implement the ogis processmodel.by providing these interoperability standards, the ogis will also provide the means for creating thefollowing: an interoperable application environment consisting of a configurable user workbench supplying thespecific tools and data necessary to solve a problem; a shared data space and a generic data model supporting a variety of analytical and cartographicapplications; and an interoperable resource browser to explore and access information and analytical resources available on anetwork.the two main components of the architectural framework of the ogis are the ogis geodata model (ogm)and the ogis reference model (orm).geodata interoperability: a key nii requirement515the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the ogm is the core component of the framework. it consists of a hierarchical class library of geographicinformation data types that make up the shared data environment and unified data programming interface forapplications. cohesive, comprehensive, and extensible, encompassing all current geospatial and temporaldescriptions of data, presentation issues, analysis modes, and storage and communication issues, the ogm willbe the language of ogis. the ogm will be the interface to geodata transfer standards such as the spatial datatransfer standard (sdts), spatial archive and interchange format (saif), and digital geographic standard(digest). it will be interoperable with sql3mm, the next generation of sql, which will be object orientedand will support multimedia entities, including geospatial data.the ogm will support object queries, read/write of multiple formats, temporal modeling, and longdurationtransactions. to accomplish these objectives, it will include sophisticated gis definitions of spatial objects,fields, and functions.the orm describes a consistent open development environment characterized by a reusable object codebase and a set of services. the design approach for the ogm determines the set of services that must besupported in the orm. as a result, the orm requires directories of services and databases, which will supportcomplex query processing. it also specifies standard methods for requesting and delivering geospatialtransformations and processing tasks. the orm will also facilitate transformations between "private" data andogm constructs, as well as coordinate conversion and raster/vector conversion. it also manages visualizationand display and supports data acquisition.other emergent it standards and their relationship to ogisas mentioned previously, implementations of the ogis will be layered on interfaces such as corba,dce, and ole. these are, in fact, the three major interfaces that are likely to be used. the success of the ogiswill therefore ultimately depend on the robustness, completeness, stability, schedule, and market acceptance ofthese standards. corba and dce are the products of consortia; ole is the product of microsoft. the successof a consortiumproduced standard depends greatly on whether the members put in enough time and money tofinish the standard in a timely and complete fashion and whether the members understand the needs of thecommunity that might use the standard. the success of a vendorproduced de facto standard like dce dependsgreatly on the vendor's market penetration. the success of any standard depends on whether the standard solvesa problem for a sufficient number of users, with sufficient costbenefit advantage to make the standardsbasedsolution more attractive than alternative nonstandardsbased solutions.the robustness, completeness, stability, schedule, and market acceptance of other aspects of the nii alsobear on the success of the ogis. everything from the installation schedule of broadband cable to the security ofnetworkbased payment schemes will affect the demand for geodata and remote geoprocessing. reciprocally,working geodata and geoprocessing solutions will help drive the demand for bandwidth, payment schemes, andnetworkready devices, as well as the demand for more data and applications.in the domain of gis, the standards work of the fgdc and other groups focused on institutional solutionsto interoperability will contribute to acceptance of ogisbased solutions. these groups are focusing the attentionof tens of thousands of traditional gis users. these users will be motivated to publish their data electronically,and their need for data will simultaneously increase the demand for remote access to the data of others. theexpense of manually bringing data into conformance will, we believe, often be greater than the expense of usingogisbased automated methods, thereby increasing the demand for ogisbased solutions.factors in making the business case: who will build the ogis and why?the open gis consortium, inc. (ogc) is a unique membership organization dedicated to open systemapproaches to geoprocessing. by means of its consensus building and technology development activities, ogchas had a significant impact on the geodata standards community and has successfully promoted the vision ofgeodata interoperability: a key nii requirement516the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved."open gis" as the vehicle for integration of geoprocessing with the distributed architectures positioned to definethe emerging worldwide infrastructure for information management.ogc's direction is set by a board of directors selected to represent key constituencies in the geoprocessingcommunity. the ogc board speaks on behalf of both public and privatesector users interested in finding moreintegrated and effective ways to use the world's increasing wealth of geographical information to supportproblem solving in such areas as environmental monitoring and sustainment, transportation, resourcemanagement, global mapping, agricultural productivity, crisis management, and national defense.the ogis project technical committee of the ogc operates according to a formal consensus processstructured to be fair and equitable and to ensure the technical completeness of the specification. to ensure theeventual adoption of ogis as an official standard, the ogis project technical committee is represented on keygeodata, gis, and geomatics standards committees, including the international organization for standardization(iso) tc211 gis/geomatics committee and the american national standards institute (ansi) x3l1committee. in addition, the ogis technical committee maintains close ties to the federal geographic datacommittee (fgdc).the ogis project management committee is composed of representatives from the ogis projecttechnical committee, representatives of the principal member organizations, and others who represent particularconstituencies. the ogis project management committee maintains a business plan for the project and setsoverall policy for the project. the dual committee structure serves to separate technical and political issues.ogc was founded to create interoperability specifications in response to widespread recognition of thefollowing problematical conditions in the geoprocessing and geographic information community: the multiplicity of geodata formats and data structures, often proprietary, that prevent interoperability andthereby limit commercial opportunity and government effectiveness; the need to coordinate activities of the public and private sectors in producing standardized approaches tospecifying geoprocessing requirements for publicsector procurements; the need to create greater public access to public geospatial data sources; the need to preserve the value of legacy gis systems and legacy geodata; the need to incorporate geoprocessing and geographic information resources in the framework of niiinitiatives; the need to synchronize geoprocessing technology with emerging information technology (it) standardsbased on open system and distributed processing concepts; and the need to involve international corporations in the development and communication of geoprocessingstandards activity (particularly in the areas of infrastructure architecture and interoperability) to promote theintegration of resources in the context of global information infrastructure initiatives.ogc is notforprofit corporation supported by consortium membership fees, development partnerships,and cooperative agreements with federal agencies. as of april 26, 1995, the consortium included 40 members.though organized to manage multiple project tracks, ogc's initial focus is on the development of the ogis.ogc plans to establish other project tracks in areas related to implementations of the ogis architecture.who will use the ogis and why?the ogis will be used by at least the following groups: software vendors have already begun writing software that conforms to the objectbased paradigmexemplified by the ogis. gis database, visualization, desktop mapping, and application tool vendors willall be able to provide capabilities that will seamlessly integrate into applications targeted for specific enduser communities. just as the new paradigm will transform monolithic desktop applications intoenvironments of compatible components (such as printing modules that work with a spreadsheet or wordprocessor, making it unnecessary for the spreadsheet or word processor to contain its own printingsoftware), ogisbased componentsgeodata interoperability: a key nii requirement517the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.will perform specific analysis and display functions, and they will work seamlessly with other, nongisapplications. integrators working on systems that require the ability to access geodata and geoprocessing resources arealready building ogis compliance into their frameworks. data providers, public and private, who are preparing to serve data across networks, are planning andprototyping encapsulation schemes and database approaches that will depend on the ogis. this group willultimately include, for example, public digital libraries; remote sensing data vendors like spot andeosat; state and federal agencies that need to distribute data to agency offices and/or that are obliged tomake their data publicly available; major corporations with spatial data in their corporate databases; andmilitary intelligence, planning, and training groups.ultimately, the ogis will be an invisible but essential enabler of ready access to remote geodata in all ofthe application areas mentioned here, and probably in others that we have not imagined. like other standards, itwill not be noticed or understood by most people who use it. it will make an extraordinary complex activitysimple.what can the federal government do to facilitate the process?many agencies of the federal government have a critical interest in geodata interoperability, and some arealready providing support to ogc. though most of ogc's funding and technical participation comes from theprivate sector, ogc was started with funding from the u.s. army corps of engineers construction researchlaboratories (usacerl) and the doa natural resources conservation service (nrcs). both of theseagencies are still involved with ogc. also, the universal spatial data access consortium (usdac), a nasafunded digital library technology project, has joined the ogis testbed program of ogc. funding for usdaccomes from a cooperative agreement notice, "public use of earth and space science data over the internet,"provided by nasa's high performance computing and communications program. other federal agencies thatare members include u.s. doc noaašnautical charting division, u.s. dod defense mapping agency(dma), and u.s. doi geological surveyšnational mapping division.other federal agencies with a critical interest in geodata interoperability would benefit from participating inthe development of the ogis, because their specific needs would be represented and they would be in closetouch with the organizations best able to help them. also, their support would help ensure timely completion ofthe ogis, and as a result they would begin to reap the benefits of ogisbased solutions as early as possible.the federal government can also encourage other distributed computing standards efforts on which theogis will depend, such as the object management group's common object request broker architecture(corba). the ogis will have value in nondistributed environments, but its greatest potential lies in thepotential for remote geodata access.perhaps the most important way the federal government can help the cause of geodata interoperabilityšwhich is to say, to bring the nsdi into the niišis to make the ogis project a special focus for the nationalinformation infrastructure task force (iitf). technology policy at the iitf level needs to stress private sectorsupport for the standards activities of ogc. the iitf is positioned to strongly encourage major corporations(particularly telecommunications companies) to put significant resources into supporting ogis developmentwork, testbed activities, and, finally, ogisbased products and services. the iitf can provide solid leadership inshowing major corporations why this kind of interoperability ought to be considered a highpriority developmentissue.projections, including barriersogisbased implementations will become available in the same time frame in which underlying andauxiliary nii capabilities will become available. early prototype work has begun at testbed sites, even though thegeodata interoperability: a key nii requirement518the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.specification is not complete. the specification will be complete in about a year, and several testbed projects andindependent efforts by vendors are expected to begin yielding useful implementations at about that time. theogis merely prepares the world of geodata and geoprocessing to participate in the general progress ofinformation technology (it) that industry experts forecast. the ogis is being developed concurrently with otheressential software standards and components of the nii that will be developed, released, and tried in the realworld of the marketplace in the next 5 years.dissension in the software community is the principal barrier to this general progress and to the progress ofgeodata interoperability. the long history of unix shows how industry giants seeking dominance can thwartstandards efforts and stunt the growth and acceptance of a useful technology. every standards consortiumconfronts the tendency for industry leaders to fly apart because of competitive issues. from the standpoint of theogis, it is important that underlying standards like corba and dce succeed and that the members of ogcstay true to the vision of geodata interoperability across the industry.recommendationshow can the problem of geodata noninteroperability be addressed by a public andprivate partnership?as described above, the problem of geodata noninteroperability is being addressed by a standardsconsortium, the open gis consortium. ogc is developing not another geodata standard but rather a standardapproach to using distributed computing methodologies, primarily object technology and rpcs, in geodata andgeoprocessing applications. participation by public and private organizations is particularly important in this casebecause so many public agencies are critically dependent on geodata and geoprocessing, and because mostgeodata is a product of public agencies. the public agencies have everything to gain and nothing to lose fromtheir support of ogc, because (1) the ogis project gives them an opportunity to increase the certainty that thespecification, and solutions based on it, will meet their needs, (2) agency representatives have the opportunity tolearn about and learn from the most knowledgeable technology providers, and (3) the capabilities unleashed bythe ogis will enable them to vastly improve their ability to fulfill their missions.ogc members, public and private, recognize the wonderful opportunity afforded by the timing of thiseffort: no vendor has yet dominated the market with objectbased gis solutions that might hinder the effort tocreate a rational, comprehensive, standardized approach that can potentially benefit everyone. through theconsortium, vendors and integrators have an opportunity both to involve sophisticated users in their commontechnology specification effort and to share the costs of this development. all of the members believe that theircollective efforts will lead to greatly expanded markets, though they also recognize that their existing productsand services will need to change to accommodate the new paradigm. most understand that change is inevitable inthe it industry whether they embrace it or not, and ogc offers a way for them to exert a degree of control, stayat the leading edge, and make relatively sound business projections.ogc's goal is to create successful standards for geodata interoperability, and many of the discussionssurrounding its founding focused on the failings of other consortia that tried and failed in their attempts to createstandards. leading consortium lawyers and experts in technology standardization contributed to the formation ofogc and its bylaws, and their guidance continues. the challenge is to carefully craft an organization and a set ofgoals that make all participants winners, and to be sure that all the interests of all affected constituencies arerepresented.ogc has a unique organizational structure that resulted from the process described above. it has a board ofdirectors, a fulltime professional staff, and separate project tracks. the board of directors has responsibility forapproving business plans from each track, for organizing new tracks, and for setting overall consortium directionand strategy. each project track has its own membership and is managed by a management committee that isresponsible for strategic and business planning, technical oversight, and product approval. (ogc products are itstandards related to geodata interoperability.) each track also has a technical committee responsible fortechnology development and mediation of the different technical views of the membership. a testbed program ingeodata interoperability: a key nii requirement519the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.each track coordinates early implementation efforts and provides feedback to the technical committee. thetechnical committee is responsible for product development, which it then delivers to the managementcommittee for final approval. this organizational structure isolates technical issues from management issues andallows a better overall progression of product development. it also isolates the board of directors from eachproject track.the iitf is positioned to help ogc succeed by encouraging key niibuilding companies to participate inogc's efforts. clearly, the technology policy mission of the iitf is perfectly aligned with the mission of ogc.but ogc also offers the iitf a higherlevel benefit: the opportunity to observe and evaluate a stateoftheartconsortium that may be the forerunner of tomorrow's technology policy planning bodies. the continuingacceleration of technological change forces a new, more anticipatory and proactive approach to technologypolicy planning, an approach based on communitywide discussion of objectives followed by development ofstandards that channel vendors' efforts in agreedupon directions.referencescommittee on applications and technology, information infrastructure task force. 1994. the information infrastructure: reachingsociety's goals. u.s. government printing office, washington, d.c., september.national institute of standards and technology. 1994. putting the information infrastructure to work: a report of the informationinfrastructure task force committee on applications and technology. sp857. national institute of standards and technology,gaithersburg, md., may.geodata interoperability: a key nii requirement520the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.59electronic commercedan schutzercitibank corporationelectronic commerce is the ability to perform transactions involving the exchange of goods or servicesbetween two or more parties using electronic tools and techniques. it offers many advantages over traditionalpaperbased commerce: it provides the customer with more choices and customization options by better integrating the design andproduction processes with the delivery of products and services; it decreases the time and cost of search and discovery, both in terms of customers finding products andservices (e.g., shopping, navigating) and companies finding customers (e.g., advertising, target marketing); it expands the marketplace from local and regional markets to national and international markets withminimal capital outlay, equipment, space, or staff; it reduces the time between the outlay of capital and the receipt of products and services (or vice versa); it permits justintime production and payments; it allows businesses to reduce overhead and inventory through increased automation and reduced processingtimes; it decreases the high transportation and labor costs of creating, processing, distributing, storing, andretrieving paperbased information and of identifying and negotiating with potential customers and suppliers; it enables (through automated information) production of a reliable, shareable historical database of design,marketing sales, and payment information; and it facilitates increased customer responsiveness, including ondemand delivery.a convergence of several factors has recently lifted electronic commerce to a new level of utility andviability. these factors include the increased availability of communications and communications bandwidth, thereduced cost and increased userfriendliness of computers and communications, the growth of the internet andonline services, and the drive toward global competitiveness.currently, online purchases account for only 4 percent of total global purchases; online purchases via suchprecursors as the internet are practically nonexistent. but electronic commerce is likely to grow dramaticallyover this decade. for example, it is predicted that within 6 years, global shoppers will use the nationalinformation infrastructure (nii) to purchase $500 billion of goods and services; this represents almost 8 percentof current purchases worldwide. and by 2005, the number of niibased transactions is expected to rise to 17billion, which is almost half the number of transactions made in today's credit card market.electronic commerce todayin recent years, great strides have been made to automate many of the laborintensive paperbased aspectsof commerce. examples abound of corporations that use electronic data exchange (edi), electronic mail (email), electronic forms (e.g., for ordering or for contracting), and electronic catalogs, and of electronic financialnetworks that speed the transfer, settlement, and clearing of funds and other financial instruments. theseelectronic commerce521the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.electronic tools and techniques provide many benefits to both customers and merchants. edi standards, forexample, enable fast, accurate information exchange between different automated systems in routine, relativelysimple business transactions.unfortunately, despite the widespread use of numerous methods of electronic support, electronic commerceis still not the most common method of carrying out business transactions. this is so for several reasons. for onething, most business transactions still require the physical exchange of paper documents and instruments, withthe inherent costs and delays this represents. for another, current electronic commerce approaches are notsufficiently well integrated, secure, open, or easy to use: partial solutions. current electronic commerce implementations automate only a portion of the entiretransaction process. for example, although ordering and distribution of an informationbased product (suchas an electronic magazine or a software program) can be nearly simultaneous, the supporting accounting andinventory information, payment, and actual funds transfer tend to lag, often by days. this time lag, and theresulting decoupling of the accounting and payment information from the ordering and delivery of goodsand services, increases the transaction's credit risks. it also increases the likelihood of discrepancies betweenthe various information sources, requiring expensive and timeconsuming reconciliation. finally, todayelectronic commerce implementations are costly to develop and operate. their high cost of entry does notmake them feasible for many of the more spontaneous, highvolume, lowvalue electronic transactions (e.g.,the sale and distribution of electronic informationbased products, such as magazine articles and photograps)envisioned for the future. a fully integrated electronic commerce solution would let users maximize theircontrol over their cash flows; for instance, it would allow the majority of their funds to work for them inbank savings accounts and/or investments, and it would minimize cash shortfalls. it would also eliminate thegaps between ordering, distribution, and payment, enabling development of realtime links to recordkeepingand accounting systems with minimal transaction costs. rigid requirements. electronic commerce applications usually require highly structured protocols,previously established arrangements, and unique proprietary bilateral information exchanges. theseprotocols, arrangements, and exchanges for the most part involve dedicated lines and/or valueaddednetworks (vans) and batch processing. for example, edi requires rigid agreements between the two ormore transacting parties about the structure and meaning of data. these agreements are often timeconsuming to negotiate, inflexible, and difficult to maintain, especially in a rapidly changing businessenvironment. the resulting costs and necessary lead times frequently create barriers to investment in andwidespread use of electronic commerce applications by small and mediumsize companies and inhibit theexpansion of electronic commerce beyond large companies and their major trading partners. limited accessibility. the consumer cannot usually communicate or transact with vendors in a simple,direct, freeform environment in today's electronic commerce applications. for example, to access mostelectronic shopping services, a consumer must subscribe to an online service (e.g., prodigy or cable tvshopping channels) that then provides proprietary hardware and/or software with which to communicatewith the vendors that have also registered with that service. limited interoperability. most current implementations depend on proprietary solutions, which do not easilyinteroperate, if at all. internet email and the world wide web are notable exceptions. a truly interoperableelectronic commerce infrastructure would allow parties to conduct their transactions in private, withoutpaying any fees to intermediaries unless they provide some real added value, such as credit services. thisinfrastructure would make it easier for any and all interested persons to become service providers as well asconsumers. insufficient security. the lack of personal contact and the anonymity associated with doing commerce over atelecommunications network make it difficult to authenticate parties and detect intruders; this in turn makesthe system vulnerable to fraud and increases the need for security services. additionally, the speed withwhich electronic commerce can be conducted leaves parties with less to react, check, and respondappropriately, again creating the potential for system fraud and abuse. lack of sufficient security inhibits theintroduction of direct, secure, realtime electronic payment and settlement systems that can support secureexchanges without prearrangements or third parties.electronic commerce522the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. inadequate search capabilities. participants in today's electronic commerce applications must find methodsand means of navigating effectively through the sea of rapidly increasing online electronic information andservices to find trading partners and items of interest. this problem will only increase as more informationand businesses go online.electronic commerce tomorrow: building the infrastructuremany new systems and service initiatives have been announced for the internet and the evolving nii thataddress one or more of these current deficiencies to varying degrees. these include initiatives such as (1)commercenet and secure hypertext transport protocol (shttp), (2) electronic catalogs, (3) advanced searchengines, (4) emailenabled edi, and (5) digital cash. additionally, several alliances and partnerships have beenannounced that address the need for secure, affordable payment, linked in real time to ordering and billingsystems. these initiatives include ventures by open market, microsoft/visa, netscape/first data/mastercard/bank of america, cybercash/wells fargo, american express/america online, first virtual/eds, netbill,netcash, cafedigicash, mondex, netcheque, netaccount, netchex, at&t, and internet mci. more suchalliances and startups are announced each day. these initiatives promise to accelerate the future growth ofelectronic commerce and rapidly decrease the overhead and time associated with today's paper and peopleintensive activities.so, in the near future, we should see a broad range of new valueadded electronic commerce services,including the following built around trust and security: authentication over public networks; certification of information, parties, and transactions; performance bonding; electronic escrow; automated dispute resolution; transaction insurance; appraisal services; various electronic broker services; and trusted agents to resolve disputes and claims.all of these initiatives must be developed within a common framework, or we run the risk of creatingisolated, noninteroperable implementations that will inhibit progress toward truly free, open, and spontaneouselectronic commerce. the joint ventures listed here, for instance, all vary in their approach to security andprivacy, their ability to handle micropayments, and their applicability to various types of transactions. they alsodiffer in their business modelsšfor example, in their pricing strategy and in their assumptions as to who bearsthe risk in case of insufficient funds or disputes.the electronic commerce infrastructure must therefore do the following: allow for interoperability. the infrastructure must be based on a common set of services and standards thatensure interoperability. preferably, these services and standards can be used as standard building blocks thatservice providers and application designers can combine, enhance, and customize. allow for maximum flexibility to permit innovation. as the nii evolves, it will grow and maturesignificantly, possibly in ways not even imaginable today. as it grows, new services and businesses willemerge. for example, nii's electronic marketplace will provide new opportunities for narrowcasemarketing to shortlived niche markets. also, existing services and products will be redefined and modified.the electronic commerce infrastructure will have to be sufficiently flexible to accommodate all of thesechanges and be able to address new applications and new requirements as they arise.electronic commerce523the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.issues related to electronic commerce activitiessome related issues important to the design of an electronic commerce framework are discussed below: nature of information products. a particularly important class of products on the nii are products that arepure information. in the information age, a large percent of commerce will never be embodied physically.information products are enabled by information technology and not just distributed more efficiently by it.these products can include not just electronic publications, catalogs, videos, and the like, but alsointeractive video games, software programs, electronic keys and tokens, customized design specifications,and even electronic keys that can open hotel rooms, cars, storage compartments, and airport boarding gates.furthermore, these information products are not created entirely by the service provider but can be designedor customized by the customer (e.g., customers can create their own selection of articles to be bound in anelectronic book, or their own custom clothing design), adding a customerdriven activity call designed to fitwith the purchase cycle. it is also likely that, for these products, ordering, billing, payment, and distributionwould likely all happen simultaneously. advanced revenue collection methods. the electronic commerce infrastructure will need to support advancedtypes of revenue collection, in addition to traditional methods (e.g., payment upon receipt, payment inadvance). for example, an information product service provider could distribute its product widely andcharge on a usage basisšthat is, charge the user only when the information (e.g., a software program, adigital document, an electronic key that can open and start a rental car) is used. one innovative approachthat permits usage accounting and payment is called ''meterware." it provides local hardware and/or softwareto continuously record and bill customers based on their usage. meterware and other advanced revenuecollection ideas (e.g., payment schemes, such as electronic cash and electronic checks, which do not requirethe presence of an online payment processor) create opportunities for reaching new customers and fordistributing products and services; these make great sense in a low or zerodistribution cost environmentand should be supported by the electronic commerce infrastructure. transaction devices. electronic commerce transactions currently involve all manner of devices (e.g.,telephone, fax, pointofsale device), media (e.g., electronic data, image, voice, paper), and networks (cable,wire, satellite, cellular) over which they will be delivered. as a result, the system infrastructure mustaccommodate all of these devices, media, and communications networks, without degrading them to thelowest common denominator. legacy systems. the electronic commerce domain has a large quantity of legacy systems that it needs tointerface to and ultimately phase out of as it evolves to more modern systems, applications, and processes.these legacy systems and processes (e.g., paper checks, mainframebased settlement and payment systems,and edi vans) will not be replaced overnight. a successful electronic commerce infrastructure must allowthe user to easily and transparently transfer between and switch back and forth between the new, allelectronic and the older, hybrid legacy systems and processes. public data. finally, libraries of publicly available and accessible files of registered contracts, financialreports, and holdings, as well as catalogs and lists of products, data, and services, if made part of the niidomainspecific infrastructure being offered by various electronic commerce vendors and informationproviders, could be interfaced with this electronic commerce scenario to further enrich it.as new initiatives start up under a common framework and set of standards, performed over lowcostcommodity computers linked by open public networks, competitive pressures and technology advances shoulddrive down associated costs and time and increase interoperability and competitiveness. new forms of commercewill arise that were impractical under the old cost structure, and the virtual enterprise will be fully realized.consider the following examples of what life might be like in the future nii if this electronic commerceinfrastructure is realized.electronic commerce524the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.sample commercial scenariocommercial examplemary wants to lease some warehouse space in a distant city that she will use as a distribution hub. her mainconcerns are location, price, and early occupancy. she clicks an icon displayed on her pc that establishes anonline connection to several broker icons advertised on the world wide web (www). she clicks on each ofthese broker icons in turn to scan their home pages. soon, she finds a property for lease that seems to match whatshe is looking for. the property is at the intersection of two major highways, has sufficient square footage, isbeing offered for a reasonably monthly price, and is available for lease at the beginning of the next month.mary checks the broker's credentials by clicking on a certification icon. the broker's credentials areimmediately transmitted to a reference service for authentication. within seconds, the reference service sendsmary an email message with the certification attached; mary is alerted to the arrival of this message by a specialinterrupt tone on her workstation.mary then clicks on the appropriate button, and an electronic application form to lease the property appears.she begins to fill out this form and inserts her electronic bank card into the pcmcia slot in her pc, whichdigitally signs the application with her financial credentials and binds the application data to her signature so thatit cannot be altered. mary then clicks the send button to transmit the application to the broker. the applicationcontains her identification, billing address, and permission for the broker to obtain a credit reference from herbank. mary then closes the broker page and turns to another task.several minutes later, the broker has completed a review of mary's application, including obtaining andanalyzing her bank's credit reference. most of this work was performed by intelligent software agents that wereautomatically activated and tasked upon receipt of mary's electronic application form. since mary's backgroundcheck was routine and acceptable, the application's approval was automatic. accordingly, the broker can sendmary a standard rental contract. she receives this as an email attachment concurrent with notification ofacceptance of her application.mary inspects the contract and finds its terms generally acceptable, except for two items: mary wants a 15day grace period rather than the 5day period specified in the contract; and although she has no problem with her bank transferring payment electronically to the broker's bank accounton the 15th of each month, she does not want this to occur without her first being notified, in case for somereason she needs to override and stop the automatic electronic funds transfer.mary emails these exception items to the broker, who reads and approves the changes, and makes oneadditional change: he specifies the conditions under which mary is justified in stopping payment. thisinformation is communicated via email to mary, who finds the conditions acceptable and date/timestampsthem, and again digitally signs the digital contractual agreement. she sends it back to the broker, who in turndigitally cosigns it. the process is witnessed by a network notary, who digitally signs the contract as witness;holds the original digital copies; and sends authenticated copies to mary, the broker, their respective banks, andthe appropriate government agency for filing and recording.now, each month mary's bank notifies her by email that it will electronically transfer funds from heraccount to the broker's account unless it receives override instructions from her within 24 hours. mary completesthe required acknowledgment of these electronic notices and date/timestamps them. when the bankelectronically withdraws funds from her account to meet the lease agreement, this information is automaticallytransmitted to the broker via email. the broker's financial software agent automatically makes the electronicdeposit of the funds. intelligent agents for the bank and broker then automatically update all appropriate financialrecords.electronic commerce525the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.sample retail consumer scenariodave gets an email inviting him to a formal blacktie dinner tomorrow when he returns from his businesstrip. unfortunately, he does not own a tuxedo and is 3,000 miles from his home. in the past, he has ordered arental tuxedo from a neighborhood store via computer. since his last promotion, however, he has been gettinginvited regularly to formal affairs and can easily justify buying rather than renting a tux. he dials up hiselectronic yellow pages, searches for tuxedo manufacturers, explores their interactive multimediaadvertisements, and finally finds a supplier to his liking. it allows him to customize his own tuxedo design onlineand promises 24hour delivery to any location in the united states.dave requests linkage to the tuxedo manufacturer's interactive design facility. a range of charges that varyaccording to the quality of transmission (image resolution and response time) is displayed, and dave is promptedto make a selection and to insert his american advantage citibank visa smart card into his pc reader. davemakes a selection and inserts the card, which issues a payment transfer from his credit card account to theelectronic yellow pages, subject to dave's approval and authorization. dave approves the charge, digitally signsthe authorization sealed with his unique personally encrypted retinal scan recorded by a camera mounted in hisportable pc, and inserts his card, thereby transferring the payment. dave is connected to the tuxedomanufacturer design service.the tuxedo manufacturer requests that dave transfer his dimensions. dave does so easily, his dimensionsbeing stored on a file in his pc. the manufacturer then transmits a number of images of typical tuxedo designsfor dave's selection. dave enlarges a few of these images to full size, adjusted to his form and dimensions. herotates them and views them in threedimensional projection from many viewpoints, and selects one. he thenproceeds to manipulate this design using his electronic pen; he makes minor adjustments and changes to thetuxedo, raising the waist and tapering the pants legs to better match his personal taste. he next specifies thematerial: 100 percent wool, treated for stainproofing and wrinkleproofing. when it is finished, dave transmitsthe revised design to the manufacturer, along with the desired delivery date and location. the manufacturerreplies with a charge appended to the design; this specifies the delivery date and location, and the terms andconditions of payment (either a credit card charge now, or a 20percentdown cash transfer now with theremainder due in cash on delivery). dave digitally signs the electronic agreement with his american advantagecitibank visa smart card, which he uses as payment, and sends it back to the manufacturer.the manufacturer receives the order and payment, and issues the design specifications along with a uniqueorder number to its factory for manufacture. at the same time, the manufacturer sends an order to federalexpress for pickup and delivery of the completed suit that afternoon; this order includes the suit's unique ordernumber, delivery address, customer name, and identifying encrypted retinal scan. by 3:00 p.m., the completedtuxedo has been picked up by the messenger.at 8:00 the next morning, dave arrives at his office. within 2 hours, a federal express messenger arrives athis office with the suit. dave authenticates himself, matches his retinal scan, and receives the tuxedo in plenty oftime for the evening's formal dinner.scope of proposed electronic commerce infrastructure frameworkto achieve the vision outlined above for electronic commerce, we need a comprehensive architecturalframework and a set of base infrastructure services and standards around which these and future initiatives canbe designed. the architecture must permit the flexibility, interoperability, and openness needed for the successfulevolution of electronic commerce over the nii. this framework, and its service and products, will offer theconsumer a diverse set of interoperable choices, rather than a collection of independent "stovepipe" solutions.this framework should set the groundwork for developing the electronic commerce infrastructure. to thisend, we begin by discussing the basic activities and functions associated with electronic commerce, identifyingthe key building blocks required to support those functions and activities, and describing key infrastructureservices and application programming interfaces (apis). these services and apis are placed within an electroniccommerce application architectural framework consisting of a services infrastructure layer and anelectronic commerce526the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.applications layer that interface with each other and with the physical layer. these layers are defined in terms ofapis and objects from which the various electronic commerce applications can be constructed.electronic commerce activities and functionsthere are nine key activities in commerce: advertising and shopping, negotiating, ordering, billing, payment and settlement, distribution and receipt, accounting, customer service, and information and knowledge processing.the specific functions associated with these activities in an electronic commerce setting are discussedbelow. note that not all of these activities are performed in every transaction, nor are they necessarily performedin this order; indeed, they may be performed in parallel. also, the activities are not necessarily all conductedelectronically. finally, these activities can vary in complexity and importance depending on the size and scope ofthe transaction.advertising and shoppingadvertising and shopping can include the following: a potential buyer browsing electronic yellow pages and catalogs on a network; an agent shopping on behalf of one or many buyers and/or sellers; a buyer sending an electronic request for proposal (rfp), and sellers responding with various offers; sellers advertising their products and services; and buyers electronically navigating and/or browsing through the world wide web's online services.a major problem associated with the advertising and shopping activity is the cost and time expended indeveloping, maintaining, and finding relevant information, products, and services, given the plenitude ofavailable information. obviously, this problem will become increasingly complex as more data and servicesbecome available online and the choices and possibilities multiply exponentially. we need new and better waysto find services and information and to publish and update this information.negotiatingbuyers and sellers may elect to negotiate the terms of a transaction (i.e., the terms of exchange andpayment). these terms may cover delivery, refund policies, arranging for credit, installment payments, copyrightor license agreements, usage rights, distribution rights, and so on. these terms can be standardized for routinecommodity use, or customized to suit unique individual situations. often, in the case of two parties with a wellestablished business relationship, the terms of exchange are prenegotiated as standing contractual terms for alltheir future exchanges. often, this process will also include authentication of the two parties.electronic commerce527the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.orderingthe buyer eventually issues a contractual agreement of the terms of exchange and payment. thiscontractual agreement is generally issued as an order, which sets forth the quantity, price, and other terms of thetransaction. the order may be verbal, in writing, or electronic. it usually includes an acknowledgment ofagreement by the various parties in order to help prevent any future repudiation. this agreement can beconfirmed electronically through cryptographic techniques such as digital signatures.in the case of some commodity purchases, the entire transaction may begin at this ordering stage, bypassingthe advertising/shopping and negotiating activities. the ordering activity applies to all transactions, regardless ofwhether billing will be involved. for example, even requests for free public information should be issued asformal orders so that the service provider can record and account for information requests.billingonce a seller has delivered goods or services, a bill is sent to the buyer. this bill generally includesremittance information that should accompany the payment. sometimes, a seller may require payment inadvance. sometimes, a supplier sends advance shipping notification, and the customer agrees to authorizepayment upon confirmation of the arrival of the products. and in some cases, as with the free informationexample cited above, this activity is eliminated entirely.payment and settlementthe buyer, or some financial intermediary, eventually sends some form of electronic payment (this could besome form of contract or obligation, such as authenticated payment instructions or digital cash), usually alongwith some remittance information to the seller. this payment may occur for a single item, on a usage basis, orwith a single payment for multiple items or usage. settlement occurs when the payment and remittanceinformation are analyzed by the seller or the seller's agent and accepted as valid.distribution and receipteither before, after, or concurrent with payment, the seller arranges for delivery of the purchased goods orservices to the buyer, and the buyer provides the seller with proof of receipt. policies regarding customersatisfaction and return should be negotiated prior to this activity and made part of the contract between buyer andseller. for larger, more complex orders, distribution may involve more than two parties and entail complicateddistribution coordination strategies. an ancillary distribution service involves acting as a fiduciary, and holdinggoods, certificates, bonds, stocks, and the like in trust.accountingthis activity is particularly important to corporate customers and suppliers. both buyer and seller mustreconcile all electronic transactions in the accounts receivable and accounts payable, inventory information, andaccounting systems. account and management information system records must also be updated. this activitycan involve third parties, if the transacting businesses outsource their accounting services.customer servicecustomer service entails the following:electronic commerce528the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. providing the buyer with timely information as to the progress of a transaction; handling customer requirements when transactions go awryšthat is, resolving any mistakes, disputes, orcomplaints concerning product quality, delivery, or payment (this includes managing returns and refunds,further exchanges, and/or repairs); and providing expert advice and assistance in the use of the products and services.customer service also involves providing general cash management advice, including addressing foreignexchange imbalances and risk exposures, collection of delinquent payments and late fees, and repossessingproducts for which payment is long overdue.information and knowledge processinga final key activity in electronic commerce is the collection, management, analysis, and interpretation ofthe various data to make more intelligent and effective transactionrelated decisions. examples include collectingbusiness references, coordinating and managing marketing strategies, determining new product offerings,granting and extending credit, and managing market risk. performance of these tasks often involves the use ofadvanced information management techniques, including models and simulations and collaborative computingtechnologies to support conferencing and distributed workflow processes.these tasks will become more difficult as the sources of information grow in number and are ofincreasingly diverse and uncertain quality. additionally, procurement of this information may raise significantprivacy concerns and issues.a model for electronic commerceto support the electronic commerce activities and functions discussed here, and to accelerate the electroniccommerce vision described earlier, we need an open electronic commerce architecture and a set of agreeduponelectronic commerce infrastructure standards and services. these elements are detailed in the following sections;here we describe an overall model of electronic commerce.major electronic commerce applications can be built by interfacing and integrating, through apis,elemental electronic commerce building blocks and services; these latter are provided by a variety of serviceproviders and application designers. the enabling infrastructure services include those needed to providerequisite transaction integrity, authentication, and privacy. the enabling services and apis must be at a lowenough level of detail (granularity) to provide open, seamless links to the key electronic commerce buildingblocks and services; they also must be simple and flexible enough to permit user customization and continuousimprovement and evolution over time. to maximize flexibility and modularity while admitting alternativecompeting implementations, an object model is preferred for describing and invoking these building blocks.the object model has close parallels with the digital object model associated with the defense advancedresearch projects agency (darpa) digital library program. both object models share many of the same needsand attributes (i.e., the need for naming, updating, retrieving, routing, pricing, and maintaining a repository ofdigital objects, and for addressing copyright and usage concerns). three key questions remain:1. should the application objects and services be selfdescribing?2. how should they be standardized, named, accessed, and updated (i.e., should we adopt object requestbroker standards)?3. how can they best be interfaced and integrated into existing processes, procedures, and legacy systems?electronic commerce529the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.electronic commerce architecturewe envision a threelayered electronic commerce architecture, in keeping with that described in a previouscrossindustry working team (xiwt) paper, with an architecture consisting of the following: the physical communications and computing infrastructure; an enabling infrastructure services layer; and an applications layer composed of an api layer and an application object layer.this architecture includes the infrastructure services needed to support the major electronic commerceactivities discussed here and the key electronic commerce services, objects, and apis discussed below.infrastructure servicesseveral generic infrastructure services are critical to a successful electronic commerce framework: reliable communications services; common security services; access control services; translator services; a software agent management and communications infrastructure; and distributed information resource discovery, retrieval, and synchronization and replication services (e.g.,search engines, browsing, and publishing tools).all of these services will probably be needed for most other applications domains as well and have been orwill be discussed in other xiwt papers in this regard. in this section, we discuss these elements with particularreference to their application in an electronic commerce setting. we also discuss specific services unique toelectronic commerce (e.g., paying and accounting).communications servicesfor electronic commerce, existing communications mechanisms (e.g., virtual circuits, routing andaddressing, datagrams, email, file transfer protocol [ftp], http, with image and other multimedia extensions)must be extended to incorporate the following features: reliable, unalterable message delivery not subject to repudiation; acknowledgment and proof of delivery when required; negotiated pricing by usage and/or quality of service; and directory services that can be rapidly updated and that support quick retrieval.these extensions are either generally available or under development. however, to support electroniccommerce, they must work across a variety of information and communications devices (including telephones,personal computers and workstations, settop boxes, and personal information managers and communicators);humanmachine interfaces (ranging from character text to virtual reality, and from keyboard and electronic pento speech recognition and gestures); communications media (including satellites, cable, twisted wire pair, fiberoptics, and wireless, which includes constraints on available communications bandwidth and reliability); andnomadicity (which includes supporting location independence, and remote personal file storage with privacyencryption).electronic commerce530the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.common security mechanismssecurity is a critical component of any electronic commerce application and must be addressed in designingany electronic commerce service infrastructure. electronic commerce system security should provide thefollowing types of guarantees to the user: availability. the system should prevent denial of service to authorized usersšfor example, if a third partyties up the network either inadvertently or intentionally. utility. the system should ensure that the user obtains and retains value of information. information can loseits value for the user if it is revealed to unintended third parties. integrity. the system should ensure that information is delivered whole, complete, and in good order, andthat, where applicable, the information is the same as agreed upon by all parties. date and timestampingalong with digital signatures is one mechanism for ensuring the latter. authenticity. the system should ensure that the parties, objects, and information are real and not fraudulentor forged. to be sure that users are negotiating and exchanging proper objects with proper parties, thetransacting parties, devices, and controlling and exchanged objects all need to be authenticated (i.e., verifiedthat they are who or what they claim to be and that none of the information or objects have been illegallytampered with or modified). this requires mechanisms such as digital signatures, passwords and biometrics,and certification hierarchies. confidentiality. the system should ensure that information communicated and stored is kept private and canbe revealed only to persons on an approved access list. usability. the system should ensure that only the rightful owner or user maintains possession of his or herinformation. even if others cannot decode and read the stolen information, if they can take possession of theinformation, they can deny the rightful owner the use to and access to it.access control servicesonce authenticated, users need to be authorized for requested services and information. user authorizationscan be provided as a blanket binary approval or granted only under or for specified conditions, time intervals,and/or prices. authorizations can be provided to designated individuals or to designated organizationalrepresentatives. it is therefore often desirable to authorize a user in terms of his or her location andorganizational function/role, as well as on the basis of individual identity.translator servicestranslators can transform and interpret information from one system into formats more suitable to otherinteracting objects and systems. translator services should be able to adapt and evolve automatically. forexample, a translator that can interpret a small subset of electronic forms that have been linked to sql relationsand data dictionaries should be able to prompt the user for any needed additional information and update itselfaccordingly. the translator could then be incrementally expanded by further manual linking of data relations toelectronic forms, by direct user query, and by learning from example. this capability for selective incrementalexpansion would enable a user to customize translators to meet unique needs and to expand the translator easilyso as to handle larger vocabularies and collections of electronic forms/documents as needed, as well asincorporate new edi standards as they evolve and become defined. finally, such a capability would helpsimplify and speed up the edi standards process.electronic commerce531the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.software agent management and communications infrastructuresoftware agents are intelligent programs that simplify the processing, monitoring, and control of electronictransactions by automating many of the more routine user activities. software agents may be local programsrunning on the user's machine, or they may actually transport themselves over the network infrastructure andexecute on the service provider's machine.agents are a relatively new development. currently, they can do such things as filter incoming mail,coordinate calendars, and find desired information for presentation to the user. over the longer term, agents arelikely to take over more complex tasks such as negotiating, translating, and overseeing and auditing electronictransactions. some additional future uses for software agents include personalization and customization ofapplications, and personalized searching, filtering, and indexing.eventually, we may have many different agents working for us, coordinating and communicating amongthemselves. when this comes to pass, we will need standards and infrastructure to support the necessarymanagement, negotiation, and coordination not only between users and agents but also among agents, and tomaintain agent repositories where agents can be stored, retrieved, purchased, and leased.in an electronic commerce setting, software agents should be able to do the following: control the workflow governing a set of electronic commerce transactions; operate a set of conditional rules specified and agreed to by the involved parties; monitor and enforce the terms and conditions of electronic contracts; provide an intelligent interface or a facilitator to existing proprietary vans and legacy systems, performingthe necessary translations and protocols; help the user find desired products and services, and navigate the nii on the user's behalf; purchase and negotiate on behalf of the user; and operate across a wide diversity of vendor hardware and software.distributed information resource discovery and retrieval servicesgeneric services. distributed information resource discovery and retrieval services help service providerslist and publish their services, and help users find services and information of interest. these informationservices cover the ability to maintain, update, and access distributed directory services. they also cover moreadvanced navigation services such as maintaining hyperlinks, advanced keyword and context search engines, andsoftware agents, such as web crawlers, that can explore and index information sources and services of interest.these services should be easy and efficient to update as well as to access and use. they should also be capable ofbeing implemented, maintained, and accessed over a number of locations distributed across the nii.unique services. in addition to the generic services just discussed, there are a number of desirableinfrastructural services that are unique to electronic commerce: accessing currency exchange services; accessing cash management services; accessing bonding services; accessing escrow services; accessing credit services; accessing various investment services; accessing various insurance services; accessing costing services; accessing financial information and reporting services; accessing notarization services; posting and accessing regulatory notices;electronic commerce532the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. performing unique edi and electronic commercespecific document translations; linking to existing billing and payment systems; and linking to banks' accounts receivable/accounts payable services.electronic commerce building blocks: objects and object classesin addition to the services cited above, the activities and functions of electronic commerce require certainbasic building blocks: unstructured information (reports and freeform text, voice, and video); structured information (edi messages, electronic forms, contracts and contract rules, design specifications); accounts, account databases, and accounting rules; transactions; records; agents and brokers (information filters, translators, trusted third parties); objects for sale (movies/videos, software objects, contracts, information, documents); and decision support models and simulations.over time, these items will probably become increasingly comprehensive and refined.these building blocks can be best described as classes of digital objects. a digital object is an orderedsequence of bits associated with a handle, or unique identification, that can represent a collection of operations(behaviors) and information structures (attributes); and where an object class represents a collection of objectsthat share a common set of attributes and behaviors. a digital object can be composed of one or more of theseclasses; for example, an email object has both structured and unstructured information. digital objects areparticularly useful because they are associated with real objects (e.g., a contract making them easy to understand)and because they can be specified and accessed in an applicationindependent manner, making them easy tocreate, reuse, enhance, modify, and replace, and to interface with existing objects, with minimal side effects.electronic commerce activities can be specified in terms of the interactions between real objects (e.g.,transacting parties) and digital objects (e.g., electronic documents, software agents). an electronic commercearchitecture can be defined in terms of how these object classes are defined (e.g., their attributes and behaviors)and how the objects interact with one another. an electronic commerce transaction can also be implemented asan interacting network of these objects, where each object can be dynamically selected as a function of thespecific situation.electronic commerce digital objects have several important properties that are discussed below.general properties of digital objectsseveral operations and controls can be associated with any electronic commerce digital object. exchange operations. examples of permissible exchange operations include being bought, sold, andtransferred (in whole or part). exchange operations encompass a variety of transport mechanisms, includingboth existing mechanisms such as email attachments and ftps, and new and evolving mechanisms such asencasing the object inside a digital envelope (thus making the object opaque to everyone except the intendedrecipient when he or she opens the envelope). security operations. examples of security operations include making the digital object secure andconfidential (e.g., encrypted), annotating and signing the object (e.g., with digital signatures), and making ittamperproof and/or tamperevident.electronic commerce533the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. ownership and authentication controls. examples of ownership and authentication controls include ensuringthe digital object's integrity (that is, showing it to be whole, complete, and in good order); date/timestamping it; ascertaining copyright status; and linking to owners and collaborators, including evidence of theobject's source or proof of origin. usage tools and controls. examples of usage control include allowing the digital object to be created;published; displayed or read; written to and updated; and reproduced and copied (note, however, that forsome object classes, it may be desirable to inhibit copying), subject to various restrictions and charges.these controls can restrict use to particular authorized users and with selective access criteria specifyingtype of use (e.g., read only). usage controls also include such operations as enforcing intellectual propertyusage rights and charges, versioncontrol and backup, change control, and group sharing (e.g., collaborativeauthoring). objects should be able to be compressed, decompressed, and manipulated in ways appropriate totheir format (e.g., images can be rotated, enhanced, have features extracted and/or matched, enlarged andreduced in size; or video and sound can be skimmed and/or played in fast time or slow motion).some important electronic commerce digital objectsseveral key digital objects for electronic commerce are listed below.contractsexamples of contracts include the following: orders, checks, bills, loan agreements, treasury bills and bonds, letters of credit, account agreements, receipts, and electronic money/electronic checks/electronic tokens.contracts can include instructions regarding the handling, routing, storing, scheduling, and workflow of thecontract itself and of other objects contained in or referenced by the contract. these instructions can addressliabilities; acceptable forms of payment (cash, credit card, debit, or check); terms of payment (usage charges,periodic and onetime charges); billing and payment instructions (credit to merchant, automatic debit carddeductions, billing and payment addresses, due date); delivery instructions (where and how to deliver); returnpolicies; methods of error and dispute resolution; and conditions of good delivery. contracts can be negotiated,including prices, terms of payment, penalties, necessary documentation, credit checks or required insurance, andcollateral or margin. they can be written, signed, read, and amended. in many instances, contracts can also bebought, sold, and exchanged. in many cases (for example, in the case of electronic cash), contracts should not beable to be altered, reproduced, or copied.information documentsexamples of information documents include the following: balance sheets; income statements;electronic commerce534the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. official statements; monthly billing statements; stock offerings; credit history; reports; discharge summary; physician orders; process notes; electronic books; movies/videos; and video games.information documents can be unstructured, partially structured, or completely structured. they can bebrowsed or searched, and bought, sold, exchanged, and copied, under contractual constraints. they also can becreated, updated, signed, copyrighted, and read, then synchronized, morphed, compressed, and decompressed.accountsaccounts include the following information: user (name, identification, authorizations, preferences, and other profile information); address; user profile (e.g., likes, dislikes, personal secrets/information); outstandings; credits and debits; balances; tax computations; receivables and credits; payables and debits; and limits (e.g., credit limits).accounts can be opened, closed, linked, updated, blocked, stopped, or attached. they can receive deposits,debit withdrawals, and accept transfers. also, accounts and account information suchas account balances can beverified. since linked transactions (for example, billing, paying, receipt, and delivery transactions) are notgenerally simultaneous or one to one, it is often necessary to reconcile account information. the ability to linkand associate account and remittance objects to payment transactions helps simplify account reconciliation.account operations are accomplished through transactions, which are discussed later in this section. it isgenerally necessary to establish audit trails, so that the consequences of multiple transactions on an account canbe tracked.software agentssoftware agents include the following: facilitators, who provide key infrastructure services such as translation and communication/network securitymanagement and control; information filters and interpreters; translators; trusted third parties and witnesses;electronic commerce535the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. escrow agents; brokers; workflow managers; fiduciaries; expert advisors; and negotiators.an agent should be able to serve in more than one of these roles.objects for sale or exchangeboth physical objects (e.g., cars and clothing) and digital objects (e.g., program logic, digital movies, data,electronic design specifications, electronic contracts) can be sold and exchanged.transactionstransactions are generally governed by contracts and update accounts. they can operate on all the otherdigital objects and generally involve the transmission and exchange of two or more digital objects (e.g., a moviefor money, medical services for money, exchange of two currencies, etc.). they can also include the exchange ofbills and invoices and of information and services.transactions can be designed to be anonymous and untraceable or traceable and auditable. if they aredesigned to be untraceable, they lose many of their information features. a more satisfying compromise is toexecute a transaction that can only be traced with the consent, approval, and active cooperation of the user.transactions can, but do not necessarily always, include the following information: who is involved in the transaction; what is being transacted; the purpose of the transaction; the destination for payment and delivery; the transaction time frame; permissible operations; date/time stamps; and special information, such as unique identification, transaction path, sequence information, receipts andacknowledgments, links to other transactions, identification of money transferred outside nationalboundaries, certificates of authenticity, and the like.transactions can be reversed, repaired, disputed, monitored, logged/recorded, audited, and/or reconciledand linked (e.g., matched and associated) with other transactions. if the transacting parties want to make thetransaction anonymous or untraceable, then the users will forego many of the above features. a compromise is atransaction that can only be traced with the consent, approval, and active cooperation of the user or designatedescrow agents.apis: infrastructure standardsapis and information exchange protocols are needed for digital object operations and infrastructureservices. many apisšfor example, ftp, http, simple mail transport protocol (smtp), and multimediainformation exchange (mime)šalready exist and could be considered as a starting point for the nii. additionalapis specifically needed as interfaces between electronic commerce objects and infrastructure services includethe following:electronic commerce536the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. apis that enable twoway exchange of data between electronic forms and database records into databases,including calling and using translator programs (these apis would allow the automatic fillin of electronicforms from databases, and the update of database records from electronic forms); apis, where possible, complying to a plugin/plugout model that enables embedding and transmission ofelectronic forms and documents into email messages, automatic conversion of these email messages intotheir original format at the receiver site, and subsequent processing of the forms/documents; apis that allow data from an electronic form or document to be transmitted from sender to receiver as adatabase record update or file transfer via ftp; apis between translators that can translate electronic forms into database commands and/or electroniccommerce remote procedure calls, and vice versa; apis that define operations such as writing, reading, certifying, authenticating, and transporting of bill andpayment objects; and apis that link electronic orders and electronic form messages with electronic payment messages andexchanges.electronic commerce537the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.60prospects and prerequisites for local telecommunicationscompetition: public policy issues for the niigail garfield schwartz and paul e. cainteleport communications groupthe national information infrastructure (nii) vision embraces three communications components (exclusiveof information libraries and other content repositories): internodal networks, access nodes, and enduser access tothe nodes. internodal networks and access nodes are currently subject to competitive supply. enduser access isthe only nii component that remains monopolized virtually everywhere in the united states. such access iscostly to provide. costs of new or improved enduser access facilities must be justified by usage in a twowaymode, in an environment that will probably evidence substantial price elasticity. nevertheless, many risktakingaccess providers are preparing the technologies with which to offer economic, reliable enduser access, confidentthat a strong market will develop. still, the inplace, monopolyprovided, enduser access seems to be useful fora great many consumers. increased consumer demand for the greater capacity and speed of digital technologieswithin the nii will be conditioned largely by the behavior of public policy makers visàvis the entrenchedmonopolies. in this paper we discuss the opportunities for and obstacles to replacement of the monopoly for enduser access.competition in the last segment of the telecommunications industryit has long been recognized by economists that a monopoly has less incentive to innovate than does a firmin a competitive market. firms in a competitive market will seek to gain a competitive advantage over theirrivals through innovation and differentiation, thus enhancing consumer surplus as well as producer surplus.events in the longdistance and equipment segments of the telecommunications industry seem to have confirmedthe result predicted by economic theory. in the interlata long distance market, at&t's market share hasdropped from over 80 percent to less than 60 percent since 1984, and the number of carriers has risen to over4001. the facilitiesbased long distance carriers increased their deployment of fiberoptic transmission facilitiesfivefold between 1985 and 19932. longdistance rates have fallen more than 60 percent3. similar gains havebeen realized in the demonopolized market for telecommunications equipment. now handsets can be purchasedfor less than the price of a pair of movie tickets, and businesses and consumers are linking ever moresophisticated equipment (computer modems, pbxs, cellular phones, and handsets that have computer memory)to the public switchedtelephone network.theoretically, the same benefits would accrue to consumers from competition in the remainingmonopolized local exchange market as have been realized in the equipment and longdistance market segments(efficiency, diversity, innovation, and price reductions). but the grip of governmentsanctioned monopolyremains powerful. only slowly has it been understood that local exchange monopolies may temporarily drivedown costs but never as far as would competition: they may innovate, but not as swiftly as competitors would;they may improve service quality, but not as readily or effectively as they would under competition. the fullbenefits of the nii require efficient exploitation of all telecommunications technologies, including photonics,coaxial cable, fiber optics, wireless, and even the existing twisted pair technologies, linking subscribers to oneanother via a ''networkprospects and prerequisites for local telecommunications competition: public policy issuesfor the nii538the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.of networks." a competitive market for local exchange services is essential to such efficient exploitation of alltechnologies.moving from theoretical concepts to the practical reality of creating local exchange competition requirescertainty and flexibility: certainty to inspire investor confidence, and flexibility to respond to constantly changingšand largely unknownšmarket realities. achieving balance between certainty and flexibility means opening upexisting monopolized market segments as much as possible without threatening the longterm stability ofcompetition itself. both industry and public policymakers must take this challenge seriously if the nii promise isto be realized by 2000šor ever.the current status of competition in local telecommunications servicesrevenue for the local telecommunications market in 1993 exceeded $88 billion4. of that total, 99 percentwas captured by mature, traditional local telecommunications carriers. the 1 percent of the market liberated bynew entrants was composed almost entirely of private line and special access services; they have not yet begun toreach a mass market5.still, competitive access providers are optimistic about the future of competition, building or operatingnetworks in some 50 or 60 metropolitan statistical areas. a recent report by the federal communicationscommission indicates that over 10,070,308 miles of fiber has been deployed in the united states by local, longdistance, and urban telecommunications providers, and shows that the rate of deployment by competitive accessproviders far exceeds that of the incumbent local exchange carriers6. the largest competitive access provider,teleport communications group (tcg), has installed sonetbased, selfhealing, twoway ring networkscapable of transmitting information at a rate of 2.4 gigabits per second. isdn is provided over these networks,and tcg offers frame relay at up to 1.5 megabits per second and atm (switched data) service at up to 155megabits per second. with more than 167,314 miles of fiber throughout 22 networks and a strong switchdeployment program, tcg is technically positioned to serve larger markets7.cable television operators, whose networks now pass 97 percent of the nation's households and providetelevision service to more than 65 percent, are the obvious "other" enduser access provider8. cable companiesare upgrading their distribution plant and must provide for switching to offer local exchange services.experiments, such as the motorolatcitcg trial of residential service using radio frequencies over fiberopticcoaxial cable, will identify technological requirements. the promise of wireless subscriber loops has also drawnconsiderable interest from a wide range of industry participants. bidders in the fcc's recently concludedpersonal communications service (pcs) spectrum auctions committed more than $7 billion for licenses to buildand operate local telecommunications networks9. a recent merrill lynch report predicts that one of thesuccessful pcs bidders would have a 5 to 8 percent penetration of the local exchange market by 200410. there isno shortage of potential entrants seeking to provide local exchange services. meeting consumer demand willmean the employment of a variety of distribution systems, both broadband and narrowband, wireline and wireless.however, it is not yet certain what customers want and when they want it. though studies such as a recentdelphi study of industry executives and academics have projected 2010 as the outside year in which a massconsumer market will exist for networkbased interactive multimedia products and services provided overswitched broadband, market research is notably thin on the subject of what people will pay for "infotainment" orhousehold management services11. as with the case of previous telephone, television, computer, and audioproducts, much investment rides on the premise that supply will create demand.despite the limited deployment of broadband networks, consumers have not been especially hindered intheir attempts to establish themselves as providers, as well as users, of information. internet platforms and aplethora of electronic bulletin boards allow consumers to "publish" information available on demand by otherconsumers. aside from providing the conduit over which the information travels, the network operator has norole in the content of the traffic speeding over its lines. the development of local network competition will onlyhasten the development of more information services and more gateways through which consumers can shareinformation.prospects and prerequisites for local telecommunications competition: public policy issuesfor the nii539the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the prospects for local competition, therefore, depend in part on the perfection of new technologies, butmore importantly on reduction of their cost. however, factors other than technology itself pose greater obstaclesto deployment of competitive choices and must be dealt with expeditiously if investment is to accelerate. beforethe various broadband and other enduser access technologies can be widely deployed, legal, technical, andeconomic hurdles must be overcome in every state. furthermore, even though technical interconnection andinteroperability of networks have been shown to be feasible without serious difficulties, many aspects of theseamless interoperability of competitive networks remain to be resolved. among them are a number ofportability and central office interconnection arrangements. finally, economic interconnection of competinglocal networks has to be achieved so that new entrants can develop their own services and prices that maximizethe revenues from their own networks.legality of competitionregulatory barriers remain the threshold barrier to entry into local telecommunications services. only sevenstates have authorized or permit competitive local exchange carriers to provide service: new york, illinois,maryland, massachusetts, washington, michigan, and connecticut. making competition legal means removing aseries of entry barriers imposed by different government agencies whose interests are not necessarily congruent.the primary issues that must be resolved include exclusive franchises, qualifications for entry, franchise fees,and access to rightsofway and to buildings. market entry that is conditioned upon demonstrating that theincumbent carrier is providing inadequate service is an unreasonable burden, as are franchise fees or otherpayments to local government as a condition of operating, if the incumbent does not pay equivalent fees. but themost difficult ongoing issues will be the access issues, because that is the area where the incumbent monopolyhas the ability and the incentive to encourage unequal access. incumbent lecs enjoy generally unlimited accessto public rightofway and often control the rightsofway needed by entrants. they also have established accessto building entrance facilities, at no cost. new entrants must have access to those rightsofway at the same ratesand on the same terms and conditions as the incumbent.in 1995, 13 states enacted legislation removing barriers to entry and endorsing local exchange competition.federal legislation preempting state entry barriers and setting guidelines for local competition is again underconsideration. the technical and economic aspects of network interoperability are rising to the forefront ofpublic policy issues that will condition nii development.technical feasibilityat the end of the nineteenth century and continuing into the early years of this century, local exchange"competition" did thrive. but rather than a "network of networks," consumers faced a tangle of discretenetworks, all of them talking at once but none of them talking to each other. to be certain that they could reacheveryone with a phone, customers had to subscribe to the local exchange service of every provider.as that experiencešwhich gave way to governmentsanctioned monopolyšdemonstrated, the key to anefficient and effective "network of networks" is interconnection. adjacent (i.e., "noncompeting") carriersinterconnect seamlessly with each other today and have done so for more than 80 years. now competing localexchange networks must be able to interconnect with the incumbent local exchange network on the same termsand conditions as the incumbent interconnects with adjacent carriers and with itself. at a minimum, the technicalinterconnection arrangements should include the following.central office interconnection arrangementscentral office (co) interconnection arrangements are the physical facilities that connect a competitor'snetwork to the local exchange carriers' (lec) network. an efficient costbased co interconnection arrangementconsists of three elements:prospects and prerequisites for local telecommunications competition: public policy issuesfor the nii540the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. interconnection electronics. optical and electronic equipment physically located in the lec co, butremotely operated and controlled by the local competitor, used to multiplex, demultiplex, monitor, test, andcontrol transmission facilities; interconnection cable. fiber optic cables, dedicated to the local competitor's use, interconnecting the localcompetitor's location and the interconnection electronics located in the lec co(s); and interconnected services. lecprovided services and facilities interconnected to the interconnection cableand electronics at the co at costbased rates.connections to unbundled network elementsalthough the longterm nii vision presumes that existing copper pair will be replaced (or upgraded) bybroadband or at least enhanced narrowband access, a transition period during which local exchange competitorsare maximizing their return on other network investment (in internodal transport and access nodes) will occur.these competitors will be able to offer service to a mass market during the interim period only by resellingexisting subscriber loops under monopoly control. currently, whether providing singleline basic local exchangeservice or multiline local services such as centrex, lecs usually bundle together the "links" and the "ports"components of the loop. lecs must unbundle the local loop elements into links and ports, individually pricingeach element at costbased rates.seamless integration into lec interoffice networks and seamless integration into lec signalingnetworksa lec's tandem switching offices and end offices are interconnected via the lec's interoffice transmissionnetwork. adjacent lec switching offices are also interconnected via the same network for the distribution ofintralata traffic. routing through the interoffice network is governed by a local exchange routing guide(lerg) unique to each lata. the lerg prescribes routing of all traffic between and among all lec andinterexchange carrier (ixc) switching offices. costbased interconnection arrangements at tandem offices aswell as end offices are necessary, and the class 4 and 5 switches of local competitors must be incorporated intothe lerg on the same basis as (and with the same status as) the lec's end and tandem offices. this will enablea competing local exchange carrier to efficiently and effectively route calls originated by customers on itsnetwork to the lec for final termination at the premises of a lec customer, and vice versa. this includes accessto signaling system 7 (ss7) with a line information database (lidb). in an ss7/lidb environment, routing,translation, service, and account information is stored in centralized databases that lec and ixc switches canaccess through service control points (scps). local competitors must be able to interconnect and query the lecss7 databases at the scps in a nondiscriminatory manner and under equitable terms and conditions.equal status in and control over network databasescompetitive local service providers must be allowed to have their customers' telephone numbers included intelephone directories, directory assistance, lidb, advanced intelligent network (ain), 800number, and otherdatabases. their access to such resources must be equal in price, functionality, and quality to those of incumbentlocal telephone providers.prospects and prerequisites for local telecommunications competition: public policy issuesfor the nii541the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.equal rights to and control over number resourcesnumbering policy must be broadly developed and administered in a competitively neutral manner. theadministration of area codes provides an excellent example of the problems that can arise under lecadministered code assignments. in recent years, as competition has developed, the incumbent lecs haveproposed a deviation from their historic practice of assigning customers to a new area code according togeography when the existing area code numbers were approaching depletion. that is, customers in one part ofthe existing area code territory would retain their area code, while customers in an adjacent area would beassigned a new area code. the new approach favored by some local exchange carriers is to assign a different areacode to customers of the new providers (wireless and competitive access providers), allowing only the lec'scustomers to remain undisturbed by the new area code assignments. the anticompetitive effects of such a planare not hard to imagine: customers of the new carriers are difficult to locate, and only customers of the newentrants must incur the expense of changing their numbers (e.g., letterhead, business cards, advertising). suchexpenses could be a significant deterrent to a customer who might have found it otherwise economical to switchto a new entrant provider.local telephone number portabilitythe ability of customers to change service providers and to retain the same local telephone number at thesame location (service provider number portability), without having to dial extra digits and without beingburdened by "special" actions, is a critical component of the economic viability of local exchange competition.interim number portability mechanisms (such as remote call forwarding) are an inferior form of numberportability that impairs a new market entrant's service, and such impairment should be reflected ininterconnection charges.cooperative practices and procedureslocal exchange telephone companies maintain a detailed set of administrative, engineering, and operationalpractices and procedures. these coordination practices must extend to new local competitors if competition is toemerge. in addition, the traditional local telephone companies maintain a detailed set of practices governingrevenue pooling, intercompany settlements, and other financial interactions between and among established localcarriers. these also must be extended to local competitors. finally, any other intercompany notificationprocedures, standardssetting procedures, and the like, through which lecs may now or in the future interactwith one another, must be extended to include local competitors. these requirements may appear reasonable, butthe task of integrating such behaviors into huge corporations with many employees and a monopoly mentalitywill be daunting.economic viabilitythe ultimate determinants of the amount and extent of local exchange competition will be neither legal nortechnical, but economic. the ability of competitors to meet consumer demand depends ultimately on economicinterconnection arrangements for competing local networks. the new entrant must install capacity toaccommodate peakhour traffic, even though initially it has little inbound traffic. arrangements for reciprocalcompensation for the mutual exchange of local traffic should allow both carriers to recover the incremental costof capacity. this cost is low: about 0.2 cents per minute, on an average basis12. given the trivial cost ofsupplying incremental capacity, it makes practical sense to implement a "senderkeepall" arrangement, like thatnow used by internet providers, rather than to impose explicit charges on terminating carriers. senderkeepall isalso administratively simple.prospects and prerequisites for local telecommunications competition: public policy issuesfor the nii542the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.such arrangements should minimally be in place until databased number portability is established. thelack of number portability and the substitution for it of measures such as remote call forwarding impair the newentrant's ability to use its network to its full capabilities and to sign up customers. remote call forwarding andother socalled interim number portability measures entrench the incumbent's position and give it an economicadvantage because all calls are routed through the existing lec's network. under current regulation, this meansthat the lec as intermediary could collect terminating access charges from interexchange carriers for callsdestined to a customer on a competing local exchange carrier's network. although arrangements to overcomethis economic inequity might be made, they would be complex and costly. it is far more efficient to plan for andexpeditiously implement a longterm solution to the number portability problem. this must be a databasedsolution that does not put any telecommunications service provider at a disadvantage and allows all providers tomaximize the utility of their networks.universal servicethe longheld policy objective of universal service must be addressed before competitors can competeeffectively throughout mass markets for local telecommunications services. this will make it possible forefficient competition to eliminate uneconomic or noncompetitive subsidies embedded in telecommunicationspricing structures over a reasonable transition period. this is an expected result of competition. incumbent lecsargue that retail rates (especially for residential consumers) may be priced substantially below costs and will besubject to upward price pressure. but again, as distribution networks achieve economies of scale, incrementalcosts are expected to fall, mitigating such price pressures.nevertheless, social policy pressures to maintain average prices for similar offerings across the country andbetween urban and rural subscribers will constrain providers' pricing practices for a transition period of at least10 yearsšuntil competitive facilitiesbased distribution networks reach a substantial portion (say 30 to 40percent) of rural subscribers. during this period (and for certain communities, perhaps permanently) universalservice must be assured. the ability to obtain access to the public switched network voicegrade service withtone dialing on a singleparty line, at affordable rates, together with operator assistance, directory listings, andemergency service, is the minimal level of residential service and may require subsidies for some customers. ifpublic policymakers seek to mandate a higher level of service, such as broadband access, the costs will soar;currently there seems to be a political consensus that market forces should define what "basic" service requiressubsidy.subsidies to preserve universal service must be explicitly identified and administered through a providerneutral fund so as to minimize their cost and maximize their efficiency. all telecommunications serviceproviders should contribute to the subsidy fund based on their share of the market. subsidies should be portableamong carriers; that is, all local exchange carriers must have the opportunity to receive a subsidy when selectedas a service provider by a subsidized customer.prospects for local telecommunications competitionthe prospects for local competition are improving. in 1995, 13 states enacted legislation making localexchange competition possible, and most of these laws directed state regulators to provide for the requisitetechnical and financial interoperability and equal access. in nine other states, the critical issue of reciprocalcompensation for the mutual exchange of local traffic is being addressed and interim agreements have beenconcluded between new entrants and lecs. up to 15 additional states will soon develop rules for local exchangecompetition. federal legislation setting the standard for local exchange competition and giving all major industrygroups something of what they want in a competitive environment has passed the senate; many observers give itat least a 50 percent chance of becoming law in 1995. three trials designed to assess the technical requirementsfor telecommunications number portability are under way. and activity in the courts has sharply accelerated asparties seek to end existing legislative or judicial curbs on their ability to address each other's markets.prospects and prerequisites for local telecommunications competition: public policy issuesfor the nii543the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.among the restraints under attack are the lineofbusiness restrictions on the bell operating companies(bocs) entering the interexchange market or manufacturing, included in the consent decree that divested thebocs from at&t. these restrictions were intended to endure so long as the divested regional bocs (rbocs)retain control of the local exchange bottleneck, which gives them the ability and incentive to impede competitionin adjacent markets. the "viiic" test applied by the court overseeing the decree requires the u.s. department ofjustice to report that there is "no substantial possibility" that a boc seeking relief from the decree would be ableto impede competition in the adjacent, restricted market. some members of congress seek through legislation toabrogate the decree entirely, eliminating the role of the doj, without evidence that the local exchange bottleneckhas actually been broken. others would substitute a more liberal testšthe "public interest" testšfor viiic andmake the fcc rather than the doj and the court responsible for its application. still others would have the dojinvolved in an advisory capacity. and some, like the clinton administration, prefer to retain the justicedepartment's role 13.several bocs are seeking relief from the decree directly from the court. on april 3, 1995, the doj filed amotion to permit a trial supervised by the doj and the court in which ameritech could provide interexchangeservice after the development of actual competition, including facilitiesbased competition, and substantialopportunities for additional competition in local exchange service 14.the questions of how much and what type of competition warrants relief will be the key to whether or notdurable and robust local exchange competition is possible. even if all the "paper" requirements for localexchange competition are met, the incumbents can and do manipulate each and every aspect of access to theiressential facilities. as bocs have demonstrated frequently, they can frustrate and thwart competitors innumerous ways: by imposing different and unnecessary physical interconnection requirements for competitors;by engaging in pricing discrimination, tying arrangements, and selective cost deaveraging; by imposingexcessive liabilities on customers who wish to change to a competitive carrier; by engaging in sales agencyagreements that effectively foreclose markets to new entrants; and more. as a result, a public policy that seeksthe full benefits of competition for the greatest number of telecommunications customers must includesafeguards that constrain anticompetitive behavior and sanction the parties having market power that engage init. a rigorous effective competition test prior to boc relief is such a safeguard.the other required safeguard is continued regulatory oversight of the firms having market power, withlessening intervention as that market power diminishes. during the development of interexchange marketcompetition, at&t, the dominant interexchange carrier, endured economic regulation while its competitorswere free to price their services to the market. a similar approach is necessary in the local exchange market.incumbents may be granted pricing flexibility, so long as they do not reduce prices of competitive services belowtheir incremental cost; but cross subsidies from captive customers must be prevented. new entrants must faceminimal regulatory burdens because, lacking market power, they cannot harm the public interest. however, newentrants must contribute to the maintenance of universal service.conclusionby any measure, local competition does not exist today, but a more competitive environment is beingcreated. fullscale competition will require huge investments, which in turn calls for relative certainty as to thepace and outcomes of regulatory and legislative actions. if new entrants have technological innovations toexploit, their ability to complete costeffective deployment depends on their relative financial strength visàvisthe incumbent monopolies. the latter often can manipulate financial faucets by means of their negotiatedagreements with state regulators, whereas competitors depend entirely on risk markets for their capital.competitors are engaged in a historic process on the legislative, regulatory, and judicial fronts. there is nodoubt that the barriers to entry will fall. the safeguards to control residual market power as localtelecommunications competition emerges will be lodged in different government agencies, as they currently are,but the guidelines will become clearer as the benefits of competition become more obvious and more widespread.the final determinant of the pace at which these policy adjustments occur is consumer demand. demandwill surge when "infotainment" is varied and priced to mass market pocketbooks.prospects and prerequisites for local telecommunications competition: public policy issuesfor the nii544the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.notes1. industry analysis division, common carrier bureau, federal communications commission. 1995. "long distance market shares:fourth quarter 1994," federal communications commission, april 7.2. industry analysis division, common carrier bureau, federal communications commission. 1994. "fiber deployment update: endof year 1993," federal communications commission, may.3. "fcc monitoring report, table 5.5." cc docket no. 87339, june 1995.4. federal communications commission. 1994. statistics of communications common carriers.5. connecticut research. 1993. "1993 local telecommunications competition – the 'alt report.'" connecticut research,glastonbury, conn.6. industry analysis division, common carrier bureau, federal communications commission. 1994. "fiber deployment update: endof year 1993," federal communications commission, may.7. it has been announced that tcg will be contributing to a new venture of sprint, telecommunications, inc., comcast corporation,and cox cable that will package local telephone, longdistance, and personal communications with cable services into a single offeringfor residential and business consumers.8. national cable television association. 1993. "building a competitive local telecommunications marketplace," national cabletelevision association position paper, october.9. federal communications commission bulletin board, march 13, 1995. wireless co., a consortium formed by cable televisionoperators tci, cox, and comcast, and long distance carrier sprint, successfully bid over $2 billion for licenses in 29 markets.10. merrill lynch & co. 1995. "the economics of the sprint/cable alliance," merrill lynch & co., february 10.11. kraemer, joseph s., and dwight allen. n.d. "perspectives on the convergence of communications, information, retailing, andentertainment: speeding toward the interactive multimedia age," deloitte touche tohmatsu international, p. 13.12. brock, gerald. 1995. "incremental cost of local usage," prepared for cox enterprises, march, p. 2.13. see s. 652, "telecommunications competition act of 1995"; h.r. 1555 discussion draft, may 2, 1995, "communications act of1995"; and h.r. 1528, "antitrust consent decree reform act of 1995."14. united states of america v. western electric company, inc. et al., and american telephone & telegraph company, civil actionno. 820192.prospects and prerequisites for local telecommunications competition: public policy issuesfor the nii545the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.61the awakening 3.0: pcs, tsbs, or dtmftvšwhichtelecomputer architecture is right for the next generation'spublic network?john w. thompson, jr.gnostech incorporatedabstractuniversal, equitable, and affordable access to the national information infrastructure (nii) is achievableover the next 5 to 7 years. however, our switched and broadcast telecommunications industries have not beengiven the national goal and task of pursuing the network architecture and technologies that can provide suchaccess to interactive media public utility services.at present, these communications industries are pursuing variations of the personal computer and tv settop box network architecture models. these public network architectures are inappropriate and have economichandicaps that will cause them to fail in the provision of universal access and service for the american public.however, there is at least one network architecture modelšdtmftvšthat does appear capable of deliveringon the nii promise. this "architecture," based on the existing and universally available telephone and ntscvideo networks, can provide universal and affordable access to the internet, the library of congress, and anyother source of entertainment and knowledge1. however, the nii needs an encouraging, supporting, andregulating public policy. this public policy should provide for the nationwide common carriage of realtimeaddressable video to the home, equal access to cable tv head ends for interactive multimedia providers, and theappropriate video dialtone rules.statement of the problemthe national information infrastructure (nii) is totally dependent on the proper convergence of our publicswitched telecommunications network (pstn) and mass media public broadcast network (pbn) into anationwide telecomputer system. two predominant network architecture models and one emerging model arecompeting to be "the" information superhighway. the interactive media network architecture deployed by ournation's cable and telephone companies will have a significant impact on whether the general public will begin tohave equal, affordable, and universal access to the nii by 2000. over the next 5 to 7 years we can expect theinteractive multimedia industry to continue to improve pc technology and program content. however, the modelthat posits a pc in every home, connected to an online network of "pc" telecomputer systems, will fail toprovide universal access to the nii because of the economic burden of pc hardware and software purchase andmaintenance, and their constant obsolescence. similarly, today's cable and telco interactive tv trials, using thetv settop box telecomputer system model, will also fail to provide universal access because of the sameeconomic burdens of their underlying telecomputer network architecture. unless the pstn and pbn industrieschange their current architectural focus, we will continue down paths that will lead to greater division betweenour nation's information haves and havenots.to best serve the public with interactive media services will require a fully integrated system of switchedand broadcasted telecommunications common carriers. considering the asymmetrical nature of interactivemultimedia networks, the switching technology and broadband distribution media are already in place tointegrate these information superhighways in a way that is economical for universal access2. unfortunately,these key nii elements are owned primarily by two competing corporate entities within any given geographiclocality. eventhe awakening 3.0: pcs, tsbs, or dtmftvšwhich telecomputer architecture is right for thenext generation's public network?546the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the aborted bell atlantictic megamerger reveals the economic shortcomings of integrating the broadbandloops of cable tv with the switches and backoffice operations of telcos when they are not located within thesame geographic locality. as a result, these telco and cable companies are struggling to add to their networkswhat the other already has in operation. for two capitalintensive industries, this economically taxingcompetition begs the public policy question: if building the information superhighway is so expensive, why, as anation, are we trying to build two of them? the problem is that public policy has not charged our pstn andpbn industries with the goal and task of creating a new and shared "public utility" system nationwide, a publictelecomputer network (ptn) providing universal and equal access to interactive media on a common carrierbasis. this is the strange attractor toward which our communications and computing industries have beenconverging since the breakup of the bell system. however, the current chaos will continue and public networkengineers will pursue inappropriate network architectures until our country proclaims such a goal with the properlaws to encourage, support, and regulate the enterprise.backgroundbefore 1993, the public policy issues surrounding the evolving nii were mostly limited to industrialinfighting. it was the telcos versus newspapers over interpretation of the mfj, broadcasters versus studios overfin/syn rules, and everyone taking opposing positions on the fcc's proposed video dialtone rules. these publicpolicy debates were, for the most part, kept within the communications industry establishment and the hallwaysand offices of congress and the fcc. then someone said "500 tv channels," and visions of the entertainmentpossibilities and public service opportunities moved from the industry trade rags to the front pages and covers ofour national consumer publications. this media attention captured the public's imagination as the now infamousinformation superhighway.since then, most of our leading telecommunications and media executives have declared themselves"infobahn" road warriors. they rushed off to announce their respective multimedia trials and megamergers todeliver telethis, cyberthat, and your own personal virtualreality xanadu. within the next 5 to 7 years, thepublic will expect the nii, as the next generation's "public network" of switched and broadcastedcommunications, to deliver on these entertainment, education, and public utility promises. this is a majorundertaking for private industry and public policymakers. to understand the network and regulatory engineeringparadigm shift that must take place, one needs to comprehend the existing and evolving public networkinfrastructure within a common context. that context is "video dialtone."the current state of play within the industry involves two predominant, and one emerging, telecomputersystem models. a telecomputer system, in the fuzzy macro context of the nii, is the mass media system that ournation's cable and telephone companies are striving to create to deliver interactive digital everything to theconsuming public. it is the underlying hardware infrastructure that will integrate telecommunications, television,computing, and publishing into a seamless national multimedia network.the older and more familiar of the predominant telecomputer models is that of the pc in every homeconnected to the internet and other packetswitched networks of computers. this is the "pc" model. althoughthis model was a dismal failure as a mass medium during the brief videotext era, it has had a recent resurgenceencouraged by flashy multimedia pcs, guis, cdroms, and the explosive worldwide growth of the internet.the champions of this model tend to be the manufacturers and sophisticated users of advanced pcs,workstations, and highspeed data networking gear. the essential nii elements that this model brings to atelecomputer architecture are those that offer the most artistic, creative, and communication freedoms to users,programmers, and publishers.the other predominant model, getting off to a dubious start, is that of the "smart" tv settop box (tsb) inevery home interfacing with a video server. this is the "tsb" telecomputing model. this model is the result ofrecent advances in microprocessor, video compression, and network transmission technologies. the championsof this model tend to be the manufacturers of cable converters, microprocessors, and midrange computers inpartnership with cable and television companies. in apparent conflict with the pc model, the essential niielements of the tsb network architecture are those necessary for responsible mass media broadcasting andthe awakening 3.0: pcs, tsbs, or dtmftvšwhich telecomputer architecture is right for thenext generation's public network?547the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.network operator control. more important for the nii, the tsb telecomputer model recognizes the public utilitypotential of an unlimited number of addressable video channels to the consumer.these two models expose most of the essential nii elements perceived by the converging industries asnecessary for the successful introduction of a new mass medium. between the two, it is possible to extract theinherent objective of a telecomputer system. the objective is to offer all consumers the potential and opportunityfor interactive access to all multimedia publications over a public network. the objective is also to provide thisaccess in a responsible and socially acceptable manner. although each of the incumbent models has technicaland philosophical advantages over the other, neither will pass the test of being economically feasible as a massmedium.taking lessons from early industry trial experiences and failures, just now emerging is a third telecomputermodel. this telecomputer model envisions the public using ordinary telephones and touchtone (dtmf)signaling over pstn networks, and using only the buttons of an ordinary telephone as an interactive tv (itv)''remote unit" to access and interact with centralized computers broadcasting userpersonalized output overordinary ntsc video channels. this telecomputer network architecture combines the best of the other twomodels in a way that can offer universal access to interactive multimedia services. the dtmftv model candeliver on the promise of common carrier interactive tv and programmer competition. whether or not thispublic utility service will be made available to the public over the next 5 to 7 years will depend on the creation ofa new video dialtone policy, a policy that will lead to fully integrated switched and broadcasted services on anequal access and common carrier basis. such an nii policy should influence the choice of an appropriatetelecomputer network architecture by our nation's cable and telephone engineers.analysis and forecastvideo dialtone(s)to use video dialtone as a common context for nii issues, we need to define it. there are three distincttypes of video dialtone networks and regulatory models. the first compares to traditional pstn services becausethat is all it is. this video dialtone is now finding its way into the marketplace as compressed videoteleconferencing and transfers of digital data to office desktops via multimedia workstations and data networkingservices. as the economics for these switched services improve, this form of video dialtone will likely find itsway into the homes of telecommuters. over the next 5 to 7 years, videophones and pc videoconferencingwindows will penetrate the home in the same way that facsimile machines and other home office productivitytools do. this form of video dialtone, vd1, is only a common carrier's analog or digital switched service offeredon demand. switched pointtopoint and bridged twoway pointtomultipoint communications, video or not, arecovered by generally accepted pstn regulations and tariffs.the second form of video dialtone originates from satellite and local tv transmitters over federallyregulated and licensed public spectrum. it also comes from cable tv head ends transmitting over locallyfranchised rightsofway. this form of "passive" video dialtone is one means of access to the consumingaudiences of the pbn. the public's tvs are now limited to receiving their principal choices for necessary,convenient, and entertaining or interesting passive video program transmissions in this form. this nonswitchedoneway, pointtomultipoint video delivery is the most efficient and economical method to distribute highquality video programming to the viewing public on a "scheduled" basis. advances in digital video compressionand fiber optic transmission technologies have led to the potential for a quantum leap in the number of broadcastvideo channels that can be delivered to the public. these developments led to the socalled 500 channels andvideoondemand bandwagons. however, this form of video dialtone, vd2, does not yet have a recognized andaccepted common carrier regulatory model. when approached from the common carrier perspective there aresome natural, yet severe, technical and network architecture limitations. these limitations relate to channel andtime slot availability and social responsibility concerns. if the nii is to include a common carrier infrastructurethat would permit any programmer or content creator equal access to america's tv audiences, the evolvingpublic network will require a virtually unlimited number of addressed video channels.the awakening 3.0: pcs, tsbs, or dtmftvšwhich telecomputer architecture is right for thenext generation's public network?548the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the third form of video dialtone is an integrated media mix of the pstn and pbn, a "mix" permittingservices with favorable economics for a national interactive multimedia medium. it is this evolving new form ofpublic network dialtone (vd1 + vd2) that is of particular interest to the nii. this form of video dialtone isvd3. it will be those "interactive and ondemand multimedia communications" services available fromnationwide ptn networks on a common carrier basis. this form of video dialtone will satisfy the promiseshyped to the public, thus far, as the information superhighway3.telco (vd1) and cable (vd2) subscribers: one and the same (vd3)a major factor affecting the evolution of the nii and the technology to be deployed for it is the need for aclearer understanding of just who are the "public" in the evolving public network. as the now separated wirelinenetwork industries converge on the fully integrated network services "attractor," the characteristics that oncedistinguished video broadcast (vd2) from switched telephony (vd1) subscribers are rapidly blurring. thisphenomenon will dramatically influence the vd3 or "interactive video dialtone'' regulatory model. do cablesubscribers want to subsidize cable's entry into telephony any more than telco subscribers want to subsidizetelco's entry into cable? from an nii public policy standpoint, these subscribers are one and the same. theyshould not be burdened with paying for a redundant infrastructure in the name of fullservice networks'competition in a "natural monopoly" environment. to further compound this issue, as the wireless industries ofovertheair, satellite, and microwave broadcasters also converge on the same "fully integrated network services"attractor over the next 5 to 7 years, the issue of "who are the public" as public utility subscribers accessing thisevolving public network will become even more blurred.over the next 3 to 5 years we can expect that the quest for spectrum efficiency on the cabled wireline side,through digital compression and multiplexing, will apply equally to the wireless overtheair side of vd2 videobroadcasting. as the broadcasters of subscription channels (e.g., hbo, showtime, cinemax) continue to exploremulticasting opportunities (e.g., hbo1/2/3, espn and espn2), one can expect the more traditional vhf anduhf networks of the pbn (i.e., cbs, abc, nbc, fox) to want access to the same commercial opportunities.this trend, however, will require the nii to set vd2 standards for compressed audio and video broadcasting inorder to encourage a market for the associated consumer electronics (i.e., wireline and wireless broadcastreceivers) and other addressable customer premises equipment (cpe). these standards may be based on aversion of the motion pictures encoding group (mpeg) standard, the asymmetrical digital subscriber line(adsl) standard, or some future ntsc/fcc or industry digital standard. this trend of subscriber evolution iseven more apparent when one considers the eventual societal ramifications of video on demand (vod), nearvod (nvod), and other ondemand "audience targeting" (e.g., billing, screening, selectivity, transactional)functionality. the fundamental nii element at issue here is broadband household "addressability" in the massmedia environment of broadcast video networks (vd2). beginning with the addition of the ever increasingnumbers of payperview (ppv) channels that will eventually constitute nvod, over the next 3 to 5 years cabledsystems will continue to expand the commercial opportunities associated with vd2 addressability. it is thisaddressability element and the efficiency of electronic distribution that will eventually attract the direct mail andadvertising industry into the interactive multimedia (analog and digital) convergence. also attracted will be thecatalog shopping industry as a natural evolution of the switched telephone (i.e., 800number service, vd1) andbroadcasted ntsc video (i.e., vhf, uhf, catv, vd2) tv home shopping industry. as the electronicpublishing industry (i.e., audio, video, and multimedia programmers) converges on a single and fully integratedvd3 or (vd1 + vd2) communications network, the concept of "the subscriber" will evolve from one of beingeither a vd1 (i.e., telco) or vd2 (i.e., catv) subscriber to that of being a vd3 subscriber more closelyresembling an a la carte magazine subscriber in a "common carrier" postal distribution system.the awakening 3.0: pcs, tsbs, or dtmftvšwhich telecomputer architecture is right for thenext generation's public network?549the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.i have an address – therefore i amthe bottom line is that the regulatory model for the nii must acknowledge the emergence of a new publicutility network system that will eventually, among other things, enhance if not replace our national postal systemand local systems of public and private libraries, bookstores, movie theaters, and video rental outlets. the nii isa public network that carries broadcasted (vd2) and switched (vd1) information and entertainment (i.e.,electronic communications and publishing) to the "addressable households" of vd3 (i.e., nii) subscribers.utility potential comes from controlled flowassuming a proper interactive video dialtone (vd3) regulatory model for the encouragement of an niiwith public utility potential, the services for these subscribers will have to come from someone. that someonewill be those corporate and commercial entities investing in the necessary hardware, software, and "wetware"(i.e., r&d and other creative talents) for some given and recognized, regulated and unregulated, return oninvestment(s). this brings us back to ptn architecture and the technology deployment issues. the ptnhardware/software, regardless of architecture model (i.e., pc, tsb, dtmftv), for delivering and controllingthe public utility potential of an information "flow" consists of the following elements: "pipes" (e.g., ds0, isdn, sonet, ntsc, and other transmission standards); "valves" (e.g., dxc, atm, uspsšzip code, bellcore administeredšnpašaddressing plans; and otherhousehold routing and switching standards); and "reservoirs" of information (e.g., databases, film libraries, the internet, the library of congress).one must understand that the metaphor of an information superhighway (vd1) is only half of the biggerpublic works picture. the nii (i.e., vd3) is also an information superpower utility project (i.e., water, gas,electric light, sewage, vd2).although their roles are still to be determined, these corporate entities (i.e., private sector investors) for thenext decade or two will nevertheless consist of variations and combinations of lecs, ixcs, msos, broadcasters,publishers, and other agents. their highlevel ptn architecture will consist of the following elements: "dams" (i.e., technical, economic, regulatory, legislative, and licensing parameters) to channel the flow; "floodgates" (i.e., lec cos, mso head ends, and ixc pops) to regulate, meter, and bill for the flow; and "generators" (i.e., networked computers) to serve the public with information and entertainment services.the bottom line is that the dams and floodgates are ptn architectural "barriers and bottlenecks" that willcontrol the flow of information and entertainment to the public. consequently, the nii will need to provide fordams with equalaccess floodgates and pipes.latas, msas, rsas, mtas, and btas: the issue of territoryattracting the investment necessary to build this new public network, as in any public utility infrastructureproject, will likely require grants of clearly defined market territories to licensed operators. the more exclusivethe legal right to exclude nonlicensed competition in a given vd3 subscriber serving area (ssa), the moreattractive these pioneering ptn operators will become to longterm private sector investors. hence, exclusivitywill drive the private sector's funding of the nii.the awakening 3.0: pcs, tsbs, or dtmftvšwhich telecomputer architecture is right for thenext generation's public network?550the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.recommendationsset a national goalour former bell system had a simple but broad goal: a phone in every home that wanted one. since thecommunications act of 1934, this natural monopoly public network was tasked with providing the public utilityinfrastructure necessary to permit any person in the united states to communicate with any other person in anaffordable way. that goal, for its time, was enormously ambitious; but everyone, from planner to engineer,lineman to operator, clerk to executive, public consumer to regulator, intuitively understood it. it took half acentury of dedicated work by telephone pioneers to achieve that goal of universal service.the public sector should set a similar goal that will challenge the private sector to create a ubiquitousinteractive multimedia common carrier system to serve the next two to four generations of americans. that goalshould include a public telecomputer network system harnessing the diversified and collective intelligence of ourentire country for the public's convenience, interest, and necessity.develop a national channel numbering planin the same way that the public and private sector cooperated to develop a channel numbering plan for vhf(i.e., channels 2 through 13) and uhf vd2 channels, nationwide vd3 network operators will require aconsistent and national catv (i.e., community antenna tv) channel numbering plan. a video program on abroadband tv channel in one state should be on the same nii channel in any other state. this nii numberingplan should accommodate both the existing wireless and wireline broadcasters in a common carrier environment,provide for equal access by all information and entertainment generators, and reserve channel space foreducation and government. it should also make allowances for significant channel numbers (e.g., 411, 555, 800,900, 911) that will assist the public in navigating a vd3 world. such a plan is needed to offer the publicnetwork channel surfers a "logical interface" to the 500+ channel systems of the relatively near future (seereference 3 for plan proposal).define the classes of servicenii policy should also define the classes of (nondiscriminatory) vd3 service. these classes in a commoncarrier environment of addressable vd3 or (vd1 + vd2) households will consist of the classes permitted bycombining the pstn's nationwide automatic number identification (ani) capability with a numbering plan forthe growing number of addressable broadband receivers in the pbn. with the coming evolution of video "mail,"the classes of service can, if not should, be modeled after those used in our postal system (see reference 3 forplan proposal).develop a ptn test and demonstrationthe ptn will consist of vd3 network operators providing a public network of common carrier services tothe next generation. the public and private sectors should jointly develop the criteria that will define a ptnpublic utility system. the criteria should set parameters that include demonstrating the inherent capability of theptn architecture (i.e., pc, tsb, dtmftv) both to provide universal and affordable interactive multimediaaccess and to serve all of america's communities in a nondiscriminatory fashion.the awakening 3.0: pcs, tsbs, or dtmftvšwhich telecomputer architecture is right for thenext generation's public network?551the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.offer the prizethere is at least one enabling public utility technology (see reference 4 for an example) that can deliver onthe vd3 universal access promise. the public sector may need to acknowledge that this form of public utilityservice, like all others before it, requires a "federal franchise" to operate as a regulated monopoly. such afranchise will be required in order to attract the necessary capital to build an information superpower system.this approach worked before as the bell system. with a pioneering spirit, it can work again for a new generationof americans.references[1] thompson, jack. 1995. "the dtmftv telecomputer model offers the most economical approach to interactive tv," gnostechinc., annandale, va.[2] thompson, "the dtmftv telecomputer model," 1995.[3] "the awakening 2.0," the comments of gnostech incorporated to the fcc's proposed rulemaking on video dialtone (commoncarrier docket no. 87266), 1991.[4] united states patent no. 5,236,199, "interactive media system and telecomputing method using telephone keypad signaling."the awakening 3.0: pcs, tsbs, or dtmftvšwhich telecomputer architecture is right for thenext generation's public network?552the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.62effective information transfer for health care: qualityversus quantitygio wiederholdstanford universitystatement of the problemthis paper addresses two related problems created by the rapid growth of information technology: the lossof productivity due to overload on health care providers, and the loss of privacy. both occur when excessiveamounts of data are transmitted and made available. these issues are jointly related to a tradeoff in qualityversus quantity of medical information. if quality is lacking, then the introduction of modern communicationtechnology will increase health care costs rather than constrain them.background and problem descriptionthe online medical record is rapidly becoming a reality. the technology is available and social barriers toacceptance are disappearing. access to online patient data during a treatment episode will become routinelyaccepted and expected by the patient as well as by the provider. the image of an expert, in the popular view, isnow associated with a computer screen in the foreground, and medical experts are increasingly being included inthat view. eventually, online validation of health care information may become mandatory. a recent court caseinvolving a physician who had failed to use available information technology to gather candidate diagnoses wasdecided in favor of the plaintiff1, presaging new criteria for commonly accepted standards of care.the rapid growth of the internet, the improved accessibility of online libraries, and online medical recordsall provide huge increases in potential information for health care providers and medical researchers. however,most beneficial information is hidden in a huge volume of data, from which it is not easily extracted. althoughthe medical literature, largely through the efforts of the national library of medicine, is better indexed thanliterature in any other scientific field2, the volume of publications, the difficulty of assessing the significance ofreports, inconsistencies in terminology, and measures to protect the privacy of patients all place new barriers oneffective use and result in what is sometimes called information overload. this overload means that diligentresearch for any case can require an openended effort, likely consuming many hours. we consider that thecurrent and imminent presentation of information is of inadequate quality to serve the practice of health care.unfortunately, the pace of development of software to provide services that deal effectively with excessive,convoluted, heterogeneous, and complex data is slow. since the problem in the preceding years has always beenaccess, there is a lack of paradigms to deal with the issues that arise now. before, when voluminous medical datahad to be processed, intermediate staff was employed, so that the health care provider was protected both interms of load and responsibility. staff at research sites filtered and digested experimental results. staff atpharmaceutical companies filtered for benefits and effectiveness. government agencies monitored lengthy trials.publishers required refereeing and editing. students and interns discussed new reports in journal clubs. collegialinteractions provided hints and validations. but our networks encourage disengagement of intermediaries, andwhile most of the intermediate filtering tasks are aided by computerbased tools, there is no common paradigmthat ensures the quality of information products.effective information transfer for health care: quality versus quantity553the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.analysisassessing the demands placed on the national information infrastructure by health care services requiresconsidering the needs of the health care providers and their intermediaries. this analysis is thus based oncustomer pull rather than on technology push. this approach is likely to lead to lower estimates than would amodel focusing on technological capabilities. however, we will assume a progressive environment, where muchpaper has been displaced by the technologies that are on our horizon.in our model, information requests are initially generated for the delivery of health care by the providersand their intermediaries. pharmacies and laboratories are important nodes in the health care delivery system.education for providers and patients is crucial as well and will be affected by the new technologies. managers ofhealth care facilities have their needs as well, paralleled at a broader level by the needs of public health agencies.functions such as the publication of medical literature and the production of therapeutics are not covered here,since we expect that topics such as digital libraries and manufacturing in this report will do justice to those areas.services for the health care providerthe initial point in our model is the interaction of the provider with the patient. such an interaction may bethe initial encounter, where tradition demands a thorough workup and recording of physical findings; it may be avisit motivated by a problem, where diagnostic expertise is at a premium; it may be an emergency, perhaps dueto trauma, where the problem may be obvious but the treatment less so; or it may be a more routine followupvisit. in practice, the majority of visits fall into this routine category.adequate followup is crucial to health care effectiveness and is an area where information technology hasmuch to offer. having the right data at hand permits the charting of progress, as well as the therapeuticadjustments needed to improve or maintain the patient's health care status. followup care is mainly providedlocally. the majority of the consumers of such care are the older, less mobile population. it is this population thathas the more complex, longerterm illnesses that require more informationthe needs for information differ for each of the interactions described above. initial workups mainlyproduce data. the diagnostic encounter has the greatest access demands. emergency trauma care may requiresome crucial information, but it is rarely available, so that reliance is placed on tests and asking the patient orrelatives for information. note that many visits to emergency facilities, especially in urban settings, are made toobtain routine care, because of the absence of accessible clinical services. for our analysis these arerecategorized. a goal for health care modernization should be better allocation of resources to points of need, buthere we discuss only the information needs. information for followup visits should summarize the patient'shistory; unexpected findings will trigger a diagnostic routine.to assess the need for data transmission we need to look at both the distance and the media likely to carrythe needed information. media differ greatly, and all must be supported. many physical findings can bedescribed compactly with text. laboratory findings are compactly represented in numeric form. sensorbasedtests, such as ekgs and eegs, are time series, requiring some, but still modest, data volumes. sonograms can bevoluminous. the results of ultrasound scans are often presented as images. other diagnostic procedures oftenproduce images directly, such as xray or ct and similar scans that are digitally represented. highquality xraysrequire much storage and transmission capacity, whereas most digital images have larger pixels or voxels andrequire more modest storage volumes. the practitioner typically relies on intermediate specialists to interpret thedata obtained from sensors and images, although for validation access to the source material is also wanted.the distance that this information has to travel depends both on setting and data source. table 1 indicatesestimated sources of patient care information for the types of clinical encounters listed.effective information transfer for health care: quality versus quantity554the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 1 sources of information for patient care and estimated frequency of usedata typeencounter typetextnumberssensorbasedimagesestimated local/remote ratio;encounterfrequencyworkuplocal collectionaided by staffarea clinicallaboratoriesarea diagnosticservicesarea, hospitalbased servicesvery high; lowdiagnosticlocal andremote reports,expertsarea clinicallaboratoriesarea diagnosticservicesarea, hospitalbased serviceshigh; modestemergencylocal andremote historiesonsitelaboratoriesonsite devicesonsite servicesmodest; lowfollowupextended localhistoriesarea clinicallaboratoriesarea diagnosticservicesarea, hospitalbased servicesvery high; highwe conclude that use of local patient care information dominates. the requirement for remote transmissionof data for individual patient care is modest. instances will be important, as when a traumatic accident requiresemergency care and consultation with an expert specialist in a remote locale. here again, quality considerationswill be crucial. it will be important to obtain the right data rapidly, rather than getting and searching throughtomes of information. at times images may be required as well. most xrays will be done locally, although onecan construct a scenario in which an archived image is of value. any actual medical intervention will becontrolled by local insight and information.in addition to requirements for access to and display of individual data, as shown in table 1, there is a needfor online access to the literature and reference material. here issues of locality are best driven by economicconsiderations. if the volume and frequency of use are high, then the best site for access will be relatively local;if they are low, the site can be remote as long as access is easy and latency is small. these parameters are undertechnological control, and no prior assumptions need be made except that reasonable alternatives will surviveand unreasonable ones will not.management and public health needsthe requirements of broad health care information for planning and research are significant. localinstitutions must improve the use of the data they have inhouse already for better planning. new treatmentsmust be monitored to enable rapid detection of unexpected side effects. public health officials must understandwhere problems exist, what problems can be addressed within their means, and what recommendations for publicinvestment are sound.today effective use of available health care data is difficult. standards are few and superficial. for instance,the hl7 standard does not mandate any consistency of content among institutions; only the format of the accessis specified. the data collections themselves also are suspect. private physicians have few reportingrequirements, except for some listed infectious diseases. if the disease is embarrassing, then their concern for thepatient's privacy is likely to cause underreporting, because they have little reason to trust that privacy will bemaintained in the data systems. in a group practice, the medical record will be shared and potentially accessible,but the group's motivations differ little from those of an individual physician. physicians working in a largerenterprise, such as a health maintenance organization (hmo), will have more requirements placed on them byadministrators who are anxious to have adequate records. still, there is little guarantee today that data arecomplete and unbiased. the local users are able to deal with the uncertainty of mixedquality data, since theyunderstand the environment. remote and integrated analysis is less likely to be able to use local data resources,even when access is granted.effective information transfer for health care: quality versus quantity555the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.however, clinical data collection and use is an area where change is occurring. the increasing penetrationof hmos, the acceptance of online access, and the entry of local data provide the foundation. when the localinformation feedback loops are closed and providers see at the next encounter what information they collected,then quality can improve. sharing one's record with colleagues also provides an inducement to record thepatient's state completely and accurately. as the record becomes more complete, questions of rights to accesswill gain in importance.clinical data collection is broad, but rarely sufficiently deep to answer research questions. where clinicianscollect data for their own research the quality of the variables they consider crucial will be high, but the scope ofmost studies is narrow and not comparable among studies and institutions. funded, multiinstitutional researchstudies make valiant efforts to maintain consistency but rarely succeed on a broad scale. although such data willbe adequate to answer focused research questions, little management or public health information can be reliablyextracted.many funded health care and service programs mandate reporting and data collection. but, again, there islikely to be a narrow bias in collection, recording, and quality control, and, except for administrative purposes,the value is minimal. biases accrue because of the desire to justify the operation of the clinics and services, and,if the data lead to funding, such biases are strengthened. public health agencies are well aware of these problemsand therefore tend to fund new research studies or surveys rather than rely on existing data collections.quality again seems to be the main constraining factor. how can quality be improved? quality will not beimproved by mandating increased transfer of data to remote sites. the only option seems to be to share data thatare used locally, and to abstract management and public health information from such local data. feedback at alllevels is crucial, not only from encounter to encounter, but also in the comparison of intervals betweentreatments in a patient's history (especially for aged individuals), among similar patients, and among physiciansusing different approaches to practice. again, it is the actual consumers of the information that need to beempowered first.the desire to have all possible information will be moderated by the effort and time that health careproviders must spend to obtain and record it. eventually, intelligent software will emerge that can help select,extract, summarize, and abstract the relevant and properly authorized information from voluminous medicalrecords and bibliographic resources. such software will be accepted by the providers to the extent that its resultsaccord with those experienced in their human interactions and aid their productivity.we see that the demands for national information infrastructure (nii) services are more in making softwareavailable and providing interoperation standards than in providing highperformance and remote communication.today, access is constrained by problems of interoperation, concern for privacy, and the poor quality of manycollections. the effort needed to overcome these barriers is major and will take time to resolve.educationthe need for continuing education has been more formally recognized in the health care field than in mostother areas. although unmotivated engineers can spend many years doing routine corporate work until they findthemselves without marketable skills, the health care professional is faced with medical recertification, hospitaladmit privileges, and the need to maintain credibility. in urban areas the patient's choices are many and are oftenbased on contacts leading to referrals. all these factors motivate continuing education. the quality of sucheducation is decidedly mixed. boondoggles are common, and testing for proficiency of what has been learned isminimal or absent. few standards exist.in this instance, access to remote instructors and experts can be a boon. for the rural practitioner, who findsit difficult to leave the practice area, such services are especially beneficial. in urban areas, most educationalservices will be local. the demand on the nii is again difficult to gauge but may again be modest in theaggregate: fewer than 10 percent of our health care providers practice in remote areas. spending a few hours aweek on remotely accessed educational services seems to be an outer limit. the services should be fast and ofhigh quality. since time is not of the essence, the service has to compete with printed material, where it will beeffective information transfer for health care: quality versus quantity556the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.difficult, if not impossible, to match image quality. on the other hand, dynamic interaction has a role and canprovide excitement.there are natural limits to the capabilities of humans to take in information. the rate provided on atelevision screen is one indication of these limits; few people can watch multiple screens beneficially, althoughthe actual information content in a video sequence is not high. the script for an hour's tv episode is perhaps 200sparsely typewritten pages, but the story is exciting to watch, and that has added value. only a few paths arereasonable at any point in time, and the choice of paths represents the essential information. excessiverandomness of events, mtv style, is unlikely to be informative. eventually, the information retained after anhour of educational video is likely to be even less than that of the script provided.technology will allow interaction in the educational process, just as now some choices can be made whenreading a book or browsing in a library. again, the number of choices at each point is limited, probably to themagical number 7 ± 2, the capacity of the interactor's shortterm memory3 effective educational systems mustkeep the learner's capabilities in mind. to what extent intermediate representations expand the material and placehigher demands on network bandwidth is unclear.decision supportthe essence of providing information is decision support. all tasks, whether for the physician treating apatient, the manager making investment decisions, the public health official recommending strategies, and eventhe billing clerk collecting an overdue payment, can be carried out effectively only if the choices are clear. thechoices will differ depending on the setting. the manager must give more weight to the financial health of theenterprise than does the physician recommending a treatment.customerbased capacity limits can be imposed on all service types provided by the information enterprise,just as we sketched in the section on education. making choices is best supported by systems that provide alimited number of relevant choices. the same magical number (7±2) raises its head again. to reduce the volumeof data to such simple presentations means that processing modules, which fulfill the roles of intermediaries inthe health care enterprise, must be able to locate likely sources and select the relevant data. even the relevantdata will be excessive. long patient histories must be summarized4. similar patient courses can be compared,after matching of the courses based on events in patient records, such as critical symptoms, treatments applied,and outcomes obtained. it is rare that local patient populations are sufficient, and so matching has to beperformed with information integrated from multiple sites, taking environment into account.the presentation to the customer must be clear and must allow for explanations and clarifications. sourcesof data have to be identifiable, so that their suitability and reliability can be assessed. once such services areprovided, it will be easier to close the feedback loops that in turn encourage quality data. having quality dataenables sharing and effective use of the technological infrastructure being assembled.education and entertainment benefit from a mass market, and so the expansion of information into excitingsequences has a payoff in acceptance and markets. that payoff is much smaller for the review of medical recordsand the analysis of disease and treatment patterns. the volume of health care information transmitted fordecision support will be constrained by the filtering imposed by quality control.protection of privacythe public is distrustful of the protection of privacy provided for health care data, and rightly so. in manycases, to receive health care services and insurance reimbursement, patients have to sign broad releases. oncethey do, their medical information flows, with nary a filter, to the organization's billing clerks, to the insurancecompanies, and (in case of a conflict) to legal professionals. although all these people have ethical constraints onthe release of information, little formal guidance and even fewer formal restrictions are in place. in the paperworld, loss of privacy was mainly an individual concern, as in the case of the potential embarrassmenteffective information transfer for health care: quality versus quantity557the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.to a politician when the existence of a psychiatric record was revealed. in the electronic world, the potential formischief is multiplied, since broadbased searches become feasible.the insurance companies share medical information through the medical information bureau. this port isassumed be a major leak of private information. unless it can be convincingly plugged, it is likely that healthcare enterprises will have to limit access to their data if they want to (or are forced to) protect the patient's rightsto privacy. many health care institutions, after having appointed a chief information officer (cio) in the pastdecade, are now also appointing a security officer. without guidelines and tools, such an officer will probablyfurther restrict access, perhaps interfering with the legitimate requests of public health officials. it is unclear howsuch officials will deal with leaks to insurance companies and their own billing staff.legitimate concern for the protection of privacy is likely to hinder use of the information infrastructure. wedo believe that there are technological tools that can be provided to security officers and cios to make their taskfeasible5. to enable the use of information management tools, the information flow within major sectors of thehealth care enterprise has to be understood. the value and cost of information to the institution, its majorcomponents, and its correspondents has to be assessed. without control of quality the benefits are hard todetermine, and it will be difficult to make the proper investments.quality and privacy concerns are likely to differ among areas. that means that those areas must be properlydefined. once an area is defined, access rules can be provided to the security officer. a barrier must be placed inthe information flow if access is to be restricted. such a barrier is best implemented as a system module orworkstation owned by the security officer. that node (a security mediator consisting of software and its owner)is then the focus of access requests, their legitimacy, and their correct response. the security mediator must betrusted by the health care staff not to release private information and must also be trusted by the customers (bethey public health officials, insurance providers, or billing staff) to provide complete information within thebounds of the rules provided.the volume of data being transmitted out of the health care institution may be less, but the resultinginformation should be more valuable and trustworthy.recommendationsthe sources and uses of health care information are varied. technological capacities and capabilities arerapidly increasing. the informational needs of the health care enterprise can be defined and categorized. ifquality information can be provided, where quality encompasses relevance, completeness, and legitimacy, thenthe demands in the nii can be estimated, and it appears that the overall capabilities are likely to be adequate.distribution, such as access in rural areas, is still an open question. i have recommended elsewhere that the ruralelectrification services authority (rea) repeat its success of the 1930s by focusing on the provision ofinformation access to the same customers.the major point to be made is that, in order to provide health care professionals with the best means fordecision making, a reasoned balance of software and hardware investments is appropriate. software provides themeans to abstract voluminous information into decision sequences where, at every instant, the customer is notoverloaded. there is an optimal trajectory in balancing investments in the systems infrastructure versus softwareapplication support, but we have not spent much effort in understanding it. for health care providers the benefitsare to be found in the quality of informationšit must be good enough, sufficiently complete, and relevantenough to aid decisionmaking. if the quality is absent, then the effort will be poorly rewarded and the risks offailure will be high. the recommendation from this point of view is therefore to move support to the informationprocessing infrastructure, so that relevant applications can be built easily and the customers satisfied. a happycustomer will in turn support the goals of the nii.an area where government support can be crucial is in helping to define and validate standards. standardssetting is best performed by customers and providers, but the validation and dissemination of standards areprecompetitive efforts that take much time and have few academic rewards. academic insights can help ensurecoherence and scalability. tests performed outside vendor locations are more likely to be trusted and are easierto demonstrate. infrastructure software, once validated, is easy to disseminate but is hard to market untileffective information transfer for health care: quality versus quantity558the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the application suites that build on the infrastructure are available. the need to support the communicationshardware infrastructure has been recognized. the support of software in that role may well be simpler, since itsreplication is nearly free.the desired balance for health information infrastructure support can be replicated in all fields ofinformation technology. we expect the parameters to differ for commerce, defense, education, entertainment,and manufacturing. the common principle we advocate is that, as we move from a supplylimited to a demandconstrained information world, our analysis and actual service methods must change.references[1] harbeson v. parke davis, 746 f.2d 517 (9th cir. 1984).[2] wiederhold, gio. 1995. ''digital libraries," communications of the acm, april.[3] george miller.[4] isabelle de zeghergeets et al. 1988. "summarization and display of online medical records," m.d./computing 5(3):38œ46.[5] willis ware, "the new faces of privacy," p7831, rand corporation, 1994.effective information transfer for health care: quality versus quantity559the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.63integrating technology with practice: a technologyenhanced, fieldbased teacher preparation programronald d. zellner, jon denton, and luana zellnertexas a&m universitystate initiative centers for professional development andtechnologythis report describes the development and structure of the texas education collaborative (tec), whichwas established at texas a&m university as a means of restructuring the teacher preparation program andimproving public education. the tec was funded as part of a statewide initiative calling for a series of centers tofoster collaboration between k12 and higher education institutions and the incorporation of technology into theeducational process.texas has recognized a need, as have most states, for the systematic restructuring of its educational systemto meet the needs of a changing society and world community. as part of its response to this need, the texaseducation agency (tea) funded a program to develop such centers throughout the state. the program for thesecenters was established by the texas state legislature in 1991, enabling the state board of education (sboe)and the texas higher education coordinating board to establish competitive procedures for one or moreinstitutions of higher education to establish a center. the initial allocation for the centers was $10.2 million. toqualify, centers needed to include a university with an approved teacher education program, members frompublic schools, regional education service centers, and other entities or businesses. eight centers were approvedby the sboe in 1992, six in 1993, and three in 1994. currently, this collaborative effort includes 31 institutionsof higher education, 7 junior colleges, 15 regional service centers, and 193 campus sites.focus of the state initiativethe state of texas, like most states, is currently undertaking a general review of teacher preparation andcertification processes. texas is a culturally diverse state, with the school population spread out over a wide anddiverse geographical area. for example, el paso is closer to los angeles, california, than it is to beaumont,texas, and closer to two other state capitals (phoenix and sante fe) than it is to the texas capital, austin; andamarillo is closer to four other state capitals (sante fe, oklahoma city, topeka, and denver) than it is to austin.texas is divided into 254 counties with 1,063 school districts and 6,322 public schools (972 high schools, 955junior high, intermediate, or middle schools, 3,618 elementary schools, 743 elementary and secondary schools,and 34 schools in correctional institutions). as a means of aiding these districts, the state is divided into 20geographical regions, each with an education service center (esc) that has support staff, services, and trainingfor all of the school districts located within its boundaries.consideration, partly related to this diversity, is being given to processes and certification requirements,which may vary from one region to another according to the needs of the particular geographical areas, aintegrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program560the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.consideration that has potential for a variety of outcomes. other factors influencing the acceptance of multiplecertification requirements are related to philosophical, political, and financial concerns.the centers were established to take a leading role in the revision of the certification process. they areintended to improve teacher preparation through the integration of technology and the development of innovativeteaching practices and staff development programs throughout the state's colleges of education. this initiativewas based on the premise that the success of schools and students is directly linked to the success of the state'steacher preparation programs. all centers are to focus on five components: collaboration, restructuring educatorpreparation, staff development, technology, and addressing the needs of a multicultural student population.collaboration between the institutions of higher education and k12 institutions is to be a major componentof the centers' activities. classroom teachers are to serve as mentors and role models for their interns; teachers,principals, and esc personnel may help develop courses, teach, or coteach university courses at their home baseor at the university. university professors are to be in the schools as resources and consultants, and to learn fromthe mentor teachers. the governance structure must reflect the cultural diversity of the state, and no category ofrepresentation may be larger than the k12 teacher representation.the intended restructuring of educator preparation programs, in conjunction with the collaborationemphasis, is focused primarily on establishing programs and instruction that are field based. new programs arealso to include regular collaborative decision making by all partners; an emphasis on teachers as lifelonglearners; routine use of technologies (multimedia, computer based, longdistance telecommunicationstechnologies); research and development of new technologybased instructional techniques; and innovativeteaching practices in multicultural classrooms. these developments are intended to influence several majorcomponents in the structure of teacher preparation programs, including the composition, location, and structureof methods courses, the constitution of the teacher preparation faculty, the role of technology, and the culturaldiversity represented.the following presents a comparison of traditional preparation approaches to program components found inthe various centers.to help ensure complete success in restructuring education, these centers are also to include major staffdevelopment components that will provide inservice training to practicing teachers. a major emphasis of thisstaff development effort is on helping teachers to become lifelong learners. thus, by keeping current throughouttheir teaching careers, these teachers will be able to teach their students the skills that they will need to besuccessful in the twentyfirst century. in this way, student achievement will be linked to teacher performance,and both will be enhanced through campus improvement plans.another major initiative is the incorporation of technology into the centers to expand the delivery ofinstruction in the k12 classroom and in teacher preparation classes. coupled with fieldbased instruction, thisendeavor will help to prepare students majoring in education to teach in the classrooms of tomorrow. the centersare also to provide for the development of new technologybased instructional techniques and innovativeteaching practices. through direct contact with these activities, inservice and preservice teachers will learn theappropriate skills and will be encouraged to integrate technology into their teaching practices.multicultural education provides information about various groups as well as the skills needed to work withthem. to provide cultural diversity and adequately prepare inservice and preservice teachers, the centers arerequired to implement public school programs and services that reflect diverse cultural, socioeconomic, andgradelevel environments. the inclusion of minority teacher candidates in the centers' recruiting and trainingefforts will also be a high priority, particularly in subject areas where teacher demand exceeds supply.systemic development and evaluationeach center is implemented and evaluated in an ongoing developmental program involving k12, highereducation, and state constituents. this collaboration reflects the program's commitment to systemic change andrecognizes that all shareholders must be involved if adequate systemic change is to occur. to provide programsand services throughout the state and prepare teachers to meet the full range of learner diversity and needs, thestate board of education funding requirements for the centers indicate that funding may be made to centers bygeographical areas. this is particularly important in a state as culturally and geographically diverse as texas.integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program561the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.these centers have also been identified as one of the critical communities to be involved in a program ofprofessional development grants under the texas statewide systemic initiative. the centers will thus provide astructure for subsequent, related initiatives and help ensure that the needs of education will be met throughout thestate.each center is required to develop an extensive ongoing evaluation design composed of both internal andexternal evaluations. progress reports to the texas education agency are required from each center quarterly aswell as annually. visits by statewide evaluators are conducted to review the progress of the centers regarding thefive main componentsšcollaboration, restructuring educator preparation, staff development, technology, andmulticultural education. both qualitative and quantitative data are to be considered in the statewide evaluation ofthe centers' activities and progress.the texas education collaborativethe texas education collaborative (tec) was established in 1992 as one of the eight original centersfunded by tea. there are currently 17 centers for professional development and technology throughout thestate. in the first year, the tec partners included two universities with approved teacher education programs,texas a&m university and prairie view a&m university, in collaboration with eight schools representing fiveindependent school districts; two state regional education service centers; a community college; and parent andbusiness representatives. the five independent school districts were bryan i.s.d., college station i.s.d., conroei.s.d., somerville i.s.d., and waller i.s.d. original business partners included apple computer, gte, ibm,3m, and digital equipment corporation. the goals and activities of the collaborative were established inconjunction with the tea guidelines and included the following components.teacher education programs and environments for teaching and learning the general goals in this area were directed at the establishment of a framework for curriculum developmentand included the following:šcoordinated efforts of faculty across all college of education (coe) departments for collaboration onresearch, proposal writing, course development and delivery, and coordination of course contents related tocultural issues;šintegration of technology into coe course instruction, content, and management;šmore university faculty delivering courses in schools via partnerships with school faculty;šchange from course evaluation to content evaluation by peer groups; andšshift from carnegie course units to block units over 5 years. school faculty, administrators, and counselors will become determine instructional methods and materials,identify and develop interdisciplinary curricular themes, schedule classes, and evaluate student performance. schools will be designated as fieldbased tec sites for preservice and inservice professional developmentand practicum activities for teachers, teaching candidates and administrative interns (thus providing "realworld" experience and the involvement of expert teachers). each tec site will become a resource institution for restructuring k12 education and teacher education atall levels (focusing on issues such as inservice training of new techniques, involvement of parents andcommunity, collaboration, strategies for students from minority cultural groups, mentoring, etc.).stateoftheart teaching practices, curriculum, and instructional knowledge tec sites will become centers for intellectual inquiry where everyone is both a teacher and a learner.academic and clinical experiences will include the following:štechnology applications and technologyassisted instruction;integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program562the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.šintegration of interdisciplinary content and pedagogical methods;šcollaboration and teamwork among school and university faculty and preservice teachers;šreflective decisionmaking; andšselfevaluation. preservice and inservice professional development will emphasize cultural diversity, incorporate strategiesfor dropout prevention, and encourage students from underrepresented groups in all subject areas. teaching candidates will demonstrate conceptual understanding by applying information to new situations.assessment will be done by school and university faculty and will include observation of teaching behaviorand candidateproduced portfolios of multimedia projects. new technologies will be used to help students solve problems, synthesize ideas, apply information to newsituations, and develop oral and written communication skills.governance structure of the tecthe governance structure will include teachers, administrators, education service center staff, and universityfaculty and administrators. this structure is to reflect the cultural diversity of the state. the coordinating councilis composed according to tea requirements, including the provision that no one group be larger than the k12teacher group.budgetthe initial funding allocation was for 1 year, with subsequent funding potential for 4 additional years. thefirst 2 years of the tec were funded at $2 million; $1.5 million of this was slated for equipment and theremainder for personnel and operating expenses.the equipment expenditures represent the need to equip faculty and classrooms with computer and relatedtechnologies to foster important, needed changes in teaching and operating activities. a number of compressedvideo sites for twoway video teleconferencing and instruction were established and utilized for instruction.summary of tec activitiesduring the 1992œ93 year, the tec activities focused primarily on the acquisition and setup of technologyhardware and software at university and school sites, training in the use of the technology, and development ofcollaborative structures among school and university faculty. activities included technology staff development atschool and university sites, technology conferences for school and university faculty, frequent sharing meetings,collaboration within and among schools, teambuilding activities, a retreat for school and university faculty todevelop a shared vision for restructuring teacher preparation, and development of proposals for five programsrelated to various aspects of restructuring.technology hardware and software acquisitionsyear onethe bulk of the first year's acquisitions were computers, productivity software, network hardware, andsupport multimedia presentation resources. these resources were considered necessary to prepare teachers anduniversity faculty in the use of technologies in order to begin the process of changing teaching activities inclassrooms at both levels. if student teachers are eventually to use such technologies in their classroom activitiesas teachers, they will need to experience such benefits as learners. all teacher preparation university facultyreceived computers for their offices to encourage the transformation of their daily activities. in addition, fullyintegrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program563the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.equipped laboratories, networks, and multimedia workstations were established for faculty and student use.classroom presentation stations were also provided to ensure infusion of these technologies into the actualteaching process. computer teaching laboratories were updated for undergraduate and graduate educationaltechnology classes. consequently, student teachers would ultimately learn about the use of technology, learnfrom technology based materials, and learn with technology through their own developmental projects. a similarmodel was maintained for teachers and students in the schools.table 1 gives a breakdown of the number of computers acquired during this period.table 1 computers acquired during year oneuniversitiesschoolstotalmacintosh7568143powerbook291443ibm3737ibm notebook22total14382225year twothe second year's equipment purchases, using the remainder of the initial $1.5 million equipment budget,included more computers for faculty, teachers, and classrooms. additional faculty became involved in the tecactivities and were equipped with the necessary hardware and software. the multimedia workstation labs wereenhanced with more and more powerful computers and support hardware. additional software was provided forthe development of presentation and instructional materials. multimedia presentation stations were acquired forclassrooms in the various curriculum units to support the use of technologies in methods classes and additionalportable presentation units were provided for instructional use throughout the college. email systems wereestablished and incorporated into a major part of the communication process. again, parallel resources wereacquired for the classrooms in the participating schools and were used in a wide variety of activities by teachersand students. several local computer networks were established in the schools to support instructional andmanagement functions. software consisted primarily of productivity packages (word processing, spreadsheet,database), presentation packages, hypertext development programs, and graphics production packages.a major component for the delivery of instruction and faculty training was the acquisition andestablishment of a compressed video teleconference network linking schools in the participating districts with theuniversity and one another. at least one school in each of the participating districts was supplied with a completeteleconferencing station and the necessary support components. these sites were established in association withthe transtexas teleconferencing network (ttvn), which is centered at texas a&m and has sites locatedthroughout the state. this association greatly enhanced the ability to quickly create a functional network andprovided an extensive network for instruction and conferencing for meetings and supervision of students in thefield. this provided additional opportunities for collaboration between university faculty and teachers andstudents in the schools. each teleconference site provides twoway video and audio communication with theaddition of interactive video and computerbased materials that can be viewed and accessed at any of theparticipating sites in a session. table 2 presents the timetable for the establishment and initial use of theseteleconferencing sites.year three (first quarter)as the tec began its third year, the focus was shifting from technology and skill acquisition to thedevelopment of applications and programs. the majority of the equipment was purchased in the first 2 years ofintegrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program564the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.table 2 timetable for establishment and initial use of teleconferencing sitesnumber of site sessionssiteinstallation datedec.šfeb.marchšmaytotalanson jones academydec. 9312719jones intermediatefeb. 9431518somerville jr. highdec. 93222042southwood valley elem.dec. 9312719washington jr. highmarch 94099coepvamuapril 94101coetamufeb. 94142741the program. recent acquisitions were directed at improvement of existing facilities through softwarepurchases and upgrades, memory upgrades, video boards, and so on. two extensive network facilities and fileservers were acquired and are currently being established as a resource for communications and resourcedelivery. the focus in technology resources is now on keeping up with new advances in hardware and softwareand on meeting maintenance concerns such as monthly charges for telecommunications and video teleconferenceactivities. five schools were added during the past year as evolving tec sites and will continue theirinvolvement, and six more schools were added as evolving tec sites this quarter. these eleven schools received$13,000 each in startup funds for equipment and staff development. table 3 gives a breakdown of there sourcesin the current tec inventory.table 3 breakdown of resources in tec inventoryitemschoolsuniversitytotaldesktop computers96109205portable computers172946network resources112video teleconferencing sites527projection systems13518vcrs055televisions347laser disk players151530compact disk players707printers341549camcorders088staff development: teachers, administration, and university facultyyear oneeach tec school site council held at least four meetings during 1992œ93. each council included at least sitecoordinators, two teachers, a principal, and business, parent, and esc representatives as voting members. exofficio members are university liaisons and other university and school district representatives. main topics ofdiscussion during these meetings were the definition of roles, technology (acquisition of hardware and software,networking, access, demonstration), staff development, budget expenditures, and field experiences for teachingcandidates. discussions at tec staff meetings focused on issues of alternative, authentic, and integratedassessment as well as implications of such assessment techniques on classroom context and curricular reform.integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program565the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.several individual faculty and administrators from tec schools and universities attended seminars andcourses on professional development schools to increase their understanding of schooluniversity collaboration.furthermore, the tec sponsored several events to foster schooluniversity collaboration and to empowerteachers as full participants in restructuring teacher preparation programs.the tec facilitated collaboration among school and university faculty and administrators through tecstaff meetings, challenge events, a planning retreat, school site council meetings, and curriculum planning groupsat schools and universities. collaborative meetings and events included the following: a series of challenge events were held to bring together school and university representatives to work onteam building, trust building, and group problem solving, and to begin conversations about restructuredteacher preparation. the challenge events helped break down communication barriers among constituencygroups, but little was accomplished in actual planning of restructured teacher preparation programs.teachers reported using similar team building strategies with their students and colleagues. approximately75 people participated in at least one challenge event, 35 people in two events, and 18 people in all threeevents. nearly half of the participants were classroom teachers. seven tec staff meetings were held in 1993, providing opportunities for collaborative sharing and planningamong site coordinators across tec school and university sites. staff included tec site coordinators,university liaisons, a director, evaluation personnel, and technology personnel. the tamu college ofeducation issued requests for proposals from faculty of various departments who work with schools onrestructuring teacher education.discussion included arrangements for compressed video setups at school sites, a technology sharing timefor site coordinators, the need for site coordinators to visit other school sites, expectations for preserviceteachers in the tec, development of a tec newsletter, and the development of multimedia interdisciplinary units. five tec school partners each had at least five site council meetings in 1992œ93. groups also discussedtime involved in planning field experiences, how to implement them, what early field experiences mightlook like, and what the final product might look like. teachers received training on restructuring and peercoaching. the tamu college of education held allday workshops for faculty across departments to explore theorganizational structure of teacher education programs. a technology integration task force held severalmeetings to redesign upperdivision teacher education courses for the integration of evolving technologies.a secondary teacher preparation committee composed of university and school faculty met several times inspring 1993 and developed a training proposal. all tec school site coordinators are kept informed regarding exemplary software through staff meetings,conferences, catalogs, and other listings; in addition, a special technology workshop, "tec talk," was heldon april 16 for tec site coordinators. teachers at tec school sites received technology staff developmenttraining approximately once a week, beginning in october 1992. these sessions were conducted by tamutechnology facilitators. staff development at school sites included word processing, graphics, spreadsheetand database components of clarisworks, aldus persuasion, and hypercard, and multimedia applications,as well as individualized training and instructional applications.several projects were funded with tec funds to address the various goals in the state initiative. oneproject, the alternative school leadership program, was designed to prepare leaders for the different leadershiproles required in an emerging fullservice professional development school. project time was designed toprepare teachers who would serve as instructional leaders within their schools. it was also designed to assistcolleagues in general education, and teachers who can collaborate with community leaders and health and humanservice agency personnel. a critical element of this program was early identification of atrisk and special needsstudents, which would result in referral to appropriate support services in the community.both the university faculty and the classroom teachers were provided a series of workshops on the operationof the new equipment, the use of the various software packages, and the application of the software to dailymaintenance and instructional materials development. these workshops were conducted in both the universityand school labs. educational technology faculty and a technical staff of graduate students were madeintegrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program566the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.available as needed to assist faculty with questions and problems as they developed to ensure smooth integrationof these resources into their teaching.year twoduring the second year, the tec was expanded to include faculty from 12 schools, and plans were made toexpand to 18 schools. training workshops continued to be offered in the schools by both university and schoolpersonnel. workshops and help sessions for university faculty were continued. many training sessions wereoffered over the videoconferencing networks to help provide training without interfering with the teachers' dailyroutines. a number of formal workshops were offered at the university for teachers from participating schoolsand focused on applying software to their areas of teaching. some teachers took existing university technologyclasses that were appropriate to their needs. the tec sponsored attendance at a number of national technologyand educational research conferences for teachers and university faculty.year three (first quarter)there are two general changes regarding staff development activities. first, expertise and support haveevolved at some of the sites to the extent that recently trained site specialists can now deliver staff developmentsupport. second, there has been a move from general orientation and basic skills training to more advancedtraining and individualized instruction in specific topics and locations. however, there are several new evolvingsites with faculty in need of general orientation. a staff of technologists has been available for troubleshootingproblems and training individuals who have questions or problems. during this quarter, there were 201participants in group training sessions and 46 people scheduled in individual sessions. more individuals wereintroduced to video teleconferencing operation and capabilities by participating in sessions and through specialtraining materials that were developed. these activities involved both university faculty and teachers. also,topics were no longer focused only on the hardware and software needs of the participants. for example, a seriesof workshops initiated in the fall of 1994 covered the following topics: the role of schooluniversity partnerships in creating the professional development school of the future; multimedia use in collaboratives; national standards; teacher tactics, portfolio preparation, classroom management; and future roles of fieldbased education.there were several ancillary activities this quarter that highlighted the collaborative component of the tecthrough functions extending its resources and influence to other groups and that brought benefits from thesegroups for tec participants. a conference on education, inclusion, and diversity was conducted by the collegeof education at prairie view a&m with partial support from the tec. between 300 and 350 educatorsparticipated in activities related to cultural diversity in education. another program was cosponsored by the tecsite school system and a city government to highlight the accomplishments of the schools with support fromgte and the tec. approximately 400 citizens and educators attended this activity.a 2day educational technology conference, cosponsored by the tamu educational technology program,the tec, and the region vi educational service center, was attended by more than 400 teachers, administrators,university faculty, and students. some participating sites used tec funds to send teachers as part of their staffdevelopment efforts. this conference provided an arena for presentation of projects and activities and anopportunity for teachers and university faculty from all curriculum areas to interact on topics relating toinstructional technology.integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program567the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.development of teacher preparation/preservice coursesyear oneit is intended that all students involved in the tec will be "prepared" for using technology in their studentteaching experience. since teachers tend to teach as they were taught, one of the first initiatives here was, asdescribed above, aimed at changing the instructional processes and techniques used in methods classes to modelthe use of technologies. faculty training and resource acquisition was a necessary prerequisite for this to happen.in addition, courses in educational technology for preservice teachers were maintained and closely allied to theequipment and applications needs of the fieldbased activities. thus, all of the various activities of the tec areinterlinked and dependent on one another. regarding the actual restructuring of teacher education, programefforts during the first year were focused on establishing technological and social systems infrastructures for thetec partner schools to facilitate restructuring of teacher education programs in a collaborative manner.examples of these activities include the following: more than 40 site council meetings, involving over 80 teachers, university faculty, and parents, were held toformulate the instructional programs of 169 teachers and some 3,900 students during the past year. work began on a program for secondary school teacher preparation (grades 7 though 12) that includes aholistic approach to educational issues, sensitivity to the needs of teachers, and particular attention to theneeds of students with diverse backgrounds and learning styles. community members, texas a&m faculty, and preservice teachers spent quality time each week with atrisk learners in order to provide each one with a support network. preservice teachers worked oneonone with learners who have emotional and behavioral disorders. teachers and 80 students from four grade levels worked together in a multilevel learning community andconducted research on an interdisciplinary experiencebased curriculum. efforts were made to recruit and support cohorts of preservice teachers from socially, culturally, andlinguistically diverse backgrounds, and to involve them in the governance of teacher preparation programs.direct field experience in diverse community settings and classroom/teaching experience in schoolsrepresenting culturally diverse communities are intended.year twoa collegewide review team identified three program orientations that span the range of professionaltraining for preservice teachers: young learners, middle school, and secondary school. because of the numberof preservice teachers (approximately 1,400) being served by current programs, a gradual transition was plannedfor the implementation of these programs.these programs will provide distinct phases for the preservice preparation of students. the first phase isentitled children, families, and communities for the young learners and middle school programs and selfdirected experiences for the secondary school program. this phase occurs before actual admission to teachereducation and includes experiences where potential candidates learn about children, communities, and familiesand explore education as a profession. this phase involves field work in the community. future teachers mustdemonstrate that they can communicate with children and families who speak a language other than english andthey must develop a portfolio that includes meaningful interactions and involvement with a child or childrenfrom backgrounds different from their own.the second phase also occurs before admission to teacher education and is entitled children, schools, andsociety for the young learners and middle school programs and teaching and schools in modern society forthe secondary program. prospective students study the impact of schooling on society and consider what itmeans to educate children for their roles in democracy. both phases require potential teacher candidates tocollect evidence of experiences that illustrate their awareness of linguistic, socioeconomic, and cultural diversity.integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program568the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.students in the secondary program also complete a third phase, developing skills to meet the needs ofsociety and students, before they apply for admission.candidates in the secondary program proceed to two semesters of practicums after being admitted toteacher education, while candidates in the young learners and middle school programs are assigned to cohortsthat are guided by teams consisting of a university faculty member, a schoolbased clinical faculty member, anda doctoral student specializing in teacher education. candidates in the young learners and middle schoolprograms then proceed through the professional development phase, entitled children, teachers, and classrooms.a number of trial and experimental programs have been conducted at various tec sites. twelve universityfaculty and 24 school basedfaculty have designed, implemented, and taught university courses to 65 futuremiddle school teachers on one campus. a pilot program, called writing buddies, was implemented at a middleschool site. preservice teachers who enrolled in an undergraduate reading methods course participated in thisfield experience, which provided a tutorial program for students. the program was supervised by practicingprofessional teachers. a university professor conducted the lecture on campus while selected teachers conductedseminars ''on site" with students enrolled in the course. a similar program was conducted in an elementaryschool in which undergraduates are paired with elementary students to work on writing skills. the futureteachers meet with their "writing buddies" at least once a week. another elementary school served as a site forbilingual programs in the district. additional goals include the establishment of a block of fieldbased methodscourses taught there. the first block of courses was offered during the spring semester, 1994, and taught on siteby a team of university professors. a high school program involved approximately 80 students in the pilot of aninterdisciplinary program where the traditional freshman english and u.s. history courses were integrated.teachers and student teachers participating in 90minute instructional blocks used instructional strategies oncentral themes that integrated topics across the traditional subject areas. through these activities, preservicestudents not only gain classroom experience but are also involved with the conceptualization and planning of theactivities.year three (first quarter)during the fall of 1994, 100 preservice teachers registered for an undergraduate course that required themto spend 2 hours a week in lecture at the university for instruction in language arts methods. the students wereassigned to 1hour labs on an elementary school campus. nine teachers served as lab instructors supporting thismethods course. twenty students were selected from the 100 participants in the reading methods course andwriting buddies program to continue for two more semesters. this is one example of putting the goals ofcollaboration and fieldbased instruction into action and will serve as a model for other courses.a collaborative model for teacher preparationcollaborationthe "need" for establishing centers for professional development and technology was determined by thetexas education agency (tea). once established, members of the collaborative focused on the educationalgoals of the state by specifically addressing educational concerns of the individual site members. it was agreedthat they would address these concerns together, working on solutions that would benefit site needs and teacherpreparation programs. they did this through a series of meetings that included the following:1. site council meetings at school site campuses;2. specific program focus group meetings at the university level concerned with teacher preparation; and3. development council meetings, which included site council representatives as well as university facultyinterested in working with collaborative members on specific programmatic change.integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program569the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.public school problems addressed by the collaborative included the following:1. raising taas (test for academic assessment of skills) scores of children at school sites identified asbeing at risk for school failure;2. lowering the dropout rate;3. improving attendance and interest in school;4. providing staff development opportunities to improve teacher instructional skills in the use of technologyin classroom instruction;5. developing inclusion models for teaching strategies that help all learners succeed;6. providing time for teachers to collectively plan integrated learning experiences;7. providing time for teachers to create computer programs that enhance learning; and8. improving teacher retention.teacher training program problems addressed by the collaborative included the following:1. tying realworld experience with methods courses to accommodate 900 preservice teachers in a trainingprogram per year, 18 tec sites, and 13 actively involved tenure track faculty plus 2 adjunct faculty;2. providing more "practice" in application of pedagogy through field experiences coupled with methodscourses;3. providing instruction for teachertraining faculty in the integration of technology into course content;4. ensuring that preservice teachers see "best practice" in their field experiences. examples: integratedinstruction by teachers, application of technology into classroom instruction by teachers, andcollaboration between teachers in planning instruction;5. ensuring that technology is used by teachertraining faculty in instruction;6. redesigning methods courses so they can be taught in a multitude of settings. examples: university classsetting (traditional approach), school site setting, through telecommunication, broadcasting from site tosite or university broadcasting to site(s); and7. developing a system of communication between tec sites, site coordinators, university liaisons, andteacher trainers.the following are examples of collaborative efforts:1. development of a totally integrated, technologyenhanced curriculum for a fourth grade focusing on thegeneral theme of texas for interdisciplinary investigations in history, cultures, ecology, commerce, andthe arts. this collaborative project was designed by teachers and university faculty.2. blocked teacher preparation methods courses taught on site by either tenure track faculty or by trainedclinical faculty (teachers) in pilot programs at six tec sites in the collaborative. the courses includefield experiences designed by the classroom teachers in collaboration with university professors.classroom teachers serve as mentors to preservice teachers with some projects offering seminarsconducted by participating classroom teachers.3. several conferences on subjects such as distance learning; a language institute for minorities; linking offamilies, school, and culture; best practices for inclusion of children with disabilities; and aspects oftechnology.4. a monthly newsletter documenting activities taking place among the sites, advertising for pen palsbetween sites, describing university courses taught through the use of telecommunications, andpublicizing staff development opportunities at sites.integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program570the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.restructuring teacher preparationthe mission of the tec is to develop and implement restructured teacher preparation programs that aretechnology enhanced and field based. the tec is actively engaging faculty from schools, universities, and escsto work together as agents of change in creating new approaches for preparing teachers and administrators for alllevels of public education. restructuring was defined by goodlad in 1990 as reexamining the purposes ofeducation, and reworking the structures and practices of schools and university preparation programs in order toimprove teaching and learning. as defined by tea,teacher preparation programs must be developed which integrate field experiences in school and communityhuman service. these programs should include advice and counsel from parents, care providers and governmentalagencies. teachers completing these programs shall be lifelong learners who continue to seek new levels ofunderstanding content, pedagogy and culture.three new teacher preparation programsin addition to elementary and secondary teacher preparation programs, a third program has been added forspecific training in middle school methodology. this was a result of efforts to restructure teacher trainingprograms that followed the 1987 texas standards for teacher training programs. these programs are currentlyconsidered pilots for those to be adopted in the fall of 1995. final approval of the restructured programs willcome about in april 1995 when the tec conducts its program presentation to the texas commission onstandards for the teaching profession. the newly restructured programs are described as follows.young learners program. this program is designed for candidates in texas a&m's teacher trainingprogram who seek certification in grades k through 4. the young learners program has methods and fieldexperiences designed to address the needs of educating the young child. there is a preprofessional sequence ofcourse work that teacher training candidates must complete before being admitted to the professional sequence,which is designed to cover three semesters of professional development course work, including field experiencesthat give progressive teaching responsibility to the preservice teacher.preprofessional sequence of courses išchildren, families, and communities . methods courses focus onlanguage and literacy, educational psychology, and human development. one course serves as a place holderwith a field experience requirement. students are required to work with families, agencies, and services in theirfield experience.preprofessional sequence of courses iišchildren, schools, and society . the second suggested sequenceof courses before admission to teacher education focuses on the foundations of education in a multiculturalsociety and understanding special populations.professional semester išadmission to teacher education: focus on young learners, teachers, andschools. upon admission to teacher education, preservice teachers sign up for a 12hour block of courses with afield experience requirement. different tec sites offer different beginning field experiences, but all requirestudents to interact with children in a tutorial arrangement. these tutorial mentoring experiences have a varietyof titles and themes. all reflect the needs of the individual campuses. writing buddies, reading buddies, aggiefriends, math magicians, and host volunteers are some of the programs that serve as vehicles for more than125 preservice teachers at a time on each campus. it is a winwin situation for the schools and for the teachertraining program. preservice teachers are strongly encouraged to take a university course in microcomputerawareness during this semester.professional semesters ii and iiišfocus on young learners, teachers, and schools. preservice teachersenroll in a 12hour block that includes a practicum experience focusing on working with and teaching smallgroups of children. students are given increased experience in instruction that prepares them for the studentteaching experience in the third semester.integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program571the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.middle school program. this program is designed for candidates in texas a&m's teacher trainingprogram who seek certification in grades 4 through 8. the middle school program has methods and fieldexperiences designed to address the needs of educating the young adolescent. there is a preprofessionalsequence of course work that teacher training candidates must complete before being admitted to theprofessional sequence. the professional sequence is designed to cover three semesters of professionaldevelopment course work that includes field experiences at the middle school, junior high, and high schoollevels. teaching responsibilities are progressive and offer a variety of settings.preprofessional sequence of courses išchildren, families, and communities . the 6 hours of suggestedcourses focus on language and literacy, and adolescent development. one of the two courses serves as a placeholder with a field experience requirement. students are required to work with families, agencies, and services ina middle school setting. they become part of a school families program, shadowing various school personnelduring the semester. this could be working with the school nurse, secretary, counselor, truancy officer, principal,or special education teacher.preprofessional sequence of courses iišchildren, schools, and society . the second suggested sequenceof courses before admission to teacher education focuses on the foundations of education in a multiculturalsociety and understanding special populations. a course in microcomputer awareness is a requirement.professional semester išadmission to teacher education: focus on middle school students, teachers,and schools. upon admission to teacher education, preservice teachers enroll in a block of course work thatincludes a field experience requiring assignments that integrate the curriculum. these courses are taught at aschool site.professional semesters ii and iiišfocus on middle school students, teachers, and schools. blockedcourses are similar to the young learners program except that specific methods courses are designed to addressthe needs of the middle school student.secondary program. this program is designed for candidates in texas a&m's teacher training programwho seek certification in grades 9 through 12. it consists of four phases and two practicums (the practicumsoccur in phase iv of the program). the use of technology is stressed in phases ii through iv. in phase ii the preservice teacher learns about technology and how to use it. in phase iii, preservice teachers study how studentscan learn from technology, and in phase iv of the training program, preservice teachers study how their studentscan learn with technology as an integral part of their instruction.phase išselfdirected experiences with children. students document their experiences in a variety ofsettings before admission to the teacher education program in phase iii.phase iišunderstanding teaching and schools in modern society. the goal is for preservice teachers todevelop an understanding of responsibilities of the teaching profession. preservice teachers shadow professionalteachers working with students in the middle school, junior high, and high school settings.phase iiišdeveloping skills to meet the needs of society and students, admission into teacher education.the goal is to develop teaching skills responsive to the cultural styles, values, and identity of children and adultsfrom all racial, ethnic, linguistic, class, and religious backgrounds regardless of learning and/or behavioralcharacteristics.phase ivšdeveloping and demonstrating skills to organize content and technology for use inclassrooms. the goal in the first practicum experience of phase iv is for preservice teachers to develop skills inorganizing content and technology for use in classrooms. the goal of the second practicum experience is todemonstrate skills of organizing content and technology. preservice teachers demonstrate proficiency ininstructional skill during this practicum (similar to the traditional student teaching experience).additional staff and role changesas stated above in the description of collaborative activities, current pilot teacher training programs areaddressing texas education agency's expectations in restructuring teacher education. the need for coordinationintegrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program572the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.of collaborative efforts between tec sites and university teacher training programs has required the addition ofpersonnel to ensure progress toward meeting the goals of the tec as well as the different expectations ofcooperating teachers, university supervisors, and student teachers: role and requirements of the university liaison. the university liaison serves as a facilitator of informationbetween the collaborative, the assigned site, and the universities involved. the site assesses its needs; theliaison and site coordinators work collaboratively in finding solutions to meet the needs identified. requirements of cooperating teachers who have a tec student teacher . cooperating teachers agree topromote and learn along with their student teachers the uses of technology and innovative teaching practicesin their classrooms. they agree to assist in guiding the student teacher to plan and implement technologyand innovative practices in the classroom. requirements of student teachers assigned to a tec site. student teacher candidates agree to the following:šdemonstrate a strong interest in learning and using technology and innovative teaching practices in theclassroom;šdesign a unit that has a multimedia component and/or interdisciplinary focus; andšincorporate technology in classroom activities such as record keeping, lesson plans, instructional activities,and student projects.following are examples of the number of preservice teachers affected by new fieldbased programs at tecsites: south knoll elementary school (125 preservice teachers): writing buddies program, a tutorial program. southwood valley elementary school, college station (20 preservice teachers): preservice teachersinterested in the young learners program enroll in two blocked courses taught by a team of two facultymembers and a clinical faculty member from the elementary campus. jane long middle school, bryan (40 preservice teachers): as part of a methods course requirement, preservice teachers observe and assist in all aspects of a school. they shadow different personnel, observing avariety of roles. some examples include the school office staff, the counselor, a physical education andwellness class, inschool suspension, and/or a classroom one day per week. this fits with the theme of phasei of the middle school program, which focuses on children, families, and communities. these same studentsobserve life at an elementary school on a different day each week. a&m consolidated high & college station junior high schools (40 preservice teachers): students in thismethods course, which is designed for preservice teachers interested in secondary education, work in twoschool environments. twenty students are located at the high school and participate in observation andassigned activities that involve all functions of the school. another 20 students participate in similaractivities at the junior high campus. at midterm, the groups switch. the field experience is designed to givestudents the opportunity to study adolescent behavior, develop an appreciation for human diversity, andmodel professional behavior. crockett elementary, bryan (27 preservice teachers): students enroll in 15 hours of methods courses. theyare assigned to specific classes with instruction conducted by texas a&m faculty and clinical faculty. theyparticipate in this experience prior to their semester of student teaching. this is just a short overview ofsome of our site activities. somerville middle school block, somerville (27 preservice teachers): students are enrolled in five coursestaught by clinical faculty who are mentored by a professor from texas a&m. the preservice teachers arein the field every afternoon 4 days a week. they participate in classes and followup seminars after schoolwith clinical faculty. somerville high school: integrated units, integrated class curriculum, team planning every day, blockscheduling.integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program573the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.other tec sites are participating in the collaborative in different ways. they either have student teachersand are actively involved in redesigning student teaching experiences, or they participate in compressed videolectures by doing projects with classes at different sites in the collaborative. the last 3 years have brought aboutmany significant changes in the way we teach at the university and in the way students are taught in the schools.staff developmentactivities have included inservice training for university faculty, teachers, student teachers, andundergraduates in the use of macintosh computers, and computer software such as hyperstudio, clarisworks,inspiration, and filemaker pro. staff development is also offered in email, internet, telnet, compressed video,development of interdisciplinary instruction, understanding diverse cultures, and mentoring preservice teachers.at one site, 5th, 6th, and 7thgrade students are trained to be a part of the school's "tech team." they helpteachers and students with technology in their classrooms. they also learn how to use telnet and how to accessinformation via the internet.technologydistance learningcompressed video units located at five tec sites are currently used for instruction and conferences betweentexas a&m university and the sites. they are also used for student projects collaboratively planned betweenclassrooms at different sites.some examples of ongoing and future projects include the following: the "global village" project is currently conducted through the center for international business studies,texas a&m college of business administration. some of the topics covered in past compressed videosessions have included (1) comparison of communism, socialism, and capitalism, (2) nafta, (3) theeuropean economy, and (4) japan. the "totally texas" project is a collaborative effort of people from several tec sites who are creating anintegrated multimedia curriculum for 4th and 7th grades. the material will be stored on cdrom. sitesconnect via the distance learning system either weekly or biweekly to discuss the various units each site willcontribute to the curriculum. teachers share ideas and resources with each other without having to travelgreat distances to do so. distance learning was used in conducting a presentation on technology usage to legislators in austin.legislators were able to talk directly to students and teachers at tec sites regarding their use of distancelearning and computer programs in instruction. students and teachers were able to demonstrate variousstudent projects and computer programs to tec sites and legislators at the same time. eight sites throughouttexasšlubbock, laredo, the state capital in austin, and the five tec sitesšwere online at the same time. a teleconference via compressed video is planned between students in sweden and students at a 4thgradeclass in bryan, texas. the students plan to exchange information about a typical day in their schools andtowns. this project is being funded by the swedish government. another distance learning project includesstudents from somerville, texas, sharing their multimedia projects with students in laramie, wyoming.internetseveral turkish schools will have connections to the internet in early march with students in texas. theywill become "keypals."integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program574the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.multimedia projects for the classroomundergraduates enrolled in a technology course on the texas a&m campus meet via compressed videowith practicing teachers at a tec site to develop instructional materials that support interdisciplinary themes.materials have been created to support such themes as "africa" and "westward expansion."integrating technology with practice: a technologyenhanced, fieldbased teacherpreparation program575the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.64regnet: an npr regulatory reform initiative toward nii/gii collaboratoriesjohn p. ziebarth, national center for supercomputing applicationsw. neil thompson, u.s. nuclear regulatory commissionj.d. nyhart, massachusetts institute of technologykenneth kaplan, massachusetts institute of technologybill ribarsky, georgia institute of technologygio wiederhold, stanford universitymichael r. genesereth, stanford universitykenneth gilpatric, national performance review netresults.regnet andadministrative conference of the united states (formerly)tim e. roxey, national performance review regnet. industry, baltimore gas and electric, andcouncil for excellence in governmentwilliam j. olmstead, u.s. nuclear regulatory commissionben slone, finite matters ltd.jim acklin, regulatory information alliancesummarythe premise of this paper is that the pressures of u.s. growth into a national economy, associated societalcomplexities, and advances in present information technology are rapidly leading the united states out of thepostindustrial age and into the information age. the relationship between the various components of ourgovernment and its citizens in an information age are substantially different from those found in the postindustrial age. the actual structure of our government has been tuned for an industrial age level of societalcomplexity and technology sophistication, and it is being challenged by current conditions.given that the preamble to the constitution of the united states was probably the first written occurrence ofa national goal and objective, the original structure of the government was crafted so that the implementation ofgovernment, or governance, allowed citizens to achieve the preamble's intent. for individuals for whom this wasnot the case, governmental process allowed changes to be made.regulatory reform initiatives today, especially those involving the implementation of technology, have asignificantly greater degree of difficulty in being able to allow citizens to enjoy the intent specified in thenational goals and objectives. this is predominantly due to the lack of a national information plan that dealsspecifically with mapping citizens' needs and rights to regulatory information to national goals and objectives.when an entity consistently fails to deliver to its customers the products that allow the customers to enjoy theachievement of the entity's goals and objectives, then that entity is doomed to fail. if the entity is a federal, state,or local government, then that entity is not permitted to fail and action is required to be taken.in the following sections, there is a development of several key attributes. these attributes are complexitiesin regulatory interaction, complexities in implemented levels of technology, and unit of governance involved inthe interaction. the discussion shows that when societal complexity and/or technology advances causegovernment to be engrossed in too fine a level of detail that is too fine, then the structure of government changesto accommodate the complexities. the most dramatic revision so far has been the development of the fourthbranch (referring to the creation of the regulatory agencies and programs) of our federal government.it is time now for the various levels of governance to redefine themselves and their customers and return toa citizencentered philosophy.regnet: an npr regulatory reform initiative toward nii/gii collaboratories576the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.statement of the problemperhaps to fully understand regnet, it is best to understand regnet's customers and the problems that theyexperience. first and foremost among the customers is the private citizen in the united states who is seekingregulatory information or to have an impact on changing the system. next there are citizens who formthemselves into public interest groups, typically centered on a particular topic or concern. then there arecorporationsšboth forprofit and notforprofitsšthat represent various commercial or private interests. finally,there are various agencies of government at the local, state, and federal levels.each of these customer groups will pursue regulatory information for different reasons. the individualcitizen may seek regulatory information to achieve some level of assurance that the regulations are safe and saneand adequately deal with the citizen's concerns. public interest groups will typically look through regulations inorder to find out how the public might better comply with them. in some public interest groups, the intent is tobecome better aware of the regulation in order to lobby industry or government to more effectively meet therequirements. corporations will use the body of regulatory information in their daytoday operations to remainin compliance with the rules as they pursue their corporate goals and objectives. finally, the various regulatorswill look at their regulations in one of two different ways. the first is in their role of oversight, or watchdog, andin this case they will review regulations to inspect or enforce them on the industry sector they oversee. thesecond way the government uses the regulatory information is with the intention of finetuning or improving it.unfortunately, because of the vast complexity of government today, these regnet customers typicallybecome frustrated in their pursuit of information. this complexity stems from the many agencies andorganizations (international, federal, state, local, and tribal) that issue regulations independently and withoutconsistency. the breadth of this problem encompasses all regulators and regulated entities (industry,government, and the private sector).this frustration of dealing with government is complicated by the inability to deal with the immensecollection of regulatory information using present paperbased methods of access. in addition, thecommunications between people and their government is limited by technologies (telephones, fax, personalmeetings) that have not been able to deal with the enormity of these complex information structures.the primary cause of this frustration is that the national goals and objectives are not being met by thecurrent implementation of technology.this discussion, though complicated and fundamental, is centered on the least complex form ofgovernmental interactionšthe onetoone model. this simplest interaction is used to enforce rules and to gatheror publish information. the other three forms of interaction, discussed in the background section below, are allmore complex than the previous.all four of these levels of regulatory interaction have serious problems associated with them. how do weknow that this is the case? customer satisfaction is low, costs to maintain the bureaucracy are high, and theeffectiveness of many regulations is poor to nonexistent. even the ancedotal stories of regulatory abuses andexcess are making for bestselling novels these days. the solution is to revise the regulatory structurešthecurrent effort is called either reinvention or reengineering. at best, reinvention will blend intelligenttechnological implementations with thoughtful structural modifications in such a way that quality regulatoryservices are delivered to as wide a customer base as possible. this is the socalled citizencentered form ofgovernance.backgroundthe six planning and demonstration projects contemplated in this paper are best understood as having threeconceptual dimensions, as shown in figure 1.one dimension is technological. each project is defined as having a locus on a spectrum of increasingsophistication in communication and information technology. by accurately situating a given project on the itsophistication dimension, we can begin to see the interrelatedness of all six projects. we are also able to resolvequestions of how, when, and wherešwith respect both to conducting the project itself and to operation of theproject's end product by its eventual users.regnet: an npr regulatory reform initiative toward nii/gii collaboratories577the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.a second dimension relates to regulatory functions. each project has its space on a spectrum of increasingforum complexity or multiplicity of interested entities. again, by carefully defining the position of a particularproject along a dimension of regulatoryfunction complexity, we can appreciate project interrelatedness. we canthen address the issues of who will or should be involved in project operation and eventual product use, and whatexactly they will or should do.a third conceptual dimension embraces the entire list of governmental units and agencies, graded byincreasing scopešlocal, county and municipal, tribal and state, federal and internationalšeach governmentalunit with a discrete set of "customers" and entities subject to its authority. by relating our six projects to such aspectrum of agencies and associated client sets, bearing in mind national goals and objectives, we can approachan understanding of why these proposed projects are crucial and should be funded.figure 1 complexities owing to three conceptual dimensions of demonstration projects.xaxis: information technologyinformation (the content) and technology (the conduit) have been with us for a long time. no doubt,properties of the conduit have always had a heavy effect on the content, just as qualities and purposes of thecontent affect conduit developments. acknowledging the reciprocal relationship and importance of the two, it ishelpful to keep them distinct in analysis. to the degree that conduit and content can be considered separately, theformer is discussed in this section. the following section, on regulatory functions, treats content. following that,in a section on governmental units and their customers, we treat contextšthe considerations that give relevanceto conduit and content.students of the human history of information often parse the subject into four general periods, reflectingavailable and utilized technologies. earliest was an oral or bardic culture, when human memory was the chieftool for information exchange (perhaps supplemented by musical instruments, cave paintings, and carvedobjects). later, scribal culture arose, adding to the serviceable toolkit such devices as pen, brush, ink, paper, andexpert familiarity with their combined use. in the fifteenth century, the use of movable metal type in printingpresses changed human society fundamentally. more recently, the theory goes, a similarly profound shift hasstartedšdigitization of information. it will affect human thinking, working, playing, and relationships inunknowable waysšregarding entity boundaries and properties, groups, ownable or meaningful objects,transactions, and disputes about all of the above.the foregoing historical framework is a useful aid for understanding issues we face today. our problemsstem in significant part from cultural conflict. the entities, habits, tools, methods, and competencies of printregnet: an npr regulatory reform initiative toward nii/gii collaboratories578the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.culture are in the process of incomplete and uneasy evolution into those of digitizedinformation culture. letterpress was the first technological invention enabling the print or book period. photography, telegraph, typewriters,photocopiers, and faxes expanded the power of printed word. those methods form the core of a conservativemodel of it sophistication.technological advances in the recent two decades and growing user familiarity with those methods suggestthat there may be a very detailed spectrum of sophistication in information technology, but we offer below asimplified scale for sorting the proposed six projects.we describe four marks on the spectrum of information technology sophistication:1. conservative model of it sophisticationšpaper documents, photocopying, telefax and hardcopycommunications, physical libraries, facetoface meetings and forums.2. progressive standard model of it sophisticationšelectronic documents and copying, email and filetransfer communications, nonphysical libraries and transaction forums (e.g., bbs, commercial onlineservices).3. emerging model of it sophisticationšelectronic information objects and copying, graphical datarepresentation, nonimmersive virtual reality techniques, virtual transaction forums (e.g., www, mosaic).4. advanced model of it sophisticationšvirtual 3d interfaces and data representation, competent aiagents, immersive vr.yaxis: regulatory functionsbefore discussing regulatory functions, we should step back and look at functions of government morebroadly. regardless of conduits available, one function of rulership is the gathering and publishing ofinformation. ancient polities in egypt, mesopotamia, and rome, whatever their varied structures and operationaldifferences, all gathered information from their subjects and the world at large, processed it in some fashion, anddisseminated it. information typically collected would include, for instance, head counts, land boundaries, talliesof wealth and capability, and assessments of intention. disseminations commonly took the form of rules, orders,decisions, and descriptions.more modern governmentsšfor example, those of florence in the 1500s, france in the 1600s, and britainin the 1700sšwere information entities just as the ancient polities had been, but the moderns had aquantitatively different problem. often they had larger populations and accumulations of wealth, moresophisticated communications technology (conduits), and certainly more complex data, both to gather and topublish. those modern polities handled the increased volume and complexity, generally, by inventing tiered andnested organizational hierarchies with specified subject matter authorityšoffices, bureaus, ministries, andcommittees.in the 1780s, the founders of the united states chose a threepart form of government that we are allfamiliar with: the legislative, executive, and judicial branches. one was to write laws, another to administerthem, and the third to resolve cases and disputes. each functional power was separated from the other two,counterweighting and limiting them. our country's founders were not openly concerned with informationalvolume and complexity. they aimed instead to secure entities and their properties by averting in the long termboth the tyranny and the license, respectively, of too little and too much liberty.that constitutional division of government functions has served us remarkably well. however, though itmay well have sufficed for the degree of informational complexity foreseen by the founders, it did not anticipatethe industrial revolution and associated advances in information technology that would unintentionally engenderan extensive increase in complexity. when in due course informational volume and complexity became too greatfor government to handle, threatening to overwhelm resources of the federal executive, judiciary, and legislaturewith details, a fourth branch of national government was invented: the administrative and regulatory agencies(figure 2).regulation as we understand it today was largely unknown during the first century of this republic.individual states had engaged in economic regulation for a long time (of turnpikes, bridges, mills, foodstuffs,civic hygiene, and licensing of physicians, for example). but federal involvement with private productiveenterpriseregnet: an npr regulatory reform initiative toward nii/gii collaboratories579the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.was limited to canals, the national highway system, and banks. a strong revenue base, a trained civil service, andsystematic economic informationšnecessary conditions of a modern regulatory statešwere lacking until afterthe civil war.figure 2 three branches of government plus administrative and regulatory agencies as a fourth, to handleincreasing volume and complexity of information.requirements of organized ingenuity, production, and distribution for nationwide conflict combined afterthe civil war with private economies of scale and a competitive, almost predatory work ethic (expressed in thedoctrines of social darwinism and laissezfaire). results of the combination were dense concentrations of wealthand productivity in transportation (rails, shipping), commodities (oils, steel, sugar), and finance. also, significantabuses of market freedom occurred, some groups were treated with shocking unfairness, and gilded figuresflaunted their success and power. the corrective was a wave of regulation. it started tentatively with theinterstate commerce commission (1887) and the sherman act (1890) and continued, massively, through theprogressive era (1902œ14).two other great regulatory expansions happened later. one we call the new deal (1933œ38). chiefly, itreallocated power among economic interest groups. the other began in the late 1960s and lasted about a decade.in it, political issue groups sought substantial reform of civil, social, and consumer rights, environmentalprotection, and renewed political responsiveness after vietnam and watergate. between each of the majorexpansionary waves, there were interglacials (so to speak) when implementation, adjustment, consolidation, andreassessment took place with respect to the preceding regulatory impetus. the oscillatory cycle evidently runslike this: perceived abuses or inequity prompt disillusionment with a free market; regulatory discipline is appliedregnet: an npr regulatory reform initiative toward nii/gii collaboratories580the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.to remedy the market failure; inefficiency or ineffectiveness in due course spoils the regulators; and marketefficiencies and discipline are pressed forward to correct the regulatory failure.as we have suggested, administrative and regulatory agencies arose in part as a consequence of thismounting complexity of information. in addition to the function of information gathering and publishing,agencies typically included the three constitutional functions. the traditional and general branchesšexecutive,judicial, and legislativešin effect allowed a limited portion of their respective functions to be delegated toexpert and focused subordinate agencies. in concept, these agencies would spare the executive branch, the court,and the congress from detailed and repetitive chores of administration, dispute resolution, and rule making. inthe ordinary scheme, agencies would report to both the executive branch and congress, and agency decisionswould be subject to review by the court. an agency's four functions would each connect and operate in differentways with its customer set, from an information base held partly in common.these four functions are described below.1. a onetoone exchange between entities. the most primitive level of forum complexity is a onetooneexchange (figure 3). two entities, face to face, engage in a transaction. entity and face may be virtual orelectronic; matter transacted may be hard copy or digitized; an entity may constitute a group ofsubentities or itself be a subentity of a larger organization; the transaction may be asynchronous or realtime, enduring a millisecond or a decadešnone of these local circumstances alters the essential topologyof the onetoone exchange. an agency's information gathering and disseminating function fits cleanly atthis primitive level. when a private entity seeks information from an agency (a foia request, say, or ageneral inquiry about policy and procedure), or when an agency asks the private entity to forwardinformation (a tax return or application form or compliance data, for instance), the relationship is onetoone. much of an agency's administrative function also fits in this category. investigation, inspection,granting of licenses, and specific enforcement of rules all can be visualized as an administrative officeron one side of the table and a regulatory subject entity on the other.2. a onetotwo relationship. slightly more complicated is the onetotwo entity relationship (figure 4). it isthe model for most adjudicatory or disputeresolution proceedings. a judge or hearing officer or neutralobserver sits on one side of the table; on the other side sit two adversarial parties. there may beexchanges between the two adversaries (e.g., discovery or settlement discussions), but transactions withthe forum officer tend to be bound by formal requirements. traditional adjudication and alternativedispute resolution (adr) are both accommodated in this category.3. a onetomany relationship. this category comprises traditional rulemaking scenarios or anyproceedings in which a single officer or official body sits on one side of the table and multiple interestedparties sit on the other (figure 5). again, it does not matter that the commissioners transact decisionalinformation among themselves or that groups of interested parties form coalitions and exchangeinformation. the topological complexity is formally undisturbed: one to many.4. a manytomany configuration (figure 6). the manytomany relationship can be conceptualized in twoways. it can be seen as an extreme development of the onetomany configuration, when the singleofficer on the one side of the table is reduced in authority from central decisionmaker to forummoderator, meeting president, or coequal participant. a legislative assembly or negotiated rule makingfits this pattern. alternatively, this category can be visualized as a roundtable forum without chairingofficer or moderator and with all entities having equal power to affect decisions of the forum.our account of regulatory development is intended as a summary and overview, and may be too simplified.there have been many efforts over the years to reform regulationšboth its substance and its proceduresšinnarrowly confined areas within a given agency and across broad sweeps of government. certainly, our shorthistory gives no idea of the extensive variety of regulatory function practiced today and the detailed articulationof those functions with private entities. in important ways, the detailed variety and extensive articulation areaspects of the problem that our six proposed projects are designed to address.for the purposes of this paper, we have contrived a measuring system for the regulatory function dimensionof analysis, a sequence of check marks. it accommodates all of the variety of agency functions as well as themyriad contemporary articulations of regulator with regulate or other external entity. it does so in aregnet: an npr regulatory reform initiative toward nii/gii collaboratories581the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 3 a onetooneexchange.figure 4 a onetotwo relationship.fashion that seems sensible and accessible both to students of governance and administrative law and toexperts in communications and information technology and software engineering. it may be that there are manyworkable measuring schemes. we develop one below, borrowing substantially from formulations inmicroeconomics, information physics, and psychology.regnet: an npr regulatory reform initiative toward nii/gii collaboratories582the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 5 a onetomany relationship.zaxis: government agencies and customersa detailed exploration of business process reengineering or government reinvention is beyond the scope ofthis paper. but two central principles from those undertakings inform our discussions here: (1) productiveentities or units exist, and their current boundaries and methods of operation often reflect historical accident,ingrained habit, or risk aversion; and (2) both entity boundaries and productive processes frequently need to bereengineered so as to be customer driven, market and competition oriented, enterprising, decentralized, andcommunity responsive.understandings of public entity ''effectiveness" and "efficiency" differ from those applicable to private,profitmaking entities. and the concept of a public entity's customers or clients is not the same as that applyingto a private entity. still, there are strong analogies that assist analysis.we hear anecdotally that some 82,000 governmental units have been identified within the united states (notincluding metanational entities, such as treaty organization or international disputeresolution bodies). of course,each governmental unit has various sets of clients or customers or respondent entities grouped according tosubject matter issues for formal proceedings. a comprehensive map of these groups would be dizzyinglycomplex, but a simplified scale is developed below that allows us to tear apart the complexity, to locategovernmental units and their customers along a spectrum of entity scope, and to describe regions where each ofthe six proposed projects are relevant both as to project operations and as to endproduct users.regnet: an npr regulatory reform initiative toward nii/gii collaboratories583the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.there are four check marks on the government entity axis:1. local governance and associated customers (e.g., ptas, neighborhood councils, tenant associations,town meetings, and subcounty authorities),2. counties, cities, or municipalities (e.g., government offices, school boards, or zoning and naturalresources authorities),3. tribal or state governance units (e.g., courts, agencies, legislatures, and executives), and4. national or international governance units (e.g., the federal sovereign and its branches, and internationalentities).figure 6 a manytomany configurationthe planthis discussion includes the development of a comprehensive national plan called regnet that supportsnational goals and objectives vital to economic prosperity and security. the regnet national plan integrates (1)ongoing reinventing government and regulatory reform initiatives, and (2) advanced highperformancecomputing, communications, and information technology with (3) intergovernmental, interacademic,interindustry, and public interests. this plan is an integral part of the proposed technology development anddemonstration projects initiated in this paper.the following is a summary of the regnet national plan attributes, which reflect human and technologicalneeds, two separate but interdependent requirements to meet in furthering national goals and objectives.regnet: an npr regulatory reform initiative toward nii/gii collaboratories584the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.regnetša human and technologybased definition the regnet national plan is a system that links human communications and information (hci) needs toregulatory information and regulatory processes (rirp) through highperformance computing,communications, and information technology (hpccit). it is multilevel based (intergovernmental, interacademic, interindustry, and intercitizen). it supports all users who want or need to access regulatory information and/or to participate in regulatoryprocesses (international; national, including federal, state, local, and tribal governments; industry; and thepublic)šfor example, all regulators and all regulated entities (industry, public, private, and nonprofitsectors; and the public). it is a national, integrated, intelligent, distributed, multimedia, world wide web (www)based regulatorycommunications and information system and people network.regnetša citizenbased definition under this definition, regnet is an advanced regulatory information system that provides the public andindustry with easy access to onestop shopping for the regulatory information and participation needed to (1)reduce regulatory burden, (2) increase efficiency, effectiveness, and profitability, and (3) become moreeconomically competitive both nationally and globally for economic growth. it provides full access to (1) all intergovernmental regulatory information (existing laws, rules, andregulations), and (2) communication links for participating in regulatory processes (regulatory compliancereviews; making, modifying, or eliminating laws, rules, regulations, policies, standards, guidelines, andpositions; and alternative dispute resolution and negotiations).regnetša concept and initiative for regulatory reform and economic development regnet is an interdisciplinary consortium of intergovernmental, interacademic, interindustry, and citizenentities. it is a grand challenge (as defined in the high performance computing act of 1991). it supports national challenge applications (as defined by ostp, national coordination office for hpccinformation infrastructure technology and applications). it is intelligent (as defined by arpa and others). it is distributed (globally). it is intergovernmental (international, federal, state, local, and tribal governments), interacademic,interindustry, and intercitizen based. it provides citizen and industry access to regulatory information. it provides opportunity for intercitizen, interindustry, and intergovernment participation in regulatoryprocesses and in regulatory reform. it is of the people, by the people and, for the people. it requires cooperation, collaboration through coordination, and integration at all levels (see citizencentricmodel). it is a comprehensive regulatory reform initiative that plans for the use of advanced computing,communications, and information technology. it supports all national and international user levels through the national information infrastructure (nii) andglobal information infrastructure (gii).such a plan implies the full integration of appropriate and applicable data, information, and material inspecific domains of interests. this is also the objective of the vice president's national performance reviewregnet: an npr regulatory reform initiative toward nii/gii collaboratories585the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.(npr) regnet projectšto coordinate the planning, development, and implementation of a distributed,intelligent regulatory information system. this plan will include consideration of both national and globalinformation infrastructure environments.this plan recognizes that the need for information and material in the private sector is different from that ofthe public and nonprofit sectors. for example, governments need information to develop laws, rules, regulations,policies, procedures, and practices, whereas the public and industry need access to government processes tobecome better informed and more able to participate in the decisionmaking processes that affect their future. inaddition, the private sector's needs derive from the desire to be in compliance with regulations and associatedprocesses. with information fully distributed to all interested parties, individuals will be adequately prepared tosupport national goals and objectives.deployment issuesonly nine participating hpccit federal agencies are coordinated by the national coordination office forhigh performance computing and communications (hpcc). this coordination is tied directly to the nation'seconomic policy in support of national high performance computing (see the high performance computing actof 1991). there are no other federal policy level offices responsible for coordinating hpccit national goals andobjectives, leaving more than 130 remaining federal agencies, departments, and commissions without hpccitpolicy coordination and integration. this lack of coordination and integration results in inconsistencies,inefficiencies, wasted time, and misdirection of precious human resources and funding. the eventuality andschedule for regnet deployment rest, in part, on enforcement of hpccit policy throughout the federalgovernment, but more importantly on intergovernmental cooperation, collaboration, coordination, andintegration, perhaps starting with the national governors' association.as it approaches the twentyfirst century, the united states faces challenges that transcend state lines andnational boarders. this country must shape economic policies for a competitive marketplace. regnet willcontribute to hpccit, nii, and gii by playing a vital role in meeting national economic and security goals andobjectives.the ncsa regnet project addresses and responds to these challenges by underpinning a plan anddemonstration project that is based on national goals and objectives (reinventing government, regulatory reform,grand challenge applications, and national challenge applications vital to national prosperity, including nii andgii initiatives).the issues are important ones for the country's federal, state, and local governments and, more significantly,for its people and businesses. the proposed projects directly support national goals and objectives of nii, gii,and intergovernmental regulatory communication and information needs. also, the following national challengeapplications have been identified by the hpccit subcommittee: public access to government information; health care education and training; digital libraries; electronic commerce; advanced manufacturing; environmental monitoring; and crisis management.this discussion supports the development of a national plan that would be able to encompass and embracedisparate demonstration projects affecting international, federal, state, local, and tribal governments as theyaffect american citizens. we describe five demonstration projects in federal, state, city, tribal, and ruralenvironments. in essence, a needs assessment of stakeholders has been performed by congress and the executivebranch (e.g., bills and hearings in support of the high performance computing act of december 1991, andsubsequent legislation, and executive orders). regulators, the regulated, and the public need easy access toregnet: an npr regulatory reform initiative toward nii/gii collaboratories586the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.regulatory information. intergovernmental regulatory collaboration that depends on this easy access toinformation requires the planned use of advanced communication and information technology initiatives. theseinitiatives have been supported in the vice president's npr, the office of science technology policy (ostp),industry groups, various federal regulatory agencies, and academic and research institutions. regulatory reformis under way and needs the vision, coordination, and integration planning recommended here.an intelligent, threedimensional visual model will be developed to help communicate this comprehensiveregnet national plan and to illustrate the communications and information interrelationships, including thoseamong the various government, academic, and industrial organizations and the citizen. the collaborativepartnership refers to this model as the citizencentric model since it focuses on the people of the united statespreliminary visual models have already been developed for previous npr meeting and proposals. the first is atwodimensional concentric circle diagram representing the regnet national plan gii. at the center of thisdiagram is the frustrated citizen desiring access to regulatory information. each succeeding concentric circlerepresents an organizational entity (level) that has access to the same regulatory information, and regulatoryprocess, as the citizen. combined, these circles (levels of users) represent a national regulatory informationinfrastructure. in turn, each country of the world (more than 180 countries) will eventually have an infrastructurein support of similar levels of regulatory information users and will become a part of the gii. a pieshapedsector of these concentric circles represents all regulatory information users in each state of the united states.these levels (international, federal, state, local, and tribal governments in addition to national laboratories,industry, research, and academic institutions) form the model for intergovernmental, interacademic,interindustry, and public coordination and integration of the regnet national plan. different organizationalentities within each state can have interrelationships both internal and external to that state. for other nations, themodel can be replicated with their appropriate organizational entities represented.a second threedimensional, intelligent, interface model will be developed for regnet. this visualizationmodel represents the entire intergovernmental intelligent distributed regulatory information system model. thegii represented here includes (1) an intergovernmental regulatory definition model, (2) an interindustry facility/product definition model, (3) a costbenefit, risk assessment, economic assessment definition model, and (4) ananalytical definition model. all regulations for all governmental bodies (international, federal, state, local, andtribal), all industries (all sea, land, air, and space industries), all hard analytical tools (structural, mechanical,electrical, architectural, illumination, acoustical, etc.), and all soft analytical tools (costbenefit, economic, risk,and environmental assessments) are incorporated into this intelligent interactive visualization model. theregnet national plan will incorporate regulations and regulatory processes using intelligent, distributedcomputer systems, will access the information from anywhere, and will make the information comparable andintegrable for the customers. these features include the integration of the enterpriselevel information andsophisticated human interface characteristics.the concept of an intergovernmental regulatory collaborative, called regnet by the npr, came about aftermany fragmented and independent attempts made by government, academic and research institutions, andindustry groups to integrate and coordinate regulatory communications and information needs of the regulator,the regulated, and public entities. regnet, as an intergovernmental regulatory collaborative partnership with thepeople and their institutions, will supply communications and information technology needs to a broad clientele(including research, development, and application teams). the initial concept of a "collaboratory" was proposedas a "center without walls, in which the nation's researchers could perform their research without regard togeographical locationšinteracting with colleagues, accessing instrumentation, sharing data and computationalresources, and accessing information in digital libraries." this discussion incorporates the "collaboratory"concept for use by all public, private, nonprofit sectors and the public.the regnet national plan encompasses all regulators and regulated stakeholders (industries and the public)under the reinventing government and regulatory reform initiatives of npr. it also will support researchers'needs for such collaboration. its grassroots base can provide generic distributed benefits across the nation. theplan will depend on a global network of computers to facilitate the performance of its functions and is envisionedto offer a complete infrastructure of software, hardware, and networked resources to enable a full range ofcollaborative work environments among intergovernmental, interacademic, interindustry, and public sectors.regnet: an npr regulatory reform initiative toward nii/gii collaboratories587the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.an nii initiative is being implemented in the united states, of which current, largescale regulatorymovements should become a part. the deployment and use of hpccit and the nii are both essential to meetinggoals and objectives in the global marketplace. regulatory reform is thus part of a movement with a greatpositive inertia behind it.the demonstration projectsfor the past two to three decades computing has grown from the dominance of large mainframe computersrunning in information system department glass palaces to desktop personal computers with slow processors tovery high speed performance through special handbuilt processors on small compact supercomputers to the eraof the microprocessor having extremely high power in a desktop machine. applications on these new desktopmachines scale easily to highperformance computers with many of these same microprocessors tied together. inthe past, people for the most part have been able to ignore the use of these desktop machines if they chose to, orif they worked in an environment where computers were absent. many professionals relied on their clerical staffto use computers for word processing and spreadsheets, but their own involvement beyond this level was usuallynot required and is still often avoided. traditionally, only those involved in science and engineering pursuedfaster and more widespread computing and networking technologies.beginning in 1993, when the national center for supercomputing applications released mosaic, a wwwbrowser, a fundamental and permanent change began to evolve in all sectors of society, which are now pursuingconnections to the internet and are installing currentgeneration computers at home and at work. this is nottaking place because of the computers themselves, not even because of the internet, since that has been aroundfor over two decades. what is different now is the human interface. through the use of www technologies andeastytouse graphical browsers, information and the ways we learn, discuss, and collaborate have becomeprimary commodities on the internet. nothing will remain the same in any segment of society from this dayforward because our society at large is on the brink of the information age. the "customers" within our societyare beginning to demand technology and the information it brings: students and parents are driving teachers to beable to access new information sources, teachers are driving administrators to connect schools, purchasers ofgoods and services are going to the www for products and information about products, and citizens arebeginning to view and communicate with the government electronically and are expecting services to beavailable via the internet.regulatory reform has been discussed for many years. regulations at the local, state, and national levelconsume citizens' time and money at an alarming rate and create for the nation a complex, unmanageable systemthat yields no obvious advantage. for example: edward crane, president, and david boaz, executive vice president, regulations, cato institute, describesregulation as "a costly, wasteful, and jobkilling burden on the economy" (washington post, february 6,1995). stephen moore, director of fiscal policy at the cato institute, calculates that regulations add as much as 33percent to the cost of building an airplane engine and as much as 95 percent to the price of a new vaccine. cost to the economy of regulationš$880 billion to $1.65 trillion per year, according to william g. laffer ofthe heritage foundation. cost to the economy of regulationš$400 billion, or roughly $4000 per household, according to thomashopkins of the rochester institute of technology. a tax foundation report found that the tax compliance burden is 10 times heavier on a corporation with lessthan $1 million in sales than on one with $10 million in sales.information technologies and the tools of communication across the www, which are evolving at anincredible rate, have finally given the nation hope that regulatory reform is possible. it would be total chaos totry to address every possible demonstration project of how technology can be used to solve or overcomeobstacles to regulatory reform. our decision in this paper and in the related documents has been to choose fiveprojects thatregnet: an npr regulatory reform initiative toward nii/gii collaboratories588the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.will bring together the primary tools as they are now viewed and to apply them to significant but reachable goalsand demonstrations. the partners in this set of demonstrations represent industrial, academic, and governmentorganizations that will form the basis for extended collaboration across the nation by all segments of society onregulatory problems in every area of society.collaboration software is one cyberspace tool that will become as common as electronic mail for users injust a few years, and rule making is one important regulatory activity that will benefit from collaboration acrossthe internet. with current and evolving tools, the www will allow various segments of society to work together,electronically, to decide on a regulation and to automatically manage the process of consensus and measurementrelated to a proposed rule. virtual negotiations and adjudication of regulations, all of which will be electronicallyavailable, will allow all segments of society to play a part in the regulatory process. with regulations becoming apart of an openly available electronic database for the nation, it will become essential that access to them beginto be automated and intelligent and that analysis toolsšfrom 3d visualization tools that are accessible acrossthe www to virtual reality (vr) analysis tools, which require immersion and highperformance computingšbecome automated and effective. the demonstration of these tools in a rural government test bed is essential andputs measurable and meaningful results out for open debate and evaluation. it must be investigated how thesetools can be used effectively by the general citizenry of the nation in real everyday situations requiringinteraction with government at all levels.these projects will push the limits of available technology in a regulatory environment and will drive futuredevelopments tying technology to information and regulations. the collaborating institutions represent a broad,experienced basis of institutions for other academic, industrial, and government organizations to becomeinvolved with in research and demonstration collaborations.the five technology demonstration projects represent the kinds of technologies that are able to reshape theway people interact with their government (human interface and visualization technologies). these projects,along with the regnet national plan, described above, provide a success path that governments at all levels canfollow to move their regulatory functions effectively into the information age. other methods that do notinvolve a linking plan and demonstration projects are prone to be less successful.these projects were selected because there is some existing development under way and because there is aneed to integrate their technologies into the overall nii plan. in addition, there is a requirement to formallyunderstand the links between these projects and the national goals and objectives. these projects will leveragethe existing nii activities (hpcc, atp, trp, etc.) it should be noted that these projects leverage significantinvestments from industry, notforprofits, and other publicly interested activities.demonstration project one: collaborative participatory rulemaking environmentšrulenet regulatory complexity: significant technological complexity: emerging government unit: federal principal investigator: bill olmstead, u.s. nuclear regulatory commission (u.s. nrc)this project will develop and demonstrate the concepts associated with a collaborative rulemakingworkspace utilizing the internet and www technologies. this type of workspace is being developed by anintergovernmental partnership along with some interindustry support. one of the first uses for this type ofworkspace (that is, the content for the rule making workspace) is in the area of collaborative participatory rulemaking in the nuclear energy domain. this places this project at the significant level point on the regulatorycomplexity axis and at the emerging point on the technological complexity axis. additionally, since the topicunder discussion will be some proposed rule within the nuclear industry domain, regulated by a federal agency(the u.s. nrc), this places the project at the federal level on the governmental unit axis. being at the federallevel of primary governmental involvement means that there can potentially be many thousands of participants onregnet: an npr regulatory reform initiative toward nii/gii collaboratories589the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.one side of the discussion and several (regulators) on the other side. additionally, there can also be participationboth by industry consortia groups and by public interest groups.this type of workspace environment will be central to many of the national challenge applications wherepublic interactive access is a requirement. this project is designed to test technology as it applies to this mode ofregulatory interaction.this first project actually is composed of two primary work efforts, a part that has already been planned andis being funded now (caught on the fly as it were) and extensions to the original planned work scope that willallow this project to more readily fit within the overarching regnet scheme. by capturing the first part of thisproject in this way, it is possible to build quickly upon the results of already planned activities with minimalinterruptions to work scopes and schedules and to gain maximum benefits for the overall efforts.the intent as represented here is to more fully support the initial, already planned phase of this project andthen add to it the pieces that allow the links to the national goals and objectives, the national regulatoryinformation plan, and the other projects within this grouping.phase one: activities captured on the flythis first phase of the project represents a new initiative with u.s. nrc. but lawrence livermore nationallaboratory (llnl) has conducted two previous electronic forums or dialogues in partnership with the npr: thevice president's electronic open meeting in december 1994, and the healingnet initiated after the april 19,1995, bombing of the federal building in oklahoma city. a third electronic forum is being built for thediscussion of federal acquisition reform (farnet, one of the several ongoing npr netresults activities).llnl will provide all of the technical and administrative support for the rulenet project including, but notlimited to, document preparation and conversion to html and other formats as appropriate, technical expertisein the form of an expert/facilitator, technical analysis and categorization of comments, "listproc" distributions,www home page development, mosaictype server with file transfer protocol and telnet capabilities,telecommunication links, document analysis tools, creation of separate electronic meeting areas where registeredmembers may caucus, and other tasks as necessary. it is necessary for the contractor to assign technical staff toperform the tasks as outlined. each staff member will have the appropriate expertise to accomplish their functionwithin the assigned task. the key position of expert/facilitator is considered essential. upon the acceptance of theproject by the u.s. nrc and doe, and acceptance of u.s. nrc funding by doe, llnl will mobilize a teamconsisting of engineers, computer scientists, graphic artists, technical editors, and a facilitator/moderator/coordinator to design and lay out the rulenet www site. this task will consist of six subtasks:1. develop and implement a process to identify and register rulenet participants.2. develop and implement a reference library with word and subject search capabilityšsearches will beintegrated over wais indexed information; gopher documents; and inquiry indexed information.3. develop and implement rulenet www site, anonymous file transfer protocol, gopher, and listproccapabilities.4. receive, categorize, and clarify rulenet comments.5. provide comment analysis and resolution of conflict guidelines.6. conduct electronic forum.phase two: extensions for regnetthe second phase of the work effort is tailored to three areas:regnet: an npr regulatory reform initiative toward nii/gii collaboratories590the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. automated management and consensus development; metrics; and implementation and postimplementation debriefs.in the area of automated management and consensus development, this project is designed to allowflexibility in the way that the single entity (the u.s. nrc) can interact both with the several entities (theregulated industry and their industry consortia representatives) and with the many (the general public). each ofthese three groups have differing roles and responsibilities in this type of collaborative rulemaking process andeach needs to have its interest represented. the automation aspect of this project is geared toward developingmechanisms whereby some degree of moderator interaction can assist in managing the potentially thousands ofinteractions from the general public while still allowing the primary discussions between the regulated andregulator to take place. in addition, the ability to achieve consensus from the general public by intelligentlyparsing their comments will be of tremendous use for the regulator and for industry when it comes to finalizingproposed rules.in the arena of metrics, this project will be of benefit by establishing some demographics on a federal level,rulemaking activity. the preliminary numbers of interest would be items such as the number of regulators,industry representatives, and general public participants involved. the impact of rules on each group asperceived by the group correlated to the impact on each group as perceived by the other groups would also be ofinterest.finally, this project would have a postimplementation debriefing phase where an evaluation of all pertinentmetrics would be made and folded into the regnet national plan as well as into the other four projects of thisoverall effort.to implement this collaborative workspace, information infrastructure tools are necessary. since acollaborative workspace allows users to visualize information in new and dynamic ways, users may want tomake notes about what they find, or to query an object to obtain specific information about the object in thespace. an annotation system would allow users to integrate their notes directly into the collaborative workspaceand to obtain detailed textual, graphic, or other information while still within the collaborative workspace.additionally, there will be times when the threads of negotiation become extremely complex and users may needto have topicgenerating "bayesian systems" to help cross posthreads for evaluation. this capability will beespecially important in a regulatory environment where those representing several (sometimes competing)interests must work together, negotiate, or resolve differences about matters of complex and specific detail. withthe annotation system, they will be able to compile or exchange notes connected with specific features andactions. the annotations could be passed among remote users or even used in an asynchronous mode, wherenotes left at one time are collected, organized, and reviewed later.demonstration project two: virtual regulatory negotiations: pilot testing 3d technologies regulatory complexity: low technological complexity: emerging to advanced government unit: local, state, federal principal investigators: j.d. nyhart and kenneth kaplan, massachusetts institute of technologythis project seeks to develop, and try out in simulation and then in real life, an electronic, physical, andprocess environment in which negotiations common in regulatory processes can be carried out on a virtual ordispersed basis. there will be an evaluation of the activities and products associated with each goal identifiedbelow. in the first year, existing 3d technologies will be used in the simulations and compared. in the second,new advanced technology will be developed and deployed based on the lessons and evaluations in the first year.it is now possible to put into practice combined audio/video/computeraided communication to enablemultiple parties located apart from each other to negotiate agreements in real time, anywhere in the world,anytime they choose, and to have computerbased tools to help in the process. as the clintongore technologyregnet: an npr regulatory reform initiative toward nii/gii collaboratories591the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.policy statement of february 22, 1993, stated, "fast communication makes it possible for teams to work closelyon a project even if the team members are physically distant from each other." the observation appliesparticularly well to negotiation.virtual environments are emerging as a revolutionary means of communication. for example, one powerfulapplication currently being explored is the dissemination and display of u.s. government documents, includingthose concerning federal regulatory processes. if, for instance, the current vast regulatory material could betransformed from text format into a visually dynamic environment, then movement through the material wouldbe translated into an activity similar to walking through a building.virtual documentary environments (vdes), when combined with distance audio, video, and computercapabilities, become cogent negotiating tools or negotiating environments, where representative parties fromgovernment, industry, and the public can discuss decisions while simultaneously referencing the easilymanageable virtual documentary environment. through new applications of information technology, regnet canhelp push back the frontiers of dispersed (or distance) communication, specifically, the communications ofreaching agreement, or negotiation. this project would build multimedia technologyšincluding modeling andother softwarešas integrative tools to be jointly used by the negotiating parties who are located in differentgeographic places. the results would be extremely helpful not only to industry, government, and academia butalso to the independent citizen trying to understand and negotiate the intricate maze of regulatory legislation.this project sits at the intersection of the technological advances of the nii and the need to improve, visiblyand effectively, the processes of regulation. underlying issues include the following: enhancing citizens' ability to participate effectively in regulatory decisions affecting them; ameliorating the bilateral mode of most regulatory hearing processes; pending devolution of regulatory power from the federal government to the states, accentuating the desirefor effective, longdistance communications and negotiations; and capitalizing on opportunities provided by technology to resolve the above issues.project goalsšfirst year goal 1. construct a visualization, demonstrating the virtual or dispersed uses by multiple parties of existinginformation technology and space for reaching agreements. two selected scenarios will be developed andevaluated that are typical of those found in the regulatory arena: program initiation and rule making. goal 2. test and evaluate the technology and space in the pilot stage by a simulation or mock runthrough ofthe two scenarios. goal 3. compare and evaluate the information technologies used in each of the two simulated negotiationsand the experience, through protocols or interviews of the participants and analysis of any embeddedmonitoring software. the nature of the data collected will be informed by the evaluative work conducted bynpr and omb to date; we will seek to be consistent with them. to the appropriate extent, we hope toinclude existing metrics now in use by npr and omb and, based on the experience of the project, to add tothem.project goalsšsecond year: goal 4. apply lessons from the simulations and evaluation of existing technology in two parallel directions.the first is the development of new 3d technology to create an advanced environment for regulatorynegotiations of the type simulated in the first year. the second is the testing of that technology, again first insimulation and then in real life.governmental and private partners participating during the second year would be secured at the outset, or asclose as possible to it, contingent on interim periodic reviews. there would therefore be buyin at the beginning,reaffirmed throughout the project. we will evaluate the experience, technology, and space in each reallifevirtual negotiation.regnet: an npr regulatory reform initiative toward nii/gii collaboratories592the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. goal 5. analyze the evaluation metrics and protocols used throughout the project for their applicability innpr's or omb's continuing monitoring of the nii. the second year will include efforts to provide measuresof the effects of advances in the new information infrastructure on different processes of reaching agreementat various points in the regulatory structure. the question is whether new information technology is effectivein advancing the processes of reaching agreement. does the technology work? is it sought out? does it seemto make a difference in the eyes of the people who are brought under its mantle?demonstration project three: virtual regulatory environment regulatory complexity: low to significant technological complexity: emerging to advanced government units: local, state, federal principal investigators: james foley, william ribarsky, larry hodges, and nicholas faust, graphics,visualization, and usability center, gis center, georgia institute of technologystatement of problemto get involved in the regulatory environment, people typically turn to experts for assistance. sometimespeople even use multiple sets of experts because of difficulties in navigating through the vast arrays of printed(or electronic) regulatory information required for understanding even simple issues. when challenged withmore complex forms of regulatory interactions (e.g., adjudication or collaborative rule making), current methodsall but mandate the use of intermediate ''experts" between the "customer" of regulation and the "supplier" of theregulatory services.the complexities of dealing with paperbased regulation, various experts, four different types of regulatoryinteractions, and the needs of everyday life typically cause people to just give up in their quest either to accessregulatory information or to participate in regulatory processes. these "quests" are fundamental rights ofamericans. the intensely complex regulatory arena is responsible for forcing many people out of the regulatoryprocesses that are their right. this project is designed to bring these people back into the process and to act astheir own "experts" in accessing information that shapes their lives.as stated earlier, it is the underlying premise of the regnet project, and of the npr, that some problemsthat stem from complexities, societal or technological, can be dealt with through thoughtful implementation oftechnologies that are able to render complex systems into simpler constructs.this project, involving collaborations between several state institutions, federal government agencies, andissues in both rural and urban areas, by its very nature meets the criterion to address nationwide infrastructuresmeeting the needs of diverse groups of citizens. this project will use significant components of the nii, add to it,and make it more useful. it will be developed by organizations that have had a major hand in using andsustaining that infrastructure.support for the end useralthough it will use advanced technology, the proposed system will have an intuitive interface that willmake it accessible to a broad range of users at any level of technical expertise. the user will not need specializedknowledge of plans, blueprints, or layouts for sites or structures because these will be rendered as realistic threedimensional objects among which the user can navigate naturally. selection will be by directly grabbing (orshooting a ray from a 3d mouse through) an object. these acts can initiate a query to a geographic informationsystem (gis) database, so the amount of effort and skill necessary to set up these queries will be reduced. initialqueries to the gis and crossreferenced regulatory databases will also be automatically structured, based on theregnet: an npr regulatory reform initiative toward nii/gii collaboratories593the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.properties of the site and the work to be performed there. as a result, it will be easier for even inexperiencedusers to get information quickly and, based on what they have found, carry out further queries.proposed workthis project will develop the vreg, a realtime 3d navigation system for exploring crossreferenceddatabases. the crossreferenced databases will be formulated and implemented as part of this project and willconsist, on the one hand, of regulatory information (local, state, national, international) and, on the other hand, ofgeographical information (with accompanying layers of geological, economic, local climate, and other data). thedatabases will be organized as servers with their own sets of calls for placing or retrieving information. sincequeries could be made over networks, the database servers could be distributed. thus communities couldmaintain their own servers of local regulations or geographical and economic datašthey would just use theappropriate calls for organization and data placement. the geographical servers will be extensions of those thatalready exist and will rely on the massive capability in government (e.g., army databases) and business (e.g.,geographic information companies such as erdas). the regulatory servers will be developed and linked withthe geographical servers as part of this project. a central index, using the regnet www server, will be set upwith addresses and information about the satellite sites.serving as a modelthe vgis system upon which vreg is based is highly flexible and portable. all it takes is different terraindata, 3d building plans, and appropriate sets of gis layers to explore a multitude of distributed regulationscenarios. as part of the project, we will provide documentation on how the terrain data server and gis layersare set up. we will also provide documentation for the crossreferenced regulation server. in addition, some ofthe regulatory information (e.g., federal regulations) will be reusable in many situations. since all 3d graphicsin vreg ultimately use open gl, which is the de facto standard in 3d graphics, our software can run on avariety of different platforms. communications using standard tcp/ip protocols will ensure that our system canbe used in a distributed fashion by anyone who has access to the internet. in addition, standard unix socketmechanisms will be used for distributed communication on local area networks.the vgis system we have developed has truly immersive capability for navigating and understandingcomplex and dynamic terrainbased databases. the system provides the means for visualizing terrain modelsconsisting of elevation and imagery data, along with gis raster layers, protruding features, buildings, vehicles,and other objects. we have implemented windowbased and virtual reality versions and in both cases provide adirect manipulation, visual interface for accessing the gis data. unique terrain data structures and algorithmsallow rendering of large, highresolution data sets at interactive rates.vgis represents a significant advance over previous 2d, mapbased systems or 3d giss with decidedlynoninteractive rates (having typical delays to produce images of several seconds or more). it can be usedanywhere a traditional gis can be used. thus it can be used for urban planning, evaluation of geology,vegetation, soil, waterway, or road patterns, flood planning, and many other tasks. in addition, the ability to havedetailed 3d views and to jump to a different location to see the view from there opens new possibilities.planners for new buildings or other facilities can see full 3d views from their prospective sites or can see theview from nearby existing buildings with their planned facility in place.vgis is implemented using the simple virtual environment (sve) tool kit, a deviceindependent library thatprovides mechanisms and software tools for developing virtual environment (ve) applications. sve is based onthe graphics library (gl) of silicon graphics, and vgis has been run on a number of different hardware systemswith gl support.vgis can be used either with a workstation windowbased interface, or with an immersive virtual realityinterface. the conventional windowbased interface in vgis parallels the functionality of the immersiveinterface. the 2d workstation mouse can be used to rotate the user's viewpoint, move through the environment,regnet: an npr regulatory reform initiative toward nii/gii collaboratories594the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.select objects, and pull down menus. the user's view is displayed in a workstation window. in the immersiveenvironment, users wear a headmounted display (hmd) and hold a threedimensional mouse controller, withsix degreeoffreedom trackers attached to both the hmd and the 3d mouse. the head tracking allows thehmd graphics to be generated in accordance with the user's viewpoint, so that the user feels a sense of presencewithin the virtual environment.system featuresusers of vgis have complete freedom in navigating the virtual world described by the databases. both thewindowbased and immersive interfaces allow flight through six degrees of freedom (pitch, yaw, roll, and threedimensional translation). to avoid getting lost or to find specific locations, the user can toggle a labeled gridcoordinate overlay or bring up an overview 2d finder map. the user can then select and jump to locations oneither the finder map or in the 3d world. this provides fast capability to view the 3d world from any locationwithin it. in query mode, selection of any feature, terrain location, building, or other object in the 3d worldbrings up gis information about the location or object. since it would be straightforward, within the gis, toprovide links to other data associated with an object, this selection could, for example, bring up a full, 3d modelof a building interior. as an alternative, the user could merely fly through the building wall to enter the interiormodel. individual units, such as vehicles, or groups of units can be placed on or allowed to move over the terrain,and symbolic information, such as markers or lines of demarcation, can be laid down or moved around. theseare just some of the features available in the vgis system.use in this projectour vreg implementation will provide a visual interface for fast and easy retrieval of geography orlocationspecific information. this capability itself can be quite useful, especially when one has simultaneousaccess to a database of regulatory information as we propose here. in addition, we propose to build for varioustypes of facilities and situations a knowledge base that can be used by the system to automatically retrieveappropriate information from the gis and crossreference it with relevant regulations. thus, for example, if onewere siting a fuel storage facility, the system would know to look for information on substrate geology, watertable and water runoff, whether the site is earthquake prone, whether other sites with flammable or toxicmaterials are nearby, and so on. the system could then also pull up relevant keyworded local, state, and federalregulations.this approach offers the possibility of a dynamic regulation model that streamlines the process and reducesthe number of regulations. in the above example, it would be fairly straightforward to pinpoint redundant local,state, or federal regulations if they were accessible via the same server structure. one could then have a processof removing or ignoring redundant regulations. also, the gis offers the capability to store and retrieve (in anorganized fashion) highly detailed inputs for engineering, economic, environmental, or other models; we willextend the knowledge base to enable automatic searches for this input information. once in place the modelscould thus be run easily for userselected scenarios. if validated by government agencies, model outputs could beused to satisfy sets of simple goals, and one could do away with large numbers of regulations. our approachwould also allow the user to play "what if," exploring different siting options, plant output structures, and thelike to meet goals, maximize economic gains, or optimize other effects. since the data are available andorganized, it would also be possible for the user to obtain "realtime" environmental and economic impact studies.a testbed projectwe will develop distributed gis and regulatory servers for champaign county, ill., (ncsa location) andfor the atlanta, ga., area (georgia tech location). the champaign county database will build on the regulatoryand other information already available via ccnet (the champaign county network), whereas the atlantaregnet: an npr regulatory reform initiative toward nii/gii collaboratories595the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.database will organize around extensive gis information available for the area. thus we will use significantlydifferent localesšone rural and one urbanšin our testbed project. we will apply our tools to a complexscenario, such as the decommissioning of a nuclear power plant and the shipping of waste materials to a storagefacility. for the purpose of this exercise only, we could place the decommissioned power plant in the urban areaand the storage facility in the rural area. we would then have to deal with a welter of different local regulationsoverlaid with a set of federal regulations for the two sites. we would use the distributed servers (allowingcommon pictures of the sites) and various collaborative tools (such as the annotation system described below) todo this.a 3d annotation system for notetaking and notesharinga 3d environment allows the user to visualize information in new and effective ways. althoughvisualizing and exploring a complex model or an extended site, the user may want to make notes about what heor she finds, or to query an object to obtain specific information about the object in the space. an annotationsystem should allow the user to integrate his notes directly into the environment and to obtain detailed textual,verbal, or other information while still within the 3d space. this capability will be especially important in aregulatory environment where those representing several (sometimes competing) interests must work together,negotiate, or resolve differences about matters of complex and specific detail. with the annotation system theywill be able to compile or exchange notes connected with specific features and actions (or time frames fordynamic simulations) in 3d models and terrains. the annotations could be passed among remote users or evenused in an asynchronous mode where notes left at one time are collected, organized, and reviewed at a later time.demonstration project four: intelligent distributed access to regulatory information regulatory complexity: low to significant technological complexity: emerging to advanced government units: local, state, federalthe recent increase in complexity of our regulatory environment has made it more difficult for affected citizens and businesses to find and understand applicable regulations; monitors to assess compliance with regulations; and regulatory bodies and involved participants to evaluate proposed changes.a network space for regulatory information and operation, regnet, could be created as a means to decreasethe regulatory complexity. the creation of regnet would require the cooperation of authorities, businesses,consumer groups, and experts in the domains being addressed and would require academic involvement to studythe technological and organizational problems and opportunities. regnet development would require industrialparticipation to provide the operational services using appropriate technology and would require the cooperationof local, state, and federal regulatory agencies to put the technology to use.an application to provide online access to the text of regulations via the www could be developed. at thispoint, citizens, monitors, and regulators would be able to browse and access this text via computer programs likemosaic and wais, moving smoothly from the regulations of one jurisdiction to those of another.the application would provide linkages among these documents, based on the terms used within theregulations. a modern standard for storage of regulationsšperhaps a publicly accessible base such as thedefense advanced research projects agency (darpa)funded persistent object basešwould be selected foruse. available search systems would be enhanced with thesauruses to span the application among terms and toregnet: an npr regulatory reform initiative toward nii/gii collaboratories596the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.aid nonexperts in completing their searches. important input to such thesauruses would be obtained by analyzingsearch strategies and failures from the initial phases.the ordinary text of regulations with formal logic representations would be augmented. such aninformation structure, building on top of the linkages and thesauruses, would enable new, crucial services, suchas topicbased retrieval of relevant regulations, automatic conformance checking, and automated determinationof conflicts and overlaps among regulations. such analysis tools would empower workers in their specificdomains and allow cooperation without centralization.demonstration project five: rural government testbed project regulatory complexity: low to moderate technological complexity: emerging to advanced government units: local, state, federalproblem definitionlarge metropolitan areas often have branches of federal and state government agencies, in addition tocounty and local government offices. moreover, these offices are typically located in the same section of the city.many of the offices that a citizen in a large metropolitan area needs to communicate with for life and businessmay at least be located near each other. however, in rural areas, there are rarely local branch offices of all thefederal and state agencies with which rural citizens need to interact. the rural testbed demonstration project willdevelop mechanisms for citizens to interact with local, county, state, and federal government agenciesseamlessly, utilizing emerging technologies. the regulatory complexity of this project will typically be eitherone to one, or one to two.location for the projectšchampaign county, illinoisthe rural testbed project will be conducted in champaign county, ill. the closest large cities to champaigncounty are chicago, st. louis, and indianapolis, each approximately 21/2 hours' driving distance, and too far toconduct daily business, even if a citizen were to brave visiting all agencies in person. more than 90 percent ofchampaign county is farmland, with a university community at its center. illinois has a large number of localgovernments (including city, county, township, and state), so the rural testbed is set in a particularly challengingregion.a virtual regulatory environment will be useful only if the citizenry is knowledgeable enough about thetechnology to use it. champaign county was selected for this project because it has been nationally recognizedfor its innovativeness and advances in the nii. champaign county network (ccnet) is a collaborative projectbetween the national center for supercomputing applications, the university of illinois, and the champaigncounty chamber of commerce. the mission of ccnet is to create a supportive environment for champaigncounty to cooperatively develop applications that use advanced communications technologies. this has beenaccomplished through the work of a technical committee and six applications task forces exploring applicationsof the nii in education, health care, agribusiness, government, and libraries.since early 1993, over 300 community leaders have been working together to build both the humaninfrastructure and the technical infrastructure to achieve the goals of ccnet. today, there are more than 15 sitesin the community connected to the internet at high speeds. nine sites in the communityšincluding schools,businesses, and a public libraryšreceive data over the cabletv at 2.5 mbps and send data over the telephoneline at 14.4 kbps eight schools are connected to the internet via isdn connections sending and receiving data at56 kbps or 128 kbps. in addition, ameritech and ccnet recently announced a partnership through whichameritech is offering an isdn package connecting individual users to the internet at 128 kbps. wirelessregnet: an npr regulatory reform initiative toward nii/gii collaboratories597the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.connections are being tested. champaign county also has a free net called prairienet, which has more than 8,000registered users and is rapidly growing. the county recently announced that, thanks to information availableonline, courts outside the county seat will be able to serve local residents.the remarkable thing about all the ccnet connections is that they have been community initiated. ccnetserves as a catalyst, bringing sophisticated technologies and internet connections to champaign county muchsooner than they will be available in comparable communities.although the community leadership has been building an electronically connected community inchampaign county, the entire citizenry has become educated about the internet and tools electronic mail anddata exchange. numerous newspaper and television news stories have covered the local efforts and successes,and there have been countless public presentations and demonstrations to special interest groups.participants in the ccnet project have been using electronic mail for nearly 2 years and have beenexploring the www using ncsa mosaic. through the rural government testbed project, the citizens ofchampaign county will be able to use the technology for specific purposes and realize the benefits of technologyfor interacting with multiple government agencies. the level of awareness and of current electronic connectivityin champaign county optimizes our chances of success in the rural government testbed.project concept and dimensionsthe rural government testbed project will develop a system whereby citizens can use one technology tointeract with a variety of governmental agencies at various levels. the goal is to eliminate frustration withgovernment bureaucracy and to return power and independence to individuals. though the tools developed maybe applied to a nearly endless number of situations, we will focus on two primary applications. both applicationscan be one to one or one to two, involve a range of governmental agencies, and are a cross between the emergingand complex categories of technology complexity. the second application may lend itself to a onetomanyregulatory complexity.the basic concept is to allow a citizen to access relevant regulations via an online system (the internet) andto electronically file forms required for specific reallife tasks. this can be accomplished by providinggovernment information on the www, with citizens using a www browser or navigator such as ncsamosaic. theoretically, all forms submitted by citizens would be processed automatically and governmentdatabases would be maintained automatically. but, for the testbed, individual persons at the government agencywill intervene by updating their database with forms and information received from the citizens. collaborativetools will be built into the browser so that citizens can have advisors helping them to make decisions inregulatory environments, if they wish. in addition, text searching capabilities developed as part of the digitallibrary initiative at the university of illinois will be incorporated into the tools for ease of use.rural government testbed: application 1changing residence is one of the most stressful life situations. the stress of moving is exacerbated by themultitudes of contacts that the individual has to maintain separately with different government agencies. for themost part, the individual is responsible for updating the various agencies about his or her whereabouts.with the rural government testbed we will build an environment where interacting with levels ofgovernment before and after a change of residence is drastically simplified. currently, when a citizen moves, heor she has to communicate with the following agencies at different levels: local/city: register children in school, notify water and power services; county: register to vote with county clerk, pay real estate tax with county assessor; state: renew driver's license at the secretary of state, replace license plates, update vehicle registration; and federal: leave forwarding address with postmaster, send address change to internal revenue service.regnet: an npr regulatory reform initiative toward nii/gii collaboratories598the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.in addition to these government agencies, the citizen must also update his or her address with variousprivate organizations such as magazine publications, newspapers, insurance carriers, telephone services, andeven online services. if the rural government testbed works to smooth residential transitions, these privateorganizations could connect to the same system in the future.rural government testbed: application 2a person who wants to establish a small business today has to learn about and comply with an almostunbearable number of zoning, environmental, employment, and other regulations. the time it takes to learn allthe regulations to ensure compliance and to fill out all accompanying forms can be daunting, if not completelystifling. large corporations have inhouse attorneys and other experts to handle interactions with governmentagencies and to keep educated about the most recent relevant regulations. some potential small business ownershave almost no choice but to franchise with one of the large corporations. this difficulty dramatically reducesopportunities for innovation in american business. for example, a person who wants to open a particular type ofbusinessšfor instance, a dry cleaneršhas to interact with a large set of government agencies in order to set upthe business: local/city: zoning and sign permit, sales tax regulations; county: "doing business as" (dba) name registration; state: state department of revenue for a retail license; and federal: osha for compliance with regulations, the department of immigration and naturalization for eachemployee, and epa for an environmental impact statement.it turns out that many of the state and federal regulations, at least, already exist in digital format. they willneed to be reformatted to fit into the testbed project. even if all the regulations and forms necessary to establish anew small business were available online through the rural government testbed, there would still be twoproblems: finding relevant sections of regulations, and dealing with the vast amount of information and legalramifications.to solve these problems, we will include two additional features in the rural government testbed project:1. a textsearch mechanism developed as part of the digital library initiative (dli) at the university ofillinois so that users and potential small business owners could search lengthy regulations for the relevantportions, based on proximity and keywords. since many of the regulations are already in digital format,the search indexes would be generated and available to expedite the search process.2. a potential small business owner may want to fill out forms and read through regulations with theassistance of professional counsel such as an attorney or an accountant. we will therefore experimentwith collaborative features in www browsers, including collaborative visualization as well as videoconferencing. hypernews features can be built into the testbed to encourage small business owners tolearn from each other.rural government testbed technologyeach of the government agencies identified in the applications above will be connected to the internet athigh enough speeds to serve information on the www. some of them are already connected (the local schools).in addition, the public library in champaign county is connected to the internet, providing public access, andtwo other libraries will be connected to provide further public access. one hundred community members will beconnected via the ameritech isdn solution. these 100 will be selected based on factors that indicate theirpotential for using the testbed, such as their skill with internet tools, whether they own a small business, andwhether they are located in an area where the service is available.regnet: an npr regulatory reform initiative toward nii/gii collaboratories599the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.the first 6 months of the project will be devoted to recruiting individual and agency participants,identifying connection solutions, adapting the browser software to include searching and collaborative tools, andputting relevant information from government agencies online. extensive training will be conducted for theparticipants and for the general public during months 5 through 7. the connections will be in place for 1 year forthe testbed, and usage will be monitored. the efficiency will be evaluated by examining usage statistics andeliciting continual feedback from participants online, via public forums, and through individual communications.the software will be revised as further needs are identified.note that, although the rural government testbed will be developed for two applications, the system couldbe migrated to another application if the proper agency were involved. for instance, an agribusiness applicationmight be developed by working with county farm bureaus, and with the usda.conclusionsthe proposed projects, the planning, and the demonstrations will work together to create a synergistic planwith proven capability for the nation. although the projects' focus will be to empower the rural and tribalcommunities to function within the intergovernmental system that surrounds them, the results will automaticallyextend to the other city, state, and federal governments that also must function in the new environment.information technology is one of the major ways that the current structure of regulatory control can be simplifiedand made useful to citizens trying to function in the nii. this project will also work with, and solicit resourcesfrom, international organizations to include the results of a reinvented regulatory government in a growing gii.the deployment and use of highperformance computing, communications, and information technology andthe nii are essential to meeting national goals and objectives in the global marketplace.statutes and executive orderswith the plan proposed here, modified by the lessons learned from the implementation of the demonstrationprojects also proposed here, future intergovernmental regulatory projects can be developed in such a way thatthey more effectively support the national goals and objectives and national challenge applications. these goalsand applications are found in a series of statutes and executive orders: the high performance computing act of 1991, public law 102194, december 9, 1991, and subsequentlegislation in support of advances in computer science and technology that are vital to the nation'sprosperity, national and economic security, industrial production, engineering, and scientific advancement; high performance computing and communications: technology for the national informationinfrastructure, a supplement to the president's fiscal year 1995 budget; science in the national interest,president william j. clinton, vice president albert gore, jr., august 1994, executive office of thepresident office of science and technology policy; future scenarios, national challenge applications, including: public access to government information,health care, education and training, digital libraries, electronic commerce, advanced manufacturing,environmental monitoring and crisis management, high performance computing and communications(hpcc), information infrastructure technology and applications, national coordination office for hpcc,executive office of the president, office of science and technology policy, february 1994; the clinton administration's development of an advanced national information infrastructure (nii) and theglobal information infrastructure (gii) as top u.s. priorities; see global information infrastructure: agendafor cooperation, al gore, vice president of the united states, ronald h. brown, secretary of commerceand chairman, information infrastructure task force, february 1995; executive order 12866, regulatory planning and review, september 30, 1993; executive order 12839, reduction of 100,000 federal positions, february 10, 1993;regnet: an npr regulatory reform initiative toward nii/gii collaboratories600the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. executive order 12861, elimination of onehalf of executive branch internal regulations, september 11,1993; executive order 12864, united states advisory council on the national information infrastructure,september 15, 1993; and executive order 12875, enhancing the intergovernmental partnership, october 26, 1993.note1. "technology for america's growth: a new direction to build economic strength," p. 20.references1. the high performance computing act of 1991, public law 102194, december 9, 1991.2. high performance computing and communications: technology for the national information infrastructure, supplement to the president'sfiscal year 1995 budget; science in the national interest, president william j. clinton, vice president albert gore, jr., august1994, executive office of the president office of science and technology policy.3. information infrastructure technology and applications, national coordination office for hpcc, executive office of the president,office of science and technology policy, february 1994.4. global information infrastructure: agenda for cooperation, al gore, vice president of the united states, ronald h. brown, secretary ofcommerce and chairman, information infrastructure task force, february 1995.5. virtual gis: a realtime 3d geographic information system, david koller, peter lindstrom, william ribarsky, larry hodges, nickfaust, and gregory turner, graphics, visualization & usability center, georgia institute of technology, tech report gitgvu9514, 1995, submitted to ieee visualization 1995.6. the simple virtual environment library, version 1.4, user's guide, g.d. kessler, r. kooper, j.c. verlinden, and l. hodges, graphics,visualization & usability center, georgia institute of technology, tech report gitgvu9434, 1994.7. graphics library programming guide, silicon graphics computer systems, mountain view, california, 1991.8. levelofdetail management for realtime rendering of phototextured terrain, p. lindstrom, d. koller, l.f. hodges, w. ribarsky, n.faust, and g. turner, graphics, visualization & usability center, georgia institute of technology, tech report gitgvu9506(1995), submitted to presence.9. the virtual annotation system, reid harmon, walter patterson, william ribarsky, and jay bolter, georgia institute of technology, techreport gitgvu9520.10. creating a government that works better and costs less: status report september 1994, report of the national performance review(npr) cdrom, vice president al gore, 1994.11. uniform resource locator http://www.npr.gov/, npr home page.12. uniform resource locator http://nuke.westlab.com/regnet.industry/, regnet industry home page.13. creating a new civilization: the politics of the third wave, alvin and heidi toffler, turner press, 1994œ1995.14. the death of common sense: how law is suffocating america, philip k. howard, random house, 1994.15. executive order 12291, federal regulation, february 17, 1981.16. executive order 12498, regulatory planning process, january 4, 1985.17. executive order 12612, federalism, october 26, 1987.18. executive order 12637, productivity improvement program for the federal government, april 27, 1988.19. executive order 12839, reduction of 100,000 federal positions, february 10, 1993.20. executive order 12861, elimination of onehalf of executive branch internal regulations, september 11, 1993.21. executive order 12864, united states advisory council on the national information infrastructure, september 15, 1993.22. executive order 12866, regulatory planning and review, september 30, 1993.23. executive order 12875, enhancing the intergovernmental partnership, october 26, 1993.regnet: an npr regulatory reform initiative toward nii/gii collaboratories601the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.appendixjohn p. ziebarth, ph.d., associate directornational center for supercomputing applicationscab261605 e. springfieldchampaign, il 61820217/2441961 voice217/2441987 faxziebarth@ncsa.uiuc.eduw. neil thompson, npr regnet. gov, coordinatoradvisory committee on reactor safeguards rotational assignmentu.s. nuclear regulatory commissionwashington, dc 20555301/4155858 voicewnt@nrc.govnthompso@tmn.comj.d. nyhart, professor of management and ocean engineeringmassachusetts institute of technologysloan school of management50 memorial drive, e52542cambridge, ma 021421437617/2531582 voice617/2532660 faxjdnyhart@mit.edukenneth kaplan, principal research scientistdepartment of architecture and research laboratory for electronicsmassachusetts institute of technologycambridge, ma 02139617/2589122 voice617/2587231 faxkkap@mit.edubill ribarsky, associate director for servicegeorgia institute of technologyoit/educational technologiesgraphic visualization and usability centerroom 229 himanatlanta, ga 303320710404/8946148 voice404/8949548 faxbill.ribarsky@oit.gatech.eduregnet: an npr regulatory reform initiative toward nii/gii collaboratories602the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.gio wiederhold, ph.d., professorcomputer science department (also, by courtesy, electrical engineering and medicine)gates building 4a, room 433stanford universitystanford, ca 943059040415/7258363 voice415/7252588 faxsiroker@cs.stanford.edumichael r. genesereth, professorcomputer science departmentgates buildingstanford universitystanford, ca 943059040415/7258363 voice415/7252588/7411 faxgenesereth@cs.stanford.edukenneth gilpatric, esq.national performance review netresults.regnet, consultantadministrative conference of the united states (formerly)1615 manchester lane, nwwashington, dc 20011202/8827204 voice202/8829487 faxken.gilpatric@npr.gsa.govtim e. roxeynational performance review regnet. industry, leadbaltimore gas and electric, project managerregulatory projects1650 calvert cliffs parkwaylusby, md 20656council for excellence in government (ceg), principal investigatorsuite 8591620 l street, nwwashington, dc 20036410/4952065 voice410/5864928 pagertimr@access.digex.netwilliam j. olmstead, associate general counsel for licensing and regulationoffice of general counselcommission staff officeu.s. nuclear regulatory commissionwashington, dc 20555301/4151740 voicewjo@nrc.govolmstead@tmn.comregnet: an npr regulatory reform initiative toward nii/gii collaboratories603the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.ben slone, presidentfinite matters ltd.2694 fairground roadgoochland, va 23063804/5566631 voicesloneb@nuke.westlab.comjim acklin, ceoregulatory information alliance8806 sleepy hollow lanepotomac, md 20854301/9832029 voiceacklinj@aol.comregnet: an npr regulatory reform initiative toward nii/gii collaboratories604the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.65electronic document interchange and distribution based onthe portable document format, an open interchange formatstephen n. zilles and richard cohnadobe systems incorporatedabstractthe ability to interchange information in electronic form is key to the effectiveness of the nationalinformation infrastructure initiative (nii). our long history with documents, their use, and their managementmakes the interchange of electronic documents a core component of any information infrastructure. from thevery beginning of networking, both local and global, documents have been primary components of theinformation flow across the networks. however, the interchange of documents has been limited by the lack ofcommon formats that can be created and received anywhere within the global network. electronic documentinterchange had been stuck in a typewriter world while printed documents increased in visual sophistication. inaddition, much of the work on electronic documents has been on their production and not on their consumption.yet there are many readers/consumers for every author/producer. only recently have we seen the emergence offormats that are portable, independent of computer platform, and capable of representing essentially all visuallyrich documents.electronic document production can be viewed as having two steps: the creation of the content of thedocument and the composition and formatting of the content into a final form presentation. electronicinterchange may take place after either step. in this paper we present the requirements for interchange of finalform documents and describe an open document format, the portable document format (pdf), that meets thoserequirements. there are a number of reasons why it is important to be able to interchange formatted documents.there are legal requirements to be able to reference particular lines of a document. there is a legacy of printeddocuments that can be converted to electronic form. there are important design decisions that go into thepresentation of the content that can be captured only in the final form. the portable document format is designedto faithfully represent any document, including documents with typographic text, tabular data, pictorial images,artwork, and figures. in addition, it extends the visual presentation with electronic aids such as annotationcapabilities, hypertext links, electronic tables of contents, and full word search indexes. finally, pdf isextensible and will interwork with formats for electronic interchange of the document content, such as thehypertext markup language (html) used in the world wide web.backgrounda solution to the problem of electronic document interchange must serve all the steps of document usage.the solution must facilitate the production of electronic documents, and, what is more important, it mustfacilitate the consumption of these documents. here, consumption includes viewing, reading, printing, reusing,and annotating the documents. there are far more readers than authors for most documents. serving consumershas a much bigger economic impact than does serving authors. replacing paper distribution with electronicdistribution increases timeliness, reduces use of natural resources, and produces greater efficiency andproductivity. it also allows the power of the computer to be applied to aiding the consumption of the document;for example, hyperlinks to other documents and searches for words and phrases can greatly facilitate finding thedocuments and portions thereof that interest the consumer.electronic document interchange and distribution based on the portable document format,an open interchange format605the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.from a consumption point of view, the critical need in electronic document interchange is the ability toview, print, or read the document everywhere that someone has access to it. if the document can also be revisedor edited then that is so much the better, but it is not required for use of the document.final form and revisable form interchangethere are two different ways to produce interchangeable electronic documents. although there are manysteps in the production of a visually rich document, the process of composition and layout partitions productioninto two parts. composition and layout is the process which takes a representation of the content of a documentand places that content onto a twodimensional space (or sequence of twodimensional spaces), usually calledpages.in the process of composition and layout, a number of decisions, called formatting decisions, are made:which fonts in which sizes and weights are used for which pieces of content, where on the page the content isplaced, whether there are added content fragments such as headers and footers, and so on. these formattingdecisions may be made automatically based on rules provided to the composition and layout process; they maybe made by a human designer interacting with the composition and layout process; or they may be made using acombination of these two approaches.the representation of the content before composition and layout is called revisable form. revision isrelatively easy because the formatting decisions have not been made or have only been tentatively made and canbe revised when changes occur. the representation of the content after composition and layout is called finalform.the interchange of electronic documents can be done either in revisable form or in final form. if therevisable form is interchanged, then either the formatting decisions must be made entirely by the consumer of theelectronic document or the rules for making the formatting decisions must be interchanged with the revisableform electronic document. if the first approach is chosen, then there is no way to guarantee how the documentwill appear to the consumer. even if the second approach is chosen, existing formatting languages do notguarantee identical final form output when given the same revisable form input. some formatting decisions arealways left to the consumer's composition and layout software. therefore, different composition and layoutprocesses may produce different final form output.the interchange of revisable form electronic documents can meet many authors' needs. both the standardgeneralized markup language (sgml) and the hypertext markup language (html) are successfully used tointerchange significant and interesting documents. but there are cases where these formats are not sufficient tomeet the needs of the author. for these cases, interchange of final form electronic documents is necessary.requirement for page fidelitythe key problem with interchanging only revisable form documents is the inability to guarantee pagefidelity. page fidelity means that a given revisable form document will always produce the same final formoutput no matter where it is processed. there are a number of reasons why page fidelity is required.the most obvious reason is that the composition and layout process involve a human designer's decisions.only in the final form is it possible to capture these decisions. these formatting decisions are important in thepresentation of the represented information. this is quite obvious in advertisements, where design plays animportant role in the effectiveness of communicating the intended message. it is perhaps less obvious but equallyimportant in the design of other information presentations. for example, the placement of text in association withgraphical structures, such as in a map of the washington, d.c., subway system (figure 1), will greatly affectwhether the presentation can be understood. in addition, formatting rules may not adequately describe complexinformation presentations, such as mathematics or complex tables, which may need to be hand designed forelectronic document interchange and distribution based on the portable document format,an open interchange format606the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 1 metro system map. source: courtesy of washington metropolitan area transit authority.effective communication. (figure 2 has simple examples of mathematics and tables.) finally, thecomposition and layout design may reflect the artistic expression of the designer, making the document morepleasing to read (figure 3).the rich tradition of printed documents has established the practice of using page and line numbers toreference portions of documents, for legal and other reasons. these references will work for electronicdocuments only if page fidelity can be guaranteed. many governmental bodies have built these references intotheir procedures and require that they be preserved, in electronic as well as in paper documents.the final set of cases does not require page fidelity; they are just more simply handled with a final formrepresentation than a revisable one. documents that exist only in paper form, legacy documents, can be scannedand their content recognized to produce an electronic document. although this recognized content could berepresented in many forms, it is simplest when the content is represented in final form. then it is not necessary todecide, for each piece of recognized content, what part of the original document content, such as body text,header, or footer, it belonged to. since the final form is preserved, the reader of the document can correctlyperform the recognition function. (see figure 2 for an example of a legacy page that would be difficult tocategorize correctly.)finally, preparing a document as an sgml or html document typically involves a substantial amount ofmarkup of sections of content to ensure that a ruledriven composition and layout process will produce theintended effect. for documents that are of transient existence, it may be far simpler to produce the compositionand layout by hand and to interchange the final form representation than to spend time tuning the document for aruledriven process.electronic document interchange and distribution based on the portable document format,an open interchange format607the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 2requirements for a final form document interchange formatfor a final form representation to be a suitable electronic document interchange format for the nii, it shouldmeet a number of requirements: it should have an open, published specification with sufficient detail to allow multiple implementations.there are many definitions of openness, but the key component of them all is that independentimplementations of the specification are possible. this gives the users of the format some guarantee of therebeing reasonable cost products that support the format. with multiple implementations, the question ofinteroperability is important. this can be facilitated, although not guaranteed, by establishing conformancetest suites for the open specification. it should provide page fidelity. this is a complex requirement. for text, this means representing the fonts,sizes, weights, spacing, alignment, leading, and so on that are used in the composition and layout of theoriginal document. for graphics, this means representing the shapes, the lines and curves, whether they arefilled or stroked, any shading or gradients, and all the scaling, rotations, and positioning of the graphicelements. forelectronic document interchange and distribution based on the portable document format,an open interchange format608the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 3images, this means doing automatic conversion from the resolution of the sample space of the image to theresolution of the device, representing both binary and continuous tone images, with or without compressionof the image data. for all of the above, the shapes and colors must be preserved where defined in deviceand resolutionindependent terms. it should provide a representation for electronic navigation. the format should be able to express hyperlinkswithin and among documents. these links should be able to refer to documents in formats other than thisfinal form format, such as html documents, videos, or animations. the format should be able to representa table of contents or index using links into the appropriate places in the document. the format should alsoallow searches for words or phrases within the document and positioning at successive hits. it should be resource independent. the ability to view an electronic document should not depend on theresources available where it is viewed. there are two parts to this requirement. a standard set of resources,such as a set of standard fonts, can be defined and required at every site of use. these resources need not betransmitted with the document. for resources, such as fonts, that are not in the standard set, there must beprovision for inclusion of the resource in the document. it should provide access to the content of the document for the visually disabled. this means, at a minimum,being able to provide ascii text that is marked up with tags that are compliant with the internationalstandard iso12083 (icadd dtd). the icadd (international committee for accessible documentdesign) markup must include the structural and navigational information that is present in the document. it should be possible to create electronic documents in this format using a wide range of documentgeneration processes. ideally, any application that can generate final form output should be usable togenerate an electronic document in this format. there should also be a means for paper documents to beconverted into this format.electronic document interchange and distribution based on the portable document format,an open interchange format609the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved. it should be platform independent; that is, the format should be independent of the choice of hardware andoperating systems, and transportable from one platform to any other platform. it should also be independentof the authoring application; the authoring application should not be required to view the electronicdocument. it should effectively use storage. in particular, it should use relevant, data typedependent compressiontechniques. it should scale well. applications using the format should perform nearly as well on huge (20,000page)documents as they do on small ones; the performance on a large document or a (large) collection of smallerdocuments should be similar. this requirement implies that any component of the electronic document berandomly accessible. it should integrate with other nii technologies. it should be able to represent hyperlinks to other documentsusing the universal resource identifiers (uris) defined by the world wide web (www) and it should beidentifiable by a uri. it should be possible to encrypt an electronic document both for privacy and forauthentication. it should be possible to revise and reuse the documents represented in the format. editing is a broad notionthat begins with the ability to add annotations, electronic ''sticky notes," to documents. at the next level,preplanned replacement of objects, such as editing fields on a form, might be allowed. above that, onemight allow replacement, extraction and/or deletion of whole pages, and, finally, arbitrary revision of thecontent of pages. it should be extensible, meaning that new data types and information can be added to the representationwithout breaking previous consumer applications that accept the format. examples of extensions would beadding a new data object for sound annotations or adding information that would allow editing of someobject.these requirements might be satisfied in a number of different ways. we describe below a particularformat, the portable document format (pdf) which has been developed by adobe systems inc., and thearchitectures that make interchange of electronic documents practical.an architectural basis for interchangethere are three basic architectures that facilitate interchange of electronic documents: (1) the architecturefor document preparation, (2) the architecture of the document representation, and (3) the architecture forextension. these three architectures are illustrated in figure 4. the architecture for document preparation isshown on the lefthand side of the figure and encompasses both electronic and paper document preparationprocesses. the righthand side of the figure shows consumption of prepared documents. the portable documentformat (pdf) is the architecture for document representation and is the link between these components. therighthand side of the figure shows consumption at two levels. there is an optional cataloguing step that buildsfull text indexes for one or a collection of documents. above this, viewing and printing pdf documents areshown. the architecture for extension is indicated by the "search" plugin, which allows access to indexes builtby the cataloguing process.the architecture for document preparationto be effective, any system for electronic document interchange must be able to capture documents in allthe forms in which they are generated. this is facilitated by the recent shift to electronic preparation ofdocuments, but it must include a pathway for paper documents as well.electronic document interchange and distribution based on the portable document format,an open interchange format610the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.figure 4unlike the wide range of forms that revisable documents can take, there are relatively few final formformats in use today. (this is another reason that it makes sense to have a final form interchange format.) since,historically, final forms were prepared for printing, one can capture documents in final form by replacing orusing components of the printing architectures of the various operating systems. two such approaches have beenused with pdf: (1) in operating systems with selectable print drivers, adding a print driver that generates pdfand (2) translating existing visually rich print formats to pdf. both these pathways are shown in the upper leftcorner of figure 4. pdfwriter is the replacement print driver and the acrobat distiller translates postscript (ps)language files into pdf.some operating systems, such as the mac os and windows, have a standard interface, the gui (graphicaluser interface), which can be used by any application both to display information on the screen and to print whatis displayed. by replacing the print driver that lies beneath the gui it is possible to capture the document thatwould have been printed and to convert it into the electronic document interchange format.for operating systems without a gui interface to printing and for applications that choose to generate theirprint format directly, the postscript language is the industry standard for describing visually rich final formdocuments. therefore, the electronic document interchange format can be created by translating, or "distilling,"postscript language files. this distillation process converts the print description into a form more suitable forviewing and electronic navigation. the postscript language has been extended, for distillation, to allowinformation on navigation to be included with the print description, allowing the distillation process toautomatically generate navigational aids.the above two approaches to creation of pdf documents work with electronic preparation processes. butthere is also an archive of legacy documents that were never prepared electronically or are not now available inelectronic form. for these documents there is a third pathway to pdf, shown in the lower left corner of figure 4.paper documents can be scanned and converted to raster images. these images are then fed to a recognitionprogram, acrobat capture, that identifies the textual and nontextual parts of the document. the textual parts areconverted to coded text with appropriate font information including font name, weight, size, posture. thenontextual parts remain as images. this process produces an electronic representation of the paper document thathas the same final form as the original and is much smaller than the scanned version. because the paperdocument is a final form, the same final form format can be used, without loss of fidelity, for paper documentsand for the electronically generated documents.electronic document interchange and distribution based on the portable document format,an open interchange format611the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.page fidelity is important. the current state of recognition technology, though very good, is not infallible;there are always some characters that cannot be identified with a high level of confidence. because the pdfformat allows text and image data to be freely intermixed, characters or text fragments whose recognitionconfidence falls below an adjustable level can be placed in the pdf document as images. these images can thenbe read by a human reader even if a mechanical reader could not interpret them. (figure 2 shows a documentcaptured by this process.)the architecture of the document representationthere is more to the architecture of the document representation than meeting the above requirements for afinal form representation. architectures need to be robust and flexible if they are to be useful over a continuingspan of years. pdf has such an architecture.the pdf architecture certainly meets these requirements, as will be clear below. most importantly, pdf hasan open specification: the portable document format reference manual (isbn 0201626284) has beenpublished for several years, and implementations have been produced by several vendors.pdf also goes beyond the final form requirements. for example, the content of pdf files can be randomlyaccessed and the files themselves can be generated in a single pass through the document being converted topdf form. in addition, incremental changes to a pdf file require only incremental additions to the file ratherthan a complete rewrite of the file. these are aspects of pdf that are important with respect to the efficiency ofthe generation and viewing processes.the pdf file format is based on long experience both with a practical document interchange format andwith applications that were constructed on top of that format. adobe illustrator is a graphics design programwhose intermediate file format is based on the postscript language. by making the intermediate format also be aprint format, the output of adobe illustrator could easily be imported into other applications because they couldprint the objects without having to interpret the semantics. in addition, because the underlying semantics werepublished, these objects could be read by other applications when required. the lessons learned in thedevelopment of adobe illustrator went into the design of pdf.pdf, like the adobe illustrator file format, is based on the postscript language. pdf uses the postscriptlanguage imaging model, which has proven itself over 12 years of experience as being capable of faithfullyrepresenting visually rich documents. yet the postscript file format was designed for printing, not for interactiveaccess. to improve system performance for interactive access, pdf has a restructured and simplified descriptionlanguage.experience with the postscript language has shown that, although having a full programming languagecapability is useful, a properly chosen set of highlevel combinations of the postscript language primitives canbe used to describe most, if not all, final form pages. therefore, pdf has a fixed vocabulary of highleveloperators that can be more efficiently implemented than arbitrary combinations of the lowerlevel primitives.the user model for pdf documentsthe user sees a pdf document as a collection of pages. each page has a content portion that represents thefinal form of that page and a number of virtual overlays that augment the page in various ways. for example,there are overlay layers for annotations, such as electronic sticky notes, voice annotations, and the like. there areoverlay layers for hyperlinks to other parts of the same document or hyperlinks to other documents and otherkinds of objects, such as video segments or animations. there is an overlay layer that identifies the threading ofthe content of articles from page to page and from column to column. each of the overlay layers is associatedwith the content portion geometrically. each overlay object has an associated rectangle that encompasses theportion of content associated with the object.each of the layers is independent of the others. this allows information in one layer to be extracted,replaced, or imported without affecting the other layers. this facilitates exporting annotations made on multipleelectronic document interchange and distribution based on the portable document format,an open interchange format612the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.copies of a document sent out for review and then reimporting all the review annotations into a single documentfor responding to the reviewers' comments. this also makes it possible to define hyperlinks and threads on thelayout of a document that only has the test portion present and then to replace the textonly pages with pages thatinclude the figures and images to create the finished document.in addition to the pageoriented navigational layers, there are two documentlevel navigation aids. there isa set of bookmark or outline objects that allow a table of contents or index to be defined into the set of pages.each bookmark is a link to a particular destination in the document. a destination specifies the target page andthe area on that page that is the target for display. destinations can be specified directly or named and referred toby name. using named destinations, especially for links to other documents, allows the other documents to berevised without invalidating the destination reference.finally, associated with each page is an optional thumbnail image of the page content. these thumbnailscan be arrayed in sequence in a slide sorter array and can be used both to navigate among pages and to reorder,move, delete, and insert pages within and among documents.the abstract model of a pdf document: a treeabstractly, the pdf document is represented as a series of trees. a primary tree represents the set of pagesand secondary trees represent the documentlevel objects described in the user model. each page is itself a smalltree with a branch for the representation of the page content; a branch for the resources, such as fonts and imagesused on the page; a branch for the annotations and links defined on the page; and a branch for the optionalthumbnail image. the page content is represented as a sequence of highlevel postscript language imagingmodel operators. the resources used are represented as references to resource objects that can be shared amongpages. there is an array of annotation and link objects.the representation of the abstract treethe abstract document tree is represented in terms of the primitive building blocks of the postscriptlanguage. there are five simple objects and three complex objects. the simple objects are the null object (whichis a placeholder), the boolean object (which is either true or false), the number object (which is an integer orfixed point), the string object (which has between 0 and 65535 octets), and the name object (which is a readonlystring).the three complex objects are arrays, dictionaries, and streams. arrays are sequences of 0 to 65535 objectsthat may be mixed type and may include other arrays. dictionaries are sets of up to 65535 keyvalue pairs wherethe key is a name and the value is any object. streams are composed of a dictionary and an arbitrary sequence ofoctets. the dictionary allows the content of the streams to be encoded and/or compressed to improve spaceefficiency. encoding algorithms are used to limit the range of octets that appear in the representation of thestream. those defined in pdf are asciihex (each hex digit is represented as an octet) and ascii85 (each fouroctets of the stream are represented as five octets). these both produce octet strings restricted to the 7bit asciigraphic character space. compression algorithms are used to reduce storage requirements. those defined in pdfare lzw (licensed from univac), run length, ccitt group 3 and group 4 fax and dct (jpeg).the terminal nodes of the abstract tree are represented by simple objects and streams. the nonterminalnodes are represented by arrays and dictionaries. the branches (arcs) of the tree are represented in one of twoways. the simplest way is that the referenced object is directly present in the nonterminal node object. this iscalled a direct object. the second form of branch is an indirect object reference. objects can be made intoindirect objects by giving the (direct) object an object number and a generation number. these indirect objectscan then be referenced by using the object number and generation number in place of the occurrence of the directobject.indirect objects and indirect object references allow objects to be shared. for example, a font used onseveral pages need only be stored once in the document. they also allow the values of certain keys, such as theelectronic document interchange and distribution based on the portable document format,an open interchange format613the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.length of a stream, to be deferred until the value is known. this property is needed to allow pdf to be producedin one pass through the input to the pdf generation process.the pdf file structureindirect objects and indirect object references do not allow direct access to the objects. this problem issolved by the pdf file structure. there are four parts to the file structure. the first part is a header, whichidentifies the file as being a pdf file and indicates the version of pdf being used in the file. the second part isthe body, which is a sequence of indirect objects. the third part is the crossreference table. this table is adirectory that maps object numbers to offsets in the (body of the) file structure. this allows direct access to theindirectly referenced objects.the final part is the trailer, which serves several purposes. it is the last thing in the file and it has the offsetof the corresponding crossreference table. it also has a dictionary object. this dictionary is the size of the crossreference table. it indicates which indirect object is the root of the document tree. it indicates which object is the"info dictionary," a set of keys that allow attributes to be associated with the document. these keys include suchinformation as author, creation date, etc.finally, the trailer dictionary can have an id key whose value has two parts. both parts are typically hashfunctions applied to parts of the document and key information about the document. the first hash is createdwhen the document is first stored; it is never modified after that. the second is changed whenever the documentis stored. by storing these ids with file specifications referencing the document, one can more accuratelydetermine that the document retrieved via a given file specification is the document that is desired.the trailer is structured to allow pdf files to be incrementally updated. this allows pdf files to be edited,say deleting some pages or adding links or annotations, without having to rewrite the entire file. for large files,this can be a significant savings. this is accomplished by adding any new indirect objects after the existing finaltrailer and appending a new crossreference table and trailer to the end of the file. the new crossreference tableprovides access to the new objects and hides any deleted objects. this mechanism also provides a form of"undo" capability. move the end of the file back to the last byte of the previous trailer and all changes madesince that trailer was written will be removed.the purpose of the generation numbers in the indirect object definition and reference is to allow reuse oftable entries in the crossreference table when objects are deleted. this keeps the crossreference table fromgrowing arbitrarily large. any indirect object reference is looked up in the endmost crossreference table in thedocument. if the generation number in that crossreference table does not match the generation number in theindirect reference, then the reference object no longer exists, the reference is bad, and an error is reported.deleted or unused entries in the crossreference table are threaded on a list of free entries.resourcesthe general form and representation of a pdf file have been outlined. there are, however, several areasthat need further detail. the page content representation is designed to refer to a collection of resources externalto the pages. these include the representations of color spaces, fonts, images, and shareable content fragments.for deviceindependent color spaces, the color space resource contains the information needed to map colors inthat color space to the standard cie 1931 xyz color space and thereby ensure accurate reproduction across arange of devices. images are represented as arrays of sample values that come from a specified color space andmay be compressed. page content fragments are represented as content language subroutines that can be referredto from content. for example, a corporate logo might be used on many pages, but the content operators that drawthe logo need be stored only once as a resource.typically, however, the resources that are most critical for ensured reproduction are the font resources. thecorrect fonts are needed to be able to faithfully reproduce the text as it was published. pdf has a threelevelapproach to font resources. first, there is a set of 13 fonts (12 textual fonts and 1 symbol font) that must beelectronic document interchange and distribution based on the portable document format,an open interchange format614the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.available to the viewer. these fonts can be assumed to exist at any consumer of a pdf document. for otherfonts, there are two solutions. the fonts may be embedded within the document or substitutions may be made forthe fonts if they are not available on the consumer's platform. fonts that are embedded may be either adobetype 1 fonts or truetype fonts and may be the full font or a subset of the font sufficient to display the documentin which they are embedded.the font architecture divides the font representation into three separate parts: the font dictionary, the fontencoding, and the font descriptor. the font dictionary represents the font and may refer to a font encoding and/ora font descriptor. the font encoding maps octet values that occur in a string into the names of the glyphs in afont. the font descriptor has the metrics of the font, including the width and height of glyphs and attributes suchas the weight of stems, whether it is italic, the height of lowercase letters, and so on. the font shape data, ifincluded, are part of the font descriptor. if the font shape data are not included, then the other information in thefont descriptor can be used to provide substitute fonts. substitute fonts work for textual fonts and replace theexpected glyphs with glyphs that have the same width, height, and weight as the original glyphs. if page fidelityis required, then the font shape data should be embedded; but font substitution can be used to reduce documentsize where the omitted fonts are either expected at the consumer's location or font substitution is adequate forreading the document.hyperlinksthe hyperlink mechanism has two parts: the specification of where the link is and the specification of wherethe link goes. the first specification is given as a rectangular area defined on the page content. the secondspecification is called an action. there are a number of different action types. the simplest is moving to adifferent destination within the same document. a more complex action is moving to a destination in anotherpdf document. the destination may be a position in the document, a named destination, or the beginning of anarticle thread. instead of making another pdf document available for viewing, the external reference may launchan application on a particular file, such as a fragment of sound or a video. all these external references useplatform independent file names, which may be relative to the file containing the reference document, to refer toexternal entities.the url (uniform resource locator), as defined for the world wide web, is another form of allowedreference to an external document. a url identifies a file (or part thereof) that may be anywhere in theelectronically reachable world. when the url is followed, the object retrieved is typed and then a program thatcan process that type is invoked to display the object. any type for which there is a viewing program, includingpdf, can thereby be displayed.extensibilitypdf is particularly extensible. it is constructed from simple building blocks; the pdf file is a treeconstructed from leaves that are simple data types or streams and with arrays and dictionaries as the nonterminalnodes. in general, additional keys can be added to dictionaries without affecting viewers who do not understandthe new keys. these additional keys may be used to add information needed to control and/or represent newcontent object types and to define editing on existing objects. because of the flexibility of the extensionmechanism, a registry has been defined to help avoid key name conflicts that might arise when severalextensions are simultaneously present.the architecture for extensionsthe third component architecture for final form electronic document interchange is the extensionarchitecture for consumers of the electronic documents. viewing a pdf document is always possible and thepdf specification defines what viewing means. but, if there are extensions within the pdf file, there must be aelectronic document interchange and distribution based on the portable document format,an open interchange format615the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.way to give semantic interpretation to the extension data. in addition, vendors may want to integrate a pdfconsumer application, such as acrobat exchange, with their applications. for example, a service that providesinformation on stocks and bonds may want to seamlessly display wellformatted reports on particular stocks.this service would like to include the display of pdf documents with their nonpdf information. providingsemantics both for extension data and for application integration can be accomplished using the extensionarchitecture for pdf viewers.it is reasonable to look at pdf viewers as operating system extensions. these viewers provide a basiccapability to view and print any pdf document. by extending the view and print application programminginterfaces (apis), more powerful applications can be constructed on top of the basic view and print capabilitiesof a pdf viewer. these extended applications, called plugins, can access extended data stored in the pdf file,change the viewer's user interface, extend the functionality of pdf, create new link and action types, and definelimited editing of pdf files.the client search mechanism shown in figure 4 was done as a plugin to acrobat exchange. the searchplugin presumes that collections of pdf documents have been indexed using acrobat catalog. the plugin iscapable of accessing the resulting indexes, retrieving selected documents, and highlighting occurrences thatmatch the search criteria. this plugin is shipped with acrobat exchange but could be replaced by other vendorswith another mechanism for building indexes and retrieving documents. hence, pdf files can be incorporatedinto many document management systems.deployment and the futurethe process of pdf deployment has already begun. one can find a variety of documents in pdf form onthe world wide web, on cdroms, and from other electronic sources. these documents range from tax formsfrom the internal revenue service, to color newspapers, commercial advertisements and catalogs, productdrawings and specifications, and standard business and legal documents.use of pdf is likely to increase as more document producers understand the technology and learn that it iswell adapted to current document production processes. the greatest barrier to expansion of consumption isawareness on the part of the consumers. there are free viewers for pdf files, the acrobat readers, available formost major user platforms (dos, sun unix, macintosh os, windows) and more support is coming. theseviewers are available online through a variety of services, are embedded in cdroms, and are distributed ondiskette.at this level, the barrier to deployment is primarily education. but there are also opportunities to improvethe quality of electronic document interchange. some examples of these improvements are better support fornavigational aids; support for other content types, such as audio and video; support for a structuraldecomposition of the document, as is done in sgml; and support for a higher level of document editing.current document production processes naturally produce the final form of the document, but they do notnecessarily enable navigation aids such as hyperlinks and bookmarks/tables of contents. the documentproduction architecture does provide a pathway for this information to be passed to the distillation process andthrough the print drivers. as producers enable this pathway in their document production products, it willbecome standard to automatically translate the representation of navigational information in a documentproduction product into the corresponding pdf representation of navigational aids.another direction for future development is the inclusion of additional content types within a pdf file.(there is already support for referencing foreign content types stored in separate files via the hyperlinkmechanism.) some of the obvious content types that should be included are audio, video, and animation. there isalso a need for orchestrating multiple actions/events when content is expanded beyond typical pages. much ofthe barrier to inclusion of these other content types is in the lack of standard formats for these content types.because pdf is designed to run across all platforms, there is a particular need for standards that are capable ofbeing implemented in all environments. for example, standards that require hardware assists are not as useful asstandards that can be helped by hardware assists but do not require them.electronic document interchange and distribution based on the portable document format,an open interchange format616the unpredictable certainty: white paperscopyright national academy of sciences. all rights reserved.a final form interchange format guarantees viewability of the information in a document, but it does notnecessarily provide for reuse or revision of the information. structural information representations, such assgml and html, can simplify reuse, but they do not capture the decisions of human layout designers. bestwould be a format that allowed both views: the final form view for browsing and reading, and the ability torecover the structural form for repurposing, editing, or structurebased content retrieval. pdf will be extendedto allow the formatted content to be related to the structural information from which it was produced and toallow that structured information to be retrieved for further use.clearly the final form document contains some of the information that is needed to edit the document, but itis equally clear that without extensions to represent structure as well as form the document may not containinformation about how the components of the final form were created and how they might be changed. suchsimple things as what text was automatically generated, what elements were grouped together to be treated as awhole, and into what containers text was flowed need not be represented in the final form.the pdf representation was constructed from a set of primitive building blocks that are also suitable forrepresenting structural and other information needed for editing. augmenting the final form with this kind ofinformation, using these powerful and flexible building blocks, would allow the final form document format tooffer revisability. as a simple example, one might use pdf to represent a photo album as a collection of scannedimages placed on pages. a simple editor might be defined that allows these photos to be reusedšsay, to make agreeting card by combining text with images selected from the photo album. the greeting cards thus constructedcould be represented in pdf using extensions that allow editing of the added text. more complex editing taskscan be accommodated by capturing more information about the editing context within the pdf file generated bythe editing application. for some applications, the pdf file might be the only storage format needed; it would beboth revisable and final form.conclusionthe business case for final form electronic document interchange is relatively straightforward. there aresignificant savings to be achieved simply by replacing paper distribution with electronic distribution, whether ornot the document is printed at the receiving site. the key success factor is whether the document can beconsumed once received. consumption most often means access to the document's contents in the form in whichthey were published. this can be achieved by having a small number of final form interchange formats(preferably one) and universal distribution of viewers for these formats. the portable document format (pdf) isa more than suitable final form interchange format with freely distributable viewers.for practical interchange, there must be tools to conveniently produce the interchange format from existing(and future) document production processes. the interchange format must be able to be transmitted through theelectronic networks and included on disks, diskettes, cdroms, and other physical distribution media. it mustbe open to allow multiple implementations and to ensure against the demise of any particular implementation.finally, it must be extensible to allow growth with the changing requirements of information distribution. thesefeatures all are met by pdf.pdf provides a universal format for distributing information as an electronic document. the informationcan always be viewed and printed. and, with extensions, it may be edited and integrated with other informationsystem components.electronic document interchange and distribution based on the portable document format,an open interchange format617
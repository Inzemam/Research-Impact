detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/11106computer science: reflections on the field, reflections fromthe field216 pages | 6 x 9 | paperbackisbn 9780309093019 | doi 10.17226/11106committee on the fundamentals of computer science: challenges and opportunities;computer science and telecommunications board; division on engineering andphysical sciences; national research councilcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.˘˝˛˛$'''''''''''''''computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, n.w. washington, dc 20001notice: the project that is the subject of this report was approved by thegoverning board of the national research council, whose members are drawnfrom the councils of the national academy of sciences, the national academy ofengineering, and the institute of medicine. the members of the committeeresponsible for the report were chosen for their special competences and withregard for appropriate balance.support for this project was provided by the national science foundation undergrant no. ccr9981754. any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarilyreflect the views of the sponsor.international standard book number 0309093015 (book)international standard book number 0309545293 (pdf)copies of this report are available from the national academies press, 500 fifthstreet, n.w., lockbox 285, washington, dc 20055; (800) 6246242 or (202) 3343313in the washington metropolitan area; internet, http://www.nap.edu.copyright 2004 by the national academy of sciences. all rights reserved.printed in the united states of americacomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the generalwelfare. upon the authority of the charter granted to it by the congress in 1863,the academy has a mandate that requires it to advise the federal government onscientific and technical matters. dr. bruce m. alberts is president of the nationalacademy of sciences.the national academy of engineering was established in 1964, under the charterof the national academy of sciences, as a parallel organization of outstandingengineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsorsengineering programs aimed at meeting national needs, encourages educationand research, and recognizes the superior achievements of engineers. dr. wm. a.wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy ofsciences to secure the services of eminent members of appropriate professions inthe examination of policy matters pertaining to the health of the public. theinstitute acts under the responsibility given to the national academy of sciencesby its congressional charter to be an adviser to the federal government and, uponits own initiative, to identify issues of medical care, research, and education.dr.harvey v. fineberg is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology withthe academys purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the nationalacademy of sciences and the national academy of engineering in providingservices to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute ofmedicine. dr. bruce m. alberts and dr. wm. a. wulf are chair and vice chair,respectively, of the national research council.www.nationalacademies.orgcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.vcommittee on the fundamentals of computer science:challenges and opportunitiesmary shaw, carnegie mellon university, chairalfred v. aho, columbia universitycharles h. bennett, ibm researchalan biermann, duke universityedward w. felten, princeton universityjames d. foley, georgia institute of technologymark d. hill, university of wisconsin at madisonjon m. kleinberg, cornell universitydaphne koller, stanford universityjames r. larus, microsoft researchtom m. mitchell, carnegie mellon universitychristos h. papadimitriou, university of california, berkeleylarry l. peterson, princeton universitymadhu sudan, massachusetts institute of technologykevin j. sullivan, university of virginiajeffrey d. ullman, stanford university and gradience corporationstaffjon eisenberg, senior program officer and study directorlynette i. millett, program officerd.c. drake, senior project assistant (through november 2003)computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.vicomputer science and telecommunications boarddavid liddle, u.s. venture partners, cochairjeannette m. wing, carnegie mellon university, cochaireric benhamou, 3com corporationdavid d. clark, massachusetts institute of technology, cstbmember emerituswilliam dally, stanford universitymark e. dean, ibm systems groupdeborah l. estrin, university of california, los angelesjoan feigenbaum, yale universityhector garciamolina, stanford universitykevin kahn, intel corporationjames kajiya, microsoft corporationmichael katz, university of california, berkeleyrandy h. katz, university of california, berkeleywendy a. kellogg, ibm t.j. watson research centersara kiesler, carnegie mellon universitybutler w. lampson, microsoft corporation, cstb member emeritusteresa h. meng, stanford universitytom m. mitchell, carnegie mellon universitydaniel pike, gci cable and entertainmenteric schmidt, google inc.fred b. schneider, cornell universitywilliam stead, vanderbilt universityandrew j. viterbi, viterbi group, llccharles brownstein, directorkristen batch, research associatejennifer m. bishop, program associatejanet briscoe, administrative officerjon eisenberg, senior program officerrenee hawkins, financial associatemargaret marsh huynh, senior project assistantherbert s. lin, senior scientistlynette i. millett, program officerjanice sabuda, senior project assistantbrandye williams, staff assistantfor more information on cstb, see its web site at <http://www.cstb.org>,write to cstb, national research council, 500 fifth street, n.w., washington, dc 20001, call at (202) 3342605, or email the cstb at cstb@nas.edu.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.prefaceviithe blossoming of computer science (cs) research is evident in theinformation technology that has migrated from a specialized toolconfined to the laboratory or corporate back office to a ubiquitouspresence in machines and devices that now figure in the lives of virtuallyevery individual. this widespread diffusion of information technologycan obscure the nature of computer science research underlying the itfrom the perspective of many outside the field, computer science is seennot as a basic area of systematic inquiry but as a tool to support otherendeavors.mindful of these issues, the national science foundations computerand information science and engineering directorate asked the computer science and telecommunications board of the nationalacademies to conduct a study that would improve understanding of csresearch among the scientific community at large, policymakers, and thegeneral public. by describing in accessible form the fields intellectualcharacter and by conveying a sense of its vibrancy through a set ofexamples, the committee also aims to prepare readers for what the futuremight hold and inspire cs researchers to help create it.this volume, the product of that study, is divided into two parts thatcontain nine chapters.the volumes prelude, emily shops at virtualemporia.com, takes anowfamiliar use of computingshopping onlineand illustrates howcs research has made this seemingly simple activity possible.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.viiiprefacepart onechapter 1, the essential character of computer scienceoffers the committees concise characterization of cs research. like csresearchers more generally, the committee members evince a wide rangeof perspectives that mirror the broad reach of computation into the veryfabric of our intellectual and physical lives. recognizing the richness anddiversity of the field, the committee expressly decided not to provideeither a comprehensive list of research topics or a taxonomy of researchareas, nor to develop criteria for what research is inside and outside of cs.instead, the committees approach is to describe some key ideas that lie atthe core of cs but not to define boundaries.part twochapters 2 through 9comprises two dozen essays written by committee members, participants in a june 67, 2001, symposiumorganized by the committee, and other invited authors. the essaysdescribe several aspects of cs research and some of the results from theperspectives of their authors. by providing this diverse set of views on csresearch, the committee aims to express some of the spark that motivatesand excites cs researchers. the essays have a deliberately historical focus,for three reasons: (1) as described above, the committee decided not topresent a research agenda, either explicit or implicit; (2) other publications look at current, hot topics in cs and these tend, in any case, tobecome dated quickly; and (3) results that have proven durable best illustrate the strengths of cs.the prelude and part one are intended to be accessible to all readers(as are many of the essays). but because this report is also intended toreach scientists and engineers from a variety of disciplines, a few of theessays do presume some familiarity with some technical concepts.the committee would like to thank all of the participants in the june2001 symposium; presentations and informal discussions at that event provided important input to the committee. julie sussman, ppa, provided anumber of helpful suggestions concerning the manuscript. the reviewerslisted below provided many valuable suggestions for improvement.mary shaw, chaircommittee on the fundamentals of computer science:challenges and opportunitiescomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.acknowledgment of reviewersixthis report has been reviewed in draft form by individuals chosenfor their diverse perspectives and technical expertise, in accordancewith procedures approved by the national research councilõs reportreview committee. the purpose of this independent review is to providecandid and critical comments that will assist the institution in making itspublished report as sound as possible and to ensure that the report meetsinstitutional standards for objectivity, evidence, and responsiveness tothe study charge. the review comments and draft manuscript remainconfidential to protect the integrity of the deliberative process. we wishto thank the following individuals for their review of this report:david d. clark, massachusetts institute of technologyrobert l. constable, cornell universityronald fedkiw, stanford universityjoan feigenbaum, yale universityjuris hartmanis, cornell universityjames jay horning, intertrustanna r. karlin, university of washingtonrichard karp, university of california, berkeleywendy a. kellogg, ibm researchmonica s. lam, stanford universitybutler w. lampson, microsoft researchfred b. schneider, cornell universitylynn andrea stein, olin collegecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.xacknowledgment of reviewersgerald jay sussman, massachusetts institute of technologythomas n. theis, ibm t.j. watson research centerjeanette m. wing, carnegie mellon universitymargaret h. wright, new york universityalthough the reviewers listed above provided many constructivecomments and suggestions, they were not asked to endorse the conclusions or recommendations nor did they see the final draft of the reportbefore its release. the review of this report was overseen by lawrencesnyder, university of washington. appointed by the national researchcouncil, he was responsible for making certain that an independentexamination of this report was carried out in accordance with institutionalprocedures and that all review comments were carefully considered.responsibility for the final content of this report rests entirely with theauthoring committee and the institution.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.preludeemily shops at virtualemporia.com1part onethe essential character of computer science91the essential character of computer science11what is computer science?, 12salient characteristics of computer science research, 15computer science research involves symbols and theirmanipulation, 15computer science research involves the creation andmanipulation of abstraction, 17computer science research creates and studiesalgorithms, 19computer science research creates artificial constructs,notably unlimited by physical laws, 19computer science research exploits and addressesexponential growth, 20computer science research seeks the fundamental limits onwhat can be computed, 21computer science research often focuses on the complex,analytic, rational action that is associated withhuman intelligence, 23xicontentscomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.xiicontentspart twoselected perspectives on computer science252exponential growth, computability, andcomplexity27harnessing moores law, 28mark d. hill, university of wisconsin, madisoncomputability and complexity, 37jon kleinberg, cornell university, andchristos papadimitriou, university of california, berkeleyquantum information processing, 51charles h. bennett, ibm research3simulation57the real scientific hero of 1953, 58steven strogatz, cornell universitymaking a computational splash, 61ronald fedkiw, stanford university4abstraction, representation, and notations65abstraction: imposing order on complexity in softwaredesign, 66mary shaw, carnegie mellon universityprogramming languages and computer science, 74alfred v. aho, columbia university, andjames larus, microsoft research5data, representation, and information79database systems: a textbook case of research paying off, 80jim gray, microsoft researchcomputer science is to information as chemistry is tomatter, 88michael lesk, rutgers universityhistory and the fundamentals of computer science, 96edward l. ayers, university of virginia6achieving intelligence101the experimentanalyzegeneralize loop in computer scienceresearch: a case study, 103tom mitchell, carnegie mellon universityim sorry dave, im afraid i cant do that: linguistics,statistics, and naturallanguage processing circa 2001, 111lillian lee, cornell universitycomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.contentsxiiicomputer game playing: beating humanity at itsown game, 119daphne koller, stanford university, andalan biermann, duke university7building computing systems of practical scale127the internet: an experiment that escaped from the lab, 129larry peterson, princeton university, anddavid clark, massachusetts institute of technologymanytomany communication: a new medium, 134amy bruckman, georgia institute of technologycryptography, 144madhu sudan, massachusetts institute of technologystrategies for software engineering research, 151mary shaw, carnegie mellon university8research behind everyday computation159how you got microsoft word, 161jeffrey ullman, stanford university and gradience corporationvisicalc, spreadsheets, and programming for the masses,or how a killer app was born, 167james d. foley, georgia institute of technologyinternet searching, 174peter norvig, google inc.9personal statements of passion about computerscience research179the legacy of computer science, 180gerald jay sussman, massachusetts institute of technologyfairy tales, 184allen newell, carnegie mellon universityrevisiting what is computer science, 189allen newell, carnegie mellon universityappendixagenda of july 2526, 2001, symposium193computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.1preludeemily shops at virtualemporia.comjust a decade ago, the internet was the domain of specialists and technologyaficionados, requiring knowledge of file systems, format compatibilities, and operating system commands. even the more userfriendly systems such as email andnet news principally served relatively small communities of technically savvypeople.until recently, the internet, the world wide web, and ecommerce all wouldhave seemed akin to magic to all but the most techsavvy. yet despite todayõs widespread acceptance of and familiarity with computer capabilities, the details of howcommonly used computer systems work remains a mystery for nonspecialists. it isnot magic, of course, that is at work. nor did todayõs system arise as a result of adirect evolution of previous technology.like many radical innovations, ecommerce, for one, was not planned or evenanticipated by those involved in either research or commerce. rather, it evolvedfrom a series of technical results that were pursued with other motivationsñsuchas sharing computer resources or scientific information among researchers.examining the scientific roots of ecommerce shows how research pursued for itsown sake can enable important, often unanticipated capabilities.we take as our example a hypothetical online retailer, virtualemporia.com, andreveal some of the magicñthat is, the research foundationsñbehind the nowsimple operation of ordering a book online. thus it is possible to identify some ofthe computer science (cs) research that enables retailing online and to providepointers to discussions of that research later in this volume. also noted are someof the ways that a virtual store can provide an intellectual boost over a conventionalstore through its capabilities for searching and indexing, exchanging informationamong customers, and providing an enormous catalog of items.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.2computer science: reflectionsemily is planning to take atrip next week and sheõs lookingfor something to read on theairplane. in the past, she wouldhave gone to a bookstore duringbusiness hours and browsedthrough the shelves to select abook from the limited stock athand. today, emily has theoption of sitting down at acomputer, at any time of the dayor night, to select and buy a bookor many other types of products.emilyõs computeremilyõs family, like morethan half of the households in theunited states, owns a personalcomputer and related software.her gardenvariety home pc,available for roughly $600, is ableto run much more sophisticatedprogramsñand run them muchfasterñthan the first computerher employer bought only 20years ago. indeed, the very ideaof a home computer, outside theprofessional or aficionado market,is only about 20 years old.if emily were to pause andconsider the functionalityavailable to her, she might marvelthat no other machine is asflexible, generalpurpose, ormalleable as a computer. the ideathat machines can be adapted tocompletely new situations forwhich they were not originallydesigned usually borders on thefarcical. emilyõs pencil will notalso serve as a stapler if the needarises. computers, though, arealthough shopping online is now a routineexperience for millions, its familiarity andsimplicity mask the sophisticated eventsbehind the scenes. indeed, what emilydoes in the next 5 minutes would havebeen impossible without many discoveriesand inventions from computer science.the cost of computing power hasdecreased dramatically. a typical 2003home pc, which had a 2.66 ghz systemclock, 256 mb of ram, and a 40 gb harddrive, outperformed the ibm/xt pc,released in 1983, which had a 4.77 mhzsystem clock, 640 kb of ram, and a 10mb hard drive, by a factor of roughly500ñat onetenth the cost. hill (inchapter2) describes the phenomenon ofexponential growth in computing power,and he shows how computer scienceresearch strives to design computers thatsustain this remarkable rate ofimprovement in the cost/performance ratioof computers.emilyõs computer and software areremarkable not only for their low cost butalso for their high complexity. thehardware comprises billions of transistors,and the software installed on the computeris defined by tens of millions of lines ofcode; hundreds of millions of lines ofadditional software is available. thecapability of the computer is built up fromthese tiny elements.shaw (in chapter 4) describes howabstraction hides the complex and oftenmessy details of a piece of hardware orsoftware in favor of a simpler, morefocused view of the aspects of immediatecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.prelude: emily shops at virtualemporia.com3universal symbol manipulatorscovering the entire gamut ofinformation processing. the rightsoftware transforms a computerfrom a web browser to a homeaccountant to a photo editor to ajukebox to a mailbox to a game,virtually at your whim.although emilyõs computerarrived at her home with a largeamount of software preinstalled(including the web browser thatemily uses to shop atvirtualemporia.com), it can alsorun tens of thousands ofadditional programs. of course,emily will actually use only asmall fraction of those programs,but she has enormous variety tochoose from. these programspotentially enable emily to useher computer for many differenttasks, ranging from applicationsinconceivable before we hadcomputersñsuch as email,online messenger services, or chatroomsñto computerbasedenhancements to more traditionaltasks like creating and typesettingdocuments, organizing abusiness, or tracking investments.relevance. this makes it possible to buildenormously complex systems out of atower of abstractions, one layer at a time.systematic research on algorithms anddata structures was also necessary tobuild such complex software and hardwaresystems.nothing elseñaside from a personñis auniversal machine that can be taught orprogrammed to accomplish a very widerange of new tasks. kleinberg andpapadimitriou (in chapter 2) show how theidea of universality rests on the notion ofthe universal turing machine and thechurchturing hypothesis about theuniversality of computers.the capabilities of emilyõs computer canbe improved as current programs areimproved, and even notyetconceivedcapabilities can be added as newprograms are written. software toolsñprograms that manipulate and transformother programsñmake it possible tocreate new applications. the mostimportant of these tools are theprogramming languages that provide thegrammar, syntax, and semanticsprogrammers use to convey their ideas tocomputers. a wide range of programminglanguages and tools are available todayand new languages and tools are thesubject of computer science research. seeaho and larus in chapter 4.humancomputer interaction research hasled to improved user interfaces that makeit easier for people to work with thissoftware. foley (in chapter 8) discusseshow research on user interfaces madespreadsheet software easier to use.ullman (in chapter 8) describes thecomputer science research that led totodayõs word processors.foley (in chapter 8) describes how avariety of cs research results madepossible another kind of computerprogram, the spreadsheet.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.4computer science: reflectionsvisiting virtualemporia.comequally remarkably, emilyõscomputer is easily connected tothe internet. this allows her toobtain information from billionsof web pages, communicate withothers using email or chatprograms, or use a myriad ofother internetdelivered services.once she is online and hasopened a web browser, she canvisit virtualemporia.com simplyby typing òhttp://www.virtualemporia.comó orselecting a bookmark. hercomputer translates the name ofthe destination site into aninternet address and sends amessage across the internet tovirtualemporia, asking for a webpage, which contains links to therest of virtualemporiaõs network.the internet, which made data exchangeamong distant computers fast, easy, andcommonplace, marked a striking changein the use of computers. the internet alsorepresented a striking change in howcommunications networks were designed.the internet is a distributed, faulttolerantsystem for communicating amongcomputers. its design specifies very littleabout the form or content of datañthataspect is left to the applications andsystems enabled by the internetõs packetdatatransport mechanisms (see petersonand clark in chapter 7). with few changesto its design or implementation, theinternet grew from approximately 300,000interconnected machines (hosts) in 1990to over 170 million hosts in 2003.1although the internet was originallyconstructed to allow researchers to sharedata and remotely access expensivecomputers, users of the early internetquickly made email the most popularapplication, and it remains the mostcommon use of the internet today. today,the internet is opening up many newavenues for communication among people(see bruckman in chapter 7).the internetõs success derives in largepart from the fact that it provides effectiveabstractions to enable computersthroughout the world to find each otherand exchange data. here, emilyõs browserand virtualemporia.comõs web site use acommunication protocol called http that isbuilt on top of the internet.the world wide web, another of themany uses of the internet, owes its originsin part to earlier computer scienceresearch on hypertext, which provides away of navigating among linkeddocuments.1internet systems consortium (isc), 2004, òinternet domain survey,ó isc, redwood city,calif., january. available online at http://www.isc.org/ds/.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.prelude: emily shops at virtualemporia.com5shoppingwhen emily visitsvirtualemporia.comõs web site,virtualemporia.com recognizesemily as an established customer.it retrieves her records andproduces a customized homepage, based on her past shoppinghabits, that suggests a few itemsshe might want to buy. as shenavigates through the site, thepages she views are alsocustomized. virtualemporia.comsuggests new books that mightinterest her, based on previousitems she has purchased orsearched for. in addition, whenshe looks at items or adds themto her shopping cart,virtualemporia shows her moreitems that customers with similarshopping interests have selected.emily can also refine herchoices by reading reviews on thevirtualemporia site, includingreviews published by recognizedcritics and comments from othercustomersñwhich she takes moreor less seriously depending onthe credibility ratings thesecomments have accumulated.emily can browse throughthe items for sale or search thedatabase of available products.emily first enters a search requestby authorõs name. she receives aresponse in a second or less, eventhough virtualemporiaõsdatabase contains informationabout millions of books and otheritems for sale, has tens of millionsof registered customers, and mayresearch on collaborative filtering led tomachinelearning algorithms that correlatea new shopperõs interests with those ofother shoppers in the database, so thatdata from the entire shopping communitycan be used to learn emilyõs likelyinterests. mitchell (in chapter 6) discussessome other applications of machinelearning.computer science research on reputationsystems allows virtualemporia to providesome indications of how much trust oneshould place in opinions contributed bystrangers.like many modern organizations such asairlines, banks, and governments,virtualemporia could not exist in its currentform without database systems. gray (inchapter 5) describes the research that ledto the relational model for databases andmodern database systems. building onthis discovery, computer scientists havedeveloped a rich collection ofrepresentations and algorithms formanipulating data; this makes it feasible tostore, index, search, update, and sortbillions of records in reasonable amountscomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.6computer science: reflectionsreceive queries from hundreds ofusers each second. emily does notfind the book (she doesnõt quiteremember the authorõs name), butshe does remember the name of acharacter in the book, which sheuses in a fullcontent search tolocate the desired book.the online setting enablesbusiness partnerships that werepreviously difficult or impossible.virtualemporia also allows emilyto consider offerings frommultiple booksellers, includingnot only a large seller who canprovide almost all bookscurrently in print, but alsonumerous usedbook sellers, bothcompanies and individuals. whenemily searches for a book, she notonly gets information about howto order that book and othereditions of that book but also theopportunity to buy a used copyfrom the many booksellers andindividuals who list books atvirtualemporia.paying for the bookafter some browsing andsearching, emily finds the booksshe wants and goes to thecheckout page, which asks for acredit card number.emilyõs credit cardinformation is automaticallyencrypted by her web browserbefore it is sent over the internet,making it harder for anunauthorized person to obtain it,even though the information isof time, and to ensure their integrity in theface of overlapping queries and updates.the ability to store and rapidly processsuch enormous and growing volumes ofinformation also depends on work leadingto the everincreasing performance ofcomputers (see hill in chapter 2).sudan (in chapter 7) explains how publickey encryption, a technology stemmingfrom cryptology research, makes itpossible for a business such asvirtualemporia.com to engage in securecommunications with millions ofindividuals.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.prelude: emily shops at virtualemporia.com7traveling across the internetõsshared infrastructure and couldpotentially be processed by tensor hundreds of computers alongthe path between her computerand virtualemporia.shipping the bookvirtualemporiaõs shippingsystem makes use of softwarealgorithms to decide whichwarehouse to use for the booksemily has ordered, whether thebooks will all be shipped in onepackage or separately to helpspeed the books to emily, whichshipper to select, and so forth.even though the books arephysical objects that traveled byairplane and truck, their rapidjourney is assisted by theshipperõs computerized logisticssystems that inventory, schedule,and track the booksõ journey.these systems also make itpossible for emily to track apackage through a web site andreceive updates on when she canexpect the package to arrive.conclusionemilyõs books will arrive ather home a few days later.impatiently awaiting herpurchase, emily starts to wonderwhy the book even had to be aphysical object: why couldnõt itjust be delivered as a digitalobject, transmitted over theinternet? indeed, computerscience research has alreadyinformation encoded with a public key canbe decrypted only with the correspondingprivate key. virtualemporia.com publishesits public key, which anyone can use toencrypt information and securely send it tovirtualemporia.com. onlyvirtualemporia.com, however, knows theprivate key for decrypting thesemessages. this technique can be appliedboth ways: software on emilyõs computercan use the secure channel tovirtualemporia.com to create a new one inthe reverse direction (fromvirtualemporia.com to emily). thereforeconfidential twoway communication canproceed, protected from prying eyes.a special secure version of the worldwide web communication protocolprovides a layer of abstraction that makesit possible for emily to use securecommunication without knowing about thedetails.inventory management and routing areboth examples of activities involvinginformation manipulation. before computerscience, these processes could beaccomplished only by hand. computerizedinventory and routing algorithms havemade express shipping much moreefficient. the result has been a reductionof both the costs and the elapsed time fordelivery.like shopping, package tracking dependson database systems. it also depends onwireless communications from the deliverypeople who carry portable wirelessdevices. these devices owe their originsto research on wireless networks and lowpower devices.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.8computer science: reflectionsprovided many of the necessaryfoundations of electronic booksand has built prototypes of all thebasic components. electronicbooks have just entered thecommercial marketplace and maybecome more commonplace onceinexpensive and practical booklike reading devices becomeavailable.an òearly adopter,ó emilydecides she wants something toread right away. she goes tovirtualemporiaõs ebooksdepartment, orders anddownloads a book, and startsreading it within minutes.perhaps what is most remarkable about emilyõs shopping experience is thatitõs so commonplace today (and electronic books may become commonplace justover the horizon), while just 10 years ago it seemed farfetchedñeven as thecomputer science groundwork was being laid. what other stories, which may seemfantastic today even as computer science research is laying their foundations, willwe come to label mundane 10 and 20 years from now?computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.part onethe essential character ofcomputer sciencecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.111the essential character ofcomputer sciencecomputer science began to emerge as a distinct field in the 1940sand 1950s with the development of the first electronic digital computers. their creation reflected several coincident factorsñthedevelopment of computational theory in the 1930s and 1940s, the existence of compelling computational applications (starting with wartimeneeds such as codebreaking and the calculation of artillery trajectories),and the availability of electronic components with which to implementcomputers for those applications. a number of mathematicians, engineers,economists, and physicists turned their attention to mastering andenhancing the capabilities of this novel tool. but computers proved sopowerful and absorbing, so interesting and openended, and so uniquelychallenging that many of these people realized, a decade or so later, thatthey had in fact left their original disciplines and were pioneering anew field.computer science embraces questions ranging from the properties ofelectronic devices to the character of human understanding, from individual designed components to globally distributed systems, and fromthe purely theoretical to the highly pragmatic. its research methods arecorrespondingly inclusive, spanning theory, analysis, experimentation,construction, and modeling. computer science encompasses basic researchthat seeks fundamental understanding of computational phenomena, aswell as applied research. the two are often coupled; grappling with practical problems inspires fundamental insights. given this breadth anddiversity, the discussion that follows does not aim to explicitly or comprecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.12computer science: reflectionshensively define computer science or to catalog all of the research areas.(indeed, such an effort would inevitably bog down in classifying subdisciplines of the field and declaring interdisciplinary activities as òinó oròout.ó) instead, the approach is to indicate and illustrate the essentialcharacter of the field through a sampling of representative topics.what is computer science?computer science is the study of computers and what they can doñtheinherent powers and limitations of abstract computers, the design and characteristics of real computers, and the innumerable applications of computers to solving problems. computer scientists seek to understand how to representand to reason about processes and information. they create languages forrepresenting these phenomena and develop methods for analyzing andcreating the phenomena. they create abstractions, including abstractionsthat are themselves used to compose, manipulate, and represent otherabstractions. they study the symbolic representation, implementation,manipulation, and communication of information. they create, study,experiment on, and improve realworld computational and informationsystemsñthe working hardware and software artifacts that embody thecomputing capabilities. they develop models, methods, and technologiesto help design, realize, and operate these artifacts. they amplify humanintellect through the automation of rote tasks and construction of newcapabilities.computer hardware and software have been central to computer science since its origins. however, computer science also encompasses thestudy and more general application of principles and theories rooted in ormotivated by hardware and software phenomena. computer science hasthus come to encompass topics once distinctly part of other disciplines,such as mathematics, originally motivated by computing and conceptualquestions of informationhandling tasks such as naturallanguage processing. computer science research is often intimately intertwined withapplication, as the need to solve practical problems drives new theoreticalbreakthroughs.the accompanying essays in this volume elaborate on some important examples of the results of computer science research. these include:¥the universal turing machine and the churchturing thesis, whichprovide a theoretical underpinning for understanding computing (seekleinberg and papadimitriou in chapter 2);¥computer programs that achieve, exceed, or augment human performance levels in challenging intellectual tasks (see koller and biermanand also mitchell in chapter 6);computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.the essential character of computer science13¥the theory of algorithmsñformal expressions of proceduresñwhichseparate algorithmic behavior from the specific code that implements it(see kleinberg and papadimitriou in chapter 2);¥programming languages, which are notations specifically designedto represent computational processes, or how things happen (see ahoand larus in chapter 4);¥the relational model of data, which provided a systematic way toexpress complex associations among data and revolutionized the database industry and provided the basis for nearly all business computing(see gray in chapter 5);¥the internet, a reliable system created from unreliable parts thatdefines a simple, generalpurpose abstraction of a packet network onwhich a wide range of applications can be constructed (see peterson andclark in chapter 7);¥simulation, which permits the study and visualization of bothnatural and manmade phenomena (see fedkiw in chapter 3); and¥software systems that allow nonexperts to use computers (seefoley and also ullman in chapter 8).computer scienceõs striking research advances have touched our livesin profound ways as we work, learn, play, and communicate. even thoughcomputer science is a relatively young field, born only within the pastseven decades, the pace of innovation in it has been extraordinary. onceesoteric, expensive, and reserved for specialized tasks, computers are nowseemingly everywhere. applications and technologies that are now fixtures in many peopleõs lives and work (such as office automation,ecommerce, and search engines) were nonexistent or barely visible just adecade ago. the personal computer itself was first introduced less thenthree decades ago, yet most office workers are now assigned a pc as amatter of course, and roughly half of all households in the united statesown at least one. computers are central to the daily operations of banks,brokerage houses, airlines, telephone systems, and supermarkets. evenmore computers lurk hidden inside handheld cell phones, personal digitalassistants, and automobiles; for example, a typical midmarket automobilecontains a network and dozens of processors. as the size and cost ofcomputer hardware shrink, computers will continue to proliferate evenmore widely (see hill in chapter 2). all these computers have providedsociety with tremendous social and economic benefits even as they posenew challenges for public policy and social organizations.advances in computer science have also led to fundamental changesin many scientific and engineering disciplines, enabling, for example,complex numerical calculations that would simply be infeasible ifattempted by hand. computer science has provided highly useful toolscomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.14computer science: reflectionsfor controlling experiments, collecting, exchanging, and analyzing data,modeling and simulation, and for sharing scientific information. indeed,finding the right data structure or algorithm can revolutionize the way inwhich a scientist thinks about a problem. for example, computer sciencealgorithms made it possible to put together a vast amount of data fromsequencing machines when the human genome was sequenced. in recentyears, reflecting informationõs central importance in scholarly work andscience, computing has also taken on new importance in many other disciplines as well; ayers (in chapter 5), for example, discusses ways inwhich historians are using computing. computer scienceõs computationalparadigm has also shaped new modes of inquiry in other fields, such asgenomics and related areas of biology.one driver of computer science innovation is the doubling of hardware performance that we have seen roughly every 11/2 to 2 years (see hillin chapter 2). another is the invention of myriad new applications ofcomputers, whose creation is made possible by the tremendous flexibilityof software. computer applications are largely limited only by humanimagination, although there are fundamental limits on what is computable and there are significant engineering challenges in building complexsystems.this volume explores computer science research, emphasizing howresearch leads both to deeper understanding of computation and tonumerous practical applications. many others have celebrated the accomplishments of computer science or offered predictions about future directions.1 the emphasis in this volume is on why and how computerscientists do their research.part two of this volume consists of a set of individually authoredessays that provide a sampling of perspectives on several areas of computer science research. these are intended to exemplify some of theobservations made in this chapter and illustrate some of the flavor ofcomputer science.this chapter broadly considers the essential character of computerscience (what does computer science investigate, design, and create?),and its methods and modes of research (how do computer scientists1for example, cathy a. lazere and dennis elliott shasha, 1998, out of their minds: thelives and discoveries of 15 great computer scientists, copernicus books, celebrates accomplishments of the field. works describing future research directions include cstb, nrc, 1992,computing the future, national academy press; peter j. denning and robert m. metcalfe,eds., 1997, beyond calculation: the next fifty years of computing, springerverlag; and peterj. denning, ed., 1999, talking back to the machine: computers and human aspiration, springerverlag. bruce w. arden, 1980, what can be automated: the computer science and engineeringresearch study, mit press, was an attempt to comprehensively survey the field.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.the essential character of computer science15approach problems? what methods do they use? and what types of resultsemerge?). the discussion below elucidates seven major themes withoutattempting to fully enumerate all the research subfields within computerscience, to prescribe a research agenda, or to define the boundaries ofcomputer science.salient characteristics ofcomputer science researchthe character of a research field arises from the phenomena it seeks tounderstand and manipulate together with the types of questions the discipline seeks to answer about these phenomena. this section identifiesphenomena and intellectual challenges central to computer science,describing the key ideas and the identifying work that has helped todevelop those ideas. these broad themes (summarized in box 1.1), moretimeless than a comprehensive inventory of current topics would be, portray core ideas that computer science is concerned with.computer science research involves symbols andtheir manipulationtwo of the fundamental techniques of computer science research arethe manipulation of discrete information and symbolic representation.some information is inherently discrete, such as money. discrete approximation enables every kind of information to be represented within thecomputer by a sequence of bits (choices of 0 or 1) or the often morebox 1.1salient characteristics of computer science research¥computer science research involves symbols and their manipulation.¥computer science research involves the creation and manipulation ofabstractions.¥computer science research creates and studies algorithms.¥computer science research creates artificial constructs, notably unlimitedby physical laws.¥computer science research exploits and addresses exponential growth.¥computer science research seeks the fundamental limits on what can becomputed.¥computer science research often focuses on the complex, analytic, rationalaction that is associated with human intelligence.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.16computer science: reflectionsconvenient bytes (sequences of 8 bits, representing characters such asletters) to which the proper interpretation is applied. a digital image ofany one of van goghõs paintings of sunflowers (call it sunflowers, forshort) divides the continuous painted canvas into many small rectangularregions called pixels and gives the (approximate) color at each of these.the collection of pixels can, in turn, be thought of as a sequence of bits. bitsequences, being a òdigitaló or discretevalued representation, cannotfully represent precise, realvalued or òanalogó information (e.g., the precise amount of yellow in a sunflower). practically, though, this apparentlimitation can be overcome by using a sufficiently large enough numberof bits, to, for example, represent the analog information as precisely asthe eye can see or the ear can hear.this sequence of bits can be processed in one way so that a person cansee the sunflowers image on a display or processed another way for aprinter to print an image. this sequence of bits could be sent in an emailmessage to a friend or posted to a web page. in principle, the samesequence of bits could be passed to an audio interpreter; however, theaudio produced by sunflowers would not sound very good, since this bitsequence is unlikely to represent anything reasonable in the symbol system used by the audio player. sunflowers could be executed as a programon the computer, since programs are themselves bit strings; again, however, the result will probably not produce anything meaningful, since it isunlikely the paintingõs representation is a sequence that will do something useful.more subtle than discrete approximation, yet more powerful, is thetechnique of symbolic representation. here the underlying bits are usedto represent symbols in some notation, and that notation is used to represent information in a way that permits analysis and further processing.for example:¥for analysis of differences among sunflowers or between sunflowers and marigolds, flowers could be described in terms of their geneticcode.¥for graphical animation, a sunflower might be represented by adescription of the color and shape of its parts together with how they areconnected to each other.¥to denote different varieties, sunflowers might be represented bya set of symbols consisting of words in english.the right symbol systems, properly interpreted, allow one to writeprograms that represent and manipulate sounds or images, that simulatephysical phenomena, and even that create artificial systems without ananalog in nature.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.the essential character of computer science17several of the essays in this volume deal with symbolic representation. the essay by kleinberg and papadimitriou in chapter 2 discussesthe formal representation of computation itself as symbol strings. lesk inchapter 5 mentions many of the important kinds of representations beingstored in computers as òdigital libraries.ócomputer science research involves the creation andmanipulation of abstractioncomputer science often involves formulating and manipulatingabstractions, which are coordinated sets of definitions that capture different aspects of a particular entityñfrom broad concept to its detailedrepresentation in bitsñand the relations through which some of the definitions refine others or provide additional concepts to help realize others.one aspect of abstraction is that sequences of bits take on specificuseful interpretations that make sense only in particular contexts. forexample, a bit sequence can be thought ofñin some contextsñas aninteger in binary notation; using that abstraction, it makes sense to dividethe integer by 3 and get another sequence of bits that is the quotient. butin a context that abstracts bit sequences as images, it would not be reasonable to divide sunflowers by 3 and get something meaningful, nor would itmake sense to divide a dna sequence by 3. but the abstraction of bits as,for example, images, should permit operations that make no sense onintegers, such as cropping (removing bits that represent pixels at the edgesof an image) or applying a watercolor filter in an imagemanipulationprogram. similarly, the abstraction of bits as dna sequences should permit analysis of differences among plant species that would make no sensefor integers or images.abstraction also makes it possible to perceive and manipulate thesame computerrepresented òobjectó at many levels. objects at one levelmake available to higher levels a fixed set of operations, and are perceived by the higher level as if these were the only operations that couldever be performed on them. for example, the bit sequence representingsunflowers and other paintings could be thought of as part of a relation(twodimensional table of data), in which each row contains the bits representing an image, another bit sequence representing the name of thepainting, a third bit sequence representing the name of the artist, and soon. at the lowest level, there are operations that manipulate the individual entries in each row; for example, the image bit string could beconverted to a black and white image by manipulating the color information. at the next level, one considers the entire relation as an object; operations on the relation, rather than manipulating pixels or displaying images,instead perform such operations as òfind all the images of paintings bycomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.18computer science: reflectionsvan gogh.ó if the image abstraction permitted operations like òtell if thisimage has a region that the typical person would perceive as a sunflower,óthen the relation abstraction would permit searches like òfind the namesof all paintings with one or more sunflowers in them.ó alas, that sort ofoperation on images is beyond the state of the art at the beginning of the21st century. however, operations at the imagelevel abstraction can beused to determine if an image has a large region of yellowishorange, andthus to support a question at the level of the table such as òfind the namesof paintings with large yelloworange regions.óas alluded to above, abstractions apply to procedures as well as data.consider a bit sequence representing a piece of computer code that performs a sequence of operations on sunflowers. one set of bits might provide the (detailed) machine instructions to display the image on a particular computer. another set of bits might be the program to animate amodel of a sunflower blowing in the breezeñwhen interpreted by thecompiler for that language (and, of course, when the object code is thenexecuted). yet a third set of bits might be in an animation language thatpermits the motion of a sunflower to be described at a high level.the print function common across applications on most systemstoday, which allows a wide range of data objects to be printed on a widerange of printers, is a powerful example of abstraction. the user need notknow about all the details hiding behind this simple interfaceñthe òprintócommand is an abstraction that shows the user only what he or she needsto see.in each case, the bits are interpreted by a model that abstracts procedures above the actual machine instructions that a computer executes.observe also that bits can at the same time be both data and procedureñthe compiler reads a program to produce an output (data) that will laterbe executed by a computer (procedure).the internet works today because of abstractions that were productsof the human imagination. computer scientists imagined òpacketsó ofinformation flowing through pipes, and they (symbolically) worked outthe consequences of that idea to determine the new laws those flows ofinformation must obey. this conceptualization led to the development ofprotocols that govern how data flows through the internet, what happenswhen packets get lost, and so on. the internetõs constructs extend wellbeyond mathematics or models to highlevel design principles such askeeping the network core simple and keeping detailed functionality at theedges. the power of a good abstraction is illustrated by the fact that theinternetõs basic protocols have reliably carried traffic on the network sinceits creation, even as this traffic has changed enormously not just in scalebut also in behavior, from email to streaming media and peertopeer filesharing networks.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.the essential character of computer science19in part two, shaw (in chapter 4) presents abstractions as a key ideain software design. the essay by aho and larus (in chapter 4) discusseshow the abstractions in computer languages bridge the gap betweencomputer hardware and humans. programming languages offer ways toencapsulate the details of algorithms and define procedural and dataabstractions that allow the algorithms to be used without knowledge oftheir details. peterson and clark (in chapter 7) discuss the importantabstractions of the internet, and gray (in chapter 5) covers abstractionsthat have proved especially useful for representing data.computer science research creates and studies algorithmsoften, computing research is less about how to represent static objects(such as an image or a driverõslicense record) and more about developingand studying algorithms (precise ways to do a particular task) that willperform operations on objects. with respect to images, such operationsmight include cropping an image, finding boundaries between regions ofdifferent color or texture, or telling whether an image has at least 20percent of its colors between yellow and orange. given a symbolic representation of a sunflower as a set of measurements and some assumptionsabout the strength of the stem and the force of a breeze, a virtual realityalgorithm could display that sunflower swaying in the breeze. programming languages offer ways to encapsulate the details of these algorithmsand define procedural abstractions that allow the algorithms to be usedwithout requiring knowledge of their details.computer science also concerns itself with the amount of time analgorithm takes (its running time), and some computer scientists try tofind algorithms that take less time than others to do similar things, sometimes dramatically reducing the time required. it would not do if, forexample, it took all day to tell if a typicalsized image were 20 percentyellow. sometimes, such research yields major breakthroughs that offerorderofmagnitude improvements in the time required to solve a particular type of problem.part two includes a discussion of programming languages in ahoand larus (in chapter 4) and a treatment of algorithms and their runningtime in kleinberg and papadimitriou (in chapter 2). indeed, essentiallyall the essays in this volume mention algorithms and/or programminglanguage representations for one or another kind of information.computer science research creates artificial constructs,notably unlimited by physical lawsanimated sunflowers blowing in the breeze are an example of howthe computational world can mimic, at an extremely accurate level, thecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.20computer science: reflectionsbehavior of the real world. while computer scientists often addresses theworld as it is, computer science also deals with the world as it could be, orwith an entirely invented world behaving under different assumptions.computer scientists can, for example, easily change the rules, makingwater more viscous or weakening the effect of gravity. for example, videogame characters often obey physical laws that have never been seen innature (òroadrunner physicsó). computational models also describe complex natural phenomena that are hard to observe in nature. such modelscan describe both natural and unnatural worlds. for example, it is entirelypossible to make ocean waves a bit more exciting to watch by alteringgravity or the viscosity of waterñchanges impossible in the laboratorybut routine within the world of computing.fedkiw (in chapter 3) describes how computergenerated animationproduces such images as the waves in the movie the perfect storm.peterson and clark (in chapter 7) describe the internet model. bruckman(in chapter 7) examines online virtual communities.computer science research exploits andaddresses exponential growththe speed of light doesnõt change at all, and dna strands have grownin length only over geologic time. but computer science must deal withmachines that, by various measures of size and speed (for fixed cost),double in performance roughly every 11/2 to 2 years (see the essay onmooreõs law by hill in chapter 2).2 although these performance improvements make it easier for computer scientists to solve some problems, theycan also magnify computer and system design challenges. over time, thecritical resource to be optimized changes; designers today must take intoaccount that obtaining two numbers from a computerõs main memory cantake a hundred times longer than multiplying them together whereas justthe opposite was once true. moreover, the improvements create commensurate demands that the machines be used to solve ever more complexproblems and larger or harder instances of the same problem and thateverlarger systems be built.2there are really two related trends that go by the name mooreõs law. the original formof mooreõs law states that the number of transistors per chip doubles every 18 months. thepopular form of mooreõs law states that as a result of increasing transistor density and othertechnological progress, such as the density of magnetic disk drives, the performance of acomparablecost computer doubles roughly every 11/2 to 2 years or, alternatively, that thecost for the same performance halves. this combination of extremely rapid exponentialgrowth and the ability to exploit the improvements in many different combinations of costand performance is unprecedented in the history of technology.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.the essential character of computer science21computer scientists have a rule of thumb that every time things get10 times larger or faster a qualitatively different class of research challenges emerges. to get a sense of this, suppose you are playing bridge,and you need to examine your hand to see whether you hold no cards ofthe suit being played, and therefore are allowed to trump the trick. sinceyour hand consists of at most 13 cards at any time, you can usually figureit out quickly by scanning each of your cards. but suppose bridge wereplayed with hands of a million cards, with 10,000 suits, each of 100 ranks.could you figure out which card to play before your opponents got up toget a snack? computers deal with similar situations all the time, and toavoid scanning all 1,000,000 cards, organizations for data have beendevised that allow us to design algorithms that home in on the desiredcard much more quickly than can the obvious òscan all cardsó algorithm.the expectation of rapid improvement in capabilities leads to a formof research, perhaps unique to computer science, sometimes called òlivingin the future.ó although almost every technology goes through someperiod of exponential growth, computers are remarkable because thedoubling time is unusually short and the exponential growth period hasbeen unusually long. as a result, it often makes sense to think about whatthe world will be like when computing machines are 10 times faster, orable to communicate with 100 times the bandwidth. certain things thatone cannot now do, or cannot do for a reasonable cost, are suddenly goingto become possible. many research successes that turned into successful products were the result of such confident, forward thinking.in addition to hillõs, many of the essays in this volume show howcomputer science research has both exploited the exponential improvement in our tools and dealt with the exponential growth in requirements.the essay by koller and biermann (in chapter 6) examines chess playing,where the goals of being able to defeat better and better players havelargely been met. peterson and clark (in chapter 7) show how we arecoping with the evergrowing internet.computer science research seeks the fundamental limits onwhat can be computedin addition to developing knowledge that supports practical engineering applications, computer science investigates fundamental limitsregarding computation. it has been known since the 1930s that there areundecidable problems, those that can be stated succinctly yet cannot beanswered. the canonical example of these is, can you write a computerprogram that given an arbitrary program p and arbitrary input n to thatprogram, can answer the question, does the program p ever stop runningwhen given the input n? the discovery that the answer to this question iscomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.22computer science: reflectionsno is fundamental in the sense that it applies to all possible programsrather than being a statement about a particular piece of programmingcode.perhaps more important is the discovery that not everything that isdecidable (a computer can solve it, somehow) is tractable (solvable insufficiently little time that we can expect to get answers to reasonablylarge instances of the problem in reasonable time). some problems, suchas searching for cards described above, are tractable. even the dumb algorithm of examining each one of n cards in oneõs hand would only taketime proportional to n. however, there are other problems that are nottractableñwhere time or computing resources proportional to the size ofthe instance to be solved are insufficient. instead, they require time orresources that are exponential in the size of the input nñthat is, time thatgrows like 2n. this sort of analysis extends more generally to how therequired computing resources relate to the nature and structure of a problem as well as its size. for some types of problems, the issue is howaccurately something can be computed; here computer science seeks anunderstanding of the fundamental limits in the accuracy of what can becomputed and the accuracy of what can be computed quickly (as inapproximation algorithms).an example of a tractability problem is the traveling salesman problem (tsp), where one is given n òcitiesó (nodes of a graph) with distancesbetween each pair. the goal is for the salesman (who, following our earlier example, is selling sunflower oil) to visit each city once and return tothe starting city, but minimize the total distance traveled. an obvioussolution, starting from one of the n cities, has n ð 1 choices for the first city,n ð 2 choices for the next, and so on, a total of (n ð 1)(n ð 2) ...(2)(1)choices. unfortunately, this number of choices grows faster than 2n. whatis worse, the theory of intractable problems provides powerful evidencethat there is no way to solve the tsp in substantially less time. as a result,even the exponential improvement in the speed of computers (òmooreõslaw,ó as mentioned above) can do almost nothing to increase the size ofproblem instance that can be dealt with; perhaps one can handle a fewmore cities each year, at best. of course, even when it is impractical tosolve a problem generally and optimally, the situation is not necessarilyhopeless. in practice, salesmen can make their rounds fairly efficientlybecause algorithms that do well enough most of the time have beendiscovered.the fundamental limit on our ability to compute applies not only tothe tsp, but also to thousands of problems encountered daily. that is, onecan solve these problems by computer only when the problem instance israther small, and the situation can never get much better. research oncomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.the essential character of computer science23fundamental limits thus informs practical application, by showing thatsome system designs are ultimately a dead end.in part two, kleinberg and papadimitriou (in chapter 2) provide adiscussion of the precise implications of the theory of intractable problems, and sudan (in chapter 7) discusses cryptography, a domain wherethe intractability of a problem is a help, not a hindrance, because the goalis assurance that an intruder with a fast computer cannot discover secretswithin any reasonable amount of time.computer science research often focuses on the complex, analytic,rational action that is associated with human intelligenceone aspiration of computer science has been to understand and emulate capabilities that are associated with intelligent human behavior. oneclass of these activities is devoted to enabling computers to solve problems that humans can solve easily, yet that appear very difficult formachines. a human can easily pick out the presence of a sunflower in animage, and yet a computer of the early 21st century can do so only withdifficulty and limited accuracy. the problems of image understanding,language understanding, locomotion, game playing, and problem solvingeach provide enormous challenges for the computer scientist. a secondclass of intelligencerelated research is to understand how the humanmind works by trying to solve problems the way humans do. it turns out,for example, that the simple òif a then bó logic harking back to aristotledoes not capture nearly enough of the way people think and make inferences about their environment. a third class of intelligencerelatedresearch is enabling computers to interact with the world by enablingthem to sense the environment, move within it, and manipulate objects.these efforts are elaborated in several of the essays. in chapter 6,koller and biermann discuss game playing by computer as a testbed forapproaches that emulate intelligence. lee tells about approaches to computer understanding of natural language. mitchell explores the development of machinelearning techniques.part two of this volume explores some of the depth and breadth ofcomputer science through essays written by computer scientists. theessays provide several researchersõ own views about their research areasand convey what excites these researchers about computer science.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.part twoselected perspectives oncomputer sciencethe character of computer science research can perhaps best beappreciated by seeing some of the things computer scientists doand why they choose to do them. in part two, computer scientistsexplain not only some of the results achieved in several areas of computerscience research but also what interests and excites them about the research.the diversity of the topics addressed in these essays reflects the diversityof the field itself.the essays in part two are organized by chapter into several clusters:¥exponential growth, computability, and complexity. how computerscience research makes possible a sustained growth in computing powerand how theoretical models of computation help us understand intrinsiclimits on what is computable (chapter 2).¥simulation. how computer models can be used to simulate aspectsof the physical world (chapter 3).¥ abstraction, representation, and notations. how abstraction is usedto express understanding of a problem, manage complexity, and selectthe appropriate level of detail and degree of generality (chapter 4).¥data, representation, and information. how computer science hasdeveloped new ways of storing, retrieving, and manipulating data, andhow these techniques can profoundly influence the models of a widerange of professionals (chapter 5).¥achieving intelligence. how computer scienceõs aspiration to emulate human intelligence has resulted in advances in machine learning,computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.26computer science: reflectionscomputersõ naturallanguage understanding, and improved strategies forgamelike situations (chapter 6).¥building computing systems of practical scale. how the design, development, and largescale deployment of working computing systemsñnotably, the internetñnot only are of great practical value but also constitute a diverse and fruitful area of research by which computer scientistsmay improve these systems and create new ones (chapter 7).¥research behind everyday computation. how computer scientistsõefforts in the service of human effectiveness have led to such advances asspreadsheets, textformatting programs, and information retrieval fromthe internet, and how these and other innovations have had a very broadimpact (chapter 8).¥personal passion about computer science research. how several computer scientists explain their commitment to computer science research(chapter 9).computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.272exponential growth, computability,and complexitythe essay by hill that follows describes how exponential growth incomputing capability drives technological and intellectual progress andhow computer science research works to sustain this remarkable growth.next, kleinberg and papadimitriou address the intellectual essence ofcomputation. they provide glimpses not only into the foundationalmodels that define computation but also into theoreticiansõ thinking aboutthese modelsñwhatõs convincing, whatõs surprising, whatõs not.finally, bennett describes how quantum computing research seeks tododge a fundamental physical limit of current computing technology andstretch our conception of computation.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.28computer science: reflectionsharnessing mooreõs lawmark d. hill, university of wisconsin, madisonfor most products, the claim òbetter and cheaperó casts suspicion onthe salesperson. for computerrelated products, however, òbetter andcheaperó has been wildly true for decades. in this essay, we seek to explainthis success so as to give readers a foundation with which to appreciatewhat the future might hold. specifically, we explain how rapid technological progress (e.g., the technologistõs mooreõs law) has been harnessedto enable better and cheaper computers (the popular mediaõs mooreõslaw). we then touch upon future prospects for technological progress,computer implementation challenges, and potential impacts.how does computer hardware work?before delving into our rapid progress in improving computers, it isimportant to reflect on how computers work. as discussed in chapter 1 ofthis volume, computers are not designed to perform a single task. insteadthey are machines that can perform many different computational tasks,including tasks that are not specified or even conceived until after thecomputer is deployed.we enable this flexibility by using software. software represents acomputational task to hardware as a collection of commands. each command is called an instruction and the set of instructions that the hardwaresupportsñits vocabularyñis its instruction set architecture. most instructions are simple. an òaddó instruction adds two numbers and indicateswhich instruction is next. a òbranchó instruction compares two numbersto choose which instruction is next. the instructions manipulate data,such as numbers and characters.moreover, instructions can manipulate other instructions, since mostmodern computers store instructions just like data. treating instructionsas data enables programs, such as compilers, that can translate highlevellanguage programs (e.g., written in java) into machine instructions (seeaho and larus in chapter 4).computer hardware has three basic components:¥a processor executes instructions. today, it is most common toimplement a processor on a single silicon chip called a microprocessor.most computers today employ a single microprocessor, but larger computers employ multiple microprocessors.¥memory stores instructions and data. current desktop computermemories are implemented in several chips and backed up by one orcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity29more magnetic disks. larger computers can employ hundreds of memorychips and disks.¥input/output devices connect a computer to both humans (e.g., keyboards, displays, mice, and speakers) (see foley in chapter 8) and to othercomputers via networks (see peterson and clark in chapter 7).hardware designers seek to make processors that execute instructions faster, memory that provides more information, and input/outputdevices that communicate more effectively. furthermore, we can and dofocus on making these primitive elements faster without worrying aboutwhat the software running on the hardware is actually doing.but how do we know this all works? why is relatively simple hardware sufficient for most computing? which instructions do i need in myinstruction set architecture? kleinberg and papadimitriou in this chapteraddress these questions using the formal foundations of the universalturing machine and the churchturing hypothesis. these foundationsallow hardware designers to concentrate on the practical questions ofengineering effective hardware.mooreõs law and exponential growththe key enabler of òbetter and cheaperó computers has been rapidtechnological progress. arguably, the most important enabler has beenthe progress in the number of transistors (switches) per semiconductorintegrated circuit (chip). in 1965, gordon moore used four data pointsto predict that the number of transistors per chip would double every2 years. he was not far off! the trend over the last 35 years has indeedbeen exponential, with the number of transistors per chip doublingapproximately every 18 months. technologists call this trend mooreõs law.some commentators have implied that exponential growth, such aswith mooreõs law, is unique to computer technology. this belief is incorrect. in fact, exponential growth is common and occurs whenever the rateof increase of a quantity is proportional to the size of the quantity.examples include compound interest and unconstrained biological population growth. for example, $100 earning compound interest at a rate of10 percent will more than double in 8 years: $214 = $100 (1 + 0.10)8.to understand the implications of rapid exponential growth, consider the absurd example of your annual salary starting at a modest $16but then doubling every 18 months for 36 years. this improvement corresponds to an annual growth rate of 59 percent. in the early years, yourfinances would be very tight (e.g., $64 after 3 years). you would have tolive with your parents. with patience, you will eventually be able to buya car ($16,000 after 15 years). then you can move out and buy a housecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.30computer science: reflections($100,000 after 24 years). eventually you will be very rich and have todream up fundamentally new ways to spend money ($300 million after36years).the potency of mooreõs law is evident when we observe that it hasfollowed the rate and duration of the absurd income example just given.sixteen transistors per chip is a mid1960s number, while 300 milliontransistors per chip is an early 21st century count. of course, mooreõs lawis not a law of nature. rather it is a business expectation in the semiconductor industry: to stay even with their competitors, technologistsshould solve the problems to double the number of transistors per chipevery 18 months.fortunately, the number of transistors per chip is not the only technological aspect that is achieving rapid, exponential growth rates. thesmaller transistors enabling mooreõs law also switch much faster. moreover, improvements rivaling (or exceeding) mooreõs law have occurredinmagnetic disk storage capacity and effective fiber optic networkbandwidth.unfortunately, some other technologies are improving much moreslowly, notably the roundtrip delays to memory chips, to disks, andacross networks. memory chip delays are improving slowly becausememory chip manufacturers optimize for larger memory capacity instead.disk delays seem limited by the inertia of mechanically moving macroscopic mass, while network delays are ultimately limited by the speed oflight.how we have harnessed exponential growthwhile it is obvious that rapid technological improvements providegreat opportunities for implementing computers, it is also the case thatrapid and differential rates pose great challenges. rapid change meansthat ideas that were too wasteful (e.g., in number of transistors) soonbecome appropriate and then become inadequate. the processors in 1960scomputers required hundreds or thousands of chips. it was not until 1970that is was even possible to implement a primitive processor on a singlechip (and that first socalled microprocessor could only access a memory of300 bytes, i.e., 0.0003 megabytes!).subsequently, mooreõs law enabled microprocessors to use more andfaster transistors. new microprocessor designers exploited this transistorbonanza to obtain computers that got faster more rapidly than the transistors got faster. central to this effort are methods of using many transistorsto cooperate sidebyside in what we call parallelism. not surprisingly,approaches for exploiting transistor parallelism depend on scale or òleveló(much as approaches for organizing humans to work in parallel dependcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity31greatly on whether 10, 1000, or 1 million people are available). with transistors in a microprocessor, we have organized our focus on bit andinstruction level parallelism. near the end of this essay, we discuss howthe currently modest use of thread level parallelism needs to be greatlyexpanded.during the 1970s and earlier 1980s, microprocessor developmentfocused on accelerating the execution of each instruction using bit levelparallelism. designers made changes to enable each instruction to both(a) manipulate a larger range of numbers (and symbols) and (b) performthose manipulations faster using more transistors side by side. earlymicroprocessors, for example, slowly performed manipulations on integerstaking on one of 65,536 (216) values (or fewer). in a single instruction, acurrent microprocessor can rapidly transform data whose values rangethrough billions (232) or quintrillions (264) of integer and fractional values.since the mid1980s, many microprocessor improvements havefocused on parallelism between instructions, not within instructions, withwhat is called instruction level parallelism. thus, instead of executinginstruction a, then instruction b, and then instruction c, we try to doinstructions a, b, and c at the same time. to exploit instruction levelparallelism past a few instructions, however, it is necessary to predict theoutcomes of program decisions (often embodied in branch instructions)and speculatively execute subsequent instructions. instruction level parallelism techniques have been so successful that modern microprocessorscan have several dozen instructions in execution at any one time! nevertheless, most microprocessors still appear to execute instructions one at atime. this illusion allows these microprocessors to run existing software,unmodified. it enables performance gains without endangering the billions of dollars invested to develop and deploy existing software.the fact that technologies progress at different rates also poseschallenges to computer implementations. twenty years ago, the time toexecute a multiply instruction and the time to access a computerõs mainmemory were comparable. differential rates of improvement now make amultiplication more than 100 times faster than accessing main memory.computer architects have responded with a plethora of types and layersof caches. caches are smaller, faster memories that transparently hold themostrecently accessed subset of the information from a larger, slowermemory. caches are faster than main memory, because they are smaller(fewer bits to reach and select among) and can be implemented in technologies more optimized for speed. caches often hold a very small fraction of the larger memory (e.g., a 64 kilobyte cache for a 64 megabytememory). nevertheless, they are able to satisfy a substantial fraction ofrequests quickly (e.g., 98 percent) due to locality (the property that, atmany sizes and time scales, recently accessed information is highly likelycomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.32computer science: reflectionsto be reaccessed quickly). in some ways, caches work for reasons similarto why your cellular phone can hold many of the numbers you actuallycall. in fact, caches are so effective that current microprocessor designersspend most of their transistors on caches! more generally, caching benefitsmany aspects of computer systems, as locality is common and smallermemories are faster than larger ones in many technologies.meeting the above challenges has enabled computers whose performance has been doubling every 2 years. ambiguously, this trend is alsocalled mooreõs law (e.g., by the popular press). at this point, we have twomooreõs laws:¥technologistõs mooreõs law: number of transistors per chip doublesevery 18 months, and¥popular mooreõs law: computer performance doubles every 2 years.this popular mooreõs law has provided incredible opportunities forthe rest of computer science. everyone from researchers to productdesigners can dream up many ideas and ask not whether something willbe practical, but when (assuming, of course, one is not trying to solveproblems that cannot be solved or require execution times that defeatexponential growth with exponential complexityñsee kleinberg andpapadimitriou). this performance has facilitated major achievements,such as beating a chess grandmaster (koller and biermann, in chapter 6),more realistic computer graphics (fedkiw, in chapter 3), better naturallanguage processing (lee, in chapter 6), and sequencing the humangenome. in these and other cases, however, tremendous algorithmic andsoftware advances were also necessary to effectively use faster hardware.many times, however, cost reduction matters more than increasingperformance. in these cases, rather than using more transistors to obtainmore performance at constant cost, it makes more sense to use a constantnumber of transistors to obtain the same performance at reduced cost.fortuitously, it turns out that every 2 years or so one can obtain the samelevel of performance for half the cost. in a decade, jim gray has observed, itwill be possible to buy an equivalent computer for the sales tax on onetoday (even if the sales tax is as low as 3 percent, which approximatelyequals 1/25). cost matters because more costeffective computation caneffectively be more widely applied. this cost reduction has enabled therapid spread of inexpensive computers and enabled the explosive growthof the internet (peterson and clark, in chapter 7). once again, creativeinnovations, such as word processors (ullman, in chapter 8), spreadsheets (foley, in chapter 8), and compelling multiuser environments(bruckman, in chapter 7) are necessary to make costeffective hardwareuseful.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity331see the international technology roadmap for semiconductors web site at http://public.itrs.net.future of mooreõs law and beyondrecently, there have been several accounts predicting the end of thetechnologistõs mooreõs law. since at least the early 1970s, there have beennumerous predictions of its demise. however, technologistsõ creativityhas repeatedly solved the challenges to keep it on track. we are bullishthat mooreõs law is safe for at least another decade, due to technologiesalready operating in the laboratory, backed by the motivation and visionof a trilliondollar industry.1nevertheless, it seems probable that the doubling time for conventional chips will increase and the doubling will eventually halt as atomiclimits are approached. there is already evidence that the doubling timefor memory chips is now closer to every 2 years instead of every 18months. a contributing factor to this slowdown is the exponentiallyincreasing cost of building factories for fabricating chips.taking a longer view, however, innovation beyond the technologistõsmooreõs law is likely. computing has already been implemented by aseries of technologies: mechanical switches, vacuum tubes, discrete transistors, and now chips driven by mooreõs law. eventually, almost all computing technologies will be supplanted by newer ones. one emergingcandidate uses synthetic organic molecules to perform switching and storage. another seeks to exploit quantum superposition to change both themodel and the implementation of some future computers (see bennett inthis chapter). both approaches, however, are unproven and may yet besurpassed by other technologies, some of which are not yet invented.moreover, we are not yet close to òfundamentaló limits. we are manyorders of magnitude from subatomic energy and storage limits imposedby currently understood physics. furthermore, there is an existence proofthat it is possible to organize computers in a way that is much better formany tasks: the human brain.as an aside, some argue that we do not need more computing performance. while it is indeed the case that other constraints, such as lowpower, low noise, and low environmental impact, are becoming moreimportant, we argue for more costeffective computer performance. first,all past predictions that there was enough computing performance havebeen wrong. two decades ago, some predicted that the ultimate personalcomputer would support three mõs: one million instructions per second,one megabyte, and one million display elements. todayõs personal computers routinely exceed the first two attributes by two orders of magnitude and still seem inadequate. second, there are clear opportunities ofcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.34computer science: reflectionsapplying more computing performance (and more algorithms) to makehumancomputer interactions closer to the naturallanguage exchangesenvisioned in 2001: a space odyssey a third of a century ago. third, ascomputing and communication get more costeffective, surprising newapplications are likely to be invented. some of us used mainframes fordecades without predicting the spreadsheet, while others used theinternet for years without predicting the world wide web. it will besurprising if an orderofmagnitude improvement in costeffective computer performance does not enable a new disruptive application.harnessing future exponential growthin the coming decades, we seek to harness future technology growth,be it slow, fast, or discontinuous. how will we use more transistors (ormore exotic technologies) to enable hardware that will support the creative applications being forecast in the rest of this volume?how do we exploit billions and trillions of transistors? (a computerwith one gigabyte of memory, for example, has more than 8 billion transistors.)one known way is to go beyond bit and instructionlevel parallelism toalso exploit threadlevel parallelism. when a processor executes a sequenceof instructions, we say it is executing a thread. threadlevel parallelism isexploited when software specifies multiple threads to execute and hardware is capable of executing the threads concurrently. today, severalimportant applications are specified with multiple threads, includingdatabase management systems (gray, in chapter 5) and web crawlers(norvig, in chapter 8); multiple threading is also used to beat humanchess champions (koller and biermann, in chapter 6). unfortunately, toomany current applications are programmed with a single thread, even ifthe problem could be solved in parallel. much established business software, for example, was written for a single thread, even when the problem was once solved by many clerks operating in parallel. nevertheless,creating multithreaded software has been difficult. in addition, the motivation to create it has been reduced by the fact that many current computersexecute only one thread at a time. emerging techniques for supportingmultiple threads on a microprocessor, however, promise to make multithreading hardware much more widely available. moreover, there is alsotremendous additional potential for using threads executing in parallelon multiple computers that communicate via local or widearea networks. in the extreme, computers around the world could be harnessed tosolve problems on demand. this vision is now known as grid computing,since it strives to deliver customers access to computing resources muchas the power grid delivers customers power. we encourage future effortsto exploit parallel threads in all their forms, because doing so representscomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity35the bestknown way to grow computing performance much faster thanmooreõs law enables for single processors.we also see additional opportunity provided by technology discontinuities and synergies. first, mooreõs law will soon enable a complete (albeitsimple) computer on a chip. todayõs personal computers, for example,use a complete processor on a chipñthe microprocessorñtogether withseveral memory and support chips. singlechip computers could dramatically cut costs and expand the effectiveness of systems. while we haveprimitive singlechip systems today, more powerful ones might unleashprogress in a manner similar to that unleashed by the increasing performance of microprocessors over the last three decades. for at least the nextdecade, however, singlechip solutions must focus on systems smallerthan personal computers (because personal computers use too muchmemory to fit on a chip). nevertheless, the history of computing hasshown that new smaller systems are great catalysts for change: mainframes to minicomputers to personal computers to personal digital assistants to cellular phones. second, technologists are now fabricating sensorsand emitters on chips. this technology holds the promise of systems thatintegrate with their environment at unprecedently low cost. a currentsuccess story is an accelerometer for triggering automobile airbag deployment. third, the further integration of computers with communicationwill make the world even smaller. communication, with and withoutwires, will enable ubiquitous connectivity. the world wide web showsus how communication magnifies the value of computation. now, imaginea web where you are always online everywhere!each of these trends will further integrate computers into our lives. inmany cases, integration allows the computers to òdisappear.ó when emilyclicked to buy a book in the prelude to this volume, she was not evenaware of most of the computers that implemented the transaction. furthermore, she may not recognize her cellular phone, personal digital assistant,or pacemaker as computers; an integration that is an appropriate andnatural consequence of our abilities to hide complexity from users.our success in hiding computers when they work, however, bringswith it a responsibility to hide them when they fail. imagine web servicesas available as telephones and personal computers as dependable as televisions. numerous solutions will be needed to enable this dependability,taking into account needs and appropriate costs. large commercial systems may seek 10 to 100 times improvements in availability for smalloverheads (e.g., 10 percent), while critical functions like pacemakers maytolerate tripling hardware costs. in many cases, the underlying hardwaremay get more unreliable, because transistors are so small (and susceptibleto cosmic rays) and numerous (more chances to fail). while some improvements can be done in hardware, transparently to software, othercomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.36computer science: reflectionssolutions will require hardware and software changes. in many cases, wewill have to design systems assuming that parts will fail. unlike thecurrent web, however, we should seek to ensure that all systems maskalmost all of those failures from users. by laying a more reliable foundation, we can expand the realms in which society can depend on information technology.the last halfcentury has seen substantial computing advances andimpacts on society. we expect the synergies just discussed to provideplenty of nontechnical and technical challenges and opportunities. forsociety, the real information revolution may be coming soon. on the technical side, there is much work to be done. arthur c. clarke said, òanysufficiently advanced technology is indistinguishable from magic.ó letõscreate some more magic!computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity37computability and complexityjon kleinberg, cornell university, andchristos papadimitriou, university of california, berkeleythe quest for the quintic formulaone of the great obsessions of renaissance sages was the solutionof polynomial equations: find an x that causes a certain polynomial toevaluate to 0. today we all learn in school how to solve quadraticequations (polynomial equations of degree two, such as ax2 + bx + c = 0),even though many of us have to look up the formula every time (itõs  xabbac1242/). versions of this formula were known to thebabylonians as early as 2000 bc, and they were rediscovered in manyancient cultures. the discovery of similar but much more complicatedformulas for solving equations of degree three and fourñthe cubic andquartic formulaeñhad to wait until the 16th century ad. during the nextthree centuries, the greatest minds in europe strove unsuccessfully todiscover the quintic formula, cracking equations of degree five, until theflowering of modern algebra brought the quest to a sudden, surprisingresolution: a proof that there is no quintic formula.this story, on first hearing, can engender a few natural reactions.among them, surpriseñwhatõs the obstacle to a quintic formula? whywas it so hard to prove it didnõt exist? and, more subtly, a mild sense ofperplexityñwhat do we mean by a quintic formula anyway? why canõtwe write òthe largest x such that ax5 + bx4 + cx3 + dx2 + ex + f = 0ó anddeclare this to be a formula?so we back up. by a òformulaó in this story, we meant a particularthing: a finite sequence of steps that begins with the given values of thecoefficients and ends with a root x; each step consists of one of the basicarithmetic operations applied to certain of the quantities already computed, or else it consists of the extraction of a root of one of the quantitiesalready computed. now we can assert more precisely, thanks to the workof the 19thcentury mathematicians abel and galois: there is no quinticformula.viewed from the safe distance of a few centuries, the story is clearlyone about computation, and it contains many of the key ingredients thatarise in later efforts to model computation: we take a computationalprocess that we understand intuitively (solving an equation, in this case),formulate a precise model, and from the model derive some highly unexpected consequences about the computational power of the process. it isprecisely this approach that we wish to apply to computation in general.but moving from this example to a fully general model of computationcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.38computer science: reflectionsrequires some further fundamental ideas, because the notion of a òformulaóña straightline recipe of arithmetic operationsñomits two of thecrucial properties of generalpurpose computation. first, computation canbe repetitive; we should be able to perform some action over and overuntil a certain stopping condition is satisfied. second, computation shouldcontain òadaptiveó steps of the following form: test whether a certaincondition is satisfied; if it is, then perform action a; if it isnõt, then performaction b. neither of these is present in straightline formulas; but a littlereflection convinces one that they are necessary to specify many of theother activities that we would consider computational.computation as a universal technologyso, guided by this intuition, let us move beyond stylized forms ofcomputation and seek to understand generalpurpose computation in allits richnessñfor if we could do this, then we might find similarly surprising consequences that apply much more broadly. such was the goal ofalan turing in the 1930s, and such was also the goal of a host of othermathematical logicians at that time.turingõs entry in this field is particularly compelling, not so muchbecause of its mathematical elegance but because of its basic, commonsensical motivation and power. he sought a streamlined mathematicaldescription of what goes on when a person is performing calculations in alarge notebook: he or she writes down and erases symbols, turns thepages left or right, keeps a limited number of symbols in his or hermemory. the computing device turing proposedñthe turing machineñhas access to an infinite sequence of òpages,ó each of which can hold onlyone symbol. at any time, the machine can be in one of a finite set ofpossible òstates of mindóñits working memory. the flow of control proceeds simply as follows: based on its current state, and the symbol it iscurrently reading, the machine may write a new symbol on the currentpage (erasing the existing one), move to the next or preceding page, andchange its state. subject to these rules, the turing machine processes theinput it is given and may eventually choose to halt, at which point thenotebook contains its output.why should we accept this model? first, it accords well with commonsense. it seems to be the way that symbolic computation, as performedslowly and painfully by humans, proceeds. indeed, with some practice,one can implement seemingly any natural symbolic task on a turingmachine. second, it is robustña version of the model with very small setsof available symbols and states (say, eight of each) is, in a precise sense,just as powerful as a version with an arbitrary finite set of each, only thecontrol rules become more complicated. moreover, it does not matter thatcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity39the òpagesó on which the computation is performed are arranged in alinear sequence; it would not add to the power of the model if we arrangedthem instead in an arbitrary web of connections. finally, and most crucially, turingõs model is precisely equivalent to the other formalismsproposed in his day, among them, and preceding it, alonzo churchõslambda calculusñand it is also equivalent to modern generalpurposeprogramming languages such as c and java (with access to an arbitraryamount of memory).for the accumulation of these reasons, we are justified in believingthat we have arrived at the òrightó model of computation; and this is thecontent of the churchturing thesis: a symbolic function is computable ifand only if it can be computed on a turing machine (or its equivalents). itis important to notice what is being claimed: we have not derived thenotion of òcomputabilityó from a set of more primitive axioms; rather,after extensive thought experiments, we have asserted that òcomputabilityócorresponds precisely to what can be computed on a turing machine.accepting the turing machine as the basis for our precise definitionof computability is a momentous step. its first consequence is that ofuniversality: there is a òuniversal turing machineó u that does the following. as input, u receives the description of a turing machine m (theòcodeó of m, written in uõs book) and an input n to m (a later chapter inthe same book). as output, u returns the result, if any, of running m on n.today, we would think of u simply as an interpreterñit executes a stepbystep simulation of any turing machine m presented to it. indeed, ourstyle of writing programs and then executing them is so ingrained in ourview of computation that it takes a moment to appreciate the consequences that flow from a universal machine. it means that programs anddata are really the same thing: a program is just a sequence of symbolsthat looks like any other piece of input; but when fed to a universalmachine, this input wakes up and begins to compute. think of mobilecode, java applets, email viruses: your computer downloads them asdata and then runs them as programs.the principle of interchangeable partsñthat components of a machinecan be massproduced independently, and a working whole can beassembled from a random selection of one part from each kindñwas thedisruptive insight that underpinned the industrial revolution. universality is perhaps an even more radical approach to assembling systems: asingle computer on your desk can run your word processor, your spreadsheet, and your online calendar, as well as new applications not yet conceived of or written. and while this may seem completely natural, mostof technology in fact does not work this way at all. in most aspects ofoneõs technological life, the device is the application; they are one and thesame. if you own a radio and want to watch tv, you must buy a newcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.40computer science: reflectionsdevice. if you want to drive, you use a car; but if you want to fly, you usean airplane. your car cannot download a new set of instructions andsuddenly be able to fly, or to maneuver underwater. but the computational world has a flexibility of application that cannot really be imaginedelsewhereñand that is because the world of computation is powered byuniversal machines.we have all seen the consequences of this flexibility very clearly in thepast decade, as the world wide web became a new medium within amere 7 years of its introduction. if we look at other mediañat the phone,the radio, the televisionñit took a much longer time from their inceptionsto widespread prominence. what was the difference? of course there aremany factors that one can point to, but mingled among them is the universality of the computers on which web protocols and interfaces run.people had already bought personal computers for their homes, and builtoffice information systems around them, before the web ever existed.when the web arrived, it could spread through this infrastructure withamazing speed. one cannot really imagine an analogue of this process forthe televisionñit would be as though millions of americans had beeninduced to buy large inert boxes for their living rooms, and a decade latersomeone dreamed up the technology to begin broadcasting pictures tothem. but this is more or less what happened with the web.the limits of computationcomputer science was born knowing its own limitations. for thestrength of the universal machine leads directly to a second, and morenegative, consequenceñuncomputability, the fact that certain naturalcomputational tasks cannot be carried out by turing machines or, byextension, computer programs. the leap from the notion of universalityto this impossibility result is surprisingly effortless, if ingenious. it isrooted in two basic issuesñfirst, the surprising difficulty in determiningthe òultimateó behavior of a program; and second, the selfreferentialcharacter of the universal machine u.to appreciate the first of these, recall that our universal machine usimply performed a stepbystep simulation of a turing machine m on aninput n. this means that if m computes forever, never halting, then uõssimulation of m will run forever as well. this is the notion of an òinfiniteloop,ó familiar to beginning (and experienced) programmers everywhereñyour program keeps running with no sign of any output. do youstop it and see whatõs wrong, or wait to see if itõs just taking longer thanexpected to come up with an answer? we might well want somethingstronger than the blind simulation that u provides; we might want aòuniversal termination detectoróña turing machine d that behaves ascomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity41follows. given a description of a turing machine m and an input n to m,the machine d performs a finite number of steps, and then correctlyreports whether or not m will ever halt with a result when it is run on n.(so in particular, the machine d itself halts on every input.)could one build such a thing? oneõs first reaction is to start dreamingup tricks by which one could look at a program and determine whether itwill halt or notñlooking for obviously repetitive behavior with no stopping condition. but gradually the problem begins to look hopelessly difficult.maybe the program youõre analyzing for termination is systematicallyenumerating the natural numbers, searching for a counterexample to afamous conjecture in mathematics; and it will only stop when it finds one.so a demonstration that this single program eventually terminates mustimplicitly resolve this mathematical conjecture! could detecting the termination of programs really be as hard as automating mathematics?this thought experiment raises the suggestion that we should perhaps be considering the problem from the other direction, trying to showthat it is not possible to build a universal termination detector. anotherline of reasoning that might make us start considering such an impossibility result is, as suggested above, the selfreferential nature of the universalmachine u: u is a turing machine that can simulate the behavior of anyturing machine. so, in particular, we could run u on a description ofitself; what would happen? when you find yourself asking questions likethis about a mathematical objectñquestions in which the object refers toitselfñthere are often explosive consequences lying just ahead. indeed,the dangerous properties of selfreference appear in ordinary discourse.from ancient greece we have eubulidesõ paradox, which asserts, òthisstatement is falseó: is this a true statement or a false one? or considerbertrand russellõs hypothetical barber, who only shaves the set of allpeople who do not shave themselvesñwho then shaves the barberhimself?in the case at hand, we can exploit the selfreference inherent in universality to prove that there is no universal termination detector byshowing that there is no turing machine that correctly implements auniversal termination detector (box 2.1).this is a first, fundamental impossibility result for computationñanatural problem that cannot be solved computationally. and starting withthis result, impossibility spreads like a shock wave through the space ofproblems. we might want a universal equivalence tester: given twoturing machines m and m, are they equivalent? do they produce thesame result on every input? but it is easy to convince yourself that if wecould build an equivalence tester, we could use it to build a terminationdetector, which we know cannot exist. and so: there is no universalequivalence tester. we might want a universal code verifier: given acomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.42computer science: reflectionsbox 2.1there is no universal termination detectorwe begin by observing that the set of all turing machines, while clearlyinfinite, can be enumerated in a list m1; m2; m3, . . . in such a way that each turingmachine appears once in the list.to do this, we can write a description of each turing machine as a sequenceof symbols and then order these descriptions in alphabetical order; we first list alldescriptions with one symbol (if there are any), then all descriptions with twosymbols, and so forth.our impossibility proof proceeds by assuming that there exists a universaltermination detector; we then show how this leads to a contradiction, establishingthat our initial assumption cannot be valid. so to begin, let d be a turing machinethat is a universal termination detector.we construct, from d, a turing machine x that will lead to the contradiction.on input n, here is what x does. it first invokes the termination detector d todetermine whether the machine mn will ever halt when run with input n. (this is thecore of the selfreference: we investigate the behavior of mn on its own position nin the alphabetical listing of turing machines.) if it turns out that mn will never halton n, then x halts. but if it turns out that mn will halt when run on n, then x gratuitously throws itself into an infinite loop, never halting.x is not a very useful program; but that is not the point. the point is to noticethat x is itself a turing machine, and hence is one of the machines from our list; letus suppose that x is really mk. the selfreference paradoxes we mentionedaboveñeubulidesõ and russellõsñwere both triggered by asking a natural question that exposed the latent contradiction. our proof here employs such a question,and it is this: does x halt when it is run on input k?we consider this question as follows. suppose that x halts when it is run onk. then, since we know x is really mk, it follows that mk halts on k; and so, by ourconstruction of x, x should not halt when it is run on k. on the other hand, supposethat x does not halt when it is run on k. then, again using the fact that x is reallymk, it follows that mk does not halt on k; and so x should halt on k. so neitheroutcome is possibleñx cannot halt on k, and it cannot fail to halt on k! this is acontradiction, so there cannot be such a machine x, and hence there cannot be auniversal termination detector d.this style of proof is often referred to as diagonalization, and it was introduced by cantor to show that one cannot put the real numbers in onetoonecorrespondence with the integers. the term òdiagonalizationó here comes from thefollowing intuition. we imagine drawing an infinite twodimensional table whoserows correspond to the turing machines m1; m2; m3; . . . and whose columnscorrespond to all the possible inputs 1; 2; 3; . . . . each entry of this tableñsay theentry at the meeting of row i and column jñindicates whether or not machine mihalts when it is run on input j. viewed in these terms, our supposed machine xòwalks down the diagonaló of this table; on input k, it consults the table entry at themeeting of row k and column k, and essentially inverts the answer that it findsthere.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity43turing machine m and an òundesirableó output n, is there any input thatwill cause m to produce the output n? but again, from a code verifier wecould easily build a termination detector. no termination detector, nocode verifier.suppose you want to verify that the program youõve just written willnever access a restricted area of memory; or suppose you want to ensurethat a certain revisionña transformation of the programñwill not causeit to change its behavior. research in the area of programming languageshas developed powerful techniques for program verification and transformation tasks, and they are used effectively to analyze complex programs in practice (aho and larus, in chapter 4, discuss the transformationproblem in the context of compiler optimization). such techniques, however, are developed in a constant struggle against the negative resultsdiscussed above: over the years researchers have carved out broader andbroader tractable special cases of the problem, but to solve these verification and transformation tasks in their full generalityñto perform themcorrectly on all possible programsñis provably impossible. such resultsimpose fundamental limitations on our ability to implement and reasonabout complex pieces of software; they are among the laws that constrainour world.when finite is not good enoughcomputers, as we think of them now, did not exist when turingcarried out his seminal work. but by the 1950s and 1960s, as truly automated computation became increasingly available, a growing amount ofattention was devoted to the study and development of algorithmsñstepbystep computational procedures, made precise by the notion ofcomputability. and as this development began to gather force, it becameclear that uncomputability was only one of the laws that constrained ourability to solve problems. the world abounded in problems whose solvability was not in doubtñbut for which solving any but the smallestinstances seemed practically infeasible.some of the most vivid of these problems came from the area ofoperations research, a field that sprang in large part from the epiphanyñconceived during world war ii, and spilling into civilian life ever afterñthat there was a science to the efficient coordinated movement of armiesand organizations, the efficient allocation of supplies and raw materials.thus, we can consider the traveling salesman problem: you are given amap of n cities and the distances between them, as well as a òbudgetó b;you must visit all the cities via a tour whose total length is at most b. orconsider the matching problem: you must pair up 2n newly admittedcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.44computer science: reflectionscollege studentsñsome of whom donõt like one anotherñinto n pairs ofroommates, so that each pair consists of students who will get along.in the 1960s, jack edmonds came up with a beautiful and efficientalgorithm to solve the matching problem; and he wrote a paper describing the method. but how should one describe the result, actually? òacomputational solution to the matching problemó?ñthis is not quite right,since thereõs an obvious way to solve it: try all possible pairings, and seeif any of them works. there was no question that the matching problemhad a computable solution in turingõs sense. the crux of the result was inthe efficiency of the solution. jack edmonds understood this difficultyvery clearly: òi am claiming, as a mathematical result, the existence of agood algorithm for finding a . . . matching in a graph. there is an obviousfinite algorithm, but that algorithm increases in difficulty exponentiallywith the size of the graph. it is by no means obvious whether or not thereexists an algorithm whose difficulty increases only algebraically with thesize of the graph.óit is hard to find much to add to this. there is clearly an algorithm thatsolves the matching problem in a number of steps that is exponential innñat least n factorial. but try to imagine how long this would take. evenon the fastest computers we have today the problem of forming 30 pairscould require an amount of time comparable to the age of the universe.yes, the solution is computable; yes, we can even imagine how the wholecomputation will proceed; but such a method is of no real value to us atall if we are seeking a solution to a problem of even moderate size. weneed to find a qualitatively faster method; and this is exactly what jackedmonds had accomplished.edmondsõs concern with ògood algorithmsó fit naturally into aresearch agenda that was being pursued contemporaneously by jurishartmanis and richard stearnsñthat of determining the intrinsic computational complexity of natural problems, by determining the smallestnumber of computational steps that are required to produce a solution.and so, following this line of attack, we seek algorithms that require onlyòpolynomial timeóñon an input of size n, a ògoodó algorithm should usea number of steps that is bounded by a polynomial function of n such asn2 or n5. clearly polynomial functions grow much more slowly thanexponential ones, and they have a very desirable scaling propertyñif youincrease your input size by a constant multiplicative factor, the runningtime goes up by a predictable constant factor as well. but however muchone tries to justify our interest in polynomial time on theoretical grounds,its primary justification follows the same lines as the churchturing thesisthat we saw earlier: it accords well with our experience from practice.problems solvable by polynomialtime algorithms tend overwhelminglyto be efficiently solvable in practice; and for problems that lack polynomialcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity45time algorithms, one tends to encounter enormously difficult instanceswith some regularity.the choice of polynomial time as a mathematical surrogate for efficiency has served computer science very wellñit has been a powerfulguide to the design of good algorithms. and algorithmic efficiency hasproved to be a far subtler concept than we could have imagined. consideragain the traveling salesman problem and the matching problem. foreach, the òsearch spaceó is enormous: for the traveling salesman, anyordering of the n cities forms a tour that must in principle be considered;for the matching problem, any set of pairs that matches all of them is acandidate solution. and yet, despite similarities at this level, theirbehavior from the point of view of computational difficulty seems to beutterly divergent. matching has a polynomialtime algorithm, and verylarge problems are solved every day in practice. for the traveling salesman problem, on the other hand, we are famously without a polynomialtime algorithm, and the solution of relatively small instances can stillrequire a major computational effort.where does this enormous difference in computational complexitylie? what features of a computational problem determine its underlyingdifficulty? the ongoing effort to resolve these questions is a core researchactivity in computer science today; it has led to a rich theory of computational intractabilityñincluding the notion of npcompleteness, which hasspread from computer science into the physical, natural, and social sciences. it has also led to the celebrated òp versus npó question (box 2.2),which has drawn the attention of mathematicians as well as computerscientists and is now featured on several lists of the foremost open questions in mathematics.exponential growth is a recurring theme in computer science, and it isrevealing to juxtapose two of its fundamental roles: in mooreõs law, whichcharts the exponential growth in the power of computing machinery (seehillõs essay in this chapter), and in computational complexity, whichasserts that the effort needed to solve certain problems grows exponentially in their size. do these two principles cancel out? if our computingpower is growing exponentially over time, will we really be bothered byproblems of exponential complexity? the answer is that mooreõs lawdoes not make our concerns about exponential complexity go away, andit is important to realize why. the full search space for a 17city instanceof the traveling salesman problem is 16 times larger than the searchspace for a 16city instance. so if exhaustively searching the space ofsolutions for a 16city problem is at the limit of your current computerõsabilities, and if computing power doubles every year and a half, thenyouõll need to wait 6 years before your new computer can tackle a 17cityproblem by brute forceñ6 years to be able to solve a problem that is onlycomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.46computer science: reflectionsbox 2.2does p equal np?we have been concerned with the set of all problems that can be solved by apolynomialtime algorithm; letõs use p to denote this set of problems.now, we believe that the traveling salesman problem is very difficult to solvecomputationally; it is likely that this problem does not belong to the set p we havejust defined. but there is at least one good thing we can say about its tractability.suppose we are given n cities, the distances between them, and a budget b; andsuppose that in fact there exists a tour through all these cities of length at most b.then there exists a way for someone to prove this to us fairly easily: he or shecould simply show us the order in which we should visit the cities; we would thentabulate the total distance of this tour and verify that it is at most b.so the traveling salesman problem may not be efficiently solvable, but it is atleast efficiently verifiable: if there is a short tour among the cities we are given, thenthere exists a òcertificateóñthe tour itselfñthat enables us to verify this fact inpolynomial time. this is the crux of the traveling salesman problemõs complexity,coiled like the òtrickó that helps you solve a difficult puzzle: itõs hard to find a shorttour on your own, but itõs easy to be convinced when the short tour is actuallyrevealed to you. this notion of a certificateñthe extra piece of information thatenables you to verify the answerñcan be formalized for computational problemsin a general way. as a result, we can consider the set of all problems that areefficiently verifiable in this sense. this is the set npñthe set of all problems forwhich solutions can be checked (though not necessarily solved) in polynomial time.it is easy to show that any problem in p must also belong to np; essentially,this is as easy as arguing that if we can solve a problem on our own in polynomialtime, then we can verify any solution in polynomial time as wellñeven without thehelp of an additional certificate. but what about the other side of the question: isthere a problem that belongs to np but not to p, a problem for which verifying iseasy but solving is hard? although there is widespread belief that such problemsmust exist, the issue remains unresolved; this is the famous òp versus npó question.to address this question, it is natural to seek out the òhardestó problems innp, for they are the best candidates for problems that belong to np but not to p.one city larger! waiting for mooreõs law to deliver better computingpower only gets you so far, and it does not beat down the exponentialcomplexity of a deeply intractable problem. what is needed is not justbetter hardware on which to apply brute force, but also a better algorithmfor finding a solutionñsomething like what edmonds found for thematching problem.the fact that simplystated problems can have enormous complexity,with solutions that are computationally very difficult to find, has led tonew perspectives on a number of wellstudied ideas. cryptography hascomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity47how can we formalize the notion that one problem is at least as hard as another?the answer lies in reducibility, an idea that came up implicitly when we discussedcomputational impossibility. we say that a problem a is òreducibleó to a problem bif, given a òblack boxó capable of solving instances of b in polynomial time, we candesign a polynomialtime algorithm for a. in other words, we are able to solve a bydrawing on a solution to b as a òresource.ó it follows that if we actually had apolynomialtime algorithm for problem b, we could use this as our òblack box,ó andhence design a polynomialtime algorithm for problem a. or, simply running thisreasoning backward, if there is no polynomialtime algorithm for a, then there cannotbe a polynomialtime algorithm for b: problem b is at least as hard as problem a.so here is a natural thing we might search for: a single problem b in np withthe property that every problem in np is reducible to b. such a problem would,quite conclusively, be among the hardest problems in npña solution to it wouldimply a solution to everything in np. but do such problems exist? why shouldthere be a single problem that is this powerful?in the early 1970s, steve cook in north america and leonid levin in thesoviet union independently made the crucial breakthrough, discovering a numberof natural problems in np with precisely this property: everything in np can bereduced to them. today we say that such a problem is ònpcomplete,ó andresearch over the past decades has led to the discovery that there are literallythousands of natural npcomplete problems, across all the sciences. for example,determining the winner(s) under a wide variety of election and auction schemes isnpcomplete. optimizing the layout of the gates in a computer chip is npcomplete. finding the folding of minimum energy for discrete models of proteins isnpcomplete. the traveling salesman problemñthe tough nut that started us onthis roadñis npcomplete. and an important thing to bear in mind is this: becauseevery problem in np is reducible to any of these, they are all reducible to eachother. there is a polynomialtime algorithm for one if and only if there is a polynomialtime algorithm for all. so we have come to realize that researchers in a host ofdifferent areas, struggling over a spectrum of computationally intractable problems, have in a fairly precise sense all been struggling over the same problem; thisis the great insight that npcompleteness has given us. the original questionremains open.been revolutionized by the theory of complexity, for the design of securecommunication protocols is a field that exists in a mirrorworld wheredifficult computational problemsñcodes that are easy to apply and hardto breakñare resources to be cultivated (see sudan in chapter 7). thersa publickey cryptosystem was inspired by the presumed difficulty offactoring large integers, with the prime factors of a number n forming thehidden òkeyó whose knowledge allows for easy decryption. the ageoldnotion of randomnessña concept that is intuitively apparent but notoriously tricky to defineñhas been given an appealing formalization basedcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.48computer science: reflectionson computational complexity: a sequence of digits appears òrandomó toan observer if it is computationally difficult for the observer to predict thenext digit with odds significantly better than guessing.for designers of algorithms, we have seen that their struggle with thecomplexity of computation has proceeded at a number of different levels.one boundary divides the computable from the uncomputableñit is feasible to build a stepbystep interpreter for computer programs, but onecannot design an algorithm that decides whether arbitrary programs willterminate. another boundary divides polynomialtime solvability fromthe exponential growth of bruteforce search. but while polynomial timeis indeed a good highlevel means for gauging computational tractability,there are an increasing number of applications, typically involving verylarge datasets, where simply having a polynomialtime algorithm is farfrom adequate. suppose the size of the input is measured in terabytes(millions of megabytes); an algorithm that takes a number of steps equalto the cube of the input size is no more useful in practice than one thatnever terminates.none of this is to say that polynomial time has lost its relevance to thedesign of algorithms. but for many largescale problems, we are facedwith a reprise of the situation in the 1950s and 1960s, when we tumbledfrom a concern with computability to a concern with computational complexity: as our realworld computational needs expand, our guidelines forwhat constitutes an òacceptableó algorithm become increasingly stringent.and this in turn has led to new lines of research, focusing for example onalgorithms that must run in time very close to the size of the input itself;algorithms that must òstreamó through the input data in one pass, unableto store significant parts of it for postprocessing.while algorithm design has deepened into the study of increasinglytimeefficient techniques, it has also opened outward, revealing that running time is just one of many sources of complexity that must be faced. inmany applicationsñscheduling under realtime conditions, or managinga busy networkñthe input is not a static object but a dynamic one, withnew data arriving continuously over time. decisions must be made andsolutions must be constructed adaptively, without knowledge of futureinput. in other applications, the computation is distributed over manyprocessors, and one seeks algorithms that require as little communication,and synchronization, as possible. and through all these settings, fromnpcomplete problems onward, algorithms have been increasinglydesigned and analyzed with the understanding that the optimal solutionmay be unattainable, and that the optimum may have to serve only as animplicit benchmark against which the quality of the algorithmõs solutioncan be measured.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity49the lens of computationour contemplation of computation has led us quickly to the òp versusnpó question, now considered among the deepest open problems in mathematics and computer science. more recently, our views on complexityhave been influenced by the striking confluence of computation and quantum physics: what happens to our standard notions of running time andcomplexity when the computation unfolds according to the principles ofquantum mechanics? it is now known that access to such a hypotheticalòquantum computeró would yield polynomialtime algorithms for certain problems (including integer factorization and the breaking of thersa cryptosystem) that are believed to be intractable on standard computers. what are the ultimate limits of quantum computers? and arethere theoretical obstacles to building them (in addition to the practicalones currently braved in labs all over the world)? these questions, purelycomputational in their origin, present some of the most daunting challenges facing theoretical physics today.biology is a field where the synergy with computation seems to goever deeper the more we look. let us leave aside all the ways in whichsophisticated algorithmic ideas are transforming the practice of biologywith vast genomic and structural databases, massive numerical simulationsof molecules, and the fearsome symbolcrunching of the human genomeproject. instead, consider how we might view molecular biology itselfthrough a computational lens, with the cell as an informationprocessingengine. for fundamentally, a biological system like a cell is simply achemical system in which the information content is explicit. the genomeis part of the overall chemical system, but it is not there to take part in thechemistry itself; rather, it is there as a sophisticated encoding of the chemical processes that will take place. it is a programming abstractionñit isthe representation of the real thing, coexisting with the thing itself.just as computation distinguishes itself from the rest of technology,so are biological systems intrinsically different from all other physical andchemical systemsñfor they too have separated the application from thedevice. we can take a cell and change a few symbols in its genomeñsplicein a new gene or twoñand we can cause its chemistry to change completely. we have replaced one piece of code with another; the deviceñthecellular hardwareñhas been left alone, while we simply changed theapplication running on it. this phenomenon really seems to have no parallel in the other sciences. surely, the nonbiological world obeys fundamental laws, but it does not containñand actually implementñan explicitrepresentation of these laws. where in the solar system are the few molecules, encoding the laws of gravity and motion, which we could modifyto cause the planets to follow more eccentric orbits?computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.50computer science: reflectionscomputation as a technology that follows its own laws; computationas the quintessence of universality; computation as a powerful perspective on the world and on scienceñthese are issues that still drive ourstudy of the phenomenon today. and the more we grapple with theunderlying principles of computation, the more we see their reflectionsand imprints on all disciplinesñin the way structured tasks can be cast asstylized computational activities; in the surprising complexity of simplesystems; and in the rich and organic interplay between informationand code.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity51quantum information processingcharles h. bennett, ibm researchcomputer science is based on a very fruitful abstraction whose rootsare as old as mathematics but whose power was fully appreciated only inthe 20th century: the notion that information and computation are aworthwhile object of study in their own right, independent of the physicalapparatus used to carry the information or perform the computation.while at the most obvious level, mooreõs law (see hill in this chapter) is ahardware success story, this success has only been possible because information is such an abstract stuff: make a bit a thousand times smaller andit remains useful for the same purposes as before, unlike a thousandfoldsmaller car or potato.although mooreõs law has survived many early predictions of itsdemise, no exponential growth can go on forever. the present few decadesare clearly an exceptional time in the history of computing. sooner or latersomething will have to give, since at the present rate of shrinkage, information technology will reach atomic dimensions within 20 years. accordingly, considerable thought and longrange planning are already beingdevoted to the challenges of designing and fabricating devices at theatomic scale and getting them to work reliably, a field broadly known asnanotechnology. however, it has long been known that atoms and othertiny objects obey laws of quantum physics that in many respects defycommon sense. for example, observing an atom disturbs its motion, whilenot observing it allows it to spread out and behave as if it were in severaldifferent places at the same time. until recently, computer designersconsidered such quantum effects mostly as a nuisance that would causesmall devices to be less reliable and more errorprone than their largercousins.what is new, and what makes quantum informatics a coherent discipline, rather than a vexing set of technological problems, is the realizationthat quantum effects are not just a nuisance; rather, they offer a new andmore comprehensive way of thinking about information, which can beexploited to perform important and otherwise impossible informationprocessing tasks. already they have been used to create unconditionallysecure cryptographic key agreement protocols, and in the future, if aquantum computer can be built, it could easily perform some computations (most notably the factorization of large numbers) that would takelonger than the age of the universe by the best known algorithms, notonly on todayõs supercomputers, but also on the supercomputers of 2050(by which time we predict mooreõs law will have ended).computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.52computer science: reflectionsthe way in which quantum effects can speed up computation is not asimple quantitative improvement, such as would result from using a fasterprocessor, or some fixed number of parallel processors to speed up computations by some constant factor depending on the hardware. rather itis a qualitative improvement in the functional form of the computationcost, similar to what one typically gets by discovering a smarter algorithmto solve a problem that previously seemed hard. with quantum information processing the physical form of information, for the first time, has aqualitative bearing on the efficiency with which it can be processed, andthe things that can be done with it. quantum computers do not speed upall computations equally: a few, like factoring, are sped up superpolynomially; general np search problems like the traveling salesmanproblem are sped up quadratically, while other problems are not sped upat all.but to say quantum computers offer the hope of using physical effectsto dramatically speed up some computations is putting things backwards.in hindsight, it should rather be said that the laws of quantum physics,which as far as we know today apply to everything in the universe,provide a more powerful arsenal of physically performable mathematicaloperations than turing imagined when he formalized the notion of acomputer. availing ourselves of this more powerful arsenal, although itdoes not enlarge the class of computable functions, makes some computational problems qualitatively easier than they seemed before. unlike theformer hope of a qualitative speedup from analog processing, which haslargely been dashed by the ability of digital computers, via discreteapproximation to simulate analog processes more accurately than theycan be reproducibly performed in nature, quantum computation offers arealistic hope of qualitatively enlarging the scope of feasible computation.however, it should be noted that actually building a useful quantumcomputer presents formidable technical challenges, which will probablybe overcome eventually, but not any time soon. the confidence that theseobstacles can ultimately be overcome rests largely on the theory ofquantum error correction and fault tolerance, developed since 1995. thistheory, which is analogous to the classical theory of fault tolerance developed by von neumann and others in the days when computers weremade of vacuum tubes and relays, allows arbitrarily large reliable quantum computations to be done on imperfect hardware, provided the hardware exceeds some threshold of reliability. the technical problem is thatthe quantum threshold is higher than the classical threshold discoveredby von neumann, while todayõs quantum computing hardware processesquantum information far less reliably than vacuum tubes process classicalinformation, so there remains a gap of several orders of magnitude thatcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity53still needs to be closed. there is every reason to believe it can be closed,but how and when remain to be seen.just how do quantum information and the hardware used to processit differ from the ordinary classical type of information processing formalized by turing? states of a turing machine tape, or any other digitalstorage medium, are in principle reliably distinguishable, and the tapecan be copied accurately without disturbing it. this is a reasonable idealization of the behavior of macroscopic information processing hardwarelike punch cards, electromechanical relays, and even todayõs mostadvanced microprocessors and memory chips. but it has been knownsince the early 20th century that at an atomic and subatomic scale actualmatter behaves more subtly: not all states are reliably distinguishableeven in principle, and information stored in such states cannot be copiedwithout disturbing it. speaking metaphorically, quantum information islike the information in a dream: attempting to describe your dream tosomeone else changes your memory of it, so you begin to forget the dreamand remember only what you said about it.this dreamlike behavior notwithstanding, quantum information obeysexact and wellunderstood laws. the socalled superposition principle holdsthat the possible states of any physical system correspond to directions ina ddimensional space (òhilbert spaceó), where the dimension d is characteristic of the system and represents the systemõs maximum number ofreliably distinguishable states. two states are reliably distinguishable ifand only if their hilbert space directions are orthogonal (as is usually thecase with macroscopic systems). physically implementable operations onthe system always conserve or reduce distinguishability, correspondingroughly to rigid rotations and projections in the hilbert space. in thesimplest nontrivial case d = 2 and the system (e.g., the internal state of anelectron or photon, for example) is called a qubit. if two reliably distinguishable states of the qubit (e.g., horizontal and vertical polarizations fora photon) are arbitrarily designated |0> and |1>, a general state may beexpressed as a linear combination |0> +|1>, where and  are complex numbers such that ||2 + ||2 = 1. another quantum principle, theprojection postulate, holds that if a qubit in this state is subjected to anobservation that would reliably distinguish |0> from |1>, the qubitbehaves like |0> with probability ||2 and like |1> with probability||2. observing the system again yields no new information: the systemagain behaves like |0> or |1> according to the result of the first observation. more generally, observing a quantum system causes it to behaveprobabilistically, losing information about its previous state, except whenthe system was already in one of the states the observation was designedto distinguish. the art of quantum computing consists of accurately precomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.54computer science: reflectionsparing the quantum computer in a desired initial state, performing asequence of accurately controlled manipulations (rotations) of its statewithout observing it during the intermediate stages of the computation,and then finally observing the final state to obtain a useful output.this may sound a lot like analog computation, which is not believedto be significantly more powerful than conventional digital computation.how, one might ask, can a qubitñsay a photon, which may be polarizedat an arbitrary angle relative to the horizontalñbe more powerful thanan analog systemñsay a mechanical wheel oriented at angle relative tosome standard position? at first sight, the wheel would appear morepowerful. not only can its orientation be accurately manipulated, but alsothe orientation (unlike a photonõs) can be observed quite precisely without significantly disturbing it. the essential difference, which makes quantum computation more powerful than analog computation, comes fromthe way individual informationbearing subsystems (e.g., qubits) combine to form a larger system (e.g., a quantum register, or a whole quantumcomputer). an n qubit register has 2n reliably distinguishable states corresponding to the nbit strings |000é> through |111é1>; more generallythe hilbert space of a compound quantum system has a dimensionalityequal to the product of the hilbert space dimensions of its parts. by thesuperposition principle, the general state of an n qubit register corresponds to a direction in a 2n dimensional space; and during the computation the state may undergo controlled rotations in this large space. bycontrast, an n wheel analog computerõs state lives in a parameter space ofonly n dimensions; more generally, an analog systemõs parameter spacehas a dimensionality equal to the sum of the dimensionalities of its parts.the quantum computerõs advantage comes from its enormously largerstate space. however, the advantage is rather subtle, because at the end ofa quantum computation, in order to get a classical output, it is necessaryto make an observation, and, by the projection postulate, doing so collapses the 2n parameter quantum state back down to n classical bits. thissevere bottleneck at the end of the computation means that an n qubitquantum computer cannot process any greater volume of informationthan an n bit classical computer. however, for some computations, it cando the processing in far fewer steps because of the extra maneuveringroom the large hilbert space provides during intermediate stages of thecomputation.as noted earlier, the big technical barrier to be overcome in constructing a quantum computer is to make the rate of hardware errors, duringthe unobserved portion of the computation preceding the final observation, sufficiently small. this is qualitatively similar to the problem digitalcomputers faced in the era of vacuum tubes and relays, but quantitativelyworse, because a quantum computer needs to be isolated much morecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.exponential growth, computability, and complexity55carefully from its environment to attain a given level of reliability. inparticular, because of the disturbing effect of observation on quantumsystems, quantum computers must be designed to prevent informationabout the data being processed from leaking out of the computer into theenvironment before the end of the computation. (if such premature leakage took place, the computation would begin to behave like a classicalprobabilistic computation instead of a quantum one, and the advantagesof quantum speedup would be lost.) fortunately the formalism of quantum error correction and fault tolerance allows arbitrarily good protectionagainst such leakage to be achieved, provided the basic hardware exceedssome finite threshold of reliability.aside from computation per se, quantum information science includesthe disciplines of quantum communication and quantum cryptography.the latter field is at a far more mature stage of development than isquantum computation per se, with successful laboratory experiments andeven a few startup companies. quantum cryptography is based on thefact that eavesdropping on quantum systems disturbs them. in a typicalimplementation, a random sequence of n faint polarized light pulses issent through an optical fiber, after which the sender and receiver, bypublic discussion of the sent and received signals, estimate the amount ofdisturbance and hence of potential eavesdropping. if it is too great, theyabort the protocol. otherwise they can proceed to derive from the sentand received signals a shorter sequence of random secret key bits onwhich the eavesdropper has arbitrarily little information. more preciselyit can be shown that any eavesdropping strategy obeying the laws ofquantum physics, even when assisted by unlimited computing power,yields only exponentially little expected information on the final keysequence, if any. the eavesdropper thus faces the dilemma of eavesdropping gently and learning essentially nothing, or eavesdropping stronglyand causing the protocol to abort. quantum cryptography is practicalbecause it does not require a full fledged quantum computer, only classical computers supplemented by equipment for generating, transporting,and detecting quantum signals, all of which are available today. unlikeclassical methods of key agreement, it is unconditionally secure, and thusimpervious to future improvements in algorithms or computing power.although it was inspired by physics, quantum information processing is a mathematically coherent and wellcharacterized extension of classical information processing. indeed the latter can now best be viewed asa useful but limited subset of the larger subject of quantum informationprocessing, somewhat as the real numbers are a useful but limited subsetof the complex numbers. to continue the analogy, quantum informationprocessing provides solutions, or improved solutions, to some problemsin classical computation and cryptography, just as the complex planecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.56computer science: reflectionsprovides solutions and insights into problems in real analysis not explicitly involving complex variables. for this reason, even aside from its technological implications, quantum informatics is an intellectually excitingdiscipline, with farreaching implications for the basic mathematical andphysical sciences, both theoretical and experimental. it is already providing new ways of thinking about a wide variety of scientific and technicalquestions, and has begun to affect how science is taught, in a way that willbring a deeper understanding of the fruitful abstraction that is information not only to computer scientists but also to a broad segment of the laypublic.for further information:¥jozef gruska, quantum computing (mcgrawhill, 1999, isbn 007 709503 0)¥michael nielsen and isaac l. chuang, quantum computation and quantuminformation (cambridge university press, 2000, isbn 0 0521 63235 8)¥u.s. national science foundation report on quantum information science:http://www.nsf.gov/pubs/2000/nsf00101/nsf00101.htm¥online course notes:http://www.theory.caltech.edu/%7epreskill/ph219/http://www.cs.berkeley.edu/~vazirani/qc.html¥other quantum information web sites:http://www.research.ibm.com/quantuminfo/http://www.iro.umontreal.ca/labs/theorique/index.html.enhttp://www.qubit.org/computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.573simulationin chapter 2, the essay by kleinberg and papadimitriou discusses theuniversality of computers. to exploit this universality, we must specialize the computing engine to a specific task. this specializationultimately takes the form of a program, but the program does not appearby magicñits creator needs to understand the task deeply. further, aprogram must handle all variants of the task, not just those that the programmer can imagine. usually we address this challenge by creating amodel of the taskõs problem domain. the model may be expressed in manydifferent ways, but it can be understood in isolation from any particularprogram that implements it. these models may be designed to matchphysical reality, or they may express alternative sets of laws. in this way,simulation allows computer scientistsñand othersñto explain phenomenathat may be difficult, dangerous, or impossible to explore in reality.the use of digital computer simulations for scientific research is half acentury old. strogatz reflects on the explanatory power of fermiõs earlysimulations and their profound effects. these early models of nonlinearsystems gave better insight into the underlying phenomena than eitherthe physical experiments (which proceeded too fast) or the math (whichwas too hard for the times).fedkiw describes the modern analog of fermiõs experimentsñthe useof simulation to visualize complex phenomena such as turbulent flow. heshows how this mode of research complements the use of experimentsand closedform mathematics.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.58computer science: reflectionsthe real scientific hero of 1953steven strogatz, cornell universitynote: originally published in the new york times,this oped appeared on march 4, 2003.reprinted by permission of the author.last week newspapers and magazines devoted tens of thousands ofwords to the 50th anniversary of the discovery of the chemical structureof dna. while james d. watson and francis crick certainly deserved agood party, there was no mention of another scientific feat that also turned50 this yearñone whose ramifications may ultimately turn out to be asprofound as those of the double helix.in 1953, enrico fermi and two of his colleagues at los alamos scientific laboratory, john pasta and stanislaw ulam, invented the concept ofa òcomputer experiment.ó suddenly the computer became a telescope forthe mind, a way of exploring inaccessible processes like the collision ofblack holes or the frenzied dance of subatomic particlesñphenomenathat are too large or too fast to be visualized by traditional experiments,and too complex to be handled by pencilandpaper mathematics. thecomputer experiment offered a third way of doing science. over the past50 years, it has helped scientists to see the invisible and imagine theinconceivable.fermi and his colleagues introduced this revolutionary approach tobetter understand entropy, the tendency of all systems to decay to statesof ever greater disorder. to observe the predicted descent into chaos inunprecedented detail, fermi and his team created a virtual world, a simulation taking place inside the circuits of an electronic behemoth known asmaniac, the most powerful supercomputer of its era. their test probleminvolved a deliberately simplified model of a vibrating atomic lattice,consisting of 64 identical particles (representing atoms) linked end to endby springs (representing the chemical bonds between them).this structure was akin to a guitar string, but with an unfamiliarfeature: normally, a guitar string behaves òlinearlyóñpull it to the sideand it pulls back, pull it twice as far and it pulls back twice as hard. forceand response are proportional. in the 300 years since isaac newtoninvented calculus, mathematicians and physicists had mastered the analysis of systems like that, where causes are strictly proportional to effects,and the whole is exactly equal to the sum of the parts.but thatõs not how the bonds between real atoms behave. twice thestretch does not produce exactly twice the force. fermi suspected that thiscomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.simulation59nonlinear character of chemical bonds might be the key to the inevitableincrease of entropy. unfortunately, it also made the mathematics impenetrable. a nonlinear system like this couldnõt be analyzed by breaking itinto pieces. indeed, thatõs the hallmark of a nonlinear system: the partsdonõt add up to the whole. understanding a system like this defied allknown methods. it was a mathematical monster.undaunted, fermi and his collaborators plucked their virtual stringand let maniac grind away, calculating hundreds of simultaneous interactions, updating all the forces and positions, marching the virtual stringforward in time in a series of slowmotion snapshots. they expected tosee its shape degenerate into a random vibration, the musical counterpartof which would be a meaningless hiss, like static on the radio.what the computer revealed was astonishing. instead of a hiss, thestring played an eerie tune, almost like music from an alien civilization.starting from a pure tone, it progressively added a series of overtones,replacing one with another, gradually changing the timbre. then itsuddenly reversed direction, deleting overtones in the opposite sequence,before finally returning almost precisely to the original tone. evencreepier, it repeated this strange melody again and again, indefinitely, butalways with subtle variations on the theme.fermi loved this resultñhe referred to it affectionately as a òlittlediscovery.ó he had never guessed that nonlinear systems could harborsuch a penchant for order.in the 50 years since this pioneering study, scientists and engineershave learned to harness nonlinear systems, making use of their capacityfor selforganization. lasers, now used everywhere from eye surgery tocheckout scanners, rely on trillions of atoms emitting light waves inunison. superconductors transmit electrical current without resistance,the byproduct of billions of pairs of electrons marching in lockstep. theresulting technology has spawned the worldõs most sensitive detectors,used by doctors to pinpoint diseased tissues in the brains of epilepticswithout the need for invasive surgery, and by geologists to locate oilburied deep underground.but perhaps the most important lesson of fermiõs study is how feebleeven the best minds are at grasping the dynamics of large, nonlinearsystems. faced with a thicket of interlocking feedback loops, where everything affects everything else, our familiar ways of thinking fall apart. tosolve the most important problems of our time, weõre going to have tochange the way we do science.for example, cancer will not be cured by biologists working alone. itssolution will require a melding of both great discoveries of 1953. manycancers, perhaps most of them, involve the derangement of biochemicalnetworks that choreograph the activity of thousands of genes and procomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.60computer science: reflectionsteins. as fermi and his colleagues taught us, a complex system like thiscanõt be understood merely by cataloging its parts and the rules governing their interactions. the nonlinear logic of cancer will be fathomed onlythrough the collaborative efforts of molecular biologistsñthe heirs todr.watson and dr. crickñand mathematicians who specialize in complex systems, the heirs to fermi, pasta, and ulam.can such an alliance take place? well, it can if scientists embrace theexample set by an unstoppable 86yearold who, following his codiscoveryof the double helix, became increasingly interested in computer simulations of complex systems in the brain.happy anniversary, dr. crick. and a toast to the memory of enricofermi.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.simulation61making a computational splashronald fedkiw, stanford universityhave you ever sat in a movie theater with a box of popcorn and a softdrink watching a movie like the perfect storm and wondered where hollywood found a cameraman brave enough to operate a camera under suchdangerous conditions? well, that particular cameraman was sitting in asmall office north of san francisco at a company called industrial lightand magic, which was founded in 1975 by george lucas of star warsfame. there, a computer graphics specialist, our òcameraman,ó safelyoperated the camera with the mouse attached to his computer. in fact, thecamera was little more than a computer program that calculated the relative positions of boats and waves in order to add captivating imagery to atale about fishermen struggling with treacherous waves on rough seas.hollywood films are created to captivate and entertain, and thus theyfrequently contain exciting scenes with natural phenomena such as waterwaves, smoke, tornados, or even lava erupting from volcanoes. obviously, weõre not going to put george clooney on a boat in the middle ofthe atlantic ocean and subsequently pulverize it with waves until it sinks.in fact, weõre not going to put anyone on a boat in such a treacherousstorm, assuming we could even find such a storm in the first place. itturns out to be a lot easier to make waves out of math than out of water.for centuries, applied mathematicians and physicists have derivedmathematical equations describing the behavior of a variety of substancesincluding water waves in the ocean and the metal hull of a ship (box 3.1).these equations are quite complicated and can be solved only in specialsituations or with the aid of simplifying assumptions that usually rule outproblems of practical interest. however, in the last half century, the adventof computer technology has led to a revolution in the study of these typesof equations. using approximation theory, numerical analysts havedevised a number of algorithms that enable one to program computers toestimate solutions to many of the equations governing the physical worldto any desired accuracy. moreover, these numerical solutions provideuseful information for practical problems of interest to both scientists andengineers.solving such problems falls into an area of research referred to asòscientific computing.ó scientific computing has become the third branchof research in many engineering departments, joining theory and experiment as classical approaches to obtaining information about the worldaround us. while scientific computing has classically been applied tophysical problems such as those faced in mechanical, aerospace, and struccomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.62computer science: reflectionsbox 3.1the equations that describe the movement of liquids and gasesthe mathematical equations that describe the movement of liquids and gasesare known as the navierstokes equations. these equations describe how fluidparticles move through space, the effects of internal friction or viscosity, and theway sound waves are transmitted through a fluid. while viscosity is relativelystraightforward to account for, the particle motion and sound wave transmissioncan be rather difficult to deal with. computational methods for approximating theseeffects have to determine which way the particles and the sound waves are moving(and they usually move in different directions) and account for both this directionality and the speed of propagation. when modeling highspeed gas flows containing shock waves as shown in plate 4, it is important to accurately resolve both theparticle motion and the sound wave transmission effects. on the other hand, whenmodeling sloshing water or other liquids as shown in plates 1 and 2, precise treatment of the sound waves is not necessary in order to obtain adequate solutions ofthe equations. moreover, the treatment of fastmoving sound waves would makethe problem computationally much more expensive to solve. to remedy this difficulty, liquids can be assumed to be incompressible; that is, they preserve volume.this assumption removes the directional component and the stiffness of the soundwaves, making the problem computationally tractable. moreover, this is a physicallyrealistic assumption for liquids, especially when one considers that a liquidõs resistance to compression is responsible for the strong forces produced by hydraulicsdevices.tural engineering, the fastest growing application areas may currently bein electrical and computer engineering and in biology and medicine. morebroadly, scientific computing encompasses both computational scienceand computational mathematics. we emphasize that mathematics is thefundamental language for problem solving, and when confronted with aproblem most scientific researchers attempt to formulate a mathematicaldescription of that problem. once the mathematical description exists, theproblem is potentially solvable on a computer using either existing algorithmic techniques or newly devised methods. thus, scientific computinghas become a fundamental requirement for solving problems in signalprocessing, image analysis, robotics, computer vision, human computerinteraction, and computer graphics.returning to our cameraman and his goal of captivating us with dramatic sequences of waves pounding on a small ship, we now see thatthere are a lot of potential resources for creating òspecial effectsó waveson the computer. in the particular case of the perfect storm, industriallight and magic (ilm) had recently hired a professor of atmosphericcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.simulation63science (who specializes in scientific computing) to use his knowledge ofoceans and waves, especially his algorithmic knowledge, to constructnumerical solutions of large crashing waves on the ilm computers.the perfect storm was not the first instance of scientific computingalgorithms being used to make movies of boats and waves. a few yearsearlier, a department of defense (dod)oriented company called aret”associates started a small spinoff company called aret” entertainmentto create computer graphics software that constructs artificial, but surprisingly realistic, ocean wave models. one can imagine that it could beparticularly useful to be able to reproduce ocean wave models based on,for example, current wind speeds. if one could measure the wind andestimate what the ocean wave pattern should look like, this could becompared to the actual wave patterns in order to ascertain if there havebeen any recent outside influences on the wave patterns. that is, onecould subtract the estimated wave pattern from the current wave pattern,thus revealing the wake of a large ship that passed through the watereven some time ago. this becomes a lot more interesting when one realizesthat underwater ships (submarines) can create surface disturbances aswell. the implications for to the surveillance community are obvious.using its knowledge of scientific computing algorithms for studyingwaves, aret” entertainment developed computer graphics software tocreate ocean waves based on wind speeds. this software has won numerous awards and was used to create ocean waves in many films such aswater world and james cameronõs titanic. another recent film, cast away,used similar technology to simulate a plane crashing into the ocean,stranding tom hanks on a deserted island.as mentioned earlier, scientific computing algorithms can be used tosimulate a variety of natural phenomena, including the water shown inplates 1 and 2, and the smoke and fire shown in plate 3. while theseeveryday events (smoke, fire, and water) are easy to relate to, similarnumerical techniques can also be applied to events that the human eyewill miss completely. for example, plate 4 shows a computer simulationof a highspeed shock wave interacting with a bubble of helium. scientificcomputing is having a growing impact in the fields of imaging and dataanalysis as well. this is important, for example, in medicine, biology, andeven surveillance. for example, plate 5 shows several data points obtainedfrom mri imaging of a ratõs brain along with a threedimensional geometric reconstruction of the rat brain obtained by using numericalmethods on a computer.possibly the most exciting area for future applications of scientificcomputing is the computer simulation and study of humans themselves.researchers in biomechanics and medicine are currently working to writedown mathematical equations and numerical models that describe mostcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.64computer science: reflectionsof the human body from the cardiovascular system to muscles, bones, andorgans. as these equations and models are formulated, there is an evergrowing need for new computational algorithms that can be used for thecomputer simulation of biological structures. plate 6 shows a sample computer simulation of a skeleton running. geometric models were used forthe bones, mathematical equations were used to describe the limitedmotion allowed for by the connective tissue in joints, and special modelsfor soft tissue were used to simulate the muscles (the red regions in plate6represent the biceps muscles). plate 7 shows 30 muscles of the upper limbrepresented as a tetrahedral mesh ready for finite element simulation.computer simulation of humans is of interest to a wide variety of commercial industries as well. the entertainment industry would like to simulate virtual actors, the textile industry would like to simulate both runwaymodels and everyday customers trying on virtual clothing, and so on. seeplate 8 for a computer simulation of a piece of draped cloth.as both everyday personal computers and national laboratory supercomputers continue to increase in speed and as better algorithms aredeveloped, the size, complexity, and realism of the problems that can besimulated on these computers increase as well. in addition, as researchersbranch out into new and exciting research areas, they will formulate mathematical descriptions of their problems that are subsequently amenable tocomputer simulation. the future is bright in a number of research areas,and where researchers go, math and computer algorithms are sure tofollow.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.654abstraction, representation, andnotationsmodels capture phenomenañof the world or of the imaginationñin such a way that a generalpurpose computer can emulate,simulate, or create the phenomena. but the models are usuallynot obvious. the real world is complex and nonlinear, thereõs too muchdetail to deal with, and relationships among the details are often hidden.computer scientists deal with this problem by careful, deliberate creationof abstractions that express the models. these abstractions are representedsymbolically, in notations appropriate to the phenomena. the design oflanguages for these models and for analyzing, processing, and executingthem is a core activity of computer science.indeed, abstraction is a quintessential activity of computer scienceñthe intellectual tool that allows computer scientists to express their understanding of a problem, manage complexity, and select the level of detailand degree of generality they need at the moment. computer scientistscreate and discard abstractions as freely as engineers and architects createand discard design sketches. shaw describes the role of abstraction in building software, both thestuff of programsñalgorithms and representationsñand the role thatspecification and formal reasoning play in developing those abstractions.specific softwaredesign techniques such as information hiding and hierarchical organization provide ways to organize the abstract definitionsand the information they control. aho and larus describe how programming languages provide a notation to encode abstractions so as to allowtheir direct execution by computer.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.66computer science: reflectionsabstraction: imposing order on complexityin software designmary shaw, carnegie mellon universitythe success of a complex designed system depends on the correctorganization and interaction of thousands, even millions, of individualparts. if the designer must reason about all the parts at once, the complexity of the design task often overwhelms human capability. softwaredesigners, like other designers, manage this complexity by separating thedesign task into relatively independent parts. often, this entails designing large systems as hierarchical collections of subsystems, with the subsystems further decomposed into subsubsystems, and so on until theindividual components are of manageable size.for typical consumer products, the subsystems are physical components that can be put together on assembly lines. but the principle ofhierarchical system organization does not require an assembly line.simon1 tells a parable of two watchmakers, hora and tempus. both madeexcellent watches and were often visited by their customers. their watcheswere similar, each with about 1000 parts, but hora prospered whiletempus became progressively poorer and eventually lost his shop. tempus,it seems, made his watches in such a way that a partially assembled watchfell apart any time he put it down to deal with an interruption. hora, onthe other hand, made stable subassemblies of about 10 parts and assembledthese into 10 larger assemblies, then joined these to make each watch. soany time one of the watchmakers was interrupted by a customer, tempushad to restart from scratch on the current watch, but hora only lost thework of the current 10unit assemblyña small fraction of tempusõ loss.software systems do not require manual assembly of parts, but theyare large, complex, and amenable to a similar sort of discipline. softwaredesign benefits from hierarchical system organization based on subsystems that are relatively independent and that have known, simple,interactions. software designers create conceptual subassemblies withcoherent, comprehensible capabilities, similar to horaõs subassemblies.but whereas horaõs subassemblies might have been selected for convenience and physical organization, computer scientists are more likely tocreate structure around concepts and responsibilities. in doing so theycan often state the idea, or abstraction, that is realized by the structure; forexample, the capabilities of a software component are often described in1herbert a. simon, 1997, sciences of the artificial, 3rd ed., mit press, cambridge, mass.,pp. 188ff.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.abstraction, representation, and notations67terms of the componentõs observable properties, rather than the details ofthe componentõs implementation. while these abstractions may correspond to discrete software components (the analog of physical parts), thisis not necessarily the case. so, for example, a computer scientist mightcreate an abstraction for the software that computes a satellite trajectorybut might equally well create an abstraction for a communication protocol whose implementation is woven through all the separate softwarecomponents of a system. indeed, the abstractions of computer science canbe used in nonhierarchical as well as hierarchical structures. the abstractions of computer science are not in general the grand theories of thesciences (though we have those as well; see kleinberg and papadimitriouin chapter 2), but rather specific conceptual units designed for specifictasks.we represent these software abstractions in a combination of notationsñthe descriptive notations of specifications, the imperative notationsof programming, the descriptive notations of diagrams, and even narrative prose. this combination of descriptive and imperative languagesprovides separate descriptions of what is to be done (the specification)and how it is to be done (the implementation). a software componentcorresponding to an abstraction has a descriptive (sometimes formal)specification of its abstract capabilities, an operational (usually imperative) definition of its implementation, and some assuranceñwith varyingdegrees of rigor and completenessñthat the specification is consistentwith the implementation. formal descriptive notations, in particular, haveevolved more or less together with operational notations, and progresswith each depends on progress with the other. the result is that we candesign largescale systems software purposefully, rather than throughpure virtuosity, craft, or blind luck. we have not achievedñindeed,may never achieveñthe goal of complete formal specifications andprogramminglanguage implementations that are verifiably consistentwith those specifications. nevertheless, the joint history of these notationsshows how supporting abstractions at one scale enables exploration ofabstractions at a larger scale.abstractions in software systemsin the beginningñthat is to say, in the 1950sñsoftware designersexpressed programs and data directly in the representation provided bythe computer hardware or in somewhat more legible òassembly languagesó that mapped directly to the hardware. this required great conceptual leaps from problem domain to machine primitives, which limitedthe sophistication of the results. the late 1950s saw the introduction ofprogramming languages that allowed the programmer to describe comcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.68computer science: reflectionsputations through formulas that were compiled into the hardware representation. similarly, the descriptions of information representation originally referred directly to hardware memory locations (òthe flag field isbits 6 to 8 of the third word of the recordó). programming languages ofthe 1960s developed notations for describing information in somewhatmore abstract terms than the machine representation, so that the programmer could refer directly to òflagó and have that reference translatedautomatically to whichever bits were appropriate. not only are the moreabstract languages easier to read and write, but they also provide a degreeof decoupling between the program and the underlying hardware representation that simplifies modification of the program.in 1967 knuth2 showed us how to think systematically about theconcept of a data structure (such as a stack, queue, list, tree, graph, matrix,or set) in isolation from its representation and about the concept of analgorithm (such as search, sort, traversal, or matrix inversion) in isolationfrom the particular program that implements it. this separation liberatedus to think independently about the abstractionñthe algorithms and datadescriptions that describe a result and its implementationñthe specific program and data declarations that implement those ideas on a computer.the next few years saw the development of many elegant and sophisticated algorithms with associated data representations. sometimes thespeed of the algorithm depended on a special trick of representation.such was the case with inplace heapsort, a sorting algorithm that beginsby regardingñabstractingñthe values to be sorted as a onedimensionalunsorted array. as the heapsort algorithm runs, it rearranges the valuesin a particularly elegant way so that one end of the array can be abstractedas a progressively growing tree, and when the algorithm terminates, theentire array has become an abstract tree with the sorted values in a simpletoextract order. in most actual programs that implemented heapsort,though, these abstractions were not described explicitly, so any programmer who changed the program had to depend on intuition and sketchy,often obsolete, prose documentation to determine the original programmerõsintentions. further, the program that implemented the algorithms had nospecial relation to the data structures. this situation was fraught withopportunities for confusion and for lapses of discipline, which led toundocumented (frequently unintended) dependencies on representationtricks. unsurprisingly, program errors often occurred when another programmer subsequently changed the data representation. in response tothis problem, in the 1970s a notion of òtypeó emerged to help document2donald knuth, 1967, the art of computer programming, vol. 1, addisonwesley, boston,mass.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.abstraction, representation, and notations69the intended uses of data. for example, we came to understand thatreferring to record fields abstractlyñby a symbolic name rather than byabsolute offset from the start of a data blockñmade programs easier tounderstand as well as to modify, and that this could often be done without making the program run slower.at the same time, the intense interest in algorithms dragged representation along as a poor cousin. in the early 1970s, there was a growingsense that ògetting the data structures rightó was a key to good softwaredesign. parnas3 elaborated this idea, arguing that a focus on data structures should lead to organizing software modules around data structuresrather than around collections of procedures. further, he advanced thethenradical proposition that not all information about how data is represented should be shared, because programmers who used the data wouldrely on things that might subsequently change. better, he said, to specifywhat a module would accomplish and allow privileged access to thedetails only for selected code whose definition was in the same module asthe representation. the abstract description should provide all theinformation required to use the component, and the implementer of thecomponent would only be obligated to keep the promises made in thatdescription. he elaborated this idea as òinformation hiding.ó parnas subsequently spent several years at the naval research laboratory applyingthese ideas to the specification of the a7e avionics system, showing thatthe idea could scale up to practical realworld systems.this was one of the precursors of objectoriented programming andthe marketplace for independently developed components that can beused unchanged in larger systems, from components that invoke by procedure calls from a larger system through java applets that download intoweb browsers and thirdparty filters for photoprocessing programs.computer scientists are still working out the consequences of usingabstract descriptions to encapsulate details. abstractions can, in somecircumstances, be used in many software systems rather than customdefined for a specific use. however, the interactions between parts can besubtleñincluding not only the syntactic rules for invoking the parts butalso the semantics of their computationsñand the problems associatedwith making independently developed parts work properly togetherremain an active research area.so why isnõt such a layered abstract description just a house of cards,ready to tumble down in the slightest whiff of wind? because we partitionour tasks so that we deal with different concerns at different levels of3david l. parnas, 1972, òon the criteria to be used in decomposing systems into modules,ó communications of the acm 15(2):10531058.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.70computer science: reflectionsabstraction; by establishing reasonable confidence in each level of abstraction and understanding the relations between the levels, we build ourconfidence in the whole system. some of our confidence is operational:we use tools with a demonstrated record of success. chief among thesetools are the programming languages, supported by compilers that automatically convert the abstractions to code (see aho and larus in thischapter). other confidence comes from testingña kind of endtoendcheck that the actual software behaves, at least to the extent we can check,like the system we intended to develop. deeper confidence is instilled byformal analysis of the symbolic representation of the software, whichbrings us to the second part of the story.specifications of software systemsin the beginning, programming was an art form and debugging wasvery much ad hoc. in 1967, floyd4 showed how to reason formally aboutthe effect a program has on its data. more concretely, he showed that foreach operation a simple program makes, you can state a formal relationbetween the previous and following program state; further, you can compose these relations to determine what the program actually computes.specifically he showed that given a program, a claim about what thatprogram computes, and a formal definition of the programming language,you can derive the starting conditions, if any, for which that claim is true.hoare and dijkstra created similar but different formal rules for reasoning about programs in pascallike languages in this way.the immediate reaction, that programs could be òproved correctó(actually, that the implementation of a program could be shown to beconsistent with its specification) proved overly optimistic. however, thepossibility of reasoning formally about a program changed the way peoplethought about programming and stimulated interest in formal specification of components and of programming languagesñfor precision inexplanation, if not for proof. formal specifications have now been receivedwell for making intentions precise and for some specific classes of analysis,but the original promise remains unfulfilled. for example, there remainsa gap between specifications of practical realworld systems and the complete, static specifications of the dream. other remaining problems includeeffective specifications of properties other than functionality, tractabilityof analysis, and scaling to problems of realistic size.4r.w. floyd, 1967, òassigning meanings to programs,ó proceedings of symposia in appliedmathematics, vol. 1932, american mathematical society, providence, r.i.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.abstraction, representation, and notations71in 1972, hoare5 showed how to extend this formalized reasoning toencapsulations of the sort parnas was exploring. this showed how toformalize the crucial abstraction step that expresses the relation betweenthe abstraction and its implementation. later in the 1970s, theoreticalcomputer scientists linked the pragmatic notion of types that allowedcompilers to do some compiletime checking to a theoretical model oftype theory.one of the obstacles to òproving programs correctó was the difficultyin creating a correct formal definition of the programming language inwhich the programs were written. the first approach was to add formalspecifications to the programming language, as in alphard, leaving proofdetails to the programmer. the formal analysis task was daunting, and itwas rarely carried out. further, many of the properties of interest about aparticular program do not lend themselves to expression in formal logic.the second approach was to work hard on a simple common programming language such as pascal to obtain formal specifications of the language semantics with only modest changes to the language, with a resultsuch as euclid. this revealed capabilities of programming languages thatdo not lend themselves to formalization. the third approach was to designa family of programming languages such as ml that attempt to includeonly constructs that lend themselves to formal analysis (assuming, ofcourse, a correct implementation of the compiler). these languages requirea style of software development that is an awkward match for manysoftware problems that involve explicit state and multiple cooperatingthreads of execution.formal specifications have found a home in practice not so much inverification of full programs as in the use of specifications to clarifyrequirements and design. the cost of repairing a problem increases drastically the later the problem is discovered, so this clarification is of substantial practical importance. in addition, specific critical aspects of aprogram may be analyzed formally, for example through static analysisor model checking.the interaction of abstraction and specificationthis brings us to the third part of our story: the coupling betweenprogress in the operational notations of programming languages and thedescriptive notations of formal specification systems. we can measure5c.a.r. hoare, 1972, òproofs of correctness of data representations,ó acta informatica1:271281.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.72computer science: reflectionsprogress in programming language abstraction, at least qualitatively, bythe scale of the supported abstractionsñthe quantity of machine coderepresented by a single abstract construct. we can measure progress informal specification, equally qualitatively, by the fraction of a complexsoftware system that is amenable to formal specification and analysis.and we see in the history of both, that formal reasoning about programshas grown hand in hand with the capability of the languages to expresshigherlevel abstractions about the software. neither advances very farwithout waiting for the other to catch up.we can see this in the development of type systems. one of the earliest type systems was the fortran variable naming convention: operationson variables whose names began with i, j, k, l, or m were compiled withfixedpoint arithmetic, while operations on all other variables were compiled with floatingpoint arithmetic. this approach was primitive, but itprovided immediate benefit to the programmer, namely correct machinecode. a few years later, algol 60 provided explicit syntax for distinguishing types, but this provided little benefit to the programmer beyond thefixed/floating point discriminationñand it was often ignored. later languages that enforced type checking ran into programmer opposition totaking the time to write declarations, and the practice became acceptableonly when it became clear that the type declarations enabled analysis thatwas immediately useful, namely discovering problems at compile timerather than execution time.so type systems originally entered programming languages as amechanism for making sure at compile time that the runtime valuessupplied for expression evaluation or procedure calls would be legitimate. (morris later called this òneanderthal verification.ó) but the nuancesof this determination are subtle and extensive, and type systems soonfound a role in the research area of formal semantics of programminglanguages. here they found a theoretical constituency, spawning theirown problems and solutions.meanwhile, abstract data types were merging with the inheritancemechanisms of smalltalk to become objectoriented design and programming models. the inheritance mechanisms provided ways to express complex similarities among types, and the separation of specification fromimplementation in abstract data types allowed management of the codethat implemented families of components related by inheritance. inheritance structures can be complex, and formal analysis techniques for reasoning about these structures soon followed.with wider adoption of mllike languages in the 1990s, the functional programming languages began to address practical problems,thereby drawing increasing attention from software developers for whomcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.abstraction, representation, and notations73correctness is a critical concernñand for whom the prospect of assurances about the software justifies extra investment in analysis.the operational abstraction and symbolic analysis lines of researchmade strong contact again in the development of the java language, whichincorporates strong assurances about type safety with objectorientedabstraction.so two facets of programming language designñlanguage mechanisms to support abstraction and incorporation of formal specificationand semantics in languagesñhave an intertwined history, with advanceson each line stimulated by problems from both lines, and with progresson one line sometimes stalled until the other line catches up.additional observationshow are the results of research on languages, models, and formalisms to be evaluated? for operational abstractions, the models and thedetailed specifications of relevant properties have a utilitarian function,so appropriate evaluation criteria should reflect the needs of softwaredevelopers. expertise in any field requires not only higherorder reasoning skills, but also a large store of facts, together with a certain amount ofcontext about their implications and appropriate use.6 it follows that models and tools intended to support experts should support rich bodies ofoperational knowledge. further, they should support large vocabulariesof established knowledge as well as the theoretical base for deriving information of interest.contrast this with the criteria against which mathematical systemsare evaluated. mathematics values elegance and minimality of mechanism; derived results are favored over added content because they arecorrect and consistent by their construction. these criteria are appropriatefor languages whose function is to help understand the semantic basis ofprogramming languages and the possibility of formal reasoning.given the differences in appropriate base language size that arisefrom the different objectives, it is small wonder that different criteria areappropriate, or that observers applying such different criteria reach different conclusions about different research results.6this is true across a wide range of problem domains; studies have demonstrated it formedical diagnosis, physics, chess, financial analysis, architecture, scientific research, policydecision making, and others (raj reddy, 1988, òfoundations and grand challenges of artificial intelligence,ó ai magazine, winter; herbert a. simon, 1989, òhuman experts andknowledgebased systems,ó pp. 121 in concepts and characteristics of knowledgebased systems (m. tokoro, y. anzai, and a. yonezawa, eds.), northholland publishing, amsterdam).computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.74computer science: reflectionsprogramming languages and computer sciencealfred v. aho, columbia university, andjames larus, microsoft researchsoftware affects virtually every modern personõs life, often profoundly,but few appreciate the vast size and scope of the worldwide infrastructure behind it or the ongoing research aimed at improving it. hundreds ofbillions of lines of software code are currently in use, with many morebillions added annually, and they virtually run the gamut of conceivableapplications. it has been possible to build all this software because wehave been successful in inventing a wide spectrum of programming languages for describing the tasks we want computers to do. but like humanlanguages, they are sometimes quirky and imperfect. thus computer scientists are continually evolving more accurate, expressive, and convenient ways in which humans may communicate to computers.programming languages are different in many respects from humanlanguages. a computer is capable of executing arithmetic or logicaloperations at blinding speeds, but it is in fact a device thatõs frustratinglysimplemindedñforever fixed in a concrete world of bits, bytes, arithmetic, and logic (see hill in chapter 2). thus a computer must be givenstraightforward, unambiguous, stepbystep instructions. humans, bycontrast, can often solve highly complex problems using their innatestrengths of formulating and employing abstraction.to get a feel for the extent of this òsemantic gap,ó imagine explainingto a young child how to prepare a meal. given that the child likely has noexperience or context to draw upon, every step must be described clearlyand completely, and omitting even the simplest detail can lead to a messyfailure. explaining tasks to computers is in many ways more difficult,because computers not only require far more detail but that detail mustalso be expressed in a primitive difficulttoread notation such as binarynumbers.as an example of how programming languages bridge the gap betweenprogrammers and computers, consider numerical computation, one ofthe earliest applications of computers, dating back to world war ii. acommon mathematical operation is to multiply two vectors of numbers.humans will use a notation such as a*b to indicate the multiplication(i.e., dot product) of vector a and vector bñknowing that this is shorthand for all of the individual steps actually needed to perform the multiplication. computers, on the other hand, know nothing about vectors orthe rules for multiplying them. they can only move numbers around;perform addition, multiplication, and other primitive mathematical operacomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.abstraction, representation, and notations75tions on them; and make simple decisions. expressed in terms of theseprimitive operations, a simple vector multiplication routine might requireroughly 20 computer instructions, while a more sophisticated version,which improves performance by using techniques like instructionlevelparallelism and caches (see hill in chapter 2), might require a few hundred instructions. someone looking at this machinelanguage routinecould easily be excused for not spotting the simple mathematical operation embodied in the complicated sequence of machine instructions.a highlevel programming language addresses this òsemantic gapóbetween human and machine in several ways. it can provide operationsspecifically designed to help formulate and solve a particular type ofproblem. a programming language specifically intended for numericcomputation might use the humanfriendly, concise notation a*b. it savesprogrammers from repeatedly reimplementing (or misimplementing) thesame operations. a software tool called a òcompileró translates the higherlevel program into instructions executable by a computer.programmers soon realized that a program written in a highlevellanguage could be run on more than one computer. because the hardwarepeculiarities of a particular computer could be hidden in a compiler, ratherthan exposed in a language, programs could often be written in a portablelanguage that can be run on several computers. this separation of highlevel programs and computers expanded the market for commercial software and helped foster the innovative software industry.another advantage of compilers is that a program written in a highlevel language often runs faster. compilers, as a result of several decadesof fundamental research on program analysis, code generation, and codeoptimization techniques, are generally far better at translating programsinto efficient sequences of computer instructions than are human programmers. the comparison is interesting and edifying.programmers can occasionally produce small and ingenious pieces ofmachine code that run much faster than the machine instructions generated by a compiler. however, as a program grows to thousands of lines ormore, a compilerõs systematic, analytical approach usually results in higherquality translations that not only execute far more effectively but alsocontain fewer errors.program optimization is a very fertile area of computer science research.a compiler improves a program by changing the process by which itcomputes its result to a slightly different approach that executes faster. acompiler is allowed to make a change only if it does not affect the resultthat the program computes.interestingly, true optimization is a goal that is provably impossible.an analysis algorithm that predicts if a nontrivial modification affects aprogramõs result can be used to solve the program equivalence problem,computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.76computer science: reflectionswhich is provably impossible because of turingõs result (see kleinbergand papadimitriou in chapter 2). compilers sidestep this conundrum bymodifying a program only when it is possible to demonstrate that thechange leaves the programõs result unaffected. otherwise, they assumethe worst and leave alone programs in which there is any doubt about theconsequences of a change. the interplay between turingõs fundamentalresult, which predates programming languages and compilers by manyyears, and the vast number of practical and effective tools for analyzingand optimizing programs is emblematic of computer science as a whole,which continues to make steady progress despite many fundamental limitations on computability.the past half century has seen the development of thousands ofprogramming languages that use many different approaches to writingprograms. for example, some languages, socalled imperative languages,specify how a computation is to be done, while declarative languagesfocus on what the computer is supposed to do. some languages aregeneralpurpose, but many others are intended for specific applicationdomains. for example, the languages c and c++ are commonly used insystems programming, sql in writing queries for databases, and postscriptin describing the layout of printed material. innovations and new applications typically produce new languages. for example, the internet spurreddevelopment of java for writing client/server applications and javascriptand flash for animating web pages.one might ask, òare all of these languages necessary?ó turingõs researchon the nature of computing (see kleinberg and papadimitriou in chapter2)offers one answer to this question. since almost every programming language is equivalent to turingõs universal computing machine, they are allin principle capable of expressing the same algorithms. but the choice ofan inappropriate language can greatly complicate programming. it is notunlike asking whether a bicycle, car, and airplane are interchangeablemodes of transportation. just as it would be cumbersome, at best, to fly ajet to the grocery store to buy milk, so using the wrong programminglanguage can make a program much longer and much more difficult towrite and execute.today, most programs are written by teams of programmers. in thisworld, many programming problems and errors arise from misunderstandings of intent, misinformation, and human shortcomings, so language designers have come to recognize that programming languagesconvey information among human programmers, as well as to computers.language designers soon realized that programming languages mustbe extensible as well as computationally universal, as no one languagecould provide operations appropriate for all types of problems. languagestoday offer many general mechanisms for programmers to use in addresscomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.abstraction, representation, and notations77ing their specific problems. one of the early and most fundamental ofthese mechanisms introduced into programming languages was the òprocedure,ó which collects and names the code to perform a particular operation. so, for example, a programmer who wants to implement operationsthat involve multiplying vectors in a language in which this capability isnot built in could create a procedure with a meaningful name, such asòmultiplyvector,ó and simply cite that name to invoke that procedurewhenever neededñas opposed to rewriting the same set of instructionseach time. and programmers could then use the procedure in other programs rather than reinventing the wheel each time. procedures of this sorthave understandably become the fundamental building blocks of todayõsprograms.another early insight is built on the fact that statements in a programtypically execute in one of a small number of different patterns; thus thepatterns themselves could be added to the vocabulary of a language ratherthan relying on a programmer to express the patterns with simpler (and alarger number of) statements. for example, a common idiom is to executea group of statements repeatedly while a condition holds true. this iswritten:while (condition) dostatementsearlier languages did not provide this feature and instead relied onprogrammers to construct it, each time it was needed, from simpler statements:test:if (not condition) then goto done;statementsgoto test;done:the latter approach has several problems: the program is longer, theprogrammerõs intent is more difficult to discern, and possibilities forerrors increase. for example, if the first statement said ògoto testóinstead of ògoto done,ó this piece of code would never terminate.incorporation of new constructs to aid in the development of morerobust software systems has been a continuing major trend in programminglanguage development. in addition to wellstructured features forcontrolling programs such as the òwhile loop,ó other improvements includefeatures that permit dividing up software into modules, strong type checkingto catch some errors at compile time rather than run time, and incorporacomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.78computer science: reflectionstion of automated memory management that frees the programmer fromworrying about details of allocating and deallocating storage. these features not only improve the ability of a programming language to expressa programmerõs intent but also offer better facilities for detecting inconsistencies and other errors in programs. todayõs huge and evergrowing software infrastructure presents anenormous challenge for programmers, software companies, and societyas whole. because programs are written by people, they contain defectsknown as bugs. even the best programs, written using the most advancedsoftware engineering techniques, contain between 10 and 10,000 errorsper million lines of new code. some defects are minor, while others havethe potential to disrupt society significantly.the constantly evolving programming languages, techniques, andtools have done much to improve the quality of software. but the software revolution is always in need of some sweetening. programminglanguage researchers are devoting increasing attention to producing programs with far fewer defects and systems with much higher levels of faulttolerance. they are also developing software verification tools of greaterpower and rigor that can be used throughout the software developmentprocess. the ultimate research goal is to produce programming languagesand software development tools with which robust software systems canbe created routinely and economically for all of tomorrowõs applications.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.795data, representation, and informationthe preceding two chapters address the creation of models that capture phenomena of interest and the abstractions both for data andfor computation that reduce these models to forms that can beexecuted by computer. we turn now to the ways computer scientists dealwith information, especially in its static form as data that can be manipulated by programs.gray begins by narrating a long line of research on databasesñstorehouses of related, structured, and durable data. we see here that theobjects of research are not data per se but rather designs of òschemasó thatallow deliberate inquiry and manipulation. gray couples this review withintrospection about the ways in which database researchers approachthese problems.databases support storage and retrieval of information by definingñin advanceña complex structure for the data that supports the intendedoperations. in contrast, lesk reviews research on retrieving informationfrom documents that are formatted to meet the needs of applicationsrather than predefined schematized formats.interpretation of information is at the heart of what historians do, andayers explains how information technology is transforming their paradigms. he proposes that history is essentially model buildingñconstructing explanations based on available informationñand suggests that themethods of computer science are influencing this core aspect of historicalanalysis.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.80computer science: reflectionsdatabase systems:a textbook case of research paying offjim gray, microsoft researcha small research investment helped produce u.s. market dominancein the $14 billion database industry. government and industry funding ofa few research projects created the ideas for several generations ofproducts and trained the people who built those products. continuingresearch is now creating the ideas and training the people for the nextgeneration of products.industry profilethe database industry generated about $14 billion in revenue in 2002and is growing at 20 percent per year, even though the overall technologysector is almost static. among software sectors, the database industry issecond only to operating system software. database industry leaders areall u.s.based corporations: ibm, microsoft, and oracle are the threelargest. there are several specialty vendors: tandem sells over $1 billion/year of faulttolerant transaction processing systems, teradata sells about$1 billion/year of datamining systems, and companies like informationresources associates, verity, fulcrum, and others sell specialized dataand textmining software.in addition to these wellestablished companies, there is a vibrantgroup of small companies specializing in applicationspecific databasesñfor text retrieval, spatial and geographical data, scientific data, imagedata, and so on. an emerging group of companies offer xmlorienteddatabases. desktop databases are another important market focused onextreme ease of use, small size, and disconnected (offline) operation.historical perspectivecompanies began automating their backoffice bookkeeping in the1960s. the cobol programming language and its recordoriented filemodel were the workhorses of this effort. typically, a batch of transactions was applied to the oldtapemaster, producing a newtapemasterand printout for the next business day. during that era, there was considerable experimentation with systems to manage an online database thatcould capture transactions as they happened. at first these systems weread hoc, but late in that decade network and hierarchical database productsemerged. a cobol subcommittee defined a network data model stancomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information81dard (dbtg) that formed the basis for most systems during the 1970s.indeed, in 1980 dbtgbased cullinet was the leading software company.however, there were some problems with dbtg. dbtg uses a lowlevel, recordatatime procedural language to access information. theprogrammer has to navigate through the database, following pointersfrom record to record. if the database is redesigned, as often happens overa decade, then all the old programs have to be rewritten.the relational data model, enunciated by ibm researcher ted codd ina 1970 communications of the association for computing machinery article,1was a major advance over dbtg. the relational model unified data andmetadata so that there was only one form of data representation. it defineda nonprocedural data access language based on algebra or logic. it waseasier for end users to visualize and understand than the pointersandrecordsbased dbtg model.the research community (both industry and university) embracedthe relational data model and extended it during the 1970s. most significantly, researchers showed that a nonprocedural language could be compiled to give performance comparable to the best recordoriented databasesystems. this research produced a generation of systems and people thatformed the basis for products from ibm, ingres, oracle, informix, sybase,and others. the sql relational database language was standardized byansi/iso between 1982 and 1986. by 1990, virtually all database systemsprovided an sql interface (including network, hierarchical, and objectoriented systems).meanwhile the database research agenda moved on to geographically distributed databases and to parallel data access. theoretical workon distributed databases led to prototypes that in turn led to products.today, all the major database systems offer the ability to distribute andreplicate data among nodes of a computer network. intense research ondata replication during the late 1980s and early 1990s gave rise to a secondgeneration of replication products that are now the mainstays of mobilecomputing.research of the 1980s showed how to execute each of the relationaldata operators in parallelñgiving hundredfold and thousandfold speedups. the results of this research began to appear in the products of severalmajor database companies. with the proliferation of data mining in the1990s, huge databases emerged. interactive access to these databasesrequires that the system use multiple processors and multiple disks toread all the data in parallel. in addition, these problems require near1e.f. codd, 1970, òa relational model of data from large shared data banks,ó communications of the acm 13(6):377387. available online at http://www.acm.org/classics/nov95/.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.82computer science: reflectionslinear time search algorithms. university and industrial research of theprevious decade had solved these problems and forms the basis of thecurrent vldb (very large database) datamining systems.rollup and drilldown data reporting systems had been a mainstay ofdecisionsupport systems ever since the 1960s. in the middle 1990s, theresearch community really focused on datamining algorithms. theyinvented very efficient data cube and materialized view algorithms thatform the basis for the current generation of business intelligence products.the most recent round of governmentsponsored research creating anew industry comes from the national science foundationõs digitallibraries program, which spawned google. it was founded by a group ofòdatabaseó graduate students who took a fresh look at how informationshould be organized and presented in the internet era.current research directionsthere continues to be active and valuable research on representingand indexing data, adding inference to data search, compiling queriesmore efficiently, executing queries in parallel, integrating data from heterogeneous data sources, analyzing performance, and extending the transaction model to handle long transactions and workflow (transactions thatinvolve human as well as computer steps). the availability of huge volumes of data on the internet has prompted the study of data integration,mediation, and federation in which a portal system presents a unificationof several data sources by pulling data on demand from different parts ofthe internet.in addition, there is great interest in unifying objectoriented conceptswith the relational model. new data types (image, document, and drawing) are best viewed as the methods that implement them rather than bythe bytes that represent them. by adding procedures to the databasesystem, one gets active databases, data inference, and data encapsulation.this objectoriented approach is an area of active research and fermentboth in academe and industry. it seems that in 2003, the research prototypesare mostly done and this is an area that is rapidly moving into products.the internet is full of semistructured datañdata that has a bit ofschema and metadata, but is mostly a loose collection of facts. xml hasemerged as the standard representation of semistructured data, but thereis no consensus on how such data should be stored, indexed, or searched.there have been intense research efforts to answer these questions. prototypes have been built at universities and industrial research labs, and nowproducts are in development.the database research community now has a major focus on streamdata processing. traditionally, databases have been stored locally and arecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information83updated by transactions. sensor networks, financial markets, telephonecalls, credit card transactions, and other data sources present streams ofdata rather than a static database. the stream data processing researchersare exploring languages and algorithms for querying such streams andproviding approximate answers.now that nearly all information is online, data security and data privacyare extremely serious and important problems. a small, but growing, part ofthe database community is looking at ways to protect peopleõs privacy bylimiting the ways data is used. this work also has implications for protecting intellectual property (e.g., digital rights management, watermarking)and protecting data integrity by digitally signing documents and thenreplicating them so that the documents cannot be altered or destroyed.case historiesthe u.s. government funded many database research projects from1972 to the present. projects at the university of california at los angelesgave rise to teradata and produced many excellent students. projects atcomputer corp. of america (sdd1, daplex, multibase, and hipac)pioneered distributed database technology and objectoriented databasetechnology. projects at stanford university fostered deductive databasetechnology, data integration technology, query optimization technology,and the popular yahoo! and google internet sites. work at carnegiemellon university gave rise to general transaction models and ultimatelyto the transarc corporation. there have been many other successes fromat&t, the university of texas at austin, brown and harvard universities,the university of maryland, the university of michigan, massachusettsinstitute of technology, princeton university, and the university oftoronto among others. it is not possible to enumerate all the contributions here, but we highlight three representative research projects thathad a major impact on the industry.project ingresproject ingres started at the university of california at berkeley in1972. inspired by coddõs paper on relational databases, several facultymembers (stonebraker, rowe, wong, and others) started a project todesign and build a relational system. incidental to this work, theyinvented a query language (quel), relational optimization techniques, alanguage binding technique, and interesting storage strategies. they alsopioneered work on distributed databases.the ingres academic system formed the basis for the ingres productnow owned by computer associates. students trained on ingres went oncomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.84computer science: reflectionsto start or staff all the major database companies (at&t, britton lee, hp,informix, ibm, oracle, tandem, sybase). the ingres project went on toinvestigate distributed databases, database inference, active databases,and extensible databases. it was rechristened postgres, which is now thebasis of the digital library and scientific database efforts within the university of california system. recently, postgres spun off to become thebasis for a new objectrelational system from the startup illustra information technologies.system rcoddõs ideas were inspired by seeing the problems ibm and its customers were having with ibmõs ims product and the dbtg network datamodel. his relational model was at first very controversial; people thoughtthat the model was too simplistic and that it could never give good performance. ibm research management took a gamble and chartered a small(10person) systems effort to prototype a relational system based on coddõsideas. that system produced a prototype that eventually grew into thedb2 product series. along the way, the ibm team pioneered ideas inquery optimization, data independence (views), transactions (logging andlocking), and security (the grantrevoke model). in addition, the sqlquery language from system r was the basis for the ansi/iso standard.the system r group went on to investigate distributed databases(project r*) and objectoriented extensible databases (project starburst).these research projects have pioneered new ideas and algorithms. theresults appear in ibmõs database products and those of other vendors.gammanot all research ideas work out. during the 1970s there was greatenthusiasm for database machinesñspecialpurpose computers that wouldbe much faster than generalpurpose operating systems running conventional database systems. these research projects were often based onexotic hardware like bubble memories, headpertrack disks, or associative ram. the problem was that generalpurpose systems were improving at 50 percent per year, so it was difficult for exotic systems to competewith them. by 1980, most researchers realized the futility of specialpurposeapproaches and the databasemachine community switched to researchon using arrays of generalpurpose processors and disks to process datain parallel.the university of wisconsin hosted the major proponents of this ideain the united states. funded by the government and industry, thoseresearchers prototyped and built a parallel database machine calledcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information85gamma. that system produced ideas and a generation of students whowent on to staff all the database vendors. today the parallel systems fromibm, tandem, oracle, informix, sybase, and microsoft all have a directlineage from the wisconsin research on parallel database systems. theuse of parallel database systems for data mining is the fastestgrowingcomponent of the database server industry.the gamma project evolved into the exodus project at wisconsin(focusing on an extensible objectoriented database). exodus has nowevolved to the paradise system, which combines objectoriented andparallel database techniques to represent, store, and quickly process hugeearthobserving satellite databases.and then there is sciencein addition to creating a huge industry, database theory, science, andengineering constitute a key part of computer science today. representing knowledge within a computer is one of the central challenges of computer science (box 5.1). database research has focused primarily on thisfundamental issue. many universities have faculty investigating theseproblems and offer classes that teach the concepts developed by thisresearch program.box 5.1how do you know? a longrange view of database researchhow can knowledge be represented so that algorithms can make new inferences from the knowledge base? this problem has challenged philosophers formillennia. there has been progress. euclid axiomized geometry and proved itsbasic theorems, and in doing so implicitly demonstrated mechanical reasoningfrom first principles. george booleõs laws of thought created a predicate calculus,and laplaceõs work on probability was a first start on statistical inference.each of these threadsñproofs, predicate calculus, and statistical inferenceñwere major advances; but each requires substantial human creativity to fit newproblems to the solution. wouldnõt it be nice if we could just put all the books andjournals in a library that would automatically organize them and start producingnew answers?there are huge gaps between our current tools and the goal of a selforganizinglibrary, but computer scientists are trying to fill the gaps with better algorithms andbetter ways of representing knowledge. databases are one branch of this effort torepresent information and reason about it. the database community has taken abottomup approach, working with simple data representations and developing acalculus for asking and answering questions about the database.continuedcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.86computer science: reflectionsthe fundamental approach of database researchers is to insist that the information must be schematizedñthe information must be represented in a predefinedschema that assigns a meaning to each value. the authortitlesubjectabstractschema of a library system is a typical example of this approach. the schema isused both to organize the data and to make it easy to express questions about thedatabase.database researchers have labored to make it easy to define the schema,easy to add data to the database, and easy to pose questions to the database.early database systems were dreadfully difficult to useñlargely because welacked the algorithms to automatically index huge databases and lacked powerfulquery tools. today there are good tools to define schemas, and graphical tools thatmake it easy to explore and analyze the contents of a database.this has required invention at all levels of the problem. at the lowest levelswe had to discover efficient algorithms to sort, index, and organize numeric, text,temporal, and spatial information so that higherlevel software could just pick froma wide variety of organizations and algorithms. these lowlevel algorithms maskdata placement so that it can be spread among hundreds or thousands of disks;they mask concurrency so that the higherlevel software can view a consistentdata snapshot, even though the data is in flux. the lowlevel software includesenough redundancy so that once data is placed in the database, it is safe toassume that the data will never be lost. one major advance was the theory andalgorithms to automatically guarantee these concurrencyreliability properties.text, spatial, and temporal databases have always posed special challenges.certainly there have been huge advances in indexing these databases, butresearchers still have many more problems to solve. the advent of image, video,and sound databases raises new issues. in particular, we are now able to extracta huge number of features from images and sounds, but we have no really goodways to index these features. this is just another aspect of the òcurse of dimensionalityó faced by database systems in the datamining and data analysis area.when each object has more than a dozen attributes, traditional indexing techniques give little help in reducing the approximate search space.so, there are still many unsolved research challenges for the lowlevel database òplumbers.óthe higherlevel software that uses this plumbing has been a huge success.early on, the research community embraced the relational data model championedby ted codd. codd advocated the use of nonprocedural setoriented programming to define schemas and to pose queries. after a decade of experimentation,these research ideas evolved into the sql database language. having this highlevel nonprocedural language was a boon both to application programmers and todatabase implementers. application programmers could write much simpler programs. the database implementers faced the challenge of optimizing and executing sql. because it is so high level (sql is a nonprocedural functional dataflowlanguage), sql allows data to be distributed across many computers and disks.because the programs do not mention any physical structures, the implementer isfree to use whatever òplumbingó is available. and because the language is functional, it can be executed in parallel.box 5.1 continuedcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information87techniques for implementing the relational data model and algorithms forefficiently executing database queries remain a core part of the database researchagenda. over the last decade, the traditional database systems have grown toinclude analytics (data cubes), and also datamining algorithms borrowed from themachinelearning and statistics communities. there is increasing interest in solving information retrieval and multimedia database issues.today, there are very good tools for defining and querying traditional database systems; but, there are still major research challenges in the traditional database field. the major focus is automating as much of the data administration tasksas possibleñmaking the database system selfhealing and selfmanaging.we are still far from the goal of building systems that automatically ingestinformation, reason about it, and produce answers on demand. but the goal iscloser, and it seems attainable within this century.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.88computer science: reflectionscomputer science is to information aschemistry is to mattermichael lesk, rutgers universityin other countries computer science is often called òinformaticsó orsome similar name. much computer science research derives from theneed to access, process, store, or otherwise exploit some resource of useful information. just as chemistry is driven to large extent by the need tounderstand substances, computing is driven by a need to handle data andinformation. as an example of the way chemistry has developed, seeoliver sacksõs book uncle tungsten: memories of a chemical boyhood(vintage books, 2002). he describes his explorations through the differentmetals, learning the properties of each, and understanding their applications. similarly, in the history of computer science, our information needsand our information capabilities have driven parts of the research agenda.information retrieval systems take some kind of information, such as textdocuments or pictures, and try to retrieve topics or concepts based onwords or shapes. deducing the concept from the bytes can be difficult,and the way we approach the problem depends on what kind of bytes wehave and how many of them we have.our experimental method is to see if we can build a system that willprovide some useful access to information or service. if it works, thosealgorithms and that kind of data become a new field: look at areas likegeographic information systems. if not, people may abandon the areauntil we see a new motivation to exploit that kind of data. for example,facerecognition algorithms have received a new impetus from securityneeds, speeding up progress in the last few years. an effective strategy tomove computer science forward is to provide some new kind of information and see if we can make it useful.chemistry, of course, involves a dichotomy between substances andreactions. just as we can (and frequently do) think of computer science interms of algorithms, we can talk about chemistry in terms of reactions.however, chemistry has historically focused on substances: the encyclopedias and indexes in chemistry tend to be organized and focused oncompounds, with reaction names and schemes getting less space on theshelf. chemistry is becoming more balanced as we understand reactionsbetter; computer science has always been more heavily oriented towardalgorithms, but we cannot ignore the driving force of new kinds of data.the history of information retrieval, for example, has been driven bythe kinds of information we could store and use. in the 1960s, for example,storage was extremely expensive. research projects were limited to textcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information89materials. even then, storage costs meant that a research project could justbarely manage to have a single ascii document available for processing.for example, gerard saltonõs smart system, one of the leading textretrieval systems for many years (see saltonõs book, the smart automatic retrieval system, prenticehall, 1971), did most of its processing oncollections of a few hundred abstracts. the only collections of òfull documentsó were a collection of 80 extended abstracts, each a page or twolong, and a collection of under a thousand stories from time magazine,each less than a page in length. the biggest collection was 1400 abstractsin aeronautical engineering. with this data, salton was able to experimenton the effectiveness of retrieval methods using suffixing, thesauri, andsimple phrase finding. salton also laid down the standard methodologyfor evaluating retrieval systems, based on cyril cleverdonõs measures ofòrecalló (percentage of the relevant material that is retrieved in responseto a query) and òprecisionó (the percentage of the material retrieved thatis relevant). a system with perfect recall finds all relevant material, makingno errors of omission and leaving out nothing the user wanted. in contrast, a system with perfect precision finds only relevant material, makingno errors of commission and not bothering the user with stuff of nointerest. the smart system produced these measures for many retrievalexperiments and its methodology was widely used, making text retrievalone of the earliest areas of computer science with agreedon evaluationmethods. salton was not able to do anything with image retrieval at thetime; there were no such data available for him.another idea shaped by the amount of information available wasòrelevance feedback,ó the idea of identifying useful documents from afirst retrieval pass in order to improve the results of a later retrieval. withso few documents, high precision seemed like an unnecessary goal. it wassimply not possible to retrieve more material than somebody could lookat. thus, the research focused on high recall (also stimulated by the insistence by some users that they had to have every single relevant document). relevance feedback helped recall. by contrast, the use of phrasesearching to improve precision was tried but never got much attentionsimply because it did not have the scope to produce much improvementin the running systems.the basic problem is that we wish to search for concepts, and whatwe have in natural language are words and phrases. when our documents are few and short, the main problem is not to miss any, and theresearch at the time stressed algorithms that found related words viaassociations or improved recall with techniques like relevance feedback.then, of course, several other advancesñcomputer typesetting andword processing to generate material and cheap disks to hold itñled tomuch larger text collections. figure 5.1 shows the decline in the price ofcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.90computer science: reflectionsfigure 5.1decline in the price of disk space, 1950 to 2004.disk space since the first disks in the mid1950s, generally following thecostperformance trends of mooreõs law.cheaper storage led to larger and larger text collections online. nowthere are many terabytes of data on the web. these vastly larger volumesmean that precision has now become more important, since a commonproblem is to wade through vastly too many documents. not surprisingly, in the mid1980s efforts started on separating the multiple meanings of words like òbankó or òpineó and became the research area ofòsense disambiguation.ó2 with sense disambiguation, it is possible toimagine searching for only one meaning of an ambiguous word, thusavoiding many erroneous retrievals.largescale research on text processing took off with the availabilityof the trec (text retrieval evaluation conference) data. thanks to thenational institute of standards and technology, several hundred megabytes of text were provided (in each of several years) for research use.this stimulated more work on query analysis, text handling, searching2see michael lesk, 1986, òhow to tell a pine cone from an ice cream cone,ó proceedingssigdoc, pp. 2628.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information91algorithms, and related areas; see the series titled trec conference proceedings, edited by donna harmon of nist.document clustering appeared as an important way to shorten longsearch results. clustering enables a system to report not, say, 5000 documents but rather 10 groups of 500 documents each, and the user can thenexplore the group or groups that seem relevant. salton anticipated thefuture possibility of such algorithms, as did others.3 until we got largecollections, though, clustering did not find application in the documentretrieval world. now one routinely sees search engines using these techniques, and faster clustering algorithms have been developed.thus the algorithms explored switched from recall aids to precisionaids as the quantity of available data increased. manual thesauri, forexample, have dropped out of favor for retrieval, partly because of theircost but also because their goal is to increase recall, which is not todayõsproblem. in terms of finding the concepts hinted at by words and phrases,our goals now are to sharpen rather than broaden these concepts: thusdisambiguation and phrase matching, and not as much work on thesauriand term associations.again, multilingual searching started to matter, because multilingualcollections became available. multilingual research shows a more preciseexample of particular information resources driving research. the canadian government made its parliamentary proceedings (called hansard)available in both french and english, with paragraphbyparagraph translation. this data stimulated a number of projects looking at how to handlebilingual material, including work on automatic alignment of the paralleltexts, automatic linking of similar words in the two languages, and so on.4a similar effect was seen with the brown corpus of tagged englishtext, where the part of speech of each word (e.g., whether a word is anoun or a verb) was identified. this produced a few years of work onalgorithms that learned how to assign parts of speech to words in runningtext based on statistical techniques, such as the work by garside.53see, for example, n. jardine and c.j. van rijsbergen, 1971, òthe use of hierarchicalclustering in information retrieval,ó information storage and retrieval 7:217240.4see, for example, t.k. landauer and m.l. littman, 1990, òfully automatic crosslanguage document retrieval using latent semantic indexing,ó proceedings of the sixthannual conference of the uw centre for the new oxford english dictionary and text research,pp. 3138, university of waterloo centre for the new oed and text research, waterloo,ontario, october; or i. dagan and ken church, 1997, òtermight: coordinating humans andmachines in bilingual terminology acquisition,ó machine translation 12(1/2):89107.5roger garside, 1987, òthe claws wordtagging system,ó in r. garside, g. leech, andg. sampson (eds.), the computational analysis of english: a corpusbased approach, longman,london.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.92computer science: reflectionsone might see an analogy to various new fields of chemistry. therecognition that pesticides like ddt were environmental pollutants led toa new interest in biodegradability, and the freon propellants used inaerosol cans stimulated research in reactions in the upper atmosphere.new substances stimulated a need to study reactions that previously hadnot been a top priority for chemistry and chemical engineering.as storage became cheaper, image storage was now as practical astext storage had been a decade earlier. starting in the 1980s we saw theibm qbic project demonstrating that something could be done to retrieveimages directly, without having to index them by text words first.6 projectslike this were stimulated by the availability of òclip artó such as thecorel image disks. several different projects were driven by the easyaccess to images in this way, with technology moving on from color andtexture to more accurate shape processing. at berkeley, for example, theòblobworldó project made major improvements in shape detection andrecognition, as described in carson et al.7 these projects demonstratedthat retrieval could be done with images as well as with words, and thatproperties of images could be found that were usable as concepts forsearching.another new kind of data that became feasible to process was sound,in particular human speech. here it was the defense advanced researchprojects agency (darpa) that took the lead, providing the switchboard corpus of spoken english. again, the availability of a substantialfile of tagged information helped stimulate many research projects thatused this corpus and developed much of the technology that eventuallywent into the commercial speech recognition products we now have. aswith the trec contests, the competitions run by darpa based on itsspoken language data pushed the industry and the researchers to newadvances. national needs created a new technology; one is reminded ofthe development of synthetic rubber during world war ii or the advancesin catalysis needed to make explosives during world war i.yet another kind of new data was geocoded data, introducing a newset of conceptual ideas related to place. geographical data started showing up in machinereadable form during the 1980s, especially with therelease of the dual independent map encoding (dime) files after the 19806see, for example, wayne niblack, ron barber, william equitz, myron flickner, eduardoh. glasman, dragutin petkovic, peter yanker, christos faloutsos, and gabriel taubin, 1993,òthe qbic project: querying images by content, using color, texture, and shape,ó storageand retrieval for image and video databases (spie), pp. 173187.7c. carson, m. thomas, s. belongie, j.m. hellerstein, and j. malik, 1999, òblobworld: asystem for regionbased image indexing and retrieval,ó proceedings of the third annualconference on visual information systems, springerverlag, amsterdam, pp. 509516.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information93census and the topologically integrated geographic encoding and referencing (tiger) files from the 1990 census. the availability, free of charge,of a complete u.s. street map stimulated much research on systems todisplay maps, to give driving directions, and the like.8 when aerial photographs also became available, there was the triumph of microsoftõsòterraserver,ó which made it possible to look at a wide swath of theworld from the sky along with correlated street and topographic maps.9more recently, in the 1990s, we have started to look at video searchand retrieval. after all, if a cdrom contains about 300,000 times asmany bytes per pound as a deck of punched cards, and a digitized videohas about 500,000 times as many bytes per second as the ascii script itcomes from, we should be about where we were in the 1960s with videotoday. and indeed there are a few projects, most notably the informediaproject at carnegie mellon university, that experiment with video signals;they do not yet have ways of searching enormous collections, but they aredeveloping algorithms that exploit whatever they can find in the video:scene breaks, closedcaptioning, and so on.again, there is the problem of deducing concepts from a new kind ofinformation. we started with the problem of words in one language needing to be combined when synonymous, picked apart when ambiguous,and moved on to detecting synonyms across multiple languages and thento concepts depicted in pictures and sounds. now we see research such asthat by jezekiel benarie associating words like òrunó or òhopó withvideo images of people doing those actions. in the same way we get againnew chemistry when molecules like òbuckyballsó are created and stimulate new theoretical and reaction studies.defining concepts for search can be extremely difficult. for example,despite our abilities to parse and define every item in a computer language, we have made no progress on retrieval of software; people lookingfor search or sort routines depend on metadata or comments. some areasseem more flexible than others: text and naturalistic photograph processing software tends to be very general, while software to handle caddiagrams and maps tends to be more specific. algorithms are sometimesportable; both speech processing and image processing need fouriertransforms, but the literature is less connected than one might like (partly8an early publication was r. elliott and m. lesk, 1982, òroute finding in street maps bycomputers and people,ó proceedings of the aiii82 national conference on artificial intelligence, pittsburgh, pa., august, pp. 258261.9t. barclay, j. gray, and d. slutz, 2000, òmicrosoft terraserver: a spatial data warehouse,ó proceedings of acm sigmod, association for computing machinery, new york,pp. 307318.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.94computer science: reflectionsbecause of the difference between onedimensional and twodimensionaltransforms).there are many other examples of interesting computer science researchstimulated by the availability of particular kinds of information. work onstring matching today is often driven by the need to align sequences ineither protein or dna data banks. work on image analysis is heavilyinfluenced by the need to deal with medical radiographs. and there aremany other interesting projects specifically linked to an individual datasource. among examples:¥the british library scanning of the original manuscript of beowulfin collaboration with the university of kentucky, working on imageenhancement until the result of the scanning is better than reading theoriginal;¥the perseus project, demonstrating the educational applicationspossible because of the earlier thesaurus linguae graecae project, whichdigitized all the classical greek authors;¥the work in astronomical analysis stimulated by the sloan digitalsky survey;¥the creation of the field of òforensic paleontologyó at the university of texas as a result of doing mri scans of fossil bones;¥and, of course, the enormous amount of work on search enginesstimulated by the web.when one of these fields takes off, and we find wide usage of someonline resource, it benefits society. every university library gained readersas their catalogs went online and became accessible to students in theirdorm rooms. third world researchers can now access large amounts oftechnical content their libraries could rarely acquire in the past.in computer science, and in chemistry, there is a tension between thealgorithm/reaction and the data/substance. for example, should one lookup an answer or compute it? once upon a time logarithms were lookedup in tables; today we also compute them on demand. melting points andother physical properties of chemical substances are looked up in tables;perhaps with enough quantum mechanical calculation we could predictthem, but itõs impractical for most materials. predicting tomorrowõsweather might seem a difficult choice. one approach is to measure thecurrent conditions, take some equations that model the atmosphere, andcalculate forward a day. another is to measure the current conditions,look in a big database for the previous day most similar to today, and thentake the day after that one as the best prediction for tomorrow. however,so far the meteorologists feel that calculation is better. another complicated example is chess: given the time pressure of chess tournamentscomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information95against speed and storage available in computers, chess programs do theopening and the endgame by looking in tables of old data and calculatefor the middle game.to conclude, a recipe for stimulating advances in computer science isto make some data available and let people experiment with it. with theincredibly cheap disks and scanners available today, this should be easierthan ever. unfortunately, what we gain with technology we are losing tolaw and economics. many large databases are protected by copyright; fewmotion pictures, for example, are old enough to have gone out of copyright. content owners generally refuse to grant permission for wide useof their material, whether out of greed or fear: they may have figured outhow to get rich off their files of information or they may be afraid thatsomebody else might have. similarly it is hard to get permission to digitizeincopyright books, no matter how long they have been out of print. jimgray once said to me, òmay all your problems be technical.ó in the 1960si was paying people to key in aeronautical abstracts. it never occurred tous that we should be asking permission of the journals involved (i thinkwhat we did would qualify as fair use, but we didnõt even think about it).today i could scan such things much more easily, but i would not be ableto get permission. am i better off or worse off?there are now some 22 million chemical substances in the chemicalabstracts service registry and 7 million reactions. new substances continue to intrigue chemists and cause research on new reactions, with ofcourse enormous interest in biochemistry both for medicine and agriculture. similarly, we keep adding data to the web, and new kinds of information (photographs of dolphins, biological flora, and countless otherthings) can push computer scientists to new algorithms. in both cases,synthesis of specific instances into concepts is a crucial problem. as wesee more and more kinds of data, we learn more about how to extractmeaning from it, and how to present it, and we develop a need for newalgorithms to implement this knowledge. as the data gets bigger, welearn more about optimization. as it gets more complex, we learn moreabout representation. and as it gets more useful, we learn more aboutvisualization and interfaces, and we provide better service to society.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.96computer science: reflectionshistory and the fundamentals of computer scienceedward l. ayers, university of virginiawe might begin with a thought experiment: what is history? manypeople, iõve discovered, think of it as books and the things in books.thatõs certainly the explicit form in which we usually confront history.others, thinking less literally, might think of history as stories about thepast; that would open us to oral history, family lore, movies, novels, andthe other forms in which we get most of our history.all these images are wrong, of course, in the same way that images ofatoms as little solar systems are wrong, or pictures of evolution as profilesof ever taller and more upright apes and people are wrong. they are allmodels, radically simplified, that allow us to think about such things inthe exceedingly small amounts of time that we allot to these topics.the same is true for history, which is easiest to envision as technological progress, say, or westward expansion, of the emergence of freedomñor of increasing alienation, exploitation of the environment, or the growthof intrusive government.those of us who think about specific aspects of society or nature for aliving, of course, are never satisfied with the stories that suit the purposesof everyone else so well.we are troubled by all the things that donõt fit, all the anomalies, variance, and loose ends. we demand more complex measurement, description, and fewer smoothing metaphors and lowest common denominators.thus, to scientists, atoms appear as clouds of probability; evolutionappears as a branching, labyrinthine bush in which some branches die outand others diversify. it can certainly be argued that past human experience is as complex as anything in nature and likely much more so, if bycomplexity we mean numbers of components, variability of possibilities,and unpredictability of outcomes.yet our means of conveying that complexity remain distinctly analog:the story, the metaphor, the generalization. stories can be wonderfullycomplex, of course, but they are complex in specific ways: of implication,suggestion, evocation. thatõs what people love and what they remember.but maybe there is a different way of thinking about the past: asinformation. in fact, information is all we have. studying the past is likestudying scientific processes for which you have the data but cannot runthe experiment again, in which there is no control, and in which you cannever see the actual process you are describing and analyzing. all wehave is information in various forms: words in great abundance, billionsof numbers, millions of images, some sounds and buildings, artifacts.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information97the historianõs goal, it seems to me, should be to account for as muchof the complexity embedded in that information as we can. that, itappears, is what scientists do, and it has served them well.and how has science accounted for everincreasing amounts of complexity in the information they use? through ever more sophisticatedinstruments. the connection between computer science and history couldbe analogous to that between telescopes and stars, microscopes and cells.we could be on the cusp of a new understanding of the patterns of complexity in human behavior of the past.the problem may be that there is too much complexity in that past, ortoo much static, or too much silence. in the sciences, weõve learned how tofilter, infer, use indirect evidence, and fill in the gaps, but we have a muchmore literal approach to the human past.we have turned to computer science for tasks of more elaboratedescription, classification, representation. the digital archive my colleaguesand i have built, the valley of the shadow project, permits the manipulation of millions of discrete pieces of evidence about two communities inthe era of the american civil war. it uses sorting mechanisms, hypertextual display, animation, and the like to allow people to handle theevidence of this part of the past for themselves. this isnõt cuttingedgecomputer science, of course, but itõs darned hard and deeply disconcerting to some, for it seems to abdicate responsibility, to undermine authority,to subvert narrative, to challenge story.now, weõre trying to take this work to the next stage, to analysis. wehave composed a journal article that employs an array of technologies,especially geographic information systems and statistical analysis in thecreation of the evidence. the article presents its argument, evidence, andhistoriographical context as a complex textual, tabular, and graphicalrepresentation. xml offers a powerful means to structure text and xsl aneven more powerful means to transform it and manipulate its presentation. the text is divided into sections called òstatements,ó each supportedwith òexplanation.ó each explanation, in turn, is supported by evidenceand connected to relevant historiography.linkages, forward and backward, between evidence and narrativeare central. the historiography can be automatically sorted by author,date, or title; the evidence can be arranged by date, topic, or type. bothevidence and historiographical entries are linked to the places in the analysis where they are invoked. the article is meant to be used online, but itcan be printed in a fixed format with all the limitations and advantagesofprint.so, what are the implications of thinking of the past in the hardheaded sense of admitting that all we really have of the past is information? one implication might be great humility, since all we have for mostcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.98computer science: reflectionsof the past are the fossils of former human experience, words frozen in inkand images frozen in line and color. another implication might be hubris:if we suddenly have powerful new instruments, might we be on thethreshold of a revolution in our understanding of the past? weõve beenthere before.a connection between history and social science was tried before,during the first days of accessible computers. historians taught themselves statistical methods and even programming languages so that theycould adopt the techniques, models, and insights of sociology and political science. in the 1950s and 1960s the creators of the new political historycalled on historians to emulate the precision, explicitness, replicability,and inclusivity of the quantitative social sciences. for two decades thatquantitative history flourished, promising to revolutionize the field. andto a considerable extent it did: it changed our ideas of social mobility,political identification, family formation, patterns of crime, economicgrowth, and the consequences of ethnic identity. it explicitly linked thepast to the present and held out a history of obvious and immediate use.but that quantitative social science history collapsed suddenly, thevictim of its own inflated claims, limited method and machinery, andchanging academic fashion. by the mid1980s, history, along with manyof the humanities and social sciences, had taken the linguistic turn. ratherthan software manuals and codebooks, graduate students carried booksof french philosophy and german literary interpretation. the social science of choice shifted from sociology to anthropology; texts replacedtables. a new generation defined itself in opposition to social scientificmethods just as energetically as an earlier generation had seen in thosemethods the best means of writing a truly democratic history. the firstcomputer revolution largely failed.the first effort at that history fell into decline in part because historianscould not abide the distance between their most deeply held beliefs andwhat the statistical machinery permitted, the abstraction it imposed. historyhas traditionally been built around contingency and particularity, but themost powerful tools of statistics are built on sampling and extrapolation,on generalization and tendency. older forms of social history talked aboutvague and sometimes dubious classifications in part because that waswhat the older technology of tabulation permitted us to see. it has becomeincreasingly clear across the social sciences that such flat ways of describing social life are inadequate; satisfying explanations must be dynamic,interactive, reflexive, and subtle, refusing to reify structures of social lifeor culture. the new technology permits a new crossfertilization.ironically, social science history faded just as computers became widelyavailable, just as new kinds of social science history became feasible. nolonger is there any need for whitecoated attendants at huge mainframescomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.data, representation, and information99and expensive proprietary software. rather than reducing people to rowsand columns, searchable databases now permit researchers to maintainthe identities of individuals in those databases and to represent entirepopulations rather than samples. moreover, the record can now includethings social science history could only imagine before the web: completely indexed newspapers, with the original readable on the screen;completely searchable letters and diaries by the thousands; and interactivemaps with all property holders identified and linked to other records.visualization of patterns in the data, moreover, far outstrips the possibilities of numerical calculation alone. manipulable histograms, maps, andtime lines promise a social history that is simultaneously sophisticatedand accessible. we have what earlier generations of social science historiansdreamed of: a fast and widely accessible network linked to cheap andpowerful computers running common software with wellestablishedstandards for the handling of numbers, texts, and images. new possibilities of collaboration and cumulative research beckon. perhaps the time isright to reclaim a worthy vision of a disciplined and explicit social scientific history that we abandoned too soon.what does this have to do with computer science? everything, itseems to me. if you want hard problems, historians have them. andwhatõs the hardest problem of all right now? the capture of the veryinformation that is history. can computer science imagine ways to capture historical information more efficiently? can it offer ways to workwith the spotty, broken, dirty, contradictory, nonstandardized information we work with?the second hard problem is the integration of this disparate evidencein time and space, offering new precision, clarity, and verifiability, as wellas opening new questions and new ways of answering them.if we can think of these ways, then we face virtually limitless possibilities. is there a more fundamental challenge or opportunity for computer science than helping us to figure out human society over humantime?computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.1016achieving intelligenceone of the great aspirations of computer science has been to understand and emulate capabilities that we recognize as expressive ofintelligence in humans. research has addressed tasks rangingfrom our sensory interactions with the world (vision, speech, locomotion)to the cognitive (analysis, game playing, problem solving). this quest tounderstand human intelligence in all its forms also stimulates researchwhose results propagate back into the rest of computer scienceñforexample, lists, search, and machine learning.going beyond simply retrieving information, machine learning drawsinferences from available data. mitchell describes the application ofclassifying text documents automatically and shows how this researchexemplifies the experimentanalyzegeneralize style of experimentalresearch.one of the exemplars of intelligent behavior is naturallanguage processing in all its formsñincluding speech recognition and generation,naturallanguage understanding, and machine translation. lee describeshow, in the area of statistical naturallanguage understanding, the commitment to an empirical computational perspective draws meaning fromdata and brings interdisciplinary contributions together.games have frequently provided settings for exploring new computing techniques. they make excellent testbeds because they areusually circumscribed in scope, have welldefined rules, employ humanperformance standards for comparison, and are not on the critical path tocomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.102computer science: reflectionsmakeorbreak projects. in addition, they are engaging and appealing,which makes it easy to recruit early users. koller and biermann examinethe history of computer science endeavors in chess and checkers, showinghow success depends both on òsmartsó (improved representations andalgorithms) and sheer computer power.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence103the experimentanalyzegeneralize loop incomputer science research: a case studytom mitchell, carnegie mellon universitymuch research in computer science involves an iterative process ofattacking some new application problem, developing a computer program to solve that specific problem, and then stepping back to learn ageneral principle or algorithm, along with a precise description of thegeneral class of problems to which it can be applied. this experimentanalyzegeneralize loop lies at the heart of experimental computer scienceresearch, and it is largely responsible for the continuous growth overseveral decades in our knowledge of robust, effective computer algorithms.here we illustrate the approach with a case study involving machinelearning algorithms for automatically classifying text. in particular, wesee how attempts to train a text classifier for classifying web pages led toa fundamental insight into a new class of learning algorithms.automatically classifying text documents such as emails, web pages,and online memos is a problem of obvious importance. it would clearlybe useful for computers to automatically classify emails into categoriessuch as òspam,ó òmeeting invitations,ó and so on, or to automaticallyclassify web pages into categories such as òpersonal home page,ó òproduct announcement,ó and others. computer scientists have studied theproblem of automatic text classification for a number of years, over timedeveloping increasingly effective algorithms that achieve higher classification accuracy and accommodate a broader range of text documents.machine learning for text classificationone approach to developing text classification software involvesmachine learning. in most software development, programmers writedetailed algorithms as linebyline recipes to be executed by the computer. in contrast, machine learning involves training the software byinstead showing it examples of inputs and outputs of the desired program.the computer then learns (estimates) the general inputoutput functionfrom the examples provided. for instance, to train a program to classifyweb pages into categories such as òpersonal home pageó or òproductdescription,ó we would present a set of training examples consisting ofindividual web pages (example inputs) and the correct classification foreach (example outputs). the machinelearning system uses these trainingexamples to produce a general program that achieves high accuracy onthese examples, and presumably on novel future inputs as well. while itcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.104computer science: reflectionsis unrealistic to expect perfect classification accuracy over novel futureexamples, in fact these accuracies for many text classification problemsare well above 90 percent, and are often higher than those that can beachieved by manual programming (because humans generally donõt knowthe general classification rule either!).what kind of machinelearning approach can analyze such trainingexamples to learn the correct classification rule? one popular machinelearning approach is a bayesian classifier employing a bagofwordsrepresentation for web pages. the bagofwords representation, depictedin figure 6.1, describes each text document by the frequency of occurrenceof each word in the document. although this representation removesinformation such as the exact sequence in which words occur, it has beenfound to be highly effective for document classification.once the trainingexample web pages are described in terms of theirword frequencies, the classifier can calculate from these training examplesthe average frequency for each word, within each different class of webpages. a new document can then be automatically classified by firstobserving its own word frequencies and then assigning it to the classwhose average frequencies are most similar to its own. the naive bayesclassifier uses this general approach. more precisely, it uses the trainingdata to estimate the probability p(wi|cj) that a word drawn at randomfrom a random document from class cj will be the word wi (e.g.,p(òphoneó|home page) is the probability that a random word found on arandom home page will be the word òphoneó). thousands of such probability terms are estimated during training (i.e., if english containsapproximately 105 words, and if we consider only two distinct classes ofweb pages, then the program will estimate such 2  105 probabilities).these learned probabilities, along with the estimated class priors, arethen used to classify a new document, d, by calculating the probabilityp(wi|d) that d belongs to the class cj based on its observed words.figure6.2 summarizes the training and classification procedures for thenaive bayes classifier (this type of bayes classifier is called ònałveó becauseit makes the assumption that words occur independently within documents of each class, and òbayesianó because it uses bayes rules alongwith the learned probability terms to classify new documents).improving accuracy by learning from unlabeled examplesalthough the naive bayes classifier can often achieve accuracies of90percent or higher when trained to discriminate classes of web pagessuch as òpersonal home pageó versus òacademic course web page,ó itoften requires many hundreds or thousands of training examples to reachthis accuracy. thus, the primary cost in developing the classifier involvescomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.105figure 6.1in the bagofwords approach, text documents are described solely by the frequencies of the words they contain.here, the web page on the left is represented by the frequencies of each word it contains (shown by the list of words andfrequencies on the right).computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.106computer science: reflectionsfigure 6.2nałve bayes learner based on the bagofwords representation fromfigure 6.1.handlabeling the training examples. this leads to the interesting question:can we devise learning methods that achieve higher accuracy when givenunlabeled examples (e.g., additional web pages without their classification) in addition to a set of labeled examples? at first it may seem that theanswer must be no, because providing unlabeled examples amounts toproviding an example input to the program without providing the desiredoutput. surprisingly, if the text documents we wish to classify are webpages, then we shall see that the answer is yes, due to a particular characteristic of web pages.the characteristic that makes it possible to benefit from unlabeleddata when learning to classify web pages is illustrated in figure 6.3. notefirst that web pages typically appear on the web along with hyperlinksthat point to them. we can therefore think of each example web page asbeing described by two sets of features: the words occurring within thepage (which we will call x1) and the words occurring on hyperlinks thatpoint to this page (which we will call x2). furthermore, in many cases thex1 features alone are sufficient to classify the page without x2 (i.e., evencomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence107figure 6.3redundantly sufficient features for classifying web pages. note thatthe class of this web page (òfaculty home pageó) can be inferred from either (a)the words on the web page or (b) the words on the hyperlinks that point to thepage. in such cases we say that these two feature sets are òredundantly sufficientó to classify the example.when ignoring the hyperlinks, it is obvious that the web page of figure6.3belongs to the class òfaculty home pageó). similarly, the x2 features takenalone may also be sufficient to classify the page (i.e., if the hyperlinkpointing to the page contains the words òprofessor faloutsos,ó the page ismost probably a òfaculty home pageó). in short, we say in this case thatthe features describing the example web pages are redundantly sufficientcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.108computer science: reflectionsto perform the classification. furthermore, notice that the hyperlink wordstend to be quite independent of the exact words on the page, given theclass of the page (in part because the hyperlinks and the pages to whichthey point are often written by different authors).this characteristic of web pages suggests the following training procedure for using a combination of labeled and unlabeled examples: first,we use the labeled training examples to train two different naive bayesclassifiers. one of these classifiers uses only the x1 features; the other usesonly the x2 features. the first classifier is then applied to the unlabeledtraining examples, and it selects the example d that it is most confident inclassifying (i.e., the example that yields the highest probability p(cj|d)).itis then allowed to assign that label to this previously unlabeled example,and the second classifier is now retrained using this new labeled examplealong with the original labeled examples. the identical process can beexecuted reversing the roles of the two classifiers, using each to train theother. this process is called cotraining. furthermore, the process can berepeated many times, each time assigning a few mostconfident labels tothe initially unlabeled examples.does this learning algorithm work in practice? it does. blum andmitchell,1 for example, describe experiments using cotraining to classifyweb pages into the categories òacademic course home pageó or not. starting with just 12 labeled web pages, they found that cotraining with anadditional 788 unlabeled web pages reduced the classification error rateby a factor of two, from 11 percent to 5 percent.analyze the specific solution to generalize the principlegiven the empirical success of this cotraining algorithm for classifying web pages, it is natural to ask: what is the general class of machinelearning problems for which unlabeled data can be proven to improveclassification accuracy? intuitively, the reason cotraining works whenlearning to classify web pages is that (1) the examples can be described bytwo different sets of features (hyperlink words, page words) that areredundantly sufficient, and (2) the two features are distributed somewhatindependently, so that an example with an easytoclassify hyperlink islikely to point to a web page of average classification difficulty. we can1a. blum and t. mitchell, 1998, òcombining labeled and unlabeled data with cotraining,ó proceedings of the 1998 conference on computational learning theory, associationfor computing machinery, new york.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence109therefore train two classifiers, using each to produce new, highconfidencelabels for training the other.blum and mitchell make this intuitive characterization more preciseand formal.2 they prove that for the class of learning problems theydefine, one can learn successfully from a small set of labeled examplesand a larger volume of unlabeled examples. the essence of their characterization is as follows. in general, one can view the problem of learning aclassifier as the problem of estimating some unknown classifier functionf:x y given only a set of labeled inputoutput examples {xi,f(xi)} anda set of unlabeled examples {xi} with unknown output. the cotrainingproblem setting can then be defined as a special case of learning aclassifier, where (1) the input instances x can be described as x1 x2 (i.e.,x1 = hyperlink words, x2 = web page words), and where (2) one cancompute f based on either x1 or x2 (formally, there exist functions g1 and g2such that f(x) = g1(x1) = g2(x2) for any x = x1,x2. they then go on tocharacterize the impact of unlabeled data on learning behavior in severalsituations. for example, they show that if one makes the additional assumption that x1 and x2 are conditionally independent given the class y, thenany function that is learnable from noisy labeled data can also be learnedfrom a small set of labeled data that produces betterthanrandom accuracy, plus unlabeled data.summarythis case study shows how the attempt to find more accurate learningalgorithms for web page classification motivated the development of aspecialized algorithm, which in turn motivated a formal analysis to understand the precise class of problems for which the learning algorithm couldbe proven to succeed. in fact, the insights provided by this analysis have,in turn, led to the development of more accurate learning algorithms forthis class of problems (e.g., collins and singer,3 and muslea et al.4). further2a. blum and t. mitchell, 1998, òcombining labeled and unlabeled data with cotraining,ó proceedings of the 1998 conference on computational learning theory, associationfor computing machinery, new york.3m. collins and y. singer, 1999, òunsupervised models for named entity classification,óproceedings of the joint sigdat conference on empirical methods in natural language processingand very large corpora, association for computational linguistics, east stroudsburg, pa.,pp. 100110.4i. muslea, s. minton, and c. knoblock, 2000, òselective sampling with redundant views,óproceedings of the seventeenth national conference on artificial intelligence, aaai press, menlopark, calif., pp. 621626.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.110computer science: reflectionsmore, it made it apparent that the cotraining algorithm published byblum and mitchell was in fact similar to an earlier algorithm by yarowski5for learning to disambiguate word senses (e.g., learning whether òbankórefers to a financial institution, or a place near a river). in yarowskiõs case,the features x1 and x2 are the linguistic context in which the word occurs,and the document in which it occurs.this case study illustrates the useful interplay between experimentand theory in advancing computer science. the advance in our understanding of the science of computation can be described in this case usingstatements of the form òfor problems that exhibit structure s, algorithm awill exhibit property p.ó in some cases we have only experimental evidence to support conjectures of this form, in some cases analytical proofs,but in many cases a blend of the two, and a family of related statementsrather than a single one.of course the interplay between experiment and analysis is generallymessy, and not quite as clean as post facto reports would like to make itappear! in many cases the formal models motivated by specific applications do not fully capture the complexities of the application. in our owncase study, for instance, the assumption that hyperlink words can be usedto classify the web page they point to is not quite validñsome hyperlinkwords such as òclick hereó provide no information at all about the pagethey point to! nevertheless, theoretical characterizations of the problemare useful even if incomplete and approximate, provided they capture asignificant problem structure that impacts on the design and performanceof algorithms. and the experimentanalyzegeneralize cycle of researchoften leads to a second and third generation of experiments and of theoretical models that better characterize the application problems, just ascurrent theoretical research on using unlabeled data is now consideringproblem formalizations that relax the assumption violated by the òclickhereó hyperlinks.5d. yarowsky, 1995, òunsupervised word sense disambiguation rivaling supervisedmethods,ó proceedings of the 33rd annual meeting of the association for computationallinguistics, association for computational linguistics, east stroudsburg, pa., pp. 189196.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence111òiõm sorry dave, iõm afraid i canõt do tható:linguistics, statistics, and naturallanguageprocessing circa 2001lillian lee, cornell universityits the year 2000, but where are the flying cars? i was promised flying cars.ñavery brooks, ibm commercialaccording to many popculture visions of the future, technology willeventually produce the machine that can speak to us. examples rangefrom the false maria in fritz langõs 1926 film metropolis to knight riderõskitt (a talking car) to star warsõ c3po (said to have been modeled onthe false maria). and, of course, there is the hal 9000 computer from2001: a space odyssey; in one of the filmõs most famous scenes, the astronaut dave asks hal to open a pod bay door on the spacecraft, to whichhal responds, òiõm sorry dave, iõm afraid i canõt do that.ónaturallanguage processing, or nlp, is the field of computer sciencedevoted to creating such machinesñthat is, enabling computers to usehuman languages both as input and as output. the area is quite broad,encompassing problems ranging from simultaneous multilanguagetranslation to advanced search engine development to the design ofcomputer interfaces capable of combining speech, diagrams, and othermodalities simultaneously. a natural consequence of this wide range ofinquiry is the integration of ideas from computer science with work frommany other fields, including linguistics, which provides models of language;psychology, which provides models of cognitive processes; informationtheory, which provides models of communication; and mathematics andstatistics, which provide tools for analyzing and acquiring such models.the interaction of these ideas together with advances in machinelearning (see mitchell in this chapter) has resulted in concerted researchactivity in statistical naturallanguage processing: making computerslanguageenabled by having them acquire linguistic information directlyfrom samples of language itself. in this essay, we describe the history ofstatistical nlp; the twists and turns of the story serve to highlight thesometimes complex interplay between computer science and other fields.although currently a major focus of research, the datadriven, computational approach to language processing was for some time held indeep disregard because it directly conflicts with another commonly heldviewpoint: human language is so complex that language samples aloneseemingly cannot yield enough information to understand it. indeed, it isoften said that nlp is òaicompleteó (a pun on npcompleteness; seecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.112computer science: reflectionskleinberg and papadimitriou in chapter 2), meaning that the most difficultproblems in artificial intelligence manifest themselves in human languagephenomena. this belief in language use as the touchstone of intelligentbehavior dates back at least to the 1950 proposal of the turing test6 as away to gauge whether machine intelligence has been achieved; as turingwrote, òthe question and answer method seems to be suitable for introducing almost any one of the fields of human endeavor that we wish toinclude.óthe reader might be somewhat surprised to hear that language understanding is so hard. after all, human children get the hang of it in a fewyears, word processing software now corrects (some of) our grammaticalerrors, and tv ads show us phones capable of effortless translation. onemight therefore be led to believe that hal is just around the corner.such is not the case, however. in order to appreciate this point, wetemporarily divert from describing statistical nlpõs historyñwhichtouches upon hamilton versus madison, the sleeping habits of colorlessgreen ideas, and what happens when one fires a linguistñto examine afew examples illustrating why understanding human language is such adifficult problem.ambiguity and language analysisat last, a computer that understands you like your mother.ñ1985 mcdonnelldouglas adthe snippet quoted above indicates the early confidence at least onecompany had in the feasibility of getting computers to understand humanlanguage. but in fact, that very sentence is illustrative of the host of difficulties that arise in trying to analyze human utterances, and so, ironically,it is quite unlikely that the system being promoted would have been up tothe task. a momentõs reflection reveals that the sentence admits at leastthree different interpretations:1.the computer understands you as well as your mother understands you.2.the computer understands that you like your mother.3.the computer understands you as well as it understands yourmother.6roughly speaking, a computer will have passed the turing test if it can engage inconversations indistinguishable from those of a human.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence113that is, the sentence is ambiguous; and yet we humans seem toinstantaneously rule out all the alternatives except the first (and presumably the intended) one. we do so based on a great deal of backgroundknowledge, including understanding what advertisements typically tryto convince us of. how are we to get such information into a computer?a number of other types of ambiguity are also lurking here. forexample, consider the speech recognition problem: how can we distinguish between this utterance, when spoken, and ò. . . a computer thatunderstands your lie cured motheró? we also have a word sense ambiguityproblem: how do we know that here òmotheró means òa female parent,órather than the oxford english dictionaryapproved alternative of òa caskor vat used in vinegarmakingó? again, it is our broad knowledge aboutthe world and the context of the remark that allows us humans to makethese decisions easily.now, one might be tempted to think that all these ambiguities arisebecause our example sentence is highly unusual (although the ad writersprobably did not set out to craft a strange sentence). or, one might arguethat these ambiguities are somehow artificial because the alternative interpretations are so unrealistic that an nlp system could easily filter themout. but ambiguities crop up in many situations. for example, in òcopythe local patient files to diskó (which seems like a perfectly plausiblecommand to issue to a computer), is it the patients or the files that arelocal?7 again, we need to know the specifics of the situation in order todecide. and in multilingual settings, extra ambiguities may arise. here isa sequence of seven japanese kanji characters:since japanese doesnõt have spaces between words, one is faced withthe initial task of deciding what the component words are. in particular,this character sequence corresponds to at least two possible wordsequences, òpresident, both, business, generalmanageró (= òa presidentas well as a general manager of businessó) and òpresident, subsidiarybusiness, tsutomu (a name), generalmanageró (= ?). it requires a fair bitof linguistic information to choose the correct alternative.87or, perhaps, the files themselves are patient? but our knowledge about the world rulesthis possibility out.8to take an analogous example in english, consider the nonworddelimited sequence ofletters òtheyouthevent.ó this corresponds to the word sequences òthe youth event,ó òtheyout he vent,ó and òthe you the vent.ócomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.114computer science: reflectionsto sum up, we see that the nlp task is highly daunting, for to resolvethe many ambiguities that arise in trying to analyze even a single sentencerequires deep knowledge not just about language but also about the world.and so when hal says, òiõm afraid i canõt do that,ó nlp researchers aretempted to respond, òiõm afraid you might be right.ófirth things firstbut before we assume that the only viable approach to nlp is a massive knowledgeengineering project, let us go back to the early approachesto the problem. in the 1940s and 1950s, one prominent trend in linguisticswas explicitly empirical and in particular distributional, as exemplifiedby the work of zellig harris (who started the first linguistics program inthe united states). the idea was that correlations (cooccurrences) foundin language data are important sources of information, or, as the influential linguist j.r. firth declared in 1957, òyou shall know a word by thecompany it keeps.ósuch notions accord quite happily with ideas put forth by claudeshannon in his landmark 1948 paper establishing the field of informationtheory; speaking from an engineering perspective, he identified the probability of a messageõs being chosen from among several alternatives, ratherthan the messageõs actual content, as its critical characteristic. influencedby this work, warren weaver in 1949 proposed treating the problem oftranslating between languages as an application of cryptography (seesudan in chapter 7), with one language viewed as an encrypted form ofanother. and alan turingõs work on cracking german codes duringworld war ii led to the development of the goodturing formula, animportant tool for computing certain statistical properties of language.in yet a third area, 1941 saw the statisticians frederick mosteller andfrederick williams address the question of whether it was alexanderhamilton or james madison who wrote the various pseudonymous federalist papers. unlike previous attempts, which were based on historicaldata and arguments, mosteller and williams used the patterns of wordoccurrences in the texts as evidence. this work led up to the famedmosteller and wallace statistical study that many consider to have settledthe authorship of the disputed papers.thus, we see arising independently from a variety of fields the ideathat language can be viewed from a datadriven, empirical perspectiveñand a datadriven perspective leads naturally to a computationalperspective.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence115a òcó changehowever, datadriven approaches fell out of favor in the late 1950s.one of the commonly cited factors is a 1957 argument by linguist (andstudent of harris) noam chomsky, who believed that language behaviorshould be analyzed at a much deeper level than its surface statistics. heclaimed,it is fair to assume that neither sentence (1) [colorless green ideassleep furiously] nor (2) [furiously sleep ideas green colorless] . . . hasever occurred. . . . hence, in any [computed] statistical model . . . thesesentences will be ruled out on identical grounds as equally òremoteófrom english. yet (1), though nonsensical, is grammatical, while (2) isnot.9that is, we humans know that sentence (1), which at least obeys(some) rules of grammar, is indeed more probable than (2), which is justword salad; but (the claim goes), since both sentences are so rare, they willhave identical statisticsñi.e., a frequency of zeroñin any sample ofenglish. chomskyõs criticism is essentially that datadriven approacheswill always suffer from a lack of data, and hence are doomed to failure.this observation turned out to be remarkably prescient: even now,when billions of words of text are available online, perfectly reasonablephrases are not present. thus, the socalled sparse data problem continues to be a serious challenge for statistical nlp even today. and so, theeffect of chomskyõs claim, together with some negative results formachine learning and a general lack of computing power at the time, wasto cause researchers to turn away from empirical approaches and towardknowledgebased approaches where human experts encoded relevantinformation in computerusable form.this change in perspective led to several new lines of fundamental,interdisciplinary research. for example, chomskyõs work viewing language as a formal, mathematically describable object has had a lastingimpact on both linguistics and computer science; indeed, the chomskyhierarchy, a sequence of increasingly more powerful classes of grammars,is a staple of the undergraduate computer science curriculum. conversely,the highly influential work of, among others, kazimierz adjukiewicz,joachim lambek, david k. lewis, and richard montague adopted thelambda calculus, a fundamental concept in the study of programminglanguages, to model the semantics of natural languages.9interestingly, this claim has become so famous as to be selfnegating, as simple websearches on òcolorless green ideas sleep furiouslyó and its reversal will show.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.116computer science: reflectionsthe empiricists strike backby the 1980s, the tide had begun to shift once again, in part because ofthe work done by the speech recognition group at ibm. these researchers,influenced by ideas from information theory, explored the power ofprobabilistic models of language combined with access to much moresophisticated algorithmic and data resources than had previously beenavailable. in the realm of speech recognition, their ideas form the core ofthe design of modern systems; and given the recent successes of suchsoftwareñlargevocabulary continuousspeech recognition programs arenow available on the marketñit behooves us to examine how these systems work.given some acoustic signal, which we denote by the variable a, wecan think of the speech recognition problem as that of transcription:determining what sentence is most likely to have produced a. probabilitiesarise because of the everpresent problem of ambiguity: as mentionedabove, several word sequences, such as òyour lie cured motheró versusòyou like your mother,ó can give rise to similar spoken output. therefore,modern speech recognition systems incorporate information both aboutthe acoustic signal and the language behind the signal. more specifically,they rephrase the problem as determining which sentence s maximizesthe product p(a|s) p(s). the first term measures how likely the acousticsignal would be if s were actually the sentence being uttered (again, weuse probabilities because humans donõt pronounce words the same wayall the time). the second term measures the probability of the sentence sitself; for example, as chomsky noted, òcolorless green ideas sleep furiouslyó is intuitively more likely to be uttered than the reversal of thephrase. it is in computing this second term, p(s), where statistical nlptechniques come into play, since accurate estimation of these sentenceprobabilities requires developing probabilistic models of language. thesemodels are acquired by processing tens of millions of words or more. thisis by no means a simple procedure; even linguistically naive modelsrequire the use of sophisticated computational and statistical techniquesbecause of the sparse data problem foreseen by chomsky. but usingprobabilistic models, large datasets, and powerful learning algorithms(both for p(s) and p(a|s)) has led to our achieving the milestone ofcommercialgrade speech recognition products capable of handling continuous speech ranging over a large vocabulary.but let us return to our story. buoyed by the successes in speechrecognition in the 1970s and 1980s (substantial performance gains overknowledgebased systems were posted), researchers began applying datadriven approaches to many problems in naturallanguage processing, in aturnaround so extreme that it has been deemed a òrevolution.ó indeed,computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence117now empirical methods are used at all levels of language analysis. this isnot just due to increased resources: a succession of breakthroughs inmachinelearning algorithms has allowed us to leverage existing resourcesmuch more effectively. at the same time, evidence from psychologyshows that human learning may be more statistically based than previously thought; for instance, work by jenny saffran, richard aslin, andelissa newport reveals that 8monthold infants can learn to divide continuous speech into word segments based simply on the statistics ofsounds following one another. hence, it seems that the òrevolutionó ishere to stay.of course, we must not go overboard and mistakenly conclude thatthe successes of statistical nlp render linguistics irrelevant (rash statements to this effect have been made in the past, e.g., the notorious remark,òevery time i fire a linguist, my performance goes upó). the informationand insight that linguists, psychologists, and others have gathered aboutlanguage is invaluable in creating highperformance broaddomain language understanding systems; for instance, in the speech recognition setting described above, a better understanding of language structure canlead to better language models. moreover, truly interdisciplinary researchhas furthered our understanding of the human language faculty. oneimportant example of this is the development of the headdriven phrasestructure grammar (hpsg) formalismñthis is a way of analyzing naturallanguage utterances that truly marries deep linguistic information withcomputer science mechanisms, such as unification and recursive datatypes, for representing and propagating this information throughout theutteranceõs structure. in sum, although many challenges remain (forinstance, while the speechrecognition systems mentioned above are verygood at transcription, they are a long way from engaging in true languageunderstanding), computational techniques and datadriven methods arenow an integral part both of building systems capable of handlinglanguage in a domainindependent, flexible, and graceful way, and ofimproving our understanding of language itself.acknowledgmentsthanks to the members of and reviewers for the cstb fundamentalsof computer science study, and especially alan biermann, for their helpful feedback. also, thanks to alex acero, takako aikawa, mike bailey,regina barzilay, eric brill, chris brockett, claire cardie, joshua goodman,ed hovy, rebecca hwa, john lafferty, bob moore, greg morrisett,fernando pereira, hisami suzuki, and many others for stimulating discussions and very useful comments. rie kubota ando provided the japaneseexample. the use of the term òrevolutionó to describe the reascendancecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.118computer science: reflectionsof statistical methods comes from julia hirschbergõs 1998 invited addressto the american association for artificial intelligence. the mcdonnelldouglas ad and some of its analyses were presented in a lecture by stuartshieber. all errors are mine alone. this paper is based on work supportedin part by the national science foundation under itr/im grant iis0081334 and a sloan research fellowship. any opinions, findings, andconclusions or recommendations expressed above are those of the authorand do not necessarily reflect the views of the national science foundation or the sloan foundation.bibliographychomsky, noam, 1957, òsyntactic structures,ó number iv in janua linguarum. mouton,the hague, the netherlands.firth, john rupert, 1957, òa synopsis of linguistic theory 19301955,ó pp. 132 in the philological societyõs studies in linguistic analysis. blackwell, oxford. reprinted in selectedpapers of j.r. firth, f. palmer (ed.), longman, 1968.good, irving j., 1953, òthe population frequencies of species and the estimation of population parameters,ó biometrika 40(3,4):237264.harris, zellig, 1951, methods in structural linguistics, university of chicago press. reprintedby phoenix books in 1960 under the title structural linguistics.montague, richard, 1974, formal philosophy: selected papers of richard montague, richmondh. thomason (ed.), yale university press, new haven, conn.mosteller, frederick, and david l. wallace, 1984, applied bayesian and classical inference: thecase of the federalist papers, springerverlag. first edition published in 1964 under thetitle inference and disputed authorship: the federalist.pollard, carl, and ivan sag, 1994, headdriven phrase structure grammar, university ofchicago press and csli publications.saffran, jenny r., richard n. aslin, and elissa l. newport, 1996, òstatistical learning by8monthold infants,ó science 274(5294):19261928, december.shannon, claude e., 1948, òa mathematical theory of communication,ó bell system technical journal 27:379423 and 623656.turing, alan m., 1950, òcomputing machinery and intelligence,ó mind lix:433460.weaver, warren, 1949, òtranslation,ó memorandum. reprinted in w.n. locke and a.d.booth, eds., machine translation of languages: fourteen essays, mit press, cambridge,mass., 1955.for further readingcharniak, eugene, 1993, statistical language learning, mit press, cambridge, mass.jurafsky, daniel, and james h. martin, 2000, speech and language processing: an introductionto natural language processing, computational linguistics, and speech recognition, prenticehall. contributing writers: andrew keller, keith vander linden, and nigel ward.manning, christopher d., and hinrich schtze, 1999, foundations of statistical natural language processing, mit press, cambridge, mass.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence119computer game playing:beating humanity at its own gamedaphne koller, stanford university, andalan biermann, duke universitythe idea of getting a computer to play a complex game such ascheckers or chess has been present in computer science research from itsearliest days. the earliest effort even predated real computers. in 1769,baron wolfgang von kempelen displayed a chessplaying automatoncalled òturk.ó it drew a lot of attention, until people realized that thecabinet of the òmachineó concealed a human dwarf who was a chessexpert.the first real attempt to show how a computer could play a game wasby claude shannon, one of the fathers of information science. the basicidea is to define a game tree that tells us all of the possible move sequencesin the game. we can then ask, at each point in the tree, what a rational(selfish) player would do at that point. the answer comes from an analysis of the game tree beginning at the end of the tree (the termination of thegame). for example, assume that one player has black pieces and theother white pieces. we can mark each game termination point as bñblackwins, wñwhite wins, or dñdraw. then we can work our way from thetermination points of the game backwards: if the white player, at her turn,has a move leading to a position marked w, then she can take that moveand guarantee a win; in this case, this position is labeled with w. otherwise, if she has a move leading to a position marked d, then she can forcea draw, and the position is labeled with d. if all of her moves lead topositions marked b, then this position is a guaranteed win for black(assuming he plays optimally), and it is marked with b. similar propagation rules apply to positions controlled by the black player. thus, assuming perfect play by each player, we can completely understand the winpotential of every board position.this procedure is a great theoretical tool for thinking about a game.for example, it shows that, assuming both players play perfectly, we candetermine without playing a single move which of the players will win,or if the game has a forced draw. we simply carry out the above procedure and check whether the initial position is marked with a b, a w, or ad. unfortunately, for almost all realistic games, this procedure cannot becarried out because the size of the computation is too large. for example,the game tree for chess has approximately 10120 trajectories. as shannonpoints out, a computer that evaluated a million positions per secondwould require over 1095 years just to decide on its first move!computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.120computer science: reflectionsshannonõs idea was to explore only part of the game tree. at eachposition, the computer looks forward a certain number of steps, and thensomehow evaluates the positions reached even if they are not wins orlosses. for example, it can give a higher score to positions where it hasmore pieces. the computer then selects an optimal move relative to thatmyopic perspective about the game.turing, the inventor of the turing machine (see òcomputability andcomplexityó by kleinberg and papadimitriou in chapter 2) was the first totry and implement this idea. he wrote the first program capable of playinga full game of chess. but this program never ran on a computer; it washandsimulated against a very weak player, who managed to beat it anyway.the first attempt to get an actual computer to play a fullscale realgame was made by arthur samuel. in the mid1940s, samuel was a professor of electrical engineering at the university of illinois and becameactive in a project to design one of the first electronic computers. it wasthere that he conceived the idea of a checkers program that would beatthe world champion and demonstrate the power of electronic computers.apparently the program was not finished while he was at the universityof illinois, perhaps because the computer was not completed in time.in 1949, samuel joined ibmõs poughkeepsie laboratory and workedon ibmõs first stored program computer, the 701. samuelõs work oncheckers was not part of his job. he did the work in his spare time, without telling his supervisors. he got the computer operators (who had toauthorize and run all computer programs) to run his program by tellingthem it was testing the capabilities of the 701. this program was one ofthe first programs to run on ibmõs very first production computer. thecomputer spent its days being tested for accounting and scientific computation, and its nights playing checkers.interestingly, samuelõs justification for using the machine was actually valid. indeed, because his checkers program was one of the earliestexamples of nonnumerical computation, samuel greatly influenced theinstruction set of early ibm computers. the logical instructions of thesecomputers were put in at his instigation and were quickly adopted by allcomputer designers, because they are useful for most nonnumericalcomputation.samuelõs program was the first to play checkers at a reasonable level.it not only implemented shannonõs ideas; it also extended them substantially by introducing new and important algorithmic ideas. for example,shannonõs original proposal was to search all of the paths in the game upto a given depth. samuelõs program used a more sophisticated algorithm,called alphabeta pruning, that avoided exploring paths that it could provewere suboptimal. this algorithm allowed samuel to almost double thenumber of steps that it looked into the future before making the decision.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence121figure 6.4the alphabeta search procedure makes move a without examiningany of the paths marked +.alphabeta pruning allowed samuelõs program to prune largenumbers of paths in the tree that would never be taken in optimal play.consider the example in figure 6.4. here, the machine, playing white, hastwo moves, leading to positions a and b. the machine examines the pathsthat continue beyond position a and determines that the quality (according to some measure) of the best position it can reach from a is 10. it thenstarts evaluating position b, where the black player moves. the first moveby black leads to position b1; this position is evaluated, by exploring thepaths below it, to have a value of 5 (to white). now, consider any othermove by black, say to b2. either b2 is worth more than 5, or less than 5. ifit is worth more than 5, then it is a better position for white, and black,playing to win, would not take it. if it is worth less than 5, then black willtake it, and the result will only be worse for white. thus, no matter what,if the machine moves to position b, it can expect to end up with a positionvalued at 5 or less. as it can get 10 by moving to position a, it will nevermove to b. and it can determine this without ever exploring b2, b3, . . .,b12, or any of their descendants. this powerful idea coupled with othermechanisms enabled the search routines to run hundreds or even thousands of times faster than would have otherwise been possible.besides having sophisticated search capability, samuelõs program wasthe first program that learned by itself how to perform a task better. samuelhimself was a mediocre checkers player. but he was a very smart computer scientist. he came up with a method by which the computer couldcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.122computer science: reflectionsgradually improve its play as it played more and more games. recall thatthe computer has to evaluate each nonterminal position. one could, forexample, count the number of pieces of each type on the board and addthem up with appropriate weights. but is a king worth twice as much as aregular piece, or perhaps five times as much? samuel came up with thefollowing idea: letõs learn these weights by seeing which set of weightsleads to better play. thus, the computer would play using one set ofweights, and after seeing whether it won or lost the game, it would go andadjust the weights to get a more correct evaluation of the positions it hadencountered along the way. if it won, it would aim to increase its valuation for these positions, and if it lost, it would decrease them. although nosingle run is reliable in determining the value of a position (a bad positionmight, by chance, lead to a win), over time the valuations tended to getbetter. although the computer started out playing very poor checkers, itended up playing better than samuel.samuelõs program, even today, is a very impressive achievement,since the 701 had 10 kilobytes of main memory and used a magnetic tape(and not a disk) for storage. thus, it had less memory than one of themusical greeting cards that one can buy at the store for a few dollars.when it was about to be demonstrated, thomas j. watson, sr., thefounder and president of ibm, remarked that the demonstration wouldraise the price of ibm stock 15 points. it did. samuelõs program and othersgenerated a lot of excitement, and many people were led to believe thatcomputers would soon be better than any human player. indeed, in 1957,allen newell and eventual nobelprize winner herbert simon predictedthat in 10 years, a computer would be world chess champion. unfortunately, these predictions were premature. impressive as samuelõs achievement was, it was not a very good checkers player. although it was able tobeat samuel, most competent checkers players beat it without much strain.perhaps an early failure was to be expected in this new science, andpeople began to write much better programs. this effort was carried overto the more popular game of chess, and a number of individuals aroundthe country began bringing their programs to an annual chess tournament. it was fun to see computers play against each other, but the naggingfact remained: against humans these programs did not offer muchcompetition.these events led to much thought. what was going on here? theanswer seemed to be in the nature of the games of checkers and chess.researchers realized that the game trees were growing in size at an exponential rate as one looks further ahead in the sequences of possible moves.computer programs were wasting their time doing a uniform search ofevery possible move sequence while humans searched only selected paths.humans had knowledge of which paths were likely to yield interestingcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence123results, and they could look much deeper than machines. that is howhumans could win.it seemed obvious that the way to get machines to defeat humans wasto design the programs to play more like people: rather than using a lot ofcomputer cycles to explore all these irrelevant paths, the computer shouldspend more time thinking about which paths to explore. like people,machines should invest their resources only in selected paths and theyshould look very far down them. during the 1970s, the annual chesstournament featured many programs that followed this theory: build in amechanism that simulates a humanõs ability to search the interestingpaths, and create an ability in the machine to see very far into the futuremoves of the game. it was quite common for program designers to analyzea clever move by their program and brag about the long search that wasused to find it. only a fool who did not understand the modern lessons ofgame playing would try a uniform search algorithm.but the next surprise came from a chess program developed at northwestern university by david slate, larry atkin, and others. they createda uniform search algorithm despite the common wisdom and won theannual tournament. at first, people felt it was just luck, but when theresult was repeated a few times, they had to take notice. why did theuniform search program defy all reason and defeat selected path programs that look much deeper? the answer came from a new rationalization: a program doing uniform search to a fixed depth plays perfectly tothat depth. it never makes a mistake. yet a selective search program doesmake an occasional mistake within that depth, and when it does theuniform search program grabs the advantage. usually the advantage isenough to win the game. so uniform search programs dominated thefield, and variations on them still are being used.but the question remained: when would these programs finallybecome good enough to defeat humans? although the simonnewell prediction asserted 10 years from 1957, the reality was that it took 40 years. acomputer first became world champion in 1997, when ibmõs deep bluedefeated garry kasparov. in a match that involved six games, deep bluewon two games, lost one, and tied three. the ultimate goal of a 40yearquest had been achieved. but examine for a moment the computationalresources that were brought against the human player. deep blue ran onthe powerful rs/6000 machine running 32 processors in parallel and alsohad specialpurpose circuitry to generate chess moves. deep blue benefited greatly from harnessing mooreõs law (see hill in chapter 2). thisgiant machine examined 200 million chess moves per second! but a lot ofcomputer cycles were not the only factor in deep blueõs victory. the deepblue team had spent years building special chess knowledge into thesoftware and hardware of the system, and the systemõs play had beencomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.124computer science: reflectionscriticized regularly by chess masters during its development and improvedthrough many redesigns and modifications. only through this coordinated and longterm effort was the win possible.the years also proved the importance of the learning techniquesdeveloped by samuel in his checkers program. for example, the worldõsbest backgammon program, one which plays at worldchampion level, isa program developed by gerry tesauro (also at ibm). the main idea inthis program is not search: search does not work well in backgammon,where even a single play of the game involves thousands of differentpossible successor states. the main idea in this program is a learningalgorithm that learns to evaluate the quality of different positions. thisprogramõs evaluation was so good that it changed the evaluation of certainpositions, and therefore changed the way in which experts play the game!this story teaches us very much about game playing. but it alsoteaches us about the nature of computation and the nature of humanproblem solving. here are some of the major lessons:¥our intuitions and thoughtful insights into a process as complicated asplaying one of these board games are very unreliable. we learn things in information science by writing a program and observing its behavior. we maytheorize or estimate or consult our experts at length on some problems,but often we cannot find the answer without doing the experiment. onnumerous occasions, we found the experts were wrong and the world ofexperiment held an amazing lesson for us.¥the process of decision making in games is vastly more complex than weimagined. it was a combination of increased computer speed combinedwith decades of research on search and evaluation methods that eventually made it possible to defeat a great chess champion. it was not clear inthe early days how much of either of theseñmachine speed or quality ofsearchñwould be needed, and it is a profound lesson to observe howmuch of each was required. this provides us with plenty of warning thatwhen we examine other decision processes such as in economic, political,or medical diagnosis situations, we may find similar difficulties.¥it is not always the case that to achieve humanlevel performance, a computer should use the same techniques that a human does. the way a computerdoes computation is inherently different from the way the brain works,and different approaches might be suitable for these two òcomputingarchitectures.ó however, the performance of deep blue also shows thatquantity (a large amount of computer cycles) might occasionally lead toqualityña performance that might be at the level of a human, or indeedindistinguishable from that of a human. (in the match between deep blueand kasparov, several of kasparovõs advisors accused ibm of cheating byhaving human players feeding moves to deep blue.) but is this muchcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.achieving intelligence125bruteforce computation really necessary in order to play highqualitychess? we have the feeling that there must be a more efficient use ofmachines that will do the same job. but we have no idea what it is.perhaps, in order to solve that problem, we need to really understandhow intelligence works and how to implement it within a computer.¥rather than designing a program in its entirety, it may be better to let thecomputer learn for itself. there is a lot of knowledge that people might havebased on their experience but do not know how to make explicit. sometimes there is simply no substitute for handson experience, even for acomputer.¥the research in game playing over the years has had ramifications formany other areas of computing. for example, computer chess research atibm demonstrated computing technology that has also been used to attackproblems related to computations on environmental issues, modelingfinancial data, the design of automobiles, and the development of innovative drug therapies. samuelõs ideas on how to get programs to improvetheir performance by learning have provided a basis for tackling applications as diverse as learning to fly a helicopter, learning to search the web,or learning to plan operations in a large factory.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.1277building computing systems ofpractical scalecomputer science values not only fundamental knowledge but alsoworking systems that are widely useful. we deliver the benefits ofcomputer science research through these largescale, complexcomputing systems, and their design, development, and deployment constitute a fruitful area of research in itself. in doing so, we often raise newquestions about fundamental issues.started as an attempt to share scarce computing resources, the internethas become a ubiquitous global utility service, powering personal andcommercial transactions and creating domestic and international policychallenges. along the way it has provided a testbed for research thatranges from highspeed communication to social interactions to new business models. the remarkable success of the internet, including itsscalability and heterogeneity, results from inspired use of engineeringdesign principles. peterson and clark show how some basic principles ofgenerality, layers of abstraction, codified interfaces, and virtual resourcesled to a system architecture that has survived many orders of magnitudeof growth.the creators of the internet did not anticipatecouldnõt have anticipatedall of its consequences, including the emergence of the worldwide web as the principal publicaccess mechanism for the internet. theworld wide web emerged through the synergy of universal naming,browsers, widespread convenient internet access, the hyper text transfer protocol, a series of markup languages, and the (relative) platformindependence of these mechanisms. the web has presented new opporcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.128computer science: reflectionstunitiesincluding support for communication within distributed communitiesand it has also led to a number of new problems, not the leastof which are security and privacy. bruckman assesses the emerging use ofthe internet as a communication medium that links widely dispersedcommunities, and she analyzes the factors behind the development ofthese communities.sudan reviews the history of the cryptography and security mechanismsthat underlie secure web protocols and other forms of secure computercommunication. also evident is another example of how new opportunities arise when we find a way to eliminate a significant premise of atechnologyin this case, the advance exchange of decryption information, or the codebook. sudan also shows how the computational paradigm has changed even the basic notion of what constitutes proof in anauthentication system.software engineering research is concerned with better ways todesign, analyze, develop, evaluate, maintain, and evolve the complexsystems that deliver the computing services described in peterson andclark, bruckman, and sudan. shaw describes how software engineeringresearchers formulate and evaluate research of this kind and employ avariety of approaches to address the subdisciplines different types ofproblems.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale129the internet: an experiment that escapedfrom the lablarry peterson, princeton university, anddavid clark, massachusetts institute of technologythe recent explosion of the internet onto the worlds consciousness isone of the most visible successes of the computer science research community. the impact of the internet in enabling commerce, allowing peopleto communicate with each other, and connecting us to vast stores ofinformation and entertainment is undeniable. what is surprising to mostpeople who now take the internet for granted is that the underlying architecture that has allowed the internet to grow to its current scale wasdefined over 25 years ago.this remarkable story began in the late 1970s when a collection ofcomputer science researchers, then numbering less than a hundred, firstdeployed an experimental packetswitch network on tens of computersconnected by 56kbps links. they built the network with the modest goalof being able to remotely enter jobs on each others computers, but moreimportantly, as an experiment to help them better understand the principles of network communication and faulttolerant communication. onlyin their wildest dreams did they imagine that their experiment wouldenter the mainstream of society, or that over the next 25 years both thebandwidth of its underlying links and the number of users it connectswould each grow by six orders of magnitude (to 10gbps links and 100million users, respectively). that a single architecture not only survivedthis growth, but also in fact enabled it, is a testament to the soundness ofits design.layering and abstractionseveral design principles, many of them sharpened by years of experience building early operating systems like multics, helped shape theinternet architecture.the most important of these was to employ multiple layers of abstraction (see earlier essays) to manage the complexity of the system. networks cannot claim to have invented hierarchical abstraction, but theyhave become the most visible application of layering. at the lowest level,electricalmagnetic signals propagate over some medium, such as a copperwire or an optical fiber. at the next level, bits are encoded onto thesesignals. groups of bits are then collected together, so abstractly we canthink of machines sending selfcontained messages to each other. at thecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.130computer science: reflectionsnext layer, a sequence of machines forwards these messages along a routefrom the original source to the ultimate destination. at a still higher layer,the source and destination machines accept and deliver these messageson behalf of application processes that they host. we can think of thislayer as providing an abstract channel over which two (or more) processes communicate. finally, at the highest level, application programsextract meaning from the messages they receive from their peers.recognizing that layering is a helpful tool is one thing. understanding the right layers to define is quite another. here, the architects of theinternet were guided by another design principle, generalization. onedimension of generalization is to support as many applications as possible,including applications that have not yet been imagined. in terms recognized by all computer scientists, the goal was to build a network thatcould be programmed for many different purposes. it was not designedto just carry human voice or tv signals, as were other contemporarynetworks. instead, one of the main characteristics of the internet is thatthrough a simple matter of programming, it can support virtually anytype of communication service. the other dimension of generality is toaccommodate as many underlying communication technologies as possible. this is akin to implementing a universal machine on any number ofdifferent computational elements. looked at another way, the internet isa purely logical network, implemented primarily in software, and runningon top of a wide assortment of physical networks.next you need to codify the interfaces to the various layers. here,early internet researchers recognized the need to keep the common interfaces minimal, thereby placing the fewest constraints on the future usersof the internet, including both the designers of the underlying technologies upon which it would be built and the programmers that would writethe next generation of applications. this allows for autonomy among theentities that connect to the internet: they can run whatever operatingsystem they want, on whatever hardware they want, as long as theysupport the agreed upon interface. in this case, the key interface is betweencode modules running on different machines rather than modules runningon the same machine. such interfaces are commonly called protocols: theset of rules that define what messages can be sent between a pair ofmachines, and under what circumstances.in the early days of network design, it was not clear that we couldactually write protocol specifications with sufficient clarity and precisionthat successful communication was practical. in the 1970s it was predicted that the only way to get different computers to communicate witheach other was to have a single group of people build the code for all themachines, so that they could take into account all the details that wouldnever be specified properly in practice. today, the idea that protocols cancomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale131be well specified is accepted, but a great deal of work went into learninghow to do this, including both practical experiments and theoretical workon automatic checking of specifications.the general idea of abstraction takes many forms in the internet. inaddition to using layering as a technique for managing complexity, aform of abstraction known as hierarchical aggregation is used to managethe internets scale. today, the internet consists of tens of millions ofmachines, but these machines cannot possibly all know about each other.how then, can a message be correctly delivered from one machine toanother? the answer is that collections of machines are first aggregatedaccording to the physical network segment they are attached to, and thena second time according to the logical segment (autonomous domain) towhich they belong. this means that machines are assigned hierarchicaladdresses, such that finding a path from a source machine to a destinationmachine reduces to the problem of finding a path to the destinationdomain, which is then responsible for delivering the data to the rightphysical segment, and finally to the destination machine. thus, just aslayering involves a highlevel protocol hiding the uninteresting detailsabout a lowlevel protocol, aggregation involves high levels of theaddressing and routing hierarchy masking the uninteresting details aboutlower levels in the hierarchy.resource sharingnetworks are shared systems. many users send traffic from manyapplications across the same communication links at the same time. thegoal is the efficient exploitation of expensive resources. longdistancecommunication links are expensive, and if many people can share them,the cost per user to communicate is greatly reduced.the telephone system is a shared system, but the sharing occurs at thegranularity of a call. when a user attempts to make a call, the networkdetermines if there is capacity available. if so, that capacity is allocated tothe caller for as long as the call lasts, which might be minutes or hours. ifthere is no capacity, the caller is signaled with a busy tone.allocating communications capacity for a period of minutes or hourswas found to be very inefficient when computer applications communicated. traffic among computers seems to be very bursty, with short transmissions separated by periods of silence. to carry this traffic efficiently, amuch more finegrained sharing was proposed. the traffic to be sent isbroken into small chunks called packets, which contain both data to besent and delivery information. packets from many users come togetherand are transmitted, in turn, across the links in the network from sourcetoward destination.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.132computer science: reflectionswhen the concept of packet was first proposed, there was considerable uncertainty as to whether this degree of multiplexing would work. iftraffic from multiple users arrives to be sent at the same instant, a queueof packets must form until all can finally be sent. but if the arrival patternof packets is unpredictable, is it possible that long, persistent queues willform? will the resulting system actually be usable? the mathematics ofqueuing theory were developed to try to understand how such systemsmight work. queuing theory, of course, is not restricted to network design.it applies to checkout lines in stores, hospital emergency rooms, and anyother situation where arrival patterns and service times are predictableonly in a statistical sense. but network design has motivated a great dealof research that has taught us much about statistical properties of sharedsystems. we now know the conditions to build systems like this withpredictable stability, reasonable traffic loads, and support for a wide rangeof applications. the concept of the packet has turned out to be a veryrobust one that has passed the test of time.more recently, as the internet has grown larger, and the number ofinteracting traffic flows has grown, a new set of observations have emerged.the internet seems to display traffic patterns that are selfsimilar, whichmeans that the patterns of bursts that we see in the aggregated traffic havethe same appearance when viewed at different time scales. this hints thatthe mathematics of chaos theory may be the tool of choice to increase ourunderstanding of how these large, shared systems work.devising techniques to share resources is a recurring problem in computer science. in the era of expensive processors, timesharing systemswere developed to share them. cheaper processors brought the personalcomputer, which attempts to sidestep some of the harder sharing problems by giving each user his own machine. but sharing is a fundamentalaspect of networking, because sharing and communication among peopleand the computers that serve them is a fundamental objective. so masteryof the models, tools, and methods to think about sharing is a fundamentalobjective of computer science.concluding remarksthe internet is arguably the largest manmade information systemever deployed, as measured by the number of users and the amount ofdata sent over it, as well as in terms of the heterogeneity it accommodates,the number of state transitions that are possible, and the number ofautonomous domains it permits. whats more, it is only going to grow insize and coverage as sensors, embedded devices, and consumer electronicequipment become connected. although there have certainly been stresseson the architecture, in every case so far the keepers of the internet havecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale133been able to change the implementation while leaving the architectureand interfaces virtually unchanged. this is a testament to the soundnessof the architecture, which at its core defines a universal networkmachine.by locking down the right interfaces, but leaving the rest of therequirements underspecified, the internet has evolved in ways neverimagined. certainly this is reflected in the set of applications that run onthe internet, ranging from video conferencing to ecommerce, but it isalso now the case that the internet has grown to be so complicated that thecomputer scientists that created it can no longer fully explain or predictits behavior. in effect, the internet has become like a natural organism thatcan only be understood through experimentation, and even though it is adeterministic system, researchers are forced to create models of its behavior, just as scientists model the physical world. in the end, the internetmust be viewed as an information phenomenon: one that is capable ofsupporting an everchanging set of applications, and whose behavior canbe understood only through the use of increasingly sophisticated measurement tools and predictive models.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.134computer science: reflectionsmanytomany communication: a new mediumamy bruckman, georgia institute of technologyin the early 1990s, computermediated communication (cmc) explodedin popularity, moving from a tool used by small groups of engineers andscientists to a mass phenomenon affecting nearly every aspect of life inindustrialized nations. even in the developing world, cmc has begun toplay a significant role. yet we are just at the beginning, not the end, of thetransformations catalyzed by this technology. we can draw an analogy toan earlier era of intense social change launched by new technology: theintroduction of the car. in the early days of the internal combustion engine,cars were called horseless carriages: we understood the new technology in terms of an old, familiar one. at that stage, we could not begin toimagine the ways that cars would transform the united states and theworld, both for good and for ill. the internet is in its horseless carriagestage. at this pivotal moment, we have a unique opportunity to shape thetechnologys evolution, and the inevitable changes to society that willaccompany it.the key feature of this technology is its support for manytomanycommunications. this paper will analyze the significance of manytomany communications in key relevant application areas.with manytomany communications, individuals are becomingcreators of content, not merely recipients. for example, we are no longerrestricted to reading journalistic interpretations of current events, but cannow also share our own views with friends and family. opportunities fordiscourse on issues of import are at the foundation of a democratic society.we are experiencing a renaissance in public debate of serious matters bycitizens.manytomany communications are changing the nature of medicine.the new medical consumer arrives at the doctors office better informed.the pew center for internet life reports that fiftytwo million americanadults, or 55% of those with internet access, have used the web to gethealth or medical information, and of those, 70% said the web information influenced their decision about how to treat an illness or condition(fox and rainie, 2000). patients can talk online with others with similarailments, exchanging not just useful medical information but also emotional support. this emotional support is particularly valuable to caregivers of patients with serious illnesses, a group whose needs are oftenneglected.manytomany communications are having a growing impact on business practices. in the field of retailing, consumers can now easily sharecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale135product recommendations, giving developers of quality products acompetitive advantage. competitive price information is available withunprecedented ease of access. the free manytomany flow of information moves us closer to the ideal of an efficient market.new kinds of commerce are emerging. we are no longer bound tohave all purchases of secondhand goods go through middlemen likeconsignment shops, but can sell items directly to others. for example, inareas like antiques, collectibles, and used consumer electronics, for thefirst time in history a fair and efficient market has emerged. items thatwould otherwise have been discarded can now find their way to just theperson who needs them, leading to a less wasteful society.the remainder of this paper discusses three application areas wherethe impact of internet technology merits special attention: the expansionof scientific knowledge, entertainment, and education.accelerating the expansion of knowledgethe internets most obvious capability is to distribute information.the world wide web was invented by tim bernerslee and colleagues atcern in order to accelerate scientific progress: researchers can exchangeideas must faster than was formerly possible. this has had particularimpact in the developing world. researchers in developing nations whocould never afford subscriptions to research journals now have growingaccess to current scientific information and indirect participation in theinternational community of scientists.sociologists of science like bruno latour teach us that truth is sociallyconstructed. this is true of the most basic hard scientific facts. a newidea begins attributed to a specific person: einstein says that e = mc2.as it becomes more accepted, the attribution is dropped to a footnote:e=mc2 (einstein 1905). finally, the attribution is deemed entirelyunnecessary, and one can simply say e = mc2it has become anaccepted scientific fact (latour et al., 1986). the process of one researchersclaim rising to the level of fact is fundamentally social. initially, peopleare unsurewas the scientists work sound? do others support this finding? as such questions are asked and answered, some claims are rejectedand others become widely accepted. truth emerges not from the work ofone scientist, but from the community. it is not instantly revealed, butbegins as tentative and solidifies over time. the internet gets the mostattention for its ability to support the exchange of factual information inthe simple sense, as if it is merely a giant database that is unusually up todate. however, it is important to understand latours subtler vision ofhow new knowledge is constructed, and the way that the internet isuniquely well suited to accelerating that social process.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.136computer science: reflectionsas we move forward, we need to find ways to enhance the ability ofnetwork technology to facilitate the free exchange of scientific ideas. academic researchers in most fields are still rewarded for presenting ideas inpeerreviewed journal articles that may take years to appear in print.they are often reluctant to share ideas before they appear in officiallycredited form. corporate researchers may be reluctant to share findings atall. yet the entire endeavor of research can be enhanced through moreimmediate sharing of ideas via the internet. the challenge, then, is to findways to give individuals credit for ideas shared rapidly online, while atthe same time maintaining the quality control of peer review. the field ofonline community research focuses on these issues, especially, what motivates individuals to contribute and how quality of discourse can be maintained (preece, 2000). the design of our nextgeneration communicationsystems will help to accelerate the pace of discovery in all fields.online entertainmentas we have seen, we can view the internet as facilitating the exchangeof scientific information; however, it is more farreaching to see it as supporting the growth of a community of scientists (and information as a keyproduct of that community). this fundamental insight applies not just toscience, but to most domains. for example, the entertainment industry isno longer simply delivering content, but is using computer networks tobring groups of individuals together. new internetbased forms of entertainment fall into a variety of genres, including bulletinboard systems(bbss), chat, and games.internetbased communication provides opportunities for new kindsof socializing. in chat rooms and on bbss, people gather together todiscuss hobbies and to meet others. people with an unusual hobby findthey are no longer alone but can meet likeminded others from around theworld. some of this social activity bridges into facetoface activity. hobbyists from geographically diverse areas may meet face to face at annualconventions and then maintain those ties online. in other cases, the socialactivity is local in nature. computermediated communication is used toschedule facetoface meetings and to continue discussion between suchmeetings. sociologist robert putnam has documented a decrease inamericans participation in civic groups over the last halfcentury(putnam, 1995). the ease of coordinating social and civic groups with theaid of new communications technologies has the potential to help beginto reverse this trend.popular types of internetbased games include traditional games (likebridge and chess), fantasy sports, action games, and massively multiplayer games (mmps). the most important characteristic these gamescomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale137share is that they are social: people play with relatives, old friends, andnew friends met online. for example, the author plays bridge online withher mother (who lives hundreds of miles away) and action games withfriends from high school and graduate school living all around the country.getting together to play online gives us occasions to keep in touch. thisnew entertainment form can help to maintain what sociologists callstrong ties over distance and also create new weak ties (wellman andgulia, 1999). while one might be inclined to dismiss friendships madeonline as trivial, they are often quite meaningful to participants and sometimes have practical value as well. howard rheingold recounts howpeople who know one another from a california bbs called the wellsent hundreds of books to a bookloving group member who lost all hispossessions in a fire. well members also collaborated to arrange for themedical evacuation of a group member who became ill while in thehimalayas (rheingold, 1993). these kinds of stories are not unusual.every year, students in the design of online communities graduateclass at georgia tech are asked to write a short essay on their best andworst experiences involving internetbased communications, and similarstories emerge each time.its important to note that a number of aspects of online entertainmentare discouraging. in particular, some online games are so compelling formany players that they may devote extremely large amounts of time toplaying them. for example, as of fall 2001, the most popular mmpeverquest, by verant interactivehad 400,000 registered members(verant, 2001) who spent on average 22.5 hours per week playing (yee,2001). that mean figure includes a significant number who hardly participate at all, so the median is likely substantially higher. in other words,there are tens and possibly hundreds of thousands of people who devoteall their nonwork time to participation in the game. while we must becareful about passing judgement on how others spend their free time, theplayers themselves often find this problematicso much so that the gameis often referred to by the nickname evercrack. other games in thisnew genre have similar holding power.mmps are fun for players and profitable for game companies. in addition to charging people to buy the game initially, companies also charge amonthly fee. this lets companies make more money from a single development effort and gives them a more even revenue stream, making themless dependent on unpredictable seasonal sales and new releases. as aresult, many companies are developing new mmp titles. while traditional multiplayer games allow a handful of people to interact in the samegame space, mmps support thousands. its likely we are just at the beginning of their growth in popularity. while they provide an entertainmentoffering that is both social and active, they also tend to lead to overcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.138computer science: reflectionsfigure 7.1 aquamoose 3d: an internetbased mathlearning environment.involvement by some players. this is a key moment to ask: how can weshape this trend? how should we?one constructive course of action is to create gamelike environmentsthat have educational content. for example, aquamoose 3d is a threedimensional virtual world designed to help highschool students learnmathematics (edwards et al., 2001). see figure 7.1. you are a fish, and youspecify your motion in the graphical world mathematically. for example,swim in a sine in x and a cosine in y, and you move in a spiral and leavea spiral trail behind you. you can also create sets of rings in the water andchallenge others to guess the equation that goes through them. studentscreate these mathematical puzzles to challenge their friends. aquamooselooks much like the purely entertainment environments many studentsfind so compelling, but time spent there is educationally valuable. weneed to develop more such environments to encourage students to chooseto spend their free time wisely.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale139the learning potential of internet technologyeducation is a key area where manytomany communications havebegun to have a strong positive impact. many educational applications ofinternet technology focus on information: in distance education information is delivered to the student. in online research projects, information isretrieved. in more innovative work, information is gathered by studentsand shared. while informationoriented applications of internet technology are useful, the more exciting potential of this new learning mediumis not about information but about community and collaboration. online,groups of learners can motivate and support one anothers learningexperiences.learning from peerslearning is fundamentally a social process, and the internet has aunique potential to facilitate new kinds of learning relationships. forexample, in the one sky, many voices project by nancy songer at theuniversity of michigan (http://www.onesky.umich.edu/), kids can learnabout atmospheric phenomena from scientists working in the field(songer, 1996). more importantly, the students also learn from oneanother: kids in montana studying hurricanes can talk online with floridastudents in the midst of one. learning from peers can be a compellingexperience and is a scalable educational solution. if enough educationalprograms try to leverage the skills of adult experts, experts will ultimatelyspend all their time in public service. while the supply of experts islimited, the supply of peers is not.peers can be a powerful resource for childrens learning, if activitiesare structured to promote productive interactions. moose crossing is atextbased virtual world (or mud) in which kids 8 to 13 years old learncreative writing and objectoriented programming from one another(http://www.cc.gatech.edu/elc/moosecrossing/). see figure 7.2. thespecially designed programming language (moose) and environment(macmoose and winmoose) make it easy for young kids to learn toprogram. members dont just experience the virtual worldthey construct it collaboratively. for example, carrot1 (girl, age 9) created a swimming pool complex. using lists stored on properties of the pool object, shekeeps track of who is in the pool, sauna, or jacuzzi, and who has changedinto a bathing suit. you obviously cant jump into the pool if youre1all real names and online pseudonyms of participants have been changed to protecttheir confidentiality.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.140computer science: reflectionsfigure 7.2a textbased virtual world where kids practice creative writing andlearn objectoriented programming.already in the water . . . you need to get out first! this gives carrotopportunities for comic writing as well as programming. the textbasednature of the environment is not a technical limitation, but rather a deliberate design choice: it gives kids a context for using language playfullyand imaginatively. carrot enjoys inviting other kids over to the pool.they in turn learn about programming and writing using her work as amodel (bruckman, 1998, 2000).the online community provides a ready source of peer support forlearning. kids learn from one another, and from one anothers projects.that support is not just technical, but also emotional. in answering aquestion, one child may tell another, i got confused by that too at first.the online community provides a ready source of role models. if, forexample, girls are inclined to worry that programming might not be acool thing for a girl to do, they are surrounded by girls and womenengaging in this activity successfully and enjoying it. finally, the onlinecommunity provides an appreciative audience for completed work. kidsget excited about being creative in order to share their work with theirpeers.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale141elders as mentorssocial support for learning online can come not just from peers, teachers,and experts, but also from ordinary members of the general population,who form a vast potential resource for our childrens education. retiredpeople in particular have a great deal they can teach kids and free time tocontribute, but they need an easy and wellstructured way to do so. in thepalaver tree online project (http://www.cc.gatech.edu/elc/palaver/,the dissertation research of georgia tech phd student jason ellis), middleschool students learn about history from elders who lived through it.teachers begin with literature that is part of their normal curriculum.kids brainstorm historical questions based on what theyve learned, interview elder mentors about their personal experiences with that time, andthen write research reports about what theyve learned. see figures 7.3and 7.4. in our studies to date, kids learning about world war ii interviewed veterans, and kids learning about the civil rights years interviewedolder african americans. history learned from real people becomes moremeaningful and relevant (ellis and bruckman, 2001).figure 7.3eighthgrade students interviewing an elder mentor about her experiences growing up during the civil rights years. (all names have been changed.)computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.142computer science: reflectionsfigure 7.4the project students created based on their interview.of course it would be better for kids to meet with elders face to face,but in practice this rarely if ever happens. in interviews with teacherswho have tried such projects, we found that the logistics are too difficultto arrange for all involved. elder volunteers, when asked if they will driveto an unfamiliar place and commit to multiple visits, often hesitate. however, when asked, would you be willing to log on for half an hour a dayfor a few weeks?, they are enthusiastic. the palaver tree online community makes this not only possible but also relatively easy for the elders,students, and teachers. teachers are already overwhelmed with work,and any successful schoolbased learning technology needs to make theirlives easier, not harder.new social and technical possibilitiesculture and technology coevolve. the challenge as we move forwardis to develop a vision of what is possibleto understand the more andless desirable outcomes, and try to steer in the right direction. hardwareand software infrastructure developed over the last 40 years are just nowcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale143becoming widely available and are finding a wide variety of new applications. the wide availability of internet access has made manytomanycommunications possible, and this capability has a profound impact onhow we conduct business, manage our health, share knowledge, entertainourselves and one another, and learn.referencesbruckman, a., 1998, community support for constructionist learning, computer supported cooperative work 7:4786.bruckman, a., 2000, situated support for learning: storms weekend with rachael,journal of the learning sciences 9(3):329372.edwards, e., j. elliott, and a. bruckman, 2001, aquamoose 3d: math learning in a 3dmultiuser virtual world, paper presented at the chi 2001 conference on humanfactors in computing systems, seattle, wash.ellis, j., and a.bruckman, 2001, palaver tree online: supporting social roles in a community of oral history, paper presented at the chi 2001 conference on human factorsin computing systems, seattle, wash.fox, s., and l. rainie, 2000, the online health care revolution: how the web helps americanstake better care of themselves, pew internet and american life project, washington,d.c.latour, b., s. woolgar, and j. salk, 1986, laboratory life, princeton university press,princeton, n.j.preece, j., 2000, online communities: designing usability, supporting sociability, john wiley &sons, new york.putnam, r., 1995, bowling alone: americas declining social capital, journal of democracy6(1).rheingold, h., 1993, the virtual community: homesteading on the electronic frontier, addisonwesley, reading, mass.songer, n., 1996, exploring learning opportunities in coordinated networkenhancedclassrooms: a case of kids as global scientists, the journal of the learning sciences5(4):297327.verant, 2001, sony online entertainment to introduce new everquest servers in european markets: verant interactive.wellman, b., and m. gulia, 1999, virtual communities are communities: net surfersdont ride alone, in m.a. smith and p. kollock (eds.), communities in cyberspace,routledge, new york.yee, n., 2001, the norrathian scrolls: a study of everquest (2.5), available at http://www.nickyee.com/eqt/report.html.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.144computer science: reflectionscryptographymadhu sudan, massachusetts institute of technologyconsider the following prototypical scenario in the internet era: alicewishes to access her bank account with the bank of billionaires (bob)through the internet. she wishes to transmit to bob her account numberand password and yet does not want her internet service provider toknow her account number and password. the potential for commerceover the internet relies critically on the ability to implement this simplescenario. yet when one gets down to formalizing the goals of this scenariomathematically, one realizes this goal is almost impossible. after all theinternet service provider has access to every bit of information that entersor leaves alices computer, and bob has only a subset! fortunately, thereis a tiny crack in any such proof of impossibility (read computationalfoundations of cryptography below for details)a crack visible onlywhen inspected with a computational lensand from this crack emergescryptography, the science of encrypting and decrypting messages.cryptography has been practiced for centuries now. its need becomesevident in any situation involving longdistance communication wheresecrecy and (mis)trust are governing factors. yet, till the advent of the20th century much of cryptography has been misunderstood and practiced as black magic rather than as a science. the advent of computers,and the development of the computational perspective, has changed allthis. today, one can deal with the subject with all the rigor and precisionassociated with all other mathematical subjects. achieving this progresshas required the formalization of some notionssuch as randomness,knowledge, and proofthat we rely on commonly in our lives but whosemathematical formalization seems very elusive. it turns out that all thesenotions are essentially computational, as is cryptography. furthermorecryptography is feasible only if some very fundamental computationalhypotheses (such as np p; see kleinberg and papadimitriou in chapter2) hold out. this essay describes some of the salient events in thehistory of cryptography.cryptography in the medieval eratraditionally, messages were encrypted with codebooks. roughly, inthis setup alice and bob initially share some secret information, called thecodebook. this codebook might be something as simple as a lettertolettersubstitution rule, or something more complexsuch as a wordtowordsubstitution rule, and so on. when alice obtains some new information tocomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale145be transmitted secretly to bob, she uses the codebook to translate themessage and send it (through untrusted couriers) to the receiver. now,assuming that the courier does deliver the encrypted message, the receiveruses his copy of the codebook to translate the messages back. the courieror any other eavesdropper, however, cannot in theory decipher the message,since he does not possess the codebook.to compare the above system with some of the more recent themes incryptography, let us compare some of the basic elements of the model andthe associated assumptions. one of the primary features of the modelabove is the role of the codebook. it is what distinguishes the receiverfrom the eavesdropper (based on information the receiver possesses), andthe initial cryptography protocols assumed (mistakenly, as it turned out)that the codebook was a necessary component for secret communication.over time, the belief took on quantitative dimensionsthe larger the sizeof the codebook, the more secure the encryption. for example, a lettertoletter substitution rule applied to the english alphabet requires the testingof 26! possibilities. but most can be ruled out easily based on frequencyanalysis of english text. so lettertoletter substitution rules, involvingsmall codebooks, can be broken easily. wordtoword substitution rulesare harder to break, but it is still feasible to do so with decent computers.the last of these observations, that better computers might lead toloss of secrecy, was worrisome. maybe the advent of computers wouldlead to the ultimate demise of cryptography! in the face of such popularbeliefs, it came as a startling surprise that cryptography could potentiallythrive with advances in computing.computational foundations of cryptographythe foundations of cryptography were laid gradually. first cameinformationtheoretic foundations, starting with shannon in the 1950s,and a little later, in the 1970s, came the computational notions of merkle(1978), diffie and hellman (1976), and rivest, shamir, and adleman (1978).shannons theory (shannon, 1949) formally asserted that secret communication is possible whenever the communicating parties, alice andbob, share some secret information. furthermore, this secret informationhad to have some randomness associated with it, in order to prove thesecurity of the transmission. quantifying this notion (how random is theshared secret?) led to a quantification of how much information could beexchanged secretly. in particular, if alice and bob got together and pickeda kbit random string as the secret to share (where every bit is chosenuniformly and independent of other bits), then alice could encrypt anykbit message and send the encrypted message to bob in the clear, suchthat the eavesdropper could get no information about the message whilecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.146computer science: reflectionsbob, with knowledge of the shared secret, could decrypt the messagefrom the encrypted form. (for example, if the shared secret s is interpreted as a kbit integer and the message m to be exchanged is also a kbitinteger, then the encrypted text can simply be e = (m + s) mod 2k. bob, onreceiving e, can recover m by computing e ð s mod 2k).the importance of this result is not so much in the protocol derived asin the notions defined. how do we determine secrecy? why is the messagem being kept secret in the above exchange? how do we prove it? theanswers all turn out to be quite simple. message m is secret because itsencryption (which is a random variable dependent on the secret s) isdistributed statistically identically to the encryption of some othermessage m. thus indistinguishability is the basis for secrecy.what happens when alice tries to transmit a message of more thankbits to bob (while they only share a kbit secret)? shannons theoryexplained that in this case the eavesdropper can get some informationabout the message m from the transmission (no matter what protocolalice and bob adopt). shannon realized that this information may notprovide any usable information about the message itself, but he wasunable to exploit this lack of usability any further.how is it possible that one has information about some string m, butis not able to use it? how does one determine usability so as to have ameaningful sense of this possibility? it turns out that these questions areessentially computational, and it took the seminal work of diffie andhellman (1976) to realize the computational underpinnings and to exploitthem.diffie and hellman noticed that several forms of computation transform meaningful information into incomprehensible forms. (as a sideremark they note that small programs written in a highlevel languageimmediately reveal their purpose, whereas once they are compiled into alowlevel language it is hard to figure out what the program is doing!)they pointed out a specific algebraic computation that seems to have thisvery useful informationhiding feature. they noticed it is very easy tocompute modular exponentiation. in particular, given an nbit primenumber p, an integer g between 2 and p 1, and exponent x between 1 andp 1, it is possible to compute y = gx (mod p) in approximately n2 steps ofcomputation. furthermore if g is chosen somewhat carefully, then thismapping from x to gx (mod p) is one to one and thus invertible. however,no computationally efficient procedure was known (then or now) to compute, given g, p, and y, an integer x such that y = gx. this task of invertingmodular exponentiation is referred to as the discrete logarithm problem.relying in part on the seeming hardness of the discrete log problem,diffie and hellman suggested the following possibility for alice and bobto exchange secrets: to exchange an nbit secret alice picks a prime p andcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale147an integer g as above and sends g and p (in the clear!) to bob. she thenpicks a random integer x1 between 1 and p 1, computes   gx1 mod p, andsends this string to bob. bob responds by picking another random stringx2 and sending to alice the string  gx2 mod p. at this point, alice and bobcan both compute gxx12 mod p. (alice computes this as (  gx2) x1, while bobcomputes this as ( gx1) x2).what about the eavesdropper? surprisingly, though all this conversation was totally in the clear, the eavesdropper seems to have no efficientway to compute gxx12. she knows gx1 and  gx2 but these do not suffice togive an efficient way to compute gxx12. (in all cases we dont know thatthere is no efficient way to compute one of the unobvious casesit is justthat we dont know of any efficient way to compute them and thus conjecture that these computations are hard.) thus alice and bob have managed to share a random secret by communication in the clear that alicecan now use to send the real message m to bob.the protocol above provides a simple example of the distinction between information and its usability. together, gx1 and  gx2 specify  gxx12but not in any usable way, it seems. the essence of this distinction is acomputational one. and it leads to a very computational framework forcryptography. the protocol can be used to exchange information secretly,where secrecy is now determined using a computational notion of indistinguishability, where computational indistinguishability suggests thatno efficient algorithm can distinguish between a transcript of a conversation that exchanges a secret m and one that exchanges a secret m. (givinga precise formulation is slightly out of scope.)and what does one gain from this computational insight? for the firsttime, we have a protocol for exchanging secrets without any prior sharingof secret information (under some computational hardness assumptions).lessons learned from cryptographythe seminal work of diffie and hellman altered fundamental beliefsabout secrecy. the natural guess is that any information that bob maypossess unknown to alice would be useless in preserving the secrecy of acommunication from alice to bob. a continuation of this line of reasoningleads to the belief that alice cant send secret information to bob unlessthey shared some secrets initially. yet the above protocol altered thisbelief totally. the intractability of reversing some forms of computationscan lead to secret communication. such intractability itself relies on someother fundamental questions about computation (and in particular impliesp np). in fact, it was the emerging belief in the conjecture p np thatled diffie and hellman to propose the possibility of such publickeycomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.148computer science: reflectionsexchange protocols and posit the possibility of other publickey encryption systems.in the numerous years since, cryptography has led to numerous computational realizations of conventional wisdom (and computationalrefutations of mathematical truths!). numerous notions underlying daytoday phenomena, for which formalization had earlier proved elusive,have now been formalized. two striking examples of such notions arethose of pseudorandomness and proofs. we discuss these below.shannons theory had asserted that sharing a secret random stringwas necessary for a message to be secure. the diffiehellman protocolmanaged to evade this requirement by showing how alice and bob couldcreate a secret that they shared using public conversations only. thissecret was the string gxx12. how random is this secret? the answer depends on your foundations. information theory would declare this stringto be totally nonrandom (or deterministic) given   gx1 and  gx2. computational theory, however, seems to suggest there is some element of randomness to this string. in particular, the triples ( gx1; gx2; gx3) and ( gx1; gx2; gxx12) seem to contain the same amount of randomness, to anycomputationally bounded program that examines these triples, whenx1; x2; x3 are chosen at random independent of each other. so the computational theory allows certain (distribution on) strings to look morerandom than they are. such a phenomenon is referred to as pseudorandomness and was first formalized by blum and micali (1984). pseudorandomness provides a formal basis for a common (and even widelyexploited) belief that simple computational steps can produce a longsequence of seemingly uncorrelated or random data. it also provided afundamental building block that has since been shown to be the crux ofmuch of cryptography.a second example of a computational phenomenon is the ageoldnotion of a proof. what is a proof? turnofthe20thcentury logicians hadgrappled with this question successfully and emerged with a clean explanation. a proof is a sequence of simple assertions that concludes with thetheorem by which each assertion can be easily verified. the term easilyhere requires a computational formalism, and in fact, this led to the definition of a turing machine in the 1940s (see kleinberg and papadimitriouin chapter 2). the advent of cryptography leads us back to the notion of aproof and some seemingly impossible tasks. to see the need for thisnotion, let us revisit the scenario introduced in the opening paragraph.the goal of the scenario can be reinterpreted as follows: bob knows thatalice is the (only) person who knows a secret password m. so when someuser comes along claiming to be alice, bob asks for her password, in effectsaying prove you are alice! or equivalently prove you know m. intraditional scenarios alice would have simply typed out her password mcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale149on her keyboard and sent it over the internet. this would have corresponded to our standard intuition of a proof. unfortunately this standard notion is a replayable oneany eavesdropper could listen in to theconversation and then also prove that he/she is alice (or knows m). butthe interaction above allows alice to prove to bob that she knows m(indeed she essentially sends it to him), without revealing m to the eavesdropper. so it seems alice was able to prove her identity without revealing so much information that others can later prove they are alice! howdid we manage this? the most significant step here is that we changedour notion of a proof. conventional proofs are passive written texts. thenew notion is an interactive randomized conversation. the new notion,proposed by goldwasser, micali, and rackoff (1989), retains the power ofconviction that conventional proofs carry, but it allows for a greater levelof secrecy. proofs can no longer be replayed. as subsequent developments revealed, these proofs also tend to be much shorter and can beverified much more efficiently. so once again computational perspectivessignificantly altered conventional beliefs.the future of cryptographycryptography offers a wonderful example of a phenomenon quitecommonplace in the science of computing. the advent of the computerraises a new challenge. and the science rises to meet the new challenge bycreating a rich mathematical structure to study, analyze, and solve thenew problems. the solutions achieved (and their deployment in almostevery existing web browser!) as well as the scientific knowledge gained(proofs, pseudorandomness, knowledge) testify to the success ofcryptography so far. going into the future one expects many furtherchallenges as the scope of cryptography broadens and our desire to goonline increases. one of the biggest challenges thus far has been in creating large, possibly distributed, systems that address the security of theircontents. cryptography may be likened to the task of designing securelocks and keys. no matter how inventive and successful one is with thisaspect, it does not automatically lead to secure houses. similarly, buildingsecure computer systems involves many more challenges in terms ofdefining goals, making sure they are feasible, and then attaining themefficiently. research in this direction is expected to be highly active.to conclude on a somewhat cautious note: cryptography, like manyother scientific developments, faces the problem of being a doubleedgedsword. just as it can be used to preserve the privacy of honest individuals,so can it equally well preserve the privacy of the communications of badguys. indeed, fear of this phenomenon has led to government oversighton the use and spread of cryptography and has raised a controversialcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.150computer science: reflectionsquestion: is the negative impact sufficient to start imposing curbs oncryptographic research? hopefully, the description above is convincingwith respect to two aspects: cryptographic research is essentially justdiscovering a natural, though surprising, computational phenomenon.curbing cryptographic research will only create a blind spot in our understanding of this remarkable phenomenon. and while the tools that theresearch invents end up being powerful with some potential for misuse,knowing the exact potential and limits of these tools is perhaps the bestway to curb their misuse. keeping this in mind, one hopes that cryptographic research can continue to thrive in the future uninhibited by external pressures.referencesblum, manuel, and silvio micali, 1984, how to generate cryptographically strongsequences of pseudorandom bits, siam journal on computing 13:850864.diffie, whitfield, and martin e. hellman, 1976, new directions in cryptography, ieeetransactions on information theory 22(6):644654.goldwasser, shafi, silvio micali, and charles rackoff, 1989, the knowledge complexityof interactive proof systems, siam journal on computing 18(1):186208.merkle, ralph, 1978, secure communications over insecure channels, communications ofthe acm (april):294299.rivest, ronald l., adi shamir, and leonard adleman, 1978, a method for obtainingdigital signatures and publickey cryptosystems, communications of the acm21(2):120126.shannon, claude e., 1949, communication theory of secrecy systems, bell systems technical journal 28(6):656715.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale151strategies for software engineering researchmary shaw, carnegie mellon universitysoftware engineering is the branch of computer science that createspractical, costeffective solutions to computation and information processing problems, preferentially applying scientific knowledge, developing2 software systems in the service of mankind. like all engineering,software engineering entails making decisions under constraints of limitedtime, knowledge, and resources. the distinctive character of softwarethe form of the engineered artifact is intangible and discreteraises specialissues of the following kind about its engineering:software is designintensive; manufacturing costs are a very smallcomponent of product costs.software is symbolic, abstract, and more constrained by intellectualcomplexity than by fundamental physical laws.software engineering is particularly concerned with software thatevolves over a long useful lifetime, that serves critical functions, that isembedded in complex softwareintensive systems, or that is otherwiseused by people whose attention lies appropriately with the applicationrather than the software itself. these problems are often incompletelydefined, lack clear criteria for success, and interact with other difficultproblemsthe sorts of problems that rittel and webber dubbed wickedproblems.3software engineering rests on three principal intellectual foundations.the principal foundation is a body of core computer science concepts relating to data structures, algorithms, programming languages and theirsemantics, analysis, computability, computational models, and so on; thisis the core content of the discipline. the second is a body of engineeringknowledge related to architecture, the process of engineering, tradeoffsand costs, conventionalization and standards, quality and assurance, andothers; this provides the approach to design and problem solving that2developsoftware engineering lacks a verb that covers all the activities associatedwith a software product, from conception through client negotiation, design, implementation, validation, operation, evolution, and other maintenance. here, develop refers inclusively to all those activities. this is less than wholly satisfactory, but it isnt as bad as listingseveral verbs at every occurrence.3horst rittel and melvin webber, 1973, dilemmas in a general theory of planning,policy sciences 4:155169, elsevier scientific publishing, amsterdam.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.152computer science: reflectionsrespects the pragmatic issues of the applications. the third is the humanand social context of the engineering effort, which includes the process ofcreating and evolving artifacts, as well as issues related to policy, markets,usability, and socioeconomic impacts; this context provides a basis forshaping the engineered artifacts to be fit for their intended use.software engineering is ofteninappropriatelyconfused with mereprogramming or with software management. both associations areinappropriate, as the responsibilities of an engineer are aimed at the purposeful creation and evolution of software that satisfies a wide range oftechnical, business, and regulatory requirementsnot simply the abilityto create code that satisfies these criteria or to manage a project in anorderly, predictable fashion.software engineeringa physicist approaches problems (not just physical problems) by trying to identify masses and forces. a mathematician approaches problems(even the same problems) by trying to identify functional elements andrelations. an electrical engineer approaches problems by trying to identifythe linearly independent underlying components that can be composedto solve the problem. a programmer views problems operationally, looking for state, sequence, and processes. here we try to capture the characteristic mindset of a software engineer.computer science fundamentalsthe core body of systematic knowledge that supports software engineering is the algorithmic, representational, symbolprocessing knowledgeof computer science, together with specific knowledge about softwareand hardware systems.symbolic representations are necessary and sufficient for solving informationbased problems. control and data are both represented symbolically.as a result, for example, an analysis program can produce a symbolicdescription of the path for a machine tool; another program can take thissymbolic description as input and produce a symbolic result that is thebinary machine code for a cutting tool; and that symbolic representationcan be the direct control program for the cutting tool. notations for symbolic description of control and data enable the definition of software,both the calculations to be performed and the algorithms and data structures. this task is the bread and butter of software implementation, andthe existence of symbol strings as a uniform underlying representation ofcode, data, specification, analysis, and other descriptions simplifies bothsoftware design and tool support for software development activities.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale153abstraction enables the control of complexity. abstraction allows theintroduction of taskspecific concepts and vocabulary, as well as selectivecontrol of detail. this in turn allows separation of concerns and a crispfocus on design decisions. a designer of mechanical systems might workwith (and expand) a set of abstractions having to do with shapes, weights,and strengths, whereas a designer of accounting systems might workwith a set of abstractions having to do with customers, vendors, transactions, inventory, and currency balances. the ability to introduce problemspecific definitions that can, in most respects, be treated as part of theoriginal design language allows software design to be carried out inproblemspecific terms, separating the implementation of these abstractions as an independent problem. an additional benefit is that this leadsto models and simulations that are selective about the respects in whichthey are faithful to reality. some levels of design abstraction, characterized by common phenomena, notations, and concerns, occur repeatedlyand independent of underlying technology. the most familiar of these isthe programming language, such as java. the recent emergence of umlhas provided a set of diagrammatic4 design vocabularies that addressspecific aspects of design, such as the sequence of operations or the allowable transitions between system states. more specialized abstractions arenow being used to define software product families, or design spacesthat allow multiple similar software systems to be produced systematically and predictably.imposing structure on problems often makes them more tractable, and anumber of common structures are available. designing systems as related setsof independent components allows separation of independent concerns;hierarchy and other relations help explain the relations among the components. in practice, independence is impractical, but software designerscan reduce the uncertainty by using wellunderstood patterns of softwareorganization, called software architectures. an architecture such as a pipeand filter system, a clientserver system, or an applicationandpluginorganization provides guidance drawn from prior experience about thekinds of responsibilities to assign to each component and the rules thatgovern component interaction.precise models support analysis and prediction. these models may beformal or empirical. formal and empirical models are subject to differentstandards of proof and provide different levels of assurance in theirresults. for example, a formal model of an interaction protocol can reveal4diagrams are symbolic representations, just as text strings are. the grammars for diagrammatic notations may be more complex than those for textual symbolic representations,but the essential properties are shared.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.154computer science: reflectionsthat implementations will have internal inconsistencies or the possibilityof starvation or deadlock. the results support software design by providing predictions of properties of a system early in the system design, whenrepair is less expensive. software systems are sufficiently complex thatthey exhibit emergent properties that do not derive in obvious ways fromthe properties of the components. models that support analysis or simulation can reveal these properties early in design, as well.engineering fundamentalsthe systematic method and attention to pragmatic solutions thatshapes software engineering practice is the practical, goaldirectedmethod of engineering, together with specific knowledge about designand evaluation techniques.engineering quality resides in engineering judgment. tools, techniques,methods, models, and processes are means that support this end. thehistory of software development is peppered with methodologies fordesigning software and tools for creating and managing code. to theextent that these methods and tools relieve the designer of tedious, errorprone details, they can be very useful. they can enhance sound judgment,and they may make activities more accurate and efficient, but they cannotreplace sound judgment and a primary commitment to understandingand satisfying clients needs.engineering requires reconciling conflicting constraints and managinguncertainty. these constraints arise from requirements, from implementation considerations, and from the environment in which the softwaresystem will operate. they typically overconstrain the system, so the engineer must find reasonable compromises that reflect the clients priorities.moreover, the requirements, the available resources, and the operatingenvironment are most often not completely known in advance, and theymost often evolve as the software and system are designed. engineersgenerate and compare alternative designs, predict the properties of theresulting systems, and choose the most promising alternatives for furtherrefinement and exploration. finding sufficiently good costeffective solutions is usually preferable to optimization.engineering skills improve as a result of careful systematic reflection onexperience. a normal part of any project should be critical evaluation of thework. critical evaluation of prior and competing work is also important,especially as it informs current design decisions. one of the products ofsystematic reflection is codified experience, for example in the form of avocabulary of solution structures and the situations in which they areuseful. the designs known as software product lines or software productfamilies define frameworks for collections of related software systems;computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale155these are often created within a company to unify a set of existing productsor to guide the development of a nextgeneration product.human, social, and economic fundamentalsthe commitment to satisfying clients needs and managing effectivedevelopment organizations that guides software engineering businessdecisions is the organizational and cognitive knowledge about the humanand social context, together with specific knowledge about humancomputer interaction techniques.technology improves exponentially, but human capability does not. themooreslaw improvements in cost and power of computation (see hill inchapter 2) have enabled an unprecedented rate of improvement in technicalcapability. unfortunately, the result has often been software products thatconfuse and frustrate their users rather than providing correspondingimprovements in their efficiency or satisfaction. software developers areincreasingly aware of the need to dedicate part of the increase in computing capability to simplifying the use of the morecapable software byadapting systems to the needs of their users.cost, time, and business constraints matter, not just capability. much ofcomputer science focuses on the functionality and performance properties of software, including not only functional correctness but also, forexample, speed, reliability, and security. software engineering must alsoaddress other concerns of the client for the software, including the cost ofdevelopment and ownership, time to delivery, compliance with standardsand regulations, contributions to policy objectives, and compatibility withexisting software and business processes. these factors affect the systemdesign as well as the project organization.software development for practical softwareintensive systems usually dependson teamwork by creative people. both the scale and the diversity of knowledge involved in many modern software applications require the effortand expertise of numerous people. they must combine software designskills and knowledge of the problem domain with business objectives,client needs, and the factors that make creative people effective. as aresult, the technical substance of software engineering needs an organizational setting that coordinates their efforts. software development methodsprovide guidance about project structure, management procedures, andinformation structures for tracking the software and related documents.software engineering researchsoftware engineering researchers seek better ways to develop practicalsoftware, especially software that controls largescale softwareintensivecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.156computer science: reflectionssystems that must be highly dependable. they are often motivated by theprospect of affecting the practice of software development, by findingsimpler ways to deal with the uncertainties of wicked problems, and byimproving the body of codified or scientific knowledge that can be appliedto software development.scientific and engineering research fields can be characterized byidentifying what they value:what kinds of questions are interesting?what kinds of results help to answer these questions, and whatresearch methods can produce these results?what kinds of evidence can demonstrate the validity of a result,and how are good results distinguished from bad ones?software engineering research exhibits considerable diversity alongthese dimensions. understanding the widely followed research strategieshelps explain the character of this research area and the reasons softwareengineering researchers do the kinds of research that they do.physics, biology, and medicine have wellrefined public explanationsof their research processes. even in simplified form, these provide guidance about what counts as good research both inside and outside thefield. for example, the experimental model of physics and the doubleblind studies of medicine are understood, at least in broad outline, notonly by the research community but also by the public at large. in addition to providing guidance for the design of research in a discipline, theseparadigms establish the scope of scientific disciplines through a socialand political process of boundary setting.software engineering, however, is still in the process of articulatingthis sort of commonly understood guidance. one way to identify thecommon research strategies is to observe the types of research that areaccepted in major conferences and journals. these observations here arebased specifically on the papers submitted to and accepted by the international conference on software engineering;5 they are generally representative of the field, though there is some dissonance between researchapproaches that are advocated publicly and those that are accepted inpractice. another current activity, the impact project,6 seeks to trace the5mary shaw, 2003, writing good software engineering research papers, proceedings ofthe 25th international conference on software engineering (icse 2003), ieee computer society,pp. 726736.6impact project panel, 2001, determining the impact of software engineering researchupon practice, panel summary, proceedings of the 23rd international conference on softwareengineering (icse 2001), ieee computer society, p. 697.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.building computing systems of practical scale157influence of software engineering research on practice; the emphasis thereis on the dissemination of research rather than the research strategiesthemselves.questions software engineering researchers care aboutgenerally speaking, software engineering researchers seek better waysto develop and evaluate software. development includes all the syntheticactivities that involve creating and modifying the software, including thecode, design documents, documentation, and so on. evaluation includesall the analytic activities associated with predicting, determining, andestimating properties of the software systems, including both functionalityand extrafunctional properties such as performance or reliability.software engineering research answers questions about methods ofdevelopment or analysis, about details of designing or evaluating a particular instance, about generalizations over whole classes of systems ortechniques, or about exploratory issues concerning existence or feasibility.the most common software engineering research seeks an improvedmethod or means of developing softwarethat is, of designing, implementing, evolving, maintaining, or otherwise operating on the softwaresystem itself. research about methods for reasoning about software systems, principally analysis of correctness (testing and verification), is alsofairly common.results software engineering researchers respectthe tangible contributions of software engineering research may beprocedures or techniques for development or analysis; they may bemodels that generalize from specific examples, or they may be specifictools, solutions, or results about particular systems.by far the most common kind of software engineering research resultis a new procedure or technique for development or analysis. models ofvarious degrees of precision and formality are also common, with bettersuccess rates for quantitative than for qualitative models. tools and notations are well represented, usually as auxiliary results in combinationwith a procedure or technique.evidence software engineering researchers acceptsoftware engineers offer several kinds of evidence in support of theirresearch results. it is essential to select a form of validation that is appropriate for the type of research result and the method used to obtain theresult. as an obvious example, a formal model should be supported bycomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.158computer science: reflectionsrigorous derivation and proof, not by one or two simple examples. yet, asimple example derived from a practical system may play a major role invalidating a new type of development method.the most commonly successful kinds of validation are based on analysisand realworld experience. wellchosen examples are also successful.additional observationsas in other areas of computer science, maturation of software engineering as a research area has brought more focused, and increasinglyexplicit, expectations for the quality of researchcare in framing questions, quality of evidence, reasoning behind conclusions.software engineering remains committed to developing ways tocreate useful solutions to practical problems. this commitment to dealingwith the real world, warts and all, means that software engineeringresearchers will often have to contend with impure data and undercontrolled observations. most computer science researchers aspire to resultsthat are both theoretically well grounded and practical. unfortunately,practical problems often require either the simplification of the problemin order to achieve theoretically sound conclusions or else the sacrifice ofcertainty in the results in favor of results that address the practical aspectsof the problem. software engineering researchers tend to choose the lattercourse more often than the former.the community of computer users seems to have a boundless appetite for informationprocessing capability. fueled by our collective imagination, this appetite seems to grow even faster than mooreslaw technology growth. this demand for larger scale and complexity, coupled withan increasing emphasis on dependability and ease of useespecially forusers with little computer traininggenerates new problems, and evennew classes of problems for software engineering.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.1598research behindeveryday computationcomputer science research has led to the emergence of entirely newcapabilities used by millions of ordinary people today. usingcomputers to typeset text was probably foreseeable, but the easewith which anyone can become a smallscale publisher by using the webwas harder to envision. automating accountantsõ ledger paper wasperhaps natural, but the ability to disintermediate the financialanalysisfunctionñmoving control from a central dataprocessing department to aspreadsheet on an executiveõs deskñcould not have been. creating anindex of documents stored at various internet locations so that they couldbe shared by a community of physics researchers was, perhaps, notastounding, but leveraging the power of distributed access, the universalnamespace of the uniform resource locator (url), and easily usablebrowsers together with new algorithms for searching has led to an explosion of informationretrieval opportunities that has changed forever theway we do business.computers now perform most of the documentpreparation tasks thatused to be handled by secretaries: typing drafts, correcting drafts, positioning figures, formatting final copy, and the like. ullman traces the30year evolution of textformatting programs from quickanddirty typesetters created by graduate students to avoid typing their theses to thepowerful documentformatters of the present day.similarly, foley shows how todayõs spreadsheets, which began as aprogram to help accountants manage ledger sheets, astonished thecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.160computer science: reflectionscomputerscience world by becoming the òkiller applicationó that transferred effective control of computing to nonprogrammers in all disciplines.one of the principal personal uses of computers today is searchingthe internetõs wealth of online information. the idea of informationretrieval is far from new, but the ability to dedicate computing power toamplifying human effort has led to new wrinkles, such as attempts toestablish the relevance of information based on the number of peoplereferring to it. norvig explains the technology behind modern internetsearches.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.research behind everyday computation161how you got microsoft wordjeffrey ullman, stanford university and gradience corporationthose born after about 1975 cannot imagine the cumbersome processby which formal documents, ranging from business letters to books, wereproduced. drafts were handwritten and then typed by a secretary usinga typewriter. if corrections were needed, small errors could be handled byerasing or by applying whiteout, followed by typing of the correct words.but lengthchanging errors required that the entire page be retyped andthe errorchecking process be repeated. when computers first becamewidespread in businesses and schools, many people started typing theirdocuments on punch cards. similar to the cards that were used in thefamous florida vote of 2000, the cards represent letters when you punchout certain holes, creating òchad.óa document such as a thesis could be typed on cards, fed to thecomputer, and printed lineforline on a printer. early printers were liketypewriters, but faster. this arrangement solved the problem of handlingsmall errors, since only one or a few cards would have to be repunched.several early computerscience students saw the potential for doing better,especially as they confronted the daunting task of typing their phd theses.in 1964, jerry saltzer at the massachusetts institute of technology createdfor this purpose a pair of programs: typeset, which was an early form ofa text editor, and runoff, which was an improved formatting system. bobbalzer, at carnegie mellon university, created software similar to runoff,called lead (list, edit, and display), at about the same time. programslike runoff and lead not only reproduced what was on punch cards, butalso formatted the document by placing on one line as many words aswould fit, typically a maximum of 80 characters per line. this advancesolved the problem of having to retype large amounts of text when wordswere inserted or deleted. special commands, which were not part of thetext, could be inserted to control matters such as justification, that is,alignment of text on the right, as in a book, as well as on the left.wordprocessing software at bell labsthe typeset/runoff system inspired a group of researchers at belllaboratories, leading to joe ossannaõs nroff (new runoff) program. aòmacroó capability was added to allow repetitive formatting concepts,such as section headers or indented paragraphs, to be defined once andused easily many times. the òmsó macro package by mike lesk waswidely used as a standard for formatting of text. but the printed outputcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.162computer science: reflectionsdevice was still a soupedup typewriter, with its single size and font ofletters.in the early 1970s, work on typesetting at bell labs had progressed tothe point where the labs purchased a cat phototypesetting machine, ofthe kind used at the time to produce newspapers, for example. this deviceallowed printing to be done in several fonts and font sizes. unlike todayõslaser printers, it printed only on photographic paper, which was thenreproduced by one of several processes, such as a òxerox machine.ónroff evolved into troff (typesetter runoff) to allow documents to becreated that were then typeset on film. for the first time, it became possiblefor casual authors to produce a letter or report that looked as if it werepart of a book.however, troff was not up to the requirement faced by many scientists and engineers to set mathematical text easily. while the catprinter could, for example, print a subscript in the correct (smaller) sizeand below the line of text, there was no convenient way to tell it to do so.eqn, a program written by brian kernighan, solved that problem bytaking expressions that represented how the equation or formula shouldlook (e.g., òx sub 1ó for an x with a subscript 1, x1) and turning them intotroff, while leaving everything that wasnõt equations intact, for laterprocessing by troff. eqn is in effect an atypical programming language, since it does not let you write generalpurpose algorithms, as thebestknown programming languages do, but it helps you deal with aparticular, difficult problem, that of describing how mathematical expressions should look on the page. it is an excellent example of how theeffective design of a language or notation for a problem leads to tools thatpeople find useful and powerful. a key borrowing of eqn from the moretraditional programming languages is recursion, one of the central themesof computing, where things are defined partially in terms of themselves.for example, eqn allows one to say not only òx sub 1ó but òx subanythingó or even òanything sub anything,ó where the òanythingsó canthemselves have subscripts or any of the other operations whereby mathematical expressions are built up, such as by horizontal lines for division.thus, one can say things like òx sub {n sub i} ó to represent an x whosesubscript is itself a subscripted òn sub ió ( xni).several other components of modern word processors got their startin the same bell labs group. the first spellcheckers were developed tocompare words in the document with a list of òacceptableõõ words, modified by rules for plurals, tense modifications, and so on. also, lorindacherry developed the first grammarchecker (dict, for òdictionó) byapplying a table of rules to the document.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.research behind everyday computation163typesetting research at xerox parcin the mid1970s, a team of researchers at xerox parc (palo altoresearch center) made several important contributions toward the waywe now create documents. one was the development of the laser printer.while the xerox parc researchers could and did build a laser printer,they could not do so at a price people would pay. they estimated that itwould cost over $100,000 at that time for the printer we now buy for a fewhundred dollars. nevertheless, in order to verify their conjecture that theexistence of these devices would change the way people dealt with documents, they built prototypes of a laser printer, called dover, and gavethem to several universities. as envisioned, the dover changed markedlythe way students and faculty prepared papers. also as envisioned, thecost for building such devices has dropped dramatically over the years,and the dream of a laser printer on every desk has become real.a second innovation from the parc group was the wysiwyg(wizzywig, or òwhat you see is what you getó) editor. a prototype texteditor called bravo allowed users to see the document they were creating.in one sense, the differences between bravo and troff were small. forexample, each allowed you to specify that certain text should be italics. inbravo, as in msword and other textprocessing systems today, one couldchange fonts, say to italics, as one typed. in contrast, troff required youto type a command (ò.itó) to say òchange to italics.ó bravo allowed you tosee that you were really typing italic text (and would also let you see ifyou forgot to return to roman text when you should), while with troff,you could only tell whether you got it right when you printed your document later. on the other hand, bravo was less adept at setting complexmathematical formulas, since it did not possess the recursive descriptioncapabilities of eqn, and in fact, the successors of bravo up to this dayhave not recaptured the power of eqn in wysiwyg mode. while it mayseem obvious that for nonmathematical typesetting, wysiwyg editorsare a wonderful tool, the existence of such editors depends on developments in a number of different areas. of course, processors had to becomefast enough that it became economical to devote a processor to a singletask like editing. computer graphics technology needed to advance to astate where it was possible to offer the user a video terminal as a displayunit. operating systems needed to incorporate windows as a primitiveconcept and manage rapid communication and drawing of characters,between keyboard, processor, and display.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.164computer science: reflectionsthe tex projectat about the same time as the xerox activities, don knuth at stanfordbegan a broad study of how computers can improve the preparation andtypesetting of documents. by this time, it was clear that the font was acritical computer object. descriptions of the letters in a font have to bedeveloped so the same look can be displayed on a video terminal and alsoprinted, perhaps at a different size, on one of many different printingdevices. for this purpose, knuth developed a system called metafont fordescribing the shapes of letters and other characters in a way that allowseasy generation of the font in different sizes and slants. knuth also developed tex, a unified system for typesetting that incorporates the capabilities of troff and several specialized systems such as eqn. in addition,tex increases the care with which paragraphs and pages are laid out.texõs paragraphing algorithm looks at the words of an entire paragraphbefore deciding on linebreaks, and thereby improves the look of the textwhen compared with ògreedyó algorithms that look at text a line at a timeand fit whatever they can on each line in succession. the tex paragraphingalgorithm is an excellent example of a general computational approachcalled òdynamic programming.ó it works roughly as follows. first, weneed a measure of how well words fit on a line. a typical approach, evenfor greedy algorithms, is to decide on the optimum space between wordsand then assign a òbadnessó to a line proportional to the square of thedeviations of the spaces from the optimal. the objective then becomes tominimize the total badness of all the lines in a paragraph.now the number of ways to break a paragraph of, say, 100 words into10 lines is astronomical; it is the number of ways to pick 9 line breaksamong the 99 positions between the words, which is ò99 choose 9,ó orabout 1.7 trillion. it is clearly not feasible to consider all possible ways tobreak paragraphs. fortunately, as is often the case in computer science,the obvious way to do something is neither the best way nor the onlyway. in this case, dynamic programming let knuth find an algorithm thattakes time proportional to the length of the paragraph, rather than tosome exponential in the size of the paragraph.the dynamic programming algorithm fills out a table of least badnesses, not just for the paragraph itself, but for all paragraphs that couldbe formed from a tail or suffix of the sequence of words in the paragraph.that is, for each i, starting at 1 and going up to the number of words in theparagraph, the entry for i is the least badness of any division of the last iwords into lines. suppose we have computed this value for 1, 2, . . . up toi ð 1. to decide on the best line breaks for the last i words, we consider, forall k small enough that k words can fit on one line, the cost of placing thefirst k of the i words on one line (using the formula for the òbadnessó ofcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.research behind everyday computation165any particular line), plus the badness of the best breaking for the remaining i ð k words (as given by the table). as soon as i reaches the totalnumber of words in the paragraph, we know the cost of the best linebreaking, and we can easily figure out what that breaking is, by storing inthe table, as we go, the winning value of k for each i.in addition, one of knuthõs students, frank liang, invented an elegantalgorithm for choosing the points at which words should be hyphenated.the dictionary tells us how to hyphenate in a straghtforward way: it listsevery word it knows and shows where the hyphens may go. in additionto using a lot of space, that approach is of little use if youõve just inventeda word like òcyberpunk,ó or you need to hyphenate a proper noun. liangõsapproach was to summarize the rules of hyphenation, using a simplelanguage for expressing those rules and their priorities.imagine a collection of rulesñitõs good to hyphenate here; itõs notgood to hyphenate there. for instance, it is generally a bad idea to hyphenatebetween the òaó and òtó in ò. . . at . . . .ó but if the word happens to fit thepattern ò. . . ation . . .,ó then it is actually a good idea to hyphenatebetween the òaó and òtó if the word has to be hyphenated. each rule hasa number associated with it, and this number is given to those pointsbetween letters to which the rule applies. thus, a point might acquireseveral different numbers, from different rules. if so, take the largest ofthe numbers and ignore the rest. the magic of liangõs algorithm is this: ifthe number associated with a point is odd, then you may hyphenatethere; if it is even, you may not. for example, the rule about ò. . . at . . .ómight receive a low, even number, like 2. that number suggests that inthe absence of any more specific pattern, letõs not hyphenate here. therule about ò. . . ation . . . ,ó on the other hand, would have a high, oddnumber, such as 9. thus, the rule strongly suggests òationó is a goodhyphenation, and overrules the more general idea that òató is bad. werewe to have a specific word in mind with pattern ò. . . ation . . .ó where wedid not want the hyphen between the òaó and òtó (there are no suchexamples in common english), we could make that word a pattern with astill higher, even number, like 24.word processing todayif you are familiar with msword or a similar application such aswordperfect, you will know that many of the ideas described here areavailable to you. the application breaks paragraphs into lines in a waythat allows alignment at both left and right (although it does not use thetex algorithm for doing so). it allows changes of font at the press of abutton on the screen. it allows global changes in the way elements such assections are displayed, just as the early bell labs macro packages did. itcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.166computer science: reflectionshas a rulebased system to catch many grammatical errors, like dict did,and catches spelling errors in a similar way. it has a wysiwyg interfacelike the old bravo editor, and it easily passes its document to a laserprinter, a descendant of the dover printers of 25 years ago. the systemprovides a virtually unlimited number of fonts.perhaps as a reflection on how prevalent word processing has becomein ordinary discourse, neither microsoft nor other manufacturers of wordprocessing applications have felt the need to integrate mathematical typesetting nearly as closely as did eqn. ironically, many scientists and engineers still prefer to write using tex (which has all the power of eqn andmore), or its variant latex, a system developed by leslie lamport.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.research behind everyday computation167visicalc, spreadsheets, and programmingfor the masses, or òhow a killer app was bornójames d. foley, georgia institute of technologyan important part of computing is the study of human computerinteractionñhci. individuals and organizations generally buy computerhardware and software expecting that the computer will allow them to dosomething entirely new, or more accurately or completely or quickly oreconomically or with more enjoyment than without the computer. therole of hci is to develop computer capabilities that match what people wantto do, so that the computer can be useful to individuals and organizations.1at least three important software innovations in the last 30 years havebeen hci tours de force:¥the development of window managers at sri and parc, whichwere precursors to the macintosh and microsoft window manager software that allowed millions of people to easily use personal computers.¥the development of the first web browser by tim bernerslee in1991;2 the development of mosaic (the first widely available browser) atthe university of illinois in 1993, and their subsequent progeny netscapenavigator and internet explorer, which made the world wide webaccessible to millions of people as opposed to the thousands who hadbeen using it originally.¥visicalc, developed by dan bricklin and bob frankston in 1979.prior to visicalc, personal computers were bought mostly by hobbyists,game players, and to teach programming. the visicalc spreadsheet program (whose commercial successors include microsoftõs excel) causedmany financial analysts, business people, accountants, and students tobuy personal computers (first the apple ii and later the ibm pc). as such,visicalc was the òkiller appó that dramatically increased sales of thesecomputers.visicalcin this essay we focus on visicalc, for several reasons. first, visicalcdid spark the sales of personal computers. second, visicalc introduced1see, for example, brad a. myers, 1998, òa brief history of human computer interactiontechnology,ó acm interactions 5(2):4454. available online at http://www2.cs.cmu.edu/~ mulet/papers/uihistory.tr.html.2see http://www.w3.org/history.html.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.168computer science: reflectionsmany of its users to computer programming without their ever realizingthey were doing computer programming, thereby providing a powerfultool in an easytouse package. and finally, as with window managersand mosaic, visicalc could not have happened without a considerablebody of prior computer science research.why did visicalc spark pc sales? why did people rush to buy acomputer in order to use visicalc? simply because visicalc met a marketneed (a human need) to organize and calculate numbers more rapidlyusing an electronic spreadsheet than one could do by hand with a paperbased spreadsheet and an adding machine or an electronic calculator.there was a pentup demand, and visicalc together with the pc met thatdemand. as time went on, other uses for visicalc developedñmany of usorganize lists of address and other information in the convenient row andcolumn format of our favorite spreadsheet, perhaps never calculating asingle number. these uses may not have been what bricklin and frankstonenvisioned, but are typical of the way in which tools are adapted by theirusers to meet needs not envisioned by the toolsõ designers.what about this notion that visicalc users are actually programming?well, the òequationsó of visicalc, by which one states that a particularcell of the spreadsheet is to have a value computed from other cells, areessentially the same as the assignment statements and expressions usedin all modern programming languages. the visicalc equation ò= (b6 +b7)/(c4 + c5) ð d9ó placed in cell b9 has the same effect3 as the c programming language statement b9 = (b6 + b7)/(c4 + c5) ð d9 or the pascalstatement òb9 := (b6 + b7)/(c4 + c5) ð d9ó or the java statement òb9 =(b6+b7)/(c4 + c5) ð d9.ó but, referring back to the concept of the universal machine (see kleinberg and papadimitriou in chapter 2), it is notthe case that this limited form of programming in visicalc is the equivalent of a general programming capability.visicalc allowed and encouraged its users to program without realizingthat they were programming, using the simpletounderstand formulaethat could either be typed into a spreadsheet cell or be partially or in somecases completely entered simply by pointing to the cells in the spreadsheet that appear in the equation. hence, a reduction in the òfear of programmingó that often discouraged people from putting computers to use.how did visicalc draw on computer science research? in multipleways:3strictly speaking, as soon as any of the variables such as b6 or c5 change, the equation isreevaluated and the new value is placed in cell b9, whereas with the programming languages the assignment statement is executed only when the program flow of control comesto the statement. this means that the visicalc equation is a more powerful construct thanthe programming language assignment statement.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.research behind everyday computation1691. by using a visual òwhat you see is what you getó (wysiwyg) userinterface, in which one sees the spreadsheet and can directly manipulate cells inthe spreadsheet by pointing at them.dan bricklin states in his web pages4 that he was aware of and influenced by the early research work of doug engelbart on text editors usinga mouse, and that he had probably seen a demonstration of xerox parcõsalto computer system, which was in turn also influenced by engelbart.engelbart and parc are the critical early developers of the key elementsof contemporary windoworiented user interfaces found in the macintoshand microsoft operating systems.engelbartõs passion was to augment the human intellect by providingcomputer tools to help organize information, compose and modify documents, and work with others at a distance while doing this. with thisvision, he and a group (up to 17 researchers) developed nls (onlinesystem) between 1962 and 1969 at the stanford research institute.along the way, they invented the mouse as a way to interact with thecomputer by pointing rather than by the much slower process of movinga cursor with key strokes; developed tiled windows (nonoverlapping, asopposed to the overlapping windows of microsoft windows and theapple macintosh operating system); developed a text editor with commands to move, copy, and delete individual characters, words, or blocksof text (see ullman in chapter 8); and provided hyperlinks (just like in theworld wide web) and keyword searches (similar to the searchengineconcept of google and alta vista).englebartõs research style is not unusual; set a big, hardtoachievegoal, and create multiple tools needed to achieve that goal. the samething happened at xerox parc. the alto workstation software developed at parc starting in the early 1970s was the prototype for the applemacintosh and microsoft windows user interfaces. alto had many of thefeatures seen in these and other contemporary window interfaces: windows,icons, menus, copy and paste commands, and directmanipulation textand graphics editors.2. by using òprogramming by exampleó to simplify and speed up the processof entering equations.visicalc was the first widely used instance of òprogramming byexample,ó in which the user does programming by demonstrating operations to the computer rather than by specifying what is to be done via4see http://www.bricklin.com/visicalc.htm.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.170computer science: reflectionssome programming language. for example, to indicate that a cell shouldalways have in it the sum of several other cells, one would point at the cellthat has the sum, and then successively to the cells that are to be summed.the idea of programming by example was developed in a slightlydifferent form by moshe zloof of ibm research a few years earlier. zloofwanted to allow users to specify database queries without having to learna query programming language (such as sql). his insight was to recognize that if a user gave just an example of the query results such as a list ofpart numbers, descriptions, inventory quantity, and selling price, then thequery language statement(s) to obtain all of the results could be generatedfrom the example.visicalc applied the concept in a slightly different way than did zloof,thus both drawing on and contributing to the storehouse of computerscience knowledge (a typical pattern for cs and other research). specifically, visicalc allows the user to enter a formula (such as given above) bypointing at the cells that occur in the formula. so to enter the formulab2+c5 in cell b1, the user points at cell b1, types = to indicate that aformula is being specified, then points at cell b2, then points at cell c5,and ends by typing the enter key. the + in the formula is assumed andentered automatically, as the most common choice. if the product isdesired rather than the sum, the user types ò*óafter pointing at cell b2 andbefore pointing at cell c5.this seemingly simple idea greatly simplifies the process of enteringformulae, and it corresponds nicely to the intuition that, given a choicebetween pointing at something and typing the name of something (in thecase at hand, the spreadsheet cell), we humans will tend to point ratherthan name, as the easier of the two alternatives.3. by applying finite state machine notation as a tool for designing theprogramñwhat inputs were valid in each òstate,ó what response the computerwould make to each input, and what new program òstateó would result.the understanding of how to use state diagrams for specifying thebehavior of interactive programs was first developed by william newmanin 1968 as a researcher at imperial college, london. he, in turn, drew onthe fundamental notion of finite state machines that had been developedas yet another computer science concept. the idea as applied to visicalcis that the program can be in one of a finite number of states. in each state,only certain user actions (such as entering information into a cell, orselecting a command) are possible. each such action takes the program toanother state (or back to the same state) and at the same time causes somechange in what the user sees on the display.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.research behind everyday computation1714. by using language parsing techniques.computer scientists have developed techniques for taking expressionssuch as the earlier example of the expression (b6 + b7)/(c4 + c5) ð d9stored in cell b9 and actually performing the calculations in the appropriatesequence. with the example expression, the actual sequence of arithmeticoperations would be:add cells b6 and b7, saving the result in a place called temp1.add cells c4 and c5, saving the result in a place called temp2.add temp1 and temp2, saving the results in temp1.add temp1 and cell d9, saving the results in cell b9.this may seem simple, but try writing out a general procedure thatwill work with an arbitrary expression, with many parentheses and divisions and multiplications! not so easy, is it?the next òkiller appówhat will be the successor to visicalc, to windowing systems, to webbrowsers that will provide new capabilities to millions and tens of millionsof computer users? there are many possibilities; i believe that whateverthe next major development, the next òkiller app,ó might be, it will:¥meet an individual or organizational need.¥draw on the research of many, many computer scientists.¥leverage the existing hardware and software infrastructure, andaccelerate their growth (just) as visicalc leveraged and accelerated theavailability of pcs and email leveraged the internet and web browsersleveraged the world wide web protocols.we also note that only web browsing was well anticipated as a killerappñanticipated by vannevar bush in the 1940s and researched by anumber of prominent computer scientists from the late 1960s into the1980s.many candidates for the next òkiller appó recognize that the nowcommon use of computers by people sitting at a desk may not be themajor way of using computers in the future. a fundamentally newdirection for computing is evolving: i call this direction offthedesktopcomputing, in the sense that it will not be desktop computers but otherforms of computers that will create the next revolution. others call itubiquitous or embedded computing, in the sense that computing will beeverywhere (i.e., ubiquitous) and embedded in all manner of devices andcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.172computer science: reflectionsequipment. this is already happening. contemporary cell phones have acomputer that is as powerful as desktop computers of 10 years ago. tvsettop boxes have as much computing power as powerful engineeringworkstations of 5 years ago. the typical automobile has 10 to 20 computers with aggregate power comparable to a contemporary desktop pc.the most widespread hardware and software infrastructure thatmight be leveraged is the wireless phone system. it is already pervasive,and it is computer intensive. it is already heavily used for short textmessages, for games, and to obtain information. and, with the small sizeof wireless phones and their keypads, and the integration of wirelessfunctionality into personal digital assistants and with the clear benefit ofvoice recognition in using small devices, a trend can surely be predicted.more generally, voice recognition will become more and more pervasiveas algorithms improve and as computing power and memory becomeever more available.another hardware and software infrastructure that will be leveragedis the world wide web itself. with vast amounts of information onlineand more and more of the world citizenry connected, the use of intelligentassistants to find information, to make sense of information, and to useinformation will surely expand. current examples include comparisonshopping, money management, and travel arrangements. life has becomemore complex and fastmoving; perhaps computers can help simplify lifein some ways.automobiles already have dozens of computers, many networkedtogether. they serve just one of two purposesñoperating the car, orentertaining/communicating with the driver and passengers. many carsalready have builtin wireless phones, global positioning systems, limitedvoice recognition, and navigation aids. many people spend tens of hoursa week in a car, and there are significant personal and societal benefits tofurther leveraging the automotive computing infrastructure to make driving safer, to connect us with the outside world, and to entertain us.we spend significant amounts of time at home. many predictions aremade for computerbased home entertainment systems and the broaderòwired home,ó but the home has a very weak hardware and softwareinfrastructureñnothing like that of the car or the typical office workingenvironment or the wireless phone. i believe that homes will ultimatelyhave networked appliances, switches, lights, motion sensors, cameras,microphones, and so on. while we do not yet have a killer app, we havecandidates such as the use of computers to allow older people to livealone at home for more years before having to live with their children orin a nursing home; energy management; and home security.we should not neglect new uses of the internet and the web. amybruckman, in her essay in chapter 7, nicely likens our use of the web tocomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.research behind everyday computation173the early days when the automobile was still known as the òhorselessbuggy.ó at that time, nearly a century ago, one could not imagine the carsand highways of today.while we can and should be proactive in exploring these possibilitiesand their implications for our lives, it is also true that only time will tell.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.174computer science: reflectionsinternet searchingpeter norvig, google inc.in 300 bc, the library of alexandria held some half million scrollsand served as a cultural center for the world, uniting the thoughts ofgreek, egyptian, macedonian, and babylonian culture. according to theletter of aristeas, the library had òa large budget in order to collect, ifpossible, all the books in the world.ó whole fields of study such asgeometry and grammar grew from the scholars who had the fortune totravel to and study at alexandria.in 2004 ad, the internet forms a distributed library of billions ofpages, one that is accessible to anyone, anywhere in the world, at the clickof a mouse. every day hundreds of millions of òtripsó to the library startwith a query to an internet search engine. if i want to research a topic suchas òlibrary of alexandria,ó i can type those words to a search engine andin less than a second have access to a list of 31,000 pages, sorted by theirusefulness to the query. this unprecedented ease of access to informationhas revolutionized the way research is done by students, scientists,journalists, shoppers, and others. it opens up an online marketplace ofproducts, services, and ideas that benefits both information providersand seekers; sellers and buyers; consumers and advertisers.how is it that an internet search engine can find the answers to aquery so quickly? it is a fourstep process:1.crawling the web, following links to find pages;2.indexing the pages to create an index from every word to everyplace it occurs;3.ranking the pages so the best ones show up first; and4.displaying the results in a way that is easy for the user to understand.crawling is conceptually quite simple: starting at some wellknownsites on the web, recursively follow every hypertext link, recording thepages encountered along the way. in computer science this is called thetransitive closure of the link relation. however, the conceptual simplicityhides a large number of practical complications: sites may be busy ordown at one point and come back to life later; pages may be duplicated atmultiple sites (or with different urls at the same site) and must be dealtwith accordingly; many pages have text that does not conform to thestandards for html, http redirection, robot exclusion, or other protocols; some information is hard to access because it is hidden behind aform, flash animation, or javascript program. finally, the necessity ofcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.research behind everyday computation175crawling 100 million pages a day means that building a crawler is anexercise in distributed computing, requiring many computers that mustwork together and schedule their actions so as to get to all the pageswithout overwhelming any one site with too many requests at once.a search engineõs index is similar to the index in the back of a book: itis used to find the pages on which a word occurs. there are two maindifferences: the search engineõs index lists every occurrence of every word,not just the important concepts, and the number of pages is in the billions,not hundreds. various techniques of compression and clever representation are used to keep the index òsmall,ó but it is still measured in terabytes(millions of megabytes), which again means that distributed computing isrequired. most modern search engines index link data as well as worddata. it is useful to know how many pages link to a given page, and whatthe quality of those pages is. this kind of analysis is similar to citationanalysis in bibliographic work and helps establish which pages areauthoritative. algorithms such as pagerank and hits are used to assigna numeric measure of authority to each page. for example, the pagerankalgorithm says that the rank of a page is a function of the sum of the ranksof the pages that link to the page. if we let pr(p) be the pagerank of pagep, out(p) be the number of outgoing links from page p, links(p) be the setof pages that link to page p, and n be the total number of pages in theindex, then we can define pagerank by pr(p)npr(i)/out(i)links(p)rri/()1where r is a parameter that indicates the probability that a user will choosenot to follow a link, but will instead restart at some other page. the r/nterm means that each of the n pages is equally likely to be the restartpoint, although it is also possible to use a smaller subset of wellknownpages as the restart candidates. note that the formula for pagerank isrecursiveñpr appears on both the right and lefthand sides of the equation. the equation can be solved by iterating several times, or by employing algorithms that compute the eigenvalues of a (4billionby4billion)matrix.the two steps above are query independentñthey do not depend onthe userõs query and thus can be done before a query is issued, with thecost shared among all users. this is why a search takes a second or less,rather than the days it would take if a search engine had to crawl the webanew for each query. we now consider what happens when a user typesa query.consider the query [ònational academiesó computer science], wherethe square brackets denote the beginning and end of the query, and thecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.176computer science: reflectionsquotation marks indicate that the enclosed words must be found as anexact phrase match. the first step in responding to this query is to look inthe index for the hit lists corresponding to each of the four wordsònational,ó òacademies,ó òcomputer,ó and òscience.ó these four lists arethen intersected to yield the set of pages that mention all four words.because ònational academiesó was entered as a phrase, only hits wherethese two words appear adjacent and in that order are counted. the resultis a list of 19,000 or so pages.the next step is ranking these 19,000 pages to decide which ones aremost relevant. in traditional information retrieval this is done by countingthe number of occurrences of each word, weighing rare words moreheavily than frequent words, and normalizing for the length of the page.a number of refinements on this scheme have been developed, so it iscommon to give more credit for pages where the words occur near eachother, where the words are in bold or large font, or in a title, or where thewords occur in the anchor text of a link that points to the page. in additionthe queryindependent authority of each page is factored in. the result isa numeric score for each page that can be used to sort them bestfirst. for ourfourword query, most search engines agree that the computer science andtelecommunications board home page at www7.nationalacademies.org/cstb/ is the best result, although one preferred the national academiesnews page at www.nas.edu/topnews/ and one inexplicably chose a yearold news story that mentioned the academies.the final step is displaying the results. traditionally this is done bylisting a short description of each result in ranksorted order. the description will include the title of the page and may include additional information such as a short abstract or excerpt from the page. some search enginesgenerate queryindependent abstracts while others customize eachexcerpt to show at least some of the words from the query. displaying thiskind of querydependent excerpt means that the search engine must keepa copy of the full text of the pages (in addition to the index) at a cost ofseveral more terabytes. some search engines attempt to cluster the resultpages into coherent categories or folders, although this technology is notyet mature.studies have shown that the most popular uses of computers are email,word processing, and internet searching. of the three, internet searchingis by far the most sophisticated example of computer science technology.building a highquality search engine requires extensive knowledge andexperience in information retrieval, data structure design, user interfaces,and distributed systems implementation.future advances in searching will increasingly depend on statisticalnaturallanguage processing (see lee in chapter 6) and machinelearning(see mitchell in chapter 6) techniques. with so much datañbillions ofcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.research behind everyday computation177pages, tens of billions of links, and hundreds of millions of queries perdayñit makes sense to use datamining approaches to automaticallyimprove the system. for example, several search engines now do spellingcorrection of user queries. it turns out that the vast amount of correctlyand incorrectly spelled text available to a search engine makes it easier tocreate a good spelling corrector than traditional techniques based on dictionaries. it is likely that there will be other examples of text understanding that can be accomplished better with a dataoriented approach; this isan area that search engines are just beginning to explore.recommended readingchakrabarti, s., b. dom, d. gibson, j. kleinberg, s.r. kumar, p. raghavan, s. rajagopalan,and a. tomkins, 1999, òhypersearching the web,ó scientific american, june, pp. 5460.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.1799personal statements of passionabout computer science researchwe close with some reflections by computer scientists on thenature of the field and the sources of their passion in their ownwork.sussman identifies a distinctive characteristic of computer science asòprocedural epistemologyóñthe representation of imperative knowledgethat allows us to treat a symbolic expression as data for purposes ofanalysis and as procedure for purposes of dynamic interpretation. theability to represent data and procedures in symbolic form provides theenormous power of precise expression and reasoning.newell writing in 1976 is enchanted by the notion that computertechnology may incorporate intelligent behavior into objects. we cancreate òfrozen action to be thawed when needed,ó and the action can beconditional on the state of the world.we close with one of the oldest attempts among computer scientiststo define computer science. in a reprinted letter first published in 1967,newell, perlis, and simon characterize computer science broadly as thestudy of the phenomena surrounding computers.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.180computer science: reflectionsthe legacy of computer sciencegerald jay sussman, massachusetts institute of technologywe have witnessed and participated in great advances, in transportation, in computation, in communication, and in biotechnology. but theadvances that look like giant steps to us will pale into insignificance bycontrast with the even bigger steps in the future. sometimes i try to imagine what we, the technologists of the second half of the 20th century, willbe remembered for, if anything, hundreds of years from now.in the distant past there were people who lived on the banks of thenile river. each year the nile overflowed its banks, wiping out landboundaries but providing fertile soil for growing crops. as a matter ofeconomic necessity the egyptians invented ways of surveying the land.they also invented ways of measuring time, to help predict the yearlydeluge. similar discoveries were made in many places in the world. holdersof this practical knowledge were held in high esteem, and the knowledgewas transferred to future generations through secret cults. these earlysurveyors laid the foundation for the development of geometry (òearthmeasurementó in greek) by pythagoras and euclid and their colleaguesaround 350 bc. geometry is a precise language for talking about space. itcan be taught to children. (euclidõs elements has been used in this way formore than 2000 years.) it makes the children smarter, by giving themways of expressing knowledge about arrangements in space and time. itis because of these greeks that we can tell a child, òif you build it out oftriangles it will not collapse the way it does when you build it out ofrectangles.óthe rhind papyrus from egypt (c. 1650 bc) is the earliest documentthat we have that discusses what we now think of as algebra problems.diophantus, another greek, wrote a book about these ideas in the thirdcentury a.d. algebra was further developed by abu abdallah ibn musaalkhwarizmi (c. 780ðc. 850) and others. (note: òalgebraó = alõjabr is anarabic word meaning òthe recombining of broken parts.ó) algebra is alsoa precise language that gives us the ability to express knowledge aboutthe relationships among quantities, and to make deductions from thatknowledge, without necessarily knowing the values of those quantities.for a long time people were able to predict the motions of some of theheavenly bodies using ad hoc theories derived from observation andphilosophical considerations. claudius ptolemy wrote the almagest, afamous compendium of this knowledge, in the second century. about 350years ago descartes, galileo, newton, leibnitz, euler, and their contemporaries turned mechanics into a formal science. in the process theycomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.personal passion about computer science research181invented continuous variables, coordinate geometry, and calculus. wenow can talk about motion precisely. this achievement gives us the wordsto say such sentences as, òwhen the car struck the tree it was going50km/hour.ó now every child can understand this sentence and knowwhat is meant by it.in each of these cases there was an advance in human intelligence,ultimately available to ordinary children, that was precipitated by anadvance in mathematics, the precise means of expression. such advancesare preceded by a long history of informal development of practical technique. we are now in the midst of an intellectual revolution that promisesto have as much impact on human culture as the cases i have justdescribed.we have been programming universal computers for about 50 years.the practice of computation arose from military, scientific, business, andaccounting applications. just as the early egyptian surveyors probablythought of themselves as experts in the development and application ofsurveying instruments, so have we developed a priestly cult of òcomputer scientists.ó but, as i have pointed out (h. abelson, g.j. sussman,and j. sussman, structure and interpretation of computer programs, 2ndedition, mit press, cambridge, mass., 1996):computer science is not a science, and its ultimate significance haslittle to do with computers. the computer revolution is a revolution inthe way we think and in the way we express what we think. the essenceof this change is the emergence of what might best be called proceduralepistemologyñthe study of the structure of knowledge from an imperative point of view, as opposed to the more declarative point of viewtaken by classical mathematical subjects. traditional mathematicsprovides a framework for dealing precisely with notions of òwhat is.ócomputation provides a framework for dealing precisely with notionsof òhow to.ócomputation provides us with new tools to express ourselves. thishas already had an impact on the way we teach other engineering subjects. for example, one often hears a student or teacher complain that thestudent knows the òtheoryó of the material but cannot effectively solveproblems. we should not be surprised: the student has no formal way tolearn technique. we expect the student to learn to solve problems by aninefficient process: the student watches the teacher solve a few problems,hoping to abstract the general procedures from the teacherõs behaviorwith particular examples. the student is never given any instructions onhow to abstract from examples, nor is the student given any language forexpressing what has been learned. it is hard to learn what one cannotexpress.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.182computer science: reflectionsin particular, in an introductory subject on electrical circuits we showstudents the mathematical descriptions of the behaviors of idealized circuitelements such as resistors, capacitors, inductors, diodes, and transistors.we also show them the formulation of kirchoffõs laws, which describe thebehaviors of interconnections. from these facts it is possible, in principle,to deduce the behavior of an interconnected combination of components.however, it is not easy to teach the techniques of circuit analysis. theproblem is that for most interesting circuits there are many equations andthe equations are quite complicated. so it takes organizational skills andjudgment to effectively formulate the useful equations and to deduce theinteresting behaviors from those equations.traditionally, we try to communicate these skills by carefully solvingselected problems on a blackboard, explaining our reasoning andorganization. we hope that the students can learn by emulation, from ourexamples. however, the process of induction of a general plan fromspecific examples does not work very well, so it takes many examples andmuch hard work on the part of the faculty and students to transfer theskills.however, if i can assume that my students are literate in a computerprogramming language, then i can use programs to communicate ideasabout how to solve problems: i can write programs that describe the general technique of solving a class of problems and give that program to thestudents to read. such a program is precise and unambiguousñit can beexecuted by a dumb computer! in a nicely designed computer language awellwritten program can be read by students, who will then have a precise description of the general method to guide their understanding. witha readable program and a few wellchosen examples it is much easier tolearn the skills. such intellectual skills are very hard to transfer withoutthe medium of computer programming. indeed, òa computer language isnot just a way of getting a computer to perform operations but rather it isa novel formal medium for expressing ideas about methodology. thusprograms must be written for people to read, and only incidentally formachines to executeó (abelson et al., structure and interpretation of computer programs, 1996).i have used computational descriptions to communicate methodologicalideas in teaching subjects in electrical circuits and in signals and systems.jack wisdom and i have written a book and are teaching a class that usescomputational techniques to communicate a deeper understanding ofclassical mechanics. our class is targeted for advanced undergraduatesand graduate students in physics and engineering. in our class computational algorithms are used to express the methods used in the analysis ofdynamical phenomena. expressing the methods in a computer languageforces them to be unambiguous and computationally effective. studentscomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.personal passion about computer science research183are expected to read our programs and to extend them and to write newones. the task of formulating a method as a computerexecutable program and debugging that program is a powerful exercise in the learningprocess. also, once formalized procedurally, a mathematical idea becomesa tool that can be used directly to compute results.we may think that teaching engineering and science is quite removedfrom daily culture, but this is wrong. back in 1980 (a long time ago!) i waswalking around an exhibit of primitive personal computers at a tradeshow. i passed a station where a small girl (perhaps 9 years old) wastyping furiously at a computer. while i watched, she reached over to aman standing nearby and pulled on his sleeve and said: òdaddy! daddy!this computer is very smart. its basic knows about recursive definitions!ó i am sure that her father had no idea what she was talking about.but notice: the idea of a recursive definition was only a mathematicianõsdream in the 1930s. it was advanced computer science in the 1950s and1960s. by 1980 a little girl had a deep enough operational understandingof the idea to construct an effective test and to appreciate its significance.at this moment in history we are only at the beginning of an intellectual revolution based on the assimilation of computational ideas. theprevious revolutions took a long time for the consequences to actualize. itis hard to predict where this one will lead. i see one of the deepest consequences of computational thinking entering our society in the transformation of our view of ourselves. previous revolutions have entered cultureby affecting the way we think and the way we talk: we discuss economicphenomena in terms of òmarket forces.ó we talk about geopolitical developments as having òmomentum.ó we think it is hard to accomplish anorganizational change because of òinertia.ó in exactly the same way wewill find computational metaphors sneaking into our vocabulary. wealready hear ourselves describing some social interactions as ònetworking.ó we may òpingó a friend to see if he òacks,ó indicating that he isavailable. but these are still rather superficial. more telling is the fact thatwe can describe people and organizations as having òbugs,ó and thatthese can be òpatched.ó perhaps the most important consequence of computational thinking will be in the development of an understanding ofourselves as computational beings. indeed, my personal experience as acomputer programmer has made me aware that many of my own problems are bugs that can be analyzed and debugged, often with great effort,and sometimes patched.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.184computer science: reflectionsfairy talesallen newell, carnegie mellon universitynote: this essay is reproduced by permission ofallen newell archives, carnegie mellon university,in the print (nonelectronic) form of this reportand can be read there on pages 184 through 188.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.personal passion about computer science research185note: see page 184.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.186computer science: reflectionsnote: see page 184.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.personal passion about computer science research187note: see page 184.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.188computer science: reflectionsnote: see page 184.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.personal passion about computer science research189revisiting òwhat is computer scienceóallen newell, carnegie mellon universitynote: this essay is reproduced by permission ofallen newell archives, carnegie mellon university,in the print (nonelectronic) form of this reportand can be read there on pages 189 through 192.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.190computer science: reflectionsnote: see page 189.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.personal passion about computer science research191note: see page 189.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.192computer science: reflectionsnote: see page 189.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.193appendixagenda of july 2526, 2001, symposiumwednesday, july 25, 20011:30 p.m.opening remarksmary shaw, carnegie mellon university, and chair, committee on the fundamentals of computer science1:45  3:15session 1: impacts of computer scienceedward l. ayers, university of virginiañunderstandingthe past as informationsusan landau, sun microsystemsñthe profound effect ofcs on the practice and teaching of mathematicsmichael lesk, national science foundationñcomputerscience is to information as chemistry is to matter3:15  3:30break3:30  4:00guest speakerwilliam a. wulf, national academy of engineeringñtheengineering and science fundamentals of computer sciencecomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.194appendix4:00  5:30session 2: sampling of hard research questions incomputer sciencesriram rajamani, microsoft researchñspecifying and checking properties of programslillian lee, cornell universityñóiõm sorry dave, iõm afraidi canõt do tható: linguistics, statistics, and natural languageprocessing in 2001chee yap, new york universityñtoward robust geometriccomputation5:30reception6:30 p.m.dinnerthursday, july 26, 20017:30 a.m.continental breakfast8:30  10:30session 3: cs research: content and characterursula martin, university of st. andrewsñwhat is computer science?ñthe european perspectiveneil immerman, university of massachusetts, amherstñon the unusual effectiveness of logic in computer scienceamy bruckman, georgia institute of technologyñsynergiesbetween educational theory and computer sciencegerald sussman, massachusetts institute of technologyñthe legacy of computer science10:30  10:45break10:45  12:00wrapup discussionñwhat makes computer sciencevital and exciting?allparticipant discussion, moderated by jim foley, georgiainstitute of technologycomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.plate 1computergenerated water being poured into a glass.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.2computer science: reflectionsplate 2a ball making a splash in a tank of computergenerated water.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.plates3plate 3computergenerated smoke rolls past a sphere (a) and a computergenerated candle flame (b).abcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.4computer science: reflectionsplate 4computer simulation of a shock wave impinging on a helium bubble.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.plates5plate 5(a) mri data points from a ratõs brain and (b) computer reconstructionof the brain geometry.abcomputer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.6computer science: reflectionsplate 6computer simulation of a running skeleton with biceps.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.plates7plate 7a tetrahedral muscle mesh ready for finite element simulation.computer science: reflections on the field, reflections from the fieldcopyright national academy of sciences. all rights reserved.8computer science: reflectionsplate 8computer simulation of a piece of draped cloth.